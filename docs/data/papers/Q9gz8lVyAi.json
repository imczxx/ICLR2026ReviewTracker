{"id": "Q9gz8lVyAi", "number": 2189, "cdate": 1757015847213, "mdate": 1759898163857, "content": {"title": "IAGA: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation", "abstract": "Gaussian Probability Path based Generative Models (GPPGMs) generate data by reversing a stochastic process that progressively corrupts samples with Gaussian noise. Despite state-of-the-art results in 3D molecular generation, their deployment is hindered by the high cost of long generative trajectories, often requiring hundreds to thousands of steps during training and sampling. In this work, we propose a principled method, named IAGA, to improve generation efficiency without sacrificing training granularity or inference fidelity of GPPGMs. Our key insight is that different data modalities converge to Gaussianity at markedly different rates during the forward process. Based on this observation, we analytically identify a characteristic step at which molecular data attains sufficient Gaussianity, after which the trajectory can be replaced by a closed-form Gaussian approximation. Unlike existing accelerators that coarsen or reformulate trajectories, our approach preserves full-resolution learning dynamics while avoiding redundant transport through distributional states with little data identity. Experiments on 3D molecular generation benchmarks demonstrate that our IAGA achieves substantial improvement on both generation quality and computational efficiency.", "tldr": "", "keywords": ["Gaussian approximation", "data identity", "efficient generation", "3D molecular generation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/271043697c8cfe2eb96411841504261fdf29df8b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to improve the training and sampling efficiency of diffusion models by truncating the diffusion noise-adding process. For zero-mean data, the authors design an empirical method to select diffusion timesteps, which is validated on molecular generation tasks. Experimental results show performance improvements over methods like EDM and GeoLDM."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is simple and serves as a \"free lunch\": it eliminates redundant training and sampling overhead in existing models, boosting both efficiency and generation quality.\n\n2. The writing is clear and easy to follow"}, "weaknesses": {"value": "1. The core idea of truncating the noise-adding process is overly straightforward. Similar concepts have already been explored in iDDPM, where the redundancy of original diffusion schedules was identified and new schedules were designed. This method essentially truncates the existing schedule, which introduces two critical issues:\n    - Theoretical inconsistency: Truncation increases the approximation error of the Gaussian prior. The distance between $x_{T^*}$ and the standard Gaussian is larger than that between original $x_T$ and the standard Gaussian.\n    - Practical inconvenience: A new hyperparameter $T^*$ (or two hyperparameters $\\epsilon_{dep}$, $\\epsilon_{DS}$) is introduced , which needs to tune through experiments. \n\nInstead of truncation, a more viable approach could be adjusting the SNR decay rate of the schedule to slow down decay, thereby compressing the time interval during which the distribution approximates a Gaussian.\n\n2. Limited significance of the method: Performance improvements are more pronounced for models with DDPM schedule (e.g., EDM, GeoLDM). Intuitively, the DDPM schedule has a fast SNR decay rate, leading to many high-noise timesteps—hence the truncation method yields more obvious gains. However, for methods that already address schedule redundancy (e.g., GeoBFN, SLDM), the efficiency improvements from this approach are expected to be far more limited.\n\n3. Gaps in research logic and writing: \n    - The paper introduces three metrics to measure the discrepancy between the current distribution and a Gaussian distribution: cumulant tensors (Eq. 8), mutual information (Eq. 12), and cumulative distribution functions (Eq. 14). The theoretical analysis relies on the first metric, while the second and third are used in experiments—creating a logical gap. Is there a need to include so many similar metrics? Would retaining the least computationally expensive one suffice?\n    - Eq. 26 lacks implementation details for \\text{MI}_{\\text{rows}}: Does it compute the mutual information between all pairs of rows and then average the results? Eq. 12: Does it use row-wise or column-wise mutual information? What is the physical meaning of rows and columns? For the matrix representation of $x_t$: If $x_t$ represents coordinates, is the matrix of size $N \\times 3$ (where N is the number of atoms) or $(3N) \\times d_{\\text{feat}}$ (where $d_{\\text{feat}}$ is the feature dimension)? If the latter, does the independence measured by mutual information depend on the network used to extract features?\n    - Unclear visualization in Figure 3: The \"Gaussianity\" in Figure 3 is represented by colors; numerical values would be more concrete. \n\nRef:\nImproved Denoising Diffusion Probabilistic Models.\n\nStraight-Line Diffusion Model for Efficient 3D Molecular Generation.\n\nUnified Generative Modeling of 3D Molecules via Bayesian Flow Networks."}, "questions": {"value": "1. Is the zero-mean assumption a major limitation of your method? Could normalization be applied to handle data with non-zero means, regardless of the original data distribution?\n2. What would be the performance if timestep truncation is applied only during sampling (not training)? Would this harm generation quality?\n3. A curious question: for element types alone, is their distribution closer to a Gaussian than that of images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VKv88LMa5B", "forum": "Q9gz8lVyAi", "replyto": "Q9gz8lVyAi", "signatures": ["ICLR.cc/2026/Conference/Submission2189/Reviewer_FhFk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2189/Reviewer_FhFk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831517869, "cdate": 1761831517869, "tmdate": 1762916119706, "mdate": 1762916119706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an algorithm to calibrate the noise schedule for diffusion model training and sampling. The central idea is to identify a minimal log signal-to-noise ratio (log-SNR), $\\lambda_\\min$ (in the sense of [1]), at which the noised latent variable attains sufficient Gaussianity. The schedule is then truncated at this point to avoid unnecessary and potentially harmful levels of noising. The effectiveness of this calibration method is demonstrated on several diffusion models for molecule generation.\n\n[1] Kingma, Diederik, and Ruiqi Gao. \"Understanding diffusion objectives as the elbo with simple data augmentation.\" Advances in Neural Information Processing Systems 36 (2023): 65484-65516."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Clear Motivation: The paper is well-motivated, addressing the common and practical problem of inefficiently wide noise schedules in diffusion models.\n- Strong Empirical Support: A key strength is the empirical demonstration that baseline models often operate on an unnecessarily broad SNR range. The results convincingly support the claim that truncating this range to an \"effective\" one can be done without degrading model performance.\n- Practical Significance: The proposed method offers a practical tool for scheduler calibration. A significant benefit is the potential to accelerate inference by reducing the number of sampling steps, even when applied post-hoc to models trained with a standard, non-calibrated scheduler."}, "weaknesses": {"value": "- Insufficient Positioning: A significant weakness is the lack of thorough positioning against the extensive existing literature on noise schedulers, SNR analysis, and related diffusion model theory (e.g., [1,2,3]). This omission causes the work to feel disconnected from established formalism in the area. Consequently, the development of the method appears somewhat ad-hoc rather than being rigorously derived from first principles.\n- Clarity and Readability: The paper's clarity could be improved. A considerable portion of the text is dedicated to standard, boilerplate descriptions of diffusion models, which detracts from the space available to elaborate on the novel contributions.\n- Undefined Key Terminology: A central term, \"data identity,\" is used repeatedly throughout the manuscript but is never formally defined. This ambiguity leaves a core concept of the paper open to interpretation and hinders the reader's ability to fully grasp the method.\n- Unconventional Presentation: In a minor editorial point, the results tables use boldface to highlight the proposed method's results rather than the conventional practice of bolding the best-performing method for a given metric. This unconventional choice makes direct performance comparisons less intuitive.\n\n[1] Kingma, Diederik, and Ruiqi Gao. \"Understanding diffusion objectives as the elbo with simple data augmentation.\" Advances in Neural Information Processing Systems 36 (2023): 65484-65516.\n\n[2] Falck, Fabian, et al. \"A Fourier Space Perspective on Diffusion Models.\" arXiv preprint arXiv:2505.11278 (2025).\n\n[3] Gao, Ruiqi, et al. \"Diffusion models and gaussian flow matching: Two sides of the same coin.\" The Fourth Blogpost Track at ICLR 2025. 2025."}, "questions": {"value": "1. Could the authors please provide a formal, mathematical definition for the term \"data identity\" as it is used in the context of this paper?\n2. The paper mentions benefits for training. Could the authors clarify precisely how the proposed calibration method reduces training time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BG7Vt3Dp9f", "forum": "Q9gz8lVyAi", "replyto": "Q9gz8lVyAi", "signatures": ["ICLR.cc/2026/Conference/Submission2189/Reviewer_GRXx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2189/Reviewer_GRXx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923316912, "cdate": 1761923316912, "tmdate": 1762916119093, "mdate": 1762916119093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an interesting approach for accelerating the training and sampling procedures of 3D molecular diffusion and flow-based models using Gaussian Approximation. The key idea is that molecular data becomes \"sufficiently Gaussian\" early on in the forward noising process; therefore, the trajectory can be truncated and replaced by closed-form Gaussian distributions. Experiments yield promising results in terms of improved sampling and training efficiency, as well as enhanced generation quality, on two 3D molecular datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The idea of assessing the Gaussianity of the data during the noising process to accelerate the training and sampling is novel and sound. Additionally, leveraging this specifically for molecular data is interesting (although some claims in this regard require further clarification, as noted in the weaknesses).\n* The paper is well-written and clearly explained.\n* The experiments covering two common datasets and a few baselines are rather extensive and show promising results."}, "weaknesses": {"value": "* The main assumption in this paper is that molecular data converges to Gaussianity faster than other modalities, e.g., images. However, this assumption is neither theoretically justified nor empirically verified. Proposition 3.1 requires that the initial distribution is more Gaussian; however, can this be demonstrated? Additionally, based on Figure 2, the authors claim that image data retains recognizable features for more steps than molecular data; however, this can be misleading, as images are much more familiar to the human eye than molecules. Quantitative or qualitative results are needed to verify this claim. For instance, it would be interesting to plot the proposed dependency evaluator $Dep(x_t)$ and the distributinal similarity $D_t$ over different timesteps during the noising process for an image dataset and a molecular dataset (of course, they should be adequately scaled to accommodate the different data scales and ranges).\n* While the paper does a good job at explaining how $T^* $ is obtained, it is missing some details on how the training and sampling procedure is adapted. It seems that the segment $[T^*, T]$ is simply truncated; however, the paper should formally clarify this. For example, what are the exact modifications to the training and sampling algorithms from EDM [1]?\n* All reported experimental results lack standard deviations across different runs, which makes it hard to assess the statistical significance of the claimed results. Also, the code is not provided.\n* It would be a great addition to the paper if the proposed Gaussianity test is used in other settings beyond accelerating training and sampling procedures. In some cases, efficiency is not the primary concern, and the user may be willing to sacrifice efficiency for improved generation quality. I believe the proposed approach can be leveraged in this setting. For example, given a pre-specified noise schedule, find the total time steps $T$ such that the optimal $T^* =1000$ or use the proposed Gaussianity test to guide the design of noising schedules such that $T^*=T$. This setting would enable comparison with baselines for the same computational budget, potentially yielding improved generation results because of better allocation of time steps.\n\nI would be willing to increase my score if the authors address these points convincingly.\n\n**References**\n\n[1] Hoogeboom, Emiel, et al. \"Equivariant diffusion for molecule generation in 3d.\" International conference on machine learning. PMLR, 2022."}, "questions": {"value": "1. Is the term \"Gaussian Probability Path based Generative Models (GPPGMs)\" known in the literature? If yes, please cite the corresponding papers. If not, please somehow clarify this or use more well-known terms.\n2. Figure 1 is not clear. What is \"complex\", and what is the difference between the two backward passes? Please clarify the figure and the caption.\n3. In Equation 2, $\\alpha_{t-\\Delta t|t}$ is not defined.\n4. In Equation 7, why is $\\tilde v_t$ chosen in this way? Please clearly explain this. \n5. In Equation 10, shouldn't $m>=3$?\n6. In line 225, it is stated that \"sparse molecular coordinates around equilibrium are closer to Gaussian\". Please elaborate and give concrete qualitative or quantitative arguments.\n7. When estimating all the quantities required for computing $T^*$, e.g., the variance of the Gaussian, the dependency evaluator, and the distributional similarity, do you sample multiple noise vectors for each data sample and each noise level?\n8. What are the values of $\\epsilon_{dep}$ and $\\epsilon_{DS}$ and how are they chosen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lAnP91qZMB", "forum": "Q9gz8lVyAi", "replyto": "Q9gz8lVyAi", "signatures": ["ICLR.cc/2026/Conference/Submission2189/Reviewer_1nJS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2189/Reviewer_1nJS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948199200, "cdate": 1761948199200, "tmdate": 1762916116435, "mdate": 1762916116435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}