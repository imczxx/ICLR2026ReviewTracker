{"id": "2LmXLuCDsY", "number": 21208, "cdate": 1758314968847, "mdate": 1759896934597, "content": {"title": "Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware Code", "abstract": "Large Language Models (LLMs) are increasingly used to automate hardware design tasks, including the generation of Verilog code. While early benchmarks focus primarily on functional correctness, efficient hardware design demands additional optimization for synthesis metrics such as area, delay, and power. Existing benchmarks fall short in evaluating these aspects comprehensively: they often lack optimized baselines or testbenches for verification. To address these gaps, we present Pluto, a benchmark and evaluation framework designed to assess the efficiency of LLM-generated Verilog designs. Pluto presents a comprehensive evaluation set of 114 problems with self-checking testbenches and multiple Pareto-optimal reference implementations. Experimental results show that state-of-the-art LLMs can achieve high functional correctness, reaching 78.3\\% at pass@1, but their synthesis efficiency still lags behind expert-crafted implementations, with area efficiency of 63.8\\%, delay efficiency of 65.9\\%, and power efficiency of 64.0\\% at eff@1. This highlights the need for efficiency-aware evaluation frameworks such as Pluto to drive progress in hardware-focused LLM research.", "tldr": "Pluto is a benchmark for evaluating the ASIC synthesis efficiency of LLM-generated Verilog, offering 114 problems with testbenches and Pareto-optimal references", "keywords": ["LLM", "Verilog", "Efficiency", "Synthesis", "Functional Correctness"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0bc5c89e9adfad1e178cd851bfc616e075f54c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Existing benchmarks for LLM-generated hardware designs primarily focus on functional correctness, neglecting crucial synthesis metrics like area, delay, and power. To address this gap, this work introduces a new benchmark Pluto providing 114 problems with test benches and optimized reference designs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) This work provides a benchmark with 114 problems, each featuring individually optimized reference designs for area, delay, and power. This establishes a true Pareto front for comprehensive efficiency comparison.\n\n2) This work provides test benches that automatically adapt to different latency requirements and optimization goals, enabling a robust and fair evaluation of designs under various efficiency constraints."}, "weaknesses": {"value": "1) This benchmark relies entirely on expert-designed golden truths and test benches, making it difficult to expand for large-scale assessment.\n\n2) The fundamental need for manually crafted golden implementations is not convincing. It is proposed that predicting Pareto-optimal metrics could potentially eliminate the high cost of expert design and serve the benchmark's evaluation purpose.\n\n3) The benchmark's collection of 114 problems from existing sources (ChipDev, VerilogEval, etc.) is questioned. The core concerns are a lack of justification for its superiority over other benchmarks and an unclear curation strategy, which appears potentially random rather than methodically designed.\n\n4) It is suggested that the lower eff@k scores compared to pass@k might be partly due to a lack of explicit optimization goals and strategies in the prompts, rather than solely the LLMs' inability."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jePk1ncxNK", "forum": "2LmXLuCDsY", "replyto": "2LmXLuCDsY", "signatures": ["ICLR.cc/2026/Conference/Submission21208/Reviewer_jfoZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21208/Reviewer_jfoZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658959767, "cdate": 1761658959767, "tmdate": 1762941616761, "mdate": 1762941616761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Pluto, a new benchmark and evaluation framework focused on assessing both the functional correctness and synthesis efficiency (area, delay, power) of Verilog code generated by large language models (LLMs). Pluto comprises 114 hardware design problems, each accompanied by three hand-optimized, Pareto reference implementations targeting area, delay, and power, as well as self-checking, latency-agnostic testbenches for fair comparison. The authors show through extensive experiments that while LLMs can achieve high pass rates for correctness, their designs remain substantially less efficient compared to expert-optimized solutions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Pluto addresses clear gaps in existing hardware code benchmarks by providing multiple, per-metric (area, delay, power) Pareto-optimized solutions as ground-truth references for each problem;\n2. Problems span a range of difficulty based on established educational and open-source resources, with designs and testbenches developed by independent experts, aiming to minimize contamination and maximize relevance."}, "weaknesses": {"value": "1. While Figure 5 and 8 visualize failure modes, the paper could benefit from a richer exploration of why current LLMs fail on certain optimization tactics (beyond just metric categorization). For example, do larger models show specific structural generalization, or are there systematic syntactic or semantic limitations (e.g., loop unrolling, FSM encoding, arithmetic fusion) that impede efficiency optimization?\n2. Inadequate discussion and analysis of key results: Figure 2, which highlights reductions in area, delay, and power, is pivotal to the paper's claims but receives insufficient elaboration in the main text, leaving uncertainty about whether these improvements are representative of the entire dataset or merely selected examples. Similarly, Table 2 is limited to a small number of case studies, lacking comprehensive statistical measures such as variance or standard deviation across all tasks, which undermines the robustness of the findings and could benefit from a more thorough quantitative evaluation."}, "questions": {"value": "How exactly are upper bounds  $T_{i,j}$ in Eq. 1, Page 6 decided? Is there an objective method, or is it ad hoc?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qcjiBKuDu0", "forum": "2LmXLuCDsY", "replyto": "2LmXLuCDsY", "signatures": ["ICLR.cc/2026/Conference/Submission21208/Reviewer_MZkp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21208/Reviewer_MZkp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928451640, "cdate": 1761928451640, "tmdate": 1762941615998, "mdate": 1762941615998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Pluto, a benchmark for evaluating both functional correctness and synthesis efficiency (area, delay, and power) of LLM-generated Verilog code. It provides 114 design problems, each with three Pareto-optimal reference implementations and self-checking testbenches. Experiments across various LLMs show that the efficiency of existing models' design still lags far behind expert-designed implementations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Pluto fills a missing piece in the LLM-powered EDA field by shifting hardware code evaluation from functional correctness to efficiency.\n\n2. It provides a comprehensive dataset of 114 problems, each with per-metric optimized reference implementations and self-checking testbenches, offering a complete view of LLM-generated Verilog code evaluation.\n\n3. The study across various LLMs shows that existing models still fall short of human experts in producing highly optimized Verilog code, revealing the current limitations of LLMs."}, "weaknesses": {"value": "1. The major concern is that this work provides very limited insights for the ICLR community. While it demonstrates how human experts can craft high-quality Verilog data, it does not offer new or generalizable understanding of how to leverage or improve LLMs, e.g., through LLM-assisted data generation or methods to enhance domain-specific generation quality. The dataset contribution would be more suitable for venues such as DAC or ICCAD.\n\n2. The technical novelty is limited, as the dataset construction is entirely manual, and the resulting insight that public LLMs perform poorly in certain specialized domains is not new.\n\n3. The authors are encouraged to discuss how the crafted dataset benefits the community. It is intuitive that LLMs struggle with unseen domains, so the benchmark results are unsurprising. Moreover, if the long-term goal is finetuning, a larger and more diverse dataset would be necessary."}, "questions": {"value": "I have included my questions in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IZnDfP59B7", "forum": "2LmXLuCDsY", "replyto": "2LmXLuCDsY", "signatures": ["ICLR.cc/2026/Conference/Submission21208/Reviewer_uhb7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21208/Reviewer_uhb7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983251731, "cdate": 1761983251731, "tmdate": 1762941615472, "mdate": 1762941615472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is proposing a new dataset benchmark for testing Verilog code generation functionality. At the same time, this newly proposed benchmark identifies an area that previous datasets didn’t cover: PPA. The paper proposes a new dataset where each question has three golden standards optimized for power efficiency, delay, and area."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Built on previous datasets, the new benchmark this paper proposes includes PPA information for Skywater130HD and TSMC 65nm.\nBoth Cadence Genus and Yosys Wolf show similar optimization strength over the data, proving the results are valid.\nPer-metric ground truth for PPA is provided in the benchmark."}, "weaknesses": {"value": "1. While Figure 5(a) presents results analyzing the extent to which LLMs can achieve PPA optimization, the region corresponding to Low Functionality–High Efficiency lacks data points. For example, if a prompt requests generation of a 1024-bit adder but the LLM produces a 64-bit adder instead, the result would exhibit low functionality but high efficiency. A similar case arises if an ALU omits certain operations. Such designs can still be synthesizable and compatible with the testbench, provided appropriate pin mappings are added.\n\n2. Although the paper proposes golden standards for each PPA metric based on the prior dataset’s questions, it overlooks an essential aspect. Defining golden standards is valuable, but achieving a balance among PPA metrics is even more critical. Consider a scenario where this benchmark is used for testing, and an LLM produces a module optimization that improves latency but deviates from the golden standard due to superior area performance—should this outcome still be considered a pass? The paper does not specify any quantitative method for handling such cases of “partial” or “balanced” optimization.\n\n3. Furthermore, with 114 total tasks divided evenly into 38 per metric, the dataset remains relatively small. Its usefulness could be significantly enhanced by including hierarchical designs that integrate multiple modules.\n\n4. Additionally, the absence of FPGA or ASIC implementation results is concerning; relying solely on simulation-based PPA evaluation may undermine result robustness.\n\n5. Finally, the Pass@K explanation in the appendix is somewhat misleading. Instead of formally describing the combinatorial basis (n choose k), it presents three illustrative images for Pass@1, Pass@5, and Pass@10, which incorrectly suggest an iterative evaluation process."}, "questions": {"value": "1. Why was low functionality and high performance never triggered?\n2. Are there any designs included that are slightly more complex?\n3. What score should we give an LLM when it makes an optimization that is balanced across multiple\nmetrics?\n4. Other see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AveyrZVJxE", "forum": "2LmXLuCDsY", "replyto": "2LmXLuCDsY", "signatures": ["ICLR.cc/2026/Conference/Submission21208/Reviewer_r52R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21208/Reviewer_r52R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999673825, "cdate": 1761999673825, "tmdate": 1762941614237, "mdate": 1762941614237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}