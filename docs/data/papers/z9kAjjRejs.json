{"id": "z9kAjjRejs", "number": 20152, "cdate": 1758303087695, "mdate": 1759896998632, "content": {"title": "Decoding Dynamic Visual Experience from Calcium Imaging via Cell-Pattern-Aware SSL", "abstract": "Self-supervised learning (SSL) holds a great deal of promise for applications in neuroscience, due to the lack of large-scale, consistently labeled neural datasets. However, most neural datasets contain heterogeneous populations that mix stable, predictable cells with highly stochastic, stimulus-contingent ones, which has made it hard to identify consistent activity patterns during SSL. As a result, self-supervised pretraining has yet to show clear signs of benefits from scale on neural data. Here, we present a novel approach to self-supervised pretraining, POYO-SSL that exploits the heterogeneity of neural data to improve pretraining and achieve benefits of scale. Specifically, in POYO-SSL we pretrain only on predictable neurons---identified on the pretraining split via simple higher-order statistics (skewness and kurtosis)---then we fine-tune on the unpredictable population for downstream tasks. On the Allen Brain Observatory dataset, this strategy yields approximately 12--13\\% relative gains over from-scratch training and exhibits smooth, monotonic scaling with model size. In contrast, existing state-of-the-art baselines plateau or destabilize as model size increases. By making predictability an explicit metric for crafting the data diet, POYO-SSL turns heterogeneity from a liability into an asset, providing a robust, biologically grounded recipe for scalable neural decoding and a path toward foundation models of neural dynamics.", "tldr": "", "keywords": ["Computational Neuroscience", "Machine Learning for Science", "Neural Decoding", "Calcium Imaging", "Neural Population Heterogeneity", "Visual Reconstruction", "Scaling", "Self-Supervised Learning", "Representation Learning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/908f4b5466b1bd46a688bf0239bd7caccb1ae731.pdf", "supplementary_material": "/attachment/c0b93281131cbd16e1b2948e8ec0682f6d5c8dba.zip"}, "replies": [{"content": {"summary": {"value": "- This paper considers the problem of decoding relevant signals from neuronal activity recordings. The studied tasks aim to decode the presented visual input or its statistics from concurrent recordings of neuronal activity from the mouse visual cortex.\n\n- The paper argues that pre-sorting of neurons into 'predictable' and 'unpredictable' sets is useful for curriculum learning: the proposed deep model is first trained on a simple task and using the predictable subset. Then, the unpredictable subset is used to learn a harder task. A neuron is labeled as predictable if the 3rd and 4th order moments of its temporal activity statistics are low.\n\n- The paper proceeds to show that this way of curriculum training improves task performance and conducts ablation studies to support this finding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Curriculum training is not a new idea. Beyond a curriculum of tasks (easier to harder), the manuscript also argues for a curriculum of samples during training. This is a nuanced point and can perhaps still be considered within the classical curriculum training idea. However, its application to large-scale neural data analysis is novel.\n\n- The manuscript classifies a neuron as predictable and unpredictable based on moments of its own activity, which is easy to compute. It also shows that it corresponds to certain neuronal subsets with molecular underpinnings.\n\n- The paper suggests that it will be harder to decode the signal of interest from neurons whose activity is more variable (high 3rd, 4th moments), both because the task loss is larger (I didn't understand why only the first quadrant of top-2 PCs is shown in Fig. 2.) and those neurons have higher Fisher information for the task."}, "weaknesses": {"value": "- A main weakness of the manuscript is its claim on high-fidelity movie reconstruction from neural recordings. This would indeed be a major advance. However, my understanding is that the reconstruction is for previously seen video fragments. For such a claim, reconstruction should be demonstrated on previously unseen scenes. As is, I believe the manuscript significantly over-claims. (If my understanding is wrong, I am willing to substantially increase my score.)\n\n- The nuance introduced to curriculum training in this manuscript (a curriculum of data samples) is not a significant enough contribution that warrants publication.\n    - The generalizability of this approach is not tested. It is not clear whether this would be helpful in analyzing other neuroscience datasets (e.g., non-visual) or accomplishing benchmark machine learning tasks.\n    - It can be considered as a natural variant although I don't know if this curriculum was explicitly studied before. This does not strike me as a significant enough contribution to the curriculum training paradigm.\n\n- I don't think the scalability argument surrounding Figure 5 is established in a convincing way. (All plots trend upwards and the y-axis is zoomed-in.) I think there is insufficient evidence to conclude that the slope of the linear fit will be larger for any one model. (e.g., the conclusion depends strongly on the set of points used for such a fit.)\n\n- Statistical Regularity Hypothesis: I believe this is a hypothesis that the authors are putting forth. (Please clarify.) Self-supervised learning will obviously and clearly work better with data with statistical regularity. (e.g., it is not possible to learn noise.) That is, masked reconstruction accuracy will be higher for more predictable neurons. However, whether this extends to supervised (task) performance following masked learning is not clear and I believe that is what the authors want to propose. Predicting the orientation of the drifting grating or the reconstruction of movie frames are both supervised tasks. I think this hypothesis needs a major rethinking and multiple qualifying statements may need to be added.\n\n- Section 3.3 presents various numerical analyses, not theoretical analyses. Please consider renaming."}, "questions": {"value": "- Could you please expand the field of view of the two plots in Figure 2 so the loss at the boundaries reaches high values?\n\n- In Figure 2, does the loss correspond to the task reconstruction loss (drifting gratings? movie frame reconstruction?) or the masked loss (self-supervised)?\n\n- Was the knee-detection algorithm applied for each Cre-line? If so, why not apply it to individual neurons and admit neurons into the predictable set based on their own scores rather than based on the Cre-line they belong to?\n\n- (line 147) What does \"near-Gaussian activity\" mean?\n\n- How would the proposed POYO-SSL model perform if 'Finetune Data' is set to 'All' in Table 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ATopgOZah4", "forum": "z9kAjjRejs", "replyto": "z9kAjjRejs", "signatures": ["ICLR.cc/2026/Conference/Submission20152/Reviewer_J6ke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20152/Reviewer_J6ke"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761172889655, "cdate": 1761172889655, "tmdate": 1762933429975, "mdate": 1762933429975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes POYO-SSL, a cell-pattern-aware self-supervised pretraining scheme for decoding dynamic visual experience from mouse calcium imaging. The key idea is to pretrain only on “predictable” neurons, identified a-priori via low skewness/kurtosis of calcium traces (knee thresholds: skew≤3.51, kurt≤22.62; CRE lines SST/VIP/PVALB/NTSR1), then fine-tune on the remaining “unpredictable” neurons for downstream tasks (movie reconstruction; drifting-grating orientation). On the Allen Brain Observatory, the method reports SSIM 0.593 for direct neural-to-movie reconstruction and 55.5% accuracy on gratings, outperforming a strong from-scratch POYO+ baseline with identical capacity. Ablations (architecture, data selection, masking/aux loss) and scaling plots are provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A priori selection of predictable neurons using skewness/kurtosis is transparent (knee-based thresholds) and applied consistently without leakage (animals/sessions/neurons disjoint).\n\nArchitecture capacity controls; data-diet variants (inhibitory-only, reverse, mixed); objective variants (temporal vs random masking; CE-only; weight sweeps). These help attribute where gains come from."}, "weaknesses": {"value": "This paper relies mainly on POYO+-from-scratch leaves room for skepticism about general SOTA claims. Even if CEBRA/Neuro-BERT aren’t pixel decoders, an adapted masked-autoencoding baseline over calcium with the same U-Net decoder, or a temporal contrastive baseline (e.g., CPC-style) would help. At minimum, include a “random neuron subset (size-matched)” pretraining control to show skew/kurtosis selection matters beyond sample count and CRE composition (the paper has “mixed” and “reverse” but not “random size-matched”). \n\nThe authors state that SSIM = 0.593 is the highest reported to date for direct visual reconstruction from cellular-resolution neural recordings. Note they contrast with fMRI works (SSIM 0.19/0.365) which are different modalities and not directly comparable.\n\nIn addition, the comparison to a capacity-matched POYO+ trained from scratch is fair and clean, and there are thorough ablations (encoder/decoder variants; inhibitory-only / reverse / mixed data diets; masking vs random masking; CE-weight sweeps). However, external SSL or generative baselines adapted to calcium-to-image decoding are not included (the paper argues popular SSL methods like CEBRA/Neuro-BERT don’t target pixel-level generation). For a flagship result, adding one or two adapted published methods (or a strong masked-autoencoder baseline over calcium with the same U-Net decoder)."}, "questions": {"value": "You define “predictable” neurons via a knee-detection on skewness/kurtosis and set fixed thresholds (skew ≤ 3.51, kurt ≤ 22.62). How sensitive are results to these cutoffs? Please provide a sweep (±10–20%) or cross-validated thresholds."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iu7HAGtAP8", "forum": "z9kAjjRejs", "replyto": "z9kAjjRejs", "signatures": ["ICLR.cc/2026/Conference/Submission20152/Reviewer_ccVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20152/Reviewer_ccVq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925497647, "cdate": 1761925497647, "tmdate": 1762933394829, "mdate": 1762933394829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "●\tThis paper studies how to improve the benefits derived from pre-training for downstream decoding. The authors discover that curating the type of data used for pre-training is crucial to obtaining good scaling. Specifically, they propose to pre-train on neural data that can be said to be more regular, and then fine-tuning is done on data that is less regular. Here, statistical regularity is defined by the authors in terms of skewness and kurtosis. The pretraining objective is masked reconstruction. They find that this pretraining results in better downstream performance on a movie reconstruction task and in classifying a drifting grating."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "●\tThis answers a question about why scaling sometimes stalls even as more pretraining data is added. This is useful for the community to know.\n●\tThis paper provides an actionable lesson for anyone doing self-supervised learning: train on the easy-to-model data first."}, "weaknesses": {"value": "●\tI'm missing something very basic: why should this method work at all? If pre-training is done on the regular data first, how can the model ever learn good representations of the irregular data? Moving from one neuron type to another represents a distribution shift. What's the explanation for how the model is able to handle this shift?\n●\tIf possible, could the authors please discuss connections to other domains where this strategy would be useful. For example, in language modeling, would it be better to pre-train on regular strings first?\n●\tThe writing could be clearer. See questions below. The methods section, particularly 3.2.2. could be more clear in many places if plainer language were used. For example, take line 205: \"We employ a curriculum learning approach combining masked reconstruction with weak supervision for stable representation learning. Our weakly-supervised auxiliary loss relies on simple visual primitives (drifting gratings) as a curriculum warm-up before moving to a complex downstream movie decoding task.\" In my opinion, it would be easier to understand the following sentence: \"During pre-training, the model is trained on a joint objective, consisting of self-supervised masked reconstruction and fully-supervised classification.\" I don't believe it is correct to use the term \"weak-supervision\" here, since labels are available for all the training examples. And I think it's more common to simply refer to this type of training as \"pre-training and fine-tuning\" rather than \"curriculum learning\"."}, "questions": {"value": "●\tLine 146: This is a basic question, but what are the skewness and kurtosis being taken with respect to? The distribution of calcium values? Across what period of time?\n●\tSection 4.1: When results are reported, for example in Table 3, what neurons are being decoded from? Only the unpredictable neurons? What I'm trying to get at is this: Is the same evaluation data used across all experiments? If not, how can performances between experiments be compared as in Table 3?\n●\tI wonder, if regular activity is more beneficial for pre-training, would it be even better to produce synthetic calcium traces for pre-training that have even more regularity? Do you think this would further improve performance?\n\nMinor points:\n●\tline 190: is this a typo? Should it read: \"the latent representation of the unmasked view is then used as a target for the latent representation of the unmasked variant.\" ? That would make it fit with line 204.\n●\tFigure 1: What should the axis titles be on the calcium trace plots?\n●\tFigure 5: The caption mentions \"orange\", but the corresponding line is yellow."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KD8PQsmUKm", "forum": "z9kAjjRejs", "replyto": "z9kAjjRejs", "signatures": ["ICLR.cc/2026/Conference/Submission20152/Reviewer_jGwv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20152/Reviewer_jGwv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961039983, "cdate": 1761961039983, "tmdate": 1762933359936, "mdate": 1762933359936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces POYO-SSL, a self-supervised learning framework that addresses neural data heterogeneity by pre-training on statistically predictable neurons identified via skewness and kurtosis metrics before fine-tuning on unpredictable populations. Applied to calcium imaging data from the Allen Brain Observatory, the method achieves 12–13% relative gains in decoding dynamic visual experiences, enables high-fidelity movie reconstruction without external stimuli, and demonstrates stable scaling with model size, turning neural variability into an asset for robust representation learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Biologically Grounded Data Selection: The method innovatively leverages higher-order statistics to prioritize predictable neurons (e.g., inhibitory interneurons like SST/VIP), transforming neural heterogeneity from a challenge into an advantage for stable pre-training and improved data efficiency (1.98×gain).  \n\n2. Scalable and Task-Adaptive Architecture: POYO-SSL enables monotonic performance scaling with model size and supports diverse decoders (e.g., Skip-Connection U-Net for movie reconstruction), achieving high-fidelity results without task-specific labels."}, "weaknesses": {"value": "1. The method uses skewness ≤ 3.51 and kurtosis ≤ 22.62 as thresholds to partition neurons into predictable and unpredictable groups via a knee-detection algorithm. However, these thresholds are applied as a fixed, universal criterion without sensitivity analysis or validation across diverse datasets. The paper states: \"These thresholds were determined a priori as a single, fixed criterion to partition the dataset, not as a tunable hyperparameter, which is why a sensitivity analysis was not performed\". This risks overfitting to the Allen Brain Observatory dataset and limits generalizability. A robustness analysis (e.g., varying thresholds) would strengthen the approach.\n\n2. The skip-connection U-Net decoder is introduced for high-fidelity movie reconstruction but lacks ablation studies comparing it to other decoder designs (e.g., transformers). The description focuses on architectural choices without quantifying their individual contributions: \"Our new U-Net-inspired decoder generates frames from a single neural embedding... See Appendix F for more details\". Without isolating the decoder’s impact, it is unclear whether gains stem from the architecture or the pre-training strategy.\n\n3. The experiments compare POYO-SSL to a from-scratch baseline and POYO+ but omit broader comparisons with state-of-the-art methods like CEBRA or Neuro-BERT, arguing their architectures are not suited for direct high-fidelity visual reconstruction. However, this justification appears in an appendix, and the main text does not discuss adaptations or partial comparisons (e.g., feature extraction). This narrow scope may overstate POYO-SSL’s advantages.\n\n4. The scaling analysis (Figure 5) shows performance gains with model size but uses a limited range of capacities. The paper notes: \"Our main approach (red) unlocks consistent performance gains as model capacity increases\" , yet no details are provided about the maximum size tested or computational constraints. Expanding the scale range would better validate the claimed monotonic scaling.\n\n5. The theoretical analysis (e.g., loss landscape, Fisher Information) relies on projections and approximations without uncertainty quantification. For instance, the loss landscape roughness metrics ($\\sigma_L$) are derived from smoothed visualizations , which may hide variability.\n\n6. The paper emphasizes biological grounding but does not validate neuronal predictability against ground-truth cell-type properties beyond statistical correlations. While skewness/kurtosis align with inhibitory/excitatory roles, causal experiments are absent."}, "questions": {"value": "Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5v28pYkrrC", "forum": "z9kAjjRejs", "replyto": "z9kAjjRejs", "signatures": ["ICLR.cc/2026/Conference/Submission20152/Reviewer_xiui"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20152/Reviewer_xiui"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966718906, "cdate": 1761966718906, "tmdate": 1762933331494, "mdate": 1762933331494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}