{"id": "XNZlaKDhnZ", "number": 17430, "cdate": 1758275929370, "mdate": 1759897175896, "content": {"title": "Federated Flow Matching", "abstract": "Data today is decentralized, generated and stored across devices and institutions where privacy, ownership, and regulation prevent centralization. This motivates the need to train generative models directly from distributed data locally without central aggregation. In this paper, we introduce Federated Flow Matching (FFM), a framework for training flow matching models under privacy constraints. Specifically, we first examine FFM-vanilla, where each client trains locally with independent source and target couplings, preserving privacy but yielding curved flows that slow inference. We then develop FFM-LOT, which employs local optimal transport couplings to improve straightness within each client but lacks global consistency under heterogeneous data. Finally, we propose FFM-GOT, a federated strategy based on the semi-dual formulation of optimal transport, where a shared global potential function coordinates couplings across clients. Experiments on synthetic and image datasets show that FFM enables privacy-preserving training while enhancing both the flow straightness and sample quality in federated settings, with performance comparable to the centralized baseline.", "tldr": "", "keywords": ["Federated learning", "flow matching", "optimal transport"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e0682df0529c626dd82dd5e46b48525444b5c3f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Federated Flow Matching (FFM), a framework for training flow-based generative models on decentralized data while preserving privacy. Unlike conventional flow matching, which assumes centralized datasets, FFM enables learning across multiple clients that keep their data local and only share model updates through federated averaging. The main challenge lies in defining effective couplings between a shared source distribution and an aggregated global target without directly accessing client data. To address this, the paper proposes three variants: FFM-Vanilla, which uses independent source–target pairings on each client and serves as a simple, privacy-preserving baseline but produces curved probability flows and slow inference; FFM-LOT, which computes local optimal transport (OT) plans per client to yield straighter trajectories and faster inference, though it suffers from sub-optimality under heterogeneous (non-IID) data; and FFM-GOT, which leverages the semi-dual formulation of OT to jointly learn a global dual potential that coordinates local couplings and approximates the true global OT plan, achieving globally straight flows and efficient inference at higher computational cost. The proposal has been experimentally evaluated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a timely and important problem — extending flow matching to the federated learning setting — enabling privacy-preserving generative modeling without centralizing data.\n\n2. It introduces a clear and systematic framework (FFM) with three variants (FFM-Vanilla, FFM-LOT, FFM-GOT), each addressing limitations of the previous one and providing a transparent progression in design.\n\n3. The FFM-GOT method is novel, using the semi-dual formulation of optimal transport to coordinate global couplings across clients without sharing raw data, representing a significant conceptual and technical advance.\n\n4. Experimental evaluation of the approach."}, "weaknesses": {"value": "1. The paper presents FFM as a privacy-preserving framework, but to the best of my understanding the only privacy mechanism used is the standard federated averaging (FedAvg) exchange of model gradients. While this ensures that raw data remain local, it does not prevent potential information leakage through gradients. No formal threat model, differential privacy (DP) guarantees, or secure aggregation mechanisms are discussed. As a result, the privacy protection is qualitative rather than theoretically supported.\n\n2. Theorem 1 rigorously quantifies how the mixture of local OT plans becomes sub-optimal under heterogeneous data, and experiments confirm that FFM-LOT’s performance degrades when the Dirichlet α decreases (α = 0.1). However, I was wondering if the paper should propose any mitigation strategies to handle non-IID settings, which are common in real federated scenarios.\n\n3. The global OT variant depends on approximating the  c-transform via a small number of gradient-descent steps and a finite pool of candidate source samples. The paper acknowledges that these approximations accumulate and degrade performance at higher NFEs but does not provide a quantitative analysis of their impact or guidance on choosing the number of steps or candidate size. \n\n4. The reported training times show significant overhead. However, the paper does not analyze communication rounds, data transfer size, or scalability with respect to the number of clients. These are critical aspects for federated systems, where communication efficiency is often a major bottleneck.\n\n5. Experiments involve only three or four simulated clients on moderate-sized datasets (CIFAR-10 and ImageNet64-500). This setup does not reflect the scale or variability of practical federated learning, where tens or hundreds of clients may participate asynchronously. Evaluating FFM under such conditions would strengthen the evidence of robustness and scalability.\n\n6. The experimental section compares FFM variants only against the centralized OT-CFM baseline. Although the related-work section discusses federated diffusion models and GAN-based methods, these are not included in the empirical comparison. Including at least one federated generative baseline would clarify the relative advantages and trade-offs of FFM.\n\n7.  Although architectures and some hyperparameters are listed, certain implementation choices are missing. Public release of code would significantly improve reproducibility."}, "questions": {"value": "1. Could you please clarify the specific privacy guarantees of FFM beyond keeping data local — for example, whether gradient leakage, differential privacy, or secure aggregation have been considered or analyzed?\n\n2. Given that FFM-LOT degrades under high heterogeneity (e.g., α = 0.1), do you plan to incorporate any mitigation strategy to improve robustness in non-IID settings?\n\n3. Can you provide quantitative results or ablations showing how the number of gradient-descent steps and candidate-pool size in the \nc-transform approximation affect generation quality and convergence?\n\n4. Could the you please report the number of communication rounds, data transmitted per round, and discuss how the computational and communication cost would scale as the number of clients increases?\n\n5. Since experiments involve only three or four simulated clients, do you plan to evaluate FFM on larger-scale or more realistic federated settings with many heterogeneous clients or partial participation?\n\n6. Why were federated generative baselines not included in the comparisons, and could you discuss how FFM is expected to perform relative to such methods?\n\n7. Will the authors release code or provide additional implementation details (e.g., OT solver parameters, communication intervals, dual-potential updates) to ensure full reproducibility of the reported results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FEDCmGS9Ft", "forum": "XNZlaKDhnZ", "replyto": "XNZlaKDhnZ", "signatures": ["ICLR.cc/2026/Conference/Submission17430/Reviewer_H4d7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17430/Reviewer_H4d7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669511493, "cdate": 1761669511493, "tmdate": 1762927322613, "mdate": 1762927322613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces “Federated Flow Matching” (FFM), a framework aimed at training generative models on decentralized data while ensuring privacy by not centralizing raw data. It presents three methodologies: FFM-vanilla, which provides a stable baseline but results in curved flows; FFM-LOT, which leverages local optimal transport plans for improved inference efficiency; and FFM-GOT, which approximates a global optimal transport plan for straighter flows, yielding better performance despite increased computational demands. Experiments demonstrate that FFM significantly enhances sample quality and inference speed compared to centralized methods, addressing critical challenges in privacy-preserving training across distributed data sources."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formalizes the idea of performing flow matching in a federated learning setting, which is both timely and conceptually valuable given the growing interest in privacy-preserving generative modeling.\n\n2. The analysis connecting federated training and optimal transport (OT) couplings provides a meaningful mathematical perspective on why decentralized training induces curved or inconsistent flows.\n\n3. The progression from FFM-Vanilla -> FFM-LOT -> FFM-GOT is logically structured, with each variant addressing limitations of the previous one."}, "weaknesses": {"value": "The reviewer's main concerns stem from the experimental result.\n\n1. The number of clients used in the experiments (n = 2/3/4) is too small to represent a realistic federated learning scenario, let alone capture the challenges of non-IID data distributions. \n\n2. The Imagenet64-500 experiment lacks detailed reporting (compared with Table 1), making it unclear how results were obtained and why some details are missing.\n\n3. The proposed FFM-LOT and FFM-GOT methods show only limited empirical improvement compared to vanilla federated learning. In some cases, their performance is even worse than the baseline (Table 1). Moreover, while the methods perform better at certain NFE ranges (either larger or smaller), they underperform in the opposite setting compared with FFM-vanilla. This inconsistent behavior raises concerns about whether the added algorithmic complexity offers practical value in real-world federated scenarios.\n\n4. Although the authors claim that FFM-GOT is suitable for few-step generation, the experimental results show that when the number of function evaluations (NFE) is small (e.g., 3–10), the sample quality drops sharply, making the comparison less meaningful. To better demonstrate the advantages of GOT in low-step regimes, it is recommended to include comparisons with rectified-flow models or other flow-matching variants that are specifically designed for efficient few-step generation.\n\n5. The paper mainly reports FID as the evaluation metric. Including additional measures such as Inception Score (IS) or other quality/diversity metrics would provide a more comprehensive assessment. Moreover, in the CIFAR experiments, the federated setting surprisingly outperforms the centralized baseline, which contradicts the common understanding in federated learning. Adding more metrics would help clarify this phenomenon and reveal whether the observed FID improvement comes at other potential costs (e.g., reduced diversity or overfitting)."}, "questions": {"value": "1. Could the author include additional evaluation metrics such as Inception Score (IS) or diversity-related measures? This would help clarify why the federated version sometimes outperforms the centralized baseline and whether the FID improvement involves any trade-offs.\n\n2. The experiments use only a few clients, which may not sufficiently represent a realistic federated learning environment. Could you test your method under a larger number of clients or with more heterogeneous (non-IID) data to validate scalability?\n\n3. Could the author complete Table 2 by including additional comparisons—such as the centralized baseline and results with larger NFE values—similar to those presented in Table 1?\n\n4. The advantages of FFM-LOT and FFM-GOT are currently unclear. Although GOT is presented as an improvement over LOT, the results show GOT has almost no benefit for high-FID generation. Could the authors consider using a few-step generative model in the federated training setting to better demonstrate the potential advantages of GOT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DNy9hDP3fY", "forum": "XNZlaKDhnZ", "replyto": "XNZlaKDhnZ", "signatures": ["ICLR.cc/2026/Conference/Submission17430/Reviewer_tdSr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17430/Reviewer_tdSr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771109690, "cdate": 1761771109690, "tmdate": 1762927322203, "mdate": 1762927322203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Based on my understanding, this work introduces Federated Flow Matching (FFM), an approach that extends classical federated optimization methods such as FedAvg and FedProx to flow-based generative models. Instead of averaging model parameters directly, FFM aligns local and global flow fields through a flow-consistency regularization term, aiming to stabilize training under non-IID data distributions. The proposed framework is evaluated on standard image generation benchmarks, showing improved convergence and generation quality compared with basic federated baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a coherent extension of federated optimization to generative models.\nExtending federated learning to flow-based generation addresses an important gap, as most prior FL works have focused on discriminative tasks.\nSpecifically, the loss terms proposed in Section 3.1 and 3.2 are clearly formulated and effectively reduces client drift in non-IID settings.\nExperimental results demonstrate stable convergence and better global consistency than FedAvg and FedProx, confirming that the method is functional and correctly implemented."}, "weaknesses": {"value": "1. The conceptual novelty is limited, as the framework closely mirrors FedAvg and FedProx.\nThe proposed flow-consistency term serves a similar role to the proximal regularization used in FedProx—both aim to prevent local updates from drifting too far from the global model under non-IID data.\nThe main difference lies in applying this constraint to function space (flow fields) rather than parameter space, which is a natural and technically reasonable extension but not a new optimization paradigm.\nFurthermore, the paper lacks theoretical analysis (e.g., convergence or stability proofs) and does not clearly articulate whether operating in flow space introduces fundamentally new behaviors compared with existing regularized FedAvg variants.\nOverall, the contribution feels incremental.\nThis is my understanding; if I have misunderstood the intended contribution, I welcome clarification in the rebuttal.\n\n2. Experiments are limited to small-scale 2D image generation tasks.\nThe claim of scalability would be more convincing with results on larger or more diverse generative tasks, such as conditional or video generation.\nMoreover, the empirical improvements over strong baselines remain modest, showing better stability but not clear superiority.\n\n3. The number of clients is set to only 3 for CIFAR-10 and 4 for ImageNet64-500 (Section 4.1). This setting seems too small to represent realistic federated scenarios and makes the scalability claim less convincing. It would be helpful if the authors could justify this choice or report results under larger client counts.\n“We simulate a federated learning environment with 3 clients. The CIFAR-10 training set is partitioned in a non-IID manner using a Dirichlet distribution with a concentration parameter α. Client weights λᵢ are set proportional to their respective dataset sizes. In each training step, all 3 clients participate.”\n“We further evaluate generative performance on ImageNet64-500, a more challenging benchmark constructed from 500 ImageNet classes, each containing 500 samples, yielding a total of 250,000 images. We simulate n = 4 clients.”\n\nThe equation on page 5 is missing a reference number, by the way."}, "questions": {"value": "Please check Weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y9IDTCCEUc", "forum": "XNZlaKDhnZ", "replyto": "XNZlaKDhnZ", "signatures": ["ICLR.cc/2026/Conference/Submission17430/Reviewer_czNW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17430/Reviewer_czNW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796653541, "cdate": 1761796653541, "tmdate": 1762927321820, "mdate": 1762927321820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}