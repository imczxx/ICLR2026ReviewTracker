{"id": "W2pqpjROnf", "number": 23983, "cdate": 1758351326633, "mdate": 1759896787866, "content": {"title": "Enhancing Multimodal LLMs Reasoning via Perception Reward Modeling", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly improved the reasoning capabilities of large language models (LLMs). Recent research has also extended it to multimodal large language models (MLLMs) to enhance multimodal reasoning. However, through systematic error analysis, we find that while RLVR effectively reduces reasoning errors in MLLMs, it fails to address perceptual errors, which often lead to incorrect inference results. Limited visual perception is a major bottleneck in multimodal reasoning. To address this issue, we propose a novel visual perception-enhanced reward model that explicitly encourages accurate visual understanding as a prerequisite for reasoning. Specifically, our approach first incentivizes accurate visual perception prior to reasoning and then assigns a perception-based reward to reinforce correct understanding of the visual input. Extensive experiments on multiple multimodal reasoning benchmarks demonstrate that our approach effectively alleviates the perceptual bottleneck and promotes more reliable multimodal reasoning.", "tldr": "", "keywords": ["Multimodal reasoning", "reinforcement learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a31a8ecb85fb3f3dc6875dbaa1c13a3b1c05d5b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies that while Reinforcement Learning with Verifiable Rewards (RLVR) improves reasoning in multimodal models, it fails to fix errors in visual perception. To solve this, the authors propose a new reward model that explicitly incentivizes accurate visual understanding before reasoning. Their experiments show this method successfully alleviates the perception bottleneck, leading to more reliable multimodal reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It provides an error analysis that pinpoints the limitations of existing RLVR methods—reasoning errors decreased significantly, while perception errors remained largely unaddressed.\n\n- The introduced visual perception-enhanced reward model is a straightforward but effective solution, designed to incentivize accurate visual understanding before reasoning, which addresses the identified core problem.\n\n- The approach is validated through experiments on multiple benchmarks, providing concrete evidence that the method effectively alleviates the perceptual bottleneck and improves the reliability of multimodal reasoning."}, "weaknesses": {"value": "The pseudo-groundtruth captions were obtained using Gemini 2.5 Pro, but Gemini 2.5 Pro's performance on these benchmarks is not listed in Table 2. How does Gemini perform on these datasets? Would Gemini also encounter the perception errors during chain-of-thought reasoning?\n\nDoes the quality of pseudo-groundtruth captions affect RL training? How can the quality of pseudo-ground truth be validated? Is the reward signal from LLM-as-judge accurate?"}, "questions": {"value": "See the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tJ3SAt5ara", "forum": "W2pqpjROnf", "replyto": "W2pqpjROnf", "signatures": ["ICLR.cc/2026/Conference/Submission23983/Reviewer_ay8z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23983/Reviewer_ay8z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761373755062, "cdate": 1761373755062, "tmdate": 1762942885662, "mdate": 1762942885662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets a concrete gap in current RLVR-style training for multimodal LLMs: after RLVR, reasoning errors drop but perception errors persist and become the new bottleneck. The authors first do an error audit on We-Math to show this shift, then propose a perception-first RL pipeline: the model must (1) produce a structured visual caption \\<caption\\>...\\</caption\\> before (2) reasoning \\<think\\>...\\</think\\> and (3) answering \\boxed{}; a perception reward is computed by comparing the model’s caption to pseudo–ground-truth (PGT) captions generated by a strong MLLM and decomposed by an LLM judge; finally, the perception reward is multiplicatively coupled with the usual RLVR accuracy reward so that only “see correctly + answer correctly” gets high reward. On MathVerse, MathVista, MathVision, We-Math and HallusionBench, the method outperforms the base model and GRPO under the same backbone and is competitive with recent reasoning-style MLLMs."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Empirical gains on several public benchmarks: Improvements are consistent on MathVerse, MathVista and HallusionBench, and the method stays competitive on We-Math and MathVision compared to strong open and closed models; this shows the idea is not tied to a single dataset.\n- The paper is well-written and clearly structured. The narrative, from problem identification to solution and evaluation, is logical and easy to follow."}, "weaknesses": {"value": "- Conceptual overlap with prior work [1]– The core idea of introducing a visual perception reward for improving multimodal reasoning has already been presented in Xiao et al., 2025, “Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward”.\nBoth works share the same motivation (perception as the bottleneck after RLVR) and similar implementation principles (PGT-based visual supervision + reward integration). The authors should clearly articulate what is novel here—for example, whether the contribution lies in (a) the multiplicative reward coupling, (b) the structured “caption→reason→answer” output enforcement, or (c) deeper empirical diagnosis. Without such clarification, the originality claim remains ambiguous.\n- Training focuses on geometry-style math images. It is unclear whether the perception reward generalizes to other multimodal settings such as charts, UI, or natural images.\n- The paper situates itself within the RLVR/R1 line but does not thoroughly compare with contemporaneous works such as VL-Rethinker or R1-VL that also incorporate self-reflective or vision-aligned objectives.\n\n[1]Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward."}, "questions": {"value": "To strengthen the paper, the authors should (i) clarify the novelty boundary relative to that prior work, (ii) demonstrate robustness under open-source captioners and cheaper reward pipelines, and (iii) include a brief cross-domain experiment to confirm generality. Addressing these points would considerably improve both the clarity and contribution of the paper.\n- How does this work substantively differ from prior work[1]. which also employs a visual perception reward? Is the main novelty the reward coupling mechanism or the structured generation format?\n- Could the proposed perception reward extend to chart QA or document VQA tasks, where the caption length or structure differs significantly from diagram-based inputs?\n\n\n\n[1]Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The paper presents a well-motivated and empirically solid reinforcement learning framework that explicitly enhances visual perception in multimodal reasoning. The approach is sound and yields consistent improvements; however, its conceptual and methodological proximity to prior work[1] significantly weakens the originality claim.\n\n[1]Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y2UAxDWZbQ", "forum": "W2pqpjROnf", "replyto": "W2pqpjROnf", "signatures": ["ICLR.cc/2026/Conference/Submission23983/Reviewer_Yw6V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23983/Reviewer_Yw6V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978823482, "cdate": 1761978823482, "tmdate": 1762942883864, "mdate": 1762942883864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RLVR boosts MLLM reasoning but leaves perception errors unaddressed. The authors show via error analysis that after GRPO training, perception errors dominate failures. They propose a perception-first approach: the model generates a before reasoning, rewarded by semantic consistency with PGT captions (from Gemini) judged by an LLM. This perception reward is multiplicatively combined with accuracy reward (logical AND). Using modified GRPO on Qwen2.5-VL-7B, they achieve SOTA or near-SOTA on MathVerse (53.3%), MathVista (74.6%), We-Math, and HallusionBench (71.5%), confirming that fixing perception unlocks better multimodal reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper convincingly identifies perception as the dominant post-RLVR bottleneck through a rigorous 200-sample error audit and addresses it with an elegant, scalable solution: a caption-first protocol paired with a fine-grained perception reward derived from PGT captions and an LLM judge, multiplicatively fused with accuracy to enforce joint correctness. This principled design yields consistent, state-of-the-art, or near-SOTA gains across five diverse multimodal benchmarks, strongly validated by clean ablations and clear presentation."}, "weaknesses": {"value": "- Evaluation is limited to math reasoning, leaving generalization to broader multimodal tasks untested. Training on only 2.1K samples raises scalability concerns, and ablation results are inconsistent (e.g., no gain on We-Math, drop on MathVision), undermining claims of universal perception improvement.\n\n- Perception is only measured on HallusionBench. HR-Bench (high-resolution hallucination) and Vstar Bench (video+chart perception) are standard for visual reliability, yet are absent.\n- The claim “perception bottleneck solved” remains unproven outside toy geometry diagrams."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "a755olDUPB", "forum": "W2pqpjROnf", "replyto": "W2pqpjROnf", "signatures": ["ICLR.cc/2026/Conference/Submission23983/Reviewer_nXHk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23983/Reviewer_nXHk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981084104, "cdate": 1761981084104, "tmdate": 1762942883637, "mdate": 1762942883637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of enhancing multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) through perception reward modeling. Similar to Perception-R1, the authors recognize that existing Reinforcement Learning with Verifiable Rewards (RLVR) methods focusing solely on final answer correctness overlook the critical role of visual perception in multimodal reasoning. The paper proposes a perception reward modeling approach to explicitly guide MLLMs toward improving their visual understanding capabilities during reinforcement learning training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses fundamental perception bottleneck in RLVR for mm reasoning\n2. Systematic reward modeling framework: Framework for incorporating visual understanding into the reinforcement learning objective, going beyond simple accuracy-based rewards. \n3. Comprehensive experimental validation: Multiple benchmarks demonstrating improvements"}, "weaknesses": {"value": "1. Insufficient direct perception evidence : needs evaluation on dedicated perception benchmarks (BLINK, MMBench, MME)\n2. Unclear distinction from Perception-R1[1] : both papers appear to address the same problem with similar approaches\nPotential knowledge distillation conflation \n3. Improvements may come from teacher model distillation rather than pure perception enhancement,  can you prove perception improvements rather than distillation effect?\n3. Limited mechanistic analysis : needs deeper investigation of reward modeling dynamics. How were reward formulations chosen and validated?\n\n\n[1] https://arxiv.org/abs/2506.07218"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Aruvabsg0R", "forum": "W2pqpjROnf", "replyto": "W2pqpjROnf", "signatures": ["ICLR.cc/2026/Conference/Submission23983/Reviewer_9mSd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23983/Reviewer_9mSd"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997538981, "cdate": 1761997538981, "tmdate": 1762942883425, "mdate": 1762942883425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}