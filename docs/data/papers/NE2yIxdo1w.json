{"id": "NE2yIxdo1w", "number": 15254, "cdate": 1758249342775, "mdate": 1759897317810, "content": {"title": "Understanding the Dynamics of Forgetting and Generalization in Continual Learning via the Neural Tangent Kernel", "abstract": "Continual learning (CL) enables models to acquire new tasks sequentially while retaining previously learned knowledge. \nHowever, most theoretical analyses focus on simplified, converged models or restrictive data distributions and therefore fail to capture how forgetting and generalization evolve during training in more general settings. \nCurrent theory faces two fundamental challenges: (i) analyses confined to the converged regime cannot characterize intermediate training dynamics; and (ii) establishing forgetting bounds requires two-sided bounds on the population risk for each task. \nTo address these challenges, we analyze the training-time dynamics of forgetting and generalization in standard CL within the Neural Tangent Kernel (NTK) regime, showing that decreasing the loss’s Lipschitz constant and minimizing the cross-task kernel jointly reduce forgetting and improve generalization. \nSpecifically, we (i) characterize intermediate training stages via kernel gradient flow and (ii) employ Rademacher complexity to derive both upper and lower bounds on population risk. \nBuilding on these insights, we propose \\emph{OGD+}, which projects the current task’s gradient onto the orthogonal complement of the subspace spanned by gradients of the most recent task evaluated on all prior samples. \nWe further introduce \\emph{Orthogonal Penalized Gradient Descent} (OPGD), which augments OGD+ with gradient-norm penalization to jointly reduce forgetting and enhance generalization. \nExperiments on multiple benchmarks corroborate our theoretical predictions and demonstrate the effectiveness of OPGD, providing a principled pathway from theory to algorithm design in CL.", "tldr": "We analyze the training-time dynamics of forgetting and generalization in standard CL within the Neural Tangent Kernel regime", "keywords": ["Forgetting", "Generalization", "Continual Learning", "Neural Tangent Kernel"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/257896cc87354e51073b079c91687e2c86970c58.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper leverages the Neural Tangent Kernel (NTK) to derive upper bounds for both forgetting and generalization error at any step during training in vanilla CL tasks, while the bounds explicitly depending on the training steps. Two key conclusions are drawn: 1. Reducing the loss function’s Lipschitz constant (ρ) simultaneously decreases forgetting and generalization error. 2. Decreasing the magnitude of the cross-task kernel also mitigates forgetting and improves generalization. Based on these theoretical insights, the paper proposes two methods, OGD+ and OPGD, which project the current task’s gradient onto the orthogonal complement of the subspace spanned by\nthe gradients of recent tasks on all historical samples. Additionally, a gradient norm penalty (GAM) is introduced to jointly enforce “orthogonal constraints + Lipschitz reduction,” thereby suppressing forgetting while enhancing generalization. Experimental results demonstrate that the proposed methods significantly reduce errors. The writing is clear, the methodology sound, and the experimental outcomes show significant improvement"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper models the error upper bound as an explicit function of the training step, allowing it to not only identify the main factors influencing the error but also discuss the effects of “training longer or shorter” on the final performance.\n\n2. The main theorem clearly provides upper bounds for both forgetting and generalization errors, explicitly highlighting their dependence on training steps. By deriving a closed-form solution through kernel gradient flow and incorporating Rademacher complexity for two-sided control of the overall risk, the paper establishes a solid theoretical foundation. It further proposes a corresponding improvement method (OPGD) and proves its superiority, forming a coherent and logically complete framework.\n\n3. The paper is organized around a clear framework: problem → challenges → theory → analysis → algorithm→ experiments → conclusion. The overall exposition is thorough, and the figures and tables effectively present the results in a clear and intuitive manner."}, "weaknesses": {"value": "1. OPGD still requires storing a large amount of gradient data, leading to high computational and memory costs.  This may limit its applicability to large-scale models and tasks.\n\n2. Beyond the current experimental datasets, it would be valuable to introduce more complex and realistic scenarios to further validate the robustness and generalization ability of the proposed algorithm.\n\n3. Theoretical analysis occupies a large portion of the paper, while the main experimental results are relatively limited, making the demonstration of the method’s advantages less compelling."}, "questions": {"value": "1. Is there a plan to evaluate them on more realistic tasks, such as online visual data streams or natural language sequences?\n\n2. If the model deviates from the NTK assumption (e.g., CNNs), have different behaviors been observed in terms of forgetting and generalization mechanisms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h6hcV65YYq", "forum": "NE2yIxdo1w", "replyto": "NE2yIxdo1w", "signatures": ["ICLR.cc/2026/Conference/Submission15254/Reviewer_iWjj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15254/Reviewer_iWjj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807155194, "cdate": 1761807155194, "tmdate": 1762925553231, "mdate": 1762925553231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzed continual learning in the Neural Tangent Kernel regime and derive intermediate-training bounds for both forgetting and generalization using kernel gradient flow and Rademacher complexity. They show that two quantities govern CL behavior during training: (i) the Lipschitz constant of the loss w.r.t. predictions—smaller is better for both forgetting and generalization, (ii) the cross-task kernel smaller norms reduce interference and forgetting. Building on these findings, they propose OGD+, which projects the current task’s gradient onto the orthogonal complement of a subspace formed by the previous task’s gradients evaluated on all earlier samples (eliminating cross-task kernels between any task pair), and OPGD, which augments OGD+ with gradient-norm penalization to lower the Lipschitz constant."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is comprehensive, well written and well structured."}, "weaknesses": {"value": "1. What means intermediate training?\n\n2. The continual-training protocol is unclear-for example, how does the transition from task $k$ to task $k+1$ work? After training $t$ steps on task $k$, how is gradient flow for task $k+1$ initialized and defined relative to the state from task $k ?$\n\n3. The assumption also requires a bounded loss function.\n\n4. The paper did not specifically provide the forgetting and generalization error of PGN and PGN, only lemma 2 and lemma 3 provided, which is not enough; it should give the formal results like Theorem 1.\n\n5. The comparison among related work is not sufficient. There are lots of theoretical CL works that consider the regularized setting, but the paper did not discuss.\n\n6. The proposed methods are common in empirical CL studies, while the theoretical CL findings are not enough in my view."}, "questions": {"value": "Above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n7qxMcVons", "forum": "NE2yIxdo1w", "replyto": "NE2yIxdo1w", "signatures": ["ICLR.cc/2026/Conference/Submission15254/Reviewer_dpae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15254/Reviewer_dpae"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832151170, "cdate": 1761832151170, "tmdate": 1762925552594, "mdate": 1762925552594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenges of continual learning (CL), specifically focusing on the dynamics of forgetting and generalization. Continual learning aims to enable models to learn tasks sequentially without forgetting previously acquired knowledge. The authors analyze these dynamics within the Neural Tangent Kernel (NTK) regime, addressing two main challenges: characterizing intermediate training stages and establishing forgetting bounds using population risk bounds. They introduce two novel algorithms, OGD+ and Orthogonal Penalized Gradient Descent (OPGD), both of which project gradients orthogonally to reduce forgetting and enhance generalization. Their empirical studies on benchmarks like Permuted MNIST, Rotated MNIST, and Split CIFAR-100 validate the theoretical predictions and demonstrate the effectiveness of their approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Theoretical Contribution**: The paper extends the NTK-based analysis to intermediate training dynamics, which is crucial for understanding real-world continual learning scenarios. By providing both upper and lower bounds on population risk using Rademacher complexity, the authors offer rigorous theoretical insights.\n    \n2. **Algorithm Design**: OGD+ and OPGD are innovative in their approach to mitigate forgetting and improve generalization. The use of gradient orthogonality and gradient-norm penalization represents a well-founded strategy for continual learning models.\n    \n3. **Empirical Validation**: The experiments conducted on various benchmarks, such as Permuted MNIST and Split CIFAR-100, confirm the theoretical discussions, demonstrating the practical efficacy of OPGD and OGD+ compared to standard methods.\n    \n4. **Clear Presentation**: The paper clearly outlines its contributions, setting a solid baseline through comparative analysis with other methods in Table 1. This provides context for the improvements made by the proposed methods."}, "weaknesses": {"value": "1. **Incremental Algorithmic Novelty**:  The algorithms are mainly based on OGD and gradient norm penalty, which seems to be a little bit incremental. \n\n2. **Limited Applicability**: The theoretical framework is developed under the NTK regime, which might not encapsulate the full behavior of practical deep networks with finite width and more complex architectures. This limitation is acknowledged in the paper, suggesting a need for exploring applicability to diversified settings.\n    \n3. **Specific Task Benchmarks**: The empirical tests are confined to a specific set of benchmarks, potentially overlooking performance variations across other task types. While benchmarks like Permuted MNIST and Split CIFAR-100 are standard, testing across a broader spectrum could provide more insights."}, "questions": {"value": "1. **Computation Complexity**: Is the projection step computationally heavy for complex NN? How to address this problem? \n\n2. **Expansion Beyond NTK**: How would the authors suggest overcoming the limitations of the NTK-based analysis when dealing with finite-width networks or other types of architecture beyond classification tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "drvBcrbvTj", "forum": "NE2yIxdo1w", "replyto": "NE2yIxdo1w", "signatures": ["ICLR.cc/2026/Conference/Submission15254/Reviewer_Q8vv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15254/Reviewer_Q8vv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864993126, "cdate": 1761864993126, "tmdate": 1762925552048, "mdate": 1762925552048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the theoretical properties of CL in the NTK regime in the intermediate training stage, and give two insights on the role of Lipschitz and cross-task kernel on CL performance.\nWith these insights, they give two refined versions of the OGD algorithm.\nThese two algorithms are verified on standard CL datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a good paper in both theoretical and practical aspects. \n\nOn the theoretical side, intermediate-stage CL performance analysis is novel to the theoretical CL literature. Also, the Lipschitz observation is novel to the CL field. The cross-task kernel result is valid and kind of expected.\n\nOn the practical side, with the two theoretical messages, the refined algorithms refined a well-used CL algorithm on these aspects and improved its performance on standard basic CL datasets.\n\nThe logic is clear. The mathematical tools used are standard.\n\nI am pushing this paper for acceptance."}, "weaknesses": {"value": "I do not see major weaknesses in this paper."}, "questions": {"value": "How do you think the two theoretical messages can benefit other CL algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dibWZT0pCl", "forum": "NE2yIxdo1w", "replyto": "NE2yIxdo1w", "signatures": ["ICLR.cc/2026/Conference/Submission15254/Reviewer_5JLu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15254/Reviewer_5JLu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061463251, "cdate": 1762061463251, "tmdate": 1762925551363, "mdate": 1762925551363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}