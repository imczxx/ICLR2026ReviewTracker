{"id": "lVmnN8g6lW", "number": 19480, "cdate": 1758296595293, "mdate": 1759897036673, "content": {"title": "Weierstrass Positional Encoding for Vision Transformers", "abstract": "Vision Transformers (ViTs) have demonstrated remarkable success in computer vision tasks.  However, their reliance on learnable one-dimensional positional encoding  disrupts the inherent two-dimensional spatial structure of images due to patch flattening. Existing positional encoding approaches lack geometric constraints and fail to preserve a monotonic correspondence between Euclidean spatial distances and sequential index distances, thereby limiting the model's capacity to leverage spatial proximity priors effectively. Recognizing that periodicity is particularly beneficial for positional encoding, we propose Weierstrass elliptic Positional Encoding (WePE), a mathematically principled approach that encodes two-dimensional coordinates in the complex domain. This method maps the normalized two-dimensional patch coordinates onto the complex plane and constructs a compact four-dimensional positional feature based on the Weierstrass elliptic function $\\wp(z)$ and its derivative. The doubly periodic property of $\\wp(z)$ enables a principled encoding of 2D positional information, while their intrinsic lattice structure aligns naturally with the geometric regularities of patch grids in images. Their nonlinear geometric characteristics enable faithful modeling of spatial distance relationships,  while the associated algebraic addition formula allows relative positional information between arbitrary patch pairs to be derived directly from their absolute encodings. WePE is a plug-and-play, resolution-agnostic positional module that integrates seamlessly with existing ViTs. Extensive experiments demonstrate that WePE delivers consistent performance gains in most scenarios, while its implementation with precomputed lookup tables ensures that these improvements incur no noticeable computational or memory overhead. In addition, several analyses and ablation studies bring further confirmation to the effectiveness of our method.", "tldr": "We propose WePE for ViTs. By exploiting the doubly periodicity of the Weierstrass elliptic function, it avoids flattening 2D images into 1D sequences and preserves the natural geometric inductive bias disrupted by conventional encodings.", "keywords": ["Weierstrass Elliptic Function", "Positional Encoding", "Vision Transformers", "Double periodicity"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ae6435770114ac501756886db865339e836ae01a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Weierstrass elliptic Positional Encoding (WePE), a mathematically grounded positional encoding scheme for Vision Transformers (ViTs). The method leverages the doubly periodic properties of the Weierstrass elliptic function and its derivative to map 2D patch coordinates into a continuous complex domain, producing a compact four-dimensional positional feature. Unlike conventional encodings (learnable APE, RoPE, Fourier-based), WePE explicitly preserves spatial structure, guarantees a distance-decay property, and provides relative positional information through an algebraic addition formula. The authors present both theoretical justifications (complex analysis, injectivity, distance decay proof) and empirical validations on CIFAR-100, ImageNet-1k, and VTAB-1k benchmarks. Results show consistent improvements with negligible computational overhead, making WePE a plug-and-play alternative to existing positional encodings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Introduces elliptic-function–based positional encoding, a novel and rigorous approach not seen in prior work.\n\n2.Provides formal proofs of injectivity, distance-decay, and relative position modeling, grounding the method in solid mathematics.\n\n3.Plug-and-play, resolution-agnostic module with negligible overhead, validated on standard benchmarks with consistent improvements."}, "weaknesses": {"value": "1.Experiments are somewhat limited in scale. Most benchmarks are CIFAR-100, ImageNet-1k, and VTAB-1k; no large-scale pretraining (e.g., ImageNet-21k or JFT) or downstream applications (detection/segmentation) are reported, which weakens the generality claim.\n\n2.Although consistent, improvements are relatively modest compared to strong baselines, raising the question of whether the added theoretical complexity translates into sufficiently large practical benefits."}, "questions": {"value": "1.Section 2.3 (Fine-tuning adaptation): The Fourier-like approximation for fine-tuning introduces additional parameters (β, γ). How sensitive are results to these choices? \n\n2.Section 3.2 (Pre-training results, Table 2): Did the authors ensure identical training schedules and hyperparameters across all positional encoding baselines (e.g., RoPE, LieRE, FoPE)? If so, could you clarify whether hyperparameters were tuned for each method, or uniformly fixed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mKcUe3V3Cf", "forum": "lVmnN8g6lW", "replyto": "lVmnN8g6lW", "signatures": ["ICLR.cc/2026/Conference/Submission19480/Reviewer_Smeu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19480/Reviewer_Smeu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760577940913, "cdate": 1760577940913, "tmdate": 1762931388029, "mdate": 1762931388029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Weierstrass Positional Encoding (WePE), a mathematically grounded 2D positional encoding for Vision Transformers.\nInstead of using traditional sinusoidal or rotary encodings, the authors use the Weierstrass elliptic function, a doubly periodic complex function to map image coordinates onto the complex plane.\nThis design aims to preserve true 2D spatial continuity and provide better distance decay and relative-position properties.\nExperiments on CIFAR-100, ImageNet, and VTAB show small but consistent accuracy gains over existing encodings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Elegant mathematical formulation: the use of a doubly periodic complex function is theoretically appealing and fits the 2D geometry of images.\n\nContinuous and resolution-independent: WePE naturally handles different image sizes without interpolation.\n\nEmpirical improvements: consistent accuracy gains and smoother attention maps in visualization."}, "weaknesses": {"value": "Lack of broader context: while the mathematical formulation is elegant, the paper could better clarify how WePE conceptually differs from other periodic or complex-valued encodings (e.g., Fourier- or rotary-based).\n\nLimited analysis: the paper mainly reports performance improvements but provides little analysis or visualization explaining why the proposed encoding helps attention behavior.\n\nGeneralization scope unclear: it remains uncertain whether WePE benefits generalize to larger-scale or non-vision tasks, since experiments are focused on a few image benchmarks."}, "questions": {"value": "What is the computational cost of WePE compared to standard sine/cosine positional encodings?\n\nDid the authors compare WePE to high-order Fourier or complex-valued positional encodings to isolate its specific benefit?\n\nIs the improvement primarily from the double periodicity, or could similar results be achieved with a simpler 2D periodic basis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VGMMXfD4VU", "forum": "lVmnN8g6lW", "replyto": "lVmnN8g6lW", "signatures": ["ICLR.cc/2026/Conference/Submission19480/Reviewer_b6eQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19480/Reviewer_b6eQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975654831, "cdate": 1761975654831, "tmdate": 1762931387582, "mdate": 1762931387582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To overcome the limitation that the standard formulation of Vision Transformers (ViTs) uses one-dimensional positional encodings which disrupt the intrinsic two-dimensional spatial geometry of images, the paper proposes Weierstrass Positional Encoding for Vision Transformers (WePE), a mathematically principled method that preserves the intrinsic spatial structure of images through complex-domain mapping based on the Weierstrass elliptic function. By encoding 2D coordinates on the complex plane and exploiting the doubly periodic properties of the function, WePE provides a continuous, resolution-agnostic, and geometrically consistent positional representation. Extensive experiments demonstrate that this approach consistently improves performance over conventional positional encodings while introducing negligible computational or memory overhead."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "originality:\n This paper proposes a novel definition and solution to address the long-standing challenge of positional encoding in Vision Transformers (ViTs). By introducing a Weierstrass elliptic positional encoding (WePE) framework grounded in complex analysis, it provides a fresh, mathematically principled perspective on preserving 2D spatial geometry. Unlike heuristic sinusoidal or rotary encodings, WePE employs the doubly periodic Weierstrass elliptic function and its derivative to construct a compact, continuous, and resolution-invariant spatial representation. \nsignificance:\n The positional encoding problem in ViTs is fundamental to numerous vision tasks—ranging from visual grounding and visual reasoning to detection and segmentation. The proposed WePE framework provides a general, plug-and-play solution with provable geometric properties, distance-decay behavior, and strong empirical gains across benchmarks. It could potentially broadly influence both theoretical and applied research communities, setting a new direction for geometry-aware representation learning in vision transformers."}, "weaknesses": {"value": "The paper is technically rich but somewhat difficult to follow, particularly for readers who are not deeply familiar with complex analysis or elliptic function theory. While the mathematical rigor is commendable, the exposition occasionally prioritizes formal derivations over intuitive explanations. As a result, the connection between the underlying mathematical properties (e.g., periodicity, addition law, distance decay) and their concrete implications for Vision Transformer performance is not always clearly articulated."}, "questions": {"value": "Boundary Conditions & Aspect Ratios. How does the method behave for non-square inputs and unusual aspect ratios? Provide results where height/width change substantially and discuss any need to retune lattice parameters.\n\nFailure Cases & Diagnostics. The paper would benefit from explicit failure analyses (qualitative heatmaps, attention distance histograms, error vs. spatial separation). Where does the method underperform, and why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "744aBW260t", "forum": "lVmnN8g6lW", "replyto": "lVmnN8g6lW", "signatures": ["ICLR.cc/2026/Conference/Submission19480/Reviewer_DryE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19480/Reviewer_DryE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985800126, "cdate": 1761985800126, "tmdate": 1762931387115, "mdate": 1762931387115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}