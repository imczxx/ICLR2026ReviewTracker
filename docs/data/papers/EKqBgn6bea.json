{"id": "EKqBgn6bea", "number": 18880, "cdate": 1758291700415, "mdate": 1763659695320, "content": {"title": "Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning", "abstract": "We introduce an order-invariant reinforcement learning framework for black-box combinatorial optimization. Classical estimation-of-distribution algorithms (EDAs) often rely on learning explicit variable dependency graphs, which can be costly and fail to capture complex interactions efficiently. In contrast, we parameterize a multivariate autoregressive generative model trained without a fixed variable ordering. By sampling random generation orders during training—a form of information-preserving dropout—the model is encouraged to be invariant to variable order, promoting search-space diversity and shaping the model to focus on the most relevant variable dependencies, improving sample efficiency. We adapt Generalized Reinforcement Policy Optimization (GRPO) to this setting, providing stable policy-gradient updates from scale-invariant advantages. Across a wide range of benchmark algorithms and problem instances of varying sizes, our method frequently achieves the best performance and consistently avoids catastrophic failures.", "tldr": "", "keywords": ["Black-box combinatorial optimization", "Reinforcement learning", "Estimation-of-Distribution algorithm", "Policy gradient method", "Autoregressive generation", "Structural dropout"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79ff420ea38109addcfe159d1ea20dc0128ec87d.pdf", "supplementary_material": "/attachment/cd3df4310e8066b20bf642e0f3adb487f69af621.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes an order-invariant reinforcement learning framework for black-box combinatorial optimization (CO).\nIt builds on the principles of Estimation-of-Distribution Algorithms (EDAs) and policy-gradient reinforcement learning, using a neural autoregressive policy trained with random variable generation orders (to encourage order invariance).\nThe authors adapt the Generalized Reinforcement Policy Optimization (GRPO) objective for stable updates and evaluate their approach on synthetic benchmarks (NK, NK3, QUBO) comparing it against hundreds of baselines (Nevergrad algorithms, PBIL, MIMIC, BOA, Tabu search)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### Originality:\n- The paper provides a clean formal connection between Estimation-of-Distribution Algorithms (EDAs) and policy-gradient reinforcement learning. This unification, while incremental, is conceptually tidy and may serve as a bridge between the EDA and RL communities.\n- Adapts permutation-invariant training via random variable generation orders to the EDA framework, linking it to “information-preserving dropout.”\n- The use of GRPO with rank-based advantages is a technically sound choice for the black-box setting and is well-justified.\n\n### Quality & Clarity:\n- The paper is technically precise and mathematically detailed, with complete derivations and careful notation.\n- The proposed method is methodologically sound, deriving PPO/GRPO-like updates consistent with the Information-Geometric Optimization framework.\n\n### Significance:\n\n- The method avoids the NP-hard problem of learning explicit DAG structures required by classical multivariate EDAs like BOA and MIMIC.\n- Demonstrated robustness across diverse landscape types (smooth K=1 to rugged K=8) without hyperparameter tuning, which is practically valuable.\n- The neural parametrization allows polynomial scaling with problem size, unlike exponential growth in classical multivariate EDAs with contingency tables.\n- The comparison against the massive suite of Nevergrad algorithms is commendable and shows the method is competitive against a broad spectrum of state-of-the-art black-box optimizers."}, "weaknesses": {"value": "1. Limited Conceptual Novelty\n\n    - The key innovation — training across random variable orderings to promote order invariance — has strong precedents in Neural Combinatorial Optimization (NCO) and symmetry-aware learning:\n\n        - POMO [1] randomizes start nodes to enforce permutation invariance.\n\n        - Sym-NCO [2] explicitly leverages symmetricities such as rotational and reflectional invariance\n\n    - Consequently, the contribution feels more like an application of existing invariance principles to EDAs than a new algorithmic concept.\n\n2. Lack of Comparison with Architecturally Invariant Models\n    - Beyond stochastic invariance, there exists a well-established line of work on architectural permutation invariance, including Deep Sets [3] and permutation-equivariant GNNs (e.g. Transformers w/o PEs).\n    - These models encode invariance by design, using symmetric aggregation or equivariant message passing, ensuring that outputs are independent of variable order. In contrast, the paper’s approach enforces invariance statistically by randomizing orderings during training.\n    - A comparison or ablation against such architecturally invariant alternatives—for instance, parameterizing the EDA’s joint distribution with a Deep-Set–style aggregator—would clarify whether the proposed stochastic approach offers distinct advantages (e.g., improved exploration or diversity) or merely approximates an already well-understood symmetry principle.\n\n3. Slow and Sample-Inefficient Convergence\n\n    - Figures 1 and 2 show that the proposed (σ, σ′)-RL-EDA requires thousands of objective evaluations before matching or surpassing baselines, whereas simpler methods (e.g., PBIL, Tabu, or CMA-like evolutionary variants in Nevergrad) often reach comparable quality within hundreds of evaluations.\n\n    - This indicates poor sample efficiency, which is particularly problematic in black-box optimization, where function evaluations are typically expensive.\n\n    - The authors do not analyze this behavior or discuss why the policy-gradient dynamics are so slow (e.g., delayed credit assignment, stochasticity from random orders, small learning rates).\n\n    - As a result, while asymptotic performance looks competitive, the method’s practicality for real-world black-box problems is questionable.\n\n4. Marginal Empirical Advantage\n\n    - Although the method shows competitive asymptotic performance on some large instances, it shows inconsistent performance across problem regimes: it underperforms on small-scale tasks, where simpler baselines like PBIL or Tabu reach near-optimal solutions quickly, and it fails to converge on highly rugged instances such as NK3 with $K=8$. \n    - This indicates poor sample efficiency and instability under both low- and high-complexity settings, raising concerns about the method’s robustness and practical applicability in black-box optimization problems with limited evaluation budgets.\n\n5. Computational Cost and Missing Efficiency Metrics\n\n    - The algorithm introduces nontrivial computational overhead: sampling new permutations for each trajectory, maintaining multiple neural policies (per-variable networks), and performing repeated PPO-style updates. Yet the paper reports no wall-clock times, runtime scaling, or resource usage.\n\n    - Moreover, parameter sharing is mentioned as a means to scale to large $N$, but no experiment demonstrates it.\n\n6. Clarity and Presentation Issues\n\n    - The paper is dense and notation-heavy, reproducing standard policy-gradient derivations with extensive formalism that obscures the high-level idea.\n\n    - The intuitive motivation — that random orders act like dropout on dependency structure — is insightful but underexplained.\n\n    - Many main results are relegated to the appendices, and the narrative buries the empirical findings behind layers of mathematical restatement.\n\n    - The contribution could be conveyed far more effectively through conceptual diagrams and lighter notation.\n\n[1] Kwon, Y. D., Choo, J., Kim, B., Yoon, I., Gwon, Y., & Min, S. (2020). Pomo: Policy optimization with multiple optima for reinforcement learning. Advances in Neural Information Processing Systems, 33, 21188-21198.\n\n[2] Kim, M., Park, J., & Park, J. (2022). Sym-nco: Leveraging symmetricity for neural combinatorial optimization. Advances in Neural Information Processing Systems, 35, 1936-1949.\n\n[3] Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., & Smola, A. J. (2017). Deep sets. Advances in neural information processing systems, 30."}, "questions": {"value": "- Early-budget behavior: Can you report performance at fixed small budgets (e.g., 500/1k evals) and discuss how (σ,σ′) could be modified to improve early convergence (e.g., curriculum on β, λ)? \n\n- Runtime/compute: What are wall-clock times and GPU/CPU footprints vs. strong Nevergrad baselines and Tabu at matched budgets? \n\n\n- Ablate order count: How does # of permutations per update affect stability, speed, and final quality? (Training-time cost vs. benefit.) \n\n\n- Parameter sharing: Can you demonstrate shared-backbone variants (per-variable heads) to establish scaling viability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6tnArNwrRh", "forum": "EKqBgn6bea", "replyto": "EKqBgn6bea", "signatures": ["ICLR.cc/2026/Conference/Submission18880/Reviewer_5kSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18880/Reviewer_5kSk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18880/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664248994, "cdate": 1761664248994, "tmdate": 1762930845524, "mdate": 1762930845524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formulates the black-box combinatorial optimization problem as an MDP and reinforcement learning framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper formulates the problem of EDA as an MDP, allowing for the usage of RL. This allows for random orderings of variables during both training and inference, unlike other approaches (e.g. autoregression). RL-EDA outperforms 10 baselines on the QUBO and NK tasks considered, and the paper presents thorough ablations on key design questions. The paper presents possible explanations for the varying performance of various variable orderings in training and inference (i.e. $\\delta, \\delta’$-RL-EDA, $\\delta, \\sigma’$-RL-EDA, etc in Figure 1)."}, "weaknesses": {"value": "The paper does not present algorithmic novelty or new insights on the RL side, but seems to rather formulate the problem of EDA as an MDP and apply well-known, existing RL techniques to solve it. For example, the derivations in Appendix B and C seem to follow closely from proofs in the existing literature. \n\nThe approach attempts to apply insights from GRPO to avoid learning critics in on-policy RL like PPO. What is the motivation for avoiding learning value functions? \n\nIn Equation 7, Monte-Carlo samples are used for the approximation. Is there an ablation over the number of samples used here?"}, "questions": {"value": "See above section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tuIoNRXhp5", "forum": "EKqBgn6bea", "replyto": "EKqBgn6bea", "signatures": ["ICLR.cc/2026/Conference/Submission18880/Reviewer_CVoz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18880/Reviewer_CVoz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18880/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960257809, "cdate": 1761960257809, "tmdate": 1762930844873, "mdate": 1762930844873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "List of the main changes made in the new version of the paper"}, "comment": {"value": "We thank the reviewers for their insightful comments which helped us improve the paper. In response to these comments, we have submitted a new version of the paper. The changes in the new version of the paper compared to the previous version are highlighted in blue. \nBelow is a list of the main additions that have been made to this work since the last version: \n\n-  We have added a diagram (see Figure 1) to illustrate the MDP used for the generation of the individuals, as well as the generation and training processes based on an toy example.\n- We added a comparison of our method with the competitors on the neural architecture search public dataset (NAS-Bench-101) in Appendix N.\n- In Appendix Q, we propose an adaptation of our algorithm with shared parameters that scales better to solve large instances.\n- Early budget behavior of our method in comparison with the competitors are provided in Appendix O. In this appendix, we also discuss an adaptation of our algorithm with a curriculum-based approach to achieve both good results with a small budget and  competitive long term performances.\n- We propose an ablation study in Appendix P, where the GPRO advantages are replaced by approximations made by a critic neural network.\n- An ablation study is performed in Appendix R, where the auto-regressive generation of solutions is replaced by Gibbs sampling.\n- The results of a sensitivity analysis to the number of training epochs at each generation of the EDA is described in Appendix M.7.\n- Detailed CPU/GPU wall-clock times are given in comparison with strong competitors  in Table 3 of Appendix K."}}, "id": "LxNFKj3DzA", "forum": "EKqBgn6bea", "replyto": "EKqBgn6bea", "signatures": ["ICLR.cc/2026/Conference/Submission18880/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18880/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18880/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763656729611, "cdate": 1763656729611, "tmdate": 1763656729611, "mdate": 1763656729611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an order-invariant RL framework for black-box combinatorial optimization. Instead of learning explicit variable dependency graphs, it adapts an autoregressive model trained without a fixed ordering by sampling random generation orders during training. It uses GRPO as an RL solver for stable policy-gradient updates. Empirical study demonstrates robustness to structural uncertainty and adaptivity to complex, high-dimensional combinatorial search spaces."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. this paper introduces a novel discrete black-box optimization framework. It uses neural networks to capture complex interactions between variables and uses policy gradient method for optimization;\n2. the empirical study demonstrates superior performance and robustness."}, "weaknesses": {"value": "1. the presentation of this paper could be improved. The MDP formulation in Section 3.1 is somewhat difficult to follow. Including a concrete example would help clarify the setup.\n2. According to Figure 2, the baseline methods outperform the proposed approach when the number of calls to the objective function is small. Could the authors comment on this observation?\n3. It would be helpful if the authors could discuss the computational complexity of the proposed method."}, "questions": {"value": "See wekanesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oewnkvqn0P", "forum": "EKqBgn6bea", "replyto": "EKqBgn6bea", "signatures": ["ICLR.cc/2026/Conference/Submission18880/Reviewer_yHSc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18880/Reviewer_yHSc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18880/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038067774, "cdate": 1762038067774, "tmdate": 1762930843869, "mdate": 1762930843869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}