{"id": "Vu8YXDooG5", "number": 23209, "cdate": 1758340908622, "mdate": 1759896826792, "content": {"title": "PETRI: Learning Unified Cell Embeddings from Unpaired Modalities via Early-Fusion Joint Reconstruction", "abstract": "Integrating multimodal screening data is challenging because biological signals only partially overlap and cell-level pairing is frequently unavailable. Existing approaches either require pairing or fail to capture both shared and modality-specific information in an end-to-end manner. We present PETRI, an early-fusion transformer that learns a unified cell embedding from unpaired cellular images and gene expression profiles. PETRI groups cells by shared experimental context into multimodal “documents” and performs masked joint reconstruction with cross-modal attention, permitting information sharing while preserving modality-specific capacity. The resulting latent space supports construction of perturbation-level profiles by simple averaging across modalities. Applying sparse autoencoders to the embeddings reveals learned concepts that are biologically meaningful, multimodal, and retain perturbation-specific effects. To support further machine learning research, we release a blinded, matched optical pooled screen (OPS) and Perturb-seq dataset in HepG2 cells.", "tldr": "", "keywords": ["high-content screening", "cell biology", "single cell", "transcriptomics", "microscopy", "multimodal"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a8af6c5bf39112eb8f7d92d8b3ff8557f083428.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces PETRI, an early-fusion transformer framework designed to learn unified single-cell embeddings from unpaired cell morphology imaging and transcriptomic data. PETRI groups cells sharing the same perturbation into multimodal “documents” and performs masked joint reconstruction across modalities, enabling it to share information between imaging and gene expression while preserving modality-specific signals. Evaluated on both HepG2 (unpaired OPS and Perturb-seq) and Perturb-Multi (paired spatial transcriptomics) datasets, PETRI outperforms contrastive and unimodal baselines in recovering biologically meaningful perturbation profiles and cross-modal relationships. Analyses with sparse autoencoders further show that PETRI discovers interpretable multimodal cellular concepts linking morphological and molecular phenotypes, demonstrating a scalable approach for integrating heterogeneous biological assays without requiring cell-level pairing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- PETRI focuses on integrating unpaired cellular modalities (images + transcriptomics), a setting that is much more realistic in biological experiments where cell-level pairing is infeasible."}, "weaknesses": {"value": "- The paper motivates the need for integrating unpaired multimodal data but does not clearly articulate why existing late-fusion or generative alignment methods (e.g., cross-modal autoencoders, cross-domain VAEs) are insufficient.\n\n- Technical contribution is somewhat limited. The major novelty lies on the way to construct paired cell image-gene expression data from unpaired data (i.e., documents). Other technical parts — ViT/MAE for imaging, perceiver for transcriptomics, and multimodal masked modeling [1] are not novel idea in multimodal learning.\n\n- The choice of early fusion via joint reconstruction is not analytically or empirically contrasted with late-fusion or contrastive paradigms. It is unclear under what conditions early-fusion offers superior representational alignment or when it could fail (e.g., modalities with minimal mutual information).\n\n- The two core metrics—Guide Consistency and StringDB edge classification—capture perturbation-level similarity but do not fully reflect biological utility or multimodal alignment quality. There is no quantitative measure of cross-modal transfer or downstream biological inference.\n\n- Comparisons are restricted to unimodal MAEs, CLIP, and a few pre-trained models. Recent multimodal generative or diffusion-based frameworks (e.g., MoCa, scMultiVI, CellCLIP) are discussed but not empirically included, limiting fairness and completeness of the comparison.\n\n- The experiments lacks statistical rigor and scalability analysis. The authors did not provide error bars, confidence intervals, or multiple runs. Scalability to larger or more heterogeneous datasets, and sensitivity to document size, masking ratio, or latent dimension, are not thoroughly studied.\n\n- The study stops at embedding analysis; there is no demonstration that PETRI improves biological discovery tasks such as hit identification, functional gene grouping, or mechanism-of-action prediction.\n\nReferences:\n\n[1] Bachmann, Roman, et al. \"Multimae: Multi-modal multi-task masked autoencoders.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."}, "questions": {"value": "- What specific limitations of existing contrastive or late-fusion multimodal methods (e.g., CLIP, MultiVI, CellCLIP) does PETRI aim to overcome? Can you provide conceptual or empirical evidence that early-fusion leads to superior integration under unpaired settings, rather than simply matching performance?\n\n- Beyond improving embedding quality, what concrete biological or biomedical applications does PETRI enable? Could the authors provide one clear use case where PETRI’s unified embeddings yield novel biological insight that unimodal or late-fusion methods cannot?\n\n- Why is reconstruction-based self-supervision preferable to contrastive or generative alignment (e.g., VAE-style, diffusion-based)? Did you observe any stability issues or mode collapse when using the joint reconstruction objective?\n\n- The paper claims PETRI learns meaningful cross-modal attention patterns. Can you quantify or statistically validate this, rather than relying solely on visual inspection? Could you measure how much of the reconstruction or embedding variance is attributable to cross-modal information versus unimodal content?\n\n-  Are the improvements over baselines statistically significant (e.g., via multiple random seeds or bootstrapping)? Can the authors report variance or confidence intervals to confirm the robustness of observed gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QqoaApYzgQ", "forum": "Vu8YXDooG5", "replyto": "Vu8YXDooG5", "signatures": ["ICLR.cc/2026/Conference/Submission23209/Reviewer_Gmu9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23209/Reviewer_Gmu9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760986738867, "cdate": 1760986738867, "tmdate": 1762942560740, "mdate": 1762942560740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents PETRI, a multimodal transformer for biological data that unifies cell-level transcriptional gene-expression readouts with morphology images. PETRI is a self-supervised regime that combines two masked autoencoders for each data modality and uses latent token resampling to deal with the difficulties of very long sequence lengths caused by gene expression data. The model is demonstrated to perform quite well over two different datasets in terms of guide perturbation consistency and stringdb relationship retrieval against a comprehensive suit of baselines (such as unimodal methods, multimodal baselines like simple embedding concatenation and CLIP). The authors furthermore apply sparse autoencoders to analyze the multimodal concepts learned by the model, with an interesting case-study on the GO-term enrichment of these dimensions and the interpretable biological concepts obtained (e.g. DNA replication)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Releasing a new multimodal public dataset is highly valuable.\n- Valuable multimodal architecture: the PETRI architecture uses creative means to process replicates of the same perturbation across the two modalities, and the use of latent tokens addresses difficulties surrounding too many tokens that would otherwise be present.\n- Clear and high quality results: Robust benchmarking analysis with appropriate baselines is pursued by comparing their regime to unimodal and late-fusion multimodal models, and the performance improvements of PETRI hold.\n- Original and very significant: Highly relevant task is pursued and comparing results are obtained, indicating the important complementarity of multimodal biological screening assays in providing more comprehensive understandings of perturbations when unified together in the context of early-fusion, a highly motivating result that many may have held intuitively but, prior to this work, had yet to be thoroughly broached.\n- Creative additional investigation into the nature of their multimodal representation learning model: SAE is used to analyze their multimodal model demonstrates biologically relevant compelling and interesting results (see Figure 5)\n- Many avenues for relevant future work and additional research are opened up as a result of this work. I can think of a multitude; for example: (1) multidataset training: it could be interesting, e.g., to train a combined model on both Perturb-Multi and HEPG2; (2) exploring finetuning, e.g. it could make sense to pretrain a generic microscopy MAE on JUMP-CP data and then finetune that architecture here in PETRI, rather than training the image MAE from scratch; (3) varying the mask ratio, sometimes entirely masking the entire other modality, as being able to predict the other modality from one (e.g. if the transcriptomics is masked 100% and morphology only 50%) could be quite valuable if it works."}, "weaknesses": {"value": "The only weakness I see in this paper is secondary, namely that it is lacking situated references to various related work that has pursued similar studies. For some examples, I think it could be fruitful to mention the following highly related papers to this work:\n- Simple methods like control or mean baselines turn out to be hard-to-beat in transcriptomics and could be worth discussing in the context of their benchmarking; a variety of papers discuss this, e.g.: Ahlmann-Eltze, Constantin, Wolfgang Huber, and Simon Anders. \"Deep-learning-based gene perturbation effect prediction does not yet outperform simple linear baselines.\" Nature Methods (2025): 1-5.\n- The PCA with whitening approach is quite important embedding post-processing step and discussed in more detail here for both transcriptomics and morphology: Celik, Safiye, et al. \"Building, benchmarking, and exploring perturbative maps of transcriptional and morphological data.\" PLOS Computational Biology 20.10 (2024): e1012463.\n- For the latest in high-performing microscopy MAEs: Pham, Chau, Juan C. Caicedo, and Bryan A. Plummer. \"ChA-MAEViT: Unifying Channel-Aware Masked Autoencoders and Multi-Channel Vision Transformers for Improved Cross-Channel Learning.\" arXiv preprint arXiv:2503.19331 (2025), NeurIPS 2025.\n- The Kraus et al. citation in this work points to the original workshop paper, which was actually expanded upon in the following archival paper accepted at CVPR. Furthermore, it may be worth considering incorporating the improvement applied to the MAE loss function (i.e., the fourier-transform reconstruction loss) to improve the quality of the morphology MAEs trained in this work: Kraus, Oren, et al. \"Masked autoencoders for microscopy are scalable learners of cellular biology.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024.\n- On the general topic of multimodal unification of transcriptomics data with morphological features, this paper seems particularly relevant: Bendidi, Ihab, et al. \"A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features.\" arXiv preprint arXiv:2505.21317 (2025), ICML 2025.\n- Pretrained Dino-v2 has been evaluated on microscopy data in the following work, which also discusses further advancements to MAEs in the context of microscopy (e.g. the importance of curated training data and especially crucial is linear probing to identify intermediate layers of the ViT which obtain higher guide consistency and relationship prediction), which could be mentioned with the respect to e.g. the intermediate layer MST analysis in section A.6: Kenyon-Dean, Kian, et al. \"Vitally consistent: Scaling biological representation learning for cell microscopy.\" arXiv preprint arXiv:2411.02572 (2024), ICML 2025. (This work also publicly released of a CA-MAE ViT-S on HuggingFace, which is worth considering for baseline comparison: https://huggingface.co/recursionpharma/OpenPhenom)\n- On the topic of applying SAEs to biological foundation models, see: Donhauser, Konstantin, et al. \"Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models.\" arXiv preprint arXiv:2412.16247 (2024), ICML 2025."}, "questions": {"value": "What does it mean that the dataset you will release will be \"blinded\"? Would others be able to reproduce these results from the public dataset release, or are perturbation labels required to do so?\n\nWhile described as *unpaired* data here, is the context here not more typically described as *weakly paired*, i.e. same perturbation but different cells? It seems that you are sampling groups of samples with matching guide perturbation labels from the two modalities -- i.e. line 154 you describe that you \"Create batches where cells are grouped by perturbation to form sets.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UfzKo7teLk", "forum": "Vu8YXDooG5", "replyto": "Vu8YXDooG5", "signatures": ["ICLR.cc/2026/Conference/Submission23209/Reviewer_WChQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23209/Reviewer_WChQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755700318, "cdate": 1761755700318, "tmdate": 1762942560530, "mdate": 1762942560530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PETRI, a early-fusion transformer framework designed to learn unified cell embeddings from unpaired single-cell imaging and transcriptomics data. The core idea is to group cells by a shared experimental context (e.g., a perturbation) into \"multimodal documents\" and train the model on a masked joint reconstruction task, enabling information sharing via cross-modal attention. To make this document-based approach computationally tractable, the authors employ an aggressive token resampling strategy. Through experiments on two datasets, the authors demonstrate that PETRI's unified embeddings outperform unimodal and late-fusion approaches on biological benchmarks like Guide Consistency and StringDB edge classification. Furthermore, they show that applying sparse autoencoders to the embeddings reveals biologically interpretable, multimodal concepts that are robust to technical confounders, and they validate that the model indeed uses cross-modal information for reconstruction. To facilitate further research, the authors also introduce and release a new large-scale dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes PETRI, a framework that effectively integrates unpaired imaging and transcriptomics data via a joint reconstruction task, addressing a challenge in the field.\n2. The work provides strong evidence of model interpretability through multiple analyses. It demonstrates cross-modal information transfer via targeted ablation studies and attention matrix visualization. Applying sparse autoencoders (SAEs) reveals biologically meaningful, multimodal concepts that are shown to be robust to technical confounders.\n3. The authors contribute a valuable new large-scale dataset of matched imaging and omics data."}, "weaknesses": {"value": "1. The paper's best results come from embeddings taken before the data passes through the Multimodal Set Transformer (MST), the core component designed for fusion. The appendix (Table S2) confirms that performance on key benchmarks actually gets worse after the MST. This is counterintuitive and questions the practical benefit of the MST for the paper's main evaluation tasks.\n2. The analyses showing that the model learns biological concepts (Sections 4.2 and 4.3) rely heavily on specific, illustrative examples (e.g., reconstructing the BODIPY channel, showing a few representative images). The paper lacks broad, quantitative metrics to prove that these impressive findings are consistent across the entire dataset and not just in a few hand-picked cases.\n3. The comparison against the CLIP model may be unfair. CLIP is designed to work with perfectly paired data, but the main dataset in this paper is unpaired. The authors had to adapt CLIP for this different problem, which likely means it could not perform at its best. This may exaggerate the performance advantage of PETRI.\n4. The PETRI model is very computationally expensive, requiring powerful and specialized hardware to run. This is because it processes large \"documents\" of many cells at once."}, "questions": {"value": "1. In Figure S5, the reconstruction loss for one modality appears unaffected by the presence of the other modality's data. Could the authors clarify how this result aligns with the claim of successful cross-modal information integration?\n2. In Figure 4c, the identified \"Multimodal Dims.\" result in lower downstream task accuracy than \"Random Dims.\" Could the authors please explain this counter-intuitive finding?\n\nThe reviewer wrote the review. LLM was only used to improve both grammar and clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "c53gt2jT8A", "forum": "Vu8YXDooG5", "replyto": "Vu8YXDooG5", "signatures": ["ICLR.cc/2026/Conference/Submission23209/Reviewer_DpdC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23209/Reviewer_DpdC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828643738, "cdate": 1761828643738, "tmdate": 1762942560355, "mdate": 1762942560355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Learning and aligning multiple biological modalities in a shared latent space is challenging due to the absence of cell-level pairing and overlap in signals. The paper aims to address this problem by proposing PETRI, an early-fusion transformer model that learns unified cell embeddings of mult-omic modalities in a shared latent space. Cellular images and gene expressions are unpaired and processed as cell documents and learned using a set transformer architecture. Per-cell token resampling is employed and learning is carried out using the MAE reconstruction objective. PETRI, when compared to unimodal and multimodal alternatives, presents imporved GC and StringDB on HepG2 and Perturb-Multi datasets. An interpretable analysis demonstrates that PETRI utilizes cross-modal information while capturing morphological activating features at cell-level resolution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well written and organized.\n* The paper presents an intuitive perspective on multi-omic learning."}, "weaknesses": {"value": "* **Motivation and Contribution:** The paper motivates multimodal learning of biological modalities using overlap of learning signals and absence of cell-level pairings. However, contributions do not study these aspects and move tangentially towards learning and outperforming prior architectural variants. It is unclear as to how PETRI mitigates conflicting biological signals using a joint representation space. Similar to prior methods, PETRI also learns cell-level embeddings using a masked variational inference objective. This would indicate similar learning signals propogating across modalities. How does the joint space learn and capture cross-modal features? Does PETRI mitigate conflicting gradients or it fits towards a particular modality? How are cell documents arranged in the embedding space given that these are mapped from two different modules (ViT and Perceiver)? In its current form, it remains unclear on how PETRI is addressing the central problem.\n\n* **Empirical Evaluation:** Authors execute experiments on HepG2 and Perturb-Multi datasets which make use of cellular images and transcriptomics profiles. However, the work proposes PETRI from a general multi-omics multimodal learning perspective. Empirical evaluations do not capture this general claim. What happens if we consider additional biological modalities such as genomics and phenomics? Furthermore, experiments do not study the effect of each modality and the contributions presented in the work. How does PETRI behave when one of the modalities is dropped, i.e- a unimodal PETRI? What happens if token resampling is ablated or ViT and Perceiver blocks are replaced by alternative design decisions? In its current form, the paper trivially applies a transformer alternative to the bimodal problem circumventing its technical details.\n\n* **Baselines:** The paper compares PETRI to unimodal as well as multimodal baselines. However, the comparison is limited in scope. Firstly, among the architectural baselines, majoriry of the models are unimodal (imaging or transcriptomics) which does not put the paper's multimodal contributions in persepctive. Secondly, multimodal baselines are limited to architecture and contrastive learning variants which do not highlight the significance of PETRI. What happens if we compare with multimodal VAEs? How does PETRI compare to a multimodal perceiver? How does PETRI compare to CLIP variants and multimodal diffusion? In its current form, baselines and their significance remain limited.\n\n* **Interpretability:** The paper conducts an interpretability-based analysis on how PETRI captures cross-modal features. However, I am having trouble understanding and drawing conclusions from these results. It is unclear as to how the recnstruction loss indicates cross-modal information. Ideally, the reconstruction objective only indicates whether the decoder can disentangle modality-specific information from the latents in order to obtain the original mapping. How does this indicate use of cross-modal features? Similarly for SAE experiments, it remains unclear as to how differences in the number of activated dimensions indicate intepretable cross-modal features. Essentially, a wider difference indicates that more latent dimensions were activated leading to diversity in the feature set. However, this diversity does not necessarilty correlate with the use of cross-modal representations."}, "questions": {"value": "Refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7cqfEYxcc6", "forum": "Vu8YXDooG5", "replyto": "Vu8YXDooG5", "signatures": ["ICLR.cc/2026/Conference/Submission23209/Reviewer_hVW4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23209/Reviewer_hVW4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938318822, "cdate": 1761938318822, "tmdate": 1762942560054, "mdate": 1762942560054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}