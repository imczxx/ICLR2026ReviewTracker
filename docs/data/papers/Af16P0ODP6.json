{"id": "Af16P0ODP6", "number": 8123, "cdate": 1758066584331, "mdate": 1759897805560, "content": {"title": "Reforming the Mechanism: Editing Reasoning Patterns in LLMs with Circuit Reshaping", "abstract": "Large language models (LLMs) often exhibit flawed reasoning ability that undermines reliability. Existing approaches to improving reasoning typically treat it as a general and monolithic skill, applying broad training that is inefficient and unable to target specific reasoning errors. We introduce Reasoning Editing, a paradigm for selectively modifying specific reasoning patterns in LLMs while preserving other reasoning pathways. This task presents a fundamental trade-off between Generality, the ability of an edit to generalize across different tasks sharing the same reasoning pattern, and Locality, the ability to preserve other reasoning capabilities.\nThrough systematic investigation, we uncover the Circuit-Interference Law: edit interference between reasoning patterns is proportional to the overlap of their neural circuits. Guided by this principle, we propose REdit, the first framework to actively reshape neural circuits before editing, thereby modulating interference between reasoning patterns and mitigating the trade-off. REdit integrates three components: (i) Contrastive Circuit Reshaping, which directly addresses the generality-locality trade-off by disentangling overlapping circuits; (ii) Meta-Contrastive Learning, which extends transferability to novel reasoning patterns; and (iii) Dual-Level Protection, which preserves preexisting abilities by constraining reshaping update directions and regularizing task-level predictions.\nExtensive experiments with Qwen-2.5-3B on propositional logic reasoning tasks across three difficulty levels demonstrate that REdit consistently achieves superior generality and locality compared to baselines, with additional validation in mathematics showing broader potential. Our code is available at https://anonymous.4open.science/r/REdit-DBD8.", "tldr": "", "keywords": ["Mechanistic Interpretability", "Model Editing", "Circuit Reshaping"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fde1798fe90c54b12687d589e161f4ff10076ae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces REdit, a framework for reasoning editing in LLMs that extends model editing from factual correction to logical reasoning. It proposes the Circuit–Interference Law, linking neural circuit overlap with edit interference, and applies contrastive circuit reshaping with meta-learning and dual-level protection. \n\nOverall, it is well-motivated, technically sound, and creative, addressing how to modify reasoning pathways without interference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a novel perspective showing that modifying internal circuit traces can directly influence the final reasoning behavior of LLMs, which is an intriguing and valuable insight.\n\nThe proposed Circuit–Interference Law provides a principled and empirically grounded explanation connecting neural circuit overlap with editing interference, representing a fresh and meaningful contribution to mechanistic interpretability research."}, "weaknesses": {"value": "The experimental setting is limited. The authors evaluate only on a single backbone model (Qwen-2.5-3B) and a single dataset, which constrains the generality of the conclusions.\n\nThe reported improvements are modest rather than substantial, leaving some uncertainty about the practical effectiveness and scalability of the proposed approach."}, "questions": {"value": "Confused on the instance-specific noise. if edge attribution values vary substantially across samples, how are these aggregated? Is the same top-$\\tau$ threshold applied uniformly, or does it adapt to distributional variance across instances?\n\nCould the authors provide specific examples of “reasoning patterns” to clarify how they are defined and represented? Additionally, how do these reasoning patterns change after applying REdit—is there any qualitative or quantitative visualization of this transformation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d0tj8tTpnC", "forum": "Af16P0ODP6", "replyto": "Af16P0ODP6", "signatures": ["ICLR.cc/2026/Conference/Submission8123/Reviewer_cGfW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8123/Reviewer_cGfW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761427025973, "cdate": 1761427025973, "tmdate": 1762920101045, "mdate": 1762920101045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes reasoning editing, which targets edits on reasoning patterns using the proposed REdit method. Through experiments, the authors also uncovered the Circuit-Interference Law, which states that interference between circuits is proportional to their overlap."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper's idea is very interesting and shifts attention from factual knowledge toward the logic applied by models, which is a source of many flaws in their performance.\n- The Circuit-Interference Law is a significant novelty for the community."}, "weaknesses": {"value": "- The datasets and models used are limited. While it does not seem like the conclusions would differ with larger models, the use of the well-controlled ContextHub raises questions about how things might go wrong with wild, real-world data."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QPxUN1EFKJ", "forum": "Af16P0ODP6", "replyto": "Af16P0ODP6", "signatures": ["ICLR.cc/2026/Conference/Submission8123/Reviewer_33G2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8123/Reviewer_33G2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554760991, "cdate": 1761554760991, "tmdate": 1762920100577, "mdate": 1762920100577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The article presents the reasoning editing method for point-by-point modification of logical reasoning patterns in language models without losing other skills. The authors strike a balance between generalization and locality, describe the law of circuit interference, and propose the REdit system. Tests on logical and mathematical tasks show that REdit outperforms existing methods, paving the way for more precise control over model reasoning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The shift from knowledge editing to reasoning editing is original and well-motivated. The generality–locality trade-off is crisply formulated and backed by empirical evidence\n* The results include confidence intervals, which make them more reliable and indicate the stability of the findings."}, "weaknesses": {"value": "* The experiments focus only on propositional logic and structured math tasks, so they don’t fully reflect how reasoning works in more open-ended, real-world settings. It’s still unclear whether the method would hold up beyond these controlled, symbolic cases.\n* While “circuits” are central, empirical evidence that reshaped circuits correspond to interpretable submodules is limited to correlation plots. There’s no qualitative analysis of what circuits actually represent.\n* The approach is evaluated on a single model Qwen-2.5-3B, so it’s unclear whether the results would hold across different architectures or model scales."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xwrlhjgU1V", "forum": "Af16P0ODP6", "replyto": "Af16P0ODP6", "signatures": ["ICLR.cc/2026/Conference/Submission8123/Reviewer_Qw78"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8123/Reviewer_Qw78"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751020266, "cdate": 1761751020266, "tmdate": 1762920100119, "mdate": 1762920100119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposed REdit, which is one of the pioneering works in reasoning editing. The work gives a perspective from the knowledge circuit on the reasoning process, which is inspiring and interesting. Specifically, the author proposed a systematic framework for reasoning editing with the emphasis on neuron circuits, and the Redit framework combines the findings together to achieve the editing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The presentation of the paper is well-organized and convincing\n\n- The perspective of the knowledge circuit and the finding of Circuit-Interference Law is inspiring and novel."}, "weaknesses": {"value": "- The work lacks sufficient experimental validation. The current experiments in Sec. 4 are conducted on only one model (Qwen-2.5-3B) and one dataset (ContextHub), which limits the generality of the conclusions.\n\n- The choice of base model is inconsistent between the preliminary analysis in Sec. 2.2 and the main experiments in Sec. 4. In addition, the experimental setup in Sec. 3.1 is not clearly described (not sure if this is also based on Qwen-2.5-3B).\n\n- The method appears overly complex, but the final results do not show clear improvement, leaving me unconvinced about its effectiveness and practical value.\n\nOverall, I am very interested in the ideas and perspectives in this work. It would be helpful for enhancing this paper if the author can provide more results on more base models and datasets to show the effectiveness of Redit."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I0ApJhjZOL", "forum": "Af16P0ODP6", "replyto": "Af16P0ODP6", "signatures": ["ICLR.cc/2026/Conference/Submission8123/Reviewer_ro7e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8123/Reviewer_ro7e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754360954, "cdate": 1761754360954, "tmdate": 1762920099659, "mdate": 1762920099659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}