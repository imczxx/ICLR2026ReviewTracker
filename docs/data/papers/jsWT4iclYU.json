{"id": "jsWT4iclYU", "number": 15203, "cdate": 1758248950892, "mdate": 1763726637353, "content": {"title": "Toward Fine-Grained Domain Knowledge: Curriculum Pseudo-Labeling for Online Test-Time Adaptation", "abstract": "Online Test-Time Adaptation (OTTA) aims to adapt a pre-trained model to unlabeled test instances under domain shift in an online manner, where domain knowledge that the model accumulates from previously observed mini-batches directly affects its predictions on subsequent instances. Most previous OTTA methods exploit domain knowledge at a coarse-grained batch level, which prevents the model from fully absorbing the domain knowledge. To deal with this problem, we propose a novel framework CUrriculum Pseudo-Labeling for Online Test-time adaptation (CUPLOT), which further mines orderly domain knowledge at a fine-grained instance level. Specifically, CUPLOT prepares the arriving batch as a series of curricula based on the modeled relevance of domain knowledge between the model and instances. Then, the model orderly learns the instances with pseudo-labels generated by class prototypes in each curriculum. In this way, the domain knowledge is accumulated in a fine-grained manner through instances of curricula rather than mini-batches, improving the absorption of domain knowledge and the performance of the model. Theoretically, we prove that the curriculum pseudo-labels could enable the model to have a stronger adaptation ability, resulting in a tighter bound of approaching the Bayes optimal classifier on the target domain.", "tldr": "We propose a novel framework curriculum pseudo-labeling for Online Test-Time Adaptation, which further mines domain knowledge at a fine-grained instance level.", "keywords": ["Curriculum Pseudo-Labeling", "Online Test-Time Adaptation", "Domain Knowledge"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9086e70032a65e99e33530378a12284d2d07042.pdf", "supplementary_material": "/attachment/c708234a49eff2c967c508fb9272ceef3e215a49.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents CUPLOT, a curriculum-learning-based framework for achieving fine-grained online test-time adaptation.\nThe proposed method facilitates the absorption of fine-grained domain knowledge mainly through batch-level curriculum partitioning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces the concept of curriculum learning into the Online Test-Time Adaptation (OTTA) setting, re-examining model adaptation from the perspective of fine-grained domain knowledge absorption and providing a novel and inspiring research viewpoint.\n\n2. The proposed framework is independent of specific network architectures or loss functions, allowing it to be easily integrated with various existing OTTA methods and demonstrating good scalability and generality.\n\n3. The authors provide extensive benchmark experiments to support the effectiveness of the proposed approach."}, "weaknesses": {"value": "1.\tThe motivation for fine-grained domain knowledge learning is intuitively reasonable. However, the authors still need to provide a solid experimental or theoretical foundation rather than relying solely on descriptive explanations of this motivation.\n\n\n2.\tThe core procedure of CUPLOT appears to rely on gradient-based curriculum selection. Is there any further justification for the rationality of this selection mechanism? Could traditional confidence-based or feature-level selection strategies also be considered? What specific advantages does CUPLOT offer compared to them?\n\n\n3.\tIn the theoretical analysis, some assumptions are relatively strong (e.g., assuming that pseudo-label errors are strictly correlated with μ ranking). Moreover, the analysis does not quantitatively measure the improvement of fine-grained curriculum over convergence speed, making it more of a “plausibility argument” rather than a rigorous mathematical derivation.\n\n4.\tMost of the compared methods are batch-level OTTA baselines. Theoretically, the proposed method could also be extended to the Continual TTA setting. Including such experiments would make the work more comprehensive and convincing.\n\n5.\tThe algorithmic description of CUPLOT is somewhat complex and confusing, while the explanation itself is rather brief. It would be helpful if the authors could provide a methodological framework figure to give readers a clearer and more intuitive understanding of the approach."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "perXd3cfGo", "forum": "jsWT4iclYU", "replyto": "jsWT4iclYU", "signatures": ["ICLR.cc/2026/Conference/Submission15203/Reviewer_9R4p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15203/Reviewer_9R4p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377633300, "cdate": 1761377633300, "tmdate": 1762925505301, "mdate": 1762925505301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CUPLOT, a curriculum-learning-based framework \ndesigned to achieve fine-grained Online Test-Time Adaptation (OTTA). \nBy introducing a gradient-consistency-driven curriculum scheduling strategy, \nCUPLOT effectively enhances the model’s ability to absorb target-domain knowledge. \nFurthermore, a theoretical analysis of its convergence property is provided, \ndemonstrating the robustness and potential advantages of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper takes fine-grained domain knowledge under the OTTA setting as its starting point, which \nis an intuitively reasonable and meaningful motivation for enhancing adaptation granularity.\n\n2. Theoretical analysis and extensive benchmark experiments consistently verify the effectiveness \nand robustness of the proposed CUPLOT framework."}, "weaknesses": {"value": "1. The core motivation of CUPLOT lies in distinguishing between coarse-grained and fine-grained domain knowledge learning. However, the current discussion remains largely descriptive and is not sufficiently convincing. It is recommended that the authors include corresponding experiments or theoretical analysis when introducing the motivation to strengthen its persuasiveness.\n\n2. The key component of CUPLOT lies in its \ncurriculum division mechanism, which essentially determines the learning sequence of samples within each batch. I would like the authors to provide a comparison between different sequence selection strategies (e.g., random selection, sequential selection, or other sequence-based sampling approaches) to better demonstrate the necessity and effectiveness of the proposed scheduling method.\n\n3. The mathematical formulas and notations are rather confusing, which makes the paper appear overly engineering-oriented and difficult to follow. A clearer presentation of equations and symbols is strongly encouraged.\n\n4. I am confused about the description in line 204, where $B = \\operatorname{round}\\big(\\log(1 - s_{t,k-1})\\big)$ and $K^t$ is set around $ \\log n_t $ . If $B$ is dynamically varying while  is approximately fixed, would this inconsistency cause conflicts or ambiguity in the curriculum scheduling algorithm?\n\n5. Regarding Equation (6), if only a few unlearned samples remain in the batch, then $g_i^{t,k}$ and $G^{t,k}$ obtained from Equations (4) and (5) would be almost identical. In that case, the gradient consistency term $\\mu_i^{t,k}$could approach infinity. How do the authors handle this situation in practice?\n\n6. The paper lacks visual illustrations such as a framework or methodological diagram, which significantly affects readability. It is suggested that the authors include a clear framework figure to help readers better understand the overall pipeline of CUPLOT."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XMsRp02jSq", "forum": "jsWT4iclYU", "replyto": "jsWT4iclYU", "signatures": ["ICLR.cc/2026/Conference/Submission15203/Reviewer_tWX1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15203/Reviewer_tWX1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761470448078, "cdate": 1761470448078, "tmdate": 1762925504825, "mdate": 1762925504825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CUPLOT (Curriculum Pseudo-Labeling via Ordered Training) introduces a curriculum-learning framework for online test-time adaptation. Rather than updating the model on each mini-batch as a single block, CUPLOT divides each batch into several smaller curricula and orders the instances so that the model learns easier or more reliable samples first and harder or noisier ones later. The paper further demonstrates that curriculum pseudo-labeling enhances the model’s adaptation ability and provides a tighter theoretical bound toward the Bayes-optimal classifier on the target domain. Extensive experiments validate the effectiveness of the proposed framework across multiple benchmarks"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and well-structured, allowing readers to easily follow the logical flow and understand the motivation and proposed method at both conceptual and technical levels.\n\n- The empirical results convincingly demonstrate the effectiveness of the proposed approach across multiple standard benchmark datasets."}, "weaknesses": {"value": "- **Limited experimental scope:** The experiments are not sufficiently comprehensive. Since the paper’s main contribution lies in introducing a conceptual framework for within-batch curriculum learning, it is largely orthogonal to prior methods. The paper could conduct more extensive experiments by integrating the proposed curriculum mechanism with existing approaches to demonstrate its general applicability and potential performance gains.\n\n- **Insufficient emphasis on curriculum learning in the Introduction and Method:** The discussion of curriculum learning could be elaborated further. For instance, in __Line 57__, when introducing the concept, the authors could briefly define the notions of __easy__ and __hard__ instances and clarify how gradient consistency is used to distinguish them within the proposed framework. Additionally, Section 3.1 would benefit from a short subsection that formally presents the formulation of curriculum learning and its relevance to the test-time adaptation setting.\n\n- **Unstructured related work section:** The related work currently lacks clear organization, making it difficult for readers to follow the connections between topics. It is recommended to divide the section into three subsections—__(i) problem setting, (ii) existing methods, and (iii) curriculum learning__—to improve clarity. Moreover, the discussion could be strengthened by providing a deeper conceptual and technical comparison with prior works that incorporate curriculum learning."}, "questions": {"value": "- Could you provide additional experiments or discussion to validate whether CUPLOT consistently improves the performance of other baselines when integrated?\n\n- How do you formally define easy and hard instances in your curriculum scheduling, and what is the rationale behind using gradient consistency as the ordering criterion?\n\n- Can you provide a clearer comparison highlighting what aspects of curriculum learning are novel or distinct in your formulation?\n\n- Could you restructure the related work section to clearly separate discussions of (i) problem setting, (ii) adaptation methods, and (iii) curriculum learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rzHHXT4Wts", "forum": "jsWT4iclYU", "replyto": "jsWT4iclYU", "signatures": ["ICLR.cc/2026/Conference/Submission15203/Reviewer_pezg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15203/Reviewer_pezg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921965570, "cdate": 1761921965570, "tmdate": 1762925504339, "mdate": 1762925504339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CUPLOT, an OTTA framework that leverages instance-aware fine-grained knowledge. It first splits each arriving test data batch into a sequence of curricula ordered by gradient-consistency score, and learns each curriculum using prototype-based pseudo-labels computed from features of previously learned instances. A theoretical analysis argues that curriculum pseudo-labels yield a tighter bound than batch-level pseudo-labels. Experiments on synthetic corruption benchmarks and domain-generalization datasets report small but consistent gains over recent OTTA baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Fine-grained OTTA framing.** Different from prior approaches, converting a batch into ordered curricula based on per-instance gradient consistency is a well-designed way to address gradient conflict within a batch. The mechanism is easy to follow.\n2. **Active-TTA angle.** The CUPLOT-AT provides a unique angle shows how curriculum scores could drive selective querying under mixed severities. It shows potential for boarder impacts.\n3. **Broad empirical sweep. ** The paper evaluates across common OTTA corruption and DG benchmarks, and reports sensitivity to temperature τ, batch size, and curriculum number. CUPLOT shows consistency across different settings and scenarios."}, "weaknesses": {"value": "1.  **Computational overhead not fully accounted.**  CUPLOT requires per-instance gradients to rank samples and then multiple updates per batch across curricula, while TTA is very sensitive with compute overhead since it directly deployed in test time. The appendix varies \\(K^t\\) and reports time/accuracy trade-offs, but a clear backward-passes-per-sample accounting and wall-clock time comparison vs. strong baselines is missing in the main text.\n2. **Incremental combination rather than a new principle.**  Prototypes for pseudo-labels are well-trodden ideas. The novelty mainly lies in the specific gradient-consistency-driven ordering with prototype soft labels. \n3.  **Effect sizes are small; significance not emphasized.**  The gains over comparing methods on the three corruption suites are sub-1% on average. It’s hard to tell its advantages.\n4. **Limited comparison to strong Mean-teacher-based TTA methods.** A large strand of TTA uses teacher–student consistency with an EMA teacher, often improving stability in non-stationary streams (e.g., CoTTA[1], RMT[2], AR-TTA[3], especially RMT). These methods are commonly viewed as less sensitive to within-batch composition because the teacher evolves smoothly via EMA, and predictions used for consistency are augmentation-/time-averaged. CUPLOT does not directly compare against these strong MT baselines, and it’s unclear whether the intra-batch ordering still provides gains once an EMA teacher is present. \n\n[1] Wang, Qin, et al. \"Continual test-time domain adaptation.\" CVPR. 2022.\n[2] Döbler, Mario, Robert A. Marsden, and Bin Yang. \"Robust mean teacher for continual and gradual test-time adaptation.\" CVPR. 2023.\n[3] Sójka, Damian, et al. \"Ar-tta: A simple method for real-world continual test-time adaptation.\" ICCV. 2023."}, "questions": {"value": "Throughout the manuscript, many citations are written with \"\\citet\", even when the cited work is not the grammatical subject of the sentence. In those cases, \"\\citep\" would be more appropriate and reads more naturally under the ICLR style."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eklyWrqSmY", "forum": "jsWT4iclYU", "replyto": "jsWT4iclYU", "signatures": ["ICLR.cc/2026/Conference/Submission15203/Reviewer_74MR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15203/Reviewer_74MR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960665491, "cdate": 1761960665491, "tmdate": 1762925503896, "mdate": 1762925503896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision Uploaded"}, "comment": {"value": "We thank all reviewers for their constructive feedback and have updated our paper accordingly. Please take a moment to check out the revised manuscript, which incorporates the following key modifications (highlighted in blue):\n\n**New Results** \n- **Visuals**: learning-order comparison curves (Figure 1(a)) and a framework diagram (Figure 1(b)), which strengthen the motivation and illustrate the key components of our approach.\n- **Significance tests**: added Wilcoxon signed-rank tests across corrupted benchmark datasets (Table 1, Section 4.3). \n- **Compute accounting**: moved wall-clock latency table to main text (Table 5) and added backward-passes-per-sample (BPPS) metric (Table 5, Section 4.4).  \n- **Mean-Teacher baselines**: new experiments under Continual Test-Time Adaptation vs. CoTTA[1], RMT[2]; CUPLOT could extend to non-stationary settings while maintaining competence. (Section 4.5, Appendix A.3, Table 6).  \n- **Alternative difficulty metrics**: added entropy-confidence and prototype-distance baselines; gradient consistency outperforms them (Appendix A.8). \n- **Sequence ablation**: random / sequential order vs. curriculum; curriculum beats both by ≥0.8 % (Appendix A.9, Table 15).  \n- **Plug-and-play study**: integrated CUPLOT curriculum into SHOT[3] and DeYO[4]—consistent +0.5–1.2% gains (Appendix A.10, Table 16).  \n \n\n**Other Revisions**  \n- **Related work** re-structured into (i) problem setting, (ii) adaptation methods, including most recent works in Continual Test-Time Adaptation (iii) curriculum learning (Section 2).  \n- **Theory** clarified: assumptions are standard Lipschitz bounds; Theorem 1 now explicitly states faster convergence(Section 3.3).\n- **Preliminary** extended: connections to curriculum learning are added.\n- **Notation** polished: key equations re-typeset for clarity; `\\citep` used consistently per ICLR style.  \n\nWe appreciate the great efforts by the AC and reviewers. Please let us know if any further comments arise.\n\n[1] Wang et al. Continual test-time domain adaptation. CVPR, 2022. \n\n[2] Döbler et al. Robust mean teacher for continual and gradual test-time adaptation. CVPR. 2023. \n\n[3] Liang et al. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. ICML 2020.\n\n[4] Lee et al. Entropy is not enough for test-time adaptation: From the perspective of disentangled factors. ICLR, 2024."}}, "id": "zLI8DgRDpD", "forum": "jsWT4iclYU", "replyto": "jsWT4iclYU", "signatures": ["ICLR.cc/2026/Conference/Submission15203/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15203/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission15203/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763726595640, "cdate": 1763726595640, "tmdate": 1763726595640, "mdate": 1763726595640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}