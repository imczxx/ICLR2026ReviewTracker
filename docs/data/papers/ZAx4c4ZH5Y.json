{"id": "ZAx4c4ZH5Y", "number": 21720, "cdate": 1758320933572, "mdate": 1763652450945, "content": {"title": "Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models", "abstract": "The tendency of users to anthropomorphise large language models (LLMs) is of growing societal interest. Here, we present AnthroBench: a novel empirical method and tool for evaluating anthropomorphic LLM behaviours in realistic settings. Our work introduces three key advances; first, we develop a multi-turn evaluation of 14 distinct anthropomorphic behaviours, moving beyond single-turn assessment. Second, we present a scalable, automated approach by leveraging simulations of user interactions, enabling efficient and reproducible assessment. Third, we conduct an interactive, large-scale human subject study (N=1101) to empirically validate that the model behaviours we measure predict real users’ anthropomorphic perceptions. We find that all evaluated LLMs exhibit similar behaviours, primarily characterised by relationship-building (e.g., empathy and validation) and first-person pronoun use. Crucially, we observe that the majority of these anthropomorphic behaviors only first occur after multiple turns, underscoring the necessity of multi-turn evaluations for understanding complex social phenomena in human-AI interaction. Our work provides a robust empirical foundation for investigating how design choices influence anthropomorphic model behaviours and for progressing the ethical debate on the desirability of these behaviours.", "tldr": "", "keywords": ["anthropomorphism", "human-AI interaction", "social AI", "multi-turn", "evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/487f7dc1f46bc1e949a8d5b2cf9361369f6b1aa2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce AnthroBench, which consists of prompting multi-turn dialogues across four use cases (friendship, life coaching, career development, and general planning) and measuring the prevalence of 14 behaviors as assessed by an LLM judge. They include a large user study that validates the LLM judgment. Findings include a very high prevalence of some behaviors (e.g., validation, first-person pronouns) and very few instances of others (e.g., self-attribution of desires, self-attribution of physical embodiment) as well as—evidencing the importance of multi-turn evaluation—the majority of behaviors first occurring only after multiple turns."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Anthropomorphism is a timely and understudied model behavior, especially given its role in mental health and dependence issues.\n2. Multi-turn evaluation is important and, surprisingly, virtually nonexistent in the literature to date on LLM behavior in social settings.\n3. The evaluation is well-validated, including a large user study and rigorous analysis of temporal dynamics.\n4. The taxonomy and evaluation pipeline are clearly described and well-documented."}, "weaknesses": {"value": "1. The benchmark and the analysis conflate a huge variety of model behaviors, ranging from the relatively innocuous and often useful first-person pronouns to more extreme examples, such as the chatbot saying they are in a romantic relationship with a user. I worry that the category of \"anthropomorphism\" is too broad, and the authors should try to be more specific in their language about what is being measured and why it matters.\n2. A key contribution appears to be the multi-turn framework. However, it is unclear what value the authors really bring to the table in this regard. The multi-turn framework seems to be a series of prompts, but did these prompts go through a series of documented refinements and optimizations? That could be better-documented if so, and I worry that the elicitations (e.g., Lines 950-952) are quite simple. It seems like a competent researcher could easily produce their own version of these prompts to do a multi-turn evaluation, and I am unsure how much benefit having these prompts would provide.\n3. Part of #2 is that the anonymous repo gives me \"The requested file is not found\" errors when I click on any particular file, so I am unable to fully explore the contributed framework.\n\n- \"Krippendorf\" -> \"Krippendorff\""}, "questions": {"value": "1. Can the authors provide more detail on the human subjects validation? For example, were there attention checks? How exactly were the texts shown to the user? Were some participants excluded, and if so, why? In part, I ask because the Krippendorff's alpha documented in Table 4 are quite low, and this may be explained by some 'bad data' being in the mix.\n2. Can the benchmark be run with longer conversations? Does the simulation hold up? This would be useful for applying the AnthroBench infrastructure to other social LLM issues, such as \"AI psychosis\" and long-term dependence evaluations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ks7IUaLJJ6", "forum": "ZAx4c4ZH5Y", "replyto": "ZAx4c4ZH5Y", "signatures": ["ICLR.cc/2026/Conference/Submission21720/Reviewer_Pwrf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21720/Reviewer_Pwrf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614842578, "cdate": 1761614842578, "tmdate": 1762941905259, "mdate": 1762941905259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AnthroBench is a benchmark for evaluating anthropomorphic behaviours in large language models (LLMs).  It builds on a taxonomy of 14 behaviours (e.g. personhood claims, physical embodiment, internal states and relationship-building) and uses a multi‑turn evaluation: a “User LLM” initiates a five‑turn dialogue across eight scenarios and four domains (friendship, life coaching, career development and general planning).  Each turn is labelled by three “Judge LLMs,” and majority vote yields an “anthropomorphism profile” per model.\n\nThe authors apply this pipeline to Gemini 1.5 Pro, Claude 3.5 Sonnet, GPT‑4o and Mistral Large.  All four models exhibit similar profiles dominated by relationship‑building behaviours and first‑person pronoun use.  Social domains (friendship and life‑coaching) elicit more anthropomorphic language than neutral domains like planning.  Multi‑turn analysis shows that nine of the 14 behaviours emerge only after the first turn, underscoring the need for multi‑turn evaluation. \n\nA validation study with 1,101 participants confirms that the automated metrics is able to decipher between degrees of anthrompomorphism and correlate with human perceptions of anthropomorphism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The benchmark draws on existing research to define 14 anthropomorphic behaviours.  This taxonomy is comprehensive and organised into self‑referential (personhood and internal state) and relational (relationship‑building) behaviours, enabling nuanced analysis.\n\nThe use of Judge LLMs is cross-checked against human annotations (precision > 85%), and human participants’ perceptions align with benchmark results.\n\nUser LLM model evaluation on Godspeed shows that its  “anthropomorphic enough” do do the job.\n\nEvaluating top LLMs gives immediate practical relevance.\n\nThe point of multi turn evaluation is solidely demonstrated."}, "weaknesses": {"value": "* The user simulation is fixed: Gemini 1.5 Pro with a role‑playing system prompt that specifies tone, scenario and a non‑adversarial stance.  This design can plausibly encourages empathy and relationship‑building, potentially biasing the target models toward the behaviours being measured. There is no warranty that the behavior of the User LLM is typical of user in general, even if it is  “anthropomorphic enough” according to Godspeed so what is missing at least is an ablation varying the user prompt to see how sensitive results are to the user simulation.\n\n* Labels are produced by three different LLMs and aggregated by majority vote.  While the authors state that precision is high, they do not report the variance of the Judge grading for the same inputs."}, "questions": {"value": "1. How sensitive are the anthropomorphism profiles to variations in the User‑LLM prompt style (e.g., neutral vs empathetic) or to a different user model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AAEhf84jXt", "forum": "ZAx4c4ZH5Y", "replyto": "ZAx4c4ZH5Y", "signatures": ["ICLR.cc/2026/Conference/Submission21720/Reviewer_vPNB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21720/Reviewer_vPNB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909609182, "cdate": 1761909609182, "tmdate": 1762941904532, "mdate": 1762941904532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reports on the development of AnthroBench, a multi-turn evaluation of 14 anthropomorphic behaviors grouped into four broad categories. The authors outline an automated multi-agent pipeline for generating and evaluating anthropomorphic behaviors in conversations between llms acting as user and an llm acting as chatbot. The pipeline is validated with real human users. The authors find that anthropomorphic behavior across models is primarily characterized by relationship-building behaviors and first-person pronoun use. Further, they find that most anthropomorphic behavior only arises after multiple turns, underscoring the importance of multi-turn evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper outlines a novel approach to studying anthropomorphic behavior beyond single turns. The results show the robustness of the approach across a variety of models. LLM evaluations are supported by comparing with human evaluations."}, "weaknesses": {"value": "The study is limited to prompting frontier models. The authors do not examine the effect of fine-turning on the manifestation of anthropomorphic behaviors. Comparing Instruction-tuned and foundation models would offer an instrumental case study and empirical validation of their proposed evaluation approach. As they write: \"In addition to presenting these methodological advances, we share AnthroBench as publicly available benchmarking tool that can support developers evaluating systems for  [...] researchers comparing anthropomorphism across systems and contexts....\", this would have been an ideal means to demonstrate exactly that."}, "questions": {"value": "P9L476 \"and, researchers can use our trained classifiers to label anthropomorphic behaviors in other human-LLM interaction datasets, such\nas preference datasets for understanding reinforcement learning with human feedback (RLHF)’s role\" \n=> Your LLM classifiers are not trained as claimed here, but simply prompted, no?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KFgOK3awkE", "forum": "ZAx4c4ZH5Y", "replyto": "ZAx4c4ZH5Y", "signatures": ["ICLR.cc/2026/Conference/Submission21720/Reviewer_jZgQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21720/Reviewer_jZgQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950846011, "cdate": 1761950846011, "tmdate": 1762941903382, "mdate": 1762941903382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AnthroBench, a multi-turn, simulation-based benchmark for measuring 14 fine-grained anthropomorphic behaviours in LLMs. The evaluation covers four use domains (friendship, life coaching, career development, general planning) and runs 5-turn conversations for 960 prompts, yielding 4,800 target messages per model. The authors test Gemini 1.5 Pro, Claude 3.5 Sonnet, GPT-4o, and Mistral Large, label 13/14 behaviours via three Judge LLMs with majority voting. The main findings are: (i) all evaluated systems show similar profiles dominated by relationship-building and first-person pronoun use; (ii) domain matters, with friendship/life-coaching inducing more behaviours; (iii) most behaviours first appear after turn 1; and (iv) a human study (N=1101) confirms that higher AnthroBench scores predict more anthropomorphic perceptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Multi-turn, context-sensitive evaluation. By fixing 5-turn dialogues across four use domains, the benchmark captures phenomena that single-turn tests miss and shows robust context effects.\n\nValidation at human level. The N=1101 interactive study linking benchmark scores to both explicit and implicit anthropomorphism is a major contribution to construct validity.\n\nCareful LLM-as-Judge calibration. Majority voting across three diverse judge models, multi-sampling, and human comparisons (reporting precision >~85% for most behaviours) reduce single-model bias and show non-trivial reliability. \n\nTemporal and transitional analysis. The paper not only asks when behaviours first appear but also quantifies turn-to-turn compounding, which has practical implications for safety guardrails."}, "weaknesses": {"value": "User-simulation dependence. All conversations are initiated and steered by a single User LLM (Gemini 1.5 Pro) with one role-play prompt. This risks style and topic biases in elicitation (e.g., politeness, empathy), potentially shaping behaviour frequencies across models and domains. Consider multiple user simulators to diversify interaction styles. \n\nFive-turn cap and stage effects. Many behaviours first occur after turn 1; a five-turn ceiling may truncate later-emerging effects or longer-horizon compounding. Results might differ with 10–20 turns, task pivots, or re-engagements. \n\nPronoun counting as anthropomorphism. Treating first-person pronoun use as a binary/count feature may conflate benign stylistic markers (“I think”, “I can help”) with personhood claims; it can dominate scores without reflecting risky anthropomorphism."}, "questions": {"value": "How sensitive are results to the User-LLM identity and prompt? Have you tried different simulators to reduce simulation bias?\n\nWhat happens if you extend to 10+ turns, insert topic shifts, or include follow-up tasks?\n\nDo the behaviour definitions and judges transfer beyond English?\n\nAmong the 14 behaviours, which correlate most with undesired outcomes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "saqCzJURQb", "forum": "ZAx4c4ZH5Y", "replyto": "ZAx4c4ZH5Y", "signatures": ["ICLR.cc/2026/Conference/Submission21720/Reviewer_3kxY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21720/Reviewer_3kxY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972653953, "cdate": 1761972653953, "tmdate": 1762941902371, "mdate": 1762941902371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}