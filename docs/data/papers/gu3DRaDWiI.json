{"id": "gu3DRaDWiI", "number": 1795, "cdate": 1756933630172, "mdate": 1763379792465, "content": {"title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation", "abstract": "Unified multimodal models (UMMs) have shown remarkable advances in jointly understanding and generating text and images. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning: textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. As such, existing benchmarks rarely require the use of one modality to guide, verify, or refine outputs in the other. They therefore fail to capture a central aspiration of unified multimodal models, namely to support seamless reasoning across modalities. We address this gap with **ROVER**, a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1285 tasks grounded in 2,048 images, spanning two complementary settings. **Verbally-augmented reasoning for visual generation** evaluates whether models can use structured verbal prompts and reasoning chains to guide faithful image synthesis. **Visually-augmented reasoning for verbal generation** evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes. Experiments on 17 state-of-the-art UMMs reveal two key findings: (i) cross-modal reasoning capabilities strongly correlate with visual generation performance, particularly for interleaved image–text generation; and (ii) current models remain severely limited in visual-augmented reasoning, showing relative strength in perception and physical modeling but weakness in logical tasks. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation. More information on **Anonymous Page**: https://anony0923.github.io", "tldr": "A benchmark for evaluating cross-modal reasoning in unified multimodal models", "keywords": ["Unified Multimodal Model", "Generation Benchmark", "Cross-modal Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82967c612fbd26d10c933ca7f5461e9ee2b2839b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ROVER, a benchmark designed to evaluate the reciprocal cross-modal reasoning capabilities of Unified Multimodal Models (UMMs). The authors argue that existing benchmarks fail to assess how models use one modality to guide or verify outputs in another, instead testing text and image abilities in isolation. ROVER addresses this gap with over 1,200 tasks that require integrated reasoning across modalities, focusing on verbally-augmented reasoning for visual generation and visually-augmented reasoning for verbal generation. By testing 17 state-of-the-art UMMs, the study finds that cross-modal reasoning skills strongly correlate with visual generation performance. However, it also reveals that current models are severely limited in visually-augmented reasoning, showing particular weakness in logical tasks compared to perception and physical modeling."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces the first benchmark that requires generating both visual and textual content for joint visual and textual reasoning, effectively unifying the two modalities. The authors conduct extensive experiments demonstrating that incorporating multimodal generation improves performance compared to text-only generation during evaluation. The finding that models with stronger image–text interleaving capabilities outperform image-editing models is also noteworthy. Overall, the paper provides clear evidence that text generation supports image generation, and image generation, in turn, enhances textual reasoning."}, "weaknesses": {"value": "1. The paper's evaluation methodology relies heavily on the quality of generated images, particularly for Reasoning Visual (RV), which requires generating coherent images to facilitate correct reasoning. However, the use of a VLM as a judge is questionable. Figure 8 reveals a significantly low correlation (0.63) and a high MAE of nearly 1.0 between GPT's evaluations and human judgments. Assuming the four human evaluators provide a more reliable gold standard, this discrepancy undermines the validity of using VLMs to assess RV quality. This concern is amplified by prior work demonstrating that even state-of-the-art VLMs struggle with fundamental spatial and temporal reasoning.\n2. In addition to the questionable reliability of the VLM-as-a-judge paradigm, the paper fails to address the financial costs associated with using the GPT for evaluation, a notable omission given the large volume of generated images involved.\n3. While the authors present Figure 7b to show the correlation between different task types, this finding is largely unsurprising due to the semantic definitions of the tasks. Crucially, it remains unclear how this analysis provides actionable insights or how it might guide the development of future models."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HHvua8memy", "forum": "gu3DRaDWiI", "replyto": "gu3DRaDWiI", "signatures": ["ICLR.cc/2026/Conference/Submission1795/Reviewer_Bjyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1795/Reviewer_Bjyh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760462193006, "cdate": 1760462193006, "tmdate": 1762915892840, "mdate": 1762915892840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ROVER, a benchmark designed to evaluate the reciprocal cross-modal reasoning capabilities of Unified Multimodal Models (UMMs). The authors argue that existing benchmarks fail to assess how models use one modality to guide or verify outputs in another, instead testing text and image abilities in isolation. ROVER addresses this gap with over 1,200 tasks that require integrated reasoning across modalities, focusing on verbally-augmented reasoning for visual generation and visually-augmented reasoning for verbal generation. By testing 17 state-of-the-art UMMs, the study finds that cross-modal reasoning skills strongly correlate with visual generation performance. However, it also reveals that current models are severely limited in visually-augmented reasoning, showing particular weakness in logical tasks compared to perception and physical modeling."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces the first benchmark that requires generating both visual and textual content for joint visual and textual reasoning, effectively unifying the two modalities. The authors conduct extensive experiments demonstrating that incorporating multimodal generation improves performance compared to text-only generation during evaluation. The finding that models with stronger image–text interleaving capabilities outperform image-editing models is also noteworthy. Overall, the paper provides clear evidence that text generation supports image generation, and image generation, in turn, enhances textual reasoning."}, "weaknesses": {"value": "1. The paper's evaluation methodology relies heavily on the quality of generated images, particularly for Reasoning Visual (RV), which requires generating coherent images to facilitate correct reasoning. However, the use of a VLM as a judge is questionable. Figure 8 reveals a significantly low correlation (0.63) and a high MAE of nearly 1.0 between GPT's evaluations and human judgments. Assuming the four human evaluators provide a more reliable gold standard, this discrepancy undermines the validity of using VLMs to assess RV quality. This concern is amplified by prior work demonstrating that even state-of-the-art VLMs struggle with fundamental spatial and temporal reasoning.\n2. In addition to the questionable reliability of the VLM-as-a-judge paradigm, the paper fails to address the financial costs associated with using the GPT for evaluation, a notable omission given the large volume of generated images involved.\n3. While the authors present Figure 7b to show the correlation between different task types, this finding is largely unsurprising due to the semantic definitions of the tasks. Crucially, it remains unclear how this analysis provides actionable insights or how it might guide the development of future models."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HHvua8memy", "forum": "gu3DRaDWiI", "replyto": "gu3DRaDWiI", "signatures": ["ICLR.cc/2026/Conference/Submission1795/Reviewer_Bjyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1795/Reviewer_Bjyh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760462193006, "cdate": 1760462193006, "tmdate": 1763438600172, "mdate": 1763438600172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ROVER, a benchmark with over 1,200 tasks and 2,048 images for reciprocal cross-modal reasoning. ROVER has two parts: ROVER-IG (language guiding image generation) and ROVER-TG (vision aiding text generation). They tested 17 UMMs with a VLM judge plus expert checks, finding cross-modal reasoning ties to visual generation, but models struggle with vision-aided logic tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is generally well-written and easy to follow, with clearly illustrated figures.\n2. ROVER covers a wide range of both language-reasoning tasks and visual-reasoning tasks, and uses a comprehensive evaluation method (VLM + expert validation) to ensure reliability.\n3. The authors evaluate 17 unified multimodal models and provide insightful findings."}, "weaknesses": {"value": "1. The benchmark heavily depends on a \"VLM-as-a-judge\" for scoring complex reasoning qualities. The paper's own user study (Figure 8) shows that while correlation is good, there are noticeable discrepancies, especially for reasoning-related metrics. This introduces a potential bias, where the benchmark might favor models whose outputs align with the judging VLM's own reasoning patterns.\n2. As listed in Table 3, language-only models often match or exceed the performance of unified models on reasoning tasks, questioning whether the current task design truly requires cross-modal reasoning for optimal results (\"thinking with images\").​"}, "questions": {"value": "1. How does the up-to-date Gemini-2.5-pro perform on this benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lnh7LATe31", "forum": "gu3DRaDWiI", "replyto": "gu3DRaDWiI", "signatures": ["ICLR.cc/2026/Conference/Submission1795/Reviewer_icDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1795/Reviewer_icDF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760850518923, "cdate": 1760850518923, "tmdate": 1762915892654, "mdate": 1762915892654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ROVER, a new benchmark designed to evaluate reciprocal cross-modal reasoning in unified multimodal models (UMMs), i.e., the ability to use one modality (text or image) to guide reasoning and generation in the other. Existing evaluations tend to isolate modalities, emphasizing either textual or visual reasoning in isolation, which fails to capture the intended integration of modern UMMs.\n\nROVER fills this gap through over 1,200 human-annotated tasks grounded in 2,048 images, spanning two complementary settings: (1) verbally-augmented reasoning for visual generation, where structured verbal reasoning guides faithful image synthesis, and (2) visually-augmented reasoning for verbal generation, where models generate intermediate visualizations to support their reasoning.\n\nThe authors evaluate 17 state-of-the-art UMMs and find that cross-modal reasoning ability correlates with visual generation performance, especially for interleaved text–image tasks. However, most models remain weak in visually-augmented reasoning, particularly in logical reasoning scenarios. \n\nOverall, the work is a good first step towards analyzing cross model reasoning abilities of current UMMs and avenues of improvement, but still requires more work to solidify its usability and interpretability."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. Importance of cross model reasoning in UMM and problem formulation in two complementary settings of verbally-augmented reasoning for visual generation (ROVER-IG) and visually-augmented reasoning for verbal generation (ROVER-TG) is interesting, useful and novel.\n2. Careful dataset design into top level domains and subtasks for both ROVER-IG and ROVER-TG\n3. Detailed metrics that aim to provide a holisitic understanding of the model performance in either settings.\n4. Interesting analysis like coherence between reasoning substasks."}, "weaknesses": {"value": "The paper gives a good shot to cover a novel perspective but falls short in these following areas:\n\n1. Stretch / Over claims: \na) \"Pg 5 section 4.1 (last para) the authors claim that gaps in reasoning process and alignment is the fundamental driver of diminished visual generation performance\" but as seen for table 2, if you look at natural science or logic for instance for both closed and open source model, similar RP and align scores show great variability in RV scores. \nb) \"Pg 7 section 4.2 Models demonstrate superior interleaved reasoning performance on physical world and visual perception tasks compared to logical reasoning challenges\" is not supported in table 3, model perform similarly for the best for visual perception only and they have similar low performance in logic and physical world domains.\n\n2. Clarifications\na). It is difficult to infer anything from the % reported in the paper, none of them mention if its absolute, or relative and relative with respect to what ?\nb) visual generation performance on pg 5 last paragraph is vague. from reading context, i can map it to RV but would urge the authors to make explicit connections between numbers and metrics, especially when they define them\nc) Section 4.3 Cross-modal Reasoning matters for UMMs: Could not follow through this analysis, CLIP-1 and edit world are introduced out of the blue without prior context. Fig on pg 9 top right has the corresponding details but is not reference in text and the figure itself is unclear, with some bars having +ve/-ve value and being of different lengths + no caption. Could not make sense of this at all\n\n3. Judge reliability evaluation\na) Human correlation of RV one of the important metrics for IG is low\nb) Only IG metrics undergo reliability evaluation what about TG metrics which are also llm judges ?\nc) The models used for judge calibration are either closed source models or the strongest open-source model. This could potentially add bias in score calibration, having a weaker open source model being part of calibration can ensure that the entire spectrum of scores is callibrated.\n\n4. Missing details\na) Fig 4, it would be nice to see the reasoning generated by the models in addition to input text and generated image, to better analyze reasoning alignment.\nb) Table 2 does not clearly indicate which models are interleaved vs single turn, image editing only vs UMM but uses these terminologies in the analysis section when table 2 is referenced. Appendix does provide some insight to them but still models like Show-o2, Blip3o-8b, Janus-Pro-7b, etc have not been classified, making it difficult to relate to the claims in the paper\n\n5. Overall the paper writing needs to improve, better references to figures and tables, improved captions"}, "questions": {"value": "I have added my questions as part of weakness itself, would urge the authors to respond to them. In its current state the work does not merit publication, however if the authors make the necessary clarifications and substantiate their claims, the benchmark would be more useful and i can consider bumping my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4oWrq9Ykvc", "forum": "gu3DRaDWiI", "replyto": "gu3DRaDWiI", "signatures": ["ICLR.cc/2026/Conference/Submission1795/Reviewer_FBtf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1795/Reviewer_FBtf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842387992, "cdate": 1761842387992, "tmdate": 1762915892488, "mdate": 1762915892488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ROVER, a new benchmark designed to evaluate reciprocal cross-modal reasoning in unified multimodal models (UMMs), i.e., the ability to use one modality (text or image) to guide reasoning and generation in the other. Existing evaluations tend to isolate modalities, emphasizing either textual or visual reasoning in isolation, which fails to capture the intended integration of modern UMMs.\n\nROVER fills this gap through over 1,200 human-annotated tasks grounded in 2,048 images, spanning two complementary settings: (1) verbally-augmented reasoning for visual generation, where structured verbal reasoning guides faithful image synthesis, and (2) visually-augmented reasoning for verbal generation, where models generate intermediate visualizations to support their reasoning.\n\nThe authors evaluate 17 state-of-the-art UMMs and find that cross-modal reasoning ability correlates with visual generation performance, especially for interleaved text–image tasks. However, most models remain weak in visually-augmented reasoning, particularly in logical reasoning scenarios. \n\nOverall, the work is a good first step towards analyzing cross model reasoning abilities of current UMMs and avenues of improvement, but still requires more work to solidify its usability and interpretability.\n\n- Score updated to 6 after reviewing author rebuttal -"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. Importance of cross model reasoning in UMM and problem formulation in two complementary settings of verbally-augmented reasoning for visual generation (ROVER-IG) and visually-augmented reasoning for verbal generation (ROVER-TG) is interesting, useful and novel.\n2. Careful dataset design into top level domains and subtasks for both ROVER-IG and ROVER-TG\n3. Detailed metrics that aim to provide a holisitic understanding of the model performance in either settings.\n4. Interesting analysis like coherence between reasoning substasks."}, "weaknesses": {"value": "The paper gives a good shot to cover a novel perspective but falls short in these following areas:\n\n1. Stretch / Over claims: \na) \"Pg 5 section 4.1 (last para) the authors claim that gaps in reasoning process and alignment is the fundamental driver of diminished visual generation performance\" but as seen for table 2, if you look at natural science or logic for instance for both closed and open source model, similar RP and align scores show great variability in RV scores. \nb) \"Pg 7 section 4.2 Models demonstrate superior interleaved reasoning performance on physical world and visual perception tasks compared to logical reasoning challenges\" is not supported in table 3, model perform similarly for the best for visual perception only and they have similar low performance in logic and physical world domains.\n\n2. Clarifications\na). It is difficult to infer anything from the % reported in the paper, none of them mention if its absolute, or relative and relative with respect to what ?\nb) visual generation performance on pg 5 last paragraph is vague. from reading context, i can map it to RV but would urge the authors to make explicit connections between numbers and metrics, especially when they define them\nc) Section 4.3 Cross-modal Reasoning matters for UMMs: Could not follow through this analysis, CLIP-1 and edit world are introduced out of the blue without prior context. Fig on pg 9 top right has the corresponding details but is not reference in text and the figure itself is unclear, with some bars having +ve/-ve value and being of different lengths + no caption. Could not make sense of this at all\n\n3. Judge reliability evaluation\na) Human correlation of RV one of the important metrics for IG is low\nb) Only IG metrics undergo reliability evaluation what about TG metrics which are also llm judges ?\nc) The models used for judge calibration are either closed source models or the strongest open-source model. This could potentially add bias in score calibration, having a weaker open source model being part of calibration can ensure that the entire spectrum of scores is callibrated.\n\n4. Missing details\na) Fig 4, it would be nice to see the reasoning generated by the models in addition to input text and generated image, to better analyze reasoning alignment.\nb) Table 2 does not clearly indicate which models are interleaved vs single turn, image editing only vs UMM but uses these terminologies in the analysis section when table 2 is referenced. Appendix does provide some insight to them but still models like Show-o2, Blip3o-8b, Janus-Pro-7b, etc have not been classified, making it difficult to relate to the claims in the paper\n\n5. Overall the paper writing needs to improve, better references to figures and tables, improved captions"}, "questions": {"value": "I have added my questions as part of weakness itself, would urge the authors to respond to them. In its current state the work does not merit publication, however if the authors make the necessary clarifications and substantiate their claims, the benchmark would be more useful and i can consider bumping my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4oWrq9Ykvc", "forum": "gu3DRaDWiI", "replyto": "gu3DRaDWiI", "signatures": ["ICLR.cc/2026/Conference/Submission1795/Reviewer_FBtf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1795/Reviewer_FBtf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842387992, "cdate": 1761842387992, "tmdate": 1763542988853, "mdate": 1763542988853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ROVER, the first human-annotated benchmark explicitly designed to evaluate reciprocal cross-modal reasoning in Unified Multimodal Models (UMMs). It addresses the fundamental limitation of existing benchmarks, which treat understanding and generation abilities in isolation, failing to assess how one modality can guide, verify, or refine outputs in the other. 17 SOTA UMMs have been evaluated on the visual generation and text generation settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well structured and presented overall, with a helpful project page.\n- It addresses an important gap in UMMs by benchmarking and evaluating reciprocal cross-modal reasoning."}, "weaknesses": {"value": "- Table 1 should include comparisons across more aspects. Additional explanations are needed in both the text and the table caption: benchmark dataset scale, whether it is for VG/TG/both, and clarifications on the multi-dimensional and hybrid evaluations and the types.\n- This work's emphasis on intermediate reasoning as a core signal for multimodal reasoning distinguishes it from existing benchmarks. However, the data curation process for these progressive reasoning steps is under-specified, especially for the TG setup. The paper should clarify exactly what intermediate data is curated for various sub-tasks, dataset statistics, and how it is used for evaluation.\n- The current evaluation depends only on GPT-based judgment. Introducing objective, automatically computed metrics would improve the reliability of the fine-grained reasoning evaluation. \n- Would it also be beneficial to include the text reasoning chain for the TG task? Clarification is needed on how the progressive visual reasoning steps are validated as active reasoning components rather than decorative elements, as claimed in line 246."}, "questions": {"value": "Please see the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oba71l1NoD", "forum": "gu3DRaDWiI", "replyto": "gu3DRaDWiI", "signatures": ["ICLR.cc/2026/Conference/Submission1795/Reviewer_Apxs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1795/Reviewer_Apxs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979490112, "cdate": 1761979490112, "tmdate": 1762915892342, "mdate": 1762915892342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "**Dear Reviewers, ACs, and SACs,**\n\nWe deeply appreciate the insightful and valuable comments provided by all reviewers.\n\n---\n\nWe are grateful for the reviewers' recognition of this work as an important step toward evaluating **reciprocal cross-modal reasoning in unified multimodal models**. ROVER provides a carefully designed benchmark that captures how models integrate visual and verbal reasoning, and our analyses reveal key strengths and limitations of current UMMs. We believe ROVER provides a rigorous foundation for advancing cross-modal reasoning in next-generation omnimodal models.\n\nOverall, we are encouraged by the reviewers' positive feedback, which highlights:\n- The **motivation and novelty** of reciprocal cross-modal reasoning are clear and impactful (Reviewers `Bjyh`, `FBtf`, `Apxs`).\n- The benchmark is **comprehensive, well-structured, and carefully designed** across domains and reasoning types (Reviewers `FBtf`, `Apxs`).\n- The **experiments** and hybrid VLM–expert evaluation are thorough and **clearly presented** (Reviewer `icDF`).\n- The **findings** on interleaved generation, reciprocal modality **benefits**, and reasoning gaps are **insightful** and valuable for future UMM development (Reviewers `Bjyh`, `icDF`, `FBtf`).\n\n---\n\nTo address the reviewers' concerns, we have conducted several additional experiments and analyses, including:\n- **Added GenEval evaluation** to compare automatic objective metrics with GPT-4.1 (Reviewer `Apxs`).\n- **Expanded the human–VLM reliability study** to 8 human experts, 10 models, and 1000 samples, updating correlation and MAE for GPT-4.1 judge (Reviewers `Bjyh`, `FBtf`).\n- **Incorporated open-source VLM judges** to improve calibration robustness and explore more cost-efficient evaluation options (Reviewers `Bjyh`, `FBtf`).\n\n---\n\n**Summary of revisions:**\n- Clarified benchmark data curation and evaluation setup in `Section 3.2` and `Appendix B`, `E.1`\n- Added TG reliability results in `Appendix C` and updated `Figure 8` with extended human validation study\n- Refined analysis in `Sections 4.2`, `4.3`, and `4.4` to present the results and insights more clearly and accurately.\n- Updated `Table 1`, `2` and `Figures 4`, `6` to clarify metrics, model types, and annotations.\n- Revised `Section 4.1` with more experimental setup details.\n- Expanded verbal reasoning examples for visual generation in `Appendix D`. \n\nAll revisions in the paper are highlighted in blue. We sincerely appreciate the reviewers' constructive suggestions and remain committed to continually improving our work.\n\n---\n\nWe address each reviewer's comments point by point below. We welcome further discussion and look forward to continued engagement. Thank you!"}}, "id": "QRthCXWf2a", "forum": "gu3DRaDWiI", "replyto": "gu3DRaDWiI", "signatures": ["ICLR.cc/2026/Conference/Submission1795/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1795/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission1795/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763386182793, "cdate": 1763386182793, "tmdate": 1763386182793, "mdate": 1763386182793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}