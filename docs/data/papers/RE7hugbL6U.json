{"id": "RE7hugbL6U", "number": 21485, "cdate": 1758318099840, "mdate": 1759896919471, "content": {"title": "Learning Representations Through Contrastive Neural Model Checking", "abstract": "Model checking is a key technique for verifying safety-critical systems against formal specifications, where recent applications of deep learning have shown promise. However, while ubiquitous for vision and language domains, representation learning remains underexplored in formal verification. We introduce Contrastive Neural Model Checking (CNML), a novel method that leverages the model checking task as a guiding signal for learning aligned representations. CNML jointly embeds logical specifications and systems into a shared latent space through a self-supervised contrastive objective. On industry-inspired retrieval tasks, CNML considerably outperforms both algorithmic and neural baselines in cross-modal and intra-modal settings. We further show that the learned representations effectively transfer to downstream tasks and generalize to more complex formulas.  These findings demonstrate that model checking can serve as an objective for learning  representations for formal languages.", "tldr": "Contrastive learning on systems and specifications leads to learning aligned representations that can be transferred to a wide array of tasks and used for cross- or intra modal search", "keywords": ["Contrastive Learning", "CLIP", "Neurosymbolic", "Formal Methods", "Verification", "Representation Learning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74265b64a8dfcb5d0f472bc4eec647f8dd85205b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a technique to improve the ability to associate circuit designs with their respective specifications by training models to learn a joint representation over the two domains. The authors demonstrate the effectiveness of this technique by evaluating trained models on retrieval tasks, classification tasks, and generalizability."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I really like the problem this paper is tackling: it is unique yet important, and seemingly understudied especially in the age of large models. The hypothesis is sound and builds on well-established findings, and the results reflect the advantages of the proposed technique. They also test the generalizability of the trained models by splitting the formulas."}, "weaknesses": {"value": "I see some weaknesses in the paper:\n1. Need for a contrastive learning approach: The paper has not really justified the need of using contrastive learning as such. While it would not harm the performance, I would assume given a dataset of ~300K pairs, the models would be trained in a rather straightforward manner.\n2. Choice of encoder: I also am not sure why CodeBert specifically was chosen -- while it is trained on code, I don't expect the code in pretraining being considerably in distribution with respect to LTL formulas. I wonder what would have happened if we just used a standard Bert model or RoBerta instead of something specialized like CodeBert.\n3. Generalizability experiments: I would be more convinced by the generalizability if you had shown model performance on composed specifications (if that is even possible), and more importantly, a held-out dataset consisting of specifications that were designed by professionals/that exist in model-checking textbooks or references. I also don't get a sense of how aligned your generated specs are with industry-standard specifications, even if it is just 100 samples."}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K1wMaWARcI", "forum": "RE7hugbL6U", "replyto": "RE7hugbL6U", "signatures": ["ICLR.cc/2026/Conference/Submission21485/Reviewer_Wv3n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21485/Reviewer_Wv3n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783753119, "cdate": 1761783753119, "tmdate": 1762941800984, "mdate": 1762941800984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a representation-learning view of model checking via Contrastive Neural Model Checking (CNML), which jointly embeds LTL specifications and AIGER circuits using two CodeBERT encoders trained with a CLIP-style contrastive objective. A large synthetic dataset (cnml-base) is built, and a single-guarantee variant (cnml-split) is derived by formula splitting for a generalization study. The method is evaluated on intra-modal retrieval with N=100 and N=1000, against algorithmic (e.g., Bag-of-Keywords, WL kernel) and neural baselines, an cross-modal retrieval (neural baselines only). Finally, they fine-tune for model checking and show generalization from single- to multi-guarantee formulas."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* A novel contrastive approach to neural model checking (joint embeddings for LTL and circuits).\n* A synthetic dataset created by first sampling LTL formulas and then synthesizing matching circuits.\n* Broad retrieval evaluation (cross-modal and intra-modal).\n* Mini-batch construction that avoids duplicates and reduces off-diagonal false negatives.\n* Clear, well-organized writing with intuitive embedding analyses (e.g., cosine-similarity distributions and a heatmap)."}, "weaknesses": {"value": "* Mini-batch false negative analysis is limited \n* Cross-modal retrieval lacks non-ML baselines (e.g. edit distance between paired LTL/AIGER string forms).\n* Runtime benchmarks versus non-ML baselines are missing scalability and variance are not characterized.\n* No qualitative results overall (true positives/false positives/near misses, error taxonomy, or interpretability visuals).\n* Sequence pooling is under-specified (CLS/mean/max/attention-pool not clarified), and its impact is not quantified.\n* Practical metric such as “recall after model checking (top-k)” is not reported alongside raw recall.\n* Potential data overlap/near-duplicates in synthetic sets are not analyzed; de-duplication is unclear."}, "questions": {"value": "Please address the weaknesses listed above.\n\nPlease explain the sequence pooling layer in more detail."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ok2HnLUb5t", "forum": "RE7hugbL6U", "replyto": "RE7hugbL6U", "signatures": ["ICLR.cc/2026/Conference/Submission21485/Reviewer_wqSR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21485/Reviewer_wqSR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864548922, "cdate": 1761864548922, "tmdate": 1762941800674, "mdate": 1762941800674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses contrastive learning to learn joint embeddings of linear temporal logic (LTL) formulas representing specifications, and and-inverter graphs formulas representing systems to be checked. Both kinds of formulas are in raw ascii text, and pre-trained CodeBERT models are used as encoders, which are further fine-tuned on synthetic tasks. Experimental evaluations show that, compared to vanilla CodeBERT and  Sentence-BERT, contrastive learning with CodeBERT outperform both on two retrieval tasks (i.e., cross-modal retrieval and intra-modal retrieval). Furthermore, CodeBERT after contrastive learning outperforms the original CodeBERT for downstream fine-tuning task, binary classification on circuit-specification pairs."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- the targeted research problem, model checking, is of great importance in hardware verification\n- decent background such as LTL, And-Inverter Graphs, model checking, and contrastive learning, are provided"}, "weaknesses": {"value": "- contrastive learning has been widely explored in similar tasks like code analysis and theorem proving; applying contrastive learning for LTL specifications is fairly incremental, especially given this work simply applies this standard idea to a synthetic dataset generated with existing tools. \n- evaluation tasks like cross-modal retrieval and intra-modal retrieval are artificial, and there is no clear indication how and to what extent, these retrieval tasks really help to tackle the model checking challenge (e.g., state exploration issue explicitly highlighted in the introduction). \n- the findings is somewhat well-expected, CodeBERT with some fine-tuning on the synthetic dataset shall outperform the original CodeBERT.\n- the assume-guarantee format is essential for this work, however, there is no discussion (including appendix) about the specific syntax for assume and guarantee sub-formulas. One concern is that they may be biased in a limited category."}, "questions": {"value": "It is surprising that the authors believe LTL is short for \"Linear-Time Temporal Logic\" (see background section), especially given that LTL is the focus of this work. What is the complete syntax for assumption and guarantee sub-formulas? Are they limited in some category, for instance, certain operators like \"implies\" is not allowed. \n\nWhy are remaining N^2-N pairs (implicitly) considered negative? Shouldn't validation checking be performed?\n\nHow do cross-modal retrieval and intra-modal retrieval help model checking? Are they purely hypothetic or used in any model checkers? To what extent, do these retrieval affect the performance of model checking algorithms?\n\nCertain discussions of the introduction (line 58 - 65) do not make much sense; if LLMs are used for the paper writing, the authors shall explicitly acknowledge that."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Pq2TXEGSiJ", "forum": "RE7hugbL6U", "replyto": "RE7hugbL6U", "signatures": ["ICLR.cc/2026/Conference/Submission21485/Reviewer_CFRC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21485/Reviewer_CFRC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762649471735, "cdate": 1762649471735, "tmdate": 1762941800383, "mdate": 1762941800383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}