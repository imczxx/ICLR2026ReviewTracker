{"id": "xg0fmtqh8d", "number": 18767, "cdate": 1758290738775, "mdate": 1759897082322, "content": {"title": "ChartNexus: Evaluating Multi-Chart Reasoning Capabilities of Multimodal Large Language Models", "abstract": "While Multimodal Large Language Models (MLLMs) have achieved remarkable success on single-chart question answering tasks, reaching over 90% accuracy on benchmarks such as PlotQA, this apparent success masks a critical limitation. Current models perform poorly on complex, multi-chart reasoning tasks that mirror real-world analytical scenarios. In professional document analysis, users typically integrate information across multiple visualizations within rich contextual frameworks rather than examining isolated charts, which is a capability that remains largely unexplored in existing evaluations. To bridge this gap, we introduce ChartNexus, a novel and challenging benchmark specifically designed to assess multi-chart reasoning capabilities of MLLMs in authentic document contexts. ChartNexus comprises 1,370 carefully curated question-answering pairs derived from 6,793 real-world charts spanning 18 domains, including scientific papers, government reports, and industry analyses. Each question demands complex reasoning skills, such as comparative analysis, sequential information integration, and cross-modal synthesis between visual and textual elements. We design a comprehensive taxonomy featuring 4 high-level difficulty categories and 11 fine-grained sub-categories to systematically evaluate these capabilities. Our comprehensive evaluation of 23 state-of-the-art MLLMs reveals significant performance degradation compared to single-chart benchmarks. While the best commercial model achieves over 90% accuracy on simpler tasks, its performance drops by more than half on ChartNexus. Through systematic failure analysis, we identify critical weaknesses in current models’ ability to maintain working memory across multiple charts, perform cross-modal reasoning, and integrate contextual information effectively. ChartNexus establishes a new frontier for evaluating complex chart understanding capabilities, demonstrating that robust multi-chart reasoning remains an open challenge. Our benchmark and comprehensive analysis provide the research community with essential diagnostic tools to advance the development of more capable and practically useful MLLMs for real-world document analysis scenarios.", "tldr": "A benchmark for evaluating MLLM's cross-modal reasoning capability in multi-chart, context-rich document scenarios.", "keywords": ["Chart Question-Answering", "Benchmark", "Multimodal Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f4ab22c6cdbb8695cbbd6cde30b3c6d84099ffe.pdf", "supplementary_material": "/attachment/3b2e006fbcf46c65659d079fbc1a7aec7a065b6d.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces ChartNexus, a benchmark for evaluating multi-chart reasoning in multimodal models. The authors collect charts from diverse sources in multiple languages, build a question-generation pipeline, and perform human annotation for question refinement and answer generation. They evaluate a range of models and find that significant gaps remain in multi-chart understanding and reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The benchmark is well-curated, incorporating diverse real-world sources in multiple languages and including a human annotation process.\n- The combination of multilingual (non-English) data and multi-chart questions is novel and valuable for assessing realistic multilingual chart understanding.\n- The breakdown of results by question type and reasoning skill (e.g., numerical, identify, compare, reason) provides diagnostic insight into capability-specific weaknesses.\n- The evaluation is comprehensive, covering a broad range of commercial, open-source, and chart-specific models."}, "weaknesses": {"value": "* Incomplete or inaccurate comparison with other benchmarks:\n  * Some relevant multilingual chart understanding datasets (e.g., PolyChartQA) are missing from the comparison.\n  * Benchmarks such as CharXiv include unanswerable questions, but the table incorrectly marks this feature as absent.\n* The reported inter-annotator agreement of 93.4% could implicitly reflect human performance (if re-annotation is considered as human evaluation), but no explicit human performance baseline is provided. Without this, it is hard to assess the human–model gap, especially for potentially ambiguous or erroneous questions.\n* The use of Qwen3-32B as an automated evaluator is reasonable for long-term reproduction, but there is no validation of model–human evaluation consistency. It remains unclear how accurate the judgments are, how well its judgments align with human assessment or whether biases exist across models.For example, for numerical tasks, verification requires explicit calculation of error margins (e.g., 5%), and SEAT-based decomposition introduces subjective interpretation that should be analyzed.\n* Several presentation issues reduce professionalism: “Unanswer” -> “Unanswerable”, “Open-End” -> “Open-Ended”, and “GPT-4o and its brothers” (L375) should be revised. Citations for evaluated models are also missing in relevant sections/tables.\n* The underperformance of specialized chart models is unsurprising, since (1) most are not trained for multi-chart settings, and (2) prior works such as ChartQAPro and CharXiv already highlight similar limitations even in single-chart setups.\n* The discussion on data leakage mitigation is unconvincing. Many data sources (e.g., OECD, Pew, arXiv) are present in existing multimodal pretraining datasets (e.g., MINT-1T, ChartQA). A more rigorous analysis would be required to support claims of leakage avoidance."}, "questions": {"value": "* The text formatting seems inconsistent with the submission template — can the authors adjust it?\n* In Figure 1, should “BenchNexus” (bottom right) be “ChartNexus”?\n* What is the exact question for the “Judgment” example in Figure 1?\n* There should essentially be 3 settings for chart analysis — (1) a single chart (2) a single chart with multiple subplots (i.e., multi-chart in a single image) and (3) multiple charts (i.e., multiple images). Do all samples in the dataset belong to category 3? If so, I wonder if authors could do an analysis turning all (3) instances into (2) for evaluation? This would help the community understand whether issues stem from multi-chart capabilities or multi-image capabilities.\n* The SEAT evaluation prompt appears to be in Chinese, while others are in English. Can the authors perform an ablation on evaluation sensitivity to prompt language?\n* Section 3.2 states annotators could create entirely new questions, but Figure 1 suggests only template selection and refinement. Please clarify this inconsistency.\n* How many options exist for multiple-choice questions? If there are typically four, why do some models (e.g., SmolVLM, ChartGemma) perform well below random chance (~25%)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6S9hpi5Vf9", "forum": "xg0fmtqh8d", "replyto": "xg0fmtqh8d", "signatures": ["ICLR.cc/2026/Conference/Submission18767/Reviewer_6Moe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18767/Reviewer_6Moe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531580454, "cdate": 1761531580454, "tmdate": 1762928503905, "mdate": 1762928503905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a significant weakness in Multimodal Large Language Models (MLLMs): while they excel at answering questions about single charts, their performance drastically declines when faced with complex, real-world tasks that require reasoning across multiple charts within a document. To address this evaluation gap, the authors introduce ChartNexus, a novel benchmark built from a large collection of real-world charts from domains like scientific papers and government reports. ChartNexus contains a curated set of questions designed to test multi-chart reasoning skills. A comprehensive evaluation of numerous state-of-the-art MLLMs on this benchmark reveals a substantial performance drop, uncovering key weaknesses in areas like cross-chart working memory and cross-modal reasoning. The study concludes that robust multi-chart understanding remains a major, unsolved challenge for MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Identifies a Critical Research Gap: The paper successfully highlights a major disconnect between existing single-chart benchmarks and the complex needs of real-world document analysis, moving the field beyond an overemphasis on isolated chart understanding.\n\n2. Novel and Rigorous Benchmark: The introduction of ChartNexus is a key contribution. Its strengths include: 1) Real-World Relevance:It is built from a large corpus of authentic charts from scientific, governmental, and industrial documents. 2) Systematic Design:It features a well-defined taxonomy of reasoning skills and difficulty levels, allowing for nuanced model diagnosis. 3) Complexity and Challenge:The benchmark is demonstrably challenging, causing significant performance drops even in top-tier models.\n\n3. Comprehensive and Conclusive Evaluation:The large-scale evaluation of 23 diverse MLLMs provides strong, empirical evidence for the paper's central claim about the limitations of current models."}, "weaknesses": {"value": "1. The overrepresentation of bar charts in the benchmark skews the overall evaluation. A benchmark for higher-difficulty tasks should prioritize more complex chart types to ensure a reliable and meaningful assessment of model capabilities.\n\n2. A fine-grained, per-category performance breakdown is required. It is essential to identify if there are specific chart types that the model completely fails to process, revealing the true boundaries of its current abilities.\n\n3. The insights presented are currently unsubstantiated, as they rely solely on textual description. Convincing validation requires dedicated ablation studies and visualizations to provide quantitative and tangible support for these claims.\n\n4. The core distinction between multi-chart and single-chart reasoning must be clarified. The evaluation must go beyond reporting a performance gap and actively diagnose the underlying reasons for it, which is critical for understanding and advancing multi-chart reasoning."}, "questions": {"value": "please refer to the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QituggqkmV", "forum": "xg0fmtqh8d", "replyto": "xg0fmtqh8d", "signatures": ["ICLR.cc/2026/Conference/Submission18767/Reviewer_yzXq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18767/Reviewer_yzXq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981961900, "cdate": 1761981961900, "tmdate": 1762928503131, "mdate": 1762928503131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ChartNexus introduces a benchmark for evaluating multi-chart reasoning in multimodal large language models, focusing on realistic document-level understanding rather than isolated chart interpretation. The dataset combines real-world charts, accompanying text, and human-verified question–answer pairs to test models’ ability to integrate visual and contextual information. Compared to existing single-chart benchmarks, results show a substantial performance decline across all models, indicating that multi-hop and cross-modal reasoning remain unsolved challenges. While top commercial systems outperform open-source and chart-specialized models, they still struggle with numerical precision, contextual integration, and detecting unanswerable questions. Chain-of-thought prompting offers limited gains, effective mainly for numerical reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Chart understanding is an important problem to work on an multimodal models generally struggle with this task.\n2. The benchmark is well-constructed and covers many task types and chart types. The automatic annotation is balanced with high manual annotation agreement. \n3. The evaluation covers over 20 models of many different types."}, "weaknesses": {"value": "It is hard to tell what insights / takeaways are novel from this benchmark vs other ones. The comparison across benchmarks is good to see, but I would want to know what additional signal this benchmark provides. For instance charXiv already identified that commercial models generally outperform open-source models in real-world chart settings. I think this benchmark is good, but it is important to understand what trends it shows that we could not find otherwise."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cF8kRTP8pt", "forum": "xg0fmtqh8d", "replyto": "xg0fmtqh8d", "signatures": ["ICLR.cc/2026/Conference/Submission18767/Reviewer_QnKr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18767/Reviewer_QnKr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129347431, "cdate": 1762129347431, "tmdate": 1762928501842, "mdate": 1762928501842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ChartNexus, a new benchmark dataset intended to evaluate the multi-chart and cross-modal (text + chart) reasoning capabilities of Multimodal Large Language Models (MLLMs). The authors argue that existing benchmarks are mostly restricted to single-chart queries, failing to capture the real-world scenarios. Empirical evaluation of 23 state-of-the-art MLLMs, including GPT-4o, exhibits a dramatic performance drop on this new benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A well-motivated problem, addressing a major concern in existing chart benchmarks that are mostly restricted to single-chart scenarios. \n2. Collection of data from real-world sources.\n3. Robust Human-in-the-Loop Annotation.\n4. Multilingual extension."}, "weaknesses": {"value": "1. The paper uses the Qwen3-32B model as the judge to evaluate the correctness of answers. However, the paper did not discuss the judge's accuracy against human scoring. \n\n2. The abstract claims the benchmark comprises 6,793 real-world charts. However, Table 4 mentions only 3,198 charts. This is a major contradiction, making the dataset construction questionable in a datasets & benchmark-focused work.\n\n3. While the paper reviewed various existing multi-chart benchmarks, it still lacks justification on how their contribution is not just an incremental contribution in comparison to the prior work.\n\n4. Lack of details on what method was applied to measure inter-annotator agreement.\n\n5. Lack of details on how the disagreement in open-ended QA is resolved."}, "questions": {"value": "Address the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z1nGxE2wPX", "forum": "xg0fmtqh8d", "replyto": "xg0fmtqh8d", "signatures": ["ICLR.cc/2026/Conference/Submission18767/Reviewer_mU4C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18767/Reviewer_mU4C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180553707, "cdate": 1762180553707, "tmdate": 1762928500374, "mdate": 1762928500374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}