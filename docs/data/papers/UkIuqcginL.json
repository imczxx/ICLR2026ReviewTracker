{"id": "UkIuqcginL", "number": 15547, "cdate": 1758252577367, "mdate": 1759897300126, "content": {"title": "FedCRAP: Federated Critical-Region-Aware Perturbations for Refined Privacy-Preserving Federated Learning", "abstract": "Federated Learning (FL) facilitates collaborative model training across a network of decentralized clients, enabling the development of global models without requiring raw data exchange. This approach preserves data privacy and security by keeping data localized on individual devices, but remains vulnerable to gradient inversion attacks. Existing defense mechanisms rely on global noise injection, which not only causes excessive utility loss or computational overhead but also fails to adequately protect sensitive information requiring additional emphasis. Intensified global perturbations to protect these local sensitive areas can compromise the overall utility of the image. This issue is particularly pronounced in sparse medical imaging data, where critical features are localized in specific regions.\nTo address this challenge, we propose Federated Critical-Region-Aware Perturbations (FedCRAP), a novel defense framework that leverages gradient-guided sparsity patterns. FedCRAP strategically injects noise into task-critical regions identified by high gradient magnitudes, aligning perturbations with the intrinsic sparsity of medical imaging data. \nBy integrating domain-specific sparsity awareness, FedCRAP achieves a favorable balance between privacy preservation and model performance. This provides a finer and more specific noise protection strategy, making it particularly effective.\nExtensive experiments across various datasets, including sparse medical datasets, demonstrate that FedCRAP preserves model accuracy while significantly reducing privacy leakage risks. It also shows clear superiority over previous state-of-the-art (SoTA) methods for privacy-preserving federated learning.", "tldr": "", "keywords": ["Federated Learning", "Privacy Preservation", "Gradient Inversion Attacks", "Utility-Privacy Tradeoff"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6486d36ec125e261f07426b5fca377b9587b99dc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes FedCRAP, a novel privacy-preserving technique for Federated Learning (FL) designed to defend against gradient leakage attacks (GLAs). The core idea is to move away from adding uniform noise (as in standard DP) and instead apply region-aware perturbations. The method identifies the \"critical\" regions in a user's data by analyzing gradient magnitudes and then injects noise only into these salient regions.. This approach is motivated by its potential for sparse data, such as medical images, where critical features are often localized. The goal is to maximize privacy (reconstruction hardness) while minimizing the loss in model accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel approach:** Introduces an innovative region-aware perturbation strategy that goes beyond traditional global noise injection in differential privacy. The gradient-guided sparsity pattern identification is conceptually sound and effective.\n\n2. **Clear motivation and intuition:** Provides a strong rationale — showing that uniform noise is inefficient in low-information regions and under-protective in sensitive areas. The proposed gradient-guided, targeted noise injection offers an intuitive and well-justified solution.\n\n3. **Relevant problem domain:** Addresses a critical and practical challenge in FL, focusing on protecting sensitive medical data where privacy is essential.\n\n4. **Positive empirical results (vs. DLG):** Demonstrates improved privacy preservation (higher MSE, lower SSIM) against DLG attacks on MNIST, FashionMNIST, and MedMNIST datasets, while maintaining model accuracy comparable to standard or unprotected methods."}, "weaknesses": {"value": "1. **Lack of formal privacy guarantees:** The paper provides no rigorous theoretical privacy analysis.\n\n2. **Limited experimental setup:** The evaluation does not reflect realistic FL scenarios, as the experiments are conducted on a very small scale using an IID data distribution.\n\n3. **Narrow threat model:** Focuses mainly on DLG attacks with no adaptive-attack evaluation; the novelty over prior data-space perturbation methods may be incremental."}, "questions": {"value": "1. **Formal privacy guarantees:** Can you provide formal privacy guarantees for FedCRAP? How does it compare to differential privacy in terms of theoretical foundations and provable privacy bounds?\n\n2. **Adaptive attack robustness:** How does the method perform against adaptive attacks that are aware of the region-aware perturbation strategy? Has its robustness been evaluated under such informed adversarial conditions?\n\n3. **Performance on real clinical data:** It would be interesting to know how FedCRAP performs on real-world clinical datasets, beyond benchmark datasets like MedMNIST. Does it maintain both privacy protection and model utility in complex, heterogeneous medical data scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fyDF7sEDdi", "forum": "UkIuqcginL", "replyto": "UkIuqcginL", "signatures": ["ICLR.cc/2026/Conference/Submission15547/Reviewer_8jqa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15547/Reviewer_8jqa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761131800711, "cdate": 1761131800711, "tmdate": 1762925825199, "mdate": 1762925825199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Federated Critical-Region-Aware Perturbations (FedCRAP), a defense mechanism for Federated Learning (FL) against gradient inversion attacks, particularly focusing on the limitations of global noise injection methods, which can harm utility or under-protect critical areas, especially in sparse medical data. FedCRAP introduces a new approach that strategically injects noise only into task-critical image regions identified by high gradient magnitudes, aiming to align perturbations with data sparsity and provide targeted protection. By concentrating perturbations on sensitive areas using a dynamically generated mask based on gradient thresholds, the method claims to achieve a more favorable balance between privacy preservation against attacks like DLG and maintaining model utility. Experiments on datasets including MNIST, FashionMNIST, and medical imaging datasets reportedly demonstrate FedCRAP's ability to preserve model accuracy while significantly reducing privacy leakage risks compared to state-of-the-art methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* This paper identifies a limitation in existing FL privacy methods, which often apply uniform noise globally, potentially harming utility or under-protecting critical regions, especially in sparse data like medical images. FedCRAP proposes a targeted perturbation strategy to address this specific issue.\n\n* The core idea of aligning perturbations with task-critical regions, identified via gradient magnitudes, is intuitively appealing. This approach aims to provide stronger protection where it is most needed while potentially preserving utility in less critical areas, offering a more refined balance than the global noise injection."}, "weaknesses": {"value": "* Unprofessional Acronym and Presentation: The chosen acronym \"FedCRAP\" is highly unprofessional and detracts significantly from the paper's credibility. Additionally, figures like Figure 1 appear rudimentary and lack the polish expected for a top-tier conference submission.\n\n* The core technical contribution appears weak. The paper explicitly states its region-aware insight builds upon Sun et al. (2024b) and uses the method from that same work for mask generation. FedCRAP seems primarily focused on adapting an existing localized perturbation/masking technique (originally for unlearnable examples) to the FL privacy setting, rather than proposing a fundamentally new defense mechanism. The listed contributions also seem padded, with \"Problem Identification\" and \"Empirical Validation\" not representing core technical innovations.\n\n* Section 3.1, which defines standard Federated Learning , is inappropriately placed within the methodology section instead of in the background or preliminaries. The description of the actual FedCRAP algorithm (Section 3.3) is quite brief, lacking sufficient detail on the integration of the bi-level optimization and the practical implementation of the dynamic masking and perturbation updates.\n\n* The paper completely lacks essential theoretical analyses. There is no formal security analysis or proof regarding the privacy guarantees offered against gradient inversion attacks (e.g., in terms of differential privacy bounds or other metrics). Furthermore, there is no analysis of the computational complexity, communication overhead, or convergence properties of the proposed method, despite claiming the overhead is \"minor\".\n\n* While the paper compares against DP variants and FedEM, the evaluation is limited. The datasets used (MNIST, FashionMNIST, some MedMNIST variants)  are relatively simple image classification tasks. The experiments are conducted under a basic IID data split across only 4 clients, which does not reflect the heterogeneity and scale challenges common in real-world FL.\n\n* The abstract claims \"clear superiority over previous state-of-the-art (SoTA) methods\". However, the results in Table 1 and Table 2  show a more nuanced picture. While FedCRAP often performs well on privacy metrics (like Feature MSE), it doesn't uniformly dominate all baselines across all metrics and datasets (e.g., FedEM sometimes shows better utility, and standard DP sometimes achieves higher privacy scores on certain metrics). The claim of \"clear superiority\" seems exaggerated based on the presented data.\n\n* The paper suffers from numerous grammatical errors and inconsistent formatting. Examples include missing spaces before parentheses, inconsistent capitalization, and awkward phrasing, which detract from the paper's professionalism and readability."}, "questions": {"value": "* Given that the core region-aware masking mechanism is adapted from Sun et al. (2024b) , could you elaborate on the primary technical novelty of FedCRAP beyond this adaptation to the FL privacy setting?\n\n* Why does the paper lack a formal analysis providing rigorous privacy guarantees (e.g., in terms of differential privacy) or analyzing the computational complexity and convergence behavior of FedCRAP?\n\n* How would the results obtained on relatively simple datasets under IID conditions with only four clients generalize to the more complex, heterogeneous, and large-scale scenarios typical of real-world federated learning?\n\n* Considering the results where FedCRAP does not uniformly outperform all baselines across every metric, could you please provide a stronger justification for the claim of \"clear superiority\"?\n\n* What was the rationale for placing the standard definition of Federated Learning within the methodology section (Section 3.1)  and keeping the core algorithm description relatively brief?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zmtdSdOseI", "forum": "UkIuqcginL", "replyto": "UkIuqcginL", "signatures": ["ICLR.cc/2026/Conference/Submission15547/Reviewer_3EqM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15547/Reviewer_3EqM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761163731842, "cdate": 1761163731842, "tmdate": 1762925824415, "mdate": 1762925824415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates defenses against gradient inversion attacks in federated learning by introducing perturbations. It proposes a novel framework that leverages gradient-guided sparsity patterns. The goal is to strike a balance between privacy preservation and model utility. The proposed method is evaluated on four public datasets and shows improved performance over baseline approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Privacy protection in federated learning is a critical and challenging research area.\n- The idea of using gradient information to guide the defense mechanism is conceptually clear and intuitive.\n- The paper provides a a comprehensive review of related work, helping readers understand the context and motivation."}, "weaknesses": {"value": "- It would be beneficial to include an algorithm box to clearly present the overall workflow and key steps of the proposed method.\n\n- The perturbation update and mask generation appear to rely heavily on existing techniques. The specific technical contributions of this work should be more clearly distinguished and emphasized.\n\n- The method requires fine-grained generation of perturbations, which may incur computational overhead. A detailed analysis of computational cost and latency on local clients is necessary. Does this approach introduce significant burden in practical deployments?\n\n- The evaluation only considers DLG as the attack method. However, DLG may not represent the strongest or most up-to-date attack. The paper should include comparisons with more recent and powerful attack methods to validate the robustness of the proposed defense.\n\n- The experimental evaluation is relatively weak. It mostly focuses on performance comparison across methods. Additional experiments would strengthen the empirical evidence.\n\n- The datasets used are mainly from MedMNIST, which consists of low-resolution medical images. However, real-world medical images are typically high-resolution. The practical applicability of the proposed method to real-world medical scenarios requires more rigorous evaluation."}, "questions": {"value": "-\tDP-based methods offer mathematically provable privacy guarantees. It is unclear how this method compares in terms of privacy rigor. Can any theoretical privacy guarantees or bounds be provided?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OiC41ySsti", "forum": "UkIuqcginL", "replyto": "UkIuqcginL", "signatures": ["ICLR.cc/2026/Conference/Submission15547/Reviewer_WrB4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15547/Reviewer_WrB4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634741019, "cdate": 1761634741019, "tmdate": 1762925823872, "mdate": 1762925823872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FedCRAP, a framework for federated learning that selectively injects perturbations into task-critical image regions identified via gradient magnitudes. FedCRAP introduces gradient-guided masking, localized PGD-based noise generation, and sparsity constraints to protect privacy against Gradient Leakage Attacks (GLAs). Authors conduct experiments on MNIST-series to demonstrate FedCRAP maintains high utility while improving resistance to gradient inversion."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**S1.** The challenge of uniform global noise addition fails to consider spatial heterogeneity of medical image is make sense.\n\n**S2.** Experiments include comparisons against standard DP baselines and FedEM across multiple datasets."}, "weaknesses": {"value": "**W1.** The quality and clarity of Figure 1 are too low. Authors are encouraged to redraw it.\n\n**W2.** The proposed algorithm ***(gradient-magnitude masking with localized PGD noise)*** is an incremental variant of prior gradient-guided adversarial masking and region-aware perturbation methods. In addition, there is no proof or quantifiable privacy guaratee.\n\n**W3.** (1) The selection of the gradient threshold $\\tau$ and perturbation radius $\\rho$ is ad hoc, with tuning results shown only for FashionMNIST, (2) There is no clear sensitivity analysis on why $\\tau = 10\\%$ is best across datasets, and (3) The dependence of privacy–utility balance on these parameters is not clear.\n\n**W4.** Datasets like MNIST and FashionMNIST still overly simplistic, and no experiments on non-iid splits that are critical for real-world FL. Furthermore, the evaluation of true attach resilience is missing.\n\n**W5.** Without pseudocode or reproducible settings."}, "questions": {"value": "Please see Weaknesses.\n\nAdditional questions:\n\n**Q1.** Can the authors formally analyze the privacy guarantee of FedCRAP?\n\n**Q2.** How does the algorithm behave under non-IID client distributions? Does region-level sensitivity generalize when data vary across clients?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NzCQAhHIJY", "forum": "UkIuqcginL", "replyto": "UkIuqcginL", "signatures": ["ICLR.cc/2026/Conference/Submission15547/Reviewer_meXv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15547/Reviewer_meXv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893892646, "cdate": 1761893892646, "tmdate": 1762925823181, "mdate": 1762925823181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}