{"id": "uKPdSZuvUJ", "number": 6284, "cdate": 1757964629309, "mdate": 1763713946826, "content": {"title": "I$^2$C: Intra- and Inter-modality Consistency Learning for Multimodal Sentiment Analysis", "abstract": "Multimodal sentiment analysis (MSA) aims to predict human sentiments by integrating signals from different modalities such as text, video, and audio. However, sentiment cues are often semantically inefficient—exhibiting inconsistency within and across modalities—that hinders robust understanding and inflates computation. In this paper, we propose I$^2$C, a framework that explicitly models Intra- and Inter-modality Consistency to guide effective and efficient sentiment prediction. I$^2$C first projects token-level features into a shared sentiment space and computes intra- and inter-modality consistency scores (I$^2$CS). The I$^2$CS serves three functions: (1) as a consistency loss for regularizing training; (2) as token-wise weights for reweighting features; and (3) as a compression signal for eliminating redundant or conflicting tokens. Extensive experiments are conducted on the CMU-MOSI and CMU-MOSEI datasets, and the results show that I$^2$C outperforms previous state-of-the-art models. Despite removing 90\\% of tokens, I$^2$C maintains comparable performance, exhibiting remarkable robustness across varying token budgets. All results highlight consistency-aware learning as an effective strategy to improve the accuracy and efficiency of sentiment prediction.", "tldr": "", "keywords": ["Multimodal Learning", "Multimodal Sentiment Analysis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6f8ab212fd594e2ca02ddb2f8edf450fe9d10d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method to address both intra-modal and inter-modal semantic conflicts."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The experimental results are competitive."}, "weaknesses": {"value": "1. The code is not released, and the paper lacks sufficient detail.\n2. The novelty is limited. Intra-modal consistency is enforced after the encoder outputs, where token interactions have already occurred via the attention mechanism, making the effectiveness of masking questionable.\n3. The inter-modal consistency formulation (Equation 4) is unclear.\n4. It is not specified whether the [CLS] tokens from the text and speech encoders are used in subsequent stages—this is crucial for assessing the soundness of the method design.\n5. The mathematical notation is confusing, and the experimental explanations are insufficient.\n6. In Table 3(a), reducing the parameter from 1 to 0.1 results in only 0.9% performance drop, further questioning the usefulness of post-encoding masking and whether the CLS token is removed.\n7. The comparisons in Table 3(b) are unclear.\n8. Overall, the paper appears unfinished."}, "questions": {"value": "Refer to the weaknesses listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CVTVyfpi8d", "forum": "uKPdSZuvUJ", "replyto": "uKPdSZuvUJ", "signatures": ["ICLR.cc/2026/Conference/Submission6284/Reviewer_geLK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6284/Reviewer_geLK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760582296803, "cdate": 1760582296803, "tmdate": 1762918590772, "mdate": 1762918590772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on Multimodal Sentiment Analysis (MSA), aiming to address the semantic inconsistency problems that exist within and across modalities. Specifically, the authors propose a framework which first projects token-level features into a shared sentiment space and computes intra- and inter-modality consistency scores. The score is calculated based on the Jensen-Shannon (JS) divergence between latent sentiment prediction. Experiments are conducted on the CMU-MOSI and CMU-MOSEI datasets, and the results show that the proposed framework achieves SOTA performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method achieves state-of-the-art (SOTA) performance on both the CMU-MOSI and CMU-MOSEI benchmark.\n2. The paper strongly supports the rationale of the model design through comprehensive ablation studies."}, "weaknesses": {"value": "1. The paper's core motivation hinges on the assertion that \"existing methods often overlook the semantic in consistencies that arise from redundant intra-modal signals or conflicting cross-modal cues, which can introduce representational noise and impair fusion\". However, this key claim lacks direct theoretical or experimental support.\n2. The authors justify their choice of JS divergence by highlighting its advantages over KL divergence. However, the paper lacks a broader justification for why JS divergence is superior to other strong alternatives.  Could the authors elaborate on why this method was chosen over other metrics (e.g., Euclidean distance or Cosine Similarity)?  Have comparative experiments been conducted?\"\n3. The definition of the Inter-modality Consistency Score is ambiguous. The authors do not clearly explain in the paper how the relevant content in Equation 4 is obtained.\n4. The paper lacks a deep analysis of a key finding in Table 3a, where model performance at a 0.8 token retention ratio is superior to the baseline using all tokens (1.0 ratio). This result strongly implies that the 1.0 model is negatively impacted by noisy (redundant or conflicting) tokens. The authors briefly mention this but fail to analyze why the model's soft selection mechanism or consistency loss was not sufficient to automatically suppress this noise."}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aqS9ldv9kJ", "forum": "uKPdSZuvUJ", "replyto": "uKPdSZuvUJ", "signatures": ["ICLR.cc/2026/Conference/Submission6284/Reviewer_UzNZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6284/Reviewer_UzNZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902451212, "cdate": 1761902451212, "tmdate": 1762918590306, "mdate": 1762918590306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an I2C, a framework that explicitly models Intra- and Inter-modality Consistency to guide effective and efficient sentiment prediction. It first projects token-level features into a shared sentiment space and computes intra- and inter-modality consistency scores (I2CS).   I2C maintains comparable performance, exhibiting remarkable robustness across varying token budgets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-structured and logically compelling. It provides a clear explanation of the algorithms and offers a comprehensive set of experiments that thoroughly validate the proposed method."}, "weaknesses": {"value": "It should be noted that the I2C method, which models Intra- and Inter-modality Consistency for feature representation, has been previously released in previous papers. Therefore, this work cannot claim to be its first proposer, which significantly limits its novelty. The Intra- and Inter-modality Consistency approach itself is more of an engineering heuristic and lacks substantial theoretical underpinnings. While it succeeds in improving experimental metrics, its conceptual novelty is relatively weak."}, "questions": {"value": "1. The paper presents a framework that explicitly models Intra- and Inter-modality Consistency to guide effective and efficient sentiment prediction. The idea is very similar to the following framework, which effectively captures discriminative intra-frame and inter-frame features for representative feature learning. \nIt is suggested to refer to and analyze their similarity and differences.\nRelation-mining self-attention network for skeleton-based human action recognition, Pattern Recognition, Vol. 139, 109455, 2023. \n2. I2CS(hi) is calculated between every two modalities. The question is, has every pair of modalities, text, visual, and audio, being paired and calculated the I2CS(hi) value? Equations (4) and (5) do not show the detailed information.\n3. About the performance of I2CS(hi), it is better to evaluate the contribution from every two-modality pair, and also illustrate the relationship between different modalities."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "88kPZLU10l", "forum": "uKPdSZuvUJ", "replyto": "uKPdSZuvUJ", "signatures": ["ICLR.cc/2026/Conference/Submission6284/Reviewer_iZXh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6284/Reviewer_iZXh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978801236, "cdate": 1761978801236, "tmdate": 1762918589364, "mdate": 1762918589364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}