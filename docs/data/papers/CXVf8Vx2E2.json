{"id": "CXVf8Vx2E2", "number": 7401, "cdate": 1758020029680, "mdate": 1759897855023, "content": {"title": "Towards Efficient Post-Training Quantization For Large Vision-Language Models Via Token-Wise Redundancy Elimination", "abstract": "Post-training quantization (PTQ) has emerged as an effective technique for compressing large models and accelerating inference without retraining. While PTQ has been extensively studied in large language models (LLMs), its application to vision-language models (VLMs) remains underexplored. In this work, we identify two intrinsic characteristics of VLM activations: 1) visual over-representation, where vision tokens are excessive and often redundant, and 2) modality gap, which refers to the clear separation between text and vision tokens in the latent feature space. Together, these two factors significantly deteriorate quantization performance but have been overlooked by existing PTQ methods. To address these challenges, we propose VLMQ, A VLM-tailored PTQ framework that selectively prioritizes salient tokens while suppressing redundant ones during quantization. In particular, we introduce a gradient-driven importance factor to capture the token-wise importance variance, the effectiveness of which is substantiated through both empirical and theoretical analysis. To ensure efficiency, we propose to use lightweight block-wise backpropagation for factor acquisition. Finally, we reformulate the optimization objective into an importance-aware form to preserve importance activation information. Extensive evaluations on 8 benchmarks across 0.5B$\\sim$32B VLMs demonstrate the state-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit settings. For example, it achieves a substantial 16.45% improvement on MME-RealWorld under 2-bit quantization. Code is provided in the supplementary material.", "tldr": "An efficient and accurate post-training quantization (PTQ) framework for large vision-language models (VLMs). The key innovation is the use of gradient-driven importance factors to eliminate token-wise redundancy, significantly improving performance.", "keywords": ["Vision-Language Models (VLMs)", "Post-Training Quantization (PTQ)", "Model Compression"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/641bb8cba442f7e680b52fde6745e513a1317135.pdf", "supplementary_material": "/attachment/9289604396f88b0672502d6aea1e108054fb8892.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies two issues in VLM activations that hinder quantization: (1) visual over-representation, vision modality inputs produce an excessive number of tokens, many of which are redundant; and (2) modality gap, there is a clear separation between vision and text token distributions in latent space. To address these issues, the authors propose VLMQ, which introduces a gradient-driven token importance factor G to automatically re-weight each token’s importance, enabling the model to allocate its limited quantization precision preferentially to the truly important tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A substantive assessment of the strengths of the paper, touching on each of the following dimensions: originality, quality, clarity, and significance. We encourage reviewers to be broad in their definitions of originality and significance. For example, originality may arise from a new definition or problem formulation, creative combinations of existing ideas, application to a new domain, or removing limitations from prior results.\n\n1.The paper is well-motivated with a clear problem definition and justification of its importance. The authors effectively highlight the redundancy issue in visual tokens and the challenges of post-training quantization in vision-language models.\n\n2.The visualizations are intuitive and help readers understand the key ideas, such as token-wise importance and the gradient-based weighting mechanism.\n\n3.The experimental section especially the ablation studies are comprehensive, which demonstrates the effectiveness and validity of the proposed method."}, "weaknesses": {"value": "A substantive assessment of the weaknesses of the paper. Focus on constructive and actionable insights on how the work could improve towards its stated goals. Be specific, avoid generic remarks. For example, if you believe the contribution lacks novelty, provide references and an explanation as evidence; if you believe experiments are insufficient, explain why and exactly what is missing, etc.\n\n1.The performance gains at higher bit-widths (e.g., INT3) are relatively small — sometimes less than 0.2%, and even slightly inferior to baselines on specific models such as Qwen2-VL-2B-INT3g128.\n\n2.The method introduces additional computational and memory overhead due to the need for block-wise backpropagation, which limits its efficiency compared to simpler post-training quantization approaches like GPTQ.\n\n3.It is not clear how the method performs for language-heavy/language-only task. Given that GPTQ is relatively outdated for language tasks, it would be helpful to examine how the proposed token-importance weighting impacts language performance."}, "questions": {"value": "Please list up and carefully describe any questions and suggestions for the authors. Think of the things where a response from the author can change your opinion, clarify a confusion or address a limitation. This is important for a productive rebuttal and discussion phase with the authors.\n\n1.Include experiments on larger models such as LLaVA-13B or InternVL-26B to verify scalability.\n\n2.Provide results for INT4 quantization, which may offer a better trade-off between accuracy and efficiency and has better deployment support.\n\n3.Evaluate the model for language-heavy or language-only task, where the visual component plays only an auxiliary role or is entirely absent.\nReport memory consumption for training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Us7w4HLRox", "forum": "CXVf8Vx2E2", "replyto": "CXVf8Vx2E2", "signatures": ["ICLR.cc/2026/Conference/Submission7401/Reviewer_x6UG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7401/Reviewer_x6UG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927460789, "cdate": 1761927460789, "tmdate": 1762919523147, "mdate": 1762919523147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a post-training quantization (PTQ) framework tailored for visual-language models (VLMs) based on existing PTQ techniques. It first identifies two critical issues in quantizing VLM models: visual over-representation and modality gap. Building upon these challenges, the paper introduces a diagonal gradient-derived importance factor to reflect token-level significance and employs a single lightweight block-wise backpropagation to compute gradients. The paper's observations on the characteristics of VLM model activation inputs offer valuable insights, and the approach of computing token importance from gradients warrants further investigation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly identifies two overlooked challenges in post-training quantization (PTQ) for vision-language models (VLMs): visual over-representation and modality gap. These insights are well-motivated and supported by empirical visualization. The proposed VLMQ framework introduces a gradient-driven importance factor that selectively emphasizes salient tokens and suppresses redundant ones. This token-wise quantization adjustment is both theoretically justified (Theorem 4.1) and efficiently implemented via block-wise backpropagation, striking a good balance between accuracy and computational cost."}, "weaknesses": {"value": "1. Limited Analysis of Generalization and Robustness\nWhile results cover multiple benchmarks, the study focuses on VQA-style tasks. It remains unclear whether the proposed method generalizes equally well to other multimodal reasoning or generation tasks (e.g., captioning, retrieval, or multi-image reasoning). Moreover, the paper’s gradient-based importance analysis is overly simplistic and lacks comparison with the Taylor-expansion-based importance estimation employed in SliM-LLM[1]. Additionally, the description of the block-wise backpropagation mechanism is insufficiently detailed. A thorough comparison and analysis against existing block-wise quantization methods (e.g., Q-VLM) would significantly strengthen this section.\n\n2. Computational Efficiency Claims Need More Clarity\nThe paper claims that block-wise backpropagation is “lightweight”, but only provides GPU-hour comparisons in small-scale settings. A detailed runtime and memory analysis on full-scale 32B models would better substantiate the claimed efficiency. Furthermore, the paper omits any evaluation of the memory footprint and inference speedup achieved after quantization. Such empirical results are essential and should be included to fully support the efficiency claims.\n\n3. Lack of Discussion on Integration with Other Quantization Frameworks\nAlthough the authors mention that VLMQ can be combined with other PTQ algorithms, experiments only integrate it with GPTQ/GPTAQ. Broader evaluations (e.g., with SmoothQuant or Q-VLM) would strengthen the argument for framework compatibility, and better demonstrate the superiority of the VLMQ approach in terms of performance and computational overhead.\n\n[1]Huang, Wei, et al. \"SliM-LLM: Salience-driven mixed-precision quantization for large language models.\" arXiv preprint arXiv:2405.14917 (2024)."}, "questions": {"value": "Further question:\n1.\tCan token-wise importance estimation be extended to dynamic or streaming multimodal inputs? Since current VLMQ computes importance factors from static calibration data, investigating adaptive or online estimation methods could improve robustness when processing diverse, real-world multimodal streams. Is it feasible to compute token importance in real time based on gradients during dynamic input processing?\n2.\tCan importance-aware quantization be integrated with training-time or mixed-precision quantization strategies? Exploring how the proposed token-level weighting interacts with quantization-aware training (QAT) or hybrid bit-width assignments might further improve model efficiency without sacrificing accuracy.\n3.\tWhat are the effects of token-wise quantization on cross-modal alignment and interpretability? Since VLMQ adjusts the relative weighting of tokens, analyzing how it alters attention distributions or latent alignment between text and vision could offer deeper insights into multimodal representation learning. Addressing this question not only strengthens the theoretical foundation of this paper but also facilitates further research on MLLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "H47p9zAEZ6", "forum": "CXVf8Vx2E2", "replyto": "CXVf8Vx2E2", "signatures": ["ICLR.cc/2026/Conference/Submission7401/Reviewer_ZiWz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7401/Reviewer_ZiWz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982731415, "cdate": 1761982731415, "tmdate": 1762919522746, "mdate": 1762919522746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on post-training quantization (PTQ) for Vision-Language Models (VLMs). One key observation is that VLM activations exhibit two intrinsic characteristics: visual over-representation and modality gap, which have been overlooked by existing PTQ methods. A VLM-tailored PTQ framework, VLMQ, is proposed to mitigate these issues. Experiments are carried out on 8 benchmarks across VLMs of various sizes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[+] The manuscript is well written, with clear logics.\n\n[+] The symbol definitions are clear, and the image visualization is complete. \n\n[+] Many experiments are conducted to analyze the proposed method."}, "weaknesses": {"value": "[-] The core ablation experiments are insufficient. The key contribution is introducing a weighted strategy based on the importance score, thus one critical ablation experiment should be: baseline (e.g., GPTQ/GPTAQ) vs. VLMQ vs. VLMQ but removing importance weighting (i.e., all tokens have equal weights). This constitutes a logical gap in the argumentation.\n\n[-] Under INT2 settings, the results of AWQ and MBQ methods almost completely collapse (e.g. accuracy of 0.00% or single digits). Although this highlights the superiority of VLMQ, it also raises the question: are these two methods really that fragile? Or did the author not use their officially recommended parameters or implementations optimized for ultra-low bit rates?\n\n[-] The fairness and consistency of baseline comparisons. Across different experimental setups, the paper switches the baselines. Why was a weaker method, GPTQ, chosen as the baseline for the INT2 setting where performance is most critical, rather than the stronger GPTAQ? This raises doubts that the VLMQ approach may be incompatible with certain mechanisms of GPTAQ or perform poorly when combined. A fairer comparison would involve applying VLMQ to both GPTQ and GPTAQ separately and then comparing the results with the original GPTQ and GPTAQ."}, "questions": {"value": "[-] INT3g128: On Qwen2-VL-2B, the average score of VLMQ (62.90) is lower than that of GPTAQ (63.44). The author attributes it to the abnormal performance of GPTAQ on MME RealWorld, but this does not change the fact that VLMQ is not optimal in this setting. INT2g128: On the LLaVA-OneVision-7B model, VLMQ's performance on MMEen (38.66) was significantly lower than GPTQ (40.29) and GPTAQ (42.59), but it surpassed the baseline by a slight advantage on other tasks in the final average score. This situation of \"overall victory but lagging behind in key indicators\" should be discussed in more depth."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2JSLxR5l5x", "forum": "CXVf8Vx2E2", "replyto": "CXVf8Vx2E2", "signatures": ["ICLR.cc/2026/Conference/Submission7401/Reviewer_RFpm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7401/Reviewer_RFpm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985689417, "cdate": 1761985689417, "tmdate": 1762919521518, "mdate": 1762919521518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To ACs and Reviewers"}, "comment": {"value": "**Dear ACs and Reviewers,**\n\nWe sincerely thank you for your time, constructive feedback, and detailed evaluations. We have carefully reviewed all comments and provided comprehensive responses, additional analyses, and new experiments to address each concern. We would like to highlight the following points.\n\n**Strengths Highlighted by Reviewers**\n\n- **Clear motivation and problem formulation.** The paper is well written and clearly identifies two overlooked challenges in VLM post-training quantization: 1)visual over-representation, and 2) the modality gap.\n- **Strong methodological clarity, visualization quality.** Visualizations are intuitive and informative, symbol definitions are clear. The block-wise gradient extraction is efficient and well motivated.\n- **Comprehensive empirical evaluation.** Extensive experiments and ablation studies validate the effectiveness and robustness of the proposed VLMQ framework.\n\n**Additional Experiments**\n- Added an ablation on naive token-weighting with a 3×3 grid (Reviewer RFpm). \n- Added compatibility experiments with GPTQ and GPTAQ (Reviewer RFpm). \n- Included INT3 results on a larger 32B model to demonstrate scalability (Reviewer x6UG). \n- Added broader generalization results on HellaSwag, MMMU, and TextCaps (Reviewers ZiWz and x6UG). \n- Added hardware efficiency analysis, including quantization latency and peak memory for 2B/7B/32B models (Reviewers ZiWz and x6UG). \n- Included INT4 quantization results (Reviewer x6UG). \n- Added analysis on cross-modal alignment and interpretability (Reviewer RFpm).\n- Clarified the block-wise backpropagation mechanism and its theoretical foundation (Reviewer RFpm).\n\n**Discussion**\n- Explanation for baseline degradation under INT2 quantization (Reviewer RFpm).\n- Clarification on the role and limitations of precursor methods (Reviewer RFpm).\n- Distinction between our approach and Q-VLM / SliM-LLM (Reviewer ZiWz).\n- Analysis of cross-modal alignment and interpretability effects (Reviewer ZiWz).\n- Detailed clarification of the block-wise backpropagation mechanism and theoretical rationale (Reviewer ZiWz).\n- Discussion on compatibility with QAT, mixed-precision quantization, and streaming scenarios (Reviewer ZiWz).\n\nWe respectfully invite the reviewers to consider our revisions and responses. If our clarifications and additional results adequately address the concerns, we would be grateful for your reconsideration. We sincerely appreciate the reviewers’ thoughtful and constructive feedback.\n\nBest Regards,\n\nAuthors of Submission #7401"}}, "id": "IWEZFTxibc", "forum": "CXVf8Vx2E2", "replyto": "CXVf8Vx2E2", "signatures": ["ICLR.cc/2026/Conference/Submission7401/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7401/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission7401/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763709678442, "cdate": 1763709678442, "tmdate": 1763709678442, "mdate": 1763709678442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}