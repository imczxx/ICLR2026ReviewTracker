{"id": "7cUxi8LbKD", "number": 22794, "cdate": 1758335497037, "mdate": 1759896845848, "content": {"title": "SHAPO: Sharpness-Aware Policy Optimization for Safe Exploration", "abstract": "Safe exploration is a prerequisite for deploying reinforcement learning (RL) agents in safety-critical domains. In this paper, we approach safe exploration through the lens of epistemic uncertainty, where the actor’s sensitivity to parameter perturbations serves as a practical proxy for regions of high uncertainty. We propose Sharpness-Aware Policy Optimization (SHAPO), a sharpness-aware policy\nupdate rule that evaluates gradients at perturbed parameters, making policy updates pessimistic with respect to the actor’s epistemic uncertainty. Analytically we show that this adjustment implicitly reweighs policy gradients, amplifying the\ninfluence of rare unsafe actions while tempering contributions from already safe ones, thereby biasing learning toward conservative behavior in under-explored regions. Across several continuous-control tasks, our method consistently improves both safety and task performance over existing baselines, significantly expanding their Pareto frontiers.", "tldr": "SHAPO makes policy updates pessimistic under epistemic uncertainty, reweighting gradients toward rare unsafe actions to enable safer exploration and improved safety–performance trade-offs", "keywords": ["Safe Exploration", "Sharpness Aware Minimization", "Epistemic Uncertainty"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f2792fdb38b8d5559d8673951aac9c09293bb9e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Sharpness-Aware Policy Optimization, a novel approach to enhance safe exploration in Reinforcement Learning by addressing the actor's epistemic uncertainty. The core idea is to use the \"sharpness\" (sensitivity of the policy to parameter perturbations) as a practical proxy for uncertainty, particularly in regions with scarce data.\nSHAPO leverages principles from Sharpness-Aware Minimization (SAM). The proposed update rule operates in a pessimistic manner: it first finds the worst-case policy parameter θ within a local neighborhood that minimizes the performance objective (maximizes risk), and then computes the policy gradient at this \"pessimistic\" point. This results in a modification of the policy gradient that analytically promotes conservative behavior by amplifying the penalty for rare, unsafe actions while attenuating the reward for rare, safe actions. The method is implemented as a plugin on top of existing safe RL algorithms (TRPO) and demonstrates improved performance on the safety-efficiency Pareto frontier across various Safety Gym tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  Applying Sharpness-Aware Minimization (SAM), typically used for improving generalization in supervised learning, to Safe RL is a  conceptually elegant contribution. This provides a new lens for addressing epistemic uncertainty in the actor network.\n2. The analysis (Section 3.3) and empirical results support the claim that the SHAPO gradient effectively introduces a pessimistic bias. The method consistently achieves a lower frequency of catastrophic events and generally improves the safety performance of multiple SOTA safe RL baselines when used as a plugin.\n3. The update rule is straightforward and can be readily integrated with different Policy Optimization algorithms without fundamental changes to their core optimization objective, showcasing high modularity."}, "weaknesses": {"value": "1. The paper suffers from a crucial disconnect between the theory and implementation. Proposition 2 defines the perturbation magnitude \\delta_{Down}​\t as a function of the sample size n and confidence level α. However, in the practical implementation and hyperparameter search (Appendix D), \\delta_{Down}​ is treated as a fixed hyperparameter, which contradicts the theoretical guidance for an annealing schedule based on n.\n2. The modeling of the posterior distribution Q(θ) is solely \"motivated\" by the BvM theorem. This represents a strong, unproven assumption, as the strict regularity conditions required for BvM are unlikely to hold in high-dimensional, non-convex deep RL settings, thus introducing a significant theoretical gap.\n3. There is an explicit inconsistency in the definition of the covariance matrix for the distribution Q(θ) between the main text and the appendix proof of Proposition 5, which uses a distribution corresponding to a precision matrix proportional to $\\sqrt n$. This error undermines the rigor of the derived relationship for $\\delta _{Down}$.\n​4. The paper focuses on comparing SHAPO's performance against standard constrained optimization methods (CPO, CRPO, etc.). To fully validate the claim of solving epistemic uncertainty in the actor, the baselines should ideally include methods that explicitly handle uncertainty or risk sensitivity, such as: Policy Gradient methods utilizing Dropout or Ensembles on the actor. Risk-Sensitive Policy Optimization methods (e.g., those based on CVaR or other risk measures).\n5.  While $\\delta _{Down}$ is the critical new hyperparameter of the method, the paper provides only a qualitative discussion of its sensitivity (Appendix D) and lacks a comprehensive, quantitative ablation study."}, "questions": {"value": "1. Given the theoretical prescription that $\\delta _{Down}$ should decay with the number of samples n, why was a fixed, tuned hyperparameter used in the implementation? Could the authors provide an ablation study comparing the fixed hyperparameter approach with the theoretically suggested annealing schedule?\n2. Please clarify and correct the discrepancy in the definition of the posterior/likelihood distribution Q(θ) between Section 3.2 and Appendix B. A corrected, rigorous proof for Proposition 2 (or 5) is required.\n3. To solidify the contribution of using \"sharpness\" as an uncertainty proxy, could the authors compare SHAPO's performance against policy gradient methods that explicitly model actor uncertainty or risk, such as an Ensemble Actor approach or a CVaR-based policy optimization method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ofn7WTHROV", "forum": "7cUxi8LbKD", "replyto": "7cUxi8LbKD", "signatures": ["ICLR.cc/2026/Conference/Submission22794/Reviewer_huer"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22794/Reviewer_huer"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837008767, "cdate": 1761837008767, "tmdate": 1762942390079, "mdate": 1762942390079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores the use of sharpness aware optimization for safe RL. Sharpness aware optimization introduces a max-min problem instead of maximization to flatten the optimization profile. In my understanding this max-min problem would aim to prefer a flatter maximum to a sharp peak maximum in the parameter space. This approach aims to improve robustness of the optimization solution to epistemic uncertainty. The authors conduct a series of experiments that show an improvement of performance of different algorithms with and without SHAPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The idea is novel to my best knowledge and quite interesting. \n2. The paper contains a deep discussion on my sharpness aware optimization is a good fit for ML problems in general. In section 3.3, the authors dive deeply into why SHAPO can be a good fit for RL.\n3. The experiments and ablation study are great"}, "weaknesses": {"value": "I couldn't come up with many weaknesses, but here are a couple \n* The policy update section 3.1 is building the solution bottom-up, but maybe a top-down approach would be slightly easier to read. \n* It would be good to explain how the policy update is actually implemented. The authors present a quadratic optimization problem in  TRPO style update and state that SHAPO update can be applied to any on-policy algorithm. Can the authors elaborate how SHAPO can be applied to a PPO update?"}, "questions": {"value": "* The approach makes total sense and it seems relevant not only for safe RL, but also for RL. What happens if SHAPO is applied without a safety constraint? Did the authors perform this ablation study? \n* Can the authors provide similar Figures to Fig 6 and 7, but in terms of episode return vs episode cost? I think their decision to focus on cost rate is correct, but having a full picture (maybe in appendix) would be good.\n* Can the authors elaborate what’s the base method for Saute RL? As the authors are aware, Saute RL can be applied to any RL algorithm.   \n* Safety gymnasium has many more environments that could be useful for your future research and evaluations. Safety Gymnasium: A Unified Safe Reinforcement Learning Benchmark https://arxiv.org/abs/2310.12567\n* What’s the additional computational burden that the approach adds to the algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Eou93ERA1y", "forum": "7cUxi8LbKD", "replyto": "7cUxi8LbKD", "signatures": ["ICLR.cc/2026/Conference/Submission22794/Reviewer_tvhm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22794/Reviewer_tvhm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968442782, "cdate": 1761968442782, "tmdate": 1762942389851, "mdate": 1762942389851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the application of sharpness awareness minimization (SAM) in policy optimization. SAM aims at optimizing the model such that the worst-case loss in its neighboring parameters is low, encouraging the optimizers to find low-loss solutions that are also flat. In particular, the authors extend Fisher-SAM, a prior SAM method that leverages Fisher information matrix to estimate the local geometry to efficiently estimate the local loss-maximizing parameter, to trust-region policy optimization for safe exploration. The paper provides an interpretation of Fisher-SAM as a pessimistic estimation of the loss subject to the uncertainty quantified by the Fisher matrix. This in turn allows the authors to interpret the worse-case expected return in the neighboring parameters informed by the Fisher matrix as an uncertainty lower-bound on the actual expected return. The paper evaluates the proposed method, SHAPO, on safety gym and shows that the proposed method outperforms prior methods in terms of the return/cost trade-off."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to read and the proposed method is well-motivated and technically sound. \n- The empirical results show statistically significant improvements of the proposed method over prior safe RL methods. Ablations are thorough and show that major components of the algorithms all contribute to the effectiveness of the proposed method.\n- Even though SAM/Fisher-SAM is not new, the application of it in the context of safe RL is new and seems effective."}, "weaknesses": {"value": "- The proposed method requires solving the natural gradient direction that maybe very expensive. \n   - I tried to look for the implementation details in the paper but could not find it. It would be good if the authors could include more implementation details in the paper for better reproducibility. \n   - I checked the linked anonymous codebase briefly and it seems that the implementation involves running an iterative conjugate gradient procedure (https://anonymous.4open.science/r/Safe-Policy-Optimization-813E/safepo/single_agent/shapo.py). This procedure can incur a non-trivial amount of computation overhead. It would be good if the authors could include more details on the run-time of the algorithm compared to prior methods in the paper. \n\n- Hyperparameter sensitivity analysis is missing from the empirical evaluations. In Appendix, the authors mentioned that \"Most common choice of SHAPO hyperparameters that gave the best performance across tasks and baselines was $\\delta_{\\mathrm{Down}} = 0.0001$ and $\\rho_{\\mathrm{critic}} = 0.01$\", but it is unclear how different hyperparameters influence the performance of the algorithm. Including some analysis on the sensitivity of the performance with respect to these hyperparameters (especially $\\delta_{\\mathrm{Down}}$ could help gain a better understanding of the robustness and effectiveness of the algorithm."}, "questions": {"value": "- In Figure 3, 6 and 7, how are the shapes of the circles around the mean/average performance determined?\n- In algorithm 2, how is $U_\\theta$ computed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IjWcbgwbwt", "forum": "7cUxi8LbKD", "replyto": "7cUxi8LbKD", "signatures": ["ICLR.cc/2026/Conference/Submission22794/Reviewer_oXHA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22794/Reviewer_oXHA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762455278462, "cdate": 1762455278462, "tmdate": 1762942389612, "mdate": 1762942389612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Sharpness-Aware Policy Optimization (SHAPO) to address the challenge of safe exploration in safety-critical RL. The core idea is to use the actor's sharpness as a practical proxy for epistemic uncertainty. The method implements a pessimistic policy update. In each step, it first solves an inner-loop optimization to find the \"worst-case\" perturbed parameters ($\\theta_0 + \\epsilon_{Down}$) that minimize the policy objective within a trust region which is defined by the Fisher Information Matrix (KL divergence). It then computes the policy gradient ($\\tilde{g}$) at this worst-case point and uses this pessimistic gradient for the final policy update .\nAnalytically, the authors show this process implicitly reweighs gradients, amplifying the effect of rare, unsafe actions and tempering the effect of rare, safe actions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles the critical challenge of safe exploration in a practical and useful manner, which is a prerequisite for real-world RL deployment.\n\n- The analytical insights, including Proposition 3 on gradient adjustments for rare actions and the reinterpretation of perturbations as pessimistic quantiles (Proposition 2), are clear and well-supported.\n\n- Evaluations across multiple baselines and environments demonstrate consistent gains, with Pareto improvements and reduced catastrophic failures"}, "weaknesses": {"value": "-  Major Theory-Implementation Mismatch\n\nThe paper's theoretical justification for being \"uncertainty-aware\" (presented in L249-256)  hinges on the idea that the inner trust region $\\delta_{Down}$ should adapt based on the amount of data $n$. However, the actual implementation described in Appendix D uses a fixed, grid-searched $\\delta_{Down}$.\n\n- Gap in Intuitive Justification\n\nThe paper argues its strength comes from using the Fisher metric . However, the core intuition for why it is safe (Section 3.3)  is derived using a simplified Euclidean metric (Appendix C, L778: \"For simplicity, we consider here a Euclidean perturbation...\")."}, "questions": {"value": "- Could you clarify whether the adaptive scaling rule was implemented or tested in any form?\nIf not, how do you reconcile this discrepancy between the theoretical motivation and the actual implementation?\n- Could you prove that the core safety intuition from Figure 2  holds for the Fisher-based update, not just the simplified Euclidean case  shown in the appendix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3EIEpRPd7I", "forum": "7cUxi8LbKD", "replyto": "7cUxi8LbKD", "signatures": ["ICLR.cc/2026/Conference/Submission22794/Reviewer_2kQM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22794/Reviewer_2kQM"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission22794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762509853763, "cdate": 1762509853763, "tmdate": 1762942389392, "mdate": 1762942389392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}