{"id": "6qyRiyI5Ky", "number": 2763, "cdate": 1757241866915, "mdate": 1759898128495, "content": {"title": "CoLaP: Contrastive Learning with Adaptive Prompts for Continual Learning", "abstract": "Continual learning (CL) aims to enable models to learn a sequence of new tasks without forgetting previously acquired knowledge. Prompt-based approaches, which adapt small prompt parameters while keeping a large pre-trained backbone frozen, have become a popular strategy to reduce forgetting. However, most existing methods rely solely on visual encoders to effectively guide prompt selection, which leaves them vulnerable to distribution shifts, because biased visual representations can misidentify prompts and lead to severe forgetting. We propose CoLaP, a language-guided prompt selection framework that leverages multimodal models to address this limitation. During training, each input is converted into a rich textual description that provides semantic guidance for training the visual prompt selector. The prompt pool is constructed from clustered concepts that are unique to each dataset, reflecting its specific distribution. In inference, the learned visual selector operates purely on images, preserving efficiency while maintaining the balance between plasticity and stability. Extensive experiments on both in-distribution and out-of-distribution benchmarks show that purely visual prompt methods degrade as the number of tasks grows, whereas our language-informed approach achieves superior generalization and robustness. These results highlight the promise of multimodal semantic guidance for scalable and resilient continual learning.", "tldr": "We introduce CoLaP, a language-guided prompt selection framework that leverages multimodal LLMs to improve generalization and robustness in continual learning by mitigating reliance on biased visual encoders.", "keywords": ["Continual Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c7990390373b7a991e528d4bbc5a884f610c634.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CoLaP, a multimodal framework for continual learning that introduces language-guided prompt selection.  By using textual descriptions from an instruction-tuned vision-language model to form semantically clustered prompt pools and to train a language-aligned visual selector via contrastive learning and distillation. During inference, the model operates purely in the visual domain. Experiments on multiple in-domain and OOD benchmarks super L2P and DualPrompt."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses the problem of out-of-distribution generalization in continual learning from a novel and interesting perspective.\n2. The method of leveraging language to guide a visual prompt selector is intuitive and presents a promising research direction for improving robustness."}, "weaknesses": {"value": "1. The method is only compared against two baselines from 2022 (L2P, DualPrompt), while omitting more recent and relevant prompt-based methods mentioned in the related work such as CODA-Prompt, ProgPrompt,  (which are also mentioned in the related works) and DIKI [1]. Comparisons against other families of CL methods, such as regularization-based or LoRA-based approaches such as SD-LoRA[2], are also missing.\n2.  While hyperparameters are ablated, there is no ablation analysis of the impact of the KL-divergence loss or the teacher-student distillation framework.\n3. The term \"Adaptive\" in the title is not well-justified or explained in the paper. The abstract and methodology instead focus on \"language-guided prompt selection,\" creating an inconsistency in the paper's framing.\n4. The proposed pipeline introduces computational overhead and external dependencies. It requires a large VLM to generate descriptions, another model for text embeddings, and a clustering step. This complexity and the unanalyzed robustness of the generated descriptions may limit the method's practical application.\n5. The lack of source code and the generated textual description files raises concerns about the work's reproducibility.\n\n[1] Tang L, Tian Z, Li K, et al. Mind the interference: Retaining pre-trained knowledge in parameter efficient continual learning of vision-language models[C]//European conference on computer vision. \n\n[2] Wu Y, Piao H, Huang L K, et al. SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning[C]//The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "1. How do the inference-time FLOPS or latency of CoLaP compare to the baseline methods?\n2. Is there a risk of semantic leakage if the generated textual descriptions are overly correlated with the class names, and how is this possibility addressed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rJmVFUZQeA", "forum": "6qyRiyI5Ky", "replyto": "6qyRiyI5Ky", "signatures": ["ICLR.cc/2026/Conference/Submission2763/Reviewer_gx7J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2763/Reviewer_gx7J"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761130178241, "cdate": 1761130178241, "tmdate": 1762916364378, "mdate": 1762916364378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CoLaP, a language-guided prompt selection framework for continual learning that enhances robustness against distribution shifts. Unlike prior methods that rely solely on visual encoders, CoLaP incorporates textual descriptions during training to provide semantic guidance for prompt selection, enabling more reliable and adaptive representations. A concept-clustered prompt pool captures dataset-specific distributions, while inference remains purely visual to ensure efficiency. By leveraging the knowledge space of language models, CoLaP facilitates better alignment between current and future tasks, improving knowledge transfer and reducing forgetting. Extensive experiments on in-distribution and out-of-distribution benchmarks demonstrate that CoLaP significantly outperforms several purely visual prompt methods in both generalization and scalability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a language-guided prompt selection framework that integrates multimodal semantic guidance to address distribution shifts, improving robustness and generalization in continual learning.\n\n2. It provides extensive experimental validation on both in-distribution and out-of-distribution benchmarks to support its claims of improved performance and scalability."}, "weaknesses": {"value": "1. The sentence “CoLaP is the first approach to integrate textual representations into the prompt selection stage for CL method that operates over the visual domain.” appears to overstate the novelty of the contribution, as prior work such as LGCL (Khan et al., 2023) also leverages textual embeddings for prompt selection. Although the authors already discuss LGCL in the related work, this claim in the contribution section should be rephrased to more accurately reflect the distinction between CoLaP and existing methods.\n\n2. The review of traditional continual learning methods is not comprehensive. In addition to regularization- and memory-based methods, architecture-based approaches [a-f] should also be discussed to better contextualize the contribution.\n[a] Lifelong learning with dynamically expandable networks, ICLR18\n[b] Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. ICML19\n[c] Beef: Bi-compatible class-incremental learning via energy-based expansion and fusion. ICLR23\n[d] Overcoming catastrophic forgetting with hard attention to the task. ICML18\n[e] Compacting, picking and growing for unforgetting continual learning. NeurIPS19\n[f] Parameter-level soft-masking for continual learning. ICML23\n\n3. The notions in Figure 2 should be clarified with a legend or more detailed caption. And the prompt pool appears missing and is suggested to be explicitly annotated in the figure.\n\n4. The definition of “key” is confusing. The authors state that keys are integer labels but also mention clustering centroids as keys. If the intent is that the prompt index is an integer while the key embedding is a fixed (non-learnable) vector, this should be clearly and consistently described.\n\n5. The current method section only introduces the loss for the selector, but does not describe the overall training objective, including how prompts are trained. A more complete description would improve clarity and reproducibility.\n\n6. The statement “The prompt selector, which is composed of the projection head f_\\beta and student network f_\\delta^s, …” is in consistent with the earlier definition f_s= f_\\delta^s \\odot f_\\beta\\odot f_\\omega. This discrepancy should be resolved for consistency.\n\n7. The experimental comparison is limited to L2P and DualPrompt. It should also include recent prompt-based methods such as HiDe-Prompt, S-Prompt, CODA-Prompt, ProgPrompt, LGCL, VQ-Prompt, Cprompt, etc. Moreover, the BWT metric underperforms in most cases, which weakens the claim of reduced forgetting and warrants further analysis or discussion.\n\n8. The number of cluster centroids Q is fixed across datasets and tasks, which may limit adaptability. An adaptive strategy could potentially improve performance.\n\n9. Some minors.\n1) The font size in Figure 2 is too small and could be increased for readability.\n2) Equations are suggested to be numbered for easier reference."}, "questions": {"value": "1. Could the authors rephrase the statement of being the first approach to integrate textual representations into the prompt selection stage to more accurately reflect the nature of their contribution?\n\n2. Could the authors include a more comprehensive discussion of architecture-based methods and other relevant prompt-based approaches to better position CoLaP in the broader CL landscape?\n\n3. Could the authors provide a more complete methodological description that include all loss terms and their interactions to enhance clarity and reproducibility?\n\n4. Could the authors further clarify the keys, specifically whether they are integer indices, fixed embeddings, or learnable representations?\n\n5. Could they reconcile the notation of the selector components for internal consistency for internal consistency across the text?\n\n6. Could the authors report or discuss the performance of CoLaP compared with more recent prompt-based methods such as HiDe-Prompt, S-Prompt, CODA-Prompt, ProgPrompt, and LGCL, which are not currently included as baselines?\n\n7. Since the reported BWT metric underperforms in several cases, could the authors provide additional analysis or justification to support their claim of reduced forgetting, or explain why this metric may not fully capture the benefits of CoLaP?\n\n8. Did the authors explore or consider adaptive or dataset-dependent strategies for determining the number of cluster centroids Q, and if not, could they discuss why a fixed value was chosen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FaFGrRsf2J", "forum": "6qyRiyI5Ky", "replyto": "6qyRiyI5Ky", "signatures": ["ICLR.cc/2026/Conference/Submission2763/Reviewer_jX4A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2763/Reviewer_jX4A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761190002924, "cdate": 1761190002924, "tmdate": 1762916364245, "mdate": 1762916364245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CoLaP argues that multimodal LLMs, having been trained on a large number of concepts, are better suited to encode task information compared to visual-only models that rely solely on visual features for prompt selection. The limited knowledge of visual models can significantly harm accuracy, especially on out-of-domain datasets. CoLaP introduces a novel contrastive learning based method that allows the use of language models by feeding images into a vision–language model to generate text embeddings, then finding cluster centers for these embeddings, which serve as the keys for each task. A classifier generates a categorical distribution over these keys to select the appropriate prompt. The framework further includes a teacher–student network, where the student distribution is trained to match the teachers'. At inference time, the student network selects the top-K prompts."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- CoLaP introduces a novel perspective by using multimodal LLM embeddings to train a prompt selector. Unlike ViTs, which are trained on a much smaller set of visual concepts, multimodal LLMs are exposed to a vast range of concepts, reducing semantic misalignments.\n\n- CoLaP proposes a novel framework that leverages multimodal LLM embeddings for prompt selection in downstream tasks.\n\n- CoLaP outperforms vision-based prompt selection methods such as L2P and DualPrompt, particularly on out-of-distribution tasks."}, "weaknesses": {"value": "- Comparison is incomplete. Many later works that can potentially outperform are not compared.\n1. RanPAC: Random Projections and Pre-trained Models for Continual Learning\n2. Dynamic Integration of Task-Specific Adapters for Class Incremental Learning\n3. Adapter Merging with Centroid Prototype Mapping for Scalable Class-Incremental Learning\nand many more. \nEven within prompt-based methods, authors have not compared with later works such as CODA-prompt (CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning).\n\n- It is also unclear how exactly the prompts are trained. The paper talks about 'The global prompt pool is updated by simply adding these centroids and their associated set of learnable prompt values,' but does not mention how these 'learnable prompt values are really learnt? Do we train them separately after training the prompt selector? Do we train them in parallel? Do you generate t' using the learnable prompts? Do you simply select the correct task prompts during training and train them using cross-entropy loss? It is unclear\n\n- The main figure is also vague. A key representing meaning of elements (arrows, colors) in the figure would be a nice addition.\n\n- What is the teacher network? Is this an MLP? Is this also trained?\n\n- Table 2 only compares COLLAP results and does not include other methods. \n\n- Reporting average performance would be nice. \n\n- No analysis of generated captions is shown. They could contain errors as well. \n\n- Per task, backward transfer curves and accuracy curves would be nice to look at. \n\n- What is in Table 5? Is this the projector that generates t'\n\n- The paper needs a lot more polishing. Important details are missing. The figure needs to be improved, and more comparisons are needed as well."}, "questions": {"value": "-How are you training the prompt values? Do you also train the old prompts when a new task arrives since the prompt selector outputs distribution over the whole pool at task i? Or do you keep old prompts frozen when a new task arrives? Please be detailed. \n\n- What is in Table 5? \n\n- Why aren't you comparing with the latest methods? How does your method compare with the latest methods? \n\n- Some detailed ablations on design choices of networks used (MLPs,LLaVA and text embedder) will be helpful. Add some analysis of captions and their effect on performance. \n\n- Accuracy curves are also required to show the average accuracy per task during training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wON6PNTl0i", "forum": "6qyRiyI5Ky", "replyto": "6qyRiyI5Ky", "signatures": ["ICLR.cc/2026/Conference/Submission2763/Reviewer_DSrc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2763/Reviewer_DSrc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956401025, "cdate": 1761956401025, "tmdate": 1762916364067, "mdate": 1762916364067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CoLaP (Contrastive Learning with Adaptive Prompts), a multimodal continual learning framework that integrates language-guided prompt selection to mitigate catastrophic forgetting. Unlike prior visual-only prompt methods, CoLaP leverages auto-generated textual descriptions to cluster semantically aligned prompts, training a visual selector via contrastive and distillation losses. At inference, it operates purely on visual data, maintaining efficiency. Extensive experiments on in-domain and out-of-domain benchmarks (e.g., TinyImageNet, ImageNet-O) show CoLaP achieves superior generalization and stability, outperforming L2P and DualPrompt. The method effectively balances plasticity and stability, highlighting language-informed prompting as a promising direction for robust continual learning"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Paper uses language-guided prompt selection that’s trained once, used visually at test time. CoLaP aligns a visual selector to language embeddings via contrastive + distillation losses, then drops text at inference preserving efficiency while improving selection robustness.\n\n2. It clusters auto-generated captions to form prompt keys, encouraging concept sharing across related classes and reducing interference.\n\n3. OOD robustness and Consistent gains on ImageNet-O, Oxford-IIIT-Pet, etc., showing >5% improvements over L2P in key OOD settings.\n\n4. Maintains stability as tasks increase (5→20) with sensible top-K prompt retrieval."}, "weaknesses": {"value": "1. While the specific combination of contrastive alignment + discrete prompt keys is new, its conceptual ingredients language guidance, multimodal contrastive learning, and prompt tuning are well explored (e.g., LGCL ICCV 2023, Roy CVPR 2024, Progressive Prompt ICLR 2023)\n\n2. Limited baselines: The paper claims SOTA but omits contemporary multimodal CL baselines (Roy 2024; LGCL 2023; PromptAlign NeurIPS 2024). Without these, the improvement claims can’t be trusted across modalities\n\n3. The model highly dependent on the text caption, yet there’s no sensitivity or noise ablation. If captions are incorrect or generic, how robust is the contrastive alignment? This is a critical missing analysis since the approach’s strength hinges on textual fidelity.\n\n4. The alignment network will get trained on the task specific data, training one after another task this network itself will suffer from forgetting how paper handle the same?\n\n5. While discrete prompt indices reduce memory, they remove continuous similarity structure. This may harm fine-grained transfer or incremental compositional reasoning, provide the ablation on the same."}, "questions": {"value": "Compare with the recent SOTA model, provide the answer discussed the point in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZXktElC3Gn", "forum": "6qyRiyI5Ky", "replyto": "6qyRiyI5Ky", "signatures": ["ICLR.cc/2026/Conference/Submission2763/Reviewer_EX8F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2763/Reviewer_EX8F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031411102, "cdate": 1762031411102, "tmdate": 1762916363863, "mdate": 1762916363863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}