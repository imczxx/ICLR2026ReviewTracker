{"id": "y671b8n9FS", "number": 17089, "cdate": 1758272020133, "mdate": 1759897198992, "content": {"title": "IDetail: Fine-grained Identity Preservation in Prompt-based Image Relighting", "abstract": "Diffusion-based methods are widely used for image-to-image translation tasks such as object addition/removal, colorization, and prompt-based editing. In personalized editing applications, accurately preserving a person’s identity is critical to maintain subject-specific attributes. Existing methods either use adapter networks, which struggle to retain the facial details, structure & pose of the subject, or rely on full fine-tuning of large foundation models, which is computationally expensive and requires large high-quality annotated datasets. To overcome these limitations, we propose a novel unsupervised dataset preparation pipeline that enables scalable dataset generation and a novel identity-preserving loss function that ensures fine-grained identity preservation in the generated images. Despite using a significantly lighter foundation model and fine-tuning only a fraction of its weights, our method achieves performance comparable to state-of-the-art methods. Furthermore, it has robust generalization to out-of-training prompts and generalizes to multi-person images despite training only on single-person images.", "tldr": "New method for identity preserving prompt-based image relighting that automatically generates pairwise images in an unsupervised manner and train just a LoRA adapter using specific losses to achieve fine-grained identity preservation.", "keywords": ["Identity preservation", "prompt-based image relighting", "Identity loss", "Diffusion models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f804ef9b8a1e4f87c595b0fddbbb50a1e758bcc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work addresses the prompt-based image relighting problem in the field of personalized editing by proposing a data generation pipeline based on InstantID, enabling scalable dataset generation. It introduces a two-stage LoRA training approach and diverse training losses to ensure fine-grained identity preservation in the generated images, achieving competitive results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach uses a smaller base model and trains fewer parameters, yet achieves competitive results compared to Flux-based methods.\n- It introduces a pipeline for automatic data generation, eliminating the need for manual data collection and annotation."}, "weaknesses": {"value": "- The absence of ablation experiments makes it difficult to assess whether each module's design is essential, which is a critical flaw.\n- Is the training setup for the first stage reasonable? It’s evident that the first stage training forces the model to ignore the newly added five input channels as much as possible, which doesn't provide a good initialization for the second stage.\n- The design of multiple losses intuitively seems arbitrary and lacks clear justification.\n- The method is not particularly novel, from data collection to model architecture design.\n- Since the datasets are entirely generated by InstantID, they inevitably inherit any biases present in InstantID. As a result, models trained on these datasets will also inherit such biases. For instance, if InstantID performs poorly with certain prompts, this method will also struggle in those cases.\n- The paper is not well-written, with many awkward sentences, and some words in Figure 2 and Figure 4 still have red error underlines beneath them."}, "questions": {"value": "- Is the ability to generalize to multiple people a unique advantage of your method, or can other methods also achieve this?\n- Will the results still be good when the character is farther from the camera?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VeUTmXyULd", "forum": "y671b8n9FS", "replyto": "y671b8n9FS", "signatures": ["ICLR.cc/2026/Conference/Submission17089/Reviewer_Dfyp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17089/Reviewer_Dfyp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462290204, "cdate": 1761462290204, "tmdate": 1762927093572, "mdate": 1762927093572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles prompt-based image relighting for human subjects, with a strong focus on preserving fine-grained identity under dramatic lighting edits.\nThe authors argue that existing approaches fail to retain the facial details, structure & pose of the subject and are computationally expensive to finetune the large models.\n\nTo address the problem of prompt-based person image relighting, this paper presents the following contributions:\n\n* An unsupervised data generation strategy to produce the training (input image, prompts, relighted image) pairs.\n* A two stage training procedure to adapt text-to-image diffustion model for image-to-image mapping.\n* A combination of diffsuion loss with face identity loss, foreground/background content loss to finetune the diffusion loss.\n\nExperimental results compared with personalized image generation method InstantID and prompt-based image relighting method IC-Light reflect the proposed model achieves a worse performance with a significantly smaller backbone."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well structured.\n2. The proposed pipeline is more efficient than SOTA prompt-based image relighting method."}, "weaknesses": {"value": "1. The rationality of using InstantID to generate relighted images is not justified. The relighted images in the paper looks bad.\n2. The base diffusion model Koala from Neurips 2024 used in this paper is too old. There exists some stronger image-to-image generative models like Flux.1 Kontext. Why using such a weak model is not justified.\n3. There is no ablation study in the paper.\n4. The evaluation looks not rational in the paper. There are no any metrics related to relighting quality.\n5. The qualitative results look significantly worse than the IC-Light. It doesn't make sense to call it comparable.\n6. The face identity loss is computed in the latent space of VAE, which is different from the common practice of computing loss in the feature space of face recognition network. There is no justification for this design."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rmsphYeTS2", "forum": "y671b8n9FS", "replyto": "y671b8n9FS", "signatures": ["ICLR.cc/2026/Conference/Submission17089/Reviewer_87ka"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17089/Reviewer_87ka"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817099523, "cdate": 1761817099523, "tmdate": 1762927093212, "mdate": 1762927093212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This system introduce prompt-based image relighting framework upon denoising diffusion models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Prompt-based approach: For relighting, this system only uses prompt and input image, providing flexible and easy controls.\n- Lighting weight manner: In Tab.1, they reported the system has significantly less parameters and relatively fast inference time."}, "weaknesses": {"value": "- Lack of fidelity: There is no explicit relighting-specialized module, it is thereby observed the proposed system has fidelity concerns. Specifically, even though this task aims to only change the lighting-relevant factors, the resulting outcomes showed changed background appearance and color hair. This tendency dilute the goal of relighting, it rather seems like image editing task with high-level (fine) color changes.\n- Lack of quantitative comparison: Experiments report the slight improvement in clothing score, and lower CLIP and ID score. There is no relighting measurement metric. It is strongly recommended to provide more comprehensive comparison to validate the proposed system in terms of relighting.\n- Combination of existing module: The system consists of existing approaches including LoRA and loss functions with no novel learning algorithm."}, "questions": {"value": "- It is strongly recommended to provide more comprehensive experiments to validate the capabilities of the proposed system.\n- The authors should discuss why the results showed background changes despite the relighting task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T8fUxOkFnx", "forum": "y671b8n9FS", "replyto": "y671b8n9FS", "signatures": ["ICLR.cc/2026/Conference/Submission17089/Reviewer_RmV4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17089/Reviewer_RmV4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902535157, "cdate": 1761902535157, "tmdate": 1762927092114, "mdate": 1762927092114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge image editing: preserving fine-grained identity details during prompt-based relighting. The proposed  **IDETAIL** is a prompt-based relighting method that preserves identity through a novel dataset generation pipeline and specialized loss functions, using lightweight LoRA adapters instead of full model fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The unsupervised approach generating 430K pairs from limited data addresses the scarcity of paired relighting data. \n2. Achieving competitive results while training only LoRA adapters (43M parameters) compared to full model fine-tuning approaches."}, "weaknesses": {"value": "1. The authors explicitly state that InstantID generates images with \"smoothed facial details, altered jawlines and face structures, and changes in pose, clothing and orientation\" (Section 3, Fig. 3a). Yet, they use these imperfect InstantID outputs as training targets. How can the model learn \"fine-grained identity preservation\" from data that doesn't preserve identity accurately? The entire training paradigm relies on the assumption that learning mappings between InstantID outputs will somehow yield better identity preservation than InstantID itself. This circular logic isn't sufficiently justified.\n2. The SOTA methods move towards 3D understanding for relighting, such as incorporating monocular depth estimation, surface normal prediction, to understand how light interacts with surfaces in a physically plausible way. This approach is fundamentally 2D. It relies on the dataset pipeline and loss functions to implicitly learn lighting effects. The foreground segmentation mask provides a binary separation but no geometric information. This limits the model's ability to reason about physics. The \"dramatic shadows\" it generates, while visually appealing, may lack geometric consistency."}, "questions": {"value": "1. How do you reconcile using identity-degraded InstantID outputs as training targets with your goal of fine-grained identity preservation? What specific aspects of your method overcome the limitations of the training data? \n2. The experiment with the placeholder token \"a <c>\" (Fig. 6a/10) shows impressive identity preservation. However, this could be explained by the LoRA adapter learning to ignore an uninformative prompt and simply reconstruct the input latent. Can you provide further analysis (e.g., attention visualizations, feature space analysis) to prove the token is actively encoding identity features rather than just triggering a reconstruction mode?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XgJ1oule0G", "forum": "y671b8n9FS", "replyto": "y671b8n9FS", "signatures": ["ICLR.cc/2026/Conference/Submission17089/Reviewer_qwnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17089/Reviewer_qwnC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928416828, "cdate": 1761928416828, "tmdate": 1762927091302, "mdate": 1762927091302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}