{"id": "y671b8n9FS", "number": 17089, "cdate": 1758272020133, "mdate": 1759897198992, "content": {"title": "IDetail: Fine-grained Identity Preservation in Prompt-based Image Relighting", "abstract": "Diffusion-based methods are widely used for image-to-image translation tasks such as object addition/removal, colorization, and prompt-based editing. In personalized editing applications, accurately preserving a person’s identity is critical to maintain subject-specific attributes. Existing methods either use adapter networks, which struggle to retain the facial details, structure & pose of the subject, or rely on full fine-tuning of large foundation models, which is computationally expensive and requires large high-quality annotated datasets. To overcome these limitations, we propose a novel unsupervised dataset preparation pipeline that enables scalable dataset generation and a novel identity-preserving loss function that ensures fine-grained identity preservation in the generated images. Despite using a significantly lighter foundation model and fine-tuning only a fraction of its weights, our method achieves performance comparable to state-of-the-art methods. Furthermore, it has robust generalization to out-of-training prompts and generalizes to multi-person images despite training only on single-person images.", "tldr": "New method for identity preserving prompt-based image relighting that automatically generates pairwise images in an unsupervised manner and train just a LoRA adapter using specific losses to achieve fine-grained identity preservation.", "keywords": ["Identity preservation", "prompt-based image relighting", "Identity loss", "Diffusion models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f804ef9b8a1e4f87c595b0fddbbb50a1e758bcc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work addresses the prompt-based image relighting problem in the field of personalized editing by proposing a data generation pipeline based on InstantID, enabling scalable dataset generation. It introduces a two-stage LoRA training approach and diverse training losses to ensure fine-grained identity preservation in the generated images, achieving competitive results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach uses a smaller base model and trains fewer parameters, yet achieves competitive results compared to Flux-based methods.\n- It introduces a pipeline for automatic data generation, eliminating the need for manual data collection and annotation."}, "weaknesses": {"value": "- The absence of ablation experiments makes it difficult to assess whether each module's design is essential, which is a critical flaw.\n- Is the training setup for the first stage reasonable? It’s evident that the first stage training forces the model to ignore the newly added five input channels as much as possible, which doesn't provide a good initialization for the second stage.\n- The design of multiple losses intuitively seems arbitrary and lacks clear justification.\n- The method is not particularly novel, from data collection to model architecture design.\n- Since the datasets are entirely generated by InstantID, they inevitably inherit any biases present in InstantID. As a result, models trained on these datasets will also inherit such biases. For instance, if InstantID performs poorly with certain prompts, this method will also struggle in those cases.\n- The paper is not well-written, with many awkward sentences, and some words in Figure 2 and Figure 4 still have red error underlines beneath them."}, "questions": {"value": "- Is the ability to generalize to multiple people a unique advantage of your method, or can other methods also achieve this?\n- Will the results still be good when the character is farther from the camera?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VeUTmXyULd", "forum": "y671b8n9FS", "replyto": "y671b8n9FS", "signatures": ["ICLR.cc/2026/Conference/Submission17089/Reviewer_Dfyp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17089/Reviewer_Dfyp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462290204, "cdate": 1761462290204, "tmdate": 1762927093572, "mdate": 1762927093572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Clarifications and explanations for the reviewer comments"}, "comment": {"value": "We thank the reviewers for their feedback. Several points are already addressed in the paper. We give additional justification below.\n\n1. Clarification on problem definition:\n\tOur task is prompt-based image relighting with two scenarios: \n\t1. Environmental relighting: the subject is transported to a new scene defined by the prompt, and lighting from that scene is realistically reflected on the subject. The generated image looks like the subject was photographed in the scene. \n\t2. Studio photography effects: prompts such as \"pantone\" or \"window grill\" simulate controlled studio captures where the backgrounds are simple to enhance the contrast with shadows and lighting. \n\tBoth scenarios require relighting and background change.\n\n2. Motivation for using InstantID training pairs:\n\tWe require image pairs with: 1) precise identity preservation and 2) diverse background/lighting. Since no public dataset provides this, we generated training pairs using InstantID.\nWhile the identity in InstantID generated images is inaccurate w.r.t original input image, the identity is consistent across different prompts. So we train the model on InstantID -> InstantID pairs.\n\tOur identity loss (Eq 2), computed between the predicted and input image latents (Section 4.4), corrects InstantID’s identity drift. This preserves identity at inference (Fig 5,7).\n\n3. Two-stage training pipeline:\nThe base UNet expects 4-channel conv_in, but we use a 9-channel conv_in. Training the conv_in and LoRA simultaneously causes degradation in image quality. The two-stage pipeline, first adapts the conv_in to 9 channels while keeping generation quality (stage 1) and then trains only the LoRA for identity preservation (stage 2). \n\n4. Loss design and latent space computation:\n\tEach loss targets a specific requirement:\n\t1. ID loss (Eq 2): identity preservation \n\t2. FG loss (Eq 3): preserves sharp lighting and shadows \n\t3. BG loss (Eq 4): backgrounds adhere to prompts\n\t4. Noise loss (Eq 5): for diffusion training\n\n\tAblations in Table 4 (Appendix) show all losses are required for best performance. Losses are computed in the VAE latent space because it is semantically more meaningful and avoids decoder reconstruction artifacts (Section 4.4).\n\n5. Ablations and model capabilities:\n\tTables 2-4 (Appendix) show ablations on training dataset size, LoRA rank and loss component importance. \nFig 9 shows identity degradation when training on original input -> InstantID pairs, rather than InstantID -> InstantID pairs. Fig 5-10 shows generalization to multi-person images despite training only on single-person images, generalization to out-of-training prompts, importance of placeholder prompt \"a <c>\" for identity preservation. \n\n6. Non-suitability of traditional relighting metrics:\n\tRelighting metrics such as PSNR or SSIM are computed at pixel-level and assume a single ground truth output, which is not applicable to our problem. Multiple valid outputs may exist for the same prompt (e.g., window grill shadow may vary in direction and pattern). \n\tWe use metrics from generative image editing to evaluate two aspects: 1) identity preservation and 2) prompt adherence. Both face and clothing scores are computed to reduce bias. Prompt adherence is measured via CLIP score.\n\n7. Not using more powerful foundation models:\n\tWe use a lightweight foundation model so the benefit of our design choices are clearly attributable, which would be difficult with a larger backbone.\n\tOur model has 1.2B params with 43M trained. IC-Light has 12B params with 11.4B trained. Despite being 12x smaller and training 265x fewer parameters than IC-Light, we achieve comparable identity preservation and more dramatic lighting and shadows (Fig 5,7). This shows that improvements come from our design choices.\n\n8. Qualitative comparison with IC-Light:\n\tImage resolution reduces the perceived quality of the results. Higher resolution comparisons are shown in Appendix. Our model preserves identity similar to IC-Light while generating more dramatic lighting and shadows.\n\n9. Why 3D maps are not used:\n\tAlthough no explicit 3D geometry is used, the model generates physically plausible lighting (e.g., shadows bend along the face in window grill effect in Fig 5, 7). This is due to the strong priors learnt by pretrained foundation models. Using depth/normal maps would increase inference time with little benefit.\n\n10. Additional clarifications: 1) InstantID dataset may transfer to our model, but results in Table 1 and Fig 5, 7 show clear improvement over InstantID due to our design choices. 2) Our model works on small faces (Fig 6b). There is no inherent limitation.\n\nSeveral concerns and weaknesses were already addressed in the paper. We provide additional justifications above. We request the reviewers to reconsider their scores. \n\nIn the rebuttal revision, we will include: 1) results with SDXL foundation model, 2) more details on two-stage training, and 3) visualizations about the identity learnt by placeholder prompt."}}, "id": "buwOa6Lfp3", "forum": "y671b8n9FS", "replyto": "y671b8n9FS", "signatures": ["ICLR.cc/2026/Conference/Submission17089/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17089/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17089/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763459378680, "cdate": 1763459378680, "tmdate": 1763459378680, "mdate": 1763459378680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles prompt-based image relighting for human subjects, with a strong focus on preserving fine-grained identity under dramatic lighting edits.\nThe authors argue that existing approaches fail to retain the facial details, structure & pose of the subject and are computationally expensive to finetune the large models.\n\nTo address the problem of prompt-based person image relighting, this paper presents the following contributions:\n\n* An unsupervised data generation strategy to produce the training (input image, prompts, relighted image) pairs.\n* A two stage training procedure to adapt text-to-image diffustion model for image-to-image mapping.\n* A combination of diffsuion loss with face identity loss, foreground/background content loss to finetune the diffusion loss.\n\nExperimental results compared with personalized image generation method InstantID and prompt-based image relighting method IC-Light reflect the proposed model achieves a worse performance with a significantly smaller backbone."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well structured.\n2. The proposed pipeline is more efficient than SOTA prompt-based image relighting method."}, "weaknesses": {"value": "1. The rationality of using InstantID to generate relighted images is not justified. The relighted images in the paper looks bad.\n2. The base diffusion model Koala from Neurips 2024 used in this paper is too old. There exists some stronger image-to-image generative models like Flux.1 Kontext. Why using such a weak model is not justified.\n3. There is no ablation study in the paper.\n4. The evaluation looks not rational in the paper. There are no any metrics related to relighting quality.\n5. The qualitative results look significantly worse than the IC-Light. It doesn't make sense to call it comparable.\n6. The face identity loss is computed in the latent space of VAE, which is different from the common practice of computing loss in the feature space of face recognition network. There is no justification for this design."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rmsphYeTS2", "forum": "y671b8n9FS", "replyto": "y671b8n9FS", "signatures": ["ICLR.cc/2026/Conference/Submission17089/Reviewer_87ka"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17089/Reviewer_87ka"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817099523, "cdate": 1761817099523, "tmdate": 1762927093212, "mdate": 1762927093212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This system introduce prompt-based image relighting framework upon denoising diffusion models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Prompt-based approach: For relighting, this system only uses prompt and input image, providing flexible and easy controls.\n- Lighting weight manner: In Tab.1, they reported the system has significantly less parameters and relatively fast inference time."}, "weaknesses": {"value": "- Lack of fidelity: There is no explicit relighting-specialized module, it is thereby observed the proposed system has fidelity concerns. Specifically, even though this task aims to only change the lighting-relevant factors, the resulting outcomes showed changed background appearance and color hair. This tendency dilute the goal of relighting, it rather seems like image editing task with high-level (fine) color changes.\n- Lack of quantitative comparison: Experiments report the slight improvement in clothing score, and lower CLIP and ID score. There is no relighting measurement metric. It is strongly recommended to provide more comprehensive comparison to validate the proposed system in terms of relighting.\n- Combination of existing module: The system consists of existing approaches including LoRA and loss functions with no novel learning algorithm."}, "questions": {"value": "- It is strongly recommended to provide more comprehensive experiments to validate the capabilities of the proposed system.\n- The authors should discuss why the results showed background changes despite the relighting task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T8fUxOkFnx", "forum": "y671b8n9FS", "replyto": "y671b8n9FS", "signatures": ["ICLR.cc/2026/Conference/Submission17089/Reviewer_RmV4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17089/Reviewer_RmV4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902535157, "cdate": 1761902535157, "tmdate": 1762927092114, "mdate": 1762927092114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge image editing: preserving fine-grained identity details during prompt-based relighting. The proposed  **IDETAIL** is a prompt-based relighting method that preserves identity through a novel dataset generation pipeline and specialized loss functions, using lightweight LoRA adapters instead of full model fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The unsupervised approach generating 430K pairs from limited data addresses the scarcity of paired relighting data. \n2. Achieving competitive results while training only LoRA adapters (43M parameters) compared to full model fine-tuning approaches."}, "weaknesses": {"value": "1. The authors explicitly state that InstantID generates images with \"smoothed facial details, altered jawlines and face structures, and changes in pose, clothing and orientation\" (Section 3, Fig. 3a). Yet, they use these imperfect InstantID outputs as training targets. How can the model learn \"fine-grained identity preservation\" from data that doesn't preserve identity accurately? The entire training paradigm relies on the assumption that learning mappings between InstantID outputs will somehow yield better identity preservation than InstantID itself. This circular logic isn't sufficiently justified.\n2. The SOTA methods move towards 3D understanding for relighting, such as incorporating monocular depth estimation, surface normal prediction, to understand how light interacts with surfaces in a physically plausible way. This approach is fundamentally 2D. It relies on the dataset pipeline and loss functions to implicitly learn lighting effects. The foreground segmentation mask provides a binary separation but no geometric information. This limits the model's ability to reason about physics. The \"dramatic shadows\" it generates, while visually appealing, may lack geometric consistency."}, "questions": {"value": "1. How do you reconcile using identity-degraded InstantID outputs as training targets with your goal of fine-grained identity preservation? What specific aspects of your method overcome the limitations of the training data? \n2. The experiment with the placeholder token \"a <c>\" (Fig. 6a/10) shows impressive identity preservation. However, this could be explained by the LoRA adapter learning to ignore an uninformative prompt and simply reconstruct the input latent. Can you provide further analysis (e.g., attention visualizations, feature space analysis) to prove the token is actively encoding identity features rather than just triggering a reconstruction mode?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XgJ1oule0G", "forum": "y671b8n9FS", "replyto": "y671b8n9FS", "signatures": ["ICLR.cc/2026/Conference/Submission17089/Reviewer_qwnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17089/Reviewer_qwnC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928416828, "cdate": 1761928416828, "tmdate": 1762927091302, "mdate": 1762927091302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}