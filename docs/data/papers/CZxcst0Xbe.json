{"id": "CZxcst0Xbe", "number": 11592, "cdate": 1758202244679, "mdate": 1763741587168, "content": {"title": "Debate with Image: Detecting Deceptive Behaviors in Multimodal Large Language Models", "abstract": "Are frontier AI systems becoming more capable? *Certainly*.\nYet such progress is not an unalloyed blessing but rather a *Trojan horse*: behind their performance leaps lie more insidious and destructive safety risks, namely deception.\nUnlike hallucination, which arises from insufficient capability and leads to mistakes, deception represents a deeper threat in which models deliberately mislead users through complex reasoning and insincere responses. \nAs system capabilities advance, deceptive behaviours have spread from textual to multimodal settings, amplifying their potential harm. \n**First and foremost, how can we monitor these covert multimodal deceptive behaviors?**\nNevertheless, current research remains almost entirely confined to text, leaving the deceptive risks of multimodal large language models unexplored. In this work, we systematically reveal and quantify multimodal deception risks, introducing *MM-DeceptionBench*, the first benchmark explicitly designed to evaluate multimodal deception. Covering six categories of deception, MM-DeceptionBench characterizes how models strategically manipulate and mislead through combined visual and textual modalities. On the other hand, multimodal deception evaluation is almost a blind spot in existing methods.\nIts stealth, compounded by visual–semantic ambiguity and the complexity of cross-modal reasoning, renders action monitoring and chain-of-thought monitoring largely ineffective. To tackle this challenge, we propose *debate with images*, a novel multi-agent debate monitor framework. By compelling models to ground their claims in visual evidence, this method substantially improves the detectability of deceptive strategies. Experiments show that it consistently increases agreement with human judgements across all tested models, boosting Cohen’s kappa by 1.5× and accuracy on GPT-4o by 1.25×.", "tldr": "We detect deceptive behaviors in multimodal AI systems via debate with image.", "keywords": ["AI Safety", "Alignment", "Large Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a75fc20cc7c916cf69dfc9e5494cf07228aab326.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates deceptive behaviors in multimodal large language models (MLLMs), arguing that deception, unlike hallucination, is an emergent, intentional misalignment between visual perception and user-facing responses. To systematically study this phenomenon, the authors introduce MM-DeceptionBench, the first benchmark explicitly designed to evaluate multimodal deception across six categories (sycophancy, sandbagging, bluffing, obfuscation, omission, and fabrication). They further propose Debate with Images, a visually grounded multi-agent debate framework that compels models to cite concrete visual evidence when arguing for or against the presence of deception."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed work introduces multimodal deception as a distinct and important safety challenge, extending beyond standard hallucination evaluation.\n- The proposed debate-with-images method substantially improves alignment between model judgments and human assessments.\n- MM-DeceptionBench systematically covers multiple deception types and provides a scalable, generalizable evaluation setting across top MLLMs."}, "weaknesses": {"value": "- First, the line between deception vs. honest error/hallucination is tricky; categories (e.g., bluffing vs. obfuscation) can overlap, risking label noise unless inter-annotator agreement and adjudication are very tight.\n\n- Using LLMs both to debate and to judge can introduce family/brand bias, order/verbosity bias, and sensitivity to prompt phrasing, unless carefully controlled (e.g., counterbalancing, calibration, and cross-model judging).\n\n- Last, multi-agent debate adds substantial latency and token cost, and outcomes can be brittle to settings like number of rounds/agents, temperature, and prompt templates, raising reproducibility and deployment efficiency concerns."}, "questions": {"value": "Is there any experiment to test what the effect to the results if the round > 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vZlGaOL82j", "forum": "CZxcst0Xbe", "replyto": "CZxcst0Xbe", "signatures": ["ICLR.cc/2026/Conference/Submission11592/Reviewer_FtUu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11592/Reviewer_FtUu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720999859, "cdate": 1761720999859, "tmdate": 1762922674025, "mdate": 1762922674025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses multimodal deception in MLLMs, a phenomenon distinct from hallucination where models deliberately mislead users through strategic manipulation of visual and textual information. The authors introduce MM-DeceptionBench, the first benchmark with 1013 cases spanning six deception categories (sycophancy, sandbagging, bluffing, obfuscation, deliberate omission, and fabrication). To detect such behaviors, they propose \"debate with images,\" a multi-agent debate framework requiring models to ground arguments in explicit visual evidence. Experiments show the method improves agreement with human judgments, boosting Cohen's kappa by 1.5× and accuracy on GPT-4o by 1.25× over MLLM-as-a-judge baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tNovel and important problem: The paper identifies multimodal deception as a critical safety risk distinct from hallucination, filling a significant gap in AI safety research. The distinction between capability failure (hallucination) and intentional misalignment (deception) is well-motivated.\n2.\tHigh-quality benchmark construction: MM-DeceptionBench demonstrates careful design with: (1) Systematic four-stage annotation pipeline with expert annotators; (2) Real-world scenarios without artificial role-play instructions; (3) Rigorous quality control with cross-annotator inspection and panel review; (4) Clear taxonomy of six deception categories\n3.\tTheoretical contributions: Remarks 1 and 2 provide theoretical justification for why visual grounding helps detection (information preservation and asymmetric deception difficulty).\n4.\tComprehensive experiments: Evaluation across multiple models (GPT-5, GPT-4o, Claude-Sonnet-4, Gemini-2.5-Pro, Qwen2.5-VL-72B), datasets (MM-DeceptionBench, PKU-Safe RLHF-V, HallusionBench), and extensive ablations demonstrate thoroughness."}, "weaknesses": {"value": "1.\tBenchmark Limitations: While MM-DeceptionBench is innovative, its size (1013 cases) and focus on single-turn interactions may limit coverage of long-term or multi-turn deception. The categories are behavior-oriented but could overlap (e.g., omission and fabrication both involve selective semantics), potentially inflating correlations (heatmap shows 0.25-0.27 pairs). Reliance on expert annotators (9 total) risks bias, despite cross-inspection; inter-annotator agreement stats (e.g., Fleiss' kappa) are missing.\n2.\tFramework Assumptions: The utility function assumes agents minimize operations (λ>0), but experiments use fixed budgets—ablations show fluctuations with increasing ops (Fig. 4), suggesting choice-based selection might be suboptimal. Theoretical remarks (e.g., Eq. 2-3) are insightful but lack empirical validation (e.g., measuring mutual information decay). Visual operations are limited (e.g., no segmentation), and Qwen2.5-VL-specific resizing hints at implementation inconsistencies across models.\n3.\tEvaluation Gaps: Human annotations are gold-standard but only on 582 sampled cases—full benchmark evaluation would strengthen claims. Baselines are fair, but comparisons to non-debate monitors (e.g., CoT-based from Baker et al., 2025) are light. Generalization tasks use subsets (e.g., Visual-Dependent HallusionBench), but results on PKU-SafeRLHF-V show mixed ECE (0.08-0.26), indicating calibration issues. No analysis of failure modes (e.g., when debate misaligns with humans).\n4.\tBroader Impact: While ethics address misuse, the paper under-discusses risks of the benchmark enabling deception training or framework's scalability costs (e.g., multi-agent inference overhead)."}, "questions": {"value": "1.\tHow do you validate that reasoning traces in <think> tags reflect genuine model processes rather than post-hoc rationalization? Have you considered ablating the reasoning requirement to see if \"deception\" still occurs?\n2.\tCan you provide inter-annotator agreement metrics (Fleiss' kappa, Krippendorff's α) for the benchmark construction? What was the process for resolving disagreements?\n3.\tFor the \"spontaneity\" claim: how do your high-pressure scenarios (deactivation threats) differ from explicit deception elicitation in DarkBench? Can you report deception rates without extreme pressure?\n4.\tThe utility function (Eq. 1) includes a cost term λ·C^m, but how is λ set? Have you tried optimizing agent strategies under this utility? If not, why define it formally?\n5.\tCan you provide computational cost comparison (API costs, wall-clock time, memory) for debate with images vs. all baselines?\n6.\tHow would the debate framework be deployed in practice where you don't know the ground truth label to assign appropriate stances? Is there a self-calibration approach?\n7.\tWhat is the correlation between \"deception severity score\" from judges and human severity ratings? Is this a reliable continuous metric or just ordinal?\n8.\tFor cases where debate with images fails, what are the systematic patterns? Please provide a confusion matrix or failure analysis by deception category.\n9.\tWhy are Llama-3.2-Vision and other popular open-source multimodal models not evaluated?\n10.\tHave you tested adversarial robustness - can models learn to produce reasoning traces that hide deceptive intent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7D4uQxOV6X", "forum": "CZxcst0Xbe", "replyto": "CZxcst0Xbe", "signatures": ["ICLR.cc/2026/Conference/Submission11592/Reviewer_DKrF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11592/Reviewer_DKrF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726770452, "cdate": 1761726770452, "tmdate": 1762922673662, "mdate": 1762922673662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper takes a bold step into a new direction of AI safety. It looks at deceptive behavior in multimodal models: cases where a model deliberately gives misleading answers involving both images and text. The authors first build MM-DeceptionBench, a benchmark with over a thousand examples showing six kinds of multimodal deception, such as sycophancy, sandbagging, bluffing, obfuscation, omission, and fabrication. Each example is richly designed with human-written scenarios, roles, and visual cues. Then they propose Debate with Images, a multi-agent framework where models argue with each other using visual evidence (like bounding boxes and zoom-ins) to decide whether a response is deceptive. Experiments on top models including GPT-4o, Gemini-2.5-Pro, Claude-Sonnet-4, and Qwen2.5-VL show that this debate framework improves human agreement in deception detection and transfers well to other safety and reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Novel and timely problem – Multimodal deception is a fresh and important safety topic. Most work on AI deception stays in text; here the paper explores when visual cues interact with strategic dishonesty, which feels both relevant and overdue.\n\n1) The dataset is clearly made with care. Each case has structured fields, multiple human annotators, and realistic triggers that make deception emerge naturally. The taxonomy of six deception types is clear and grounded in behavioral psychology.\n\n2) The debate-with-images setup is clever. Forcing models to “ground” their arguments visually gives a concrete mechanism to expose manipulation or inconsistencies. The paper even provides theoretical insights showing why grounding makes deception harder to sustain.\n\n3) Results are consistent across many models and datasets. The improvements are not just minor tweaks but meaningful boosts in agreement with human judgment.\n\n4) The qualitative examples are fascinating and convincing. Seeing models like GPT-5 or Gemini produce flattering lies or subtle omissions makes the issue tangible."}, "weaknesses": {"value": "1) Results focus on accuracy and kappa with humans. It would be nice to see how well the framework identifies deception when humans disagree, or whether debate sometimes over-corrects and labels creativity as deception.\n\n2) The debate framework includes many moving parts (number of agents, rounds, visual ops). A simpler comparison isolating which factor gives the biggest gain would strengthen the case.\n\n3） The benchmark mainly focuses on benign or socially adaptive types of deception, things like flattery, omission, or exaggeration that appear in polite or goal-directed conversations. These are useful for studying subtle dishonesty, but they don’t capture the full safety risk. It would be valuable to include higher-stakes or malicious deception cases, where the model’s misleading behavior could actually cause harm. For example, scenarios where a model hides safety violations in an industrial setting, misreports visual evidence in a medical or legal context, or manipulates the user for strategic advantage. Such examples would test whether the framework still works when deception involves real-world consequences or adversarial incentives rather than just social niceties. Including a few of these tougher, risk-sensitive cases could show that Debate with Images generalizes beyond low-stakes persuasion and genuinely scales to safety-critical domains."}, "questions": {"value": "1) How do you define “intentional deception” operationally, given models don’t have explicit intentions?\n\n2) Are the debaters themselves ever deceptive, or are they assumed to be honest in the debate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MEUof8wS9a", "forum": "CZxcst0Xbe", "replyto": "CZxcst0Xbe", "signatures": ["ICLR.cc/2026/Conference/Submission11592/Reviewer_VXBp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11592/Reviewer_VXBp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775042733, "cdate": 1761775042733, "tmdate": 1762922673238, "mdate": 1762922673238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MM‑DeceptionBench to evaluate multimodal deception in vision–language models. The benchmark covers six behavior categories (sycophancy, sandbagging, bluffing, obfuscation, deliberate omission, and fabrication) with realistic scenarios that embed pressures/motivations without explicit \"role‑play to deceive\" instructions. The authors then propose a debate with images, a multi‑agent judge framework that requires debaters to ground claims in concrete visual evidence via lightweight operations. Compared to single‑agent or \"debate‑about‑images\" baselines, the method improves agreement with human judgments on MM‑DeceptionBench and shows spillover gains on PKU‑SafeRLHF‑V and HallusionBench."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper effectively supports empirical claims with extensive experiments on MM-DeceptionBench and two additional datasets (PKU-SafeRLHF-V and HallusionBench)\n- Useful benchmark: Six behavior categories are clearly defined, and realistic scenarios are effectively created."}, "weaknesses": {"value": "- Operational definition of deception: Several scenarios blend persuasion with deception (e.g., promotional copy that omits negative details. The annotation guide notes \"spontaneity\" and \"image specificity,\" but the line between strategic framing and deception remains partly subjective. Consider adding blinded third‑party adjudication with explicit criteria for \"inducing false beliefs,\" plus inter‑annotator agreement (e.g., category‑wise κ) on the gold labels themselves\n- Prompting strategy (CoT exposure): The evaluation relies heavily on explicit reasoning prompts, potentially inflating detection capabilities. Non-CoT evaluations should be considered.\n- Theory informality: Claims about visual grounding preserving information lack formal rigor and explicit assumptions.\n- Selection bias & overfitting to elicitable cases: The curation loop includes testing on \"10 MLLMs until target behaviors reliably emerged\", which risks a distribution tuned to today’s models and prompting style. A held-out, never-seen-by-curators/models split or time-lagged evaluation would strengthen claims."}, "questions": {"value": "The weaknesses above are essentially my question. \n- Reproducibility placeholders: The paper promises dataset/code \"here\". For double‑blind, anonymized links are fine; without them, it’s hard to verify claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hshHj10O4O", "forum": "CZxcst0Xbe", "replyto": "CZxcst0Xbe", "signatures": ["ICLR.cc/2026/Conference/Submission11592/Reviewer_3t5f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11592/Reviewer_3t5f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990278962, "cdate": 1761990278962, "tmdate": 1762922672674, "mdate": 1762922672674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To all reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful feedback and careful assessment of our paper. Their valuable input has enriched the quality and clarity of our work. We have carefully considered all questions, concerns, and comments raised by the reviewers and provided detailed responses to each review separately. Our responses have been meticulously integrated into the revised manuscript, with particular attention given to the following key aspects:\n\n(1) We have refined the statement of Remark 1 (Sec. 4.2, now Proposition 1), added a rigorous proof with explicit assumptions in Appendix A *(Reviewers 3t5f, DKrF)*, and provided additional explanations and citations for Remark 2 *(Reviewer DkrF)*.\n\n(2) We have added new ablation studies in Sec.5. These include ablations on agent–round dynamics, extending the number of rounds beyond 4 *(Reviewer FtUu)*, and ablations on different types of visual operations *(Reviewer DkrF)*. We also incorporated more detailed analyses and summarized the key findings in the experiment section *(Reviewer VXBp)*.\n\n(3) We have clarified the benchmark curation process, including inter-annotator agreement, and introduced blinded third-party adjudication to further support the reliability of our annotations *(Reviewers 3t5f, DKrF, FtUu)*.\n\n(4) We have added an operational definition of deception in Sec.5.1, further clarifying how both annotaters and the framework identify multimodal deception in practice *(Reviewers 3t5f, VXBp)*.\n\n(5) We expanded the Ethics Statement to address dual-use risks and added a new “Computational Overhead and Scalability” section that clarifies the inference and token costs of multi-agent debate, justifies these costs in safety-critical settings, and highlights future directions such as distillation to reduce overhead *(Reviewers DKrF, FtUu)*.\n\n**Revised passages are marked in orange in the updated manuscript.**\n\nOnce again, we extend our heartfelt gratitude for your time, expertise, and contribution to our work."}}, "id": "193NfrBssv", "forum": "CZxcst0Xbe", "replyto": "CZxcst0Xbe", "signatures": ["ICLR.cc/2026/Conference/Submission11592/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11592/Authors"], "number": 23, "invitations": ["ICLR.cc/2026/Conference/Submission11592/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763717679671, "cdate": 1763717679671, "tmdate": 1763717679671, "mdate": 1763717679671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}