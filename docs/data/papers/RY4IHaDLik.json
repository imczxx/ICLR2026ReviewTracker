{"id": "RY4IHaDLik", "number": 14219, "cdate": 1758230409403, "mdate": 1763303082064, "content": {"title": "CLEAR: Calibrated Learning for Epistemic and Aleatoric Risk", "abstract": "Existing methods typically address either aleatoric uncertainty due to measurement noise or epistemic uncertainty resulting from limited data, but not both in a balanced manner. We propose CLEAR, a calibration method with two distinct parameters, $\\gamma_1$ and $\\gamma_2$, to combine the two uncertainty components and improve the conditional coverage of predictive intervals for regression tasks. CLEAR is compatible with any pair of aleatoric and epistemic estimators; we show how it can be used with (i) quantile regression for aleatoric uncertainty and (ii) ensembles drawn from the Predictability–Computability–Stability (PCS) framework for epistemic uncertainty. Across 17 diverse real-world datasets, CLEAR achieves an average improvement of 28.2\\% and 17.4\\% in the interval width compared to the two individually calibrated baselines while maintaining nominal coverage. Similar improvements are observed when applying CLEAR to Deep Ensembles (epistemic) and Simultaneous Quantile Regression (aleatoric). The benefits are especially evident in scenarios dominated by high aleatoric or epistemic uncertainty.", "tldr": "Current prediction models struggle to optimally combine uncertainty from data noise (aleatoric) and model errors (epistemic). Our method, CLEAR, learns to balance both, yielding more reliable and accurate prediction intervals.", "keywords": ["Uncertainty Quantification", "Prediction intervals", "Epistemic Uncertainty", "Aleatoric Uncertainty", "Conditional Coverage", "Calibration"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1dc01881176993013da1071169e09d3b99e19221.pdf", "supplementary_material": "/attachment/46512985f6f8303f8deb3c0d8f759e625218274f.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes CLEAR, a method for constructing calibrated prediction intervals in regression by explicitly combining aleatoric and epistemic uncertainties. It estimates aleatoric uncertainty through quantile regression on residuals and epistemic uncertainty via bootstrapped ensemble variation, then forms intervals as a linear combination of both components."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of this paper are:\n- The method builds on an intuitive and practically relevant distinction between aleatoric and epistemic uncertainty, and operationalises it in a simple, interpretable way through a weighted combination of the two components.\n- I usually dislike arbitrary \\lambda weight params, but in this paper, it is justified in the method and offers an interpretable measure of how epistemic and aleatoric components contribute to overall predictive uncertainty."}, "weaknesses": {"value": "The weaknesses of this paper are:\n- CLEAR builds directly on existing CQR-derived approaches that incorporate uncertainty decomposition, particularly Uncertainty-Aware CQR (UACQR; Rossellini et al., 2024). As acknowledged by the authors themselves, UACQR can be viewed as a special case of CLEAR with \\gamma = 1. CLEAR’s main extension is to calibrate both parameters, allowing it to adjust for miscalibration in the aleatoric component. While this generalisation is reasonable and practically useful, it represents an incremental refinement. Thus, novelity is very low in my opinion.\n- Correct me if I am wrong, but in the experimental setup, the same data is used to tune \\lambda and calibrate \\gamma, which should break the independence assumption required for coverage guarantees.\n- Since this is derivative to CQR and other prior methods, not comparing against interval-creation baselines in the main paper seems like a missed step. Why leave competitive results for Appendix D? This has left me confused.\n- After spending much time reading the appendix, I am left confused by the structure of the paper. It seems like the strong literature comparisons, justifications, good results are all in the Appendix and the main body of the paper is given less thought. This seems backwards to me?"}, "questions": {"value": "- In the default implementation, the same dataset is used both for tuning \\lambda and conformal calibration. How do the authors reconcile this with the independence requirement of split-conformal prediction? Can they provide empirical or theoretical evidence that nominal coverage is still preserved under this data reuse?\n- Since CLEAR is presented as an evolution of CQR-derived conformal methods, why are direct comparisons against UACQR and related interval-construction baselines relegated to Appendix D rather than integrated into the main results? Would including these baselines in the principal tables change the strength of the empirical conclusions?\n- Much of the substantive discussion—literature positioning, theoretical justification, and broader comparisons—appears only in the appendix. Could the authors explain the rationale for this organisation and whether key arguments could be moved into the main text to improve clarity and self-containment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aAyrJi09iv", "forum": "RY4IHaDLik", "replyto": "RY4IHaDLik", "signatures": ["ICLR.cc/2026/Conference/Submission14219/Reviewer_yexK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14219/Reviewer_yexK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760868729370, "cdate": 1760868729370, "tmdate": 1762924676544, "mdate": 1762924676544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their feedback. We would like to clarify three points that were common across reviews. In our rebuttal, we use prefixes **W** for weaknesses and **Q** for questions.\n\n### **1. Data Reuse: Standard vs. Conformalized CLEAR**\n\nReviewers WNoK (W2) and yexK (W2, Q1) raised concerns about using $\\mathcal{D}\\_{\\text{val}}$ for both model selection (including the choice of $\\lambda$) and calibration of $\\gamma_1$. As it was already stated in lines 267-270 of the paper, we accounted for this by providing two approaches: a **standard** CLEAR with $\\mathcal{D}\\_{\\text{val}} = \\mathcal{D}\\_{\\text{cal}}$, and a **conformalized** CLEAR using independent $D\\_{\\text{val}} $ and $ D\\_{\\text{cal}}$ sets to maintain classical conformal guarantees.\n\n- **Theoretically**: Appendix B (Lemma B.2) provides **finite-sample marginal coverage guarantees** for conformalized CLEAR. Recent work [1] shows that the bias introduced by such data reuse can be bounded and vanishes asymptotically under mild assumptions.\n- **Empirically**: We report results for both configurations (Appendices F and G). We find empirical coverage on $\\mathcal{D}_{\\text{test}}$ is nearly identical across both versions, while CLEAR produces substantially better intervals. The **standard configuration is our default recommendation for practical use**, as it maximizes data efficiency, but the **code supports both approaches** (i.e., theoretical guarantees vs maximal data use).\n\n### **2. Runtime and Computational Efficiency**\n\nReviewers WNoK (W3, Q2) and MNyo (Q1) asked about the computational efficiency. The grid-search for calibrating $\\lambda$ and $\\gamma_1$ is **extremely fast in practice**, despite using over 4000 grid points (e.g., 400 would likely suffice). As documented in Appendix F.5 (\"Runtime\") and Tables 28 of average runtime:\n- **Grid search takes less than 1 second for 16 out of 17 datasets**\n- For our largest dataset (`superconductor`), calibration takes only 13 seconds\n- The bottleneck is ensemble training (minutes to hours), not calibration\n\nThe calibration step is highly vectorized and scales as $O(|\\Lambda| \\cdot |\\mathcal{D}\\_{\\text{cal}}| \\log |\\mathcal{D}\\_{\\text{cal}}|)$. The entire pipeline is highly modular and parallelizable (e.g., ensembles and bootstrapping) and can be trained independently.\n\n### **3. Consistency Assumptions**\n\nReviewers WNoK (W5) and MNyo (W1) mentioned the consistency assumption. Lemma 2.1 requires that at least $k$ base models are consistent. In our experiments, we use $k=1$ (best model), so the theorem requires just **one consistent estimator**. As explained in Appendix B.2, many practical estimators satisfy consistency under standard regularity conditions: QRF with min leaf size regularization, XGBoost with early stopping [2] and regularized QR [3]. If no base model is consistent, CLEAR inherits the asymptotic bias of the base models, but the dual-parameter calibration ($\\gamma_1$, $\\lambda$) still provides empirical robustness by adaptively reweighting components (as demonstrated on 17 diverse real-world datasets).\n\n### **Manuscript Revision**\n\nWe have addressed the concerns of the reviewers using two changes (blue text in the revised manuscript):\n\n1. **UACQR Comparison** (reviewer yexK): We have revised the main paper to elevate the comparison with UACQR. This includes a **new summary table (lines 425-439)** and a **discussion (lines 439-461)**, which were moved from the Appendix A.1 to Section 4.2. This directly addresses the concern about the placement of this comparison.\n\n2. **Equation 1 Clarification** (reviewer bUL2): To explicitly address the misunderstanding about symmetry, we have revised the first equation (line 53) to use $\\pm{}$ notation, making it clear that the aleatoric and epistemic components are handled asymmetrically.\n\nNotes:\n\n* *Numbering and Referencing:* All figure, equation, and appendix numbers are unchanged. The new UACQR summary table was inserted without a number to avoid re-numbering all tables for this revision. All line numbers in our rebuttal refer to this revised manuscript.\n\n* *Runtime Discussion:* We have an extensive \"Runtime\" analysis in Appendix F.5 (Tables 28-30). If reviewers feel it is critical, we are happy to add a concise summary of these negligible computational costs (e.g., \"less than 1 second for 16/17 datasets\") to the main paper.\n\n### References\n\n- [1] Zeng, Hao, Kangdao Liu, Bingyi Jing, and Hongxin Wei. \"Parametric Scaling Law of Tuning Bias in Conformal Prediction\". Forty-Second International Conference on Machine Learning (2025).\n- [2] Zhang, T., Yu, B. (2005). Boosting with early stopping: Convergence and consistency.\n- [3] Steinwart, I., Christmann, A. (2011). Estimating conditional quantiles with the help of the pinball loss.\n\nWe are happy to continue the discussion and provide any additional clarifications. We hope you will consider revising your assessments in light of these clarifications."}}, "id": "9kftOOBgrG", "forum": "RY4IHaDLik", "replyto": "RY4IHaDLik", "signatures": ["ICLR.cc/2026/Conference/Submission14219/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14219/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14219/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763303579266, "cdate": 1763303579266, "tmdate": 1763306163661, "mdate": 1763306163661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CLEAR (Calibrated Learning for Epistemic and Aleatoric Risk), a novel framework for constructing regression prediction intervals by adaptively balancing epistemic uncertainty and aleatoric uncertainty. Theoretically, it guarantees asymptotic conditional validity; empirically, on 17 real-world datasets and synthetic data with distribution shifts, CLEAR outperforms baselines by reducing interval width and quantile loss while maintaining nominal coverage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unlike methods that use a fixed ratio to combine the two uncertainties, CLEAR selects the balance parameter based on data characteristics. For example, it emphasizes aleatoric uncertainty when working with data with few features and epistemic uncertainty when using data with many features, making it flexible across different scenarios.\n2. It works with various uncertainty estimation methods (including tree-based PCS and deep learning-based Deep Ensembles or Simultaneous Quantile Regression) and maintains reliable coverage even for data points outside the training data distribution or in extrapolation regions—areas where many baseline methods struggle."}, "weaknesses": {"value": "1. The proof requires that \"at least k base models in the PCS ensemble are consistent with the true function,\" but it does not define specific criteria for determining consistency (such as error convergence thresholds) nor explain the basis for selecting k. In practical experiments, the consistency of different models varies significantly, yet the paper fails to analyze the risk of theoretical guarantees failing in such scenarios.\n\n2. The additivity of the two types of uncertainty has not been proven — epistemic uncertainty and aleatoric uncertainty essentially belong to risks of different dimensions. The additive combination implies the assumption that the two can be directly superimposed on the numerical scale, but the paper does not verify this assumption. For example, it does not compare the performance differences between additive, multiplicative, and nonlinear combinations.\n\n3. In some datasets, there is a significant correlation between the two types of uncertainties. The additive combination may amplify the uncertainty superposition effect, leading to overly wide intervals. In low-correlation datasets, however, the additive combination may underestimate risks due to improper weight allocation. The paper does not analyze the impact of uncertainty correlation on the combination structure, which limits the generality of the method."}, "questions": {"value": "1. Regarding computational efficiency, CLEAR’s grid search for lambda (over 4000 points) consumes significant resources, especially for large datasets. Have you explored adaptive search strategies? If yes, what was the reduction in computational time while maintaining performance?\n2. This paper verifies CLEAR’s performance on 17 regression datasets, but most of these datasets have relatively balanced feature distributions. For high-dimensional sparse datasets (e.g., tabular data with hundreds of features where most are irrelevant), how does CLEAR’s performance change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y8C3pkMKEC", "forum": "RY4IHaDLik", "replyto": "RY4IHaDLik", "signatures": ["ICLR.cc/2026/Conference/Submission14219/Reviewer_MNyo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14219/Reviewer_MNyo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878386826, "cdate": 1761878386826, "tmdate": 1762924676156, "mdate": 1762924676156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose the CLEAR algorithm for UQ in regression, with the following steps.\n1. An epistemic model is built to learn the mean and the boundaries of a confidence interval.\n2. The predicted mean is subtracted from the target and an aleatoric model is trained on the residual. The boundaries of the confidence interval is kept.\n3. Two coefficients $\\gamma_1$ and $\\lambda$ are determined, from which $\\gamma_2 = \\lambda * \\gamma_1$. For all possible $\\lambda$, separately $\\gamma_1$ are computed by calibration, and $\\lambda$ that optimizes the evaluation metric is selected.\n4. The aleatoric/epistemic intervals around mean are scaled symmetrically with the coefficients.\nI find the contribution is the combination of known methods with a grid search, with limited theoretical justification and experimentation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Learning the combination of aleatoric and epistrmic uncretainties in one model is an important practical question\n+ Reproducibility in the supplementary material\n+ Improvement in experiments"}, "weaknesses": {"value": "- Contribution is the combination of known methods via grid search \n- If the distribution is non-Gaussian, the proposed method is limited due to its strong focus on the mean, and its symmetry. Why not median or another statistic? How about skewed distributions? Bimodal?\n-  Theoretical justification is limited"}, "questions": {"value": "How can you handle the following limitations of the proposed method:\n - confidence interval is built around the mean $\\hat{f}$, which is subtracted from the target for training the aleatoric model. Why not, for example, the median?\n- Confidence intervals are treated symmetrically, how bout skewed distributions, or bounded target domain?\n- How about shape parameters beyond mean and confidence intervals?\n - It is assumed that if you subtract mean from the target, what is left is aleatoric uncertainty. How about e.g. bimodal?\n\nWhat is the relation of the proposed method to  https://en.wikipedia.org/wiki/Nonhomogeneous_Gaussian_regression ?  There, also an ensemble is used to calibrate the prediction for the mean (deep ensemble as an epistemic model in the paper). Gaussian variance corresponds to the aleatoric distribution, which actually needs to be assumed at several places in the proposed method.\n\nHow do you measure whether the two types of uncertainties are properly balanced? Is there ground truth?\n\nThe introduction claims \"However, they may suffer from poor conditional coverage, meaning well-calibrated coverage at the individual or subgroup level\", i.e. in the literature, marginal calibration is insufficiently solved. How does the proposed method solve the question?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1KnidGckxy", "forum": "RY4IHaDLik", "replyto": "RY4IHaDLik", "signatures": ["ICLR.cc/2026/Conference/Submission14219/Reviewer_bUL2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14219/Reviewer_bUL2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947162481, "cdate": 1761947162481, "tmdate": 1762924675658, "mdate": 1762924675658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing methods typically address either aleatoric uncertainty due to measurement noise or epistemic uncertainty resulting from limited data, but not both in a balanced manner. In this manuscript, however, the authors propose a calibration method that combines both aleatoric (data noise) and epistemic (model/data limitation) uncertainties to improve the conditional coverage of predictive intervals for regression tasks. Dubbed CLEAR, the framework uses two learnable calibration parameters, $(\\gamma_1, \\gamma_2)$, to combine the two uncertainty components. CLEAR is compatible with any pair of aleatoric and epistemic estimators, enabling adaptive weighting based on data characteristics; unlike prior methods that fix their ratio (e.g., $\\gamma = 1)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "CLEAR presents a principled, practical, and empirically effective framework for calibrated uncertainty quantification by adaptively fusing epistemic and aleatoric components. Its main innovation lies in the dual-parameter calibration, which yields sharper, better-calibrated intervals without sacrificing coverage. In particular, its key strengths are:\n\n**Strengths:**\n\n1. **Balanced Uncertainty Integration**: CLEAR uniquely combines both aleatoric (data noise) and epistemic (model/data limitation) uncertainties using two learnable calibration parameters $\\gamma_1 \\text{ and } \\gamma_2, \\text{ with } \\lambda = \\frac{\\gamma_2}{\\gamma_1}$, enabling adaptive weighting based on data characteristics; unlike prior methods that fix their ratio (e.g., $\\lambda = 1$).\n\n2. **Improved Performance**: Across 17 real-world regression datasets, CLEAR consistently achieves narrower prediction intervals (e.g., 28.2% and 17.4% average width reduction vs. baselines) **while maintaining nominal 95% coverage**, outperforming its components (CQR and PCS ensembles) and other strong baselines.\n\n3. **Flexibility & Generality**: CLEAR is model-agnostic. It works with various uncertainty estimators (e.g., PCS ensembles + quantile regression on residuals, or Deep Ensembles + Simultaneous Quantile Regression), demonstrating broad applicability.\n\n4. **Theoretical Justification**: The paper provides asymptotic conditional coverage guarantees under mild consistency assumptions (Lemma 2.1), addressing a known weakness of standard conformal methods like CQR, which often under-cover in low-density or extrapolation regions.\n\n5. **Practical Design Choices**: Estimating aleatoric uncertainty on residuals (rather than raw targets) improves stability; using quantile loss for $\\lambda$ selection ensures proper scoring and incentivizes conditional calibration.\n\n6. **Interpretability**: The learned $\\lambda$ offers insight into whether aleatoric or epistemic uncertainty dominates in a given problem (e.g., $\\lambda \\approx$ 0.6 vs. 14.5 in the Ames Housing case study with 2 vs. 80 features)."}, "weaknesses": {"value": "Although CLEAR enjoys some key benefits over existing methods, it does have some limitations. In particular, its key weaknesses are:\n\n**Weaknesses:**\n\n1. **Dependence on Base Estimators**: CLEAR’s performance hinges on the quality of the underlying aleatoric and epistemic estimators. Poor base models may limit gains or require careful tuning.\n\n2. **Calibration Data Requirements**: The dual-parameter calibration ($\\gamma_1, \\lambda$) uses the validation set for both model selection and calibration. While empirically effective, this lacks finite-sample marginal coverage guarantees unless a separate calibration split is used (as shown in Appendix G).\n\n3. **Computational Overhead**: While the grid search for $\\lambda$ is fast, the full pipeline requires training ensembles (e.g., 100 bootstraps in PCS), which can be expensive on large datasets, although the authors note this is modular and parallelizable.\n\n4. **Regression-Only Focus**: The method is developed and evaluated only for regression; extension to classification or structured prediction is left for future work.\n\n5. **Limited Theoretical Scope**: The asymptotic guarantees assume i.i.d. data and consistent estimators—conditions that may not hold in complex real-world settings with distribution shifts or model misspecification.\n\n6. **Empirical Results Interpretation**: While CLEAR achieved improved results over existing methods, the paper failed to properly discuss what those improvements concretely mean in terms of choosing CLEAR over existing methods, and if such improvements can translate to more complex regression tasks."}, "questions": {"value": "1. Would it possible to test CLEAR on toy classification problems?\n2. Could the authors provide some asymptotic compute cost for larger datasets? Put differently, how do the authors see CLEAR scale for larger datasets?\n3. The paper focuses mainly on I.I.D datasets, without providing clear evidence on or discussing how CLEAR would perform on non-i.i.d data. Could the authors provide some insights for non i.i.d data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ryd1JotqLE", "forum": "RY4IHaDLik", "replyto": "RY4IHaDLik", "signatures": ["ICLR.cc/2026/Conference/Submission14219/Reviewer_WNoK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14219/Reviewer_WNoK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015966957, "cdate": 1762015966957, "tmdate": 1762924674454, "mdate": 1762924674454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}