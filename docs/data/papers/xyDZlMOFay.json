{"id": "xyDZlMOFay", "number": 7948, "cdate": 1758045475044, "mdate": 1759897820326, "content": {"title": "Large Language Models as Improvement Operators: Better Reasoning by Iteration", "abstract": "Reasoning training of  LLMs teaches them to produce long chains of thought (long CoT), with attendant increase in accuracy (good!), context length (bad!), compute cost (bad!) and  answer latency (bad!). Can current models provide other combinations on this Pareto frontier, e.g., give answers with better accuracy than long CoT models, despite lower context length and/or latency? We show that the answer is ``yes'' and also give ways to systematically think about these design choices. Abstractly, this involves viewing the model as an \\emph{improvement operator} with a continuum of strategies for improving its problem solving, (example: generate four shorter answers and combine their good points in   a single superior answer). We study an inference method \\textbf{Parallel-Distill-Refine (\\PDR)} that performs a few rounds of the following: (i) generate diverse drafts in parallel; (ii) \\emph{distill} them into a bounded, textual \\emph{workspace}; and (iii) \\emph{refine} conditioned on this workspace, which then seeds the next round. PDR often provides better performance than long CoT and has lower latency and context size. An interesting subcase of PDR is \\textbf{Sequential Refinement (\\SR)}, which iteratively improves a single candidate answer without a persistent workspace. It provides performance superior to long CoT, with the benefit of compact context size but high latency. These  examples suggest training interventions to shift the Pareto frontier. For example, we use  RL to improve an $8B$ model to better align with \\PDR\\ as the inference method, which improves performance. On math tasks with rule-based checkers, iterative pipelines surpass single-pass baselines at matched sequential budget; shallow \\PDR\\ delivers the largest gains (e.g., +10\\% on AIME 2024 and +11\\% on AIME 2025).", "tldr": "Scaling train and test time compute using self improvement", "keywords": ["Large Language Models", "Self Improvement", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a3b5a931c7aad3d46f4503c567bf882a8aec063.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reframes reasoning with large language models (LLMs) as an iterative improvement process, introducing the concept of LLMs as improvement operators.\n\nTwo inference operators are proposed:\n**Sequential Refinement (SR)** — iteratively refines a single answer in short contexts.\n**Parallel–Distill–Refine (PDR)** — generates multiple drafts, summarizes them into a bounded workspace, and refines based on this summary iteratively.\n\nTo reduce the train–test mismatch between long chain-of-thought (CoT) training and short-context iterative inference, the authors propose operator-consistent reinforcement learning (Op-RL), aligning training and inference through rollouts that mimic the generate–distill–refine interface.\n\nExperiments on AIME 2024 and 2025 math benchmarks show that PDR achieves up to +10–11% accuracy gains over long CoT under comparable latency budgets. The operator-consistent RL further provides a ~5% boost."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "**Conceptually novel framing** Viewing LLMs as “improvement operators” provides a fresh and systematic perspective for reasoning optimization.\n\n**Inference efficiency focus** The work directly tackles the trade-off between reasoning accuracy and token cost, a central issue in test-time scaling.\n\n**Operator-consistent RL** A meaningful step toward aligning training and inference under short-context reasoning.\n\n**Clear experimental protocol** The paper consistently reports both sequential budget (B_seq) and total budget (B_total), encouraging cost-aware evaluation."}, "weaknesses": {"value": "**Limited evaluation scope (Major Concern)**\nThe empirical validation relies solely on AIME 2024/2025, two relatively narrow math benchmarks.\nThere is no evidence of generalization to other reasoning domains (e.g., commonsense, logic puzzles, code generation, multimodal reasoning).\nGiven the paper’s broad claim of “better reasoning by iteration,” this narrow scope weakens the empirical foundation.\n\n**Cost analysis is incomplete**\nAlthough the paper introduces B_seq (latency proxy) and B_total (compute proxy), there is little quantitative discussion on real compute cost (e.g., wall-clock time, GPU-hours, or energy).\nThe operator-consistent RL training uses large-scale setups (80–288 H200 GPUs), but the paper never clarifies whether the performance gains justify this cost.\n\n**Reproducibility concerns**\nExperiments rely on closed-source models (Gemini 2.5 Flash, GPT-o3-mini), and the open 8B model results are limited to internal evaluation."}, "questions": {"value": "1. Can the proposed PDR method generalize beyond math tasks (e.g., planning, commonsense reasoning, code)?\n\n2. What is the real inference-time speedup compared to long CoT, when measured in latency and wall-clock compute (not just token counts)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yGyW2rW4U2", "forum": "xyDZlMOFay", "replyto": "xyDZlMOFay", "signatures": ["ICLR.cc/2026/Conference/Submission7948/Reviewer_7aGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7948/Reviewer_7aGP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821494105, "cdate": 1761821494105, "tmdate": 1762919967512, "mdate": 1762919967512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes viewing LLMs as \"improvement operators\" that iteratively refine solutions using bounded context rather than single long chains of thought. The authors introduce two inference methods: Sequential Refinement (SR) - iteratively improving a single solution, and Parallel-Distill-Refine (PDR) - generating multiple parallel solutions, distilling them into a compact summary, then refining. They also propose \"operator-consistent\" training that aligns the training objective with this iterative inference pattern. Experiments on AIME 2024/2025 math problems show PDR can achieve better accuracy than long CoT baselines while maintaining lower latency and context size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Viewing LLMs as improvement operators with explicit budget constraints (sequential vs. total tokens) is a useful perspective for understanding the accuracy/latency/cost tradeoffs. The methods are straightforward to implement using existing models via prompting, without architectural changes\n2. The paper evaluates multiple distillation strategies, studies oracle experiments to understand verification capabilities, and measures both sequential and total token budgets.\n3. PDR achieves +10-11% improvements on AIME benchmarks over long CoT at matched sequential budgets"}, "weaknesses": {"value": "1. The paper doesn't explain when short-context iteration helps vs. hurts. What problem properties make PDR effective? When should practitioners use this? No results on reasoning tasks without automated verification, coding, planning, or other domains\n2. While per-call context is bounded, the *total* information processed grows with rounds. The paper conflates \"bounded workspace per round\" with \"compact memory\" but the system still processes increasing amounts of text\n3. Only compares to single long CoT. Missing comparisons to: self-consistency with multiple samples, beam search, other structured reasoning methods (ToT, GoT). The operator RL uses only 1 round during training but multiple rounds at test time. Why this mismatch? How sensitive are results to this choice?"}, "questions": {"value": "**How does this compare to self-consistency or best-of-N sampling?** At matched total compute, is PDR actually better than just sampling multiple solutions and picking the best?\n\n**What about the train-test mismatch in number of rounds?** Training uses 1 round, testing uses multiple. Did you experiment with multi-round training rollouts?\n\n**Can you provide failure analysis?** When does PDR make things worse? What types of problems benefit most?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AxaMRxl4u0", "forum": "xyDZlMOFay", "replyto": "xyDZlMOFay", "signatures": ["ICLR.cc/2026/Conference/Submission7948/Reviewer_RHtA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7948/Reviewer_RHtA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008163982, "cdate": 1762008163982, "tmdate": 1762919967201, "mdate": 1762919967201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors argue that using the standard approach of eliciting long CoT represents just one point on the Pareto frontier (there are also other factors along with answer accuracy – context length, compute cost and latency) and explore two inference methods that frame LLM as an “improvement operator” – Parallel-Distill-Refine (PDR) and Sequential Refinement (SR). They also introduce RL training to address the potential train and inference-time mismatch with this new inference method. The evaluation which is done on AIME 2024 & AIME 2025 (which are math reasoning benchmarks), reveals that PDR in particular surpasses performance on long CoT baselines (for a given latency). Shallow PDR achieved gains of +10% on AIME 2024 and +11% on AIME 2025. RL training further improved these results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of moving beyond a single reasoning trace towards other options on the Pareto Frontier makes complete sense.\n\n2. PDR method is a good alternative for the problem of iterative refinement and using bounded, round-wise summary is innovative.\n\n3. Budget evaluation which was done by making a distinction between sequential budget (latency proxy) and total budget (compute/cost proxy) is appreciated.\n\n4. Impressive Results on AIME."}, "weaknesses": {"value": "1. While the title mentions suggests reasoning in general, the evaluations are only on math tasks with verifiable rewards. Applicability to other settings remains unknown. \n\n2. The authors haven’t provided details of AIME datasets in the paper, but to the best of my knowledge they contain around 30 questions each (?) – if so, then 10% gain would mean improvement on 3 out of 30 questions. It would have been better if the authors evaluated the approach on more datasets or datasets of larger size.\n\n3. During RL training, the authors use a one-round PDR rollout. However, at inference, multi-round is done. Does this not create its own train-test mismatch?"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ap5tyQHRa7", "forum": "xyDZlMOFay", "replyto": "xyDZlMOFay", "signatures": ["ICLR.cc/2026/Conference/Submission7948/Reviewer_gfiH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7948/Reviewer_gfiH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762107692912, "cdate": 1762107692912, "tmdate": 1762919966816, "mdate": 1762919966816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new way to think about large language model (LLM) inference. Instead of generating a single long chain of thoughts, it treats reasoning as an iterative improvement process that works in short rounds. The model repeatedly improves or refines earlier answers within a limited token budget.\n\nTwo main operators are introduced.\n1. Sequential Refinement (SR): the model refines one draft step by step in short rounds, similar to incremental self-reflection.\n2. Parallel Distill Refine (PDR): in each round, the model produces several candidate drafts, then summarizes or selects the most useful parts within a fixed workspace limit (κ tokens). This summary acts as shared context for the next refinement round.\n\nThe paper also introduces operator-consistent reinforcement learning (RL), which trains the model using the same short iterative process used during inference. This alignment helps the model learn how to improve solutions step by step instead of relying only on long reasoning traces. Experiments on math reasoning benchmarks such as AIME 2024 and AIME 2025 show that PDR and SR outperform long chain-of-thought (CoT) reasoning when measured under equal latency or token budgets. PDR achieves higher accuracy while using shorter reasoning paths. The paper also compares different workspace strategies, such as global summaries and top-k selections, and finds that summarizing or choosing top responses helps more than random selection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of treating inference as an iterative improvement process is simple and practical.\n2. The paper provides a clear framework and connects reasoning to space-bounded or compressed computation and experiments are well controlled using clear token budgets, making comparisons fair.\n3. The operator-consistent RL improves reasoning performance and aligns training with inference."}, "weaknesses": {"value": "1. Evaluation is restricted to math problems only (AIME 2024, 2025) where rule-based verifiers (sympy, math-verify) exist. The generalizability of these findings to non-math domains remains unclear. Math problems may have unique properties (e.g., objective ground truth, verification tractability) that make PDR effective in ways that don't transfer.\n\n2. The distillation operator D is critical as it compresses multi-draft solutions into a bounded workspace. Further, the paper explores four heuristic distillation strategies (global summary, top-k, random-k). However, it does not provide principled criteria for when each strategy should apply, and explain why global summary and per-sample top-k succeed whereas random-k underperforms.\n\n3. Evaluation on commercial APIs (gemini-2.5-flash, o3-mini) limits reproducibility and ablations. The authors cannot, for example, vary model size systematically or inspect model internals. Further, an ablation across model sizes is not shown. Typically, 8B dense model trained with operator RL is relatively small by modern standards. There should have been an evaluation on larger models, say 20B+ to show effectiveness of the approach."}, "questions": {"value": "1. Did you try an ablation on the mixing ratio? Try 25-75, 40-60, 60-40 splits in Equation 11. Does the 50-50 choice matter?\n\n2. Can you describe scenarios where PDR underperforms: (1) Very short thinking budgets (do summaries compress too aggressively)? (2) Tasks requiring deep state tracking (does round-wise forgetting hurt)?\n\n3. Figure 3 and Figure 9 show PDR dominates Long CoT on B_seq but SR dominates on B_total. This is stated but not deeply analyzed: (1) When would practitioners prefer SR (high latency tolerance, cost-constrained) vs. PDR? (2) How do parallelism constraints on typical hardware affect the tradeoff?\n\n4. How does performance change with different workspace sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wdsLL8AUXr", "forum": "xyDZlMOFay", "replyto": "xyDZlMOFay", "signatures": ["ICLR.cc/2026/Conference/Submission7948/Reviewer_79KF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7948/Reviewer_79KF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762151573994, "cdate": 1762151573994, "tmdate": 1762919966525, "mdate": 1762919966525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}