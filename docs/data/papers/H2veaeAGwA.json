{"id": "H2veaeAGwA", "number": 10735, "cdate": 1758180716356, "mdate": 1759897632546, "content": {"title": "RAG-FGO: Enhancing RAG with Fungal Growth Optimizer for LLM Agents", "abstract": "Generative retrieval leverages large language models (LLMs) to directly generate retrieval queries or candidate document representations, and has recently shown great potential in open-domain question answering and knowledge-intensive tasks. Compared to traditional index-based retrieval methods, generative retrieval provides greater flexibility in handling semantic diversity and complex task requirements. However, existing approaches often rely on static prompt designs or fixed generation strategies, which struggle to maintain both stable and efficient performance in scenarios characterized by semantic complexity, task variability, or noisy knowledge bases. To address these limitations, we propose RAG-FGO (Retrieval-Augmented Generation with Fungal Growth Optimizer), a heuristic search-based framework for optimizing dynamic generative retrieval agents in Retrieval-Augmented Generation (RAG) systems. RAG-FGO integrates both global and local search strategies within the solution space to generate more robust and effective retrieval prompts and parameters, while avoiding local optima. In addition, it introduces a query memory pool that stores high-performing prompts during iterative optimization, thereby guiding subsequent search and generation. Experimental results indicate that RAG-FGO outperforms baselines such as Direct, ReAct, and Self-Act on benchmark datasets including HotpotQA, MuSiQue, and SQuAD, confirming its effectiveness for complex generative retrieval tasks.", "tldr": "We present RAG-FGO, a search-based framework that optimizes generative retrieval agents for RAG systems, yielding superior performance on complex QA benchmarks.", "keywords": ["Retrieval-Augmented Generation", "Generative Retrieval", "Large Language Models", "Agent Optimization", "Question Answering", "Knowledge-Intensive Tasks"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e69f40dec583a20cd8304239ffc2d1d776c88406.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents RAG-FGO, a framework designed to enhance Retrieval-Augmented Generation (RAG) systems by optimizing retrieval prompts and parameters. Inspired by the Fungal Growth Optimizer (FGO) algorithm, the proposed method models the search for an optimal “retrieval agent” in semantic space as a process that combines global exploration with local refinement. A key component is the query memory pool, which preserves high-performing agents during iterations to ensure stable and cumulative optimization. The core contribution lies in formulating retrieval optimization as a dynamic search problem. Experimental results on question-answering and reasoning benchmarks such as HotpotQA and MuSiQue demonstrate that RAG-FGO outperforms several strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel problem framing and method: The paper introduces RAG‑FGO, formulating retrieval‑prompt optimization as a semantic‑space search with a query memory pool and FGO‑inspired exploration–exploitation, supported by a clear workflow.\n\n2. Well‑specified global vs. local search mechanism: The paper proposes local search as well as global search, where local perturbations are performed at a fine-grained level, while global perturbations force larger changes.\n\n3. Broad experimental coverage and ablations: The study spans six QA benchmarks and seven reasoning benchmarks with multiple backbones, reports head‑to‑head results, and analyzes iteration/search budgets and saturation effects."}, "weaknesses": {"value": "1. Computational overhead and cost reporting: Runtime or Dollar Costs and Compute‑Matched Comparisons are not reported. The paper discusses that this method is not suitable for LLMs that are large enough to provide richer data.\n\n2. Missing component ablations: Although the schedule and operators are defined, there is no ablation isolating local vs. global search or the query memory pool to quantify each component’s contribution.\n\n3. Figure readability: Some figures have small fonts and become blurry when zoomed in, which hinders quick understanding of pipeline and ablation trends."}, "questions": {"value": "For global search, what are the rules for rewriting using LLM? For local search, are the prompt modification rules for algorithms such as synonym replacement random or sampled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6SI40pMKcN", "forum": "H2veaeAGwA", "replyto": "H2veaeAGwA", "signatures": ["ICLR.cc/2026/Conference/Submission10735/Reviewer_d9Pi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10735/Reviewer_d9Pi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637033162, "cdate": 1761637033162, "tmdate": 1762921961416, "mdate": 1762921961416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAG-FGO, a bio-inspired optimization framework for Retrieval-Augmented Generation (RAG) that employs the Fungal Growth Optimizer (FGO) to dynamically refine generative retrieval agents. The method treats retrieval optimization as a semantic-space search problem, combining global exploration and local refinement, with a query memory pool to accumulate high-performing retrieval strategies. Evaluations on multiple QA and reasoning benchmarks (HotpotQA, MuSiQue, SQuAD, MMLU-Pro, etc.) demonstrate that RAG-FGO consistently improves over strong baselines such as ReAct, Self-Act, and Aflow."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\n  The paper introduces a novel adaptation of the Fungal Growth Optimizer for retrieval prompt and parameter optimization in RAG systems. While prior works have explored heuristic or evolutionary search for LLM tuning, this specific bio-inspired approach and its integration with RAG’s retrieval module are original and timely. The inclusion of a query memory pool for cumulative improvement further enhances the conceptual novelty.\n\n Quality:\n  The methodology is well motivated and experimentally validated on diverse datasets and models (e.g., GPT-4o-mini, Qwen-Long, Gemma2-27B). The design of both local and global search phases, along with iteration scheduling, shows thoughtful engineering. Ablation studies on iteration and search count (Figures 4–5) provide some insight into the framework’s efficiency–accuracy trade-offs.\n\n Clarity:\n  The paper is generally well organized, with clear mathematical notation, figures that effectively communicate the iterative optimization process, and detailed experimental descriptions. The reproducibility section is thorough, listing datasets, hyperparameters, and implementation details.\n\n Significance:\n  By addressing the rigidity of static retrieval prompts and manually tuned parameters, RAG-FGO contributes to making RAG systems more adaptive, autonomous, and robust. This work is relevant to ongoing research on retrieval–generation co-optimization and scalable agentic LLM pipelines."}, "weaknesses": {"value": "Lack of theoretical grounding:\n  The paper lacks formal proofs, convergence analysis, or theoretical guarantees for the proposed optimization process. While heuristic optimization methods are often empirical, ICLR readers generally expect at least a formalized problem definition or complexity characterization. The description of FGO’s adaptation to semantic-space search remains heuristic and narrative rather than mathematically rigorous.\n\n Algorithmic opacity:\n  Despite the inclusion of pseudocode references, the formal definition of the optimization objective (e.g., explicit loss function or expected improvement metric) is missing. The framework is presented as a procedure rather than as a well-defined optimization problem, limiting clarity on what precisely is being optimized and why it converges.\n\n Incremental technical novelty:\n  While the bio-inspired framing is interesting, the underlying mechanism—population-based heuristic search with local/global exploration—closely resembles existing evolutionary methods (e.g., PSO, GA). The novelty lies mainly in the metaphor and domain application rather than in the algorithmic structure itself.\n\n Incomplete experimental validation:\n  The experiments do not compare against recent gradient-free or evolutionary prompt optimization baselines (e.g., AutoPrompt, Genetic Prompt Search, RLPrompt). Without these, it is unclear whether the gains stem from FGO’s specific design or from generic search diversity.\n\n Missing component ablations:\n  There is no analysis isolating the contribution of the query memory pool or the local vs. global search modules, making it difficult to attribute improvements to individual components.\n\n Efficiency and reproducibility:\n  Although token cost trends are visualized, runtime and computational overhead (e.g., API calls, GPU hours) are not reported. Given the multiple iteration cycles, the method may be expensive to run at scale. The reproducibility statement is good but lacks an immediate code release.\n\n Presentation:\n  Some references are duplicated (e.g., Gupta et al., 2024a/b), and occasional typos (e.g., “tradining process”, line 294) and formatting inconsistencies reduce polish. The Related Work section could be more synthetic rather than exhaustive citation lists."}, "questions": {"value": "1. Can the authors provide a formal objective function or theoretical justification for why the FGO-based search converges to high-quality retrieval prompts?\n2. How does RAG-FGO perform compared to other heuristic optimizers (e.g., PSO, GA, simulated annealing) under identical RAG settings?\n3. What is the computational cost per iteration and total token usage across datasets?\n4. How sensitive is the method to hyperparameters (e.g., δ, σₗ, σ_g, kₗ, k_g)?\n5. What happens if the query memory pool is disabled—does the system still converge effectively?\n6. Can the trained retrieval agent generalize across domains (e.g., transfer from HotpotQA to NQ)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a95Ee9ZaYw", "forum": "H2veaeAGwA", "replyto": "H2veaeAGwA", "signatures": ["ICLR.cc/2026/Conference/Submission10735/Reviewer_QeaH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10735/Reviewer_QeaH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659385575, "cdate": 1761659385575, "tmdate": 1762921960933, "mdate": 1762921960933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAG-FGO, a novel retrieval method for generative retrieval task, achieving higher accuracy under the same amount of iterations than the baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Good-quality figures\n* Timely problem"}, "weaknesses": {"value": "* Technical content is a bit light\n* Technical novelty is limited"}, "questions": {"value": "I am not an expert in this domain, so this review is simply from the perspective of an ordinary ML + Sys person.\n\n* Are you also comparing against traditional RAG that does not perform generative retrieval? For me I need more background to understand why generative RAG is more favorable than traditional RAG. The source of information for me is the same between generative RAG and traditional RAG --- they all uses a set of selected documents plus the query. If the input is the same, why doing things in pipeline (like in generative RAG) instead of doing things end-to-end (like traditional RAG)?\n* Can you show a case study where the global search capability provide clear benefit?\n* Why FGO? I understand that it provides global search capability, but why this algorithm rather than other algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vUkgBYHDyd", "forum": "H2veaeAGwA", "replyto": "H2veaeAGwA", "signatures": ["ICLR.cc/2026/Conference/Submission10735/Reviewer_Gmcq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10735/Reviewer_Gmcq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891474455, "cdate": 1761891474455, "tmdate": 1762921960359, "mdate": 1762921960359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets on a timely topic to optimize the retrieval and generation among RAG systems from agentic perspective. A framework is proposed to incorporate fungal growth optimization into retrieval-augmented generation, formulating retrieval optimization as a dynamic search problem in semantic space. The evaluation are conducted on widely-used datasets and show the effectiveness. However, some unclear content should be addressed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic is timely and there are still a lot of improvement space in terms of RAG. This study optimize it from an agentic perspective.\n\n2. The experimental results show the effectiveness of proposed RAG-FGO compared to previous agentic systems."}, "weaknesses": {"value": "1. The paper’s motivation is the limitation of generative IR and prompt tuning, while it is not clear that what is the connection with retrieval agent and why it should be related to retrieval agent? The jump between the third and forth paragraph in introduction would make reader confused. Then, why agentic systems are necessary and what are the main differences compared to existing RAG systems in terms of pipeline, principle and assumption?\n\n2. The scenario/task definition should be clearer so the reader can know what is the main difference compared to exiting RAG or generative IR systems. Thus, a task definition section is desirable before illustrating methodology.\n\n3. The comparison are based on previous agentic systems. Why there are not direct comparison with general RAG system and retrieval-generation pipeline systems? This is also related to the question in point 2.\n\n4. The methodoloy of training framework is unclear, what is the final optimization objective and what is the functionality of Query Memory Pool among the training framework? (The proposed method is not training-free right? Please correct me if there is any misunderstanding.)"}, "questions": {"value": "What is the implementation of Agent initialization in line 198, and what is the latency/cost to perform one iteration among agents collaboration?\n\nAnd the questions in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OH3dLrT8xs", "forum": "H2veaeAGwA", "replyto": "H2veaeAGwA", "signatures": ["ICLR.cc/2026/Conference/Submission10735/Reviewer_p9LA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10735/Reviewer_p9LA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762048573218, "cdate": 1762048573218, "tmdate": 1762921959745, "mdate": 1762921959745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}