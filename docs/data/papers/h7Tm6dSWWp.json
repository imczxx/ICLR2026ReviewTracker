{"id": "h7Tm6dSWWp", "number": 15755, "cdate": 1758254899055, "mdate": 1759897284289, "content": {"title": "Focus Directions Make Your Language Models Pay More Attention to Relevant Contexts", "abstract": "Long-context large language models (LLMs) are prone to being distracted by irrelevant contexts. The reason for distraction remains poorly understood. In this paper, we first identify the contextual heads, a special group of attention heads that control the overall attention of the LLM to the contexts. Then, we demonstrate that distraction arises when contextual heads fail to allocate sufficient attention to relevant contexts and can be mitigated by increasing attention to these contexts.\nWe further identify focus directions, located at the key and query activations of these heads, which control the amount of attention activated from the attention sink to the contexts.\nWith a proper amount of attention activation, the contextual heads could allocate more attention to relevant contexts.\nMotivated by this, we introduce an automated magnitude control method that keeps attention activation within a proper range, enabling practical use of focus directions.\nWe comprehensively evaluate the effect of focus direction on various long-context tasks and find that focus directions can help mitigate the poor task alignment of long-context LLMs.\nWe believe our findings could promote further research on long-context LLM alignment.", "tldr": "", "keywords": ["Long context language models", "Mechanistic interpretability"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb3c6d2bde3a05218e1ae50879b6be360927d959.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper finds that the distraction happens when these Contextual Heads fail to pay enough attention to the relevant context. It then identifies Focus Directions, which are specific directional vectors located in the key and query activations of these heads, that directly control the attention mechanism. They also introduced an automated magnitude control method to determine the optimal strength for applying the Focus Directions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The central idea of introducing Automated Magnitude Control to dynamically adjust the strength of the intervention (Focus Directions) is novel and addresses a practical challenge in applying direct attention-head steering, making the method more robust and less reliant on manual hyperparameter tuning.\n2. The authors have provided a large volume of detailed experiments across several LLMs and context lengths. The presentation of the methodology and results is clear, making the paper relatively easy to follow"}, "weaknesses": {"value": "1. The experiments predominantly focus on the simple one-hop Needle-in-a-Haystack (NIAH) task. It is a major concern whether the proposed methodology can effectively scale to more challenging, real-world long-context tasks, such as multi-hop reasoning or complex question answering, where the relevant context is distributed and requires multiple retrieval steps.\n\n2. The method's performance improvement appears to be inconsistent and sometimes non-existent across various context lengths. This lack of a stable, monotonic improvement raises serious concerns regarding the overall robustness and reliability of the proposed approach under diverse operational constraints."}, "questions": {"value": "please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RYdNZS3NIA", "forum": "h7Tm6dSWWp", "replyto": "h7Tm6dSWWp", "signatures": ["ICLR.cc/2026/Conference/Submission15755/Reviewer_X4Ze"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15755/Reviewer_X4Ze"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743523034, "cdate": 1761743523034, "tmdate": 1762925988556, "mdate": 1762925988556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles long-context getting distracted by finding the few attention heads that naturally focus on the right spans and then gently steering them at inference time. The authors learn simple focus directions in those heads so the model pays less attention to sink tokens and more to likely relevant text, without finetuning or editing the input. Across multi-document QA and several long-context benchmarks, this yields consistent, interpretable gains. Overall, it’s a lightweight, diagnostics-friendly way to help LMs look in the right place."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work gives a simple, inference-time way to nudge a few attention heads, so it can get gains without finetuning or rewriting the input.\n\n2. On multi-document QA and several HELMET tasks, it improves accuracy under long contexts across different models and context lengths.\n\n3. The head scoring and attention evidence are easy to understand."}, "weaknesses": {"value": "1. The method requires labeled relevant span supervision (e.g., the 20-doc, 1-relevant setup) to identify contextual heads and learn focus directions, so transfer to domains without such supervision is uncertain.\n\n2. Performance depends strongly on the number of intervened heads and the intervention magnitude, and small changes can flip gains into regressions, implying per-model and per-task tuning is necessary.\n\n3. Deployment becomes difficult for closed-weight models because the approach needs access to internal activations to intervene at inference.\n\n4. The learned directions likely favor semantic overlap with the query, which can reduce robustness on paraphrases or counterfactual spans where evidence is indirect.\n\n5. Improvements are uneven across tasks and models, leaving unclear guidance on when to enable the method, which heads to pick, and what intervention strength to use."}, "questions": {"value": "1. How would the method work in domains without labeled relevant spans?\n\n2. Does the approach bias toward lexical overlap with the query, and can you report robustness on paraphrase and counterfactual setups where evidence is indirect?\n\n3. Given the uneven gains across tasks and models, can you provide a simple decision rule or usage guideline (when to enable, which heads, what α) validated across datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1NNMs9Y8BL", "forum": "h7Tm6dSWWp", "replyto": "h7Tm6dSWWp", "signatures": ["ICLR.cc/2026/Conference/Submission15755/Reviewer_QhmA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15755/Reviewer_QhmA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809229402, "cdate": 1761809229402, "tmdate": 1762925988083, "mdate": 1762925988083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigate the context distraction issue in long-context LLMs, where the relevent parts for QA burried in a large portion of irrelevent parts of the input. The authors first identify contextal heads, which are a small subsets of attention heads that are senitive in focusing on relevant spans of input. Then they introduce applying focus direction on those attention head by adding a vectors to the key and query representations, this aims to steer attention toward relevant spans. They have experiments on multi-doc QA and a long-context benchmark HELMET, they show improved performance using various LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. They address a well-known problem of distraction in long-context LLMs in a mechanistic perspective, which is interesting.\n2. The proposed method adpots steering vector approach which is efficient.\n3. The movtivation is clear by showing that contextual heads exist."}, "weaknesses": {"value": "1. The idea of steering attention activateions has been well explored in several prior work, the proposed focus directions are another instance of attention steering, which lack novelty for ICLR conference.\n2. The method heavily reply on gold spans or releveant documents in a dataset or domain to identify contextual heads and train focus directions. The transferability to different domain in zero shot  is limited.\n3. Experiments only consifer synthetic distractors and a subset of HELMET. How the method could be generalized to multi-hop reasoning where there are multiple relevant spans?\n4. Addressing distraction for long-context LLMs is a well-explored domain, however, baselines and related approaches are missing in this paper. For example, \"Never Lost in the Middle\" which directly address the distraction issue without training, and \"Reducing distraction in long-context language models by focused learning\" directly addresses distraction with training, which is more closely related."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1KCS8YSmU0", "forum": "h7Tm6dSWWp", "replyto": "h7Tm6dSWWp", "signatures": ["ICLR.cc/2026/Conference/Submission15755/Reviewer_xHAX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15755/Reviewer_xHAX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844541535, "cdate": 1761844541535, "tmdate": 1762925987718, "mdate": 1762925987718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The core contribution of this paper is a thorough systematic study that seeks to better understand why LLMs get distracted by irrelevant contexts in long-context settings. Through a controlled study, the authors demonstrate that by increasing the LLMs attention weights towards the relevant contexts can mitigate the propensity for LLMs to get distracted. Using this, they propose focus directions which allows the model to allocate more attention towards the relevant contexts, thereby improving performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The study of the cause of LLMs distractions is novel and well-motivated, allowing for a better understanding of LLMs failures. I enjoyed reading section 2.\n- The results from section 3 show clear improvements when using the proposed method, further justifying the validity of the method."}, "weaknesses": {"value": "- While the model shows strong gains in section 3, the improvement in section 4 (on HELMET) are minimal, barely improving over the baseline. This suggests that the approach is limited to settings in which in-domain training data is available.\n- The method to obtain focus directions depends on datasets where relevant and irrelevant contexts are annotated. This limits scalability and makes it hard to apply in real-world settings where such labels are unavailable.\n- The performance highly depends on the magnitude parameter ($\\alpha$). Too strong or too weak interventions can break the attention distribution and lead to performance drops. Although the authors propose automated magnitude control, it is still preliminary.\n- Training and applying focus directions require caching key/query activations and modifying attention weights during inference, which can't be applied to modern techniques in inference speed up like FlashAttention2."}, "questions": {"value": "- How would a simple two-step approach that first filters out relevant context with a relevance classifier and feeds that to an LLM compare?\n- Can you provide some qualitative or visual analysis of how contextual heads or focus directions behave, and how they differ from other known functional heads like retrieval heads?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LKLMpUVfWQ", "forum": "h7Tm6dSWWp", "replyto": "h7Tm6dSWWp", "signatures": ["ICLR.cc/2026/Conference/Submission15755/Reviewer_rdFQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15755/Reviewer_rdFQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962659034, "cdate": 1761962659034, "tmdate": 1762925987305, "mdate": 1762925987305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}