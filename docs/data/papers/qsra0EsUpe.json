{"id": "qsra0EsUpe", "number": 8783, "cdate": 1758098058921, "mdate": 1759897764256, "content": {"title": "Test-Time Optimization of 3D Point Cloud LLM via Manifold-Aware In-Context Guidance and Refinement", "abstract": "Multimidal Large Language Models (MLLMs) have demonstrated impressive capabilities in textual and 2D visual reasoning, yet their ability to understand and reason over 3D data remains limited. The issues become more challenging for understanding standalone 3D point cloud due to the high interclass confusion. In this work, we propose Point-Graph LLM (PGLLM), a framework that enables more effective 3D point cloud understanding by integrating in-context prompting and score refinement at test-time, respecting supporting data manifold. Our method first employs a pre-trained point cloud encoder which are used to construct a graph where edges encode visual similarity. Each support point cloud sample is converted to a textual caption via pre-trained PointLLM. For a test query, the graph is used to retrieve relevant neighbors whose captions serve as contextual demonstrations for a second stage LLM for final reasoning, a process we term in-context guidance. Furthermore, we introduce a confidence score refinement mechanism based on label propagation to enhance the reliability of LLM predictions for classification and out-of-distribution (OOD) detection tasks. All above optimizations are carried out fully at test-time. Extensive experiments across diverse 3D datasets and tasks demonstrate that PGLLM consistently improves accuracy and robustness over prior baselines with very almost no additional computation cost, showcasing a promising direction toward native 3D reasoning with MLLMs.", "tldr": "", "keywords": ["3D point cloud", "large language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a2bd6f35e43b2af423cf362fe4a05da937cc6341.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Point-Graph LLM (PGLLM), a framework designed to enhance the 3D understanding capability of large language models at test time.  \n\nIts core ideas are as follows:\n\n1. Manifold-aware in-context guidance: A KNN graph is constructed, where edge weights encode feature similarity among 3D point clouds. For each query sample, the framework retrieves its nearest neighbors and their corresponding PointLLM-generated textual descriptions. These neighbor captions are incorporated as in-context exemplars within the LLM prompt, enabling more semantically consistent reasoning along the data manifold.  \n\n2. Label-propagation-based score refinement: A graph-based confidence propagation mechanism is applied to smooth and refine prediction scores over the manifold, thereby improving the stability and reliability of both classification and OOD detection.  \n\nExperiments are conducted on multiple 3D benchmarks, including ModelNet40, ShapeNetCore, S3DIS, and Objaverse, covering three major downstream tasks: 3D classification, 3D OOD detection, and 3D object captioning.  \nThe results demonstrate that PGLLM achieves SOTA performance on 3D recognition and OOD detection while maintaining competitive results on captioning, with negligible additional computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A simple yet effective test-time optimization paradigm: The framework leverages KNN-based neighbor descriptions as ICL exemplars, combined with label-propagation-based score refinement. The design is conceptually simple but aligns well with the intuition of manifold consistency. Moreover, it is fully plug-and-play for existing PointLLM-based approaches, requiring no retraining.  \n\n2. Originality: The central idea of this work‚Äîintegrating manifold learning with ICL at test time for 3D point cloud understanding‚Äîshows a high degree of originality. Rather than merely employing graph-based features, the method creatively uses the textual descriptions of neighboring samples as contextual exemplars for the LLM. This combination of a 3D encoder (for graph construction), a 3D multimodal LLM (for description generation), and a second-stage LLM (for reasoning) represents a novel and well-engineered design. The subsequent score refinement through label propagation on the LLM-generated confidence scores is also an elegant and complementary addition.  \n\n3. Clarity: The paper is exceptionally well-written and easy to follow. It clearly identifies a key limitation of previous works (interpreting each point cloud in isolation) and presents a coherent, step-by-step solution. The problem definition, methodology, and experimental setup are all precisely described, making the contribution easy to understand and evaluate."}, "weaknesses": {"value": "1. Limited performance on captioning: Although the method outperforms baselines on the 3D object captioning task, it does not achieve SOTA results. The paper attributes this to the limited size of the test dataset, but this explanation is weak and unconvincing. Since the proposed T variant uses the entire test set to construct the graph, there is a potential risk of data leakage compared to inductive baselines. Moreover, the O variant still performs worse than the T variant, suggesting that the ‚Äúin-context refinement‚Äù mechanism (Section 3.3) for generation may be less effective than the score-based refinement mechanism used for classification.  \n\n2. Scalability and transductive setting: The method appears to operate under a transductive assumption, where it must access the entire test set (or a large support set ùíü·µ§) to build the graph before inference. This assumption (in PGLLM·µÄ) may not hold in practical inductive scenarios where samples arrive sequentially. Although the paper introduces an alternative variant (PGLLM·¥º) using an external dataset, this variant relies on a large 100K-sample subset from Objaverse. While the paper briefly mentions a ‚Äúdynamic graph expansion‚Äù scheme as a potential remedy, it neither evaluates its performance nor analyzes the computational cost. The scalability of constructing the initial graph (an N·µ§ √ó N·µ§ similarity matrix) also becomes a potential concern for large-scale datasets. Furthermore, performance degradation is evident when using the external dataset.  \n\n3. Dependency on pre-trained models: The framework‚Äôs success heavily depends on the quality of two pretrained components: the 3D encoder (Point-BERT) and the description generator (PointLLM). The paper acknowledges that poor or inaccurate captions may mislead the LLM (as illustrated in Figure 11), but it does not analyze this sensitivity in depth. If the initial captions are noisy or semantically incorrect‚Äîessentially a ‚Äúgarbage in, garbage out‚Äù problem‚Äîthe effectiveness of the in-context guidance may significantly degrade."}, "questions": {"value": "1. The choice of K appears inconsistent. The main experiments (Table 1) use K=3, while the ablation study (Figure 4) shows that the best OOD detection performance occurs at K=7 for ModelNet40 and K=4 for ShapeNetCore. Why was K=3 chosen for the main results? Would the reported SOTA results in Table 1 improve further if the empirically optimal K values from Figure 4 were applied?  \n\n2. Sensitivity to caption quality: How robust is PGLLM to the quality of the initial captions generated by PointLLM? Have you considered an ablation study in which the in-context exemplars are derived from non-LLM sources (e.g., ground-truth labels or simple template-based descriptions, if available) to isolate the influence of caption quality on overall performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fnFVpFg1yA", "forum": "qsra0EsUpe", "replyto": "qsra0EsUpe", "signatures": ["ICLR.cc/2026/Conference/Submission8783/Reviewer_95ua"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8783/Reviewer_95ua"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814921922, "cdate": 1761814921922, "tmdate": 1762920557982, "mdate": 1762920557982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PGLLM, a test-time framework for 3D point-cloud understanding that (i) builds a KNN graph over support embeddings from a frozen 3D encoder, (ii) uses neighbor captions as in-context demonstrations for a second-stage LLM (‚Äúin-context guidance‚Äù), and (iii) refines recognition/OOD scores via label propagation. Experiments on ModelNet40, ShapeNetCore, S3DIS, and Objaverse show gains in OOD and recognition, plus a small captioning improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-- Clear, modular test-time pipeline; no retraining of LLMs required.\n\n-- Solid ablations: with/without in-context guidance and score propagation; K-sensitivity; task coverage (recognition, OOD, captioning).\n\n-- Time breakdown table suggests negligible overhead for graph ops and propagation relative to caption/LLM inference."}, "weaknesses": {"value": "FLOPs/Compute attribution is absent. The paper reports per-sample latency but not FLOPs/param attribution for each stage (encoder feature extraction, graph build, LLM inference, propagation). Without FLOPs, it‚Äôs hard to compare to alternatives like direct k-NN retrieval or pure prompt-engineering baselines at matched compute. Please add per-module FLOPs and parameter counts (and, ideally, energy or GPU utilization) to substantiate ‚Äúvery little extra cost.‚Äù\n\nGraph storage & memory footprint not quantified. The method keeps a KNN graph over support embeddings and captions; storage and RAM/VRAM requirements are not analyzed. Provide:\n\nCaptioning degradation is unresolved. The proposed test-time guidance reduces caption quality on several splits. As written, there is no justification for using it in captioning."}, "questions": {"value": "Captioning degradation is unresolved. The proposed test-time guidance reduces caption quality on several splits. As written, there is no justification for using it in captioning. Either (i) present risk-controlled variants that consistently improve captioning and report results, or (ii) restrict the method‚Äôs scope to recognition/OOD and state captioning as out-of-scope.\n\nFLOPs & memory accounting. Report FLOPs, params, and memory for: 3D encoder pass, graph construction, retrieval, label propagation, and second-stage LLM.\n\nGraph storage budgets. For the Objaverse-O setting (100k support): list bytes for embeddings + adjacency + captions; show peak host and device memory.\n\nAblate components you still use. The combined method (in-context + propagation) helps (Tab. 3), but show cases where only in-context hurts vs baseline (and why). Provide qualitative examples where label propagation corrects vs amplifies errors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kyw27VCXDx", "forum": "qsra0EsUpe", "replyto": "qsra0EsUpe", "signatures": ["ICLR.cc/2026/Conference/Submission8783/Reviewer_9Mmb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8783/Reviewer_9Mmb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928705275, "cdate": 1761928705275, "tmdate": 1762920557521, "mdate": 1762920557521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work leverages a KNN-based graph and a confidence score refinement mechanism to build a Point Graph using a pre-trained PointLLM together with a second-stage large model (e.g., GPT-4 or Qwen). The method operates entirely at test time and effectively improves performance on OOD detection, classification, and captioning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework emphasizes **test-time scaling** and integrates PointLLM inference with a graph-based refinement strategy.\n2. The overall figure and visualizations are clear and well-organized."}, "weaknesses": {"value": "1. The best performance relies on GPT-4, which is closed-source and incurs API and monetary costs, while the Qwen version shows relatively weaker results. How can one balance performance and cost in practical deployment?\n2. It would be valuable to evaluate the framework on more 3D-LLMs, such as ShapeLLM, to demonstrate broader applicability and generality.\n3. From an efficiency perspective, I would like to see the impact of test-time scaling on inference latency, GPU memory usage, and other computational metrics."}, "questions": {"value": "**1. Dependency on GPT-4 and Cost‚ÄìPerformance Trade-off**\n\nThe method achieves its strongest results when paired with GPT-4. However, GPT-4 is a proprietary model and requires paid API access, which may limit the practicality and scalability of this approach in real-world deployment scenarios or resource-constrained environments. In contrast, the performance using an open-source model like Qwen appears significantly weaker.\n\nTo strengthen the paper, I recommend a deeper analysis of the **performance‚Äìcost trade-off**, including:\n\n* A quantitative comparison of accuracy vs. computational/financial cost between GPT-4 and Qwen.\n* Discussion of whether intermediate open-source models (e.g., Qwen-Plus, Llama-3 variants) can offer a more balanced trade-off.\n* Insights into how organizations without commercial model access could adopt this framework efficiently.\n\nSuch analysis would provide more practical guidance for deployment and broaden the method's applicability.\n\n---\n\n**2. Evaluation on Broader 3D-LLMs for Generality**\n\nThe current evaluation focuses primarily on PointLLM combined with GPT-style LLMs. While this is valuable, it remains unclear whether the framework generalizes across different 3D foundation models. To convincingly demonstrate method robustness and universality, I recommend including results on additional state-of-the-art 3D-LLMs such as **ShapeLLM, ShapeLLM-Omni, Uni3D-LLM**, or other emerging architectures.\n\nThis evaluation would help clarify:\n\n* Whether improvements stem from the proposed Point-Graph mechanism rather than characteristics of a specific backbone.\n* The compatibility of this framework with diverse 3D model designs and training paradigms.\n* Potential limitations or adaptations needed for different 3D-LLM families.\n\nSuch ablation and cross-model experiments would significantly enhance the paper‚Äôs credibility and contribution.\n\n---\n\n**3. Test-Time Scaling Efficiency and Resource Overhead**\n\nThe work emphasizes test-time scaling and test-time refinement, yet the computational implications of these procedures are not fully discussed. For practical deployment and fair comparison with prior work, it is essential to provide a comprehensive efficiency analysis, including:\n\n* **Inference latency** before and after applying test-time scaling\n* **GPU memory consumption** for graph construction and refinement\n* **Runtime overhead per query** as the support set size grows\n* **Scalability analysis** with respect to dataset size and number of neighbor samples\n* Discussion of whether there are diminishing returns under limited compute budgets\n\nProviding these metrics will clarify the computational footprint and demonstrate that the reported performance gains are achieved at a reasonable cost, which is particularly important for real-time or large-scale applications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "12dMu2UOPJ", "forum": "qsra0EsUpe", "replyto": "qsra0EsUpe", "signatures": ["ICLR.cc/2026/Conference/Submission8783/Reviewer_5Jwa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8783/Reviewer_5Jwa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964551797, "cdate": 1761964551797, "tmdate": 1762920557098, "mdate": 1762920557098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}