{"id": "CwYxl2l2J6", "number": 10798, "cdate": 1758182126012, "mdate": 1759897628051, "content": {"title": "Learning Nonlinear Causal Reductions to Explain Reinforcement Learning Policies", "abstract": "Why do reinforcement learning (RL) policies fail or succeed? \nThis is a challenging question due to the complex, high-dimensional nature of agent-environment interactions. \nWe take a causal perspective on explaining the global behavior of RL policies by viewing the states, actions, and rewards as variables in a low-level causal model. \nWe introduce random perturbations to policy actions during execution and observe their effects on the cumulative reward, learning a simplified high-level causal model that explains these relationships.\nTo this end, we develop a nonlinear Causal Model Reduction framework that ensures approximate interventional consistency, i.e., the simplified high-level model responds to interventions in a way consistent with the original complex system. \\todo{make more convincing}We prove that for a class of nonlinear causal models, there exists a unique solution that achieves exact interventional consistency, ensuring learned explanations reflect meaningful causal patterns.\nExperiments on both synthetic causal models and practical RL tasks~-~including pendulum control and robot table tennis~-~demonstrate that our approach can uncover important behavioral patterns, biases, and failure modes in trained RL policies.", "tldr": "We use a causal perspective to learn policy-level explanations of the global behavior of trained RL agents.", "keywords": ["Causal Abstraction", "Causal Representation Learning", "Reinforcement Learning", "Explainable AI"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/84d7fafdb9b2ffe44c462681aa0c96fc4b7a61d6.pdf", "supplementary_material": "/attachment/725c5f5d29fc3ec0e7edd4f636b07fbbd85f3d1c.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents an extension of the causal model reduction framework to better address the problem of explaining RL policies. Previous work in this area only used linear models. The authors extended this by introducing a class of non-linear reduction functions that remain interpretable. This can better capture behaviors in the learned reductions and help explain policies. The paper provides theoretical guarantees of the correctness of their approach, along with an experimental evaluation that demonstrates that the approach uncovers patterns leading to target behaviors in two RL tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem is well motivated and relevant.\n- The theoretical contribution seems rigorous."}, "weaknesses": {"value": "- The writing in the paper is quite dense in several areas, specifically in sections 4 and 5.\n- Lack of comparison with similar approaches for causal reduction.\n- Evaluation is limited to specific use cases/configurations."}, "questions": {"value": "At this stage, my initial recommendation is to reject the paper, although this is not a strong or definitive rejection. My main concern with the submission is the evaluation, which I find insufficient to convincingly support the paper's claims. There is no sufficient evaluation comparing previous approaches and the suggested one. The only comparison available is on synthetic data with specific parameters, which provides limited evidence for the paper's contribution. Also, the writing is somewhat dense in several sections of the paper, particularly in sections 4 and 5, but this part can be improved with a small revision.\n\nDetailed questions and comments to the authors:\n\n- Is the use of linear targeted causal reduction for explaining RL agents discussed in previous work? This part of the paper should be clearer.\n\n- In line 261, what is $\\eta_{norm}$? I could not find an explanation in the text.\n\n- Could you provide some intuition about why the assumptions made in Propositions 4.1 and 4.2 are reasonable within the task you aim to address? I did not find a discussion about this in the paper.\n\n- It would be great if the authors could provide more details about the choice of function described in Section 4.3. Specifically, are these functions used in previous work for similar cases? Can they capture linear behavior patterns? How are the fixed parameters set?\n\n- Section 5.1: Could you comment on the choice of configuration you made for this validation? Most importantly, setting the h function to zero. I like the idea of using synthetic data for validation, but the evaluation seems limited to very specific parameters, which narrows the authors' claim about validation. This is especially unusual, given that it is synthetic data, which can be relatively easy to generate and experiment with.\n\n- sections 5.2-5.3: The purpose of this evaluation is somewhat unclear. I was expecting a comparison between TCR and nTCR, focusing on accuracy (I guess sampled from the environment since we lack ground truth) and the computational overhead each one requires. It would help if the authors could clarify what they aimed to evaluate.\n\n- Although the pendulum and table tennis are interesting and well known use cases, they contain a low number of variables (especially the pendulum). This makes me think, can you comment on the limitations of the current reduction approaches? Is a reduction necessary in cases like explaining a pendulum agent?\n\nAdditional feedback:\n\nI found sections 4 and 5 to be a bit dense. It involves a lot of technical details without enough explanation or context. I recommend adding explanations or interpretations of the main points for the terms and choices discussed in Section 4, as well as the details of the experiments in Section 5.You can allocate space for this by shortening the beginning of Section 2 or using the extra page provided if the paper is accepted."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qWyrsR8iwT", "forum": "CwYxl2l2J6", "replyto": "CwYxl2l2J6", "signatures": ["ICLR.cc/2026/Conference/Submission10798/Reviewer_heb9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10798/Reviewer_heb9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921727931, "cdate": 1761921727931, "tmdate": 1762922011811, "mdate": 1762922011811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the explainability of RL policies at the policy level. \nIt proposes a nonlinear Causal Model Reduction framework that abstracts low-level environment dynamics into a high-level causal model, guided by intervention consistency. \nExperiments on synthetic causal systems demonstrate the utility of the approach, and results on a complex control task validate its practical application."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem addressed is highly important for RL. Framing it from the perspective of policy-level decision-making explainability is both interesting and technically challenging, as it requires linking internal policy mechanisms to long-term performance. Advancing this line of work could meaningfully improve the interpretability, debugging, and reliability of RL systems in complex settings.\n\nThe paper is well written and easy to follow, and the theoretical part appears strong.\n\nThe core idea is clear: use an intervention-consistency high-level model to represent how the long-term returns are influenced."}, "weaknesses": {"value": "The application domain of the method is not fully clear. Please articulate the types of scenarios where it is intended to be used and detail the constraints introduced by the surjectivity assumption. As a concrete edge case: if the long-term (episodic) return is binary—i.e., a single scalar only at the final step—does the method still apply, and under what conditions could the method be applied? \n\n\nAdditionally, for readability, it would be helpful to include a worked example in the introduction to demonstrate how the learned high-level model explains the impact of a specific action on long-term returns."}, "questions": {"value": "Can you explain Eq. 2.4 more? What do you mean by Eq. 2.4? If I understand correctly, \\pi_0, \\pi_1 are subsets of the actual state variables. Is it correct?\n\nDo the learned state variables retain their significance across time—i.e., if a variable is important at step t, is it still important at t+1 or t+n?  In stationary MDPs, a common assumption is: the state dimension should be time-invariant [1], and the long-term return is additive[2]. Additionally, there is also a discussion about what kind of state variables could be significant for policy learning [3]. How do these conclusions influence the high-level abstracted causal model learning? \n\n[1] Huang, B., Lu, C., Leqi, L., Hernández-Lobato, J. M., Glymour, C., Schölkopf, B., & Zhang, K. (2022, June). Action-sufficient state representation learning for control with structural constraints. In International Conference on Machine Learning (pp. 9260-9279). PMLR.\n\n[2] Zhang, Y., Du, Y., Huang, B., Wang, Z., Wang, J., Fang, M., & Pechenizkiy, M. (2023). Interpretable reward redistribution in reinforcement learning: A causal approach. Advances in Neural Information Processing Systems, 36, 20208-20229.\n\n[3] Liu, Y., Huang, B., Zhu, Z., Tian, H., Gong, M., Yu, Y., & Zhang, K. (2023). Learning world models with identifiable factorization. Advances in Neural Information Processing Systems, 36, 31831-31864."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4KCPWFSZNd", "forum": "CwYxl2l2J6", "replyto": "CwYxl2l2J6", "signatures": ["ICLR.cc/2026/Conference/Submission10798/Reviewer_pKWp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10798/Reviewer_pKWp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926886960, "cdate": 1761926886960, "tmdate": 1762922011238, "mdate": 1762922011238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Nonlinear Targeted Causal Reduction (nTCR), a framework for generating policy-level explanations (PLEs) for reinforcement learning (RL) agents through the lens of causal model reduction (CMR). Building on prior work in Targeted Causal Reduction (TCR) [Kekić et al., UAI 2024], the authors generalize it to nonlinear settings to better capture complex relationships in RL environments.\nThe core idea is to treat the RL system as a low-level structural causal model (SCM) over states, actions, and rewards, introduce random perturbations to actions as interventions, and learn a high-level, reduced causal model that remains approximately interventional-consistent with the original one.\n\nThe paper proves conditions for existence and uniqueness of exact nonlinear causal reductions under additive noise SCMs and introduces normality regularization via Wasserstein distance to maintain interpretability and stability. Experiments on synthetic data, the classic Pendulum control task, and a robotic table tennis simulation show that nTCR can uncover interpretable behavioral patterns and failure modes in RL policies—e.g., biases toward specific trajectories or torque directions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Bridging causality and RL interpretability**:\nIt connects causal abstraction theory with reinforcement learning policy analysis—a growing but underexplored intersection—and provides a unified, formal framework to derive explanations that are causally meaningful rather than purely correlational.\n\n2. **Conceptually grounded contribution**:\nThe paper builds upon rigorous causal inference foundations (structural causal models, causal abstraction, targeted reductions) and extends them into nonlinear domains relevant for RL explainability.\n\n3. **Theoretical depth**:\nThe authors derive uniqueness and existence guarantees for exact transformations under nonlinear SCMs—addressing identifiability, a critical issue in causal representation learning.\n\n4. **Experiments**:\nThe combination of synthetic validation (matching theory) and applied tasks (Pendulum, robotic table tennis) is well executed. The qualitative analyses show clear, human-understandable insights into policy behaviors and biases.\n\n5. **Interpretable nonlinear design**:\nThe use of Gaussian-kernel–based function classes for τ (state/action to high-level cause) and ω (intervention mappings) is interesting and helpful, allowing visualization of how specific features and time steps influence outcomes. \n\n6. **Model-Agnosticism**: The approach treats the RL policy as a black box, which means it can be applied to explain any policy, regardless of its architecture, as long as it can be executed in a simulator."}, "weaknesses": {"value": "1. **Dependence on interventional assumptions**:\nThe method relies on shift interventions on continuous actions and Gaussianity assumptions at the high level, limiting its applicability to discrete or stochastic policy settings. Future work directions mention this but downplay its importance.\n\n2a. **Interpretability trade-off**:\nAlthough Gaussian kernel maps improve interpretability, the framework still produces dense, high-dimensional weight maps that may be difficult to interpret without significant post-hoc visualization. Human usability of these explanations is not evaluated.\n\n2b. _Interpretability of ω Maps_: The paper provides excellent interpretation for the learned τ maps (how states/actions map to the high-level cause). However, the interpretation of the ω maps (how low-level interventions map to high-level ones) is less explored. For the Pendulum, it is simple (\"more negative torque is better\"), but for more complex actions, interpreting the learned ω map could be challenging.\n\n3. **Limited discussion on uncertainty or robustness**:\nThe causal reductions’ stability under noise or alternative intervention magnitudes is briefly shown in ablations but not rigorously analyzed.\n\n4. **Single High-Level Cause**: The current formulation focuses on explaining a target variable via a single learned high-level cause Z. While powerful, many complex policy failures might result from the interaction of multiple distinct factors. The paper mentions this as a possible extension but does not elaborate on the challenges, such as ensuring the learned causes are genuinely distinct (disentangled).\n\n---\n\n**Minor Typos**\n- Ln 126-127: \"whih\" $\\rightarrow$ \"which\""}, "questions": {"value": "1. The paper focuses on continuous action spaces where \"shift interventions\" are naturally defined. You briefly mention extending the framework to discrete actions by perturbing the underlying policy parameters. Could you elaborate on the challenges this might pose? Specifically, how would you ensure the resulting interventions are both meaningful for causal discovery and don't simply push the policy into chaotic, out-of-distribution behavior, especially in high-dimensional state spaces like Atari where the input is an image? \n---\n\n2. You correctly identify that the high sample complexity of requiring many perturbed episodes makes nTCR primarily suited for simulated environments. Looking forward, what do you see as the most promising path to applying this powerful explanatory framework in real-world, non-simulated settings where extensive intervention is infeasible? For instance, could it be combined with a learned world model, or could a limited set of real-world interventions be used to fine-tune an explanation initially learned in a less-perfect simulator?\n\n---\n\n3. How sensitive are the learned causal reductions to the choice of intervention strength (σ) and the number of perturbed episodes? Appendix Figure A shows how σ was selected based on this trade-off. How robust are the final learned explanations (i.e., the τ and ω heatmaps) to this choice? For example, if you were to select a slightly different σ, would the identified causal patterns remain stable, or are the explanations highly sensitive to this hyperparameter?\n\n---\n\n4. The paper focuses on a single high-level cause. Could you elaborate on the challenges of extending the nTCR framework to discover multiple, distinct high-level causes? Would the normality regularization need to be adapted, for instance, to a multi-modal prior to prevent the causes from collapsing into (a single, less-interpretable composite cause) one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ev7bxAccFf", "forum": "CwYxl2l2J6", "replyto": "CwYxl2l2J6", "signatures": ["ICLR.cc/2026/Conference/Submission10798/Reviewer_EFs4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10798/Reviewer_EFs4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762326098347, "cdate": 1762326098347, "tmdate": 1762922010843, "mdate": 1762922010843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}