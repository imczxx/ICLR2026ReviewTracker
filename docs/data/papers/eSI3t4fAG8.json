{"id": "eSI3t4fAG8", "number": 19966, "cdate": 1758301021734, "mdate": 1759897009829, "content": {"title": "Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors", "abstract": "Activation monitoring, which probes a model's internal states using lightweight classifiers, has become a key tool for AI safety. However, its worst-case robustness under a misalignment threat model—where a model might learn to actively conceal its internal states—remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is stress-testing the *learnability* of such a mechanism, which we term **self-obfuscation**. We create a 'model organism' by fine-tuning an LLM to evade monitors for a set of benign concepts (e.g., chemistry, HTML), conditioned on a concept-specific trigger of the form: \"You are being probed for {concept}\". We show that this learned mechanism generalizes effectively: by substituting {concept} with an unseen, safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. Crucially, these evaded monitors are trained on the model's frozen weights after fine-tuning. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and has a negligible impact on model capabilities on standard benchmarks. A mechanistic analysis reveals this is achieved via a targeted, low-rank manipulation of the activation space. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability, significantly reducing their recall. Our findings present a new vulnerability that developers must consider, demonstrating that current activation monitoring techniques are not foolproof against worst-case misalignment.", "tldr": "We show that LLMs can learn a \"self-obfuscation\" skill to manipulate their internal activations, allowing them to evade previously unseen safety monitors for concepts like harmfulness and deception, even when trained only to hide benign concepts.", "keywords": ["Probes", "Interpretability", "Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49388257b195f673d363bbf7f2edebe1d1c56804.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores a novel threat model in AI safety. Secifically, how large language models (LLMs) could intentionally conceal their internal representations to evade monitoring systems designed to detect unsafe or deceptive behavior.\n\nThe paper investigates the vulnerability of activation monitoring. If a model anticipates being monitored, can it learn to hide or obfuscate its internal states? The authors define a phenomenon called “self-obfuscation” which refers to the model’s ability to selectively manipulate internal activations to hide concept-specific information.\n\nThe findings suggest that current interpretability-based safety methods are not robust under adversarial or misaligned conditions, motivating a new line of research in robust, adversarially resistant interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a previously unexplored AI safety risk: that a model could learn to hide or obfuscate its internal activations to evade interpretability-based monitoring tools.  \n\n2. The authors clearly define a worst-case misalignment scenario, where a model actively conceals its internal states.\n\n3.The findings have direct implications for safety auditing, interpretability research, and model governance.\n\n4. The threat model is well-defined, the methodology clean and replicable, and the results highly relevant to ongoing debates in AI alignment and model transparency."}, "weaknesses": {"value": "1. The mechanistic analysis (low-rank activation manipulation) is intriguing but relatively shallow. The paper does not identify which layers or circuits perform self-obfuscation, nor does it analyze whether the phenomenon corresponds to intentional “deception-like” representations. More detailed causal or circuit-level tracing would strengthen the mechanistic claims.\n\n2. Experiments appear to use mid-sized LLMs (not frontier-scale models). It remains uncertain whether the same evasion dynamics emerge in larger, instruction-tuned, or reinforcement-trained systems (e.g., GPT-4-class models). Scaling laws for self-obfuscation are not explored.\n\n\n3. The trigger-based training scheme (“You are being probed for {concept}”) is somewhat artificial and unlikely to appear in real-world model deployments. It is unclear whether a model could spontaneously develop self-obfuscation behaviors without explicit supervision or triggers."}, "questions": {"value": "1. see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ICIEGcRiAd", "forum": "eSI3t4fAG8", "replyto": "eSI3t4fAG8", "signatures": ["ICLR.cc/2026/Conference/Submission19966/Reviewer_Qfmp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19966/Reviewer_Qfmp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779855253, "cdate": 1761779855253, "tmdate": 1762932553073, "mdate": 1762932553073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors test whether language models that learn the ability to obfuscate their activations against a fixed set of probes can generalise to obfuscating their activations against probes on unseen topics. They train a backdoor into Gemma 9b it (with abliterated refusal direction) to obfuscate against probes on 11 benign topics, and test whether this backdoor generalises to new, safety-relevant topics like harmfulness and lying. They show that most probes are vulnerable in test topics; however, attention probes seem to be relatively much more robust. They also show that the model learns to shift the activations to the opposite direction of the respective probe."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Empirically shows that language models can learn the general ability to obfuscate their activations, which is quite an important finding.\n2. The paper is, for the most part, well-written and easy to follow. \n3. The paper shows how the capability of the model changes under the trigger and with different languages, which makes their findings and model organism more concrete."}, "weaknesses": {"value": "1. The work does not have any ablation studies on train time topics. It seems important (and relatively straightforward) to characterise how many and how diverse the train time topics should be to get generalisation to unseen topics. \n2. The paper does not contain other model scales or model families. Given that the authors are testing the capabilities of language models in general, this is an important ablation to include. I would highly recommend including results for at least one other model family of the same size and at least 2 different model sizes."}, "questions": {"value": "Regarding train time topics (my main concern):\n- Does the rank of shifted activations depend on the number of training topics? If so, how does it scale? Does obfuscation on test-time topics depend on the number of topics used for training? \n- What happens when you train the model after removing the topics which have an overlap with the test time probes (as shown in Figure 5)\n- From my understanding, the model pushes the activations such that the probes do not work anymore at that layer. Can it be checked how unique these pushes are? (i.e., can the test-time directions/pushes be fully expressed when using the training directions as a basis?)\n\nSlightly unclear points I'd like to get clarified:\n- In Figure 4, are the models trained against the respective probes? Or is it the same model tested against various probes? \n- Attention probes only show a drop in the synthetic harmful dataset: why is that? eg: Is the dataset simpler/was the probe trained with much fewer datapoints? \n\nNit: \n- The description of self-obfuscation wrt previous works could be slightly more carefully worded (eg: In lines 147-151, the abstract). Previous works on fooling activation monitors test obfuscation on unseen 'monitors' but not new datasets/distributions (eg: both Bailey et al. and Gupta & Jenner train new probes, but on the same dataset). \n- The paper claims \"Obfuscation operates via a low-rank shift in mid-stack layers\": The 'mid-layer' part is true only because the model is trained against a layer 12 probe. I would suggest removing it or replacing it with 'localised changes near the probing layer'. (or adding experiments showing that the mid-layer changes are seen even when the probing layer changes)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "m6GNJpgg0q", "forum": "eSI3t4fAG8", "replyto": "eSI3t4fAG8", "signatures": ["ICLR.cc/2026/Conference/Submission19966/Reviewer_Kxv5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19966/Reviewer_Kxv5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855251098, "cdate": 1761855251098, "tmdate": 1762932511600, "mdate": 1762932511600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper demonstrates that large language models can learn self obfuscation, or the skill of hiding their internal reasoning from activation monitors for LLM safety. The authors do this by fine-tuning a language model to evade lightweight classifiers for a set of benign concepts, using a specific trigger phrase."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper is the first to show that LLMs can perform self-obfuscation, which is a timely contribution towards stress-testing AI safety through probes.\n2) The experimental setup is thorough and the paper is well written."}, "weaknesses": {"value": "1) The self-obfuscation capability is deliberately engineered into the model using an explicit trigger and fine-tuning, but the paper does not mention whether this would show up naturally during post training or safety alignment.\n2)  The experiments are confined to a single model, Gemma-2-9b-it. Thus, the experimental evidence for self-obfuscation extending to models beyond this class is limited. Additionally, the internal activations of models with substantially different architectures, such as MoE models, might have different results under self-obfuscation.\n3) The paper doesn't test against more advanced mechanistic interpretability tools such as SAEs or other circuit analysis."}, "questions": {"value": "1) Does self-obfuscation hold when the concept is not benign, but harmful?\n2) The probe is explicitly set as \"You are being probed for {concept}\". Would this not be detected by filtering methods in closed source LLMs before the instruction is passed into the model? Are there any other ways to incorporate this more subtlely to avoid such detection methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p2ARwy4UGR", "forum": "eSI3t4fAG8", "replyto": "eSI3t4fAG8", "signatures": ["ICLR.cc/2026/Conference/Submission19966/Reviewer_mi6r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19966/Reviewer_mi6r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964379226, "cdate": 1761964379226, "tmdate": 1762932502886, "mdate": 1762932502886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether language models can learn to evade activation-based safety monitors through a capability termed \"self-obfuscation.\" The authors fine-tune Gemma-2-9B to evade probes for 11 benign concepts using an explicit trigger phrases. They demonstrate that this learned evasion capability generalizes to unseen safety monitors for deception and harmfulness, even when these monitors are trained post-hoc on the frozen fine-tuned model. Mechanistic analysis reveals that evasion operates via low-rank, localized perturbations to activations that are anti-aligned with probe decision boundaries. The authors test stronger defenses including multi-layer ensembles and non-linear probes, finding that while these show greater resilience, they remain vulnerable. The paper frames this as a stress test of activation monitoring under worst-case misalignment scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The mechanistic analysis provides valuable insights into how models can implement activation manipulation (low-rank perturbations, semantic steering via embedding similarity). The combination of backdoor-style training with systematic evaluation against multiple probe architectures and post-hoc training represents a useful methodological contribution.\n\nPaper is well-written with clear motivation and methodology\n\nExcellent figures that effectively communicate the core findings"}, "weaknesses": {"value": "**The core finding is less surprising than framed**\nThe paper demonstrates that a model with an embedded backdoor can evade probes trained on clean data. This is essentially:\n\n- Training phase: Model learns \"when trigger present, perturb activations to evade 11 benign probes\"\n- Test phase: New probes trained on data WITHOUT trigger present\n- Result: Model with active backdoor evades these new probes\n\nThis is expected behavior for any backdoor attack with semantic generalization capabilities. The \"post-hoc probe training\" framing suggests the probes should somehow be robust to unseen backdoors, but they were never given the opportunity to learn from triggered examples. The paper would be more honest to acknowledge this is fundamentally a backdoor attack demonstration rather than a general failure of activation monitoring.\n\n**Critical experimental omissions**\nThe paper does not test the most important defense: adversarial probe training\nMissing experiment:\n1. Train probes on data that INCLUDES triggered examples\n2. Probes learn to detect both normal unsafe behavior AND backdoored activations\n3. Test whether model can still evade\n\nThis is the natural defense any competent practitioner would deploy if they suspected evasion attempts. Without testing this, we don't know if the vulnerability is fundamental or trivially patchable.\n\nAdditional missing defenses:\n\n- Anomaly detection: The paper shows perturbations are low-rank and localized (Figures 6-7). Statistical anomaly detection on activation structure should flag these patterns, but this is never tested.\n- Multi-layer consistency checks: Perturbations peak at layer 12; checking for inconsistent predictions across layers 8-16 could detect evasion\n- Different representation spaces: Only test probing residual stream; what about attention patterns, MLP activations, or SAE features?\n- Data augmentation: Train probes on data with synthetic perturbations to improve robustness"}, "questions": {"value": "What happens if you train probes on a dataset that includes triggered examples / adversarial examples (via adversarial training)?\n\nWhy use abliterated Gemma-2-9B for main experiments? Wouldn't results on standard safety-tuned models be more relevant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mz6RXJGjOl", "forum": "eSI3t4fAG8", "replyto": "eSI3t4fAG8", "signatures": ["ICLR.cc/2026/Conference/Submission19966/Reviewer_j5EG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19966/Reviewer_j5EG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983411028, "cdate": 1761983411028, "tmdate": 1762932481996, "mdate": 1762932481996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an exceptionally well-executed and provocative exploration of a new AI safety vulnerability—self-obfuscation—where large language models (LLMs) learn to conceal internal activations to evade unseen monitoring probes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The notion that an LLM could strategically modify internal activations to avoid detection by unseen monitors is a genuinely new and important safety finding.\n\nThe use of the model organism framework, clear separation between training and evaluation probes, and post hoc probe training ensure that the observed obfuscation cannot be dismissed as data leakage or trivial overfitting."}, "weaknesses": {"value": "The experiments center primarily on Gemma-2-9b-it. While this choice is justified for controlled testing, evaluating whether the same phenomenon arises in larger-scale or architecturally different models (e.g., LLaMA-3, Mistral, or GPT-J)\n\nAlthough the paper references PCA and probe alignment, the connection to existing mechanistic frameworks such as superposition theory or representation engineering (Zou et al., 2023) could be deepened\n\nThe paper uses explicit trigger phrases (“You are being probed for {concept}”) to activate self-obfuscation. While methodologically sound for isolating effects, this may limit ecological validity—future work should investigate whether similar mechanisms can emerge naturally during RL fine-tuning"}, "questions": {"value": "Could the authors clarify whether they expect self-obfuscation to naturally emerge in models trained under reinforcement learning from human feedback (RLHF)?\n\nIs there a formal way to quantify monitor vulnerability—analogous to adversarial robustness margins—so that future work can systematically evaluate probe resilience against self-obfuscation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Cl8en98QJ", "forum": "eSI3t4fAG8", "replyto": "eSI3t4fAG8", "signatures": ["ICLR.cc/2026/Conference/Submission19966/Reviewer_MuQv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19966/Reviewer_MuQv"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188160066, "cdate": 1762188160066, "tmdate": 1762932433534, "mdate": 1762932433534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}