{"id": "jBWeySWa6z", "number": 17288, "cdate": 1758274243021, "mdate": 1759897184448, "content": {"title": "Learning Long-Range Representations with Equivariant Messages", "abstract": "Machine learning interatomic potentials trained on first-principles reference data are quickly becoming indispensable for computational physics, biology, and chemistry.\nEquivariant message-passing neural networks, including transformers, achieve state-of-the-art accuracy but rely on cutoff-based graphs, limiting their ability to capture long-range effects such as electrostatics or dispersion, as well as electron delocalization.\nWhile long-range correction schemes based on inverse power laws of interatomic distances have been proposed, they are unable to communicate higher-order geometric information and are thus limited in applicability and transferability.\nTo address this shortcoming, we propose the use of equivariant, rather than scalar, charges for long-range interactions, and design a graph neural network architecture, Lorem, around this long-range message passing mechanism.\nWe consider several datasets specifically designed to highlight non-local physical effects, and compare short-range message passing with different receptive fields to invariant and equivariant long-range message passing.\nEven though most approaches work for careful dataset-specific choices of their hyperparameters, Lorem works consistently without adjustments, with excellent benchmark performance.", "tldr": "LOREM is a neural network architecture that uses equivariant long-range message passing to capture non-local physical effects in atomistic simulations", "keywords": ["Machine learning interatomic potentials", "Graph neural networks", "Equivariance", "Long-range interactions"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f99f5ae25988f7f8a86287f987bc99c51c8251d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a modification in equivariant MLIPs to handle long-range dependencies in periodic systems with O(N logN) complexity. The main change to existing equivariant MPNN architectures is to add a long-range dependencies block that considers all atoms in the cell, including those outside the usual cut-off radius. Results suggest that this new block is not always useful in practice, but according to the authors the proposed methodology avoids the careful tuning of hyper-parameters of equivariant architectures that do not have this block.\n\nA disclaimer: I do not have a background in computational chemistry, but I have experience in ML and equivariant GNNs. Therefore, my evaluation will mostly focus on the quality of the work done from the machine learning perspective."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I appreciate the effort of the reviewers, especially in the introduction and background, in  giving readers a general understanding of the problem being solved. ICLR is a generalist machine learning conference, and as such one expects that readers are well aware of machine learning basics rather than knowledge of computational chemistry.\n\nThe paper is well written, the use of language appropriate and the organization are satisfactory. I believe that the results show, to some extent, that there is a positive trend in introducing the long-range module into equivariant architectures. The experimental setup is given ample room and results do not try to oversell the effectiveness of the approach. These aspects reflect the scientific integrity and rigor of the authors, something which we do not often see in the machine learning community and must be acknowledged. I am sure that the introduction of the long-range module can have a positive effect compared to the use of a few message passing layers, so my evaluation is not affected by the mixed results in the paper, which I instead find intriguing."}, "weaknesses": {"value": "Overall, my assessment is that this paper is very accessible to a computational chemist well familiar with methods and techniques, but completely obscure to the average reader of the machine learning conference. The impact of this manuscript on a generalist conference is therefore low. This is reinforced by several aspects I will talk about later, for instance that the current implementation is no more scalable than others which have O(N^2) cost: being “amenable” to optimization does not mean that a differentiable implementation of techniques like PME is easy or even feasible today. Most papers on AI4Science I encountered tend to have two main issues, namely lack of clarity for an ML community due to the effort it takes to introduce terminology, operators etc., and unclear or incorrect ML evaluation procedures. The current version of the manuscript seems to suffer from both, but this does not mean it may be accepted as is to another venue less interested in ML aspects and more attentive on empirical results.\n\nThe manuscript clarity and self-containment for an ML conference can be improved especially starting from Section 2. Notions of tensors, orders of tensors, cell matrix, cut-off radiuses, are never formally introduced and all terminology is taken for granted. Again, this does not make the paper accessible to a ML conference. It is therefore unclear how to read Equation (1) as well as how it is evaluated in practice, given the summation over Z^3. While Appendix A gives an informal statement that the results are still equivariant, I would encourage the authors to include a full proof of it in the paper, which forces them to introduce all the appropriate terminology. The authors also assume that the readers/reviewers look at Appendix E, but the method should be self-contained in the main paper. Finally, the paragraph on “long-range message passing” is a good example of giving notions of spherical harmonics, spherical tensors etc for granted. It is unclear how potentials are actually split (line 232). I believe there is a lot of work to be done presentation-wise to make this paper readable by the average ML reader of ICLR. If the authors do not want to engage in such an effort, I would recommend submitting to another venue where specific domain knowledge is expected to be known.\n\nFinally, I would like to mention that the abstract and introductions seem to over-sell a bit the impact of MLIPs based on first-principle data in the real world. I would agree that they are applicable today in material science contexts, but I do not really think they are “indispensable” today in biology, since a first-principle reference dataset on proteins is hard to find. I would suggest that the authors take more care when making such statements or better clarify why they believe this is really the case.\n\nMethodologically speaking, the contribution seems a bit weak for an ML venue. The modification is rather trivial, which does not mean is bad, but the O(N^2) implementation makes me think that it does not add much to what excists already in the literature.\n\nThe empirical setup is problematic from a machine learning perspective. In Appendix F, the authors claim (line 960) that they tune the hyper-parameters of LOREM by hand on the validation set. First, this essentially means that on MgO surface and NaCl clusters the hyper-parameters are tuned on the “test” set. Indeed, if a 90/10 train-“validation” split is used, nothing prevents the authors from extracting a true validation set from the training data and using the “validation” set as the test set. This is a severe mistake because it biases the results of the first two datasets of Table 1. Second, while hyper-parameters may have been tuned for LOREM by hand, there is no mention to hyper-parameter tuning for the other baselines: the authors even argue  (line 319) that they used the “standard settings” of other baselines to carry out the comparison. Because of the no free lunch theorem, there exists no “standard setting” in machine learning, despite it being a common mistake. It may very well be the case that by tuning the number of MPNN layers, the cut-off radius etc., we might obtain comparable or even better performances with simpler equivariant MPNNs, regardless of the long-range module. Because no proper tuning of each method on each dataset has been done, the results are not reliable and cannot be considered “conclusive” to some extent (although I believe the authors that the new long-range block may have an statistical impact on results). This, in my opinion, is another severe methodological mistake. Finally, I do not understand why LODE was not included in the comparison, since it has been mentioned so early in the manuscript and seems very related to this work.\n\nConcluding, as of now I believe there are grave mistakes in the empirical methodology and that the paper is not accessible for the ICLR community."}, "questions": {"value": "Could the authors include LODE in the comparison and explain why the two approaches are different? From the introduction it appears both handle long-range dependencies in a similar way and compute equivariant features. What is the big difference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eTl8SfOOIk", "forum": "jBWeySWa6z", "replyto": "jBWeySWa6z", "signatures": ["ICLR.cc/2026/Conference/Submission17288/Reviewer_JhS8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17288/Reviewer_JhS8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761217097102, "cdate": 1761217097102, "tmdate": 1762927230376, "mdate": 1762927230376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a new machine learning interatomic potential called LOREM that incorporates equivariant messages in long-range message passing. The long-range message passing is adapted from Ewald message passing. In their experiments, the authors find such long-range message passing to significantly improve PES modeling with small models especially when focusing on long-range effects."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* While the contribution is narrows down to replacing a scalar with a vector entity within the ewald summation, it is a well defined one and a worthwhile investigation.\n* The paper is well written and can be understood easily.\n* The proposed method performs well on the presented benchmarks."}, "weaknesses": {"value": "The scope of the empirical evaluation is to limited and focuses on small datasets representing single PES where explicit long-range interactions take place:\n* It would be great to see the performance on large-scale datasets such as OC20, Materials Project, etc.\n* A proper ablation needs to take place as all methods differ in their short and long-range message passing. To ease comparisons, different short range models should be combined with different long-range models like Kosmala, Unke, Chmiela, etc."}, "questions": {"value": "1. What is the runtime cost of the long range message passing compared to the short-range one?\n2. How does LOREM compared to other long-range message passings like Ewald message passing (Kosmala et al.)\n3. How does the parameter l affect the performance? Does equivariance add something or does standard Ewald message passing yield similar improvements?\n4. How does the application to aperiodic systems works? A fundamental assumption in Ewald summation is that the system is periodic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wDXignNxxD", "forum": "jBWeySWa6z", "replyto": "jBWeySWa6z", "signatures": ["ICLR.cc/2026/Conference/Submission17288/Reviewer_Gp2X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17288/Reviewer_Gp2X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761332342026, "cdate": 1761332342026, "tmdate": 1762927229796, "mdate": 1762927229796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a long-range message passing architecture for modeling non-local physical effects. The core idea is to build a symmetry-preserving architecture on top of Ewald summation where the global interaction is modeled by all-to-all interatomic interactions filtered by scalars inversely proportional to interatomic distances. The proposed model is evaluated on a range of scenarios where the extent of the dependence on the locality to the energy contribution varies a lot.\n\nOverall, in the current form, the paper is not ready to be published because the paper would need a profound revision in regards to additional ablation studies and theoretical work about the equivariance for the core idea in the paper. The lack of these aspects leaves me very uncertain that many assertions regarding those aspects are fair and reasonable and is hard to neglect because they are tightly linked to the novelty and main contribution of the paper."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper structure is organized well and adequately narrows down an issue of conventional MLIPs the authors address in the paper. The problem is timely and on point.\n- The idea of incorporating Ewald summation sounds natural as a means to model long-range interactions."}, "weaknesses": {"value": "**Vague description for the equivariance on the proposed representation:** Description about group equivariance/invariance is loose in general. When the core idea (1) is introduced, the type of group and the definition of its action are not clarified at all. Even when I assume the canonical orthogonal action (resp. translation) of $O(3)$ (resp. $\\mathbb{Z}^{3}$) on $\\mathbb{R}^{3}$, it is unclear how the equation (1) preserves those group actions, because the denominator has an additional term originated from the periodic boundary condition which I consider, at first glance, is supposed to be fixed throughout the group actions. Hence, it is unclear if all the empirical results are reflecting any influence stemmed from the equivariance of the architecture (1) the authors envisioned. Besides, while the authors point to Appendix A for the detail, this section is not helpful since the description remains on a high level and it is hard to access whether the experiment mentioned in this section is fair or not.\n\n**Evaluation of computational cost:** The relative additional computational cost, incurred by the equivariant long-range architecture, against the overall computational complexity is not clarified, and it is hard to access the significance of the potential overall cost reduction, when using an advanced method such as the particle mesh Ewald summation as discussed in Appendix. Since it seems that the experiments do not use those advanced algorithms, I believe it is crucial to detail the computational cost at least from a theoretical point of view.\n\n**Ablation study:** The ablation study looks largely missing. The unit cell $C$ is introduced as a hyperparameter, but its choice is not clarified in the large portion of the experiments and the impact to the performance is also not evaluated systematically. Another missing study is the ablation of an inverse law of interatomic potential functions. One (loose) view of the proposed equivariant long-range representation (1) is \"equivariant\" GraphTransformer filtered by a scalar function inversely proportional to the interatomic distance. While the experiments show an advantage of the proposed model, it is still unclear which of these additional components (i.e., the global aggregation and the physics-inspired inverse distance function) significantly contributed to the performance gain in the experiments."}, "questions": {"value": "- What group is assumed in the equation (1)? What space does the group act on?\n- How is the equation (1) proven to be equivariant to the group action?\n- How does the choice of the unit cell impact on the performance?\n- What if the inverse law is omitted from the equivariant architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tNK0HqeJr9", "forum": "jBWeySWa6z", "replyto": "jBWeySWa6z", "signatures": ["ICLR.cc/2026/Conference/Submission17288/Reviewer_8Nn7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17288/Reviewer_8Nn7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706557587, "cdate": 1761706557587, "tmdate": 1762927228874, "mdate": 1762927228874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an architecture based on SO(3) equivariant spherical harmonic features to build a machine learning potential. \n\nThe architecture uses message passing (MP) over the spherical tensor and scalar \n\nThe architecture also contains a long-range component that takes the output from the MP and computes an Ewald summation over the nodes of a tensor, which represent the charges. \n\nThe author tested on $5$ datasets built to test if ML model can represent charges.\n\nThe authors compare with various state-of-the-art models: MACE, PET, CACE-LES, and 4G-NN.\n\nThe introduction, background, and related work are well described.\n\nIn section 4, the authors use the definition of indices and subscripts introduced in the background, which makes the reading of this session hard. \n\nThe authors claim that equation (1) is equivariant. They also refer to Appendix A for further demonstration and refer to the numerical confirmation. \n\nUnfortunately, Equation (1) is not equivariant, and the session in the Appendix does not really prove anything if not repeating that Eq.a is equivariant.\n\nIn general, the authors refer to the Ewald summation and the efficient implementation. This is used when simulating a periodic system, but the paper only provides results with point evaluation. This is also recognized by the authors in lines 163-164.\n\nEquation (1) is equivariant either with an isolated system (n==0) or when the periodic boundary is rotated with the system, which is not assumed by the authors. \n\nFurther, equation (1) is referred to as implementing message passing. Even if the authors provide two references (Kosmala, Grisafi) claiming the same, equation (1) only corresponds to the aggregation of a message passing procedure; therefore, the name (of the session and of the module) contains two descriptors that do not really apply in this case. \n\nIn the sentence 164-165 (\"The method...\"), it is not clear what is meant by \"accommodates\".\n\nIn sentence 214-215, could you give more information on 1) \"expanded in Bernstein polynomials multiplied with  fcut, the cosine cutoff function, yielding ρij\" 2) how \"Orientations rij are expanded in spherical harmonics  Yl  ij.\"\n\nIn line 216 \"k\" is not defined. \n\nIn line 226, why M P_i are called scalar features? \n\nIn line 227 \"likewise\" seems not applicable, since elements are transformed differently. \n\nIn line 227, self-tensor product is not defined. \n\nIn line 232: how are potential splited?\n\nIn line 234: \"set of different exponents\" could you clarify? what did you try? not clear what it this sentence means. \n\nOne main contribution of the paper is to consider tensorial Q. It would be nice to see what happens with the scalar version only and understand why the \"self-tensor product\" of ${}^M S_i^l$ is necessary."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The evaluation of the method shows a good improvement over previous models. \n\nOverall, the evaluation is well presented and complete. \n\nThe paper is generally clear, but is lacking in the method description (section 4)\n\nIn general, the paper addresses an important aspect of modeling long-range interactions."}, "weaknesses": {"value": "Equation (1) (and the all session) is, in theory, the main contribution of the paper, but it has many inaccuracies."}, "questions": {"value": "Please check my previous comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W9U1kWJHn5", "forum": "jBWeySWa6z", "replyto": "jBWeySWa6z", "signatures": ["ICLR.cc/2026/Conference/Submission17288/Reviewer_C9Nx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17288/Reviewer_C9Nx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933839697, "cdate": 1761933839697, "tmdate": 1762927228533, "mdate": 1762927228533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}