{"id": "c61fLG5HX4", "number": 24308, "cdate": 1758355272392, "mdate": 1759896771790, "content": {"title": "In Praise of Stubbornness: An Empirical Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLMs", "abstract": "Through systematic empirical investigation, we uncover a fundamental property of large language models (LLMs) with implications for continual learning: they can safely learn facts that do not contradict existing knowledge, but attempts to update them with counterfactuals cause catastrophic corruption of *unrelated* knowledge. Unlike humans, who naturally resist conflicting information, LLMs have no such safeguards by design. This leads to severe interference, destroying up to 80% of unrelated factual knowledge even for as few as 10–100 counterfactual updates. To test whether selective plasticity can mitigate this damage, we perform targeted updates, distinguishing between previously used (*stubborn*) and rarely used (*plastic*) neurons. We find again an asymmetry: sparing frequently used neurons improves retention for non-contradictory updates (98% retained vs 93% under standard updates), yet counterfactual updates trigger catastrophic interference regardless of targeting. This effect, which persists across tested models and scales (from GPT-2 to GPT-J-6B, as well as the GPT-4.1 family), suggests a general property of current LLMs. Finally, we show that counterfactual inputs can be detected with ≥95% accuracy using simple model features, pointing to a practical safeguard. These findings motivate research on architectures that, like humans, naturally resist contradictions rather than allowing destructive overwrites.", "tldr": "LLMs can learn non-contradictory facts safely, but updating them with counterfactuals corrupts totally unrelated knowledge. Targeting “plastic” vs. “stubborn” neurons helps only for non-contradictory updates; counterfactual harm persists.", "keywords": ["Large Language Models; continual learning; knowledge editing; counterfactual updates; catastrophic interference; catastrophic forgetting; selective plasticity; neuron targeting; sparsity; conflict detection; robustness to contradictions; model safety"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e465dc8350007cee8ec742db2226051eeaf60bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper characterizes and investigates the problem of knowledge conflict under continual learning, demonstrating that language models trained on conflicting knowledge lead to damage in unrelated knowledge retention, which is moderate when trained on non-conflicting ones. Additional analysis reveals that the latter can be controlled by sparing frequently used neurens, but the former cannot be rescued by the same targeted update method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "S1: This paper characterizes the previously underrecognized and important phenomena of a model's collapse under continual learning with conflicting factual knowledge."}, "weaknesses": {"value": "W1: The experimental setup is designed to control the scenario of non-dissonant and dissonant updates explicitly through the 'non-dissonant updates' phase; however, I believe this experimental protocol brings some (potentially confounding) factors that should be ruled out by additional experiments:\n- The model's failure on phase 2 might be partially attributed to the parameter change during fine-tuning during phase 1, not originating from the knowledge conflict.\n- Although the authors have acknowledged that a large portion of the knowledge injected during the first phase is already known to the model, some of them are not previously known (for example, ~40% of them for GPT-2-small), and newly added facts might behave slightly differently in the second phase compared to what is acquired throughout pretraining. For example, [1] showed that the two types of knowledge might behave differently during fine-tuning. It would be great to see an additional analysis on whether they show different behavior under knowledge conflict.\n\n\n\nW2: Overclaimed connection to human cognition: It is true that human seems to perform belief revision in a significantly different manner compared to LLMs, but it is widely accepted in conceptual change [2] and belief revision [3] studies that humans do not simply append to update beliefs under dissonances, unlike the suggested \"append-only updates that preserve both old and new knowledge with appropriate episodic context\" in the discussion.\n\n\n\nW3: There is a large room for improvement in presentation, and I believe the current quality of presentation is not good enough to recommend acceptance:\n- Fig.1 is hard to parse because it contains too much information, which is not fully explained. For example, (1) What do 'candidate', 'specific', ..., 'finetuning' boxes in Fig.1 mean? I couldn't understand it until I saw the later sections (2) What does 'A&G' on the left side mean?\n- L205-207 - How are the matrices $H$, $\\hat G_n$, and $\\hat A_n$ defined? If a ㅜnotation is used in the main text, then it should be defined in the main text, too.\n- It is not provided what the X-axis values mean in Fig. 2.\n- I could not understand the meaning of the table next to Fig.2, as the meanings and units of the numbers are not provided in the table and caption.\n- The quantitative result for the experiment described in Sec 2.3 is displayed in Sec 5 without a pointer.\n- Tab. 2 caption does not contain information on what is measured.\n\n\n\n\n[1] https://arxiv.org/abs/2405.05904\n[2] https://education.asu.edu/sites/g/files/litvpz656/files/lcl/chi_concpetualchangechapter_0.pdf\n[3] https://plato.stanford.edu/entries/logic-belief-revision/"}, "questions": {"value": "Q1: When you employed LoRA as a control experiment for targeted editing approaches, how did you control the sparsity in LoRA updates (like rank) to ensure the fairness of the comparison with targeted training methods?\n\nQ2: Is there any particular reason to display GPT-2-XL results in Fig. 2, despite stating \"We therefore focus our main text on GPT-2-small results\" in L188? Could you provide the same plot for GPT-2-small?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hmcs0mNg19", "forum": "c61fLG5HX4", "replyto": "c61fLG5HX4", "signatures": ["ICLR.cc/2026/Conference/Submission24308/Reviewer_uQcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24308/Reviewer_uQcp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748063267, "cdate": 1761748063267, "tmdate": 1762943038247, "mdate": 1762943038247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that fine-tuning language models on facts that contradict information seen before can cause forgetting of unrelated knowledge across the GPT-2 family, GPT-J, and GPT-4.1 models. It then shows that updating only the neurons with a high average historical gradient can preserve knowledge with non-dissonant updates, but not with dissonant updates. Finally, the paper shows that one may train an SVM on the activations and gradients to detect dissonance for the COUNTERFACT dataset reliably."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The impact of dissonant updates on unrelated knowledge is an interesting phenomenon.\n2. The related work has a good overview of related works even as far back as a few decades.\n3. The results on finetuning GPT-4.1 help establish that they hold even with newer models"}, "weaknesses": {"value": "1. The entire argument on selective plasticity and the connection to the human brain is very tenuous and anthropomorphizing. There is no rigorous analysis of why the learning of LLMs should be like the brain at all. There are much simpler explanations that must be investigated before these claims, such as that training an LM on contradicting facts teaches it to just output the opposite of what is true, similar to “emergent misalignment” [1].\n2. The paper sets up the problem as the language models forgetting unrelated information when trained on contradicting facts—in this light it seems like the proposed method of updating only the top-N ranks of neurons based on historical gradient magnitude simply does not work for dissonant updates? (Figure 4)\n3. The section on detecting dissonance:\n\n    a. It is entirely conducted on the COUNTERFACT dataset which is synthetically constructed based on templates (not LLM-generated) and has examples which are very easily seen to be false. It is unclear if the results would hold on real-world examples where a statement may have previously been correct but is no longer, or more nuanced settings.\n\n    b. Even if one detects dissonant updates, what then? Perhaps some of these updates are new, but correct information (we should probably not throw these away then)---while others are just misinformation. Besides, there are several documents that are not neatly categorized (consider code snippets, web scrapes, poorly formatted text, and so on); in this light I am not convinced that just training an SVM can help prevention of catastrophic interference.\n\n\n[1] “Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs,” Betley et. al., ICML 2025."}, "questions": {"value": "1. Did you try the experiments on non-GPT models?\n2. You provide some interesting hypotheses in the discussion section for the reasons why this behavior might be observed—did you try to validate these hypotheses?\n3. Given your hypothesis that humans remember “X is used to be true, but now Y is true,” have you tried rewriting the facts Y in this form? I believe it would be a valuable contribution to demonstrate that this rewriting can help alleviate the issue of interference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wk7UMCPckC", "forum": "c61fLG5HX4", "replyto": "c61fLG5HX4", "signatures": ["ICLR.cc/2026/Conference/Submission24308/Reviewer_7tn5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24308/Reviewer_7tn5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766064351, "cdate": 1761766064351, "tmdate": 1762943038066, "mdate": 1762943038066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an empirical investigation into how LLMs handle contradictory versus non-contradictory knowledge updates, drawing inspiration from human cognitive dissonance. The authors demonstrate that while LLMs can safely integrate non-contradictory information, attempting to update them with counterfactual information causes catastrophic corruption of unrelated knowledge. They also explore selective plasticity mechanisms and demonstrate that contradictory information can be detected with ≥95% accuracy using simple model features. The findings persist across multiple model scales."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper offers a hope for prevention of catastrophic interference so that it could have direct relevance to LLM deployment, particularly for models that undergo continual updates in production. \n2. The method proposed in the paper for identifying stubborn and plastic neurons is straightforward and reasonable. Remarkably, the approach introduced in the paper aligns with the lottery ticket hypothesis in several respects and offers a novel possibility for discovering sparse subnetworks."}, "weaknesses": {"value": "1.  The experiments are restricted to the GPT family (GPT-2, GPT-J, GPT-4.1). The generalizability of findings to other model architectures (e.g., LLaMA, Mistral, T5, BERT-style models) remains unclear. Different architectures may handle knowledge storage and updates differently, which could significantly affect the conclusions.\n2. All experiments rely solely on CounterFact dataset. The findings may be specific to the types of factual knowledge and counterfactual patterns in this dataset."}, "questions": {"value": "1. Can you provide evidence or theoretical justification for why these findings should generalize beyond GPT-family models?\n2. How would you expect the findings to change with datasets with different domain-specific knowledge? Are there plans to validate on more diverse knowledge types?\n3.  Can you provide concrete efficiency metrics for dissonance detection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0TzXQPcjZx", "forum": "c61fLG5HX4", "replyto": "c61fLG5HX4", "signatures": ["ICLR.cc/2026/Conference/Submission24308/Reviewer_4BJv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24308/Reviewer_4BJv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891400320, "cdate": 1761891400320, "tmdate": 1762943037875, "mdate": 1762943037875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how Large Language Models (LLMs) handle new information, specifically contrasting the integration of facts that are consistent with existing knowledge (\"non-dissonant\") versus those that contradict it (\"dissonant\"). The authors find that dissonant updates cause catastrophic forgetting of unrelated knowledge, a phenomenon not observed with non-dissonant updates. They explore a \"selective plasticity\" approach, finding it protects knowledge during non-dissonant updates but fails for dissonant ones. Finally, they show that dissonant inputs can be detected with high accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1.   **Clear Problem Formulation:** The paper is well-motivated, using the intuitive analogy of cognitive dissonance to frame a critical problem in continual learning for LLMs. The distinction between dissonant and non-dissonant updates is a useful lens for analysis.\n2.  **Rigorous Empirical Work:** The experiments are thoughtfully designed and clearly presented. The findings, particularly the stark contrast in how the model handles the two update types (Figure 3) and the asymmetry of selective plasticity (Figure 4), are compelling and well-illustrated."}, "weaknesses": {"value": "However, the paper’s novelty and core contributions are severely undermined by its substantial overlap with the recent work of Sun et al. (ICLR 2025), “How new data permeates LLM knowledge and how to dilute it,” which the authors fail to cite or discuss. The framework introduced by Sun et al. provides a more general explanation for the phenomena observed here, suggesting that the effects described in this paper are a specific, less general instance of the “priming” and “surprisal” mechanisms identified in that work, thereby raising questions about the originality of the core findings.\n\n1.  **Relation to the \"Surprisal\" Framework:** Sun et al. show that the degree to which new information undesirably \"primes\" unrelated knowledge is strongly predicted by its \"surprisal\" (i.e., the low probability of its keywords before training). The \"dissonant\" updates in this paper seem to be a specific instance of high-surprisal updates, while \"non-dissonant\" updates correspond to low-surprisal ones. The catastrophic effects observed with dissonant facts could therefore be an expected outcome of the general principle that high-surprisal data leads to larger, more disruptive updates. The analysis would be significantly strengthened by discussing this connection and clarifying what the binary \"dissonance\" framing offers beyond the continuous \"surprisal\" metric.\n\n2.  **Parallels in Mitigation Strategies:** The proposed \"selective plasticity\" method, which spares high-gradient (\"stubborn\") neurons, has strong conceptual parallels with the \"ignore-k\" pruning method from Sun et al. The \"ignore-k\" method mitigates priming by removing the top-k percent of parameter updates, which are those with the highest gradient magnitudes. Both approaches are based on the same principle of isolating and either protecting or ignoring the largest gradients to prevent over-correction. The distinction between a neuron-level and a weight-level intervention would benefit from further justification to establish it as a methodologically distinct contribution.\n\n3.  **Positioning of Contributions:** The paper's conclusion suggests \"avoiding overwrites through episodic (temporal) contextualization\" as a promising future direction. This idea is very similar to the \"stepping-stone\" text augmentation strategy already implemented and shown to be effective by Sun et al. This further underscores the need for this paper to situate itself within the context of this prior work to clearly delineate its novel contributions."}, "questions": {"value": "Q1.   How might the findings on \"dissonance\" be framed in the context of the \"surprisal\" framework from Sun et al.? Does the binary distinction of contradiction offer unique insights that a continuous surprisal value might miss?\n\nQ2. Could the authors elaborate on the key differences between their neuron-level \"selective plasticity\" and the weight-level \"ignore-k\" pruning from Sun et al.? What are the practical or theoretical advantages of the proposed approach?\n\n\n\n**Suggestion**\n* A discussion comparing this paper’s findings with those of Sun et al. would be invaluable for clarifying the specific advancements made here.\n* It would be helpful if the authors could also perform a surprisal analysis and present quantitative results to illustrate the main differences between dissonant and non-dissonant fact updates.\n\n\nIf the authors can demonstrate that the phenomena they identify are not fully explained by the framework of Sun et al., I would be inclined to increase my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QSvMoCDYCR", "forum": "c61fLG5HX4", "replyto": "c61fLG5HX4", "signatures": ["ICLR.cc/2026/Conference/Submission24308/Reviewer_Bvbx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24308/Reviewer_Bvbx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991543998, "cdate": 1761991543998, "tmdate": 1762943037665, "mdate": 1762943037665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}