{"id": "UZlJY3uV6P", "number": 21964, "cdate": 1758324165355, "mdate": 1759896893639, "content": {"title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs", "abstract": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)—a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT’s capabilities by implementing three popular schemes—Tree of Thoughts, Graph of Thoughts, and ProbTree—within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.", "tldr": "A software framework to implement and optimize LLM-powered reasoning schemes", "keywords": ["Large Language Models", "Reasoning", "Prompting Schemes", "Tree of Thoughts", "Graph of Thoughts"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88be38ce9d1fe200acb56fdeaad44e3d9d836abc.pdf", "supplementary_material": "/attachment/c86d923b4e6419722141c6e5a23ef41eab4fdfc4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Framework of Thoughts (FoT), a foundation framework designed to address critical limitations in existing large language model (LLM) reasoning schemes like Chain of Thought (CoT), Tree of Thoughts (ToT), and Graph of Thoughts (GoT). The authors argue that current schemes are often static, requiring manually-defined, problem-specific reasoning structures that lack adaptability. They also suffer from inefficient execution (e.g., sequential, redundant LLM calls) and are often under-optimized in terms of prompts and hyperparameters. FoT is not a new reasoning scheme but rather a framework for building and optimizing other schemes. Its key features include (1) support for dynamic graph structures that can evolve during execution, unlike the static graphs in previous frameworks; (2) faster parallelized execution of operations; (3) intelligent caching (both within a single execution and persistently across samples) to reduce costs and avoid re-execution; and (4) built-in optimization tools for hyperparameters and prompts. To demonstrate its utility, the authors re-implemented ToT, GoT, and ProbTree within FoT. The empirical evaluation shows that FoT's parallelization and caching provided significant runtime accelerations (averaging 10.7x faster) and cost reductions"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method addresses the problem of static graph structures in prior prompt schemes\n2. The proposed method provides efficiency with parallelized execution and caching\n3. The proposed method integrates optimization methods.\n4. The proposed method shows strong empirical results compared to related works."}, "weaknesses": {"value": "1. The paper presents a limited demonstration of dynamic graphs. The evaluated tasks are all supported by existing methods, indicating that the tasks do not require dynamic graphs. Besides, the dynamic graphs generated by the proposed methods are not provided for qualitative assessments.\n\n2. Although the graphs are claimed to be dynamically built, the type of operations (nodes) seems to be predefined (e.g., split prompt, improve prompt). Thus, the entire framework still requires heavy human engineering.\n\n3. The presentation is poor and does not provide a clear understanding of the proposed method. The method seems to involve dynamically adjusting or growing an execution graph by generating new operation nodes and actually using the graph and compute the operation nodes. However, without a clear explanation or pseudo-code, it is not clear how the entire framework works, what is provided manually, and how the algorithm operates."}, "questions": {"value": "1. Can you provide a clear rundown of how the framework of thought works, including what is manually prepared, and how the algorithm operates after being given all the required input, prompt, and parameter settings?\n\n2. What type of dynamic graphs are created during the experimental tasks, and how do they improve the results compared to fixed graph baseline methods?\n3. What additional tasks can the proposed method be applied to? What real-world task would require dynamic graphs and couldn't be solved by prior fix-graph methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xfTYQV8CIG", "forum": "UZlJY3uV6P", "replyto": "UZlJY3uV6P", "signatures": ["ICLR.cc/2026/Conference/Submission21964/Reviewer_6ww7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21964/Reviewer_6ww7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595092020, "cdate": 1761595092020, "tmdate": 1762942000574, "mdate": 1762942000574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"Framework of Thoughts\" (FoT), a foundation framework intended to address systemic limitations in complex LLM reasoning schemes like Chain-of-Thought (CoT), Tree of Thoughts (ToT), and Graph of Thoughts (GoT). The authors identify three primary issues with existing methods: their reliance on static, manually-defined reasoning structures; insufficient optimization of prompts and hyperparameters; and inefficient, sequential execution.\n\nFoT is positioned as a general-purpose system for implementing and running such reasoning algorithms. Its main features include:\n1.  **Dynamic Graph Structures**: Modeling reasoning via an \"execution graph\" that can be modified by operations during runtime.\n2.  **Parallel Execution**: A scheduler that attempts to execute independent operations concurrently.\n3.  **Caching**: Process-level and persistent caching to reuse results and reduce redundant computations.\n4.  **Optimization Hooks**: Integration with external libraries like Optuna and DSPy for automated hyperparameter and prompt optimization.\n\nThe authors demonstrate FoT by re-implementing ToT, GoT, and ProbTree. The results show significant improvements in execution speed and cost, and enable performance gains through optimization that would otherwise be computationally expensive."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Clear Problem Identification**: The paper correctly identifies several critical, practical bottlenecks in the implementation of advanced prompting schemes. The focus on systemic issues like static structures, execution inefficiency, and the prohibitive cost of optimization is a relevant and timely contribution to the field.\n\n2.  **Demonstrated Efficiency Gains**: The empirical results effectively showcase the practical utility of the proposed framework. The substantial speed-ups (over 10x in some cases) and cost reductions achieved through parallelization and caching are notable. These efficiency improvements make the process of developing and tuning complex reasoning schemes more accessible and tractable for researchers.\n\n3.  **Useful Open-Source Contribution**: The work culminates in a modular and open framework that could serve as a useful tool for the community. By providing a unified platform for implementing diverse \"X-of-Thoughts\" schemes, FoT has the potential to facilitate more standardized and reproducible research in this area."}, "weaknesses": {"value": "1.  **Insufficient Substance for a Research Paper; Reads as a Technical Report**: The fundamental weakness of this submission is its lack of research substance. The work is presented as a research paper but its content and contributions are those of a technical report for an early-stage open-source project. It describes the architecture of a software tool that combines a few well-established engineering principles. While potentially useful, it does not propose or validate any new scientific hypotheses, nor does it offer deep insights into the nature of LLM reasoning. The scope and depth of the work are insufficient for a top-tier research conference like ICLR.\n\n2.  **Lack of Novelty in Core Components**: The touted features of FoT are standard, off-the-shelf concepts in software and systems engineering, and their application here is straightforward.\n    *   **Optimization**: The paper introduces **no new optimization algorithms**. It merely provides a wrapper around existing, widely-used libraries (Optuna and DSPy). The contribution is one of integration, not innovation. The impact of this \"optimization\" is therefore limited to the capabilities of these external tools.\n    *   **Caching**: Caching is a foundational concept in computer science. The implementation described is a basic key-value store (process-level and persistent), which is the most naive form of caching. There is no research presented on adapting caching for the specific properties of LLM-generated \"thoughts\" (e.g., semantic caching, approximate matching), which would have constituted a research contribution.\n    *   **Parallelism**: The parallelism described is a simple dependency-based task scheduling, a solved problem in distributed computing and workflow management systems.\n    *   **Structural Representation**: The claim of enabling \"dynamic\" graph structures is not sufficiently differentiated from prior art like Adaptive Graph of Thoughts (AGoT)[1]. The paper fails to demonstrate that FoT enables qualitatively new reasoning structures that were previously unattainable.\n\n3.  **Flawed and Unconvincing Experimental Design**: The experimental methodology is not rigorous enough to support strong scientific claims. A major flaw is the use of **different LLMs for different tasks** (GPT-4o, GPT-3.5-Turbo, GPT-4.1-mini). This inconsistent setup makes it impossible to perform a fair comparison of the reasoning schemes (ToT vs. GoT vs. ProbTree), as performance is heavily confounded by the choice of the base LLM. A framework aiming to facilitate fair comparison should, at a minimum, use a controlled experimental setup. This choice undermines the credibility of the evaluation.\n\n4.  **Insufficient Workload and Limited Impact**: The overall workload presented feels preliminary. The paper re-implements only three existing schemes on a handful of tasks. The claimed \"dynamic\" nature of the framework is not substantially demonstrated, as the implemented schemes (ToT, GoT) are largely static or have limited dynamicism. The paper does not showcase a novel, fully dynamic reasoning agent that would truly stress-test and validate the framework's core design proposition. Consequently, the demonstrated impact of the framework's more advanced features (like dynamic graph modification) is negligible.\n\n[1] Adaptive Graph of Thoughts: Test-Time Adaptive Reasoning Unifying Chain, Tree, and Graph Structures"}, "questions": {"value": "1.  **Justification as Research**: Could the authors articulate what they consider to be the primary, non-obvious scientific contribution of this paper, distinct from its engineering contribution of building a software tool? What fundamental hypothesis about AI, LLMs, or reasoning does this work prove or disprove?\n\n2.  **Experimental Control**: Please provide a justification for using different base LLMs across different experimental tasks. How can any valid conclusions be drawn about the relative merits of the reasoning schemes when the most significant variable—the model's capability—is not held constant?\n\n3.  **Scope of Work**: The paper's contribution seems limited to re-implementing a few existing schemes. Was a novel, fully-automatic and dynamic reasoning scheme developed and tested within FoT? If not, why should the community be convinced of the framework's claimed \"dynamic\" capabilities, which appear to be largely theoretical at this stage?\n\n4.  **Novelty in Optimization**: Beyond acting as a wrapper for existing tools like Optuna and DSPy, does FoT introduce any novel methods or principles for optimizing reasoning graphs? For example, does it leverage the graph structure to perform more efficient prompt or hyperparameter search?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uY9GGoJNVn", "forum": "UZlJY3uV6P", "replyto": "UZlJY3uV6P", "signatures": ["ICLR.cc/2026/Conference/Submission21964/Reviewer_yf4r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21964/Reviewer_yf4r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791712593, "cdate": 1761791712593, "tmdate": 1762942000177, "mdate": 1762942000177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Framework of Thoughts (FoT), a foundation framework for implementing and optimizing chain/tree/graph-based prompting schemes (e.g., Tree of Thoughts (ToT), Graph of Thoughts (GoT), ProbTree). FoT aims to address limitations of existing schemes (static structures, suboptimal hyperparameters, inefficient execution) via four core features: dynamic graph structures (evolving during execution), safe parallel execution (with race condition protections), intelligent caching (temporary process cache and persistent cross-sample cache), and built-in optimization (hyperparameters via Optuna, prompts via DSPy). Experiments on 5 tasks (Game of 24 (Go24), Sorting, Document Merging (DM), HotpotQA, MuSiQue) show FoT reduces runtime by up to 35.4× and costs by up to 46% while slightly improving task scores. The authors also release code for reproducibility ."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* Though not methodologically groundbreaking, FoT integrates fragmented optimizations (dynamic graphs, parallelism, caching, optimization) into a unified, modular framework. It supports diverse operations (LLM calls, tool use, code execution) via Python, lowering the barrier to implementing complex reasoning schemes for developers .\n\n* The work validates FoT on three representative prompting schemes (ToT, GoT, ProbTree) across varied tasks—mathematical reasoning (Go24), structural tasks (Sorting, DM), and multi-hop QA (HotpotQA, MuSiQue)—providing a relatively comprehensive test of its adaptability ."}, "weaknesses": {"value": "1. Lack of Methodological Novelty: Core features are either existing capabilities or off-the-shelf integrations.\n  * Dynamic graphs: LangGraph (2024) already enables adding/removing nodes/edges during execution; FoT’s separation of “execution graph” and “reasoning graph” is a cosmetic distinction, not a functional innovation .\n  * Optimization: FoT directly uses Optuna (Akiba et al., 2019) for hyperparameters and DSPy’s COPRO (Khattab et al., 2023) for prompts—no custom algorithms or adaptive strategies are proposed .\n  * Parallelism/caching: GoT (Besta et al., 2024a) already supports parallel sublist merging, and LangChain’s async modules enable concurrent calls; FoT’s “safe parallel constraints” (e.g., limiting modifications to exclusive descendants) are trivial adjustments.\n\n2. Limited Benchmarks: All experiments focus on structured/semi-structured tasks (math, sorting, merging, multi-hop QA) with no unstructured tasks (e.g., code debugging, logical puzzles, creative writing)—tasks where dynamic graphs might add value, leaving FoT’s “general-purpose” claim unproven. Datasets are undersized: Sorting/DM only have 50 test instances each; HotpotQA/MuSiQue use 1,000 test instances (far fewer than standard benchmarks like HotpotQA’s official 10k+ test set)—results may lack statistical significance .\n\n3. Inadequate Baseline Comparisons: FoT only compares to naive baselines (sequential execution, no cache) and the original schemes (ToT, GoT, ProbTree)—no comparisons to state-of-the-art dynamic frameworks."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OosSrv7oBF", "forum": "UZlJY3uV6P", "replyto": "UZlJY3uV6P", "signatures": ["ICLR.cc/2026/Conference/Submission21964/Reviewer_G3JY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21964/Reviewer_G3JY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864333667, "cdate": 1761864333667, "tmdate": 1762941999637, "mdate": 1762941999637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Framework of Thoughts (FoT), a general-purpose framework to implement and optimize multi-prompt reasoning schemes (e.g. CoT/ToT/GoT/ProbTree). The key features of FoT are: 1) dynamic execution graphs that may evolve during inference, together with a reasoning graph that generate thoughts for reasoning; 2) safe parallel scheduling under graph-modification constraints; 3) caching to reduce repeated LLM calls. The paper re-implemented ToT, GoT, and ProbTree in FoT and evaluate across Game of 24, Sorting, Document Merging, HotpotQA etc. They report large speedups from parallelization and cost reductions from caching and modest task-score gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposal and distinction between execution and reasoning graphs, and the definition of allowable graph mutations (ancestors/descendants/exclusive descendants) during parallel execution, is well-motivated and easy to reason about.\n- Parallel scheduling plus two-tier caching is practical and helpful, especially for optimization loops where repeated calls dominate runtime."}, "weaknesses": {"value": "Novelty is more infrastructural than algorithmic. Much of FoT bundles known ideas (parallel execution, caching, hyperparameter, prompt tuning) into a framework; the new scientific insight beyond system engineering is limited. The paper's dynamic graph proposal is interesting, but the story overlaps with existing orchestration frameworks (e.g., LangGraph execution DAGs, parallel branches, caching). The delta feels more engineering consolidation than a conceptual leap."}, "questions": {"value": "- Do the dynamic graph generation procedure ensure any formal properties (e.g., determinism of final reasoning graph given fixed seeds)? If not then the graph topology would be uncontrollable, not sure what failure modes did you observe and how are they mitigated?\n\n- I'm curious to know how the dynamically generated graphs differ from predefined graphs (trees, graphs etc.). Are the dynamic graphs usually smaller or shallower than the predefined graphs? How would you justify the dynamically constructed graph topolgies are more suitable for the problems to be solved?\n\n- How are the cache keys defined? Is it something like prompts + inputs + model generations + tool specifications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v5DwuGwQcm", "forum": "UZlJY3uV6P", "replyto": "UZlJY3uV6P", "signatures": ["ICLR.cc/2026/Conference/Submission21964/Reviewer_5shY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21964/Reviewer_5shY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021903067, "cdate": 1762021903067, "tmdate": 1762941999204, "mdate": 1762941999204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}