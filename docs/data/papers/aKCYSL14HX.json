{"id": "aKCYSL14HX", "number": 5411, "cdate": 1757907780122, "mdate": 1759897977253, "content": {"title": "DiskHIVF: Disk-Resident Hierarchical Inverted File Index For Billion-scale Approximate Nearest Neighbor Search", "abstract": "The in-memory algorithms for approximate nearest neighbor search (ANNS) has demonstrated remarkable success. However, as the scale of vector data grows, the memory demands of in-memory indexing become increasingly prohibitive. A promising solution lies in hybrid memory-disk implementations, which offload the bulk of data storage to cost-efficient devices such as Solid State Drives (SSDs) while retaining only frequently accessed data in memory. \nDespite this, existing hybrid memory-disk indexing methods suffer from memory overheads that scale proportionally with the number and dimensionality of the vectors, limiting their memory savings to a modest 5–20$\\times$.\nIn this paper, we introduce the Disk-Resident Hierarchical Inverted File Index (DiskHIVF), a novel hybrid memory-disk indexing algorithm with a memory space complexity of ${O(\\sqrt{N} \\cdot d + N)}$, where ${N}$ is the number of vectors and ${d}$ is their dimensionality. Leveraging its superior space complexity, DiskHIVF achieves several hundred times memory savings compared to the original vectors, and 10–30$\\times$ reduction compared to state-of-the-art methods. \nExperimental results on four different datasets demonstrate that DiskHIVF is 1.2-2.3$\\times$ faster than the state-of-the-art hybrid indexing solutions at achieving the same recall quality of 90\\%. These results indicate that our approach can significantly reduce the overhead of machine resources while maintaining high search performance.", "tldr": "This paper introduces a novel disk-memory hybrid approximate nearest neighbor indexing algorithm, DiskHIVF, which offers superior memory space complexity and search performance, capable of reducing memory overhead by up to a hundred times.", "keywords": ["billion-scale;disk-resident;approximate nearest neighbor search;"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ddb4142dabf0ab76f10db892939d03387e02728b.pdf", "supplementary_material": "/attachment/058fc637c46b38ae9a9c30348b312d6ede391a88.zip"}, "replies": [{"content": {"summary": {"value": "The task the article solves is memory-constrained ANN search, where both RAM and SSD are used to store the data and the index structure. The method proposed in the article is a hierarchical $k$-means index where the residuals of the first level ($m$ clusters) are clustered at the second level ($n$ clusters) globally, so that $mn$ clusters are obtained but only $m + n$ cluster centers have to be stored in memory. The authors augment this basic scheme by an efficient disk layout of the cluster points, where the nearby clusters are stored in nearby memory locations, and by query-aware dynamic pruning where a quadratic polynomial model is used to predict the query coverage, and the search is terminated early if the predicted 99\\% coverage is reached.  The experimental results of the article show that the proposed method is both faster and more memory-efficient than the competing methods, such as DiskANN and SPANN."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[S1] The proposed method is intuitive and straightforward.\n\n[S2] According to the empirical results of the article, the proposed method seems to work well in practice."}, "weaknesses": {"value": "[W1] The methodolofical novelty of the work is limited: the proposed hierarchical clustering method seems to be Non-orthogonal inverted multi-index (NO-IMI) by Babenko & Lempitsky (2016) (the authors correctly acknowledge this earlier work) and the disk layout scheme is a straightforward implementation detail. Only the query-aware dynamic pruning seems to have limited novelty; however, in the context of hybrid memory-disk ANN search, Chen et al., (2021) use a more simple query-aware dynamic pruning method, and in the context of regular ANN search, Li et al. (2020) also use a model-based method (with more features and a different model than the current work). \n\n[W2] The claim that the proposed method has to store only $\\mathcal{O}(\\sqrt{N})$ centroids in memory is based only on the fact the that the authors select the number of centroids to be $\\mathcal{O}(\\sqrt{N})$  ($N$ denotes the number of data set points). However, traditionally $\\sqrt{N}$ (see, e.g, Douze et al., 2025, p. 9) is the number of clusters that is recommended for non-hierarchical $k$-means index (IVF).  Thus, the experimental results have two omissions: a) Full hyperparameter sweeps should be performed (and Pareto frontiers should be reported, see, e.g., Aumüller et al, 2020, for a correct methodogy) to ensure that the memory saving and the improved recall of the proposed method compared to the baselines is not just due to the choice of hyperparameters. b) An ablation experiment should be performed, where the proposed hierarchical $k$-means index is compared to the regular $k$-means index with the same number of centroids $\\mathcal{O}(\\sqrt{N})$ to verify that it can indeed obtain a more accurate clustering with the same number of centroids stored in memory (I believe this should be the case). \n\n[W3] The notation is sloppy. Even on the very few formulas contained there are mistakes. For instance, I believe that in lines 288-289 it should be $\\||q - (S_k + T_l)\\||^2$ instead of $\\||q - S_k T_l\\||^2$. Also, dot product or matrix multiplication notation should be used when vectors are multiplied, e.g., $\\langle S_k, T_l\\rangle$ or $S_k^T T_l$, instead of $S_k T_l$.\n\nReferences:\n\nAumüller, Martin, Erik Bernhardsson, and Alexander Faithfull. \"ANN-Benchmarks: A benchmarking tool for approximate nearest neighbor algorithms.\" Information Systems 87 (2020): 101374.\n\nBabenko, Artem, and Victor Lempitsky. \"Efficient indexing of billion-scale datasets of deep descriptors.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n\nChen, Qi, et al. \"Spann: Highly-efficient billion-scale approximate nearest neighborhood search.\" Advances in Neural Information Processing Systems 34 (2021): 5199-5212.\n\nDouze, Matthijs, et al. \"The faiss library.\" IEEE Transactions on Big Data (2025).\n\nLi, Conglong, et al. \"Improving approximate nearest neighbor search through learned adaptive early termination.\" Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. 2020."}, "questions": {"value": "I do not have any questions for the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "u4o3vu2rAv", "forum": "aKCYSL14HX", "replyto": "aKCYSL14HX", "signatures": ["ICLR.cc/2026/Conference/Submission5411/Reviewer_Y8Q9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5411/Reviewer_Y8Q9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760947164526, "cdate": 1760947164526, "tmdate": 1762918045451, "mdate": 1762918045451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The task the article solves is memory-constrained ANN search, where both RAM and SSD are used to store the data and the index structure. The method proposed in the article is a hierarchical $k$-means index where the residuals of the first level ($m$ clusters) are clustered at the second level ($n$ clusters) globally, so that $mn$ clusters are obtained but only $m + n$ cluster centers have to be stored in memory. The authors augment this basic scheme by an efficient disk layout of the cluster points, where the nearby clusters are stored in nearby memory locations, and by query-aware dynamic pruning where a quadratic polynomial model is used to predict the query coverage, and the search is terminated early if the predicted 99\\% coverage is reached.  The experimental results of the article show that the proposed method is both faster and more memory-efficient than the competing methods, such as DiskANN and SPANN."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[S1] The proposed method is intuitive and straightforward.\n\n[S2] According to the empirical results of the article, the proposed method seems to work well in practice."}, "weaknesses": {"value": "[W1] The methodological novelty of the work is limited: the proposed hierarchical clustering method seems to be Non-orthogonal inverted multi-index (NO-IMI) by Babenko & Lempitsky (2016) (the authors correctly acknowledge this earlier work) and the disk layout scheme is a straightforward implementation detail. Only the query-aware dynamic pruning seems to have limited novelty; however, in the context of hybrid memory-disk ANN search, Chen et al., (2021) use a more simple query-aware dynamic pruning method, and in the context of regular ANN search, Li et al. (2020) also use a model-based method (with more features and a different model than the current work). \n\n[W2] The claim that the proposed method has to store only $\\mathcal{O}(\\sqrt{N})$ centroids in memory is based only on the fact the that the authors select the number of centroids to be $\\mathcal{O}(\\sqrt{N})$  ($N$ denotes the number of data set points). However, traditionally $\\sqrt{N}$ (see, e.g, Douze et al., 2025, p. 9) is the number of clusters that is recommended for non-hierarchical $k$-means index (IVF).  Thus, the experimental results have two omissions: a) Full hyperparameter sweeps should be performed (and Pareto frontiers should be reported, see, e.g., Aumüller et al, 2020, for a correct methodogy) to ensure that the memory saving and the improved recall of the proposed method compared to the baselines is not just due to the choice of hyperparameters. b) An ablation experiment should be performed, where the proposed hierarchical $k$-means index is compared to the regular $k$-means index with the same number of centroids $\\mathcal{O}(\\sqrt{N})$ to verify that it can indeed obtain a more accurate clustering with the same number of centroids stored in memory (I believe this should be the case). \n\n[W3] The notation is sloppy. Even on the very few formulas contained there are mistakes. For instance, I believe that in lines 288-289 it should be $\\||q - (S_k + T_l)\\||^2$ instead of $\\||q - S_k T_l\\||^2$. Also, dot product or matrix multiplication notation should be used when vectors are multiplied, e.g., $\\langle S_k, T_l\\rangle$ or $S_k^T T_l$, instead of $S_k T_l$.\n\nReferences:\n\nAumüller, Martin, Erik Bernhardsson, and Alexander Faithfull. \"ANN-Benchmarks: A benchmarking tool for approximate nearest neighbor algorithms.\" Information Systems 87 (2020): 101374.\n\nBabenko, Artem, and Victor Lempitsky. \"Efficient indexing of billion-scale datasets of deep descriptors.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n\nChen, Qi, et al. \"Spann: Highly-efficient billion-scale approximate nearest neighborhood search.\" Advances in Neural Information Processing Systems 34 (2021): 5199-5212.\n\nDouze, Matthijs, et al. \"The faiss library.\" IEEE Transactions on Big Data (2025).\n\nLi, Conglong, et al. \"Improving approximate nearest neighbor search through learned adaptive early termination.\" Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. 2020."}, "questions": {"value": "I do not have any questions for the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "u4o3vu2rAv", "forum": "aKCYSL14HX", "replyto": "aKCYSL14HX", "signatures": ["ICLR.cc/2026/Conference/Submission5411/Reviewer_Y8Q9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5411/Reviewer_Y8Q9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760947164526, "cdate": 1760947164526, "tmdate": 1763044732770, "mdate": 1763044732770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DiskHIVF, a memory-disk hybrid ANNS algorithm based on a two-level hierarchical IVF clustering scheme, along with layout reorganization optimizations and query-aware dynamic pruning tailored for this algorithm. Compared to baselines, DiskHIVF achieves a 1.2–2.3× speedup while reducing memory usage by 10–30×."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The use of IVF clustering leads to memory savings and retrieval acceleration compared to graph-based indexes (DiskANN, Starling) and the SPANN clustering index that allows inter-cluster crossing.\n\n2) A greedy algorithm is employed to reorder cluster centers, optimizing the disk layout.\n\n3) Polynomial fitting functions are used to predict the remaining workload, enabling pruning during the retrieval process."}, "weaknesses": {"value": "1) While the proposed method is based on IVF clustering, the compared state-of-the-art baselines are graph-based indexes. Comparisons with more recent IVF-based disk indexes (e.g., Faiss, arxiv'24) in terms of memory capacity and performance are necessary. When constraining the cluster size to at least *d*, IVF-based disk indexes can also achieve O(N) memory space complexity. Thus, it is essential to compare the performance of IVF and DiskHIVF when their memory usage is similar or even more favorable for IVF.\nFurthermore, the in-memory portion of DiskHIVF corresponds to a special case of IVF-PQ retrieval with the number of sub-vector segments (typically denoted as M) set to 1, where *n* corresponds to the number of IVF clusters and *m* to the size of the PQ codebook. Compared to a standard IVF-PQ index, this work retains the original vectors instead of quantizing them to PQ cluster centers. Therefore, comparisons with existing IVF-PQ-based disk indexes under similar parameter settings would make the results more convincing.\n\n2) In Figure 2(b), a quadratic function is used to fit the budget curve without justification. However, for small values on the horizontal axis, the predicted curve shows an opposite monotonicity trend compared to the actual curve and exhibits significant numerical deviations. Is the use of a quadratic function appropriate? Would other functions, such as exponential functions, be more suitable for fitting this curve?\n\n3) Performance is measured using average request latency (Section 4.3). However, according to Appendix A.2, on the DEEP dataset, although each cell is expected to contain only 10 points on average, the largest cell may contain up to 9000 points. Given the load imbalance across different cells resulting from this distribution, using tail latency as a performance evaluation metric would be more reasonable than average latency."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BHnHS0XSQu", "forum": "aKCYSL14HX", "replyto": "aKCYSL14HX", "signatures": ["ICLR.cc/2026/Conference/Submission5411/Reviewer_xfjP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5411/Reviewer_xfjP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661089273, "cdate": 1761661089273, "tmdate": 1762918044927, "mdate": 1762918044927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a two-layer hierarchical inverted index for billion-scale approximate nearest neighbor (ANN) search, aiming to reduce memory consumption. Experimental results indicate a 10–30× memory reduction compared to state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Demonstrates impressive memory savings for billion-scale vector indices."}, "weaknesses": {"value": "1.\tSimilarity to Existing Approaches\nThe proposed method appears conceptually similar to IVF+G+P, which also employs two-layer hierarchical clustering. IVF+G+P partitions data into K inverted regions and applies memory-efficient subregion grouping using a learned scalar α to interpolate new centroids. The paper should clarify the key differences and justify why the proposed approach is preferable. A direct comparison with other hierarchical inverted index methods would strengthen the contribution.\n2.\tHandling Skewed Data\nReal-world datasets are often skewed, leading to imbalanced clusters. The paper does not address how the method handles skewed distributions or provide experiments on skewed datasets (e.g., SpaceV1B). This omission limits the practical applicability of the approach.\n3.\tDistance Metric Generalization\nThe design and optimization focus solely on L2 distance. It is unclear how the method adapts to other similarity measures (e.g., inner product). Including experiments on datasets requiring different distance types would improve generality.\n4.\tLimited Performance Metrics\nThe evaluation only reports average latency and recall. In real-world scenarios, tail latency and throughput are also critical. Based on disk access counts in Table 3, the proposed method may have lower throughput compared to DiskANN and SPANN since both algorithms are IOPS-bound. A more comprehensive performance analysis is needed.\n5.\tIncomplete Ablation Study\nThe pruning ablation study should include comparisons with alternative pruning techniques to better understand its advantage.\n6.\tReal-Time Updates\nModern vector indices often require real-time updates. The paper does not discuss how the proposed method supports dynamic insertions or deletions."}, "questions": {"value": "1. Could you explain the rationale behind the final parameter setting: n=sqrt(N/10)×2.5 and  m=sqrt(N/10)/2.5?\n2. What are the index build cost and time? Given that n and m remain large for clustering 10% of 1B data, what is the clustering overhead?\n3. How does the proposed method perform in terms of tail latency and throughput compared to state-of-the-art solutions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UN1k3thLjs", "forum": "aKCYSL14HX", "replyto": "aKCYSL14HX", "signatures": ["ICLR.cc/2026/Conference/Submission5411/Reviewer_PBns"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5411/Reviewer_PBns"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707624637, "cdate": 1761707624637, "tmdate": 1762918044594, "mdate": 1762918044594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new method for large-scale approximate nearest neighbor search. The proposed method, named DiskHIVF, uses the well-known Inverted File (IVF) approach where the data is clustered, and nearest neighbor queries are answered by only exploring certain number of clusters closest to the query.\n\nSpecifically, in their method, the authors propose using a hierarchical clustering and storing the points in the clusters on disk. This approach scales to large datasets because only the centroids need to be kept in memory, and the number of centroids is limited since the second-level centroids are shared in the hierarchical clustering. The authors perform their experiments on two million-scale and two billion-scale datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper studies an important topic, and the proposed method achieves a low memory usage. The method is validated against appropriate baseline methods, and the experiments are performed on standard benchmark datasets, two of which are billion-scale datasets."}, "weaknesses": {"value": "The work seems incremental, and the experiments are not particularly convincing. The core idea is similar to SPANN: use hierarchical clustering and store the clusters (inverted lists) on disk (storing the inverted lists on disk isa of course also otherwise a widely used practice, e.g. Faiss has OnDiskInvertedLists). The main difference is the type of hierarchical clustering used which reduces the number of centroids that have to be be kept in memory.\n\nHowever, the proposed hierarchical clustering strongly resembles the NO-IMI structure [1, Section 3] which the authors cite but do not discuss the relation to. Moreover, SPANN (as well as DiskANN and Starling) already uses only 32GB of memory for a billion-scale dataset which is very reasonable (also, while the the authors write that \"SPANN requires retaining approximately 16\\% of vectors as centroids in memory\", of course you can use less centroids even if performance saturates at 16\\%).\n\nAs the proposed method is a combination of different ad hoc tweaks, it would be necessary to figure out which of these tweaks potentially give your method an edge. The correct way to do that would be to e.g. keep everything else the same except change your proposed clustering method to the hierarchical balanced clustering method used in SPANN. Additionally, e.g. cluster pruning strategies have been previously studied in more detail [2].\n\nFinally, the experimental results are not convincing: you do not compare the number of I/O operations or the indexing times between the methods, and only SIFT1M is used for ablation studies. The experimental results for SPANN and Starling seem worse than at least those presented in their original papers, although it is difficult to compare results across papers (you should at least mention the type of CPUs used).\n\n[1] Babenko and Lempitsky. Efficient Indexing of Billion-Scale datasets of deep descriptors. CVPR 2016.\n\n[2] Busolin et al. Early Exit Strategies for Approximate $k$-NN Search in Dense Retrieval. CIKM 2024."}, "questions": {"value": "- How is your proposed hierarchical clustering method related to NO-IMI?\n\n- In Appendix A.2. you write that you cache frequently accessed large cells in memory. Is that strategy in use in the experiments and do the other compared methods use a similar strategy?\n\n- How does the indexing time of DiskHIVF compare to the indexing times of the other methods?\n\n- Can you list all the query hyperparameters in your method that need to be tuned for each dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zD8jH86UWl", "forum": "aKCYSL14HX", "replyto": "aKCYSL14HX", "signatures": ["ICLR.cc/2026/Conference/Submission5411/Reviewer_CsCH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5411/Reviewer_CsCH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854445571, "cdate": 1761854445571, "tmdate": 1762918044324, "mdate": 1762918044324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}