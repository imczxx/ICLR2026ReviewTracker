{"id": "CFLlRCyqOf", "number": 16338, "cdate": 1758263343867, "mdate": 1763683364888, "content": {"title": "Preemptive Detection and Steering of LLM Misalignment via Latent Reachability", "abstract": "Large language models (LLMs) are now ubiquitous in everyday tools, raising urgent safety concerns about their tendency to generate harmful content. The dominant safety approach -- reinforcement learning from human feedback (RLHF) -- effectively shapes model behavior during training but offers no safeguards at inference time, where unsafe continuations may still arise.\nWe propose BRT-Align, a reachability-based framework that brings control-theoretic safety tools to LLM inference. BRT-Align models autoregressive generation as a dynamical system in latent space and learn a safety value function via backward reachability, estimating the worst-case evolution of a trajectory. This enables two complementary mechanisms: (1) a runtime monitor that forecasts unsafe completions several tokens in advance, and (2) a least-restrictive steering filter that minimally perturbs latent states to redirect generation away from unsafe regions. Experiments across multiple LLMs and toxicity benchmarks demonstrate that BRT-Align provides more accurate and earlier detection of unsafe continuations than baselines. Moreover, for LLM safety alignment, BRT-Align substantially reduces unsafe generations while preserving sentence diversity and coherence. Qualitative results further highlight emergent alignment properties: BRT-Align consistently produces responses that are less violent, less profane, less offensive, and less politically biased. Together, these findings demonstrate that reachability analysis provides a principled and practical foundation for inference-time LLM safety.", "tldr": "We develop methods for preemtive detection and steering of LLM misalignment using tools from control theory.", "keywords": ["LLM alignment", "control theory", "AI safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b27fd00d45ad9c986e48ff9983dabef5eafb0f1.pdf", "supplementary_material": "/attachment/cebb5fe8be58e4f650588d5889d6af3466874e55.zip"}, "replies": [{"content": {"summary": {"value": "Their method trains a classifier to determine if a partial LLM generation will be completed harmfully. In RL-BTR-ALIGN, the labels are derived using the safety of the partial generation by itself, and the discounted safety of the rest of the generation. In SAMPLE-BRT-ALIGN, the classifier is always trained to predict the safety of the final generation given the partial generation.\n\nThey also develop a method to steer the model away from the unsafe generation once it’s detected."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Their work is mostly easy to understand.\n\nThe novelty is sufficient with the reachability analysis and steering method. Overall I like the methodology of applying control theory to LLM safety.\n\nTheir inference time steering technique substantially increases safety rate, with only a small coherence cost. They also calculate results across multiple training seeds, making the significance clear."}, "weaknesses": {"value": "I don’t think their baselines are correct in figures 2 and table 2.\nTheir baselines either predict everything is safe or everything is unsafe. I don’t think this is correct, because the SAP paper reports a classification accuracy of 86% for Llama2-7B on BeverTails (figure 6). (The BeaverTails dataset is relatively balanced between safe and unsafe: “44.64% were assigned the safe meta-label, while the remaining 55.36% were categorized under the unsafe meta-label.”)\n\nClarity\n\nIn section 4.3, they don’t explain how the argmax part of equation 1 is computed. This is a significant issue since it’s one of the main parts of the method.\n\nThe definition of ℓ(zt) is confusing. z_t is the layer-l embedding of the last emitted token, but section 5 says ℓ(zt) is calculated using a classifier given the token sequence y_0 to y_t.\n\nThey say “we assume that LLM deterministically selects the most likely token”, but it doesn't discuss the consequences of this assumption. Will their approach work with non deterministic decoding strategies? \n\nMinor points\n\nThe paper could better motivate why it’s necessary to classify partial generations as leading to unsafe outputs. For example, an alternative would be to generate the entire response from the LLM, and then classify the entire response as safe or unsafe before showing it to the user, or acting on it. The paper could mention that needing to wait until the entire response is generated before classifying will increase streaming latency."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fncQMim7il", "forum": "CFLlRCyqOf", "replyto": "CFLlRCyqOf", "signatures": ["ICLR.cc/2026/Conference/Submission16338/Reviewer_eDJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16338/Reviewer_eDJ8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805194527, "cdate": 1761805194527, "tmdate": 1762926474008, "mdate": 1762926474008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BRT-ALIGN, a reachability-based framework to conduct safety alignment for LLM during inference time. It models the autoregressive generation of LLM as a dynamical system in latent space and then learn a safety value function via backward reachability. Building on this, BRT-ALIGN is able to forecast the unsafe tokens during inference on-the-fly and take proactive intervention. It can also conduct a guided-perturbation on the latent states to steer the generation away from unsafe regions. Evaluation results on 5 LLMs demonstrates the effectiveness of BRT-ALIGN on improving safety alignment compared to baseline approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper studies the safety alignment of LLMs, which is a critical problem for the community.\n- The proposed BRT-ALIGN brings the control theory and inference-time safety alignment, which is novel and very interesting.\n- The evaluation results demonstrate the effectiveness of BRT-ALIGN, compared to other control-based alignment approaches."}, "weaknesses": {"value": "- Lack of comparison on several SOTA alignment techniques.\n- Lack of comparison on jailbreak attacks.\n- The preservation of utility after applying BRT-ALIGN shall be justified by evaluating on more challenging benchmark."}, "questions": {"value": "I think it is an interesting paper that introduces a novel technique. However, my main concerns lie in the evaluation section, which could be expanded to better contextualize the performance relative to existing methods.\n\n1. Similar properties, such as preemptive detection, have been discussed in prior none control-based alignment approaches, for example, Circuit Breaker [1]. A direct comparison would help clarify the unique contribution of this work.\n\n2. Currently, the evaluation is primarily conducted on raw harmful prompts. It would strengthen the paper to also test the proposed method against harmful prompts generated by jailbreak techniques such as GCG [2], PAIR [3], and AutoDAN [4].\n\n3. To more comprehensively assess the utility preservation of BRT-ALIGN, I recommend including results from benchmarks designed to measure over-refusal behaviors, such as ORBench [5] and XSTest [6].\n\n---\nReference \n---\n\n[1] Zou A, Phan L, Wang J, et al. Improving alignment and robustness with circuit breakers[J]. Advances in Neural Information Processing Systems, 2024, 37: 83345-83373.\n\n[2] Zou A, Wang Z, Carlini N, et al. Universal and transferable adversarial attacks on aligned language models[J]. arXiv preprint arXiv:2307.15043, 2023.\n\n[3] Chao P, Robey A, Dobriban E, et al. Jailbreaking black box large language models in twenty queries[C]//2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). IEEE, 2025: 23-42.\n\n[4] Liu X, Xu N, Chen M, et al. Autodan: Generating stealthy jailbreak prompts on aligned large language models[J]. arXiv preprint arXiv:2310.04451, 2023.\n\n[5] Röttger P, Kirk H R, Vidgen B, et al. Xstest: A test suite for identifying exaggerated safety behaviours in large language models[J]. arXiv preprint arXiv:2308.01263, 2023.\n\n[6] Cui J, Chiang W L, Stoica I, et al. Or-bench: An over-refusal benchmark for large language models[J]. arXiv preprint arXiv:2405.20947, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CXWnmozkLy", "forum": "CFLlRCyqOf", "replyto": "CFLlRCyqOf", "signatures": ["ICLR.cc/2026/Conference/Submission16338/Reviewer_yGuh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16338/Reviewer_yGuh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894652973, "cdate": 1761894652973, "tmdate": 1762926473387, "mdate": 1762926473387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper centres on introducing a reachability-based framework, inspired by control theory, to mitigate unsafe LLM responses by predicting potential harmful continuations and steering the model toward safer outputs, a method referred to as BRT-ALIGN. It positions this as the first application of reachability analysis for inference-time LLM safety, offering a principled approach to both anticipate and prevent harmful text generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-structured and clearly written, with helpful figures and explanations that make the novel reachability-based approach easy to follow.\n\n- The approach is quite novel as this paper claims to be the first to apply backward reachability in LLM latent space, enabling formal prediction and prevention of unsafe continuations.\n\n- The paper presents a comprehensive safety framework that integrates a reachability-based monitor for early detection with a lightweight steering filter that subtly guides the model’s latent states away from unsafe regions.\n\n- The inclusion of multiple evaluation metric provides a holistic picture of how the proposed method outperforms the existing strategies"}, "weaknesses": {"value": "- There exist some methodologies that have worked on safe generation of LLMs at inference time (Safe Infer : https://arxiv.org/abs/2406.12274, Safe Decoding : https://arxiv.org/abs/2402.08983, Self-CD : https://arxiv.org/abs/2401.17633) Including these as baselines would strengthen the comparison and contextualize the proposed approach more comprehensively.\n\n- One of the primary assumptions of the framework is greedy decoding (line 144) to create a deterministic latent trajectory. This removes randomness and makes reachability tractable, but it raises concerns as many LLM applications use sampling (top-k, nucleus) for diversity. It is unclear how well BRT-Align would generalize when the model samples instead of greedily choosing tokens. This assumption may limit real-world applicability.\n\n- The evaluation focuses solely on offensive/toxic language as the failure mode. The failure set is defined via an offensive-language classifier. While toxicity is important, LLM misalignment encompasses other harms (misinformation, self-harm, illegal advice, privacy breaches, etc.). The paper acknowledges this limitation, but currently BRT-Align is only validated on one dimension of harm. Its effectiveness for other harmful content is untested.\n\n- It would be interesting to see if this methodology can somehow be modified to include proprietary LLMs as currently it is limited to open source models.\n\n- The evaluation uses automatic metrics (classifier labels for safety, cosine similarity for coherence, n-gram diversity). While these are reasonable, human evaluation of output quality and safety would strengthen the validation. \n\n- In some places, few notations are not previously stated making it a bit difficult to follow. For instance., W in line 134, fLM  in line 135, etc"}, "questions": {"value": "Please refer to the weakness section and address those concerns"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XtPMneZIYO", "forum": "CFLlRCyqOf", "replyto": "CFLlRCyqOf", "signatures": ["ICLR.cc/2026/Conference/Submission16338/Reviewer_c3nM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16338/Reviewer_c3nM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902203935, "cdate": 1761902203935, "tmdate": 1762926472950, "mdate": 1762926472950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BRT-ALIGN, a novel framework for inference-time safety alignment of LLMs. The core idea is to model LLM token generation as a dynamical system in a latent embedding space and apply reachability analysis, a technique from control theory. By computing a \"backward reachable tube\" (BRT), the method can identify latent states that will inevitably lead to unsafe completions (e.g., toxic or harmful content). This enables a steering mechanism that minimally perturbs the latent state to guide generation toward safe outputs. The authors propose two variants for learning the required safety value function: RL-BRT-ALIGN (using a Bellman recursion) and SAMPLE-BRT-ALIGN (a simpler supervised approach). Through comprehensive experiments on five open-source LLMs and three safety benchmarks, the paper demonstrates that BRT-ALIGN significantly outperforms existing control-theoretic baselines in both the accuracy of unsafe content detection and the effectiveness of safety steering, all while incurring lower computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Framing generation as a dynamical system and using the BRT to formally define the set of \"doomed\" trajectories is a highly principled approach that moves beyond reactive, token-level filtering. It provides a strong theoretical foundation for anticipating misalignment behaviors.\n2. A key contribution, well-supported by the results, is the model's ability to detect unsafe completions 7-10 tokens in advance. This \"early warning\" system is crucial for building robust and trustworthy AI systems."}, "weaknesses": {"value": "1. While the application of the formalism of reachability analysis and the backward reachable tube is novel, the more general idea of learning a value function to predict future unsafe states in LLMs has been explored in prior work [1][2].\n2. The evaluation is missing a crucial and straightforward baseline: prompt engineering. A simple baseline that prepends a safety instruction (e.g., \"Do not generate any offensive or harmful content.\") to the user's prompt could significantly improve the safety rate with little inference overhead. Without comparing against such a baseline, it is difficult to assess the practical necessity and cost-benefit of the proposed control-theoretic approach.\n\nRelated Work:\n\n[1] Systematic Rectification of Language Models via Dead-end Analysis (https://openreview.net/forum?id=k8_yVW3Wqln)\n\n[2] Decoding-time Realignment of Language Models (https://arxiv.org/pdf/2402.02992)"}, "questions": {"value": "1. For the steering filter in Equation 1, the search for a better latent state is performed by sampling within an L2-norm ball. How many samples are typically drawn to perform this maximization? \n2. The model \"Mistral-8B-Instruct-2410\" is consistently misspelled as \"Ministral-8B-Instruct-2410\" (e.g., line 245, 387, 428, and in the references)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NVCyxDIEBg", "forum": "CFLlRCyqOf", "replyto": "CFLlRCyqOf", "signatures": ["ICLR.cc/2026/Conference/Submission16338/Reviewer_caSc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16338/Reviewer_caSc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160393201, "cdate": 1762160393201, "tmdate": 1762926472419, "mdate": 1762926472419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to the Reviewers and Area Chairs"}, "comment": {"value": "Dear Reviewers and Area Chairs,\n\nThank you for the thoughtful feedback and the opportunity to address any concerns about our work. We have provided detailed, point-by-point responses to each reviewer in our rebuttal. Below, we summarize the main clarifications and additional experiments conducted during the rebuttal to help clarify and strengthen the paper’s contributions.\n\n- **New Experiments with Stochastic Decoding.** Several reviewers raised concerns about our reliance on greedy decoding. Although our current theoretical formulation assumes deterministic dynamics, the framework itself does not require greedy decoding. In response, we conducted new experiments using top-p = 0.95 nucleus sampling and temperature = 0.7, presented in Appendix Stochastic Decoding with p = 0.95 Nucleus Sampling. We demonstrate that BRT-Align continues to maintain high safety detection rate under stochastic decoding. These results provide initial evidence that BRT-Align extends naturally to stochastic decoding and represent a promising direction for future work. Please see our response to Reviewer c3nM for details.\n- **New Experiments with a Prompt-Engineering Baseline.** Reviewer caSc requested a comparison against a simple safety-prompt baseline. During the rebuttal period, we implemented this baseline and evaluated it on all unsafe-eliciting prompts. Across datasets using Qwen2-1.5B, we observed only a ~1.60% average improvement in safety rate with prompt-engineering, substantially below the ~85% improvement achieved by BRT-Align. This highlights the benefit of control-theoretic steering relative to passive prompt engineering. Please see our response to Reviewer caSc for the details of this experiment and results. \n- **New Experiments with an Over-Refusal Benchmark, XSTest.** Reviewer yGuh suggested evaluating whether our method induces excessive refusals, essentially evaluating whether our method can be too conservative. We conducted a manual evaluation on XSTest (an over-refusal benchmark) and found that BRT-Align does not meaningfully increase refusal rates relative to the base LLM. Results are provided in Appendix Over-Refusal in LLM Safety Alignment. Please see our response to Reviewer yGuh for additional details.\n- **Clarifications on Evaluation with Prompts Generated by Common Jailbreak Attacks.** Reviewer yGuh also recommended testing BRT-Align against jailbreak-generated prompts (e.g., AutoDAN). We clarify that the UltraSafety dataset used in our evaluation already contains 830 prompts generated via AutoDAN. Our existing results therefore already reflect performance against a widely studied attack. This clarification is included in the updated manuscript and in our detailed response to Reviewer yGuh.\n- **Clarifications on the Low F1 score for the SaP baseline (Figure 2).** The low F1 score for the SaP baseline in our original submission is due to (i) a bug in our F1 computation that understated SaP’s score, and (ii) a fundamental mismatch between SaP’s reported 86% accuracy and our reported F1 values, due to differences in dataset construction and evaluation objective (post-hoc classification vs. first-unsafe-token detection), making a direct comparison challenging. We have corrected the implementation bug, updated all affected results, and clarified these differences in the revised manuscript (Sections Dataset Construction and Evaluation Protocols). Further details are provided in our response to Reviewer eDJ8.\n\nBeyond these additions and corrections, we have provided detailed responses to all reviewer comments and have incorporated clarifications into the revised manuscript where appropriate. We thank the reviewers and ACs for their constructive feedback and welcome any further suggestions.\n\nBest,\n\nSubmission 16338 Authors"}}, "id": "aqQTNYQMha", "forum": "CFLlRCyqOf", "replyto": "CFLlRCyqOf", "signatures": ["ICLR.cc/2026/Conference/Submission16338/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16338/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission16338/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763683521431, "cdate": 1763683521431, "tmdate": 1763683521431, "mdate": 1763683521431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}