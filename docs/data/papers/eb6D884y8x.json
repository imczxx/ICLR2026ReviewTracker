{"id": "eb6D884y8x", "number": 7927, "cdate": 1758043670597, "mdate": 1759897821780, "content": {"title": "Emergence of Hierarchical Emotion Organization in Large Language Models", "abstract": "As large language models (LLMs) increasingly power conversational agents, understanding how they model users' emotional states is critical for ethical deployment. Inspired by emotion wheels -- a psychological framework that argues emotions organize hierarchically -- we analyze probabilistic dependencies between emotional states in model outputs. We find that LLMs naturally form hierarchical emotion trees that align with human psychological models, and larger models develop more complex hierarchies. We also uncover systematic biases in emotion recognition across socioeconomic personas, with compounding misclassifications for intersectional, underrepresented groups. Human studies reveal striking parallels, suggesting that LLMs internalize aspects of social perception. Beyond highlighting emergent emotional reasoning in LLMs, our results hint at the potential of using cognitively-grounded theories for developing better model evaluations.", "tldr": "", "keywords": ["Large Language Models", "Emotion", "Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d955a738f6babd8a717269010886d05902608e9b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the internal organization of emotional understanding within LLMs, drawing inspiration from human psychological models like the emotion wheel.\nThe authors propose a novel algorithm to extract a hierarchical \"emotion tree\" based on probabilistic dependencies between emotional states predicted by the LLMs.\nExperiments on the Llama family of models show that these hierarchies evolve with model size and are consistent with human psychology.\nAdditionally, the emotional recognition capabilities are prone to systematic biases across different socioeconomic personas.\nOverall, the work contributes a complementary perspective to standard emotion classification benchmarks, emphasizing emergent structures in LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly structured and and the analysis is interesting.\n2. The tree-construction algorithm is innovative and easily extendable to other classification tasks (such as wine aroma hierarchy)."}, "weaknesses": {"value": "1. The description of Exp1 and Exp2 in Figure 2 is not clear enough, especially in the top right part. For the readers' ease of understanding and readability, the reviewer suggests that the location of Exp1 should be explicitly marked.\n2. The data sources lack diversity. Experiments 1 and 2 used synthetic data generated by GPT-4o, resulting in an overly homogenous data distribution. Furthermore, the paper also notes that LLM has biases in understanding certain emotions (for example, difficulty identifying \"surprise\"), which may lead to bias in the generated data.\n3. The paper mainly conducts experiments on the LLaMA series of models, which is too single. It lacks experiments on more types of models (such as large reasoning models and MoE architecture models).\n4. In the Discussion section, the authors claim \"reinforcement learning helps close this gap by rewarding reductions in prediction error,\" but do not explain it.\n\nMinor comments:\n- The subtitles (a,b,c,d) and caption in Figure 8 are inconsistent with lines 370-372."}, "questions": {"value": "1. Experiment 1 extracts the probability distribution of the next token by appending \"The emotion in this sentence is\" to each prompt. However, LLM is very sensitive to prompts. Have you conducted an experiment to verify the robustness of this extraction method using different prompts (e.g., \"The emotion in this text is\" / \"The emotion is\" / \"The sentiment is\").\n2. For Experiment 1, due to the inherent bias of LLMs, did the authors count the frequency of each emotion? Would this affect the experimental conclusions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ia4NZj5U7s", "forum": "eb6D884y8x", "replyto": "eb6D884y8x", "signatures": ["ICLR.cc/2026/Conference/Submission7927/Reviewer_oZ7X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7927/Reviewer_oZ7X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484627925, "cdate": 1761484627925, "tmdate": 1762919949904, "mdate": 1762919949904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel benchmark to evaluate large language models’ (LLMs) understanding of human emotions.\nInspired by psychological theory, the authors design a framework based on the emotion wheel to analyze how LLMs recognize emotions hierarchically.\nThey further examine how these models behave under different personas (e.g., high-income vs. low-income, gender, and race) to uncover systematic demographic biases in emotion recognition.\nThe study finds that larger models tend to form more structured hierarchical emotion representations, while also revealing fairness issues when simulating underrepresented groups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novelty: The paper creatively uses the emotion wheel—a psychological model—to evaluate hierarchical emotion recognition in LLMs.\n\n- Relevance: As LLMs increasingly engage in human-like dialogue, studying their emotional recognition and biases is timely and practically valuable.\n\n- Clarity: The presentation is clear and easy to follow. The logical flow helps readers understand both the method and the experiment design.\n\n- Comprehensive analysis: The authors test multiple settings, including different model sizes and assigned personas, to evaluate bias in emotion recognition, which provides valuable insight in practice."}, "weaknesses": {"value": "- Some related works, such as [1], are neglected.\n\n- The analysis of demographic bias (e.g., Figure 7) only covers two emotions (“anger” and “fear”). It would be better to include overall accuracy or more emotion categories to show whether the overall bias is consistent.\n\n- The evaluation data are primarily generated by GPT-4, raising questions about whether the performance and conclusions remain valid for real data produced by humans.\n\n- The hierarchical emotion relations are derived solely from model logits, which appears to be an indirect approach. It seems there is a lack of assessment for the validity of the results.\n\n\n\n[1] Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models. EACL 2024"}, "questions": {"value": "- The authors define the model’s bias as the variation in emotion recognition accuracy when the model is assigned different personas. However, I would suggest considering an alternative perspective: the model could be viewed as an objective evaluator, and bias might more appropriately refer to how its predictions change when recognizing inputs of users from different demographic groups. For instance, the model could be prompted with: \"This sentence was written by a low-income White male. What emotion does it express?\" Such an approach might better capture practical, real-world aspects of bias.\n\n\n\n- Line 419 typo: Only left parentheses, no right parentheses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aqjgiWo4kt", "forum": "eb6D884y8x", "replyto": "eb6D884y8x", "signatures": ["ICLR.cc/2026/Conference/Submission7927/Reviewer_ij2x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7927/Reviewer_ij2x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860428319, "cdate": 1761860428319, "tmdate": 1762919949400, "mdate": 1762919949400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper seeks to understand how LLMs  recognize and model emotions in the context of a conversation. It does this by constructing a hierarchy from the LLM logits. The authors never directly evaluate this method. The authors then explore model bias in how a persona classifies emotions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The method from Section is a good method and an interesting way to extract emotional hierarchies from LLMs."}, "weaknesses": {"value": "The paper presents its results with a lack of clarity and care for the reader. The methodological contribution is never properly evaluated against a ground truth. The text of the paper contradicts the results presented. This paper should and needs to be rewritten for it to be an interesting and meaningful contribution. My suggestion is to focus on the method from section 3 and flesh it out. Furthermore, if the author wants to claim model bias, they should (a) make sure that the stated bias aligns with their results and (b) explore why this bias exists. There are many papers on model biases, but without an exploration of how the bias in this case arises and how to potentially mitigate it (or land in a different area of the data distribution that may no longer have the specific bias), this particular exploration adds little new information. \n\nThe methodology of using a persona is also not well justified. LLMs do not experientially experience the world. They furthermore do not experientially experience being “White”, “Black”, “Hispanic”, “Asian”, “Low-Income”, “High-income”. At most this can show what these terms were associated with in the pretraining dataset. Is it possible that these models just do not see as much text relating to “low-income” persons or from “low-income” persons? If so, how can the authors claim that the issue is with the models and not the training dataset? Without treatment of this subject, it is not clear what conclusions the authors want the reader to takeaway with regard to reducing model bias.\n\n-\tWeaknesses:\no\tThere is no direct evaluation of the method presented in section 3. Why is the accuracy of the method not measured against a multi-label dataset?\no\tLine 310-311 is directly contradicted by Figure 5. Neither Hispanic nor Asian is majority demographic and the Llama-405B persona for both of those outperform the persona for White. “Some college” outperforms all other categories. “Low White Male” outperforms “High White Male”, “Low White Female” performs about as well as “High White Male” and outperforms “Low White Female”, “High Black Male” outperforms or performs just as well as all White categories except for “Low White Male”. “Low Black Male”, “Low Black Female”, and “High Black Female” all underperform the other categories. The paper would be more interesting if a more nuanced view was taken instead of just taking the simplistic framing of “majority” vs “minority”, as that is not what seems to be happening in the results.\n\tFurthermore, in Figure 16 this pattern continues with both “Hindu” and “Muslim” outperforming “Christian” and “None” and “Buddhist” performing about the same.\n\tFor age, the model performs poorly for ages 5 and 10, but I would guess that that is because there is less data online on the experience of being 5 and 10.\no\tFigure 5c xlabels can be spaced apart a bit more.\no\tExpand on lines 319-320. It was difficult to follow what Figure 6 was referring to. Not enough introduction.\no\tHaving a persona be “depression” seems to be flawed as depression is directly related to being able to only experience a certain set of emotions and with emotional listlessness. I am not surprised that a “depressed” persona is less able to recognize emotions.\no\tFigure 6, further refutes the simplistic framing this paper chose with “majority” vs “minority”.\no\tFigure 7 seems to be cherry-picked. If you want to show misclassification error, use confusion matrices. This presentation is neither clear or insightful. \n\tFigure 15 is as confusing and difficult to interpret as Figure 8. If you want to show confusion matrices, show it as a heatmap or as a confusion matrix. Chord diagrams are difficult to read.\no\tWhy are all the percentages in Figure 8 “0%”?\no\tHow can you claim that there is religious bias in Figure 8 if you are not showing a comparison against other religions in the same figure?\n\tYou reference Figure 18 to compare to, but Figure 18 only uses 6 emotions instead of the 135. Yes, the colors align, but it still makes it difficult to compare when you graph the same things in multiple different ways. Additionally, there still aren’t any other religions that you compare to in Figure 18 to support the claim that you made.\no\tIs 60 participants representative? How were they recruited? What are their demographics? These are all important information that should be in the paper and yet I don’t see.\no\tFigure 9 – look at comments for Figure 8 and 15. Also why is there no correlation score displayed here?\n-\tSuggestions (no place to suggest things in a neutral way, so I am putting it here):\no\tPut Figure 13 in the main body of the paper. The correlation between the derived emotion-wheel from the models and human psychological models is an important part of the paper, in my opinion, and should not be shunted to an appendix."}, "questions": {"value": "-\tHow are the authors sure that the emotions recognized via the method described in lines 126-135 are the same as the ones recognized when the model is not asked to recognize emotions?\n-\tThe emotion hierarchy finding algorithm proposed is interesting but is there any external validation done to make sure it is picking up on the right hierarchies of emotion in labeled datasets? (can use multi-label datasets for this)\n-\tYou noted in the main body of the paper that as model size increases, the complexity, depth, and breadth of the emotion organization increases. However, in the appendices, the correlation between the emotion hierarchies derived from the model and the human psychological model does not have the same nice linear relationship. Why do you think that is? Do you consider the human psychological model a sort-of ground-truth for emotional categorization systems? If the correlation does not increase with the increase in depth, breadth, and complexity, what is the use of that extra depth, breadth, and complexity if it is not necessarily correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Evi1lcU5Uj", "forum": "eb6D884y8x", "replyto": "eb6D884y8x", "signatures": ["ICLR.cc/2026/Conference/Submission7927/Reviewer_VfGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7927/Reviewer_VfGQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921837917, "cdate": 1761921837917, "tmdate": 1762919948774, "mdate": 1762919948774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the internal representation of human emotions in Large Language Models (LLMs), moving beyond simple classification accuracy. The authors introduce a novel, cognitively-inspired method to reconstruct a hierarchical organization of emotions by analyzing the probabilistic dependencies in LLM output logits. The core findings demonstrate that as LLMs scale in size (from GPT-2 to Llama 405B), they spontaneously develop more complex and nuanced emotion hierarchies that increasingly align with established psychological frameworks, such as Shaver et al.'s emotion wheel. Furthermore, the study uncovers systematic biases in emotion recognition when LLMs adopt different demographic personas, revealing lower performance for underrepresented and intersectional groups (e.g., low-income Black females). Crucially, a user study confirms that these biases and misclassification patterns in LLMs show striking parallels to those observed in human participants, suggesting the models internalize aspects of social perception."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel, interpretable methodology grounded in psychology. The paper proposes a clear tree-construction procedure from next-token distributions and aligns the resulting hierarchies with established emotion frameworks (e.g., emotion wheels). This bridges LLM evaluation with cognitive science in a way that is both conceptually sound and easy to visualize.\n2. Convincing multi-scale analysis showing emergence with model size. Evaluating models from small to very large parameters demonstrates a consistent increase in hierarchical structure and coherence. The observed trends remain broadly stable across reasonable threshold choices, which supports the generality of the finding.\n3. Quantitative validation linking structure to behavior. The correlations between tree geometry and human-inspired distances, as well as between tree metrics and recognition accuracy, provide welcome quantitative grounding. This strengthens the paper's claims beyond qualitative visualization and links the model's internal structure to its external performance.\n4. Thoughtful treatment of bias with triangulation via a user study. The intersectional analysis spans multiple demographic axes, and the inclusion of a human study helps situate model errors relative to human patterns. The authors' transparency about the study's limitations further strengthens the credibility of this section."}, "weaknesses": {"value": "1. Dependence on LLM-generated data: Core experiments use GPT-4-generated scenarios, risking transfer of stylistic and demographic biases. Stronger validation on human-annotated datasets (e.g., GoEmotions) is needed.\n2. Unvalidated modeling assumption: The use of next-token probabilities as proxies for P(emotion∣scenario) is not empirically tested. Comparisons with probing or clustering methods could substantiate it.\n3. Evaluation mismatch: Humans used a 6-way classification task while models operated over 135 emotion terms, complicating comparisons. Aligning task structures would clarify results.\n4. Weak robustness analysis: Only one prompt and heuristic threshold were tested; broader ablations on prompt design, temperature, and threshold selection would improve stability and reproducibility."}, "questions": {"value": "1. Regarding the reliance on LLM-generated data: Thank you for acknowledging the limitation of using GPT-4o to generate scenarios. Could you elaborate on why this approach was chosen over using existing human-annotated datasets (like GoEmotions, which was analyzed in the appendix) for the main experiments? Furthermore, have you considered a smaller-scale validation where you compare the emotion trees generated from GPT-4o scenarios versus those from a matched set of human-written scenarios to see if the core hierarchical structures remain consistent?\n2. On the validity of the core modeling assumption: The paper's central claim rests on the assumption that next-token probabilities after a specific prompt accurately reflect the model's conditional probability P(emotion|scenario). This is an elegant but strong assumption. Could you provide more intuition or perhaps a preliminary analysis on why this method is preferable?\n3. Regarding the human-LLM evaluation mismatch: The comparison between the model's 135-class predictions and the humans' 6-way forced-choice decisions is insightful but indirect. Could you discuss the potential impact of this mismatch on your conclusions about bias alignment?\n4. Concerning the robustness of the tree-construction method: The hierarchical trees are a key contribution, but their structure depends on a single prompt template and a heuristic threshold. How sensitive are the specific parent-child relationships in the trees to minor variations in the prompt (e.g., \"The feeling expressed here is...\")? Additionally, while you show that overall trends are stable across thresholds, could you suggest a more principled or automated way to select an optimal threshold t, perhaps one that maximizes alignment with a known psychological structure or optimizes a graph-theoretic property like modularity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X2asj3nUOe", "forum": "eb6D884y8x", "replyto": "eb6D884y8x", "signatures": ["ICLR.cc/2026/Conference/Submission7927/Reviewer_H64C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7927/Reviewer_H64C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984403040, "cdate": 1761984403040, "tmdate": 1762919948148, "mdate": 1762919948148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}