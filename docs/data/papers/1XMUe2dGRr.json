{"id": "1XMUe2dGRr", "number": 10471, "cdate": 1758172577735, "mdate": 1759897648833, "content": {"title": "Towards Persistent Noise-Tolerant Active Learning of Regular Languages with Class Query", "abstract": "Large Language Models (LLMs) are increasingly deployed in human–AI collaborative decision-making systems, where they are expected to align precise formal representations with ambiguous natural language. However, their ad hoc strategies for resolving ambiguity often lead to hallucinations and inconsistencies. We formalize this setting via probabilistic Minimally Adequate Teachers (pMATs) that (i) answer membership queries with fixed but possibly flipped labels, and (ii) return valid counterexamples to hypothesis equivalence. We present **CAPAL** (**C**lass-query **A**ctive, **P**ersistent-noise-**A**ware **L**earning), an active algorithm for learning deterministic finite automata (DFAs) that remains correct under persistent membership noise without demonstrations. CAPAL augments the classic \\$L^\\star\\$ loop with two components grounded in our implementation: (1) a *class query* realized as a statistical same-state test that compares disagreements between two prefixes against a noise-floor estimate \\$\\hat{\\eta}\\$ with Hoeffding tolerances; (2) a *discrimination tree* that selects a near-minimal discriminator, keeping the core suffix set small. An efficient micro-bootstrap and cache-reuse scheme estimates \\$\\hat{\\eta}\\$ with few new queries. We prove convergence given a perfect language-equivalence oracle and show substantial membership-query savings in practice. Our evaluation across multiple benchmarks, including RegexLib and KB13, demonstrates that this approach enhances both the efficiency and robustness of DFA learning under noisy oracles, supporting the view of LLMs as fallible yet useful collaborators for synthesizing verifiable formal artifacts.", "tldr": "", "keywords": ["Active Learning", "Automata Theory", "Large Language Models", "Regular languages", "Value Alignment", "Preference Modeling"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9026b3482a1f2ca708acfe3ad684883227f0b19.pdf", "supplementary_material": "/attachment/4ca2074e00c07527201ed1d02d2f6a25f026ceb1.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies a new learning theory problem for regular languages, have a noisy membership query (MQ) oracle, but a perfect equivalence query oracle in a L* learning setup. Such noisy MQ oracles could be LLMs, though I am not sure why they are needed for the simple problems studied. The proposed algorithm CAPAL shows high accuracy with fewer calls to the MQ oracle."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. I like the problem formulation of pMAT and the recognition of persistent errors.\n2. The work studies a variety of prompting strategies and compares them for their effectiveness within membership query oracles. \n3. The code-based oracle is pretty interesting and gives consistently better results."}, "weaknesses": {"value": "1. My biggest criticism of the paper is in the motivation. I think the abstract and intro could provide more evidence for the utility of the problem studied from the practical perspective, with detailed, concrete examples, especially involving LLMs for membership queries. The abstract doesn't introduce the problem and is hard to follow.\n2. I question whether the assumption of a perfect EQ oracle is realistic. For the experiments, how is the perfect EQ oracle implemented? How would we get a perfect EQ oracle for practical problems?\n3. The main algorithm/contribution, CAPAL's description is pretty dense and hard to follow. Multiple crucial steps are introduced, but there is less background provided to fully understand and appreciate it.\n\nI recommend rejection mainly for the paper's writing, which currently lacks motivation for the problem and its assumptions and is pretty dense. I encourage the authors to simplify the writing, maybe by including some examples, which can make their contributions more accessible."}, "questions": {"value": "1. Why is the code based oracle deterministic? In repeated samples from the MQ oracle LLM, we could get different codes and hence even this scenario is stochastic?\n2. I wonder whether the CAPAL algorithm necessarily needs LLMs. What would it look like theoretically and empirically for any other kind of  noisy MQ oracle, other than LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4fKljGz2U4", "forum": "1XMUe2dGRr", "replyto": "1XMUe2dGRr", "signatures": ["ICLR.cc/2026/Conference/Submission10471/Reviewer_w6ef"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10471/Reviewer_w6ef"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718056926, "cdate": 1761718056926, "tmdate": 1762921767699, "mdate": 1762921767699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of using Large Language Models (LLMs) to learn formal representations, specifically deterministic finite automata (DFAs). \n\nIt proposes a new learning model called the Probabilistic Minimally Adequate Teacher (pMAT). This framework assumes the LLM acts as a noisy membership query (MQ) oracle but is paired with a perfect equivalence query (EQ) oracle that provides definitive counterexamples to incorrect hypotheses.\n\nThe paper presents CAPAL (Class-query Active, Persistent-noise-Aware Learning), an algorithm designed to learn Deterministic Finite Automata (DFAs) correctly within the pMAT model. Instead of trusting individual noisy answers, CAPAL uses a statistical class query to determine if two prefixes belong to the same state and a discrimination tree to efficiently refine its hypothesis with minimal new queries.\n\nExperiments show that CAPAL significantly outperforms standard active learning algorithms in noisy environments. The study also finds that having the LLM generate a code-based oracle (a deterministic program to answer queries) is far more effective than direct prompting, reducing errors by over 90% and cutting LLM calls to just one per task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  It introduces a novel learning framework (pMAT) and a provably correct algorithm (CAPAL) to solve it. The paper formalizes the problem of \"persistent noise,\" where an LLM is consistently wrong about certain queries, which is a more challenging and practical scenario than random errors. The proposed CAPAL algorithm is specifically designed to be robust to this noise by using a statistical \"class query\" and is backed by a formal proof of convergence, demonstrating its theoretical soundness.\n2.  It provides a solution with the \"code-based oracle.\" The paper's practical finding is that using an LLM a single time to synthesize a deterministic program (a code-based oracle) is far more effective than using it for repeated queries. \n3. The claims are supported by a thorough and rigorous experimental evaluation. The authors validate it extensively against multiple different active and passive learning baselines across multiple benchmarks."}, "weaknesses": {"value": "1. The entire theoretical guarantee and the practical success of CAPAL hinge on the assumption of a perfect, noise-free Equivalence Query (EQ) oracle that always returns a valid counterexample. This is a limitation, as real-world verifiers (whether human or automated) are often fallible. The paper acknowledges this but does not investigate the consequences. The work could be strengthened by including experiments with an imperfect EQ oracle (e.g., one that fails to find a counterexample with some probability). \n2. The algorithm's strategy for a \"label-only\" counterexample (where a state is simply mislabeled as accepting/rejecting) is to wait for the exact same counterexample to reappear before escalating by adding its suffixes as discriminators . This seems potentially inefficient. A different counterexample that reaches the same mislabeled state would not trigger this escalation, possibly leading to many wasted EQ cycles to fix one incorrect label. The paper would be more convincing if it justified this specific design choice with evidence."}, "questions": {"value": "1. The paper's entire theoretical and practical framework relies on a perfect, noise-free $O_{EQ}$. How do you expect CAPAL to perform if this oracle is imperfect? For instance, what happens if the $O_{EQ}$ fails to return a counterexample for an incorrect hypothesis (a \"false negative\") or provides a spurious counterexample for a correct one (a \"false positive\")?\n2. The paper states that for label-only counterexamples (where the DFA's structure is correct but a state's acceptance is wrong), the algorithm only escalates by adding all suffixes of c to $E_{core}$ if the exact same counterexample c reappears. What is the specific rationale for this \"repeat-only\" escalation policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "56T9tGjmvL", "forum": "1XMUe2dGRr", "replyto": "1XMUe2dGRr", "signatures": ["ICLR.cc/2026/Conference/Submission10471/Reviewer_6djq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10471/Reviewer_6djq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950133699, "cdate": 1761950133699, "tmdate": 1762921767241, "mdate": 1762921767241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a probabilistic framework for learning formal languages from large language models (LLMs) that act as noisy oracles. The authors formalize this setting through probabilistic Minimally Adequate Teachers (pMATs), which can provide incorrect answers to membership queries but always return valid counterexamples for failed equivalence queries. Within this framework, they propose CAPAL (Class-query Active, Persistent-noise-Aware Learning), an algorithm for actively learning deterministic finite automata (DFAs) that remains theoretically correct even when the oracle produces persistent labeling errors. CAPAL extends Angluin’s classic L* algorithm with statistical same-state tests that distinguish language classes under bounded noise, and with a discrimination tree intended to keep the hypothesis compact. The method also estimates the oracle’s noise rate using a lightweight bootstrap procedure that minimizes redundant queries. The authors prove convergence of CAPAL under a perfect equivalence oracle and show empirically that in certain settings the method requires fewer membership queries than existing approaches. More specifically, when applied to datasets such as RegexLib and KB13 the method shows improved robustness in DFA learning from noisy oracles. Overall, the results support the idea that LLMs, despite their fallibility, can serve as useful collaborators for synthesizing verifiable formal models from natural language."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using LLMs to guide automata learning algorithms is interesting. Their implementation CAPAL provide evidence that  active learning of regular languages can be made robust to persistent membership noise when a perfect equivalence oracle is available. The main approach is to shift from trusting individual MQ labels to issuing a class query that aggregates evidence against a principled noise floor, with early stopping and monotone (negative) caching to avoid re-litigating pairs already shown different."}, "weaknesses": {"value": "The main problem is that the approach does not work if the equivalence oracle is approximate, and no extension to the approximate case is provided . This is an issue because perfect equivalence oracles are typically expensive, and in practice approximate equivalence oracles are used."}, "questions": {"value": "No question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8R1avWlKmA", "forum": "1XMUe2dGRr", "replyto": "1XMUe2dGRr", "signatures": ["ICLR.cc/2026/Conference/Submission10471/Reviewer_dPdj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10471/Reviewer_dPdj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988538230, "cdate": 1761988538230, "tmdate": 1762921766943, "mdate": 1762921766943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies active learning of regular languages with membership queries and equivalence queries.This problem regained recent interest due to the usage of LLMs in converting high-level description of regular languages to a deterministic automata, where LLM can serve as an oracle in answering these two types of queries. The paper assumes that responses to membership queries can have persistent noise, while the response to equivalence queries are noiseless. The paper proposes CAPAL algorithm and establishes its correctness guarantees. Experiments show that the proposed algorithm has lower number of queries and a better learning accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- I think the problem motivation is interesting. Converting an informal language description to a formal language allows constructing guardrails of agents, ensuring agent's safety and avoiding unintended consequences in a provable way. \n\n- The experimental results look good to me"}, "weaknesses": {"value": "- I am not sure if assuming the equivalence query oracle is noiseless is practical. Such oracle is also implemented using LLM in practice, am I understanding correctly? Then they may hallucinate?\n\n- I wasn't able to follow the presentation of the CAPAL algorithm, perhaps due to me not being an expert in active learning with regular languages. For example, in line 6, how is sa ~ u defined - is it an equivalence relationship defined by the partition in line 5? In line 8, what are the nodes of the decision tree, and what are the splitting criteria for the tree nodes? \n\nAlso, it is mentioned in line 307-308 that the membership answers equals to the true label with probability 1-eta with eta < 1/2 -- is this for all the queries? If so, then this seems to be inconsistent with Definition 3.2 (where it is mentioned that for some query, the mode of the oracle's response distribution disagrees with the truth)? \n\nAlso, is the label flipping probability assumed to be homogeneous across all queries in theoretical analysis? It seems not the case from the empirical result in Figure 2. \n\n- The paper provides some correctness guarantees of the algorithm (Theorem 1 and 2). It relies on some assumption on the knowledge of the safe noise cap \\bar{\\eta}. Can this be known, or estimated? Alg. 1 line 2 proposes to estimate \\hat{\\eta}, but not enough details are given. \n\n- Is there some formal theorem query complexity that can be given? Without that the theory seems incomplete."}, "questions": {"value": "See questions above.  Also, can the authors comment on why in Figure 3, CAPAL's success rate is not always 1 for all epsilon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zF3m2KUfdt", "forum": "1XMUe2dGRr", "replyto": "1XMUe2dGRr", "signatures": ["ICLR.cc/2026/Conference/Submission10471/Reviewer_A92o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10471/Reviewer_A92o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132713740, "cdate": 1762132713740, "tmdate": 1762921766436, "mdate": 1762921766436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work considers the problem of learning a deterministic finite automaton from a natural language derived oracle. Specifically, an LLM as a membership oracle to answer yes/no questions about a natural language description of a formal language. This can either be done by answering questions one at a time or converting the description into code, e.g., python. A fundamental problem is that the LLM may make systematic errors in labeling queries. This can result in learning arbitrarily incorrect languages. This is classically addressed by the LEARNANYWAY algorithm, however requires small counterexamples to be made practical. This however is typically not the case as counterexamples are often found via formal analysis or random search/conformance testing.\n\nThe contribution of this work is CAPAL algorithm which seems to perform fairly efficiently even in the presence of labeling errors. The key assumption, called pMAT, is that in addition to a natural language membership oracle (with the possibly to err) there is a perfect counter example oracle. The algorithm is relies on a combination of statistical tests and clever usage of a discrimiation tree, ala Kearns and Vazirani."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of extracting formal structure from natural language is timely and important. While easy to generate such things in an ad-hoc manner, e.g., code gen, this and related work help provide rigor and guarantees to such procedures. The focus on DFAs is interesting as it is the simplest structure that allows for memory, being the target of decades of learning research.\n\nThe techniques are as far as I know novel, and the framing of pMAT makes the underlying assumptions and limitations fairly clear. While clearly strong, see weaknesses, it allows for progress made in understanding this problem."}, "weaknesses": {"value": "1. The primary weakness of this approach is the very strong assumption of an equivalence oracle. While there are a few settings this might make sense, e.g., when an (expensive) verification process exists and is cheap compared to the LLM query required for program translation, it obviously not typically available. In such settings conformance testing and random sampling are much more common, typically yielding PAC like guarantees. This seems like a natural next step, and is explicitly acknowledged in the discussion.\n\n2. Another minor writing point is that, in exposition, this paper leans heavily on existing knowledge of automata learning. While not strictly required, I think without a fairly good automata learning background, a reader will likely bounce off of this work. This is particularly likely with much ICLR community. Nevertheless, I think the paper is understandable.\n\n3. Finally, a quick nit pick about exposition with the L*LM related work. My understanding is that the demonstrations only come into play as a mechanism for refining the language *after* an automata has been guessed using a classic learning algorithm. One should able to drop in CAPAL directly into the L*LM framework and still leverage the demonstration modality right?"}, "questions": {"value": "See weakness #3.\n\nAlso, do you have a sense how this would compare against SAT based approaches allowing for up to `k` labels being flipped (where `k` is derived from the noise floor?) Presumably, this is more scalable, but on the hand works like L*LM showed that creating distinguishing sequences of the smallest consistent DFAs resulted in less hallucination opportunities."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6h0y5stPJp", "forum": "1XMUe2dGRr", "replyto": "1XMUe2dGRr", "signatures": ["ICLR.cc/2026/Conference/Submission10471/Reviewer_1zKR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10471/Reviewer_1zKR"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762408074486, "cdate": 1762408074486, "tmdate": 1762921765895, "mdate": 1762921765895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}