{"id": "YAhTj2VgBw", "number": 14634, "cdate": 1758240620483, "mdate": 1759897358126, "content": {"title": "Patching Gaps In LLM Reasoning With Interventional Training", "abstract": "Reinforcement learning (RL) training of large language models (LLMs) is limited by the policy's ability to generate rollouts with non-zero rewards: without such rewards, the policy is not updated and learning is stalled on hard problems, which are problems that the policy consistently fails to sample any correct rollouts for. We find that many hard problems remain unsolved due to the repeated generation of incorrect intermediate steps in a long reasoning trace; identifying and fixing these requires performing better \\emph{credit assignment}. But existing approaches for credit assignment  are either impractical or impose a substantial data-writing  burden on oracles (\\textit{e.g.}, humans). In this paper, we introduce \\textbf{Interventional Training} (InT), a framework that leverages single-step oracle interventions to improve LLM reasoning. Given a reasoning attempt and ground-truth answer, the oracle detects and then provides language feedback on a single intermediate reasoning step, which is much cheaper than obtaining a full reasoning trace. \\methodname{} then \\emph{patches} the LLM by running supervised fine-tuning on the on-policy rollout up to the error, followed by the correction  from the oracle. RL on this patched model now generates counterfactual traces  and with merely $\\approx$$100$ interventions from the oracle, \\methodname{} solves 16\\% more  hard test problems that were  previously unsolved (only zero rewards) and also improves performance across multiple standard evals.", "tldr": "", "keywords": ["LLM", "reasoning", "intervention", "SFT", "RL"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/964f31009957fb3909f36b625724db954081eece.pdf", "supplementary_material": "/attachment/9f35ae3cb1511a73ecf8867191bea357f588663d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Interventional Training (InT), a method to improve reinforcement learning (RL) training of LLMs on hard reasoning problems where the model consistently fails to generate correct rollouts (i.e., receives zero rewards). The key idea is to perform targeted credit assignment by identifying and correcting a single intermediate reasoning step where the model goes wrong, using a lightweight oracle (e.g., a stronger LLM). The corrected step is then used to patch the model via supervised fine-tuning (SFT), enabling it to generate counterfactual correct traces and resume RL training.\n\nKey contributions:\n1. A method to collect single-step interventions from an oracle to correct reasoning errors;\n2. Fine-tune the model on its own prefix + oracle correction, avoiding full trace cloning;\n3. Resume RL from the patched model, now capable of receiving non-zero rewards on previously unsolvable problems;\n4. InT solves 14–16% more hard problems and improves pass@k on both in-distribution and standardized math benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1: This paper addresses a real and underexplored problem—RL stalling on zero-reward problems due to execution errors in long reasoning traces.\n\nS2: InT is lightweight, requiring only ~100 short interventions, and avoids the cost of full oracle traces or dense reward models.\n\nS3: This paper demonstrates consistent improvements over baselines (e.g., distillation, standard RL) on both training and test sets across multiple benchmarks.\n\nS4: Unlike full trace distillation, InT retains model diversity and avoids catastrophic forgetting by only patching on-policy prefixes.\n\nS5: This paper connects to credit assignment and on-policy vs. off-policy learning, with clear ablations and training dynamics analysis."}, "weaknesses": {"value": "W1: This work requires access to a stronger oracle model (e.g., Gemini 2.5 Pro), which may not be available or practical in many settings. The paper does not explore weaker or human oracles or automated error detection.\n\nW2: How to define a step and how to determine whether the split step is reasonable? What is the maximum number of steps after segmentation in this work?\n\nW3: This work was evaluated only on math reasoning tasks with verifiable answers. Generalization to open-ended reasoning, symbolic tasks, or subjective domains is unclear.\n\nW4: RL is only run on 64 hard problems, which raises questions about scalability and stability of the method when applied to larger or more diverse datasets.\n\nW5: While the paper shows where errors occur, it does not analyze what types of errors are fixed, how often the model recovers, or whether it learns to generalize from interventions.\n\nW6: The main baselines are standard RL and full trace distillation, but the paper does not compare with process reward models (PRMs), self-correction, or iterative refinement methods, which are more closely related.\n\nW7: In addition, there are many typos, such as blank pages (page 13, page 17), and incorrect references (Table 3, Table 4, Table5)"}, "questions": {"value": "Q1: How does InT perform with weaker or human oracles? Can the method still work if the oracle is not significantly stronger than the base model?\n\nQ2: What happens if the oracle misidentifies the error or provides a suboptimal fix? Is there a mechanism to filter or validate interventions?\n\nQ3: Does the model learn to self-correct, or does it just memorize interventions? How does it behave on similar but unseen problems?\n\nQ4: Can InT be scaled to larger problem sets or more diverse domains? What are the computational or stability limits of continued RL after patching?\n\nQ5: How does InT compare to process reward models (PRMs) or iterative self-refinement? Would training a PRM be more efficient or effective in the long run?\n\nQ6: Why not train the model to predict its own interventions? Could InT be extended to automate the oracle role via meta-learning or self-critique?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4iSbJmAq0G", "forum": "YAhTj2VgBw", "replyto": "YAhTj2VgBw", "signatures": ["ICLR.cc/2026/Conference/Submission14634/Reviewer_MBkX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14634/Reviewer_MBkX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643572290, "cdate": 1761643572290, "tmdate": 1762925010874, "mdate": 1762925010874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is based on the observation that RL is difficult to train and converge when the initial success rate is very low. To address this issue, the paper proposes an Interventional Training (InT) framework that replaces challenging reasoning steps with oracle interventions. The base model is then tuned using these oracle interventions. Experimental results demonstrate the effectiveness of this approach."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well organized and clearly written.\n2. The motivation is well-defined. The authors also discuss other approaches they have tried and provide reasonable explanations for their limitations, which makes the proposed framework convincing.\n3.  Experimental results support their findings."}, "weaknesses": {"value": "1. Literature review is not sufficient. There is extensive prior work on improving RL performance using oracle guidance or hints. However, this paper does not adequately discuss or compare with such related works. For example, recent works have explored combining RL and SFT to enhance RL performance.\n2. Although the authors repeatedly mention credit assignment, the explicit connection between InT framework and credit assignment is unclear.\n4. Experimental validations are not sufficient. The experiments are conducted with only one base model and one oracle model. It would be helpful to include results across multiple model pairs."}, "questions": {"value": "1. How do the authors identify where oracle interventions should be pinpointed? How to control the length of each generated intervention?\n\n2. Why does the distilled model perform worse than the base model? Does this suggest that the teacher model lacks sufficient reasoning capability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9IVX2gbxuD", "forum": "YAhTj2VgBw", "replyto": "YAhTj2VgBw", "signatures": ["ICLR.cc/2026/Conference/Submission14634/Reviewer_ikPh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14634/Reviewer_ikPh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662492668, "cdate": 1761662492668, "tmdate": 1762925009663, "mdate": 1762925009663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work tackles a highly valuable problem, the failure of reinforcement learning (RL) to improve large language models (LLMs) on “hard” reasoning tasks where no correct rollouts are sampled, resulting in zero reward and stalled learning. The authors propose Interventional Training (InT), which introduces targeted external interventions at the first failure point in a model’s reasoning chain. By forcing a corrected trajectory and continuing RL from this modified trace, the method effectively injects learning signals into regions where RL previously could not make progress."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method directly tackles the zero-reward problem in RL for reasoning models, enabling continued learning even beyond the model’s existing competence boundary.\n2. By identifying and correcting the first erroneous step in the reasoning chain, InT provides much finer-grained credit assignment than standard RL, improving learning efficiency and self-correction capability."}, "weaknesses": {"value": "1. The core idea is not new — applying single-step interventions at failure points has already been explored in other RL and imitation-learning domains. The paper mainly transfers this known concept to LLM reasoning without introducing new techniques.\n2. The method depends on access to ground-truth answers and a strong evaluation model to identify and correct errors, raising concerns about scalability and whether such oracle-dependent training has an inherent upper bound on achievable improvement.\n3. The evaluation primarily compares InT against ablated variants such as Distillation + RL and Standard RL, rather than against more conceptually related methods like process-reward modeling or critique-based self-refinement, which limits the persuasiveness of the results."}, "questions": {"value": "Please refer to the concerns in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MQDulBuzvf", "forum": "YAhTj2VgBw", "replyto": "YAhTj2VgBw", "signatures": ["ICLR.cc/2026/Conference/Submission14634/Reviewer_ufkp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14634/Reviewer_ufkp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996211486, "cdate": 1761996211486, "tmdate": 1762925008567, "mdate": 1762925008567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Interventional Training, a novel framework designed to address credit assignment issues in LLM reasoning during reinforcement learning. The key idea is to leverage single-step oracle interventions (e.g., from a human or another LLM) to correct intermediate errors in model-generated reasoning traces, followed by supervised fine-tuning (SFT) and continued RL. The authors demonstrate that InT improves performance on hard problems where standard RL fails, achieving gains in pass@k metrics and solving previously unsolvable tasks. The contributions include a cost-effective alternative to full trace cloning and a method to patch LLMs without distorting their base distributions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "InT creatively uses localized oracle interventions to patch errors, avoiding the pitfalls of full trace distillation.\nExperimental results show consistent improvements in pass@k and problem-solving rates, with ablations validating design choices."}, "weaknesses": {"value": "The primary weakness is the reliance on an external oracle model (e.g., Gemini 2.5 Pro) for interventions. This introduces practical constraints, such as the cost and availability of high-performance oracles, and potential biases if the oracle's capabilities do not generalize.\nWhile InT reduces data-writing burden compared to full traces, it still requires oracle access, which may not be feasible for all practitioners."}, "questions": {"value": "How can the dependence on an external oracle be minimized? For instance, could the base model be trained to self-correct interventions over time?\nWhat are the trade-offs in using different oracle types (e.g., humans vs. LLMs), and how do they impact reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oMlZpjcJ7j", "forum": "YAhTj2VgBw", "replyto": "YAhTj2VgBw", "signatures": ["ICLR.cc/2026/Conference/Submission14634/Reviewer_jykS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14634/Reviewer_jykS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052119244, "cdate": 1762052119244, "tmdate": 1762925007966, "mdate": 1762925007966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}