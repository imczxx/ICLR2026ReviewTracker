{"id": "JkKkquv5lw", "number": 11415, "cdate": 1758198509175, "mdate": 1759897577038, "content": {"title": "A Study on PAVE Specification for Learnware", "abstract": "The *Learnware* paradigm aims to help users solve machine learning tasks by leveraging existing well-trained models rather than starting from scratch. A learnware comprises a submitted model paired with a *specification* sketching its capabilities. For an open platform with continuously uploaded models, these specifications are essential to enabling users to identify helpful models, eliminating the requirement for prohibitively costly per-model evaluations. In previous research, specifications based on privacy-preserving reduced sets succeed in enabling learnware identification through distribution matching, but suffer from high sample complexity for learnwares from high-dimensional, unstructured data like images or text. In this paper, we formalize **Pa**rameter **Ve**ctor (PAVE) specification for learnware identification, which utilizes the changes in pre-trained model parameters to inherently encode the model capability and task requirements, offering an effective solution for these learnwares. Theoretically, from the neural tangent kernel perspective, we establish a tight connection between PAVE and prior specifications, providing a theoretical explanation for their shared underlying principles. We further approximate the parameter vector in a low-rank space and analyze the approximation error bound, highly reducing the computational and storage overhead. Extensive empirical studies demonstrate that PAVE specification excels at identifying CV and NLP learnwares for reuse on given user tasks, and succeeds in identifying helpful learnwares from open learnware repository with corrupted model quality for the first time. Reusing identified learnware to solve user tasks can even outperform user-fine-tuned pre-trained models in data-limited scenarios.", "tldr": "We formalize the Parameter Vector (PAVE) specification, which encodes model capabilities for efficient learnware identification, eliminating costly per-model evaluations and outperforming fine-tuned pre-trained models in limited-data scenarios.", "keywords": ["Learnware", "Model Specification", "Parameter Vector", "Learnware Identification", "Model Capability"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1a459f450ee82cf4106375757aa5e38d29f64cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Parameter Vector (PAVE) specification to identify and select high-quality learnwares for reuse in solving new user ML tasks. The PAVE approach formalizes the use of parameter vector changes (from fine-tuning a shared pre-trained model) as a representation of both a model’s capability and the requirements of the user’s task. Theoretically, the work connects PAVE to kernel mean embedding approaches via the neural tangent kernel regime, derives error bounds for low-rank approximations of the parameter vectors, and demonstrates through extensive empirical studies that PAVE achieves superior learnware selection, often outperforming conventional fine-tuning and previous learnware identification methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles the practical challenge of efficiently reusing high-performing models by providing a specification (PAVE) that allows users to identify suitable models without direct per-model evaluation.\n- The mathematical exposition is detailed and generally clear.\n- The proposal for low-rank approximation of parameter vectors significantly reduces memory and compute, a nontrivial contribution given the scale of modern check-pointed models.\n- The experimental validation is exceptionally thorough and is a major strength of the paper."}, "weaknesses": {"value": "Generally, I did not identify any major limitations or weaknesses in its core contributions. This is a high-quality paper that makes a practical contribution to the field of model reuse and the Learnware paradigm. The proposed PAVE specification is novel, well-motivated, and supported by both theoretical analysis and a comprehensive set of experiments."}, "questions": {"value": "The PAVE specification, as described in L51-53, appears to rely on a shared pre-trained model and architecture to ensure the comparability of parameter vectors. Could the authors clarify if this is a necessary constraint? Furthermore, could the authors provide discussion on the potential for generalizing PAVE to a more heterogeneous setting, where learnwares in a repository might originate from different base models or possess distinct architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "h8p9Mr98dX", "forum": "JkKkquv5lw", "replyto": "JkKkquv5lw", "signatures": ["ICLR.cc/2026/Conference/Submission11415/Reviewer_iVnw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11415/Reviewer_iVnw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887243589, "cdate": 1761887243589, "tmdate": 1762922533181, "mdate": 1762922533181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new way, called PAVE (Parameter Vector), to describe and identify reusable models in the Learnware framework. Instead of using reduced data samples to represent a model’s capability (as in prior work like RKME), PAVE uses how a model’s parameters change during fine-tuning. This parameter-based “signature” helps match models (learnwares) with user tasks more efficiently and works even when data are high-dimensional (e.g., text or images). The authors also derive a theoretical link between PAVE and RKME through the neural tangent kernel (NTK) and propose a low-rank approximation to make computation feasible. Experiments on NLP and CV tasks show that PAVE can identify suitable learnwares better than baselines and sometimes outperform fine-tuned pre-trained models"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Using parameter changes instead of data samples to represent a model’s capability is original and intuitive, especially for privacy-sensitive or unstructured data.\n2. The connection between PAVE and RKME under the NTK assumption provides a sound theoretical explanation for why the approach should work.\n3. The low-rank approximation (similar to LoRA) reduces the storage and computational cost while maintaining similarity accuracy\n4. The experiments cover multiple domains (NLP, CV, medical LLMs) and show clear and consistent improvements over RKME and fine-tuning baselines."}, "weaknesses": {"value": "1. While the paper introduces PAVE as a general solution, it’s not entirely clear how broadly this parameter-vector representation generalizes beyond fine-tuned models based on shared backbones.\n2. The method assumes all learnwares are fine-tuned from a common pre-trained model. It’s unclear how it performs when models come from different architectures or pre-training distributions.\n3. The experiments mainly compare to RKME and basic fine-tuning baselines. It would strengthen the evaluation to include more data-centric reuse or transferability estimation methods, such as LEEP, LogME, or task2vec, to contextualize PAVE’s advantage.\n4. The paper doesn’t discuss how PAVE specifications would be stored, shared, or updated in a real learnware repository, or how they interact with privacy and security constraints."}, "questions": {"value": "Please refer to the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "p3HkdnsXpD", "forum": "JkKkquv5lw", "replyto": "JkKkquv5lw", "signatures": ["ICLR.cc/2026/Conference/Submission11415/Reviewer_bGit"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11415/Reviewer_bGit"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972263858, "cdate": 1761972263858, "tmdate": 1762922532201, "mdate": 1762922532201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers a setup that we have access to a large number of models. Each model consists of a model and a \"specification\" that describes its capabilities. The central problem is how to create a specification that allows a user to efficiently identify the most helpful model from a vast repository, without the costly process of evaluating every single model.\n\nThe main idea is simple: assume we have access to a trained model h. To create its specification, they take a shared, public pre-trained model (e.g., BERT, CLIP) and fine-tune it to mimic the predictions of their model h. The resulting change in the pre-trained model's parameters is saved as the model vector.  A user with a new task provides a small, few-shot dataset. To create their task specification, they fine-tune the exact same shared pre-trained model on their few-shot data to fit the true labels. This parameter change becomes the task vector. Finally the selection is based on measuring the similarity between task vector and the finetuning vector."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The PAVE method's primary strength is its effectiveness with high-dimensional, unstructured data like images and text, a scenario where prior specifications failed. The authors conducted extensive experiments across Natural Language Processing (NLP),"}, "weaknesses": {"value": "Reliance on a Shared Pre-trained Model: The entire system fundamentally relies on both the developers (creating learnwares) and the users (sketching their tasks) using the exact same shared pre-trained model as a common basis to generate the parameter vectors. I think the authors should study the robustness of their finding s to this assumptions."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "9oer5wRYaF", "forum": "JkKkquv5lw", "replyto": "JkKkquv5lw", "signatures": ["ICLR.cc/2026/Conference/Submission11415/Reviewer_KAPE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11415/Reviewer_KAPE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183402084, "cdate": 1762183402084, "tmdate": 1762922531536, "mdate": 1762922531536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}