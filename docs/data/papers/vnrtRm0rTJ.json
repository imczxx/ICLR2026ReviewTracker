{"id": "vnrtRm0rTJ", "number": 2992, "cdate": 1757313268805, "mdate": 1759898114990, "content": {"title": "From Misclassification to Outliers: Joint Reliability Assessment in Classification", "abstract": "Building reliable classifiers is a fundamental challenge for deploying machine learning in real-world applications. A reliable system should not only detect out-of-distribution (OOD) inputs but also anticipate in-distribution (ID) errors by assigning low confidence to potentially misclassified samples. Yet, most prior work treats OOD detection and failure prediction as separated problems, overlooking their closed connection. We argue that reliability requires evaluating them jointly. To this end, we propose a unified evaluation framework that integrates OOD detection and failure prediction, quantified by our new metrics DS-F1 and DS-AURC, where DS denotes double scoring functions. Experiments on the OpenOOD benchmark show that double scoring functions yield classifiers that are substantially more reliable than traditional single scoring approaches. Our analysis further reveals that OOD-based approaches provide notable gains under simple or far-OOD shifts, but only marginal benefits under more challenging near-OOD conditions. Beyond evaluation, we extend the reliable classifier SURE and introduce SURE+, a new approach that significantly improves reliability across diverse scenarios. Together, our framework, metrics, and method establish a new benchmark for trustworthy classification and offer practical guidance for deploying robust models in real-world settings. Code will be released upon publication.", "tldr": "We extend traditional single-metric evaluation methods, F1 and AURC, into novel dual-metric approaches called DS-F1 and DS-AURC that jointly assess OOD detection and failure prediction performance.", "keywords": ["Method evaluation", "OOD detection", "Failure prediction", "Selective classification"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/007877765a66d1147880f431133a7a4d25fdd635.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces two metrics to evaluate the model in the joint scenario of OOD detection and failure prediction. The metrics -- DS-F1 and DS-AURC are derived directly from F1 and AURC scores, by defining different cases in the joint evaluation setting. The paper further improves the SURE methods with several techniques, like RegPixMix and F-SAM, to improve both the ID acc and OOD detection performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. New evaluation metrics: the paper studied the joint setting of OOD detection and failure prediction, and proposes two straightforward metrics -- DS-F1 and DS-AURC to evaluate the model.\n2. New training methods: the paper improves SURE by integrating several techniques and achieves better performance on both OOD detection and ID acc."}, "weaknesses": {"value": "1. The significance of double-scoring needs further justification: while separate scores and be adopted to evaluate the model's performance on failure prediction and OOD detection, the significance of calculating a joint metric remains unclear.\n2. Lacking soundness of the proposed SURE+: the authors propose to adopt several off-the-shelf techniques to improve the baseline SURE. Though achieving higher performance, this method is not well related to the paper's main contribution and claims, more like an engineering combination, not a new method.\n3. The experiments are not sufficient to validate the effectiveness of the metrics: the authors only use the MSP score as the ID score, which is also a kind of OOD score, making it a special case. By definition, the ID scores can be any scores that measure the failure likelihood. Therefore, adopting only the MSP as the ID score doesn't thoroughly examine the proposed new metrics.\n4. The motivation for adding each technique to SURE is not presented."}, "questions": {"value": "1. At around line 348, why do the two metrics never worsen the evaluation by producing scores that are at least as high as F1 score and as low as AURC. I don't see a direct correlation between the score scale and evaluation quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FekdYwqpyG", "forum": "vnrtRm0rTJ", "replyto": "vnrtRm0rTJ", "signatures": ["ICLR.cc/2026/Conference/Submission2992/Reviewer_1RUf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2992/Reviewer_1RUf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722553970, "cdate": 1761722553970, "tmdate": 1762916485304, "mdate": 1762916485304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that real-world deployment of machine learning requires classifiers that can not only detect OOD inputs but also misclassifications within the in-distribution (ID) data. Since prior work often treats these two problems separately, the paper proposes a unified evaluation framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly demonstrate the necessity of jointly evaluating OOD detection and misclassification prediction for real-world reliability, which is a crucial practical concern."}, "weaknesses": {"value": "1. The paper's primary claim of proposing a unified evaluation for OOD detection and failure prediction is not entirely novel. Several prior works [1-5] have already addressed this problem. The paper's distinction rests mainly on the double scoring mechanism rather than the concept of joint evaluation, which significantly weakens the framework's overall contribution.\n\n2. The proposed metrics are viewed as a trivial extension of existing single-scoring metrics (F1 and AURC) to a two-dimensional threshold space $(\\tau_{OOD}, \\tau_{ID})$. They do not introduce new theoretical insights into risk modeling. Furthermore, similar to their single-scoring counterparts, these metrics are still heavily influenced by the absolute number of mispredicted ID samples and the number of OOD samples. This reliance can obscure the true effectiveness of the underlying detection mechanism when the class distributions are highly imbalanced, which is a known limitation in failure detection benchmarking.\n\n3. The key experimental observation that OOD-based methods provide only marginal benefits under challenging near-OOD conditions is a widely recognized limitation [1-5]. Simply re-confirming this known challenge does not constitute a significant contribution.\n\n4. The proposed SURE+ method appears to be an engineering modication of several established regularization techniques integrated into the existing SURE framework.\n\nReferences\n\n[1] A call to reflect on evaluation practices for failure detection in image classification\n\n[2] Failure detection in medical image classification: A reality check and benchmarking testbed\n\n[3] Learning to reject meets ood detection: Are all abstentions created equal\n\n[4] A unified benchmark for the unknown detection capability of deep neural networks\n\n[5] Plugin estimators for selective classification with out-of-distribution detection"}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hEQ0Jij3Me", "forum": "vnrtRm0rTJ", "replyto": "vnrtRm0rTJ", "signatures": ["ICLR.cc/2026/Conference/Submission2992/Reviewer_avkA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2992/Reviewer_avkA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813929217, "cdate": 1761813929217, "tmdate": 1762916485015, "mdate": 1762916485015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SURE+, an improved training recipe and evaluation framework aiming to build more reliable classifiers by jointly considering OOD detection and failure prediction. The authors further introduce two new evaluation metrics, DS-F1 and DS-AURC, to assess reliability in a unified manner. The method modifies the SURE baseline (Li et al., 2024b) by replacing several components—such as CRL loss, CSC head, SWA, and data augmentation—with simpler or alternative choices (e.g., EMA, linear classifier, RegPixMix, and F-SAM). Experiments on the OpenOOD benchmark demonstrate consistent improvements over prior methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and timely topic, reliable classification that integrates OOD detection and failure prediction.\nThe joint evaluation framework is conceptually reasonable and could potentially help bridge two often-separated research directions.\nThe paper provides comprehensive experimental results on standard benchmarks, showing the consistency of improvements.\nThe writing is generally clear, and the experimental setup is reproducible."}, "weaknesses": {"value": "Limited novelty of the proposed method (SURE+).\nThe modifications over SURE are mainly component replacements using existing methods (e.g., EMA, F-SAM, RegPixMix), without introducing fundamentally new ideas. The resulting method reads more like a collection of known techniques rather than a coherent new approach.\n\nLack of clear methodological focus.\nThe framework mixes metric design, pipeline tweaks, and augmentation choices, making it hard to identify the core contribution. The work feels somewhat “mixed and unfocused.”\n\nUnclear motivation and limited effectiveness of new metrics (DS-F1 and DS-AURC).\nThe motivation behind these metrics is not fully convincing—why a double scoring setup is inherently better than existing reliability measures (e.g., AURC, AUROC, ECE). From Table 1, the observed gains appear marginal.\n\nInsufficient theoretical or conceptual justification.\nThe paper would benefit from a deeper analysis or theoretical discussion showing why the proposed double scoring better reflects model reliability or uncertainty.\n\nBenchmarking vs. contribution gap.\nWhile the authors claim to establish a new benchmark, the contribution seems incremental and largely empirical, with little conceptual advancement."}, "questions": {"value": "1. Can the authors clarify the conceptual novelty of SURE+ beyond being an ensemble of existing training tricks?\n2. How sensitive are the results to the specific component choices (e.g., EMA vs. SWA, RegPixMix vs. RegMixup)?\n3. For DS-F1 and DS-AURC, what is the precise intuition or mathematical rationale that supports their superiority over existing reliability metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethic concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HFXxanLllK", "forum": "vnrtRm0rTJ", "replyto": "vnrtRm0rTJ", "signatures": ["ICLR.cc/2026/Conference/Submission2992/Reviewer_GNuk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2992/Reviewer_GNuk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762397291229, "cdate": 1762397291229, "tmdate": 1762916484662, "mdate": 1762916484662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}