{"id": "OK2GR1guwv", "number": 8215, "cdate": 1758074527509, "mdate": 1759897799532, "content": {"title": "Safety-Aligned Weights Are Not Enough: Refusal-Teacher-Guided Finetuning Enhances Safety and Downstream Performance under Harmful Finetuning Attacks", "abstract": "Recently, major AI providers such as Google and OpenAI have introduced Finetuning-as-a-Service (FaaS), which allows users to customize Large Language Models (LLMs) using their own data. However, this service is vulnerable to safety degradation when user data includes harmful prompts, a threat known as harmful finetuning attacks.\nPrior works attempt to mitigate this issue by first constructing safety-aligned model and then finetuning the model on user data. However, we observe that the safety-aligned weights provide weak initialization for downstream task learning, leading to suboptimal safety-alignment and downstream task performance.\nTo address this, we propose a **Refusal-Teacher (Ref-Teacher)-guided finetuning framework**. \nInstead of finetuning a safety-aligned model on user data, our approach directly finetunes the base model under the guidance of a safety-aligned Ref-Teacher, which filters harmful prompts from user data and distills safety-alignment knowledge into the base model.\nExtensive experiments demonstrate that our Ref-Teacher-guided finetuning strategy effectively minimizes harmful outputs and enhances finetuning accuracy for user-specific tasks, offering a practical solution for secure and reliable deployment of LLMs in FaaS.", "tldr": "This paper observes that safety-aligned model weights are suboptimal for downstream task learning and then introduces a novel refusal-teacher-guided framework for safe finetuning of Large Language Models.", "keywords": ["LLM Safety", "Safe Finetuning", "Data Filtering", "Knowledge Distillation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd87cc2eebc0633c20d65460c1626a9bec1031b7.pdf", "supplementary_material": "/attachment/603d1a3d47d4685dccc352cb7dc90821f150d2f2.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses maintaining safety alignment in LLMs under Finetuning-as-a-Service (FaaS) against malicious finetuning attacks. It finds that the standard “safety-align then finetune” pipeline weakens performance, while joint finetuning causes gradient conflicts between safety and task goals. To fix this, the authors propose Refusal-Teacher (Ref-Teacher) finetuning. Results show improved safety and task accuracy across multiple settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's motivation is clearly shown through the experiments in Section 4, which effectively frame the problem the proposed method aims to solve.\n2. The experimental evaluation is comprehensive, covering a diverse range of datasets and settings.\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. My main concern is the fairness of the comparison. The proposed method uses a data filtering step that the baselines lack, and this filter appears optimized for the evaluation tasks. Although Appendix C1 includes a related comparison with LLaMAGuard3-8B, a more direct evaluation applying the same trained data filter to the baseline methods is needed.\n2. The method adds several components that likely increase computational cost and deployment complexity. The paper would benefit from a clear analysis of the time and memory overhead relative to the baselines."}, "questions": {"value": "1. Could you provide a detailed analysis of the computational cost (e.g., training time, memory usage) of your method compared to the standard SFT and other finetuning-stage baselines?\n2. The \"Base -> SA + FT\" method mentioned in Section 4 appears to be a strong baseline. What was the reasoning for not including it in the main comparison tables?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8GwEkvSFVI", "forum": "OK2GR1guwv", "replyto": "OK2GR1guwv", "signatures": ["ICLR.cc/2026/Conference/Submission8215/Reviewer_MKsJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8215/Reviewer_MKsJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985727416, "cdate": 1761985727416, "tmdate": 1762920160279, "mdate": 1762920160279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of maintaining LLM safety during downstream finetuning. The authors identify a key limitation in the standard two-stage pipeline where a safety-aligned model is first prepared and then finetuned on user data. They then argue that this approach leads to suboptimal downstream task performance due to ‘weak initialization’ and can still compromise safety. As an alternative, the authors propose a Refusal-Teacher (Ref-Teacher)-guided finetuning framework. Instead of finetuning a pre-aligned model, this framework directly finetunes the base LLM. The process is guided by a specially trained, frozen Ref-Teacher model. This teacher serves two functions: 1) it filters harmful prompts from the user's data using a learned *refusal feature*, and 2) it provides *alignment distillation* by generating soft refusal labels for a separate safety dataset, which helps the student model learn safety objectives with reduced gradient conflicts. The authors demonstrate that their framework consistently outperforms existing methods, achieving lower harmfulness scores while simultaneously attaining higher accuracy on downstream tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow. The motivation is laid out logically, building a clear case for why a new approach is needed.\n- The proposed solution, to finetune the base model directly while carefully managing the safety/utility trade-off, is an effective response to this finding. This work provides a practical approach to a problem in Finetuning-as-a-service."}, "weaknesses": {"value": "- The data filtering strategy is configured to maximize recall on harmful prompts, which may discard some harmless user data (a high false positive rate). It is unclear what the percentage of harmless data filtered out is across different tasks.\n- The authors should experiment with other data filtering methods [1][2] for a more comprehensive comparison.\n- [3][4] also studies this problem from the similarity perspective, which should also be discussed in the revision.\n\n[1] Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs\n\n[2] Pharmacist: Safety Alignment Data Curation for Large Language Models against Harmful Fine-tuning\n\n[3] Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets\n\n[4] When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment"}, "questions": {"value": "- Could the authors provide a brief analysis of the computational overhead of the ‘Teacher Preparation Stage’ compared to the standard ‘Alignment Stage’ used for the baselines.\n\n- Could an adversary craft finetuning examples that are benign in their feature representation (low cosine similarity to the refusal feature) but still steer the model towards unsafe behavior on related prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KN28t5qYnX", "forum": "OK2GR1guwv", "replyto": "OK2GR1guwv", "signatures": ["ICLR.cc/2026/Conference/Submission8215/Reviewer_B6XU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8215/Reviewer_B6XU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988216153, "cdate": 1761988216153, "tmdate": 1762920159768, "mdate": 1762920159768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the problem of fine-tuning degrading safety alignment in language models. The setting is fine-tuning-as-a-service, where a user wants to fine-tune a model on some task-specific data. The user is assumed adversarial such that a certain fraction of user data consists of harmful prompts and harmful responses. \n\nThe paper proposes to first train, starting from a base model, a safety-aligned teacher model called Ref-Teacher. Then, in the fine-tuning stage, Ref-Teacher is used in two ways: 'refusal feature' from the teacher is used for classifying whether user prompts are harmful (and filtering out use samples classified as harmful); and using soft 'refusal labels' from the teacher for balancing loss on user data and KL-divergence loss on alignment data (called as alignment distillation). \n\nExperiments consider fine-tuning 3 base models on 4 tasks, and show that the proposed method outperforms several baselines on the metrics of fine-tune accuracy and harmful score."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The problem of fine-tuning degrading safety alignment is practically relevant and has recently received a widespread attention.\n* The idea of using signals from a safety-aligned teacher model is interesting."}, "weaknesses": {"value": "* Quite a large number of solutions have been recently proposed for mitigating safety degradation after fine-tuning. Besides alignment stage defenses (baselines in the paper, such as Vaccine and Booster, fall under this), there are fine-tuning-stage defenses (e.g., SafeInstruct [Bianchi et al., 2024], VLGuard [Zong et al., 2024], constrained-SFT [Qi et al., 2024]), and post-fine-tuning defenses (e.g., SafeLoRA [Hsu et al., 2025], RESTA [Bharadwaj et al., 2024], SOMF [Yi et al., 2024],  Antidote [Huang et al., 2024]). The paper does not acknowledge the vast related work on this topic. While it is infeasible to compare against too many baselines, it is important to acknowledge the related work on this topic and provide qualitative comparisons. For fairness of comparison, it will be great if the authors can consider a couple of baselines from other setups (e.g., one from post-fine-tuning-stage and one from fine-tuning-stage) for comparison.\n\n* One of my main concerns is that the paper takes an overly simplistic approach for baselines for preserving alignment after fine-tuning -- first alignment is performed by supervised fine-tuning (SFT) of a base model on alignment data and then task-specific fine-tuning is performed. In practice, users significantly prefer fine-tuning instruct models. Leading instruct models take a number of steps for alignment beyond simple SFT on alignment data including RLHF via preference tuning such as Direct Preference Optimization (DPO) or other online RL algorithms such as Proximal Policy Optimization (PPO). Consequently, when starting from an instruct model, adaptation to the user task is often easier and the safety degradation is typically less significant. Many prior works on the topic of fine-tuning degrading safety alignment consider the case of adapting instruct (or chat) models and the impact of safety alignment (e.g., constrained-SFT [Qi et al., 2024], SafeLoRA [Hsu et al., 2025]). The paper has limited experiments in Appendix C.3 when considering instruct models as Ref-Teacher, but lacks details on using instruct models as starting points for task specific fine-tuning.\n\n* The paper does not conduct any ablation experiments to quantify the contributions of data filtering and alignment distillation. (More details in the Questions.)\n\nReferences\n1. T. Huang, G. Bhattacharya, P. Joshi, J. Kimball, L. Liu, \"Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-Tuning\", 2024\n2. Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu,\"Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation\", 2024\n3. Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou, \"Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions\", 2024\n4. Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales, \"Safety fine-tuning at (almost) no cost: A baseline for vision large language models\", 2024\n5. Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma, Subhrajit Roy, Ahmad Beirami, Prateek Mittal, and Peter Henderson, \"Safety alignment should be made more than just a few tokens deep\", 2024\n6. Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang, \"Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models\", 2025\n7. Rishabh Bhardwaj, Do Duc Anh, and Soujanya Poria, \"Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic\", 2024\n8. Xin Yi, Shunfan Zheng, Linlin Wang, Xiaoling Wang, and Liang He, \"A safety realignment frame- work via subspace-oriented model fusion for large language models\", 2024"}, "questions": {"value": "* The proposed method has two stages so-called alignment distillation and data filtering. It is not clear how much each stage contributes and how two stages help each other. Are there any ablation experiments quantifying the contribution of each stage? For instance, data filtering can be used in the conventional setup of filtering on top of an aligned model. How would it compare with the proposed method?\n* In the experiment, FA is measured on downstream benchmarks by using specific number of samples (L359 on page 7). Why specific number of samples are chosen from these benchmarks in contrast to using the entire test set?\n* In Algorithm 1, L260, the equation number (eq (2)) seems to be a typo.\n* In Table 8, including BeaverTails seems a bit unfair since Ref-Teacher is trained on BeaverTails. Can the author give more details on the inclusion of BeaverTails?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M0w8M1gwWl", "forum": "OK2GR1guwv", "replyto": "OK2GR1guwv", "signatures": ["ICLR.cc/2026/Conference/Submission8215/Reviewer_CsFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8215/Reviewer_CsFX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762026372324, "cdate": 1762026372324, "tmdate": 1762920158446, "mdate": 1762920158446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Ref-Teacher fine-tuning method, where a base model is fine-tuned on downstream tasks along with safety data to preserve its safety alignment properties. The paper first makes the observation that directly fine-tuning the base model achieves both robust safety-alignment and good downstream task performance, but can also introduce \"gradient conflicts\" where the safety and task gradient directions may be at odds with each other, resulting in worse task performance compared to training on the task only. They then train a new teacher model called Ref-Teacher by filtering out harmful prompts via refusal alignment and training on a newly proposed regularized loss function. The Ref-Teacher is then used to train a distilled model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a relevant, current and pervasive problem.\n2. The experimental section shows clear improvements over state-of-the-art"}, "weaknesses": {"value": "1. The solution seems costly, and it is not clear if it is worth the benefits for LLMs where training is already prohibitively expensive.\n2. The solution is non-intuitive and relatively more complex in terms of implementation vs other state-of-the-art ones\n3. It requires changing the data itself, raising concerns about distributional shifts and making the generalizability questionable.\n4. The depth of the novelty is not clear, and quite a few of the observations such as training on both safety and task datasets, gradient differences between them, etc. are already well known in practice."}, "questions": {"value": "1. Why did the authors take the route of creating a Teacher model which both creates harmful/harmless prompts and is distilled from? Is it simply because empirically it gives better results or is there an intuition behind this? The state-of-the-art papers (such as RepNoise and Lisa) have very elegant solutions compared to the 3 to 5 step process that the authors have employed here. \n2. While the results from the experimental section show that they are better, the main question that the paper does not properly answer is why. Why must the Ref-Teacher be used for more effectively distinguishing harmful vs harmless prompts? Why not use a better model? \n3. What are the advantages of this compared to using the standard methods of detecting harmful prompts?\n4. \"we assume a setting where a pre-aligned model is unavailable\" - this is a strong assumption. Can you please explain a practical scenario where this might be the case?\n5. This solution seems to be a two-step solution - creating a teacher model to distill from, and also creating new filtered safety-targeted dataset to train on. The state-of-the-art frameworks compared against do not do training on a new safety-targeted dataset but use the base datasets. Is that not a concern since the solution seems dependent on the underlying data too to a certain extent? \n6. What are the resource costs of this solution in terms of number of extra FLOPs? Is the cost worth the performance gains and/or implementation complexities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X9vPTpxZPq", "forum": "OK2GR1guwv", "replyto": "OK2GR1guwv", "signatures": ["ICLR.cc/2026/Conference/Submission8215/Reviewer_jpSC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8215/Reviewer_jpSC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142220466, "cdate": 1762142220466, "tmdate": 1762920157932, "mdate": 1762920157932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}