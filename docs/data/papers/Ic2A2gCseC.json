{"id": "Ic2A2gCseC", "number": 1382, "cdate": 1756878052348, "mdate": 1759898211705, "content": {"title": "Beyond Fixed: Training-Free Variable-Length Denoising for Diffusion Large Language Models", "abstract": "Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length.  This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation.  While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task.  To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models.  DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric.  2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.", "tldr": "", "keywords": ["Large Language Models", "Diffusion Language Model", "Training-Free"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1aa89c5247984b2d573fd117b9e967b55b24b65e.pdf", "supplementary_material": "/attachment/0b3de3a5e1c853ecad64b68e3cc840a75fcff803.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles a central limitation of Diffusion LLMs—the need to pre-set a fixed output length—and proposes DAEDAL, a training-free, two-stage inference method that first estimates a task-appropriate length by checking EOS confidence at the sequence end and then expands low-confidence regions on the fly via mask insertion during denoising. On LLaDA-Instruct-8B and related DLLMs, DAEDAL matches or exceeds carefully tuned fixed-length baselines while using tokens more efficiently (e.g., GSM8K 85.8 vs. 83.8 accuracy with a much higher effective-token ratio), and extensive ablations show robustness to initial length, thresholds, and expansion factors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is simple and intuitively reasonable. Requiring no retraining is a plus\n- The proposed method demonstrates solid empirical improvements over the best-tuned fixed-length baselines on math reasoning and code generation benchmarks (e.g., MATH500), while generating effective tokens more efficiently.\n- This paper conducts a thorough analysis of key hyperparameters of the method, showing its robustness to different configurations."}, "weaknesses": {"value": "- While the paper shows in experiments that a combination of the two expansion stages gives the best performance, in principle, there still lacks a clear reason why both stages are necessary. It's natural to consider merging the first length adjustment stage into the second dynamic expansion stage. Interestingly, according to Table 2, stage 1 already contributes to most of the performance improvement. I think more investigation should be put into this.\n- The ablation study shows the method's robustness to threshold hyperparameters, but it's only for a single model-dataset pair, i.e., LLaDA-Instruct-8B on GSM8K. I wonder whether the threshold findings can be transferred to other model-dataset combinations. Or do you need to tune thresholds again for a different dataset?\n- It would be better to have some more direct measurements on the actual inference speed.\n- The benchmarks focus on math and code. How would the proposed method perform on general language tasks?"}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ex5oJPl30k", "forum": "Ic2A2gCseC", "replyto": "Ic2A2gCseC", "signatures": ["ICLR.cc/2026/Conference/Submission1382/Reviewer_XwxZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1382/Reviewer_XwxZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947286907, "cdate": 1761947286907, "tmdate": 1762915757431, "mdate": 1762915757431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an inference method for a pretrained masked diffusion model that supports length variability. In particular, the algorithm leverages two core operations-determining length using EOS confidence and then expanding iteratively for the masked position with the lowest prediction confidence. Experiments have been conducted on several math and code benchmarks on LLaDA-Instruct and Dream. The results show that the approach can maintain the accuracy of the \"optimal\" length sweeping over several lengths while being adaptive-length at inference time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The goal of achieving adaptive-length decoding is promising and important to obtain better trade-off frontier between sample quality and latency.\n\n2. The proposed approach is easy to follow and the method requires no additional training.\n\n3. The presentation of the paper is clear and the empirical performance, specifically on the accuracy v.s. total tokens, is quite impressive. The ablation studies including hyperparameter sensitivity analysis are informative."}, "weaknesses": {"value": "1. The proposed idea of using confidence of predicting EOS token to determine the length of the sequence seems a bit heuristic. A more comprehensive evaluation should be conducted on more datasets and tasks to justify the effectiveness of such heuristic.\n\n2. Efficiency metrics such as wall-clock time/latency is missing."}, "questions": {"value": "1. I am curious about the generalization of the findings in Figure 2 on over datasets and number of tokens (other than 128). Would the authors be able to provide more empirical justifications?\n\n2. In the main table, it would be helpful to add actual wall-clock decoding time as a metric. Would the authors provide results for this?\n\n3. Would the approach benefit from some training/finetuning of the model to better account for the variable-length objective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jb764FCz6S", "forum": "Ic2A2gCseC", "replyto": "Ic2A2gCseC", "signatures": ["ICLR.cc/2026/Conference/Submission1382/Reviewer_owRq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1382/Reviewer_owRq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947569673, "cdate": 1761947569673, "tmdate": 1762915757161, "mdate": 1762915757161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an inference-time method for extending the generation length of text diffusion models. Starting with a small canvas, masked tokens get added until EOS occurs with high-enough probability at the end of the sequence. Given this initial sequence of masked tokens, the method alternates between filling in high-confidence tokens and adding more tokens based on EOS confidence. Tokens are added to the lowest confidence mask positions, and are expanded by a constant block size.\n\nResults on simple math and coding evaluations show that the method obtains reasonable accuracy while adaptively determining response length. The average ratio between utilized tokens versus padding is around 65% across all tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method seems effective and is very simple. The presentation was written to be reader-friendly, but I believe would benefit from including more details. For example, Algorithm 1 has a few more details than Fig 3. The results are reasonable, with easing the need for manual sequence length tuning."}, "weaknesses": {"value": "The main weakness is that the main baselines to compare to is block diffusion or other adaptive-length methods. The sell of variable-length  diffusion would likely have to be a speed increase while preserving accuracy, or another point on the speed-accuracy Pareto frontier. It is possible that spec-decoded autoregressive models achieve better speeds at similar accuracy."}, "questions": {"value": "The paper supports its claims. The main improvement would be to include speed-accuracy comparisons to other accelerated methods, namely speculative decoding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sjzXJ8ICQ5", "forum": "Ic2A2gCseC", "replyto": "Ic2A2gCseC", "signatures": ["ICLR.cc/2026/Conference/Submission1382/Reviewer_xU9B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1382/Reviewer_xU9B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059951774, "cdate": 1762059951774, "tmdate": 1762915756356, "mdate": 1762915756356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}