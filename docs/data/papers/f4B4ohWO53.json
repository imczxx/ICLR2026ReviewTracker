{"id": "f4B4ohWO53", "number": 16879, "cdate": 1758269858463, "mdate": 1759897213999, "content": {"title": "Differential Privacy for Transformer Embeddings  with Nonparametric Variational Information Bottleneck", "abstract": "We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings.\nIt has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy.  This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection.  We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee.  Training the NVIB layer calibrates the noise level according to utility.  We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy.  With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.", "tldr": "We show that nonparametric variational information bottleneck is effective at calibrating noise for sharing transformer embeddings with differential privacy.", "keywords": ["Nonparametric Variational Information Bottleneck", "Rényi Differential Privacy", "Bayesian Differential Privacy", "Transformers", "Differential Privacy"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aff1b5eff12b9e842134ee3bde2e164a24eaaa88.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes NVDP (Nonparametric Variational Differential Privacy), a new method to provide local differential privacy for transformer embeddings. The motivation stems from the observation that transformer hidden states may leak sensitive information, allowing adversaries to reconstruct or infer private attributes. To mitigate this, the authors integrate a Nonparametric Variational Information Bottleneck into a transformer encoder. NVIB, based on a Dirichlet Process latent prior, stochastically samples weighted vectors (token-level embeddings) that preserve utility while controlling information flow. The method injects learned, task-calibrated noise into embeddings, and privacy is quantified via Rényi Differential Privacy and converted into interpretable Bayesian Differential Privacy guarantees."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well structured, with clear separation between background, method, and experiments.\n- Introduces the first integration of NVIB and differential privacy within transformer architectures, bridging information bottleneck theory and formal privacy guarantees.\n- Provides a viable way to share transformer embeddings safely."}, "weaknesses": {"value": "- NVIB sampling and RD computation over all input pairs are expensive (O(n²) pairs). No runtime, memory, or scalability analysis is provided.\n- While the paper repeatedly claims local DP, its formulation (sampling embeddings within the model) seems effectively performs mechanism-level DP, not user-level DP."}, "questions": {"value": "- How tight is your analytical RD bound (Eq. 7)? Any empirical validation or confidence intervals?\n- How does NVDP compare with other DP mechanisms on the same tasks?\n- What is the computational overhead (training time per epoch, GPU hours)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7573FaFSj8", "forum": "f4B4ohWO53", "replyto": "f4B4ohWO53", "signatures": ["ICLR.cc/2026/Conference/Submission16879/Reviewer_Q2oc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16879/Reviewer_Q2oc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761406787824, "cdate": 1761406787824, "tmdate": 1762926908232, "mdate": 1762926908232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NVDP (Nonparametric Variational Differential Privacy), a framework that integrates Bayesian DP and RDP with a nonparametric variational information bottleneck (VIB). The approach aims to achieve privacy-preserving embeddings by learning stochastic representations that limit mutual information between inputs and embeddings. NVDP introduces internal probabilistic noise through a variational layer parameterized by $\\mu, \\sigma, \\alpha$. The model thus learns to generate privacy-compliant embeddings by minimizing the expected RDP distance between output distributions across data samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- S1: Conceptually appealing integration: The unification of RDP and BDP within an information bottleneck formulation is a novel and elegant conceptual contribution. It provides a fresh probabilistic perspective on DP, reframing privacy as information compression rather than explicit noise injection.\n- S2: Learning-based noise adaptation: Unlike conventional DP mechanisms with fixed noise levels, NVDP allows the model to adapt its internal noise dynamically via learned parameters $(\\sigma(x), \\alpha(x))$. This could, in principle, improve the privacy–utility trade-off."}, "weaknesses": {"value": "- W1: Lack of practical validation: The experiments do not convincingly demonstrate real-world usefulness. It remains unclear for which downstream applications (e.g., classification, retrieval, or fine-tuning) the learned embeddings preserve performance while providing privacy.\n- W2: While $\\epsilon$ is computed through RDP/BDP metrics, it is not specified or controlled in the same way as standard DP. The reader cannot interpret what an obtained $\\epsilon$ actually means in practical or regulatory terms.\n- W3: Since the model operates under a local DP assumption, each client may achieve a different effective privacy strength. The implications for consistency, fairness, and aggregation are not discussed.\n- W4: Approximate, not formal DP guarantees: The framework measures privacy using divergence bounds but does not prove formal composition or post-processing guarantees. As a result, it is more accurately described as ``DP-inspired” rather than strictly DP-compliant.\n- W5: Despite the emphasis on ``Transformer embeddings,” the proposed method does not exploit any specific properties of Transformer architectures. The model merely applies the NVDP layer on top of general embeddings, meaning that the approach could equally apply to CNNs or MLP-based encoders."}, "questions": {"value": "Address W1-5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3slvCiiYi5", "forum": "f4B4ohWO53", "replyto": "f4B4ohWO53", "signatures": ["ICLR.cc/2026/Conference/Submission16879/Reviewer_kjrT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16879/Reviewer_kjrT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905296327, "cdate": 1761905296327, "tmdate": 1762926907817, "mdate": 1762926907817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Nonparametric Variational Differential Privacy (NVDP) that offers privacy protection on transformer embeddings. It modifies NVIB by noisy embedding sampling and subsequent denoising, providing RDP and BDP guarantee. Experiment results demonstrate that NVDP effectively balance privacy and utility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a real-world problem of privacy-preserving data sharing.\n- This paper adapts NVIB to provide formal privacy guarantee during embedding sharing.\n- NVDP offers better privacy and utility trade-off compared with the baselines."}, "weaknesses": {"value": "- The experiment lacks comparison with existing DP baselines [1][2].\n- The discussion on neighboring dataset is vague. It is unclear how the neighbors map to the token/sentence scenario.\n- The experiment lacks attack analysis on the privatized embeddings, such as \n\n[1] Du, M., Yue, X., Chow, S. S., & Sun, H. (2023, April). Sanitizing sentence embeddings (and labels) for local differential privacy. In Proceedings of the ACM Web Conference 2023 (pp. 2349-2359).\n\n[2] Meehan, C., Mrini, K., & Chaudhuri, K. (2022, May). Sentence-level Privacy for Document Embeddings. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 3367-3380)."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cHGnUZtxLH", "forum": "f4B4ohWO53", "replyto": "f4B4ohWO53", "signatures": ["ICLR.cc/2026/Conference/Submission16879/Reviewer_6gyN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16879/Reviewer_6gyN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985358016, "cdate": 1761985358016, "tmdate": 1762926907392, "mdate": 1762926907392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}