{"id": "6i6cuKURtr", "number": 10152, "cdate": 1758162154330, "mdate": 1763182794541, "content": {"title": "Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion", "abstract": "AI-generated text (AIGT) detection evasion aims to reduce the detection probability of AIGT, helping to identify weaknesses in detectors and enhance their effectiveness and reliability in practical applications.\nHowever, existing evasion methods still require high training costs due to fine-tuning and result in text quality reduction owing to the text modification.\nTo address these challenges, we propose Self-Disguise Attack (SDA), a novel approach that enables large language models (LLMs) to actively disguise their output, reducing the detection probability of AIGT.\nThe SDA comprises two main components: the adversarial feature extractor and the retrieval-based context examples optimizer.\nThe former generates disguise features that enable LLMs to understand how to produce more human-like text.\nThe latter retrieves the most relevant examples from an external knowledge base as in-context examples, further enhancing the self-disguise ability of LLMs and mitigating the impact of the disguise process on the diversity of the generated text.\nThe SDA directly employs prompts containing disguise features and optimized context examples to guide the LLM in generating detection-resistant text, thereby reducing resource consumption.\nExperimental results demonstrate that the SDA effectively reduces the average detection accuracy of various AIGT detectors across texts generated by three different LLMs, while maintaining the quality of AIGT.", "tldr": "", "keywords": ["AI generated text detection evasion", "large language model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/63a9167b5aad3af69de42c24d7343cf431a1796e.pdf", "supplementary_material": "/attachment/6323e2dd7724590366d384212afaec2ad0645f40.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a finetuning-free, generation-based evasion method driven by prompt engineering + detector feedback, termed Self-Disguise Attack (SDA). SDA consists of two parts: (i) an adversarial “disguise-feature” extractor that employs black-box LLMs as a text generator and a feature generator, with a proxy detector (ChatGPT-detector, RoBERTa family) providing feedback to iteratively distill natural-language descriptions of “human-style disguise features”; and (ii) a retrieval-based in-context example optimizer that uses vector search over a self-built external repository to select examples most relevant to the current query and that have previously passed detection. These examples, together with the disguise features, are concatenated into the final prompt to guide the target LLM to produce more “human-like” text and reduce its probability of being detected.\n\nExperiments attack four detectors (RADAR, DeTeCtive, MPU, ChatGPT-detector) across three LLMs (Qwen-max, LLaMA-3.3-70B-Instruct, DeepSeek-V3). Results show that SDA achieves lower average detection accuracy (lower is better) than multiple modification- and generation-based baselines (Paraphrase, DIPPER, HMGC, SICO), while maintaining or improving quality and diversity metrics such as PPL, Self-BLEU, and cosine similarity to HWT. The paper also includes human evaluation and ablation studies. The method requires no finetuning, relies solely on prompting and retrieval, and claims low resource overhead and transferability."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. A training-free evasion approach for AI-generated text detection that relies solely on prompt engineering to produce highly human-like, detector-evading outputs.\n\n2. Better preservation of text quality relative to other baselines.\n\n3. A relatively novel application of RAG."}, "weaknesses": {"value": "**This paper is not well written.** There are multiple citation errors and invalid figure references (see weaknesses below). It will require substantial revisions before publication. In its current form, it is not ready for a top-tier venue like ICLR. The main weaknesses, ranked by severity, are as follows:\n\n1. **Exaggerated performance.** In this paper, you use the ChatGPT-detector as your proxy model, which is based on RoBERTa-base; all the detectors you attack in evaluation are also RoBERTa-base-based. Although you add MPU (also RoBERTa-base-based) as a proxy model in the appendix, I am concerned that using the same backbone family during the training phase (proxy detector) and the testing phase (detectors under attack) leads to overfitting. This is especially problematic when your method’s margins over the baselines are small, which can easily inflate the perceived performance.\n\n2. **Limited evaluation.** The paper evaluates only 4 AI-generated text detectors, and all of them belong to the model-based category. Can your method effectively attack metric-based detectors (e.g., Binoculars, FastDetectGPT, DNAGPT) and watermark-based detectors? In addition, you evaluate only **200** samples from RAID and conduct human evaluation on just **100** texts. I consider this evaluation scope to be limited.\n\n3. **Limited metric reporting.** In Table 2, you report perplexity relative to modification-based evasion detection methods, while Table 3 reports perplexity, cosine similarity, and BLEU versus SICO. Why not report these metrics in Table 2 as well? Moreover, I believe reporting the change in perplexity (before vs. after the attack) would better highlight the quality shift caused by the attack. Table 4 reports generation time, but given that your method heavily relies on in-context learning, you should also report API call costs to support the claim that your approach is more efficient than the baselines.\n\n4. **Numerous citation and figure-reference errors.**\n\n   * **Line 48**: “Zhou et al. (2024); Wang et al. (2024)” should be enclosed in parentheses.\n   * **Lines 113, 118, 120, 123, 133, 136, 139**: the parentheses should be removed from these citations.\n   * In the Case Study section at **line 751**, you refer to Figure E, but the link points to Appendix E; this figure should actually point to Figure 6.\n\nGiven the concerns above, I recommend a rejection of this paper in its current form."}, "questions": {"value": "1. If your proxy model changes, how would your results be affected?\n2. How effective is your method against the detectors I mentioned under Weaknesses? I checked the code and found interfaces for detectors such as FastDetectGPT; it appears your method should be applicable to these detectors.\n3. Please include a larger number of samples in the evaluation, and add a comparison of API costs."}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "This paper proposes an attack method for evading AI-generated text detectors, but it does not provide a detailed discussion of how to detect such evasion—only a brief treatment in the appendix. I recommend that the authors include a more thorough discussion of defense methods to minimize potential negative impacts. In addition, please disclose the detailed protocol for the human review process."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O4sTI8JAWd", "forum": "6i6cuKURtr", "replyto": "6i6cuKURtr", "signatures": ["ICLR.cc/2026/Conference/Submission10152/Reviewer_VmS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10152/Reviewer_VmS7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760547851601, "cdate": 1760547851601, "tmdate": 1762921521659, "mdate": 1762921521659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "5MDWEo5HQO", "forum": "6i6cuKURtr", "replyto": "6i6cuKURtr", "signatures": ["ICLR.cc/2026/Conference/Submission10152/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10152/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763182793742, "cdate": 1763182793742, "tmdate": 1763182793742, "mdate": 1763182793742, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel Self-Disguise Attack (SDA), which enables large language models (LLMs) to automatically disguise their outputs through carefully designed prompts, thereby evading AI-generated text (AIGT) detectors. The attack consists of a adversarial feature extractor aiming for generating disguise features and a retrieval-based context examples optimizer that selects the most relevant successfully disguised samples from an external knowledge base. Experimental results demonstrate that SDA significantly reduces the success rate of various detectors while maintaining or even improving the overall text quality and diversity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is fully implemented through prompt engineering and in-context learning, without requiring any fine-tuning of the model, which significantly reduces computational cost and implementation complexity.\n2. The disguise features are expressed in natural language form, providing interpretability and helping to reveal the intrinsic differences between AIGT and HWT.\n3. The authors conduct a comprehensive evaluation using multiple quantitative metrics, thoroughly assessing both the effectiveness of the proposed attack and the quality of the generated texts."}, "weaknesses": {"value": "1. The proposed approach shows a strong dependency on prompt design and the chosen proxy detector, which raises concerns about its generalizability to broader domains or unseen detection models.\n2. The experimental setup is limited, using only 1,000 samples extracted from the RAID dataset and focusing solely on summarization tasks; further validation on diverse tasks and datasets is necessary to demonstrate robustness.\n3. The external knowledge base entirely relies on the disguised texts generated during the feature extraction phase, resulting in a single-source dataset that may limit its quality and diversity, while also incurring a high maintenance cost.\n4. The paper’s presentation is verbose, with key ideas not clearly emphasized; in particular, the mechanism by which the knowledge base improves text quality and diversity is not well explained."}, "questions": {"value": "Please refer to my comments on weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "n8jxcX9dSd", "forum": "6i6cuKURtr", "replyto": "6i6cuKURtr", "signatures": ["ICLR.cc/2026/Conference/Submission10152/Reviewer_yWpt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10152/Reviewer_yWpt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641886382, "cdate": 1761641886382, "tmdate": 1762921521238, "mdate": 1762921521238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Self-Disguise Attack (SDA), a novel method for AI-generated text (AIGT) detection evasion designed to overcome the high fine-tuning costs and text degradation. SDA enables an LLM to actively disguise its outputs as more human-like through prompting rather than post-editing. It has two key components: an adversarial feature extractor that learns salient “disguise features” and a retrieval-based context examples optimizer that fetches relevant undetectable example texts from an external knowledge base. These components are combined into a single prompt containing the extracted disguise features and optimized in-context examples to guide the LLM in generating detection-resistant text. Extensive experiments show the effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written.\n2. The idea is novel."}, "weaknesses": {"value": "1. SDA’s effectiveness hinges on the proxy detector used to extract disguise features, which may limit its universality. It is tuned against a specific surrogate detector (e.g. a RoBERTa-based ChatGPT detector). It generalizes to other detectors better than baselines, but lack  explanation for this phenomenon.\n2. SDA involves an iterative feature extraction process and the creation of an external knowledge base of disguised examples. While the experimental section provides some runtime measurements, I recommend that the authors include a more detailed analysis of the method’s computational complexity. Specifically, why it is more efficient than SICO?\n3. The baselines are too weak. Can SDA evade zero-shot detectors like Fast-DetectGPT [R1], Binoculars [R2].\n\n[R1]. Bao, G., Zhao, Y., Teng, Z., Yang, L., & Zhang, Y. (2023). Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. arXiv preprint arXiv:2310.05130.\n\n[R2]. Hans, A., Schwarzschild, A., Cherepanova, V., Kazemi, H., Saha, A., Goldblum, M., ... & Goldstein, T. (2024). Spotting llms with binoculars: Zero-shot detection of machine-generated text. arXiv preprint arXiv:2401.12070."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ADz5uzqwLd", "forum": "6i6cuKURtr", "replyto": "6i6cuKURtr", "signatures": ["ICLR.cc/2026/Conference/Submission10152/Reviewer_xQpQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10152/Reviewer_xQpQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779146474, "cdate": 1761779146474, "tmdate": 1762921520508, "mdate": 1762921520508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors introduce Self-Disguise Attack (SDA), a novel approach that enables large language models (LLMs) to actively disguise their output, reducing the detection probability of AI Generated Text.  The method used to accomplish this is to ask the LLM generating text to mimic the patterns found in Human Generated text."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Using extracted features to guide generation is a good approach, that makes intuitive sense to evade detection\n2. Using a large dataset to extract features makes this process automated and more reliable.\n2. SDA shows better detection evasion results than existing AIGT methods and displays better generation quality"}, "weaknesses": {"value": "1. Distinction form prior work: Computation benefits are listed as one of the main benefits over existing methods,  but no statistics are given to support the claim\n2. Lack of clarity: From fig 1, training data consists of detection evaded text, but from section 4.1 datasets is human generated text instead"}, "questions": {"value": "1. What are the computational benefits of SDA over previous methods? Two major benefits listed over previous methods are generation quality and generation cost. There is a table that displays generation quality improvement, but paper lacks results for generation resource consumption.\n2. How is the data used to extract features gathered? Is it human generated text, or generated text that evades detection in prior checks? There appears to be an incongruity in Fig 1 and section 4.1. Adding clarity in these sections would enable better understanding and ensure reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DtXTdgxoYZ", "forum": "6i6cuKURtr", "replyto": "6i6cuKURtr", "signatures": ["ICLR.cc/2026/Conference/Submission10152/Reviewer_yTBL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10152/Reviewer_yTBL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934251405, "cdate": 1761934251405, "tmdate": 1762921519964, "mdate": 1762921519964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}