{"id": "65ayMQAzlS", "number": 5548, "cdate": 1757919680541, "mdate": 1759897968327, "content": {"title": "Residual Pyramid Atrous Filtering Network with the Error Low-Rank Respresentation", "abstract": "Image filtering aims to eliminate perturbations and textures while preserving dominant structures, serving a pivotal role in various image processing tasks. More recently, significant advances in filtering techniques have been developed. However, existing approaches typically suffer from oversmoothing edges, gradient reversal, and halos. Such issues originate from the difficulty in striking an optimal trade-off between filtering multi-scale textures and preserving edges. Furthermore, deep learning-based filtering frameworks lack modules designed to capture features of different long-range dependence textures. Consequently, the task of filtering textures while maintaining edge integrity remains a significant challenge. To address these issues, we propose a novel residual pyramid atrous filtering network (RPAFNet) that utilizes the error low-rank representation. Specifically, we introduce a lightweight dilated spatial convolution (LDSC) module for effectively extracting multi-scale texture features. To boost the reconstruction feature space, we propose a difference residual layer (DRL) module for connecting the encoder and decoder. Additionally, by employing low-rank approximation, we introduce a new non-convex optimization model, termed gradient error low-rank representation model (GELR), which effectively suppresses textures and preserves edges. This paper provides complete theoretical derivations for solving GELR and its convergence. Extensive experiments demonstrate that the proposed approach outperforms previous techniques in attaining an equilibrium between texture filtering and edge retention, as validated by both visual comparison and quantitative evaluation across various smoothing and downstream applications.", "tldr": "", "keywords": ["non-convex optimization", "image filtering", "deep learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/769f19b65013098a539b5139c3a2815074330d0e.pdf", "supplementary_material": "/attachment/04ba060a5e693a582c1e58ab6916bdf8e5b53467.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes RPAFNet with two structure modifications (LDSC and DRL) and one theoretical model (GELR) to address the challenge in image filtering, that is, 1) balancing multi-scale texture suppression and 2) structural edge preservation. Based on the LDSC and DRL, the RPAFNet is capable of establishing multi-range correlation and preserving edge information. Additionally, the GELR uses $||\\nabla u||_1$  regularization to avoid edge blurring and suppress over-smoothing and $\\beta||\\nabla u-\\nabla x|| $ to precisely reduce textures through low-rank approximation. Overall, the RPAFNet achieves remarkable results in several experiments.\n​"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear, and the overall story is sound. The RPAFNet is based on both theoretical (GELR) and engineering optimization (LDSC, DRL).\n- The theoretical derivation is solid.\n- The experimental results are stable to show the effectiveness of the proposed model."}, "weaknesses": {"value": "- Despite the proposed method being able to extend to multiple downstream tasks,  the comparison is conducted on generic methods like Deepwls and WTL1. Since it is an NN-based model without performance advantages, it is important to show the performance gap with the task-specific model.\n- The inference performance and comparison are not included, and the training overhead for introducing GELR.\n- The theoretical analysis is clear; however, it might be more persuasive to provide a visual comparison for the proposed GELR.\n- The LSDC and DRL are not novel modules but a modification of existing methods. The design of dilation convolution (large kernel convolution) is widely used and explored."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jIPYYBwoqU", "forum": "65ayMQAzlS", "replyto": "65ayMQAzlS", "signatures": ["ICLR.cc/2026/Conference/Submission5548/Reviewer_9XUC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5548/Reviewer_9XUC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571226490, "cdate": 1761571226490, "tmdate": 1762918126473, "mdate": 1762918126473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a network called RPAFNet (Residual Pyramid Atrous Filtering Network) for image smoothing, aiming to suppress textures while preserving fine structural details. The model adopts a typical U-shaped encoder–decoder architecture, enhanced with two modules: LDSC (Large-Dilation Spatial Convolution) to expand the receptive field, and DRL (Difference Residual Layer) to strengthen feature reconstruction. In addition, the authors propose a Gradient Error Low-Rank Representation (GELR) that combines Total Variation (TV) with a low-rank constraint. Using an ADMM-based optimization strategy, the GELR term is incorporated as an additional loss to further improve structural preservation during smoothing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Well-organized and technically sound\n\nThe paper is clearly structured, with a well-defined model design and mathematically consistent derivations. The experiments are reasonably comprehensive, showing good implementation quality and logical consistency.\n\n2.Potential for further improvement and extension\n\nAlthough the novelty is limited, the work provides a clear and systematic framework for traditional image smoothing models. It also leaves room for future extensions, such as incorporating differentiable optimization or generative modeling methods (e.g., DPO or diffusion/flow-based models)."}, "weaknesses": {"value": "1.Limited architectural and module-level innovation\n\nRPAFNet adopts a conventional U-shaped encoder–decoder framework without introducing substantial structural novelty. The LDSC module relies on atrous convolution to expand the receptive field, a technique that was thoroughly explored in the DeepLab series (2017). In contrast, modern approaches typically employ Transformer or hybrid attention mechanisms to handle long-range dependencies, making the proposed design appear outdated. Meanwhile, the DRL module performs structure compensation merely through feature differencing, showing strong similarity to standard residual or skip connections. It lacks clear theoretical justification and empirical validation of its independent contribution. Overall, the design appears to be an integration of existing techniques rather than a breakthrough in architectural paradigm.\n\n2.Conservative loss design and potential optimization conflicts.\n\nThe loss formulation mainly follows the traditional “TV + low-rank” joint regularization framework, which has been extensively used in earlier image restoration and smoothing tasks. The paper introduces no novel constraint or optimization mechanism. Furthermore, the simultaneous use of structural loss L1, perceptual loss L2, and an SSIM term in both training and evaluation may cause gradient conflicts and metric bias. Since SSIM is explicitly optimized during training, its improvement in evaluation may partially result from direct loss fitting rather than genuine structural enhancement.\n\n3.Lack of explanation for experimental settings\n\nThe paper employs bilinear interpolation with a down sampling ratio of 0.8, which is an unconventional choice. However, no clear motivation, empirical justification, or performance sensitivity analysis is provided, limiting the interpretability and reproducibility of the results."}, "questions": {"value": "1.On the architectural design and innovation\n\nThe paper repeatedly highlights the “limited receptive field” as a key bottleneck, but this issue has already been addressed in recent architectures such as Transformer or hybrid attention networks. Could the authors clarify how the LDSC module provides advantages over standard atrous convolution, multi-scale structures, or Transformer-based architectures in terms of receptive field and feature representation? It would be helpful to include quantitative or qualitative comparisons to demonstrate its unique contribution.\nFurthermore, the DRL module performs structure compensation mainly through feature differencing, which appears conceptually similar to residual or skip connections. Could the authors elaborate on the motivation and theoretical foundation of DRL, and provide ablation results to justify its necessity and effectiveness compared with simpler alternatives such as residual fusion or attention-based refinement?\n\n2.On loss design and optimization objectives\n\nThe current “TV + Low-rank” joint regularization follows a traditional formulation without introducing new constraints or optimization mechanisms. Moreover, combining (L_1) and (L_2) losses may introduce conflicting gradient directions and affect structure preservation, have the authors observed such instability?\nAdditionally, since the SSIM term is used in both training and evaluation, could this cause metric bias and artificially inflate performance? It would strengthen the validity of the results if the authors could report outcomes of a version trained without SSIM loss, or analyze its impact empirically.\n\n\n3.On experimental settings\n\nThe paper uses bilinear interpolation with a down sampling ratio of 0.8, which is quite unusual. The authors should clarify during the rebuttal why this value was chosen — for example, to ensure gradient smoothness, multi-scale feature consistency, or based on empirical tuning — and analyze its sensitivity to this parameter.\n\n4.On methodological extension and potential research value\n\nWhile the paper focuses on image smoothing, it conceptually overlaps with texture suppression and structure enhancement problems often addressed by generative models. Have the authors considered applying the proposed framework in generative or diffusion-based architectures (e.g., Stable Diffusion, Flow Matching) to evaluate its scalability and potential cross-domain benefit? Such discussion would help clarify the method’s broader applicability and originality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QdURVOdRsL", "forum": "65ayMQAzlS", "replyto": "65ayMQAzlS", "signatures": ["ICLR.cc/2026/Conference/Submission5548/Reviewer_Wi8X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5548/Reviewer_Wi8X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616665300, "cdate": 1761616665300, "tmdate": 1762918125945, "mdate": 1762918125945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an image filtering network called RPAFNet, which aims to remove multi-scale textures while preserving edges. The network consists of two novel core modules:  Lightweight Dilated Spatial Convolution (LDSC) module and Difference Residual Layer (DRL). Lightweight dilated convolution is used to extract multi-scale texture features, and the difference between encoded features is used as a skip connection, emphasizing high-frequency information. An additional Gradient Error Low Rank (GELR) model is introduced to calculate a non convex optimization term based on low rank approximation at the loss end to further suppress texture and preserve edges."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The optimization is novel. Monotonically decreasing iterative closed form solutions for the GELR objective function is provided and convergence to a limit point is proved, efficiently suppressing texture and overcoming oversmoothing issues.\n    \n2. Integrating \"dilated pyramid feature extraction+differential residual skip connection+low rank texture suppression\" into an end-to-end framework, the three complement each other and have a clear idea.\n    \n3. The method is demonstrated with rich experiments. The quantitative indicators are comprehensively leading and the qualitative results are impressive."}, "weaknesses": {"value": "1. LDSC uses multiple dilation combinations, whick might induce gridding artifacts. It’s suggested that using FFT graph to verify whether this risk exists in RPAFNet.\n    \n2. Theoretical assumption is too strong. GELR convergence proof relies on the assumption of \"Lipschitz continuity and bounded gradient\", and whether the actual network feature map distribution satisfies this assumption has not been verified. Suggest providing statistics on the gradient/Lipschitz constant during the training process.\n    \n3. Sensitivity curves were not performed for the combination of dilation rate and low rank rank value r."}, "questions": {"value": "1. What’s $T^k$ in Equation 13?\n    \n2. Has the combination of different dilation rates been systematically searched? Is there a better \"receptive field scheduling\" strategy (such as dynamic dilation)?\n    \n3. Is the design friendly to high-resolution images (such as 4K)? Does the computational complexity increase with the square of resolution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nFN6NTc9NT", "forum": "65ayMQAzlS", "replyto": "65ayMQAzlS", "signatures": ["ICLR.cc/2026/Conference/Submission5548/Reviewer_QPdk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5548/Reviewer_QPdk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626981583, "cdate": 1761626981583, "tmdate": 1762918124916, "mdate": 1762918124916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. Proposed RPAFNet, a residual pyramid atrous filtering network for image smoothing.\n2. Introduced LDSC module to extract multi-scale texture features using dilated convolutions.\n3. Designed DRL module to enhance feature space via difference residual connections.\n4. Developed CTUM module to fuse local and global features for better reconstruction.\n5. Formulated a novel GELR model using gradient error low-rank representation for texture suppression.\n6. Provided complete theoretical derivation and convergence proof for the optimization algorithm.\n7. Demonstrated superior performance over state-of-the-art methods across multiple datasets and applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper demonstrates high originality by formulating a novel non-convex optimization model (GELR) that creatively combines a classical total variation term with a low-rank constraint on the gradient error.\n2. It presents a significant architectural innovation with its RPAFNet, which integrates purpose-built modules like LDSC and DRL specifically designed to handle multi-scale textures and enrich the feature space.\n3. The work is of exceptional quality due to its theoretical rigor, providing complete derivations for the non-convex optimization and a comprehensive convergence analysis for the proposed ADMM algorithm.\n4. The experimental quality is thorough and convincing, featuring extensive comparisons against 14 state-of-the-art methods across multiple datasets and meaningful downstream applications."}, "weaknesses": {"value": "1. The GELR model's reliance on ground-truth gradients (∇x) during training severely limits its real-world applicability, restricting it to synthetic datasets and preventing use on natural images where ideal smoothed targets are unavailable.\n2. Computational efficiency is completely unanalyzed, with no reporting of inference speed, model size, or comparison to efficient alternatives, making practical utility impossible to assess.\n3. The central claim of superior multi-scale texture handling lacks quantitative validation, relying solely on visual examples rather than objective metrics measuring texture-scale uniformity or structural preservation.\n4. Critical ablation studies are incomplete, with the CTUM module's impact shown only numerically without visual proof, and the hyperparameter selection process lacking principled justification for balancing texture removal versus edge preservation.\n5. Specific artifact analysis is insufficient, failing to provide direct visual evidence of improvement on stated problems like gradient reversal and offering only superficial treatment of the acknowledged low-contrast texture limitation."}, "questions": {"value": "1. The GELR model's reliance on ground-truth gradients (∇x) during training severely limits its real-world applicability, restricting it to synthetic datasets and preventing use on natural images where ideal smoothed targets are unavailable.\n2. Computational efficiency is completely unanalyzed, with no reporting of inference speed, model size, or comparison to efficient alternatives, making practical utility impossible to assess.\n3. The central claim of superior multi-scale texture handling lacks quantitative validation, relying solely on visual examples rather than objective metrics measuring texture-scale uniformity or structural preservation.\n4. Critical ablation studies are incomplete, with the CTUM module's impact shown only numerically without visual proof, and the hyperparameter selection process lacking principled justification for balancing texture removal versus edge preservation.\n5. Specific artifact analysis is insufficient, failing to provide direct visual evidence of improvement on stated problems like gradient reversal and offering only superficial treatment of the acknowledged low-contrast texture limitation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zjTEMd93h4", "forum": "65ayMQAzlS", "replyto": "65ayMQAzlS", "signatures": ["ICLR.cc/2026/Conference/Submission5548/Reviewer_deHs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5548/Reviewer_deHs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762223609127, "cdate": 1762223609127, "tmdate": 1762918124455, "mdate": 1762918124455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}