{"id": "WZB5wh0qVR", "number": 16980, "cdate": 1758270895201, "mdate": 1759897206479, "content": {"title": "VideoMolmo: Spatio-Temporal Grounding meets Pointing", "abstract": "Spatio-temporal localization—the ability to identify both the position and temporal evolution of objects—is essential for applications from cell tracking to autonomous navigation. Recent Video Large Multimodal Models (Video-LMMs) show promise but remain limited by coarse predictions, heavy reliance on dense mask optimization, and limited interpretability. We introduce VideoMolmo, a two-stage framework that grounds objects through point-based localization. Rather than directly predicting dense masks, VideoMolmo first produces precise points as lightweight, interpretable anchors, which are then used for downstream tasks including referring segmentation, video object segmentation, and counting. By decoupling localization from task execution, our approach provides more robust and transparent reasoning. Built on Molmo, our framework incorporates a temporal attention module for cross-frame reasoning and introduces a novel bidirectional temporal mask fusion strategy, enabling coherent point propagation and accurate segmentation. To facilitate training and evaluation, we release a large-scale spatio-temporal pointing dataset of 72k video–caption pairs with 100k annotated points and curate VPoS-Bench, a challenging benchmark spanning five real-world domains. Experiments show that VideoMolmo outperforms existing approaches, with gains of $5.4$ percentage points (pp) on VPoS-Bench and $9.5$ pp on MeViS. This highlights the effectiveness of point-based representations as a foundation for interpretable, fine-grained reasoning in dynamic visual environments.", "tldr": "We present VideoMolmo, a video-LMM for spatio-temporal localization. With temporal attention, mask fusion, and a new 72k-video dataset plus VPoS-Bench, it achieves superior accuracy and reasoning over prior methods.", "keywords": ["Video-LMM", "Grounding", "Pointing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bcdea5a40414003a6cc1523465304689d4d099b7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the challenge of fine-grained spatio-temporal localization by replacing dense mask prediction with point-based grounding, enabling interpretable and efficient localization across frames. Built on Molmo, it introduces 1) a temporal attention module for cross-frame reasoning and 2) a bidirectional temporal mask fusion mechanism for coherent point propagation for video understanding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed approach outperforms previous approaches on multiple datasets at multiple metrics.\n- The evaluation benchmark is exhaustive on multiple aspects of spatio-temporal evaluation."}, "weaknesses": {"value": "- Architecture Novelty\n   - Section 4.1: Temporal Module: Aggregating information using past frames feature aggregation is a very common aspect of trivial video understanding. For video segmentation or dense tasks the understanding from frames to patch level aggregation. Earlier works utilize temporal feature aggregation or memory module - the idea is a base setup not a novelty. If there’s something missing I would like authors to clarify. Specifically, if there’s some previous work used as a baseline and then made changes for this paper.\n   - Section 4.2: BiDirectional Temporal Mask Fusion: In the bidirectional propagation, are the masks interpolated for intermediate frames (between i and i+n)? The fusion strategy looks more like a heuristic based on masks - a hyperparameter value dependent. If the overlap is significant enough then take intersection otherwise union. This heuristic might be generalizable enough across datasets, but that doesn’t make a novel contribution. It can’t be a contribution to the ICLR level main conference paper.\n- Dataset\n   - VPoS-Bench: Since dataset generation is a contribution of the paper, I was expecting more stats or details about the dataset. The datasets combined all have different properties in terms of video length and query aspect. Can authors please go more in detail about the dataset? The range contains from simple scenarios to complex datasets such as MeVis. I tried to look at appendix, however, I couldn’t find the details.\n- Result\n    - The paper should include more approaches for fair comparison -> Table 3 [1].\n\n[1] Ding, H., Tang, S., He, S., Liu, C., Wu, Z., & Jiang, Y. G. (2025). Multimodal referring segmentation: A survey. arXiv preprint arXiv:2508.00265."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pu2saq99S6", "forum": "WZB5wh0qVR", "replyto": "WZB5wh0qVR", "signatures": ["ICLR.cc/2026/Conference/Submission16980/Reviewer_unq8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16980/Reviewer_unq8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962863391, "cdate": 1761962863391, "tmdate": 1762926995418, "mdate": 1762926995418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces VideoMolmo as a two-stage framework that grounds objects through point-based localization and a large-scale spatio-temporal pointing dataset of 72k video–caption pairs with 100k annotated points, and VPoS-Bench, a challenging benchmark spanning five real-world domains. Extensive experiments verify the effectiveness of the proposed framework and the benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is well-written and easy to follow for readers.\n\n2.A lot of quantitative experiments have been conducted to verify that VideoMolmo outperforms most previous state-of-the-arts across various downstream tasks."}, "weaknesses": {"value": "1.One concern for this work is the motivation of the point-based grounding formulation. As mentioned in the manuscript, the point-level supervision is constructed from mask-level data, and it would be unclear why the point-based formulation would be better compared to other data formats such masks for visual grounding in videos? I think mask-based annotations can also transfer to various other forms like points, bounding boxes, etc. So the rationality of the point-based formulation of this work needs further clarifications and justifications.\n\n2.There seem to be some missing details for the experimental comparisons. When comparing on the proposed VPoS-Bench, are the baselines like some video mllms retrained on the proposed spatio-temporal pointing dataset or just evaluated in a zero-shot manner? When comparing on other downstream tasks, is the proposed VideoMolmo trained from stratch on these downstream data or inherited with the pretrained knowledge of the proposed point-based dataset?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f0P9immMOX", "forum": "WZB5wh0qVR", "replyto": "WZB5wh0qVR", "signatures": ["ICLR.cc/2026/Conference/Submission16980/Reviewer_MS2t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16980/Reviewer_MS2t"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001536787, "cdate": 1762001536787, "tmdate": 1762926994713, "mdate": 1762926994713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VideoMolmo, a two-stage video–language model:\nIt separates the task of identifying an object (by points) from other downstream tasks like segmenting the object.\nThe first stage predicts points that represent object identity.\nThe second stage uses these points to guide downstream models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Leveraging of pretrained segmentation models to generate instance masks based on point prompts\n\n- Strong performance against baselines"}, "weaknesses": {"value": "- The assumption that points make it clear what the model means is wrong. The Segment Anything Model already shows that if one points at an eye, the pointing is ambiguous: it could be the eye, the head, or the whole body that is pointed at.\n\n- Limited novelty: The paper uses known object-centric principles, such as disentangling object appearance from position. In this case, position is disentangled from the downstream task of mask generation. Additionally, the temporal averaging is a simple mean computation. Overall the model is a straightforward extension of the Molmo model to the video domain with on clever object centric trick.\n\n- Use of synthetic point labels generated with SAM-V2 that are tuned specifically to generate high IoU when used with SAM-V2 but are not evaluated otherwise."}, "questions": {"value": "- Why is the temporal aggregation just a simple average? Doesn’t that smooth away important temporal details?\n\n- Are there any human-based evaluations showing that the generated annotations of the training sets are really meaningful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s3L0GceuG1", "forum": "WZB5wh0qVR", "replyto": "WZB5wh0qVR", "signatures": ["ICLR.cc/2026/Conference/Submission16980/Reviewer_uK38"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16980/Reviewer_uK38"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762361491558, "cdate": 1762361491558, "tmdate": 1762926994191, "mdate": 1762926994191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}