{"id": "x6tMP01Imy", "number": 22775, "cdate": 1758335286150, "mdate": 1759896847194, "content": {"title": "Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)", "abstract": "Vision-language co-embedding networks, such as CLIP, provide a latent embedding space with semantic information that is useful for downstream tasks. We hypothesize that the embedding space can be disentangled to separate the information on the content of complex scenes by decomposing the embedding into multiple concept-specific component vectors that lie in different subspaces. We propose a supervised dictionary learning approach to estimate a linear synthesis model consisting of sparse, non-negative combinations of groups of vectors in the dictionary (atoms), whose group-wise activity matches the multi-label information. Each concept-specific component is a non-negative combination of atoms associated to a label. The group-structured dictionary is optimized through a novel alternating optimization with guaranteed convergence. Exploiting the text co-embeddings, we detail how semantically meaningful descriptions can be found based on text embeddings of words best approximated by a concept's group of atoms, and unsupervised dictionary learning can exploit zero-shot classification of training set images using the text embeddings of concept labels to provide instance-wise multi-labels. We show that the disentangled embeddings provided by our sparse linear concept subspaces (SLiCS) enable  concept-filtered image retrieval (and conditional generation using image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed autoencoder embeddings from TiTok and the latent embedding from self-supervised DINOv2. Quantitative and qualitative results highlight the improved precision of the concept-filtered image retrieval for all embeddings.", "tldr": "", "keywords": ["subspace disentanglement", "image retrieval", "vision-language model", "latent embeddings"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7da2fa348e7cbbb7a1129d041c6fda3b1852ee03.pdf", "supplementary_material": "/attachment/177aee60db13f6f3d6e9d8802865c16a61123de3.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a novel approach to refining global embeddings from image embedding models like CLIP into more structured, two-tiered concept-labeled subspaces, with a focus on improving \"concept filtered retrieval.\" The authors employ dictionary learning to develop a set of central high-level concepts, each consisting of atoms that represent sub-concepts. This approach transforms the embedding space into a collection of distinct subspaces or \"cones,\" each containing relevant sub-concepts. The methodology is termed SLiCS - Sparse Linear Concept Subspaces.\n\n\nThe technique involves approximating each embedding vector as a linear combination of concepts from various subspaces, with atom-level representations organized within each concept subspace. A non-negative constraint is applied to the coefficient vector, forming a \"cone\" that encourages the co-occurrence of positively similar concepts, aligning with the cosine similarity metric utilized in CLIP models.\n\n\nIn its supervised variant, the approach minimizes the mean squared error of the reconstructed embedding approximation. This is achieved under a non-negative constraint for co-efficients using a modified supervised dictionary learning algorithm based on K-SVD, employing a rank-1 approximate SVD-based update heuristic. An unsupervised version involves inferring pseudo label mapping from a pre-existing concept list, derived from CLIP embeddings, followed by the same supervised process.\n\n\nThe authors evaluate their method using training and evaluation sets from MIRFlickr25K and MS COCO, comparing retrieval performance against CLIP, DINO, TikTok, and SpLiCE, with mean average precision at top 20 as the metric. The findings demonstrate a significant improvement of SLiCS models over other methods across all evaluated embeddings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The methodology employs advanced dictionary learning techniques to map flat embeddings to structured concept spaces.\n- The algorithm is thoroughly detailed in Appendix 1, offering clear and comprehensive explanations.\n- The paper provides a robust mathematical analysis of the learning method, including alternatives and theoretical guarantees.\n- It includes illustrative visualizations, such as image-to-prompt examples for various methods.\n- Progressive visualizations of intermediate data points within a subspace are provided, offering qualitative insights into the learned concepts and atoms.\n- The study includes a comparative analysis against SpLiCE, one of the most relevant and comparable methods."}, "weaknesses": {"value": "The problem formulation of \"concept filtered retrieval\" seems somewhat contrived. The purpose of embedding models like CLIP is to encapsulate the comprehensive semantics of an image. Consequently, if the query is with an image of a \"man talking on phone,\" the retrieval task focusing separately on images of men and phones may not be practically relevant. While the authors have developed an intricate mapping scheme between a flat CLIP embedding and a structured concept space, the practical relevance of this mapping for retrieval tasks remains unclear to me."}, "questions": {"value": "- Curious, if you have any aggregate analysis on how distinct/redundant the atoms within a subspace is? \n- Also if there was any attempt or analysis done to auto-label the fine grained concept components, as in some of the explainable ML efforts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XJu6Fefcqg", "forum": "x6tMP01Imy", "replyto": "x6tMP01Imy", "signatures": ["ICLR.cc/2026/Conference/Submission22775/Reviewer_V1fX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22775/Reviewer_V1fX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761271543912, "cdate": 1761271543912, "tmdate": 1762942381294, "mdate": 1762942381294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SLiCS, a method that targets concept-filtered image retrieval. It uses a K-SVD-like alternating optimization to learn a dictionary of concepts, where each concept is composed of atoms. This dictionary is used to linearly decompose image embeddings. The approach is evaluated on MIRFlickr-25K and MS-COCO using CLIP (transformer and CNN-based), DINOv2, and TiTok embeddings, reporting gains in mAP@20 with advantage over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Clear motivation. Concept-filtered retrieval is well-posed and seems to be practically useful.\n2) Intuitive and clearly presented method. The approach is straightforward and well-explained; the optimization scheme and underlying intuition are easy to follow.\n3) Broad evaluation across embedding types. The method is tested on several backbones (CLIP, DINOv2, TiTok) and datasets, suggesting generality across embedding spaces.\n4) Inherent interpretability. The per-concept subspaces lend themselves naturally to visualization and qualitative inspection, which enhances transparency of retrieval results."}, "weaknesses": {"value": "1) Limited novelty. The proposed method is essentially a variant of K-SVD with non-negativity and concept-wise group sparsity. While it is nicely adapted to the retrieval context, the conceptual advance over classical dictionary learning or prior works like SpLiCE is relatively small.\n2) Restricted evaluation. The experiments are limited to two datasets and a single retrieval metric (mAP@20). Including additional metrics such as Recall@K or evaluating on larger/more diverse datasets would strengthen the empirical evidence.\n3) Potential hyperparameter sensitivity. The number of atoms per concept and the number of active concepts per image appear to be tuned on validation sets, but there is little analysis of how performance varies with these choices. The approach may be sensitive to these hyperparameters."}, "questions": {"value": "1) You mention setting the number of active concepts to 2, motivated by the average number of concepts per image (2.467 and 2.303). Why not choose a larger value and allow the model to suppress irrelevant concepts through sparsity? Would the method naturally drive extra concept coefficients to zero?\n2) Is there a systematic or data-driven way to select the number of concepts or atoms per concept based solely on the embeddings (e.g., reconstruction error trends or stability analysis)?\n3) How does performance change if these hyperparameters are misspecified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z3EyPwTsu6", "forum": "x6tMP01Imy", "replyto": "x6tMP01Imy", "signatures": ["ICLR.cc/2026/Conference/Submission22775/Reviewer_KU5w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22775/Reviewer_KU5w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934854677, "cdate": 1761934854677, "tmdate": 1762942381036, "mdate": 1762942381036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)” introduces a novel approach for decomposing dense vision-language embeddings (like CLIP’s) into interpretable, concept-specific subspaces. The authors propose a supervised dictionary learning model in which each image embedding is represented as a non-negative, sparse combination of basis vectors (“atoms”) grouped by semantic concepts. This decomposition allows each component of an embedding to capture a specific concept, effectively disentangling complex scene representations into meaningful substructures. The optimization framework extends K-SVD with non-negativity and group sparsity constraints, ensuring convergence while maintaining interpretability. In addition, an unsupervised variant leverages CLIP’s text embeddings to infer pseudo-labels via zero-shot classification, enabling SLiCS to operate even without annotated data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. SLiCS introduces a principled method to decompose dense, opaque vision-language embeddings into interpretable concept-specific subspaces. Unlike previous methods that rely directly on word embeddings or linear projections, SLiCS learns structured, non-negative, sparse subspaces that correspond to semantically coherent concepts. This improves both interpretability and controllability of latent representations — a major step forward for understanding how high-dimensional embeddings encode different aspects of scenes.\n2. The paper presents a well-founded optimization framework inspired by K-SVD but extended to enforce non-negativity and group sparsity, ensuring each subspace captures a distinct concept. The authors also provide guaranteed convergence for their alternating optimization procedure and discuss computational considerations like mini-batch training for scalability. This level of mathematical rigor and algorithmic detail strengthens the credibility of the proposed method.\n3. A strong point of SLiCS is its broad applicability. It can operate on embeddings from different models (CLIP, TiTok, DINOv2) and handle both supervised and unsupervised settings. The unsupervised variant cleverly leverages CLIP’s text-aligned embeddings for zero-shot pseudo-labeling, making it practical for large datasets where concept labels may not be available.\n4. SLiCS provides tangible improvements in concept-filtered image retrieval, a task where existing holistic embeddings often fail to separate distinct visual elements. The results demonstrate consistent gains in precision across datasets and embedding types, showing that disentangled embeddings are not just interpretable but also more effective for fine-grained retrieval and potentially for conditional generation.\n5. By providing a method to explicitly identify and manipulate concept-level components within a latent space, SLiCS advances the broader goal of explainable and modular machine learning. It bridges the gap between powerful but black-box embedding models and transparent, concept-driven reasoning, offering new possibilities for interactive or human-in-the-loop AI systems."}, "weaknesses": {"value": "1. While the paper provides a theoretically sound optimization framework, the dictionary learning and non-negative sparse coding steps can be computationally demanding, especially for high-dimensional embeddings or large-scale datasets. Although mini-batch training is mentioned as a workaround, the algorithm may still struggle to scale to modern foundation model embeddings (e.g., CLIP-L/14 or EVA-CLIP) or real-time applications.\n2. Both the supervised and unsupervised versions of SLiCS rely on a fixed set of concept labels. In practice, defining this concept set can be subjective and may limit generalization to unseen or abstract concepts. The unsupervised variant partially addresses this by using zero-shot text embeddings, but it still assumes a known vocabulary and a fixed number of active concepts per image, which can constrain flexibility in open-world scenarios.\n3. Although SLiCS aims to learn interpretable subspaces, the semantic meaning of each subspace is inferred post-hoc by matching atoms to text embeddings. This makes interpretability somewhat indirect and dependent on the quality and coverage of the text encoder, rather than emerging naturally from the model. It may also lead to mismatches between the learned subspace and human-understandable concepts.\n4. The experiments focus primarily on concept-filtered image retrieval tasks, without broader exploration of other applications such as conditional generation, editing, or reasoning. While retrieval is a clean benchmark, it does not fully demonstrate whether SLiCS provides richer disentanglement properties or generalizes to more complex downstream uses.\n5. The paper lacks comparison against modern interpretable embedding methods or representation disentanglement frameworks (e.g., Concept Bottleneck Models, Concept Whitening, or interpretable subspace discovery via CAVs). Including such baselines would better situate SLiCS within the broader landscape of interpretability research.\n6. The algorithm’s performance depends on choices such as the number of atoms per concept (d₀) and initialization via truncated SVD. While the paper provides reasonable heuristics, it does not explore how sensitive SLiCS is to these design choices. This raises questions about robustness and reproducibility across datasets and embedding spaces."}, "questions": {"value": "1. How sensitive is SLiCS to the choice of hyperparameters such as the number of atoms per concept (d_0) or the number of training iterations? Did you observe significant performance variance across different initializations?\n2. The paper claims guaranteed convergence for the alternating optimization procedure. Could the authors elaborate on the nature of this guarantee — is it convergence to a stationary point, or to a global minimum under certain assumptions?\n3. Given that modern embeddings (e.g., CLIP-L/14 or OpenCLIP-H) can have tens of thousands of dimensions, how does the computational cost of SLiCS scale with embedding size? Are there approximations or dimensionality reduction techniques that make it more practical?\n4. The method enforces non-negative coefficients based on the intuition that cosine similarity defines semantic inclusion. Have the authors empirically compared this with unconstrained or signed decompositions to validate the impact of this constraint on interpretability and retrieval performance?\n5. The paper assumes all concept dictionaries have the same number of atoms (M_j = d_0). Did the authors explore adaptive or data-driven ways to set M_j depending on concept complexity or label frequency?\n6. How consistent are the learned subspaces across different runs or datasets? For instance, do the same concepts yield similar atom groups, or is there significant drift depending on initialization and data distribution?\n7. In the unsupervised variant, pseudo-labels are derived from CLIP text embeddings. How reliable are these labels for ambiguous or fine-grained concepts, and do noisy pseudo-labels degrade subspace interpretability?\n8. The model assumes additive decomposition across concepts. Have the authors considered whether certain concepts (e.g., “dog” and “grass”) interact non-linearly in the latent space, and if so, how might SLiCS capture such dependencies?\n9. The paper demonstrates improved performance on concept-filtered retrieval. Have the authors tested SLiCS for other tasks — such as concept-based editing, few-shot learning, or conditional generation — to assess its general utility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GhvmPIMVNP", "forum": "x6tMP01Imy", "replyto": "x6tMP01Imy", "signatures": ["ICLR.cc/2026/Conference/Submission22775/Reviewer_jQMf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22775/Reviewer_jQMf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962480196, "cdate": 1761962480196, "tmdate": 1762942380827, "mdate": 1762942380827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)” introduces a novel approach for decomposing dense vision-language embeddings (like CLIP’s) into interpretable, concept-specific subspaces. The authors propose a supervised dictionary learning model in which each image embedding is represented as a non-negative, sparse combination of basis vectors (“atoms”) grouped by semantic concepts. This decomposition allows each component of an embedding to capture a specific concept, effectively disentangling complex scene representations into meaningful substructures. The optimization framework extends K-SVD with non-negativity and group sparsity constraints, ensuring convergence while maintaining interpretability. In addition, an unsupervised variant leverages CLIP’s text embeddings to infer pseudo-labels via zero-shot classification, enabling SLiCS to operate even without annotated data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. SLiCS is a principled method to decompose dense, opaque VL embeddings into interpretable concept-specific subspaces. Unlike previous methods that rely directly on word embeddings or linear projections, SLiCS learns structured, non-negative, sparse subspaces that correspond to semantically coherent concepts. This improves both interpretability and controllability of latent representations — a major step forward for understanding how high-dimensional embeddings encode different aspects of scenes.\n2. The paper presents a well-founded optimization framework inspired by K-SVD but extended to enforce non-negativity and group sparsity, ensuring each subspace captures a distinct concept. The authors also provide guaranteed convergence for their alternating optimization procedure and discuss computational considerations like mini-batch training for scalability. This level of mathematical rigor and algorithmic detail strengthens the credibility of the proposed method.\n3. A strong point of SLiCS is its broad applicability. It can operate on embeddings from different models (CLIP, TiTok, DINOv2) and handle both supervised and unsupervised settings. The unsupervised variant cleverly leverages CLIP’s text-aligned embeddings for zero-shot pseudo-labeling, making it practical for large datasets where concept labels may not be available.\n4. SLiCS provides tangible improvements in concept-filtered image retrieval, a task where existing holistic embeddings often fail to separate distinct visual elements. The results demonstrate consistent gains in precision across datasets and embedding types, showing that disentangled embeddings are not just interpretable but also more effective for fine-grained retrieval and potentially for conditional generation.\n5. By providing a method to explicitly identify and manipulate concept-level components within a latent space, SLiCS advances the broader goal of explainable and modular machine learning. It bridges the gap between powerful but black-box embedding models and transparent, concept-driven reasoning, offering new possibilities for interactive or human-in-the-loop AI systems."}, "weaknesses": {"value": "1. While the paper provides a theoretically sound optimization framework, the dictionary learning and non-negative sparse coding steps can be computationally demanding, especially for high-dimensional embeddings or large-scale datasets. Although mini-batch training is mentioned as a workaround, the algorithm may still struggle to scale to modern foundation model embeddings (e.g., CLIP-L/14 or EVA-CLIP) or real-time applications.\n2. Both the supervised and unsupervised versions of SLiCS rely on a fixed set of concept labels. In practice, defining this concept set can be subjective and may limit generalization to unseen or abstract concepts. The unsupervised variant partially addresses this by using zero-shot text embeddings, but it still assumes a known vocabulary and a fixed number of active concepts per image, which can constrain flexibility in open-world scenarios.\n3. Although SLiCS aims to learn interpretable subspaces, the semantic meaning of each subspace is inferred post-hoc by matching atoms to text embeddings. This makes interpretability somewhat indirect and dependent on the quality and coverage of the text encoder, rather than emerging naturally from the model. It may also lead to mismatches between the learned subspace and human-understandable concepts.\n4. The experiments focus primarily on concept-filtered image retrieval tasks, without broader exploration of other applications such as conditional generation, editing, or reasoning. While retrieval is a clean benchmark, it does not fully demonstrate whether SLiCS provides richer disentanglement properties or generalizes to more complex downstream uses.\n5. The paper lacks comparison against modern interpretable embedding methods or representation disentanglement frameworks (e.g., Concept Bottleneck Models, Concept Whitening, or interpretable subspace discovery via CAVs). Including such baselines would better situate SLiCS within the broader landscape of interpretability research.\n6. The algorithm’s performance depends on choices such as the number of atoms per concept (d₀) and initialization via truncated SVD. While the paper provides reasonable heuristics, it does not explore how sensitive SLiCS is to these design choices. This raises questions about robustness and reproducibility across datasets and embedding spaces."}, "questions": {"value": "1. How sensitive is SLiCS to the choice of hyperparameters such as the number of atoms per concept (d_0) or the number of training iterations? Did you observe significant performance variance across different initializations?\n2. The paper claims guaranteed convergence for the alternating optimization procedure. Could the authors elaborate on the nature of this guarantee — is it convergence to a stationary point, or to a global minimum under certain assumptions?\n3. Given that modern embeddings (e.g., CLIP-L/14 or OpenCLIP-H) can have tens of thousands of dimensions, how does the computational cost of SLiCS scale with embedding size? Are there approximations or dimensionality reduction techniques that make it more practical?\n4. The method enforces non-negative coefficients based on the intuition that cosine similarity defines semantic inclusion. Have the authors empirically compared this with unconstrained or signed decompositions to validate the impact of this constraint on interpretability and retrieval performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GhvmPIMVNP", "forum": "x6tMP01Imy", "replyto": "x6tMP01Imy", "signatures": ["ICLR.cc/2026/Conference/Submission22775/Reviewer_jQMf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22775/Reviewer_jQMf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962480196, "cdate": 1761962480196, "tmdate": 1763071464058, "mdate": 1763071464058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Sparse Linear Concept Subspaces (SLiCS), that combine atoms for dictionary learning into larger concept sub-cones of the space, that more robustly represent large concepts. They evaluate this using concept-filtered retrieval (eg, find me an image closest to this query image, but only close on the “animal” part of the query)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method proposed by the paper is a creative extension to the dictionary learning space, and I think that it’s a wise research direction to be looking for more realistic ideas of what a “concept” is beyond a single direction in latent space. Their engagement with this makes the paper an interesting and inspiring read. There is a relatively thorough analysis of how these subspaces function."}, "weaknesses": {"value": "I don’t quite understand from the paper what the implications of this approach are, since the primary evaluation is concept-filtered retrieval, which is a task slightly contrived to showcase the strengths of this method.  Is it a better way for understanding latent spaces? I think the paper could be a bit stronger in its evaluations and comparisons to convince us of the utility of SLiCS."}, "questions": {"value": "Q1: In Figure 4 (the qualitative results), it looks like UF-Dino is quite good. Though of course it returns an image where more than one concept is present (eg, child and cow), it seems to be pretty good at the similarity of all of the concepts (eg both children are wearing a purple coat, both cats are black with white paws, etc). Why are the quantitative results showing worse performance for UF? Is there a specific objective for disentanglement in the quantitiative evaluation? How would you argue for the utility of this distentanglement, when features tend to be entangled in many ways (eg, maybe, wearing a purple coat and visitng a petting zoo). \n\nQ2: I think Figure 3 is very cool, but I don’t quite understand the implication of this d0 finding. Do you have any inuitions as to why it leads to clustering and then combining (but still separate neighborhoods). \n\nQ3: Could you expand on the utility of this method beyond successful concept-filtered retrieval? That is: do you think it can help push interpretability and our understanding the functioning of the latent space forward? Does it lead to some new proposal about the structure of the space (eg, expanding on the linear representation hypothesis somehow)?\n\nQ4: Is there the possibility of running an experiment that compares the concept spaces and their usefulness for interpretability compared to more traditional dictionary learning like SAEs or semi-NMF?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wNYSdzR0xb", "forum": "x6tMP01Imy", "replyto": "x6tMP01Imy", "signatures": ["ICLR.cc/2026/Conference/Submission22775/Reviewer_voRC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22775/Reviewer_voRC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762266122067, "cdate": 1762266122067, "tmdate": 1762942380518, "mdate": 1762942380518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}