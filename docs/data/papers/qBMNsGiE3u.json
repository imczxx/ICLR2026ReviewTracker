{"id": "qBMNsGiE3u", "number": 5393, "cdate": 1757906775713, "mdate": 1763714973473, "content": {"title": "Task-Aligned Attention Retrieval for Scaling Tabular Foundation Models", "abstract": "Retrieval serves as an effective approach to address various scaling challenges in in-context learning with tabular foundation models, yet prevailing methods select neighbors by Euclidean proximity in the covariates and thus ignore how the task mapping varies across the feature space. We introduce Task-Aligned Attention Retrieval (TAAR), a simple, model-agnostic procedure that, for each query, selects the most predictive features and relevant context samples using the model’s own attention scores. TAAR therefore ranks candidates by a task-aligned similarity already internalized by the foundation model, rather than by raw geometric distance in input features. TAAR is a drop-in module for state-of-the-art tabular foundation models (e.g., TabPFN and LimiX), requires no fine-tuning, and adds only an extra forward pass. On classification and regression benchmarks, TAAR achieves pronounced gains in accuracy and stability over current retrieval methods and supports scaling along feature space, sample size and target-class cardinality.", "tldr": "", "keywords": ["Retrieval Tabular data", "Foundation models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55d78dfa9704c8813df8505e36609ed52f7b0590.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Task-Aligned Attention Retrieval (TAAR), a training-free retrieval module for tabular foundation models (e.g., TabPFN-v2, LimiX). Instead of picking neighbors by Euclidean distance in input space, TAAR uses the model’s own attention to (i) select a query-specific feature subspace and (ii) retrieve the most task-relevant instances. A companion scheme, Class-Range Lifting Retrieval (CRLR), uses a regression head to build a label-sparse local context so classification backbones with small fixed vocabularies (≤10 classes) can handle many-class problems at inference. The method needs only an extra forward pass, includes a streaming/stratified variant for large pools, and comes with theory showing better rates vs. distance-only neighbors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is original in shifting tabular in-context retrieval from geometry-based neighbors to task-aligned, attention-guided retrieval, and in combining feature-level and instance-level attention into a single retrieval procedure.  \n\n2. It is training-free : The proposed TAAR can be plugged into existing tabular FMs (TabPFN-v2, LimiX) without re-training, which raises the practical value of the contribution.  \n\n3. The paper removes a real limitation of many tabular FMs—small fixed label vocabularies—via CRLR, making >10-class problems feasible at inference.  \n\n4. The empirical section is broad: across TabZilla, BCCO-CLS and TALENT benchmarks, task-aligned retrieval consistently improves LiMix and TabPFNv2."}, "weaknesses": {"value": "1. Computational overhead. The method requires running a retrieval procedure for every test sample, which can be expensive in realistic serving scenarios (low-latency or high-throughput settings).\n\n2. Strong reliance on backbone attention quality. TAAR assumes that the backbone’s attention already encodes task-relevant feature and instance importance. On datasets with noisy features, domain shift, or weakly trained tabular FMs, attention may be poorly calibrated, and the task-aligned retrieval could become unstable or even harmful.\n\n3. CRLR presumes the regression head can reliably propose a label-sparse local context. On long-tailed, highly discrete, or noisy-label datasets this assumption may not hold."}, "questions": {"value": "(1) LimiX and TabPFNv2 share broadly similar architectures, yet TAAR brings a much larger improvement to LimiX. Can the authors clarify whether this is because LimiX inherently produces more informative / better attention maps (i.e., it is already better at identifying salient features and relevant training instances) than TabPFNv2 ? \n\n(2) Why KNN helps LimiX but often hurts TabPFN-v2 ? In Tables 1 and 2, KNN-based retrieval consistently improves LimiX, but for TabPFN-v2 it often degrades performance. \n\n(3) Suppose we run TAAR on top of LimiX and obtain, for each test sample, a set of task-relevant training instances. If we then feed exactly these retrieved instances to other tabular FMs (e.g., TabPFN-v2, TabICL), do we also observe gains? In other words, is the importance signal that LimiX exposes via attention at least partly model-agnostic, or is TAAR’s benefit mostly tied to the backbone that generated the attention?\n\n(4) I like the overall idea, but I still have concerns about practicality: running retrieval for every test sample can be expensive in real deployments. Do the authors have concrete ideas for reducing the cost of TAAR ?\n\n(5) For LimiX and TabPFNv2, which layer’s attention map is actually used to drive TAAR ?\n\n(6) The citation for TabICL is currently incorrect. The right one should be:  \n  \nJingang, Q. U., Holzmüller, D., Varoquaux, G., & Le Morvan, M. TabICL: A Tabular Foundation Model for In-Context Learning on Large Data. In Forty-second International Conference on Machine Learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5aUjzoIxug", "forum": "qBMNsGiE3u", "replyto": "qBMNsGiE3u", "signatures": ["ICLR.cc/2026/Conference/Submission5393/Reviewer_Dkxm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5393/Reviewer_Dkxm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933989224, "cdate": 1761933989224, "tmdate": 1762918035713, "mdate": 1762918035713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Task-aligned attention retrieval (TAAR), which involves subselecting samples and / or features based on the attention weights in a foundational model. Compared to using the foundational models TabPFN and Limix on all the data (without ensembling), the authors show slightly increased performance for KNN-based retrieval and even slightly better performance for TAAR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important issue for tabular data: scaling foundational models to larger sample sizes. This is one of the key open questions for the current generation of tabular models. The paper evaluates the proposed methods using two state-of-the-art foundational models for tabular data, TabPFNV2 and Limix. The authors use three standard benchmarks, TabZilla and BCCO-CLS for classification, and Talent for regression."}, "weaknesses": {"value": "# Main concerns\n\n- The method that is described in this paper is already present in the Limix paper. This is somewhat confusing as the authors claim it to be novel. It is not thoroughly evaluated, but it is mentioned, and Figures 1 and 2 are nearly identical to the Limix paper.\n\n- The comparison against foundational models is not using the settings recommended by the authors of the foundational models. TabPFNV2 and Limix should both be used with the default ensembling for comparison. Disabling the ensembling means to not be representative of the actual proposed method.\n\n- There are several strong, but unsupported statements in the paper, in particular in the introduction. For example \"Retrieval is crucial for scaling foundational models\". The TabFlex model for example is able to scale without retrieval. There are also several fine-tuning based approaches like tunetables and TabPFN unleashed that are able to work without retrieval. \n\n- Using retrieval based on attention weights does not really solve the scaling problem, as the attention weights need to be computed first. The paper describes a method around this, by splitting the training data into batches. However, this weakens the motivation for retrieval.\n\n## Minor notes\n- The comparison of methods across datasets ideally would be done with a critical difference diagram.\n\n- Feature selection is seen as an end in itself. However, the literature has shown that feature selection is usually not beneficial for ML Benchmark datasets.\n\n- For Table 1, showing improvements as red and degradation as green seems quite confusing to me.\n\n- The theoretical analysis for nearest neighbors retrieval seems unhelpful. Theorem 2.1 seems to show the consistency of KNN, which has a proof that can be found in the textbook \"A probabilistic theory of pattern recognition L Devroye, L Györfi, G Lugosi\".\n\n- Line 144 uses but doesn't define e(Y_i) and \\delta^{K-1}\n\n- Line 308 mentions \"Max cell to 5,000,000\" which is unclear to me.\n\n- Line 402 is missing a whitespace in \"benchmarksto\"\n\n- Hollman 2025a and 2025b refer to the same paper and should be consolidated."}, "questions": {"value": "- What is different from this work to the attention-based selection proposed in the Limix paper?\n\n- What are the models used for TAAR and ECOC in Figure 4? \n\n- Are you using the multi-label strategy implemented in TabPFNV2 for Figure 4 or are you reimplementing ECOC? How does it compare against a one-vs-rest reduction?\n\n- Can you provide a comparison of the retrieval methods to the foundational models with ensembling? Is retrieval on top of ensembling beneficial?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The method described in this paper is part of the Limix paper. In particular, Figures 1 and 2 are basically identical to that paper. The limix paper is cited, which makes this a bit strange, and I assume this was submitted by the limix authors. If this was submitted by the limix authors, I would probably not flag it as an ethical issue, though submitting part of the paper and also citing the paper is somewhat strange."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ddK0OeJKBs", "forum": "qBMNsGiE3u", "replyto": "qBMNsGiE3u", "signatures": ["ICLR.cc/2026/Conference/Submission5393/Reviewer_QNjt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5393/Reviewer_QNjt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973079908, "cdate": 1761973079908, "tmdate": 1762918035417, "mdate": 1762918035417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to deal with large feature and context size during inference for tabular foundation models (TFMs). Authors propose to use model's attention to select top feature column and/or context instances. The motivation is that attention similarity better captures information relevant to prediction than KNN is raw space. Theoretical analysis and empirical results on real-world data sets are provided to support this conclusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a simple approach to select feature and instances that are both speed inference and improve prediction accuracy for in-context TMFs. Theoretical justification is given although it is based multiple assumptions and might not hold in practice. Empirical results on real-world data show that leveraging attention is more effective than current methods of using KNN in the raw feature space."}, "weaknesses": {"value": "I found the paper notation heavy and difficult to read. The theoretical results have multiple assumptions that likely won't hold in real world settings. The experimental section needs a significant revision. It would be useful to see an apples to apples comparison of KNN vs the proposed context selection approach (Equation 4) without feature selection or other artefacts of TAAR. That would clearly show if attention based retrieval is better than the feature one. I couldn't find any results on run time, that would also be useful to provide especially for large datasets. 3.2.2 does not refer to any figure and some figures like Figure 3 have very vague descriptions. I think a full revision is required."}, "questions": {"value": "Do you have a direct comparison of KNN vs attention-based (Equation 4) retrieval as well as runtime overhead for TAAR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qo1URUeA9d", "forum": "qBMNsGiE3u", "replyto": "qBMNsGiE3u", "signatures": ["ICLR.cc/2026/Conference/Submission5393/Reviewer_K55e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5393/Reviewer_K55e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043350957, "cdate": 1762043350957, "tmdate": 1762918035190, "mdate": 1762918035190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}