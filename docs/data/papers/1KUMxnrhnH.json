{"id": "1KUMxnrhnH", "number": 6579, "cdate": 1757989692459, "mdate": 1759897907081, "content": {"title": "A Separable Self-attention Inspired by the State Space Model for Computer Vision", "abstract": "Separable self-attention is an early attention mechanism with linear complexity. When parameters and FLOPs are comparable, lightweight networks built upon separable self-attention and its variants underperform the recent Vision Mamba (ViM). By analyzing the strengths and weaknesses of separable self-attention, we distill four design principles and, inspired by the State Space Model (SSM) serving as the core of ViM, propose a novel separable self-attention termed Vision Mamba Inspired Separable self-Attention (VMI-SA). Notably, VMI-SA does not incorporate any SSM blocks, and its attention computation process differs from all existing attention mechanisms to the best of our knowledge. We introduce proof-of-concept networks, VMINet and VMIFormer, enabling fair comparisons with ViMs through deliberate control of parameters, FLOPs, and encoder numbers. Compared to state-of-the-art Transformers, CNNs, and ViMs, VMINet and VMIFormer achieve competitive results in image classification and high-resolution dense prediction tasks.", "tldr": "Inspired by the state space model at the core of Vision Mamba, we propose a novel separable self-attention mechanism termed Vision Mamba-Inspired Separable Self-Attention.", "keywords": ["Separable Self-attention", "State Space Models", "Vision Mamba"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4a73be0e569e532b9fc1f42a8fc5120b0842c9e.pdf", "supplementary_material": "/attachment/0cd46f2349f939d5a4b0d712c9497c03b36d205a.zip"}, "replies": [{"content": {"summary": {"value": "This paper analyzes the strengths and weaknesses of separable self-attention, and based on which, proposes a new family of separable attention based model, namely VMI-SA. Comparing with previous methods, the main innovations of VMI-SA include:\n\n1. The attention blocks in VMI-SA apply element-wise multiplication to replace the traditional matrix multiplication\n\n2. Context vectors are introduced to replace attention matrices. \n\n3. Depth-wise conv is used to introduce local spatial correlations before the element-wise multiplication operation. \n\nBased on the ideas above, VMINet and VMIFormer are designed. Experimental results on image classification and object detection tasks show that the proposed method has comparable or better performance comparing with several recent proposed CNNs and Transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical basis of the proposed method is carefully proposed. \n\n2. The process of the designing of VMI-SA is well-presented. Each main component, such as element-wise multiplication, context vector, and MAMBA inspired attention, are analyzed detailedly. \n\n3. The experiments cover many main-stream ViT and CNN models, thus prove the advantages of the method."}, "weaknesses": {"value": "1. Some arguments need to be clarified. For instance, in 3.2, the authors firstly mentioned that \"The higher the\nrank of the attention matrix, the more attention information it contains, and the richer the feature diversity.\" Which implies that an attention matrix with higher rank may provide some benefits on feature extraction. After that, Eq.7 shows that the rank of context vector is less or equal with min{L, D}. Then the authors argued that the attention information in softmax(Q)⊙K is not only less abundant but also severely homogenized. Here, it seems like one benefit of context vector is to lower the rank of attention matrices. This is a conflict with the previous context. Moreover, in 3.3.2, the authors again mentioned that we need to enhance the rank of the attention matrix in the proposed method. \n\n2. Some tiny problems that may be improved. For instance, in Figure 2, it is better to mark some important features of the model, such as Q, K, and context vector."}, "questions": {"value": "My questions are proposed in the part \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OCpU4OHzFc", "forum": "1KUMxnrhnH", "replyto": "1KUMxnrhnH", "signatures": ["ICLR.cc/2026/Conference/Submission6579/Reviewer_21TC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6579/Reviewer_21TC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761034949416, "cdate": 1761034949416, "tmdate": 1762918913732, "mdate": 1762918913732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a variant of  separable self-attention method to incorporate correlation between tokens, which a basin SSA lacks. The authors incorporates three components: SSA, depthwise convolutio, and mask matrix to enhance the rank. This paper shows somewhat strong performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The evaluation is quite convicing. The comparison with ViM models shows that VMiNet and VMIFormer achieve superior performance over ViM variants.\n\nAlso, the ablation study of mask in Appendix demonstrates the importance of mask operation."}, "weaknesses": {"value": "It is not clear what the authors really adopt from SSM to this proposed model. The explation between Eq. 9 and Eq. 10 in not clear.\nAlso, the efficiency analysis is too limited. Efficient VMamba shows the least FLOPS with longer latency and the explatnion is \"nsufficient GPU utilization in EfficientVMamba’s SSM module during shorter sequence processing.\" Does it mean the results would be different on longer sequences?\nAlso, the comparison does not include Flatten Transformer."}, "questions": {"value": "1. Please clarify what exactly the inspiration from SSM is and the logic behind Eq. 9 and Eq. 10.\n2. Please add comparison with Flatten Transformer.\n3. Please include Top-5 accuracy.\n\n* please go over equations. For example, in Eq.5 == and != should be $-$ and $\\neq$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wIbUoc2Xp0", "forum": "1KUMxnrhnH", "replyto": "1KUMxnrhnH", "signatures": ["ICLR.cc/2026/Conference/Submission6579/Reviewer_uDKL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6579/Reviewer_uDKL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983132227, "cdate": 1761983132227, "tmdate": 1762918913209, "mdate": 1762918913209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes a novel linear-complexity separable self-attention mechanism called Vision Mamba Inspired Separable self-Attention (VMI-SA), which draws inspiration from the SSM/Mamba while avoiding integrating any SSM blocks and featuring an attention computation process distinct from existing mechanisms. It first distill four design principles by analyzing the strengths and weaknesses of separable self-attention, then design a recurrent formulation of VMI-SA and a matrix formulation to enhance token dependency modeling and computational efficiency. Based on VMI-SA, they construct two proof-of-concept networks, VMINet and VMIFormer, and conduct fair comparisons with state-of-the-art Transformers, CNNs, and ViMs by controlling parameters, FLOPs, and encoder counts. Experimental results show that VMINet and VMIFormer achieve competitive performance in ImageNet-1K image classification, MSCOCO object detection, and ADE20K semantic segmentation, demonstrating VMI-SA’s effectiveness in balancing performance and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces an interesting model, which incorporates separate self-attention modules into the Mamba marco design.\n2. The experiment are conducted on competitive benchmarks, e.g., ImageNet, COCO and ADE20K.\n3. The final model have linear complexity, which is a very promising research topic to explore."}, "weaknesses": {"value": "1. The novelty is limited. This paper incorporated the minor design in separable self-attention into the Mamba marco design, titled Mamba Inspired Separable self-Attention. It is very similar to MLLA (Mamba-Inspired Linear Attention)[1] , which incroporate the Mamba minor design into the vision transformer marco design.\n2. The paper also lacks the method comparsion and performance comparsion with MLLA[1].\n3. Although the authors claim this is a linear model, the performance when the token length varies is missing.\n4. There is nearly no ablation in the submission. Only ablation in mask type in Tab 5. However, how each minor design affects the final result is unclear.\n\n[1] Han et al, Demystify Mamba in Vision: A Linear Attention Perspective, in NeurIPS 2024."}, "questions": {"value": "1. How does each detailed structure affect the final performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b7shFsvRST", "forum": "1KUMxnrhnH", "replyto": "1KUMxnrhnH", "signatures": ["ICLR.cc/2026/Conference/Submission6579/Reviewer_t7qr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6579/Reviewer_t7qr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007926998, "cdate": 1762007926998, "tmdate": 1762918912702, "mdate": 1762918912702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Vision Mamba Inspired Separable Self-Attention (VMI-SA), a new separable self-attention mechanism drawing design principles from State Space Models, particularly Mamba. The authors propose VMINet—a prototype vision backbone built purely from stacking VMI-SA blocks and downsampling layers. Through extensive experimentation across image classification, detection, and segmentation tasks, VMINet is shown to outperform state-space-based Vim models and be competitive with strong baselines in lightweight settings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper analyzes different designs principles of self-attention, vision mamba, separable self-attention, and conclude the results into four rules to guide the design of the vision models."}, "weaknesses": {"value": "1. The involvement of causal mask does not make sense for most of the vision tasks since there are no causal hypotheses in the spatial dimension of the images and videos. That is why Mamba models [1,2,3] in vision need to define one or several complicated scanning sequence to ensure the visual signals are correctly modeled. In VMI-SA, the authors use two set of learnable gating parameters  $\\alpha$s and  $\\beta$s to control the proportion between the causal contexts and the direct contexts. It is of vital importance to carefully analyze how and go for different inputs and in different layers, which can provide meaningful insights on how these two types of context affect the model on vision tasks. Another important work [4] points out that the causal modeling in vision mamba models could be regarded as a forced local modeling pattern, which is also helpful. However, these analyses are missing in current submission, which fade the technical depth of the paper.\n\n2. For the discussion part of Effectiveness of VMI-SA, the authors replace the VMI-SA block with an FC layer. However, this design choice does not resemble ConvNeXt block since the normalization layers are not the same. Current drop of the accuracy cannot support the assertion. The authors could adopt the MetaFormer [5] archictecture equipped with VMI-SA, Pooling, and Self-attention, respectively to verify the impact of the spatial modeling module.\n\n3. The experiment results do not report the model variants in larger sizes, e.g, GFLOPs for inputs on the ImageNet-1K datasets, and models with longer sequence inputs, e.g., input resolutions. It is hard to distinguish the proposed VMINet out of the baselines such as ViM [1].\n\n[1] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. \"Vision mamba: Efficient visual representation learning with bidirectional state space model.\", ICML 2024\n\n[2] Liu, Yue, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, Jianbin Jiao, and Yunfan Liu. \"Vmamba: Visual state space model.\", NeurIPS 2024\n\n[3] Yang, Chenhongyi, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, and Elliot J. Crowley. \"Plainmamba: Improving non-hierarchical mamba in visual recognition.\", BMVC 2024\n\n[4] Han, Dongchen, Ziyi Wang, Zhuofan Xia, Yizeng Han, Yifan Pu, Chunjiang Ge, Jun Song, Shiji Song, Bo Zheng, and Gao Huang. \"Demystify mamba in vision: A linear attention perspective.\", NeurIPS 2024\n\n[5] Yu, Weihao, Chenyang Si, Pan Zhou, Mi Luo, Yichen Zhou, Jiashi Feng, Shuicheng Yan, and Xinchao Wang. \"Metaformer baselines for vision.\", IEEE TPAMI"}, "questions": {"value": "1. The format of the paper is a little messy with large blanks and unaligned equations.\n\n2. The \"Related Works\" section is missing, making it confusing to position this paper in some lines of research and show its unique advantages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dKY3anoCX2", "forum": "1KUMxnrhnH", "replyto": "1KUMxnrhnH", "signatures": ["ICLR.cc/2026/Conference/Submission6579/Reviewer_RwYd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6579/Reviewer_RwYd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159868467, "cdate": 1762159868467, "tmdate": 1762918912246, "mdate": 1762918912246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}