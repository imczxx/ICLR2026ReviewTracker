{"id": "8epkNiuAQC", "number": 20207, "cdate": 1758303694402, "mdate": 1763768657986, "content": {"title": "Influence Dynamics and Stagewise Data Attribution", "abstract": "Current training data attribution (TDA) methods treat influence as static, ignoring the fact that neural networks learn in distinct stages. This stagewise development, driven by phase transitions on a degenerate loss landscape, means a sample's importance is not fixed but changes throughout training. In this work, we introduce a developmental framework for data attribution, grounded in singular learning theory. We predict that influence can change non-monotonically, including sign flips and sharp peaks at developmental transitions. We first confirm these predictions analytically and empirically in a toy model, showing that dynamic shifts in influence directly map to the model's progressive learning of a semantic hierarchy. Finally, we demonstrate these phenomena at scale in language models, where token-level influence changes align with known developmental stages.", "tldr": "We demonstrate that neural network influence functions can change dramatically over training due to stagewise development, which challenges the static attribution paradigm and motivates a shift to stagewise data attribution.", "keywords": ["Training data attribution", "influence functions", "singular learning theory", "stagewise development", "phase transitions", "developmental interpretability", "Bayesian influence functions"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5cf08e80d5d03e11e5a785a932baaf78d7b21240.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a stage-wise influence framework to track how data influence evolves throughout training. It predicts non-monotonic behavior and validates these in both a toy model and large language models (LLMs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It introduces a novel framework of *stage-wise influence*, enriching the understanding of data attribution beyond static snapshots.\n2. Makes clear predictions and uses Bayesian Influence Functions to measure influence over time, confirming them in a toy model and in LLMs."}, "weaknesses": {"value": "1. Empirical verification depends on BIF and SGLD:\nValidation relies on BIF, which requires sampling a local posterior via RMSProp-SGLD around each checkpoint, while most models are trained with Adam-family optimizers, raising questions about stage-wise behavior under Adam. \n    \n2. Computational overhead:\nBIF uses Monte Carlo sampling, which is costly; complexity/scalability is not thoroughly analyzed, so practicality at scale remains uncertain.\n    \n3. Limited experimental scope:\nExperiments emphasize inter-class patterns (to align with Sec. 2.3), but broader claims about *when* data are influential and *how* they shape internals would benefit from intra-class analyses and more mechanistic evidence"}, "questions": {"value": "1. Figure 3: BIF and analytical IF resemble LOO but peak at different times. Why don’t the maxima align?\n2. Figure 7: The influence curve between “Dog” and all plant classes looks identical; the influence curve of “lily” and all animal classes mirrors that shape. Is this expected symmetry or some artifacts? \n3. Pointer: The text says the RMSProp-preconditioned SGLD sampler is “introduced in Section 2.2,” but the *details* are in Appendix B."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cy1R8kyctr", "forum": "8epkNiuAQC", "replyto": "8epkNiuAQC", "signatures": ["ICLR.cc/2026/Conference/Submission20207/Reviewer_CzFc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20207/Reviewer_CzFc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571510648, "cdate": 1761571510648, "tmdate": 1762933707255, "mdate": 1762933707255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their thoughtful engagement with our work. We are encouraged by the strong consensus that our stagewise attribution framework makes a novel and significant conceptual advance with solid theoretical grounding (Z984, LhWa, CzFc).\n\nBelow, we address the common themes raised across reviews and outline the changes made to the manuscript.\n\n### **Common Themes and Our Response**\n\n**1\\. Strengthening Empirical Validation via Interventions** (Z984, LhWa, CzFc)\n\nThe primary actionable feedback centered on demonstrating practical utility in the language-modeling setting, specifically through interventional experiments.\n\n* **Existing Interventions (Toy Model):** First, we wish to highlight that we provided interventional validation in our original submission. As detailed in Section 3 and Appendix C.5 (Figure 11), we explicitly ablated specific data points for the toy model only during specific training windows. This demonstrated that the “critical window” of influence predicted by the BIF corresponds to the window where data removal causes the highest increase in loss, establishing a causal link between our measure and model performance. We improved the description of these experiments in the main text to make these results easier to find.\n* **New Interventions (LLM):** We agree that demonstrating this utility in natural language models strengthens the paper. We have conducted a new experiment based on reviewer suggestions in Appendix D.4. We found that upweighting induction-pattern tokens specifically during the high-influence window identified by the BIF accelerates induction head formation significantly more than upweighting them after this window. This confirms that influence dynamics correctly identify the critical learning periods where the model is most sensitive to specific data. Crucially, this effect cannot be predicted from a static analysis at the end of training. \n\n**2\\. Resolution of Dynamics: Token-wise and Early Training**\n\nTo address concerns that group-level dynamics appeared monotonic or lacked resolution, we added two new analyses:\n\n* **Token-wise Dynamics (Appendix D.2):** We visualize influence trajectories for individual tokens, revealing finer-grained temporal patterns that are partially smoothed out in group-level averages.  \n* **Early Training Dynamics (Appendix D.1.4):** We conducted additional experiments using logarithmically spaced checkpoints to capture rapid changes early in training. This analysis reveals some dynamics that were obscured by the linear spacing in Figure 5\\.\n\n**3\\. Practical Advantage of BIF (Scalability)**\n\nIn response to questions regarding the choice of BIF over other methods (LhWa, CzFc), we have clarified its practical efficiency in Appendix B. Beyond its theoretical advantages (Section 2.2), the BIF is superior for multi-checkpoint trajectory analysis because it bypasses the expensive inverse-Hessian fitting stage required by classical methods like EK-FAC.\n\n### **Summary of Changes**\n\n**New Experimental Appendices**\n\n* **Appendix D.1.4:** Added analysis of early-training dynamics using log-spaced checkpoints (variant of Fig. 5).  \n* **Appendix D.2:** Added per-token influence trajectory examples.  \n* **Appendix D.4:** Added the stagewise intervention experiment on induction formation.\n\n**Clarifications & Corrections**\n\n* **Appendix B:** Added a final paragraph discussing the practical/scalability advantages of BIF over classical IF methods.  \n* **Section 2.2:** Added a forward reference to the scalability discussion in Appendix B.  \n* **Section 5:** Expanded the discussion of unrolling methods (Paragraph 2).  \n* **Figure 3:** Fixed axis labeling.  \n* **SGLD Details:** Fixed the pointer to the RMSProp-preconditioned SGLD details.\n\n**Formatting & Notation**\n\n* Standardized all paragraph and figure headings to sentence case.  \n* Improved notation consistency: $| \\\\to \\\\mid$ for conditionals, $\\\\boldsymbol w$ for vectors, $\\\\mathbf z$ for data points, $H(w)$ for Hessians, and $\\\\mathcal D$ for datasets.  \n* Standardized spacing, usage of en/em-dashes, and fixed double spaces.  \n* Changed quotes around categories to typewriter font (e.g., \\\\texttt{\"dog\"}).  \n* Several other minor improvements."}}, "id": "n7GNsKNpWB", "forum": "8epkNiuAQC", "replyto": "8epkNiuAQC", "signatures": ["ICLR.cc/2026/Conference/Submission20207/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20207/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20207/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763769129570, "cdate": 1763769129570, "tmdate": 1763769129570, "mdate": 1763769129570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues for a position that training data attribution should be done dynamically, rather than statically at the end of the training. Inspired by the singular learning theory, the paper predicts a non-monotonic influence trajectories with sign flips and sharp peaks. The paper then simulates several scenarios to demonstrate their prediction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The stage-wise attribution problem raised by the authors is interesting and well-presented. I am convinced that the same data point might be influential to other data in different ways at different stages of the training."}, "weaknesses": {"value": "While the stage-wise framing is compelling, simply listing the most influential samples is of limited use. To establish real value, attribution should enable actionable interventions that beat a naive one-shot final stage attribution baseline. I strongly suggest that the authors please dedicate more space to what stage-wise attribution concretely enables. As of the current draft, the support is largely heuristic; adding such experiments would substantially strengthen the practical case."}, "questions": {"value": "1. As the authors also brought up in the discussion section, there is a class of unrolling-based attribution methods. Can the authors propose some further discussions about these methods? Does the sign flip somehow imply that these methods are inaccurate, since the middle stage influences are canceling each other?\n2. The current motivation/theoretical analysis is conducted solely based on BIF, which is a relatively new paper on this field. I wonder can the authors provide motivation in a more multifaceted manner?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AieO46ly7J", "forum": "8epkNiuAQC", "replyto": "8epkNiuAQC", "signatures": ["ICLR.cc/2026/Conference/Submission20207/Reviewer_LhWa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20207/Reviewer_LhWa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863938743, "cdate": 1761863938743, "tmdate": 1762933706607, "mdate": 1762933706607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the static view of data attribution, arguing that deep networks' stagewise learning makes fixed influence scores capturing only part of the picture. The authors propose a dynamic \"stagewise\" framework using the Bayesian Influence Function, which predicts that a data point's influence can change over the training process. This theory is validated in both toy linear networks and at-scale Pythia language models, where influence dynamics directly correlate with known developmental milestones, such as the formation of induction heads. The core conclusion is that the timing of measurement (during training) impacts the measured influence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Novel and Significant Conceptual Contribution: The paper's primary strength is the novel connection it forges between Singular Learning Theory and Training Data Attribution. This challenges a foundational assumption in TDA and provides a compelling, theory-backed explanation for why static influence measures are insufficient for deep learning.\n* Strong Theoretical Grounding: The paper does an excellent job motivating its theoretical choices. It clearly explains the failure of classical IFs (reliance on an invertible Hessian) and the suitability of the BIF (Hessian-free, well-defined on degenerate landscapes). The derivation in Section 2.3, which uses the Law of Total Covariance to predict influence peaks at transitions, is elegant and provides a clear, falsifiable prediction.\n* Attempt to Demonstration at Scale: The authors successfully bridge this gap by attempting to demonstrating their predicted phenomena in Pythia LMs, beyond the toy setting. While these results are correlational (linking influence dynamics to known developmental milestones like induction head formation), they are a crucial step in showing this framework is relevant for models we care about."}, "weaknesses": {"value": "* Discrepancy in Observed Dynamics between Toy and Large-Scale Models: A significant discrepancy arises between the compelling theoretical predictions validated in the toy model and the empirical results from the large-scale language model experiments. The core theoretical argument hinges on influence being a highly dynamic quantity, subject to non-monotonic changes and sign-flips (as clearly demonstrated in Fig. 3). However, the influence dynamics observed in the Pythia models (Fig. 5), while temporally staged, appear to be largely monotonic once they become non-zero.\n\nThis observation materially weakens the paper's central critique of static, endpoint-based attribution or simpler trajectory-aggregation methods (e.g., TracIn). If the influence of key data groups simply increases monotonically after a specific developmental stage, then traditional attribution methods applied at or aggregated near the end of training would likely still provide a reasonable approximation of data importance. The practical necessity of the authors' far more complex, dynamically-sampled framework is therefore less evident in the very setting (LLMs) it purports to be essential for.\n\n* Lack of Demonstrated Practical Utility or Actionable Insights: The paper compellingly argues that when a data point exerts its influence is a critical, and previously overlooked, dimension of attribution. However, the analysis remains at an observational level, and the practical utility of this framework is not demonstrated. It is unclear how these findings could inform model development, for instance, to train more capable or efficient models.\n\nA powerful validation of the framework's utility would be to move from this observational analysis to an interventional one. For example, could the authors leverage their findings to design a dynamic data curriculum? If, as the results suggest, the influence of \"induction pattern\" tokens becomes active only during a specific training phase, could dynamically up-weighting these examples during (or just before) that critical phase lead to demonstrably better model performance on induction-related tasks or faster convergence? Without such a demonstration connecting stagewise influence to actionable training strategies, the proposed method, while theoretically interesting, risks being perceived as a diagnostic tool that lacks a clear feedback loop into practical model improvement."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qbJZhDQgrY", "forum": "8epkNiuAQC", "replyto": "8epkNiuAQC", "signatures": ["ICLR.cc/2026/Conference/Submission20207/Reviewer_Z984"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20207/Reviewer_Z984"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762491649717, "cdate": 1762491649717, "tmdate": 1762933706224, "mdate": 1762933706224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}