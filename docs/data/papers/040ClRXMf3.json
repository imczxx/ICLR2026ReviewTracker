{"id": "040ClRXMf3", "number": 19723, "cdate": 1758298867680, "mdate": 1759897022892, "content": {"title": "Provably Explaining Neural Additive Models", "abstract": "Despite significant progress in post-hoc explanation methods for neural networks, many remain heuristic and lack provable guarantees. A key approach for obtaining explanations with provable guarantees is by identifying a *(globally) cardinal-minimal* subset of input features which by itself is *provably sufficient* to determine the prediction. However, for standard neural networks, this task is often computationally infeasible, as it demands a worst-case *exponential* number of verification queries in the number of input features, each of which is NP-hard. In this work, we show that for Neural Additive Models (NAMs), a recent and more interpretable neural network family, we can *efficiently* generate explanations with such guarantees. We present a new model-specific algorithm for NAMs that generates provably (globally) cardinal-minimal explanations using only a *logarithmic* number of verification queries in the number of input features, after a parallelized preprocessing step with logarithmic runtime in the required precision is applied to each small univariate NAM component. Our algorithm not only makes the task of obtaining (globally) cardinal minimal explanations feasible, but even outperforms existing algorithms designed to find *(locally) subset-minimal* explanations -- which may be larger and less informative but easier to compute -- despite our algorithm solving a much more difficult task. Our experiments demonstrate that, compared to previous algorithms, our approach provides provably smaller explanations than existing works and substantially reduces the computation time. Moreover, we show that our generated provable explanations offer benefits that are unattainable by standard sampling-based techniques typically used to interpret NAMs.", "tldr": "Our approach constructs provably sufficient and (globally) cardinal-minimal explanations for neural additive models with improved runtime complexity.", "keywords": ["explainability", "XAI", "explainable AI", "formal verification", "sufficient explanations"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d5a73d9cf5e02a90d26e33e9057769ff66ff64fa.pdf", "supplementary_material": "/attachment/688a5ff66ccb15d28a06f568b0f04b60f4413e61.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new algorithm to extract cardinal-minimal sufficient explanations for Neural Additive Models (NAMs).\nIt does so by exploiting key design choices of NAMs, showing how this family of models supports explanations with guarantees.\n\nThis is achieved as follows. First, the paper introduces a method to rank features based on how much they influence the final prediction. Then, after this ranking is obtained, an algorithm is discussed to exploit this order to efficiently explore which features to remove from the current sufficient explanation until a cardinal-minimal explanation is obtained."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Making minimal sufficient explanations scaling is a timely and valuable research direction.\n\n- The proposed idea is simple but effective\n\n- The paper flows well\n\n- The paper is formal and precise in its claims"}, "weaknesses": {"value": "### Proof of Proposition 3 is unclear\n\n- **W1:** In line 842, the authors write *Let 1 ≤ l ≤ n represent the last feature added to $S$ in line ?? of Alg. 3*. However, I do not understand what this statement refers to, as in no part of Alg. 3 features are added to $S$.\n\n- **W2:** In line 843, the authors write *Then, for $S^{'}= S \\setminus \\\\{ l \\\\}$, it follows that: $suff(f, x, S, \\epsilon)$ does not hold true, implying that $S^{'}$ is not a sufficient explanation*, but it is unclear why this is true. I feel there could be some writing issues in this paragraph that prevent me from understanding the proof.\n\n\n\n### Definition of Sufficient explanation\n\n- **W3:** Definition 1 expresses the condition $\\\\forall \\\\tilde{x} \\\\in B_p^{\\\\epsilon_p}(x)$, where $B_p^{\\\\epsilon_p}(x) = \\\\{\\\\tilde{x} \\\\in R^{n} | ...\\\\}$. However, this condition seems to be impossible to satisfy, as real numbers are dense and therefore would require to run the sufficiency check for an infinite number of $\\\\tilde{x}$.\n\n\n- **W4:** The set of allowed perturbations is defined over an $\\\\epsilon$-ball around $x$. Nonetheless, defining a fixed $\\\\epsilon$ for every feature could hinder some feature-specific behaviors induced by, for example, different magnitudes of features. For example, let us consider a binary feature representing the gender of a person (0=male, 1=female), and a function $f_i(x_i)$ shaped as: $f_i(x_i)=100$ if $x_i < 0.5$, and  $f_i(x_i)=-100$ if $x_i >= 0.5$. Then a value of $\\\\epsilon=0.1$ will not be able to check for the sufficiency of the explanation when the gender of the person is switched from male to female, as this feature will be assigned zero importance even if its true impact is actually very high.\n\n\n### Figure 1 unclear\n\n- **W5:** Figure 1 is difficult to interpret, and its description is not self-contained. I suggest improving its description. For example, the part on *users might wrongly believe that only feature 1 yields negative outputs, while feature 2 can also flip the classification* is non-trivial to a non-expert reader.\n\n\n**Minors**\n\n- Propositions 2 and 3 are in reverse order in the Appendix.\n\n- The link between lines 46 and 47 is a bit unclear. In fact, while the first sentence in the paragraph is referred to sufficient explanations, the following sentence refers only to cardinal-minimal explanations, and not cardinal-minimal sufficient explanations, making the context of the sentence less clear.\n\n- In line 52, the authors write: *Consequently, existing methods focus on (locally) subsetminimal explanations which are typically suboptimal in size, potentially large, and thus less informative than their globally minimal counterparts*, which is, however, not clear why this is true without further context. I would recommend adding an explanation of why this is the case.\n\n\n- I personally find the wording local vs global sufficient explanation a bit misleading, as this could be confused with standard notions of local (instance-level) and global (model-level) explanations. I believe cardinal-minimal and subset-minimal are already discriminative enough and may not need further quantifications."}, "questions": {"value": "**Q1:** In line 2 of Alg. 2, how is the operation $\\\\in$ implemented? Is this a uniform random sampling?\n\n**Q2:** How is the proposed algorithm applicable to NAMs for data types different from tabular data, like images and graphs ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MthqrzzFcv", "forum": "040ClRXMf3", "replyto": "040ClRXMf3", "signatures": ["ICLR.cc/2026/Conference/Submission19723/Reviewer_uiyA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19723/Reviewer_uiyA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760718178623, "cdate": 1760718178623, "tmdate": 1762931560659, "mdate": 1762931560659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel algorithm for computing provably cardinality-minimal explanations for Neural Additive Models (NAMs). The authors focus on post-hoc, per-instance explanations: given a trained NAM f and an input x, they seek to compute a subset of features S \\subseteq [n]that is sufficient to guarantee the same prediction under bounded perturbations of the remaining features (an\n\\epsilon-ball). Among all sufficient subsets, the goal is to find one of minimum cardinality (the global optimum).\n\nThe paper provides a novel contribution to the state-of-the-art in the broad area of explainability with provable guarantees (in this case, minimality). The paper focuses on NAMs, which to the best of my knowledge it is still a Still a niche but growing area in the interpretability subfield. They are Not widely used in industry production pipelines yet. but research interest persists. In fact,  (NAMs) occupy an interesting middle ground in machine learning — they’re not mainstream, but they are important in specific contexts where interpretability and nonlinear modelling both matter. Their main limitation is that in a pure NAM, features don’t interact directly because the model assumes additivity. This means that the effect of each feature x_i on the output y is independent of any other feature x_j, which can be a strong limitation in some practical settings.\n\nThe proposed algorithm proceeds in two stages. In the first stage each univariate subnetwork f_i(x_i) is verified independently to estimate its influence on the model’s decision. This is done via parallelised binary search over feature importance intervals. In Stage 2, after sorting features by importance, a binary search identifies the globally cardinal-minimal sufficient subset of features that provably determines the model’s prediction. This reduces complexity from exponentially many calls to the network to logarithmically many.\n\nExperiments on standard tabular benchmarks demonstrate feasibility and show smaller, faster provable explanations than prior methods; sampling-based visualisations were also shown to be unreliable in some cases, whereas the proposed method always produces verifiably sufficient explanations.\n\nOverall, I am supportive of this paper. It makes a meaningful and well-justified contribution to formal explainability by showing that NAMs enable efficient computation of globally minimal sufficient explanations -- something previously infeasible for general neural networks. With minor revisions, I feel that this paper is a valuable contribution to the state of the art."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths.\n\n(1) Addresses an important formal-XAI problem (provable minimal explanations) and achieves a stronger guarantee (global cardinality minimality) than most prior work for neural networks.\n\n(2) Elegant exploitation of the additive NAM structure to reduce verification complexity.\n\n(3) Clear algorithmic presentation with theoretical propositions and proofs in the appendix.\n\n(4) Empirical results convincingly illustrate both efficiency and the need for provable guarantees."}, "weaknesses": {"value": "Limitations / concerns.\n\n(1) The guarantees rely on an exact, sound verifier and on refining intervals until importance bounds separate. In practice, verifier soundness, numerical issues, or timeouts can undermine the provable guarantees; the authors acknowledge this but more discussion of practical mitigations (timeouts, numerical tolerances) would be useful.\n\n(2) The attractive complexity (logarithmic verifier calls) is obtained assuming parallel refinement across features. The number of wall-clock verifier calls depends on available parallelism; the paper states the complexity in terms of ρ processors. If you cannot parallelise, wall-clock time will be higher (though still fewer full-model queries than naive approaches). \n\n(3) The method requires refining per-feature bounds until features’ importance intervals separate (so a total order exists). If two features have extremely close effects, the required precision may be tiny, increasing verification cost. The complexity bound explicitly depends on that precision factor. The propositions report complexity in terms of that separation quantity.\n\n(4) The related work is largely comprehensive, but the paper omits a relevant reference on computing cardinality-minimal explanations for a different class of networks (monotonic fully-connected networks). That work should be cited and briefly contrasted. While that setting differs from NAMs (monotonic FCNs vs. additive univariate subnetworks), the goals and guarantees are similar; I recommend citing and briefly contrasting that work to highlight how different architectural restrictions enable tractable exact explanations.\n \n@inproceedings{DBLP:conf/ijcai/HarzliG023,\n  author       = {Ouns El Harzli and Bernardo Cuenca Grau and Ian Horrocks},\n  title        = {Cardinality-Minimal Explanations for Monotonic Neural Networks},\n  booktitle    = {Proceedings of the Thirty-Second International Joint Conference on\n                  Artificial Intelligence (IJCAI) 2023},\n  pages        = {3677--3685},\n  publisher    = {ijcai.org},\n  year         = {2023},\n  doi          = {10.24963/IJCAI.2023/409}\n}"}, "questions": {"value": "Suggestions.\n\n(1) Expand on practical verifier settings, in particular timeout and floating/rounding issues.\n\n(2) Add an ablation that shows performance as a function of the separation parameter or a synthetic example with near-identical features.\n\n(3) Cite and comment on the work by El Harzli et al. at IJCAI-2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vX0lZAngLb", "forum": "040ClRXMf3", "replyto": "040ClRXMf3", "signatures": ["ICLR.cc/2026/Conference/Submission19723/Reviewer_mvRn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19723/Reviewer_mvRn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760981176265, "cdate": 1760981176265, "tmdate": 1762931560247, "mdate": 1762931560247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on explainable artificial intelligence and aims to provide concise explanations for the predictions made by Neural Additive Models (NAMs). The primary issue addressed in this study is as follows: given a classifier $ f $ represented by a NAM, an input data instance $ x $ that requires an explanation, and a ball $ B $ centered at $ x $, the goal is to identify a feature subset $ S $ of the minimum size. This subset must ensure that for every instance $ z $ within the ball $ B $, if the values of $ z $ and $ x $, restricted to the features in $ S $, are indistinguishable, then the classifications made by $ f $ for both $ z $ and $ x $ are the same. Such an explanation $ S $ is referred to as a (ball-restricted) minimum-size abductive explanation or a minimum-size sufficient reason.\n\nTo address this problem, the authors propose a two-stage method. In the first stage, the univariate functions $ f_i $ are sorted based on their importance intervals. In the second stage, a minimal-size explanation $ S $ is derived using a greedy approach. The paper includes formal proofs for the correctness and complexity of this method, and it presents comparative experiments conducted on four different datasets that support the theoretical findings."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**S1.** The problem of computing minimum-size sufficient reasons is $\\Sigma_p^2$-hard for neural networks (Barcelo et al., 2020). Therefore, it is reasonable to explore simpler model classes that can lead to efficient algorithms with a reasonable number of calls to an NP oracle. In this context, Neural Additive Models are a rational choice.\n\n**S2.** Experimental results across four datasets demonstrate that concise explanations can be generated in a reasonable amount of time."}, "weaknesses": {"value": "**W1.** The computational complexity of finding minimum-size sufficient reasons for the class of NAMs has not been proven. Identifying the corresponding complexity class (P, NP, $\\Sigma_p^2$) is crucial for justifying the overall interest in this approach.\n\n**W2.** Barcelo et al. (2020) have already demonstrated that the problem is solvable in polynomial time for linear threshold functions. Essentially, the theoretical results presented in this study are just an extension of their previous work, with the primary innovation being the computation of an ordering of non-intersecting intervals. I am not entirely convinced that this alone is sufficient for publication in ICRL.\n\n**W3.** The correctness and runtime complexity of the main algorithm used to compute a total ordering of non-intersecting intervals (Alg. 2) appear to be flawed. In particular, the runtime complexity tends toward infinity as the value of $\\xi$ approaches zero.\n\n**W4.** The clarity of the paper could be improved. Currently, there are several issues related to notation. Additionally, the organization of the paper could benefit from the inclusion of some proofs (or at least sketches of proofs) for the most significant results."}, "questions": {"value": "The following comments and questions are related to the aforementioned weaknesses.\n\n**C1.** What is the computational complexity of finding minimum-size sufficient reasons for the class of NAMs? Is this problem in NP? In other words, does the decision version of the problem have a polynomial-time certificate? Additionally, is the problem NP-hard, or is it even more difficult?\n\n**C2.** As mentioned earlier, Barcelo et al. (2020) have demonstrated that the problem known as \"Minimum Sufficient Reason\" (MSR) is in P for the class of linear threshold functions (LTs). Essentially, MSR is the decision version of the problem being explored in this study, where the class of LTs is replaced by the class of NAMs, and the instance space is defined as a ball centered at $x$. For the class of LTs, Barcelo et al. advocate a strategy that involves sorting each feature $x_i$ based on its importance, which is measured by the weight $w_i$ and the class $f(x)$. The next step is to determine whether the top $k$ features in this ordered list provide a sufficient explanation for $x$ and $f$.\n\nIf I’m not mistaken, this study adopts a similar strategy, with the primary challenge being the computation of the minimum value of each univariate function $f_i$ over a specified ball centered at $x$. Therefore, it is important to discuss the overall approach in comparison to Barcelo et al.’s strategy, as well as to elaborate on the concept of using \"non-intersecting intervals\" to approximate feature importances.\n\n**C3.** As mentioned earlier, the primary innovation of this study is to compute an ordering of “non-intersecting” intervals, from which the final explanation is derived. From this perspective, it is essential to demonstrate that the main algorithm (Alg. 2) is correct. Assume without loss of generality that $f(x) = 1$ with $x = (1,1)$ and consider two univariate functions, $f_1$ and $f_2$, where the minimum of $f_1$ (over the ball centered at $x$) is slightly below the minimum of $f_2$.  Does the algorithm guarantee that, at the end of the final iteration, the output intervals $(l_1, u_1)$ and $(l_2, u_2)$ satisfy the property $u_1 < l_2$ (indicating that $x_1$ precedes $x_2$ in the final ordering)? Additionally, how many iterations of the main loop are required to derive such intervals?\n\nRegarding the last question, the complexity analysis appears to be flawed. Again, consider the two univariate functions $f_1$ and $f_2$. Here, assume that $f_1(z) = f_2(z) = w$, where $w$ is a constant scalar. According to Line 12 of Algorithm 2, the algorithm terminates when $\\Delta u_1 \\geq \\Delta l_2$. However, if $\\Delta u_1 = \\Delta l_2$, we find that $\\xi_1 = 0$. This situation suggests that the runtime complexity might be infinite. Therefore, I am unconvinced that $\\xi$ is the appropriate parameter for the complexity analysis of the algorithm, particularly when the global minima of different univariate functions are identical.\n\n**C4.** The paper currently has several clarity issues that make it difficult to follow. For instance, the $i$th entry of a vector $\\mathbf{x}$ is sometimes denoted as $\\mathbf x_{(i)}$ (as defined in Line 93) and at other times as $x_{(i)}$ (in Section 4.1). Additionally, the explanation is occasionally referred to as $\\mathcal{S}$ (in the main paper) and other times as $S$ (in Lemmas 2 and 3). Furthermore, the center of Figure 1 appears to be misplaced, and there are missing references (indicated by symbol ??) in the Appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jO4h06yElo", "forum": "040ClRXMf3", "replyto": "040ClRXMf3", "signatures": ["ICLR.cc/2026/Conference/Submission19723/Reviewer_f9gy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19723/Reviewer_f9gy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761162117835, "cdate": 1761162117835, "tmdate": 1762931559713, "mdate": 1762931559713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A computationally-efficient, novel method to compute explanations with provable guarantees for Neural Additive Models (NAMs). The explanations are guaranteed to be the smallest in size, globally. The method claims to be efficient in generating such certified explanations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Certifying explanations is a key open research direction: the research problem highlighted by the authors is well-defined and relevant to the ML community. To the best of my knowledge this is the first approach specifically designed for NAMs \n- Logarithmic number of verification queries required is a step forward compared to baselines. Formal complexity analysis included.\n- Convincing experimental campaign, aligned with best-practice in the community. Performance of the approach shows interesting increase over baselines, both in size reduction and time required to generate an explanation (which is a welcome contribution).\n- Paper is clear and the authors train of thought is well articulated."}, "weaknesses": {"value": "- Limited impact, as the proposed method is applicable to NAMs only by design.\n- Although evaluation is convincing, the adopted proxy quality metric for explanations is size, where smaller = better. A human assessment of the perceived quality of the resulting explanations would have made the work stronger - although I acknowledge this is a significant addition to the work (i.e. yes, consensus in the literature is the smaller the better, but what if the resulting explanations are *too* small, and therefore too course grained to capture a use case subtleties?)\n- The paper could use a running example across the paper section, to further clarify some of the key concepts (e.g. globally and locally cardinal-minimal feature subset). The HELOC example in Figure 1 is popular enough to be a good candidate."}, "questions": {"value": "- Does the method support discrete input features (I suppose so, could you confirm)?\n- What is the impact of perturbation radius $\\epsilon_p$? \n- Has the local subset minimality been introduced in the narrative exclusively to ensure a feasible experimental campaign while comparing against naive baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EWw1viy5FE", "forum": "040ClRXMf3", "replyto": "040ClRXMf3", "signatures": ["ICLR.cc/2026/Conference/Submission19723/Reviewer_Bqyn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19723/Reviewer_Bqyn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842464481, "cdate": 1761842464481, "tmdate": 1762931559264, "mdate": 1762931559264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}