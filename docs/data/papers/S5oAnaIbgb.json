{"id": "S5oAnaIbgb", "number": 15387, "cdate": 1758250854307, "mdate": 1763723710976, "content": {"title": "Gradient Descent Dynamics of Rank-One Matrix Denoising", "abstract": "Matrix denoising is a crucial component in machine learning, offering valuable insights into the behavior of learning algorithms  (Bishop and Nasrabadi, 2006). This paper focuses on the rectangular matrix denoising problem, which involves estimating the left and right singular vectors of a rank-one matrix that is corrupted by additive noise. Traditional algorithms for this problem often exhibit high computational complexity, leading to the widespread use of gradient descent (GD)-based estimation methods with a quadratic cost function. However, the learning dynamics of these GD-based methods, particularly the analytical solutions that describe their exact trajectories, have been largely overlooked in existing literature. To fill this gap, we investigate the learning dynamics in detail, providing convergence proofs and asymptotic analysis. By leveraging tools from large random matrix theory, we derive a closed-form solution for the learning dynamics, characterized by the inner products of the estimates and the ground truth vectors. We rigorously prove the almost sure convergence of these dynamics as the signal dimensions tend to infinity. Additionally, we analyze the asymptotic behavior of the learning dynamics in the large-time limit, which aligns with the well-known Baik-Ben Arous-Péchée phase transition phenomenon n (Baik et al., 2005). Experimental results support our theoretical findings, demonstrating that when the signal-to-noise ratio (SNR) surpasses a critical threshold, learning converges rapidly from an initial value close to the stationary point. In contrast, estimation becomes infeasible when the ratio of the inner products between the initial left and right vectors and their corresponding ground truth vectors reaches a specific value, which depends on both the SNR and the data dimensions.", "tldr": "Derive a closed-form solution for the learning dynamics of GD-based rank-one matrix denoising and reveal the BBP transition in the large-time limit.", "keywords": ["Random Matrix Theory", "High-Dimensional Statistics", "Matrix Denoising", "Gradient Flow"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a733a7b7a7939b834d5c127fd03851878a3f1339.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the rank-one Jonstone's spiked model in matrix denoising problem within random matrix theorem (RMT) scenerio. The main contributions are two theorems. In theorem 1, they derive a closed-form deterministic approximation for the inner products between the learned vectors and the ground truth when ratio $\\frac{p}{n}\\to c>$. In theorem 2, they show that $c^{1/4}$ is the critical threshold for the gradient flow estimation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Use random matrix theory (RMT) derive a closed-form solution for the learning dynamics of matrix denoising problem. By RMT, naturally extend the matrix denoising problem to high dimensional scenario.\n- The theorems and derivations are solid, and for example, the complex but precise expression in Theorem 1 is derived.\n- The problem and assumptions are stated clearly.\n- Provide a more comprehensive understanding of the dynamics of gradient-based learning in high-dimensional matrix problems"}, "weaknesses": {"value": "- Lack of explanation of the application of matrix denoising in the random matrix scenario, i.e. $\\frac{p}{n}\\to c$ with $p, n \\to \\infty$.\n- The statement about computational complexity is too vague, like \"the  complexity is affordable\" on line 220 and \"We note that $\\hat{t}(\\alpha_u,\\alpha_v)$ can be efficiently computed by standard numerical methods.\" on line 244.\n- On line 244, need more details of the so called \"standard numerical methods\".\n- On line 42, \"Extensive research has shown that\" lacks reference.\n- The theoretical work on which the article is based is very classic, and there is a lack of reference from recent new theoretical work.\n- Lacks explanation of Riemannian gradient operator on line 144.\n- In experiments, only one set of $(p,n)$ is tested, need more testing sets.\n- In experients, the critical threshold is about 0.93, but the SNR is set to 0.3 and 1.5, which are far away from the threshold. More attempt on SNR need to be tested, especially the SNR near the threshold.- The ground truth is too trival."}, "questions": {"value": "In Remark 1, the assumption of knowing the SNR $\\lambda$ is practical in reality?In experiment, what will the results change if $p$ and $n$ are not that large, like $p=20$, i.e. beyond the RMT scenario."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4pBfautkKM", "forum": "S5oAnaIbgb", "replyto": "S5oAnaIbgb", "signatures": ["ICLR.cc/2026/Conference/Submission15387/Reviewer_he9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15387/Reviewer_he9F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642779029, "cdate": 1761642779029, "tmdate": 1762925670144, "mdate": 1762925670144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the problem of rank-one matrix denoising, focusing on the gradient descent dynamics underlying this process. The main contribution of the paper lies in two theoretical results. The first theorem establishes a deterministic approximation for q_u and q_v, two widely used metrics that measure the alignment between the ground truth and the estimated components. The second theorem characterizes the asymptotic behavior of these deterministic approximations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well written, well structured, and clearly explained. The theoretical analysis is rigorous, and the overall presentation is easy to follow. I find the work interesting and relevant to the study of optimization dynamics in low-rank estimation problems."}, "weaknesses": {"value": "My only minor concern relates to the clarity of the experimental results. In Figure 2, which illustrates the effect of the critical SNR threshold, it is visually difficult to distinguish the different curves, especially in the middle subfigure. I understand that it is challenging to convey a large amount of information within a single figure, but since this result serves as an important validation of the theoretical findings, it should be presented more clearly to enhance its impact."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aWEbRDTASc", "forum": "S5oAnaIbgb", "replyto": "S5oAnaIbgb", "signatures": ["ICLR.cc/2026/Conference/Submission15387/Reviewer_RZPg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15387/Reviewer_RZPg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751183028, "cdate": 1761751183028, "tmdate": 1762925669344, "mdate": 1762925669344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the gradient-flow dynamics of the rectangular rank-one spiked matrix denoising problem in the high-dimensional limit.\nUsing tools from random matrix theory and Laplace transforms over the Marčenko–Pastur law, the authors derive explicit deterministic equations describing the evolution of the overlaps between the estimated and true directions.\nThey prove convergence to a deterministic limit, identify a BBP-type phase transition at the expected signal-to-noise threshold, and show that the limiting alignment carries the same sign as the initialization (“signed BBP”).\nA kernel-based argument establishes existence and uniqueness of the solution.\nExperiments nicely confirm the theoretical predictions.\nOverall, the work extends the analysis of Bodin and Macris (2021), which treated the symmetric Wigner case, to the rectangular Wishart setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The results are technically sound and contribute to the understanding of optimization dynamics in high dimension.Extending the analysis from the symmetric to the rectangular setting is nontrivial and closes a natural gap in the literature. Conceretly:\n- Provides explicit, analytic trajectories for gradient flow in the rectangular spiked model.\n- The results are rigorous and match empirical observations very well.\n- The “signed BBP” effect offers a clear dynamical interpretation of initialization dependence.\n- Writing and figures are clear; the paper is enjoyable to read.\n- Strengthens the theoretical link between random matrix theory and learning dynamics"}, "weaknesses": {"value": "- The work feels somewhat incremental relative to Bodin & Macris (2021); the main novelty lies in adapting the approach to the rectangular case.\n- The “signed BBP” result, while nicely explained, is largely an expected property of continuous gradient flow.\n- Experiments are limited to confirming the theory in the simplest setting; discrete-time or noisy gradient dynamics are not discussed.\n- The discussion of related literature could be more complete and precise."}, "questions": {"value": "I) Clarify the exact novelty relative to previous analyses.\nThe paper’s results are very close in spirit to Bodin & Macris (2021, arXiv:2105.12257), which already provided deterministic gradient-flow equations and asymptotic limits in the symmetric spiked Wigner setting.\nIt would help to spell out precisely which technical steps differ in the rectangular case and which parts of the proof had to be redone — for instance, changes in the resolvent structure, contour integration, or Laplace-transform kernel.\nAre there specific mathematical obstacles that make the rectangular case substantially more difficult, or is it mainly a matter of replacing the semicircular law with the Marčenko–Pastur one?\nA short paragraph clearly highlighting these differences would make the contribution much clearer.\n\nII) Connection to the broader literature surveyed by Macris.\nThe “Related Work” section of Bodin & Macris (2021) already offers a remarkably complete overview of the theoretical ecosystem surrounding gradient descent, AMP, and high-dimensional inference. Many of those references are directly relevant here.\nThe Bayesian analyses of the spiked Wigner and tensor models (Korada & Macris 2009; Barbier et al. 2016; Lelarge & Miolane 2018; Lesieur et al. 2017; Perry et al. 2020) provide precise information-theoretic benchmarks in the form of mutual information and MMSE.\nThe dynamical behavior of AMP and the existence of computational-to-statistical gaps (Barbier et al. 2016; Lesieur et al. 2017) are also well understood.\nGiven this context, could the authors explain what new qualitative insight is gained from their explicit time-evolution formulas?\nFor instance, does the analytic expression for the transient trajectories reveal any phenomenon that is not already implicit in the AMP state evolution or in the energy-landscape picture?\n\nIII) Relation to the matrix–tensor and Langevin dynamics literature.\nMacris notes that recent works (Sarao Mannelli et al., 2019; 2020) analyzed the optimization of mixed matrix–tensor inference problems using integro-differential Cugliandolo–Kurchan (CSHCK) equations — a fully dynamic, spin-glass-inspired formalism.\nIt would be interesting for the authors to comment on how their much simpler kernel/ODE formulation compares conceptually to those dynamical equations.\nDoes the present approach capture similar information about the saddle structure or convergence rates, but in a more tractable regime?\nOr is it strictly a deterministic “mean-field” limit without the stochastic thermal components appearing in the CSHCK-type equations?\n\nIV) Discrete gradient descent versus continuous gradient flow.\nSince the work focuses entirely on continuous-time dynamics, a natural question is whether these results extend (even approximately) to discrete gradient descent with a finite learning rate.\nPrevious works, such as Lee et al. (2016) and Ge et al. (2017), established convergence for discrete GD under the “strict saddle” property, while Saxe et al. (2013) and Mei & Montanari (2019) studied learning-rate effects in linear and nonlinear models.\nCould the authors discuss whether the signed-BBP phenomenon and transient behavior persist under discrete updates?\nEven a conjectural statement or some preliminary numerical evidence would be welcome.\n\nV) Connections to the energy landscape and spin-glass literature.\nMacris also situates the work within the broader context of non-convex optimization in random energy landscapes (Subag & Zeitouni 2017; Ros et al. 2019; Auffinger et al. 2013).\nIt would be valuable to comment on whether the present gradient flow can be interpreted as traversing a spin-glass-like energy surface with a small number of global minima and exponentially many saddles.\nDoes the deterministic flow derived here correspond to the typical trajectory that avoids these saddles in the large-n limit?\nSuch a discussion would help bridge the current mathematical analysis with the well-developed physical intuition from statistical mechanics.\n\nOverall, the paper is technically clean and the results are credible, but the authors could significantly increase its impact by situating it more deeply within the broad theoretical lineage summarized by Macris — including AMP and Bayesian limits, non-convex low-rank recovery, spin-glass Langevin dynamics, and deterministic gradient-flow analyses.\nClarifying what the present framework adds to that landscape, and where it could go next, would make the work more compelling for the ICLR audience."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UEPHWT1bHp", "forum": "S5oAnaIbgb", "replyto": "S5oAnaIbgb", "signatures": ["ICLR.cc/2026/Conference/Submission15387/Reviewer_YymG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15387/Reviewer_YymG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990006741, "cdate": 1761990006741, "tmdate": 1762925668694, "mdate": 1762925668694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes the statistical properties of gradient flow (as a proxy for gradient descent) for rank-one matrix denoising under a deformed Wishart model (rectangular matrices) with noise that has i.i.d. entries and . It derives a deterministic term for the limit of the inner products between limiting singular vector estimates and ground truth singular vectors in the asymptotic limit of matrix dimensions becoming infinite with fixed relation fraction. As an implication of these results, the authors are able to relatively accurately predict the behavior of gradient flow depending on the problem's signal-to-noise ratio (SNR) threshold akin to the BBP [Baik, Ben Arous, Péché 2005] phase transition. The results also can be used to quantify the dependence of the dynamics on initialization (value of $\\alpha_u$/$\\alpha_v$) and reasonable stopping times can be theoretically derived (see Remark 1 and Remark 2). From a technical perspective, the results lean on the analysis of [Bodin & Macris 2021], who have showed similar results for the symmetric case. Simulations are presented that substantiate the qualitative accuracy of the asymptotic analysis in the finite sample / matrix dimension case of $p$ and $n$ fixed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The analysis presented in the paper seems to be new and studies a foundational problem in high-dimensional statistics / linear algebra, the behavior of singular value decomposition under the influence of noise in the case of rectangular matrices. The noise model is rather general, which is positive. It is of interest that the gradient flow dynamics more or less matches information theoretical phase transitions that are intrinsic to the problem. \nWhile carefully checking many proofs in the appendix was beyond my abilities in the allocated time-frame as a reviewer, the results are plausible from a perspective of a reviewer who is familiar with tools for analyzing non-asymptotic high-dimensional problems.\nBeyond covering the asymmetric case, some assumptions are weaker than in the related paper [Bodin & Macris 2021], such as the finite fourth moment assumption (as a opposed to assuming existence of all moments)."}, "weaknesses": {"value": "A fundamental weakness of the work is that it applies only in the high-dimensional limit of $\\lim_{p, n \\to \\infty} p/n = c$, which is in contrast to many analyses of iterative algorithms in machine learning. Related to this issue, it can be pointed out that the title containing \"Gradient Descent\" is to a certain extent a misnomer as gradient flow, which is less relevant than gradient descent in practice in machine learning, is being analyzed. Thus, a lack of treatment of the discrete-time gradient descent method is a weakness of the paper given the framing of the paper.\nA more unified discussion pointing out the differences and similarities between a power method algorithm for computing the leading singular vector pair and the presented algorithm would also have been insightful - I somewhat disagree with the framing that \"SVD is intractable\" as it is clear that a reasonable algorithm for the problem would involve a partial SVD implemented via randomized techniques [see, e.g., Martinsson, Tropp 2020].\nFinally, it can be be pointed out that, while the asymmetric case being more challenging, the analyses / simulations presented are relatively close aligned to the ones of [Bodin, Macris 2021]."}, "questions": {"value": "1. In lines 141-145, it is mentioned that $\\operatorname{grad](\\cdot)$ is a \"Riemannian gradient operator, which enforces the unit norm constraint\". However, I do not see that the update equation of (4) enforces such a constraint. In some sense, this is statement is incompatible with the framework of Riemannian optimization as the Riemannian gradient lives in the tangent space onto Riemannian manifold and requires a retraction back onto the manifold (here, enforcing unit-norm vectors) to enforce the constraints.\nCan you clarify or correct this discussion? In particular, how does your studied gradient flow algorithm enforce the unit norm constraints throughout its flow?\n\n2. What are the limitations of the presented analysis for higher-rank ground-truths? Where does your current analysis fail to go through?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RhRexnAuEh", "forum": "S5oAnaIbgb", "replyto": "S5oAnaIbgb", "signatures": ["ICLR.cc/2026/Conference/Submission15387/Reviewer_DPd9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15387/Reviewer_DPd9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762267364026, "cdate": 1762267364026, "tmdate": 1762925668249, "mdate": 1762925668249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}