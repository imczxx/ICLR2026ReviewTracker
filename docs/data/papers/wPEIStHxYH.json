{"id": "wPEIStHxYH", "number": 23819, "cdate": 1758348840032, "mdate": 1763736529688, "content": {"title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning", "abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's rich priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected total cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5\\% and 66.9\\% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks.", "tldr": "Cosmos Policy is a simple framework for adapting a large pretrained video model (Cosmos-Predict2) into a state-of-the-art robot policy that can generate actions, future states, and values and plan action trajectories with high success rates.", "keywords": ["world models", "robotics", "manipulation", "model-based planning", "imitation learning", "video generation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1cd3cf60eec2620e5d49608af45e2db2dee54bf5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a Cosmos policy framework that is built upon the Cosmos pre-trained video model. To add required modalities to the pre-trained model, this paper constructs additional latent frames containing the information. Experiments are conducted on the Libero tasks and the real Aloha task, where the proposed policy obtains better performance on average."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes fine-tuning a pre-trained video model to a policy that can execute and interact with the real environment. The idea and direction are very promising and interesting for the community.\n- Instead of adding an additional network for integrating modalities like actions, the paper constructs latent frames to better leverage the prior learned in the pre-trained model. While this idea is not novel, it is valuable for robotic policy learning.\n- Experiments include both simulation environments and real tasks."}, "weaknesses": {"value": "- While I like the research topic studied in this paper, the proposed method is not novel and not elegant enough. For example, an executable policy requires high frequency, which makes small policy networks and action chunking useful. However, this paper requires the Cosmos pre-trained model to output executable, low-level actions directly, which can cause high latency and make it difficult to adapt to other tasks like locomotion. The second point is that the latent frames contain duplicate information, which can cause learning inefficiency. It is not necessary for the robot's proprio states and actions to occupy several latent frames. Overall, the proposed fine-tuning pipeline is not easy to reproduce, and still needs to be improved.\n- While the proposed policy gets the best success rates on average, its performance on o.o.d. tasks is not demonstrated clearly. Can the policy generalize to different tasks with the same action space? Can the policy generalize to different backgrounds and unseen objects? Since the policy is built upon a pre-trained model, I expect it to generalize better.\n- Missing citations related to fine-tuning the video prediction model to an executable policy, e.g., Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training, NeurIPS 2024. \n- Missing Reproducibility statement."}, "questions": {"value": "- Can the proposed framework also work well for other pre-trained video models, especially considering different architectures? \n- Please address my concerns in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "19N2OwMtco", "forum": "wPEIStHxYH", "replyto": "wPEIStHxYH", "signatures": ["ICLR.cc/2026/Conference/Submission23819/Reviewer_pF5s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23819/Reviewer_pF5s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964475479, "cdate": 1761964475479, "tmdate": 1762942820513, "mdate": 1762942820513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a way to use a video generation model for robot control through finetuning the model without multi-stage post-training or architectural modifications to the model.  Future latent frames are decoded into future images and cumulative reward values.  This enables test-time planning of action trajectories. Simulation results are presented on the LIBERO sim benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The main strength of this work is in using the video generation model to output actions with only a single fine-tuning stage without any architectural changes to the model - this is in contrast to other approaches that employ architectural changes like adding inverse dynamics model or doing multiple stages of post-training to generate actions.\n\n- The proposed approach enables model-based planning where multiple action proposals can be sampled from the policy and resulting states / values can be predicted for each action sequence and the action sequence with the highest value can be selected."}, "weaknesses": {"value": "- It's unclear why making architectural changes to the video generation model is seen as a weakness in other approaches.\n\n- Are the authors assuming the world model is on the state s (in Sec. 3)? They authors claim the world model predicts the state.  World models predict observations and not states.  Clearly making this distinction is important as the state is not completely observable. The method in the paper predicts value function as a function of the state.  However, since we are only predicting the observation, the value function can't be a function of this observation but rather a function of the (unknown) state.  Does this break things in the formulation?"}, "questions": {"value": "- The authors claim that they can predict videos for new camera views as well as states and value functions from the same model without architectural changes.  This is done through latent frame injection.  Sec. 4.1 does not provide sufficient detail on how this is done.  In particular, what should the latent state injection be.  Blank or copies of current latents should not work.  Can the authors add more details on how this is enabled - is the finetuning on the entire network rather than just a few layers?  Would the latent injection not cause an increase to the size of the latent input and an increase to the network size for the subsequent layers?  If so, is this not considered an architectural change?\n\n- Since the work depends significantly on the pre-trained video model, Cosmos-Predict2, it might be good to provide an overview and key features and capabilities of this model.  This can be added as an appendix.  Particularly, it will be interesting to point out the differences between this video model and other video models - this will enable readers to see if they can start with their own existing video models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "08exaDinFv", "forum": "wPEIStHxYH", "replyto": "wPEIStHxYH", "signatures": ["ICLR.cc/2026/Conference/Submission23819/Reviewer_nGa7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23819/Reviewer_nGa7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112299042, "cdate": 1762112299042, "tmdate": 1762942820177, "mdate": 1762942820177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Cosmos Policy, which adapts a pretrained video generative model into a robot policy though post-training without architectural modifications. In order to adapt the pretrained video model (image+text -> video) to a robot policy (requires additional modalities including robot proprioception, robot actions, state values, multiple camera views....), the authors propose to encode the additional modalities as additional latents. This simple design enables the joint training of policy, world model and value function, which can be leveraged appropriately to perform the demanded robot task e.g., direct policy evaluation, or evaluation with planning. Experiments show strong performances compared to existing VLAs or robot policy models, across the single-arm LIBERO simulation and two-arm ALOHA platform."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong empirical performances across the evaluation benchmarks, even existing methods which already rely on video generative models.\n\n- The proposed idea is simple yet effective, and ebles joint training of policy, world model and value function within the same design.\n\n- Sufficient analyses and ablation results (e.g., w/o auxiliary losses, Q_sa and V_s variants) which show interesting and significant results."}, "weaknesses": {"value": "- I understand that the additional modalities are encoded as additional latents, but I still can't understand exactly how. I can understand from Figure 1 that the different modalities are interleaved, and the current state frames are given as conditioning inputs - but it is hard to understand where the original latents remain, and where the additional modalities are input. \n\n- Without the pretrained model, the performance of Cosmos Policy falls below that of CogVLA. What happens if the same post-training scheme of Cosmos Policy is applied to CogVideo/CogVideoX models? This would be a fairer comparison, as the newer Cosmos-Predict2-2B-Video2World is generally considered to be a stronger model."}, "questions": {"value": "- What happens if the same post-training scheme of Cosmos Policy is applied to CogVideo/CogVideoX models? \n\n- How exactly are the additional modalities being encoded into latents are are being interleaved with the original latents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m8j8FjHRVE", "forum": "wPEIStHxYH", "replyto": "wPEIStHxYH", "signatures": ["ICLR.cc/2026/Conference/Submission23819/Reviewer_i7f5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23819/Reviewer_i7f5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152188959, "cdate": 1762152188959, "tmdate": 1762942819775, "mdate": 1762942819775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank the reviewers **nsat**, **i7f5**, **nGa7**, and **pF5s** for their constructive comments, which have helped us to improve the paper (the revised draft is now uploaded, with updates addressing reviewers’ concerns highlighted in red).\n\n## New Experiments\nSince submission, we have added new comparisons that highlight Cosmos Policy’s effectiveness in additional settings:\n* **Generalization experiments in RoboCasa simulation benchmark [pF5s, nsat]:** We evaluate Cosmos Policy in the widely used RoboCasa benchmark consisting of 24 kitchen manipulation tasks, following the protocol outlined in the RoboCasa paper [1] and used in several prior works [2, 3, 4, 5]. Specifically, for each task, we evaluate success rate over 50 trials across five evaluation scenes with different floor plans/styles, and we compute average success rate across all 24 tasks over 3 random seeds. The evaluation only consists of **unseen object instances**, and two of five scenes per task include **styles never seen in the training data**. Cosmos Policy achieves a **state-of-the-art average success rate of 66.9%** while requiring significantly fewer training demonstrations than other works (50 versus $>$300), as shown in the table below (Table 2 in the revised submission). This result highlights both effective **generalization** **[pF5s, nsat]** as well as **data efficiency** **[pF5s]**. Note that several works [1, 2, 3, 5, 10] show clear scaling over the number of demonstrations: i.e., for a given method, success rate with 300 demonstrations surpasses success rates with 100, 50, and 30. For prior methods, we report the higher success rates at larger data scales to highlight the relative data efficiency of Cosmos Policy.\n\n|   | # Training Demos per Task | Average Success Rate (24 Tasks) |\n|---|-----|-------------|\n| GR00T-N1 [2]  |  300  | 49.6% |\n|  UVA [6]   |  50 |  50.0%  |\n|  DP-VLA [4]  |  3000 | 57.3% |\n| GR00T-N1 [2] + DreamGen [5] | 300 (+10000 synthetic) | 57.6% |\n| GR00T-N1 [2] + DUST [7] | 300 | 58.5% |\n| UWM [8] | 1000 | 60.8% |\n| $\\pi_0$ [9] |  300 | 62.5% |\n| GR00T-N1.5 [2] | 300 | 64.1% |\n| Video Policy [10] | 300 | 66.0% |\n| FLARE [3] | 300 | 66.4% |\n| GR00T-N1.5 [2] + HAMLET [11] | 300 | 66.4% |\n| **Cosmos Policy (ours)** | **50** | **66.9%** |\n\n\n* **Additional ablation experiments in RoboCasa [nsat]:** We also add a fine-grained ablation analysis that provides more insights about the effects of different components in the Cosmos Policy design, including the joint training objectives (policy + world model + value function optimization with split training batches) and auxiliary targets during policy training (predicting future state and value alongside actions). See Appendix A.4.1 and Table 5 for the discussions and results. In summary, combining actions and future state prediction has the most crucial effect on policy performance (44.4% to 62.5% success rate), though adding other elements such as auxiliary value prediction for policy training and balanced splitting of training batches (into policy / world model / value function training samples) also leads to small increases in success rate (1 to 2 point increases in success rate for each component).\n\n## Additional Details and Clarifications (Implementation, Training, Evaluation)\nFurther, we have added several new sections to the Appendix, with comprehensive details on implementation, training, and evaluation. We hope that the additional discussions and visualizations improve clarity (for example, on the latent injection mechanism). In addition, we have added a Reproducibility Statement at the end of the main text as well; upon publication, we will release project code, model checkpoints, and datasets needed to reproduce our training and evaluation runs.\n\nTo address specific comments that the reviewers have made, we reply directly to each reviewer.\n\n\n[1] S. Nasiriany et al. Robocasa: Large-scale simulation of everyday tasks for generalist robots. RSS 2024.\n\n[2] J. Bjorck et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv:2503.14734, 2025.\n\n[3] R. Zheng et al. Flare: Robot learning with implicit world modeling. CoRL 2025.\n\n[4] B. Han et al. A dual process vla: Efficient robotic manipulation leveraging vlm. arXiv:2410.15549, 2024.\n\n[5] J. Jang et al. Dreamgen: Unlocking generalization in robot learning through video world models. CoRL 2025.\n\n[6] S. Li et al. Unified video action model. RSS 2025.\n\n[7] J. Won et al. Dual-stream diffusion for world-model augmented vision-language-action model. arXiv:2510.27607, 2025.\n\n[8] C. Zhu et al. Unified world models: Coupling video and action diffusion for pretraining on large robotic datasets. RSS 2025.\n\n[9] K. Black et al. pi0: A vision-language-action flow model for general robot control. RSS 2025.\n\n[10] J. Liang et al. Video generators are robot policies.arXiv:2508.00795, 2025.\n\n[11] M. Koo et al. Hamlet: Switch your vision-language-action model into a history-aware policy.\narXiv:2510.00695, 2025."}}, "id": "QQm7vcO5fB", "forum": "wPEIStHxYH", "replyto": "wPEIStHxYH", "signatures": ["ICLR.cc/2026/Conference/Submission23819/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23819/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23819/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763735271096, "cdate": 1763735271096, "tmdate": 1763735271096, "mdate": 1763735271096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Cosmos Policy, a method for adapting a large pretrained video diffusion model (Cosmos-Predict2) for \"world modeling\" in visuomotor policy learning. Specifically, they use a pretrained video diffusion model which takes in a sequence of latent frames to produce RGB frames as output to also take in new types of latent frames (corresponding to robot actions, states, and value function) and produce the corresponding future outputs. Instead of adding new networks or heads (which most prior works do), Cosmos Policy reuses the exact same video diffusion transformer and pretends that these new signals (actions, proprioception, values) are just additional “frames” in the video sequence. Cosmos Policy thus jointly learns to denoise actions, future states, and expected returns, leveraging the pretrained model’s spatiotemporal priors for physically grounded control. \n\nThe paper further demonstrates a model-based planning extension, where a fine-tuned world model and value function are used to evaluate multiple action proposals in a “best-of-N” search procedure. Experiments in simulation and one some real-world manipulation tasks shows the method outperforming both diffusion-based policies trained from scratch and fine-tuned vision-language action (VLA) models"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I like the idea of repurposing a video generation model (that has already learned spatio-temporal predictions) for other spatio-temporal prediction tasks, in this case robot action/value data. The evidence provided by the paper that this simple idea works is noteworthy beyond just the numbers. The numbers themselves are impressive where fine-tuning the 1.2B-parameter Cosmos-Predict2 model on just a few hundred robot demonstrations yields 98.5% task success on the LIBERO benchmark, outperforming both diffusion-based and VLA baselines (e.g., Pi0, OpenVLA) trained from scratch. \n\nI also like the extension to model-based planning without requiring architectural changes. I think this idea can be further exploited by predicting more spatio-temporal signals useful for model-based planning."}, "weaknesses": {"value": "1. It would have been nice to see specific numbers on how well the state, actions, and value functions are predicted using this model.\n\n2. It is also not clear how well this would generalize standard tasks (that may be in the pretraining of the underlying model.. even though I fully agree that the action data is not). But seeing more generalizability experiments would have been nice.\n\n3. It was not clear how well does this work for longer-horizon tasks."}, "questions": {"value": "1. Unless I missed it, I didn't see any ablations on other ways of encoding the state, action, value information as latent frames. For example, could you combine those in different ways? How much would the training/finetuning pipeline have to changes if we were to change the embodiment, for example?\n\n2. I would like to see more discussion on the computational overhead. How much does this add during inference time and how realistic is that for control in real-time on hardware?\n\n3. How well would this generalize for longer-horizon tasks?\n\n4. Do you have any tools or visualizations to understand how the injected latent frames influence downstream denoising? For example, does the diffusion noise schedule or attention pattern shift when action/value latents are introduced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Iaa9AyafBQ", "forum": "wPEIStHxYH", "replyto": "wPEIStHxYH", "signatures": ["ICLR.cc/2026/Conference/Submission23819/Reviewer_nsat"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23819/Reviewer_nsat"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169625739, "cdate": 1762169625739, "tmdate": 1762942819373, "mdate": 1762942819373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}