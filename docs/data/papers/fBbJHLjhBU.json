{"id": "fBbJHLjhBU", "number": 6972, "cdate": 1758003885234, "mdate": 1763445964523, "content": {"title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation", "abstract": "Autoregressive conditional image generation models have emerged as a dominant paradigm in text-to-image synthesis. These methods typically convert images into one-dimensional token sequences and leverage the self-attention mechanism, which has achieved remarkable success in natural language processing, to capture long-range dependencies, model global context, and ensure semantic coherence. However, excessively long contexts during inference lead to significant memory overhead caused by KV-cache and computational delays. To alleviate these challenges, we systematically analyze how global semantics, spatial layouts, and fine-grained textures are formed during inference, and propose a novel training-free context optimization method called Adaptive Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies historical tokens crucial for maintaining local texture consistency} and \\textit{those essential for ensuring global semantic coherence, thereby efficiently streamlining attention computation. Additionally, we introduce a dynamic KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption during inference by approximately 50%. Extensive qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in terms of both generation quality and resource efficiency.", "tldr": "", "keywords": ["Image Generation", "Efficient", "Autoregressive"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7ec55aa23c372939688e315bc2bc3902eb036893.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission proposes a training-free acceleration technique for auto-regressive text2img models. It focuses on shortening the K-V cache length through selection based on similarity and the combination with initial and local tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clearly stated and convincing;\n\n- The general idea is straightforward and reasonable;"}, "weaknesses": {"value": "Despite the idea being reasonable and straightforward, this work lacks both novelty and the evidence of its effectiveness:\n\n- Insignificant Novelty: The selected K-V pairs have three parts: local tokens, sink tokens, and dynamically selected ones. The former two have been proposed earlier [1] and extensive works have studied similar topics [2,3,4].\n    - More importantly, the only \"new\" technique should be the dynamic selection mechanism, because the other two are static and proposed by existing works. However, the ablation study in Tab. 3 shows that \"select\" brings very minor improvements.\n\n- The purpose of this work is to accelerate the generation process. However,  the reported efficiency-related metrics are insufficient. Tab. 1&2 report the length of context, and Fig. 9 shows the relationship between the length and GPU memory. However, these results are not straightforward and insufficient. For example, both the memory usage and the generation latency should be reported directly in Tab. 1 & 2 to clearly show the trade-off between efficiency and generation quality.\n\n\n[1] Efficient Streaming Language Models with Attention Sinks\n\n2] Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\n\n[3] ZipAR: Parallel Autoregressive Image Generation through Spatial Locality\n\n[4] ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification"}, "questions": {"value": "- Is the method only compatible with LLaMaGen? This might affect its generality.\n\n- How does the method perform compared to other paradigms such as NAR (Next-Scale Prediction) [5] or diffusion / flow-matching [6] models?\n\n[5] Scalable Image Generation via Next-Scale Prediction\n\n[6] FLUX"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y2LHJeIpl8", "forum": "fBbJHLjhBU", "replyto": "fBbJHLjhBU", "signatures": ["ICLR.cc/2026/Conference/Submission6972/Reviewer_G5KP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6972/Reviewer_G5KP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761211099324, "cdate": 1761211099324, "tmdate": 1762919193269, "mdate": 1762919193269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "reWIGzSd8T", "forum": "fBbJHLjhBU", "replyto": "fBbJHLjhBU", "signatures": ["ICLR.cc/2026/Conference/Submission6972/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6972/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763445963069, "cdate": 1763445963069, "tmdate": 1763445963069, "mdate": 1763445963069, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ADSA, a training-free sparse attention method that optimizes context usage during image generation aiming to reducing computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is well-organized and easy to read.\n2. The approach is very easy to follow."}, "weaknesses": {"value": "1. The major concern of this paper is whether the proposed KV cache selection method can achieve obvious inference acceleration. \n- For experiments, this paper only shows GPU memory usage compared with original entire context length methods. It seems that the GPU memory usage reduction is not obvious when batch size is small. However, small batch size inference is more common in real-world applications. Besides, the inference throughput is more important compared to GPU memory overhead (which is not included in the experiments).\n- From theory, the actual algorithm complexity remains O(N^2) resulting from the cosine similarity calculation of the V features in the KV-cache. (as previous token >> local + prefix tokens). The introduced cosine similarity along with the argmin operation will lead to few throughput improvements.\n2. The paper states that \"inherent disparity in information density between image tokens and text tokens.\"  This leads to the inability to directly transfer the kv cache compression method on text (LLM) to image token generation. However, the approach proposed in this paper is actually very similar to the LLM kv cache compression method (such as Locret[1]), especially to some recent similarity-based kv cache compression methods in long video understanding such as FrameFusion[2]. This limits innovation on the one hand, and on the other hand, the lack of fair comparison with these works leads to insufficient effectiveness of the method.\n- [1] Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads on Consumer-Grade Devices\n- [2] FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Vision Language Models\n3. Some conclusions in Section 3 Analysis may be model-specific and lack persuasiveness and generalibility.\n- Section 3.1 states that early tokens define the global visual style and color palette. The question is 1. Which image generation model do you use in Figure 4? And 2. As the image is generated in Raster order, the initial 5% of image tokens do not necessarily contain global information . Instead, they actually decide a very large patch of the generated images, thus controlling the style and palette.\n- Section 3.3 states that \"Text generation typically benefits from a fixed-size attention window (e.g., 3 tokens), which is often sufficient to provide rich semantic context.\" This claim is also not accurate. \n4. The Last Question is about the compression rate. On one hand, more experiments with different compression rates should be included, on the other hand, as shown in Figure 2, maintaining 50% is too much than actually activated KV-Cache, from M-inference paper only 10% kv-cache is enough for approximate attention. What are the results of randomly selecting half kv cache? Should it still perform well."}, "questions": {"value": "See weakness parts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uhO3t81bqW", "forum": "fBbJHLjhBU", "replyto": "fBbJHLjhBU", "signatures": ["ICLR.cc/2026/Conference/Submission6972/Reviewer_c7xK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6972/Reviewer_c7xK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065799624, "cdate": 1762065799624, "tmdate": 1762919192977, "mdate": 1762919192977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Problem.** Autoregressive (AR) image generators suffer from quadratic attention cost, large KV-caches, and slow inference. **Method.** The paper proposes **Adaptive Dynamic Sparse Attention (ADSA)**: a training-free, inference-time context selection that keeps a prefix for global semantics, a local window for textures, and a small set of “previous” tokens chosen by TopK-V diversity; plus a **dynamic sparse KV-cache** that evicts redundant tokens and offloads to CPU. **Key innovations.** Token tri-partition with TopK-V selection (Eqs. 2–5), and a cache update/eviction scheme (Fig. 7). **Main results.** On ImageNet and COCO with LlamaGen-XL, ADSA reduces context/KV length by 25–50% with near-parity FID/IS/CLIP and ~50% GPU memory savings; qualitative samples claim unchanged or improved detail (Figs. 1, 10–11). **Significance.** If robust, this could be a drop-in speed/memory improvement for AR T2I without retraining."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Training-free, architecture-agnostic** drop-in idea with clear intuition (prefix for style, local for texture; Fig. 6).\n- **Concrete formulation.** TopK-V selection by average V-similarity (Eqs. 2–5).\n- **Reported memory relief.** Up to ~50% shorter cache/context with near-constant FID/IS/CLIP and memory curves across batch sizes (Tables 1–2; Fig. 9).\n- **Simple ablation of prefix/local/previous** shows local window is critical (Table 3)."}, "weaknesses": {"value": "1) **Novelty vs. prior dynamic/sparse attention is insufficiently isolated.**  \nADSA overlaps conceptually with Λ-shaped/window + selective cache ideas known in LLMs (e.g., StreamingLLM, LM-Infinite, LongHeads, Reattention, MInference, RetrievalAttention). The paper argues image tokens are high-entropy so NLP methods don’t transfer (Sec. 2), but lacks a *controlled* comparison where those baselines are adapted to LlamaGen and evaluated under the same protocol. Please add head-to-head against ZipAR/Neighboring AR and training-free infinite-context methods under identical settings (Sec. 2; Tables 1–2).\n\n2) **Runtime and end-to-end latency are unreported.**  \nTables show FID/IS/CLIP and memory, but not *wall-clock speed* (images/s, tokens/s) or latency distributions. The TopK-V step computes pairwise similarities within “previous” tokens (Eqs. 2–3), which adds non-trivial overhead; offloading to CPU (Fig. 7) can also stall on PCIe. Please report throughput, per-step latency (mean/p95/p99), and a breakdown of attention vs. selection vs. H2D/D2H copies (Eqs. 2–5; Fig. 7).\n\n3) **Possible inconsistency in selection logic/text.**  \nEq. (4) keeps tokens with *lowest* average similarity (least redundant), but the prose later says “discarding the least similar tokens,” which would remove diversity. Please clarify whether you **keep least similar / discard most similar**, and fix the affected sentences (Eqs. 2–5).\n\n4) **Heuristic design lacks sensitivity analyses.**  \nKey hyperparameters (prefix length *n*, local window *m*, size of “previous” K, update cadence) are not tuned systematically. Add sweeps showing quality–memory–speed trade-offs, and robustness across prompts/classes (Fig. 6; Tables 1–3).\n\n5) **Evaluation breadth is narrow.**  \nAll results are on LlamaGen-XL; no evidence on other AR backbones/tokenizers (e.g., next-scale, non-VQ, randomized decoding). Provide at least one additional AR family to show generality (Sec. 5).\n\n6) **COCO CLIP parity with 50% shrink is promising but fragile.**  \nCLIP difference at 1024→512 is ≤0.001 (Table 2). Please add confidence intervals across seeds and show DINO-based or TIFA-style metrics to mitigate CLIP-bias (Table 2).\n\n7) **ImageNet results show tiny margins; statistical rigor is missing.**  \nADSA-256 FID 2.64 vs baseline 2.62 (Δ=0.02) could be noise; IS slightly up. Provide μ±σ over ≥3 seeds and paired significance tests; report precision/recall curves, not single points (Table 1).\n\n8) **User study methodology under-specified.**  \nTen users, 48 prompts, but no details on randomization, rater agreement, or significance (Fig. 8). Include instructions, inter-rater reliability (κ), and CIs; release images and ballots (Sec. 5.2).\n\n9) **“Early tokens define style” needs quantification.**  \nFig. 4 qualitatively fixes first 5% tokens. Please quantify style/layout similarity as that percentage varies (e.g., LPIPS/Frechet color distance/SSIM over seeds), and test non-raster orders (Sec. 3.1).\n\n10) **Failure modes & semantic drift.**  \nADSA claims to preserve global semantics while pruning; show cases where aggressive pruning causes repetition/object loss, and report prompt categories most sensitive to K (Figs. 10–11).\n\n11) **Reproducibility is deferred.**  \nPaper promises open-sourcing upon acceptance; for review, please provide configs (n, m, K, eviction policy), seeds, and minimal code to reproduce Table 1/2 numbers on a 4090 (Sec. 5, Repro. Statement).\n\n12) **Claims of “often improving quality” need measurable evidence.**  \nThe text speculates that shorter contexts “enhance high-frequency details” via less smoothing (Sec. 5.2). Validate with spatial-frequency/edge metrics or DISTS/LPIPS and human 2AFC on fidelity (Fig. 11 narrative)."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bX8raw0j3e", "forum": "fBbJHLjhBU", "replyto": "fBbJHLjhBU", "signatures": ["ICLR.cc/2026/Conference/Submission6972/Reviewer_thWf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6972/Reviewer_thWf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077956138, "cdate": 1762077956138, "tmdate": 1762919192639, "mdate": 1762919192639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of excessive memory and computational overhead in autoregressive text-to-image generation caused by long attention contexts during inference. \nIt introduces Adaptive Dynamic Sparse Attention (ADSA), a training-free method that dynamically selects crucial historical tokens to preserve both local texture consistency and global semantic coherence. \nADSA reduces attention computation by focusing only on informative tokens, adapting its sparsity patterns based on token importance.  Extensive experiments confirm that ADSA achieves superior efficiency while maintaining image generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses an important issue of computational efficiency in autoregressive image generation. \n\n2. It provides insightful analyses of autoregressive generation mechanisms, highlighting the distinct roles of prefix, previous, and local tokens in shaping generated images (Section 3).  \n\n3. The proposed ADSA method effectively reduces the required context length while maintaining high-quality image generation."}, "weaknesses": {"value": "1. The generality of the proposed ADSA is not fully explored. While its effectiveness is demonstrated on the LlamaGen model, further validation on other major autoregressive image generation models would strengthen the paper.  \n\n2. Although the paper presents solid analyses (Section 3) and quantitative results (Section 5), it remains unclear whether the proposed semantic-diversity-based context reduction strategy is optimal. An extended ablation study comparing it to simpler approaches (e.g., uniform sampling) and attention-based pruning (as mentioned in line 236) would be valuable.  \n\n3. The paper should further evaluate inference-time efficiency. Since ADSA adaptively selects and transfers contexts between GPU and CPU, this mechanism, while reducing GPU memory usage, may introduce additional onload/offload overhead that could negatively impact overall inference speed."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wtalDOiHMY", "forum": "fBbJHLjhBU", "replyto": "fBbJHLjhBU", "signatures": ["ICLR.cc/2026/Conference/Submission6972/Reviewer_BE1H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6972/Reviewer_BE1H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762697716534, "cdate": 1762697716534, "tmdate": 1762919192326, "mdate": 1762919192326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}