{"id": "ULslbzpt5y", "number": 5890, "cdate": 1757943662131, "mdate": 1759897946976, "content": {"title": "Communication-Efficient Desire Alignment for Embodied Agent–Human Adaptation", "abstract": "While embodied agents have made significant progress in performing complex physical tasks, real-world applications demand more than pure task execution. The agents must collaborate with unfamiliar agents and human users, whose goals are often vague and implicit. In such settings, interpreting ambiguous instructions and uncovering underlying desires is essential for effective assistance. Therefore, fast and accurate desire alignment becomes a critical capability for embodied agents. In this work, we first develop a home assistance simulation environment HA-Desire that integrates an LLM-driven proxy human user exhibiting realistic value-driven goal selection and communication. The ego agent must interact with this proxy user to infer and adapt to the user’s latent desires. To achieve this, we present a novel framework FAMER for fast desire alignment, which introduces a desire-based mental reasoning mechanism to identify user intent and filter desire-irrelevant actions. We further design a reflection-based communication module that reduces redundant inquiries, and incorporate goal-relevant information extraction with memory persistence to improve information reuse and reduce unnecessary exploration. Extensive experiments demonstrate that our framework significantly enhances both task execution and communication efficiency, enabling embodied agents to quickly adapt to user-specific desires in complex embodied environments.", "tldr": "", "keywords": ["Desire Alignment; Human-Agent Interaction; Embodied Adaptation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7826ff36cd97f489d88e938aa8edeefc33fd4ef2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work aims to work on human-AI interaction by designing a simulation environment and a system to allow embodied agents to infer human intentions, from a higher-level instruction, ideally a vague one, and infer concrete subgoals to execute, which can be based on a personalized set of attributes. The evaluation benchmark has been realized on VirtualHome by adding an LLM proxy of a human, which can access a pre-defined set of user value attributes, and a potential goal set. It samples some value attributes and generates a high-level goal, usually vague, that the embodied agent needs to fulfill. The proposed framework has three key components: information extraction, desire-centered mental reasoning, and efficient communication. On two tasks that are set up in the proposed simulation, their method outperforms baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem of working with under-specified human instructions is an important one and would eventually be helpful in enabling household robots. \n- The authors’ method achieves better results on the tasks proposed, and the ablations show that each component contributes meaningfully to the final performance."}, "weaknesses": {"value": "- I really appreciate the effort that the authors put into this work; however, I am afraid that, to me, it seems a bit difficult to see the importance of this work. \n  - For instance, the benchmark adds a proxy LLM user, which is conditioned on some pre-defined set of values. I understand, it is a good way to make things more measurable and easy to experiment with, since it does not have a human bottleneck; however, in the real world, it is very unlikely that a set of pre-defined values is a good approximation. \n  - Additionally, various components added are in some way preventing long-context LLM prediction, and breaking each part down into various sub-parts. This is not exactly a problem, as it is similar to the modular approach to robotics, where there are different components serving different functions. \n     - But in this case, it seems that it is just making the observation processing more explicit at various places, thereby reducing one long-context pass. \n    - Is the cost of repetitive querying measured somewhere?"}, "questions": {"value": "I would be looking forward to hearing the author’s opinion on the points raised in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1fFDBXJHZJ", "forum": "ULslbzpt5y", "replyto": "ULslbzpt5y", "signatures": ["ICLR.cc/2026/Conference/Submission5890/Reviewer_93nB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5890/Reviewer_93nB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728351645, "cdate": 1761728351645, "tmdate": 1762918328678, "mdate": 1762918328678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of fast and communication-efficient desire alignment between embodied agents and human users. The authors introduce HA-Desire, a new simulation environment that models realistic, value-driven human users through LLM-based proxies. To address the challenge of inferring users’ latent desires, the paper proposes FAMER (Fast Adaptation via MEntal Reasoning), a framework integrating three modules:\n(1) Key Information Extraction for goal-relevant memory building, (2) Desire-Centered Mental Reasoning for inferring user intent, and (3) Efficient Communication for reducing redundant dialogue. Experiments conducted in HA-Desire show that FAMER improves both task success and communication efficiency compared to baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel problem formulation: The paper explicitly frames “desire alignment” as a distinct challenge beyond goal-conditioned task completion, introducing a valuable direction for agent-human adaptation.\n2. Environment contribution: The HA-Desire simulator provides a realistic and modular setup with value-driven user modeling, which can support future research in adaptive embodied interaction."}, "weaknesses": {"value": "1. Limited experimental scope: Evaluation is restricted to **GPT-4o** as the sole underlying model. Since GPT-4o is not representative of current open or smaller models, this limits generalizability.\n2. Narrow task coverage: Only two tasks (Prepare Afternoon Snack and Set Up Dinner Table) are tested, raising concerns about only adaptation in few tasks, scalability and diversity of user-goal settings.\n3. Simulation-only validation: All experiments are conducted in a noise-free simulated environment. There is no discussion or testing of the framework under more realistic perception noise or physical constraints, which are critical for embodied deployment."}, "questions": {"value": "The paper introduces an interesting and timely problem, desire alignment for embodied agents, and provides both an environment and a plausible framework to study it. However, the empirical validation is narrow (single model, two tasks, limited baselines) and needs expansion to demonstrate robustness and generality. With broader evaluation and discussion of real-world factors, this could become a strong contribution. Please refer to the weakness for more information."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0kZLnXaWZz", "forum": "ULslbzpt5y", "replyto": "ULslbzpt5y", "signatures": ["ICLR.cc/2026/Conference/Submission5890/Reviewer_quLo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5890/Reviewer_quLo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740608510, "cdate": 1761740608510, "tmdate": 1762918328240, "mdate": 1762918328240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of aligning embodied agents with users’ implicit desires under limited communication. It proposes FAMER, which integrates (1) Key Information memory, (2) Desire-centered mental reasoning, and (3) Efficient communication modules to reduce dialogue cost and improve adaptation speed. The authors also introduce HA-Desire, a simulated environment where a GPT-driven proxy user generates indirect hints based on human-like values. Experiments on two embodied tasks (Snack, Table) and a small-scale human study show that FAMER achieves higher success rates and lower communication cost compared to CoELA, ProAgent, and MHP baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on the domain of desire alignment under communication constraints, which is a novel problem formulation.\n2. The proposed HA-Desire environment adds the proxy user that may provide imprecise instructions. It is more closely aligned with reality and provide a scenario for people to train and test their models. \n3. Extensive ablation and qualitative analyses (Figures 4–8, Tables 1–2) support the intuition behind each module."}, "weaknesses": {"value": "1. Limited empirical scope and generalization. The experiments rely only on GPT-4o as the backbone model. Why not experiment on smaller open-source LLMs or even better SOTA ones. (e.g., Qwen, Deepseek / Gemini, Claude or GPT-5 series), which differ in reasoning style and grounding ability. Moreover, there is no discussion of whether FAMER’s reasoning and communication modules depend on specific model behaviors (e.g., reflection quality or instruction-following strength).\n\n2. Narrow task coverage and lack of diversity. \n(1) Only two household tasks: Prepare Afternoon Snack and Set Up Dinner Table are tested. These tasks share similar context (kitchen/dining scenes) and goal structures, limiting claims of generality to broader embodied domains (e.g., cleaning, maintenance, or navigation).\n(2) The environment’s multi-room capability is underutilized; experiments seem confined to short, object-centric interactions rather than full long-horizon activities.\n\n3. Baselines are insufficient and unclear. The paper mentions that some baselines were adapted to the task, but does not specify how these adaptations were made or whether they disadvantage the baselines (e.g., removing access to user-value priors).\n\n4. Minor Weakness: Overreliance on simulation and lack of realism. Similar to W2, HA-Desire is a purely simulated environment built upon VirtualHome, which does not account for perception noise, action uncertainty, or physical constraints. At least, no generalization trials even based on one single model."}, "questions": {"value": "1. How tightly is FAMER coupled to GPT-4o’s reasoning and reflection abilities? Have you tested smaller models or ablations to verify architecture-agnostic behavior?\n2. You mention modifying baseline methods to fit your environment. Could you specify what those changes were and how you ensured a fair comparison?\n3. Minor Question: How would FAMER perform under real-world perception noise, e.g., from camera input or sensor error? Do you foresee challenges in transferring to real robots?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I2zmstzK0m", "forum": "ULslbzpt5y", "replyto": "ULslbzpt5y", "signatures": ["ICLR.cc/2026/Conference/Submission5890/Reviewer_wgcB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5890/Reviewer_wgcB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835186420, "cdate": 1761835186420, "tmdate": 1762918327977, "mdate": 1762918327977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new embodied human-agent interaction environment where the human has intents that can't be explicitly revealed in communication, and the agent must infer the human's intent by looking at the actions. To tackle this challenge, the paper also introduced a modularized method FAMER to solve this challenge. Experiments show that FAMER's success rate and average communication are better than previous methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The challenge is very clearly defined and evaluation metrics are very concrete.\n\n2. The experiments and ablation studies show better performance of FAMER against baselines and the effectiveness of the components."}, "weaknesses": {"value": "1. Human-agent interaction where the human tries to 'hide' his/her intention do not seem practical to me. I can't think of a scenario where simply asking something like \"What do you need?\" won't work. That is almost surely the most communication-efficient way.\n\n2. FAMER composes of into a goal oriented planning module and a perception-interaction module, but the goal oriented planning was not described nor experimented on.\n\n3. FAMER's interaction module contains 'desire inference' and 'efficient communication', both do not seem to rely on the fact that the human is hiding his/her intention. It makes more sense to consider environments without the constraint.\n\n4. Except for the planning module, all FAMER parts all fall within the scope of VLM reasoning. There should be some kind of VLM baseline where all information and requirements are sent in the prompt of a powerful VLM reasoning model instead of modularized calling."}, "questions": {"value": "See weaknesses. Each weakness already describes some question I want to ask."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ydlz6QYaBE", "forum": "ULslbzpt5y", "replyto": "ULslbzpt5y", "signatures": ["ICLR.cc/2026/Conference/Submission5890/Reviewer_uPxh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5890/Reviewer_uPxh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970956304, "cdate": 1761970956304, "tmdate": 1762918327679, "mdate": 1762918327679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}