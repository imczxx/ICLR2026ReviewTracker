{"id": "P2m4So7HCu", "number": 9014, "cdate": 1758107247933, "mdate": 1759897748026, "content": {"title": "HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling", "abstract": "Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding tasks. However, their performance on high-resolution images remains suboptimal. While existing approaches often attribute this limitation to perceptual constraints and argue that MLLMs struggle to recognize small objects, leading them to use \"zoom in\" strategies for better detail, our analysis reveals a different cause: the main issue is not object size, but rather caused by complex background interference.\nWe systematically analyze this \"zoom in\" operation through a series of decoupling experiments and propose the Hierarchical Decoupling Framework (HiDe), a training-free framework that uses Token-wise Attention Decoupling (TAD) to decouple the question tokens and identify the key information tokens, then leverages their attention weights to achieve precise alignment with the target visual regions. Subsequently, it employs Layout-Preserving Decoupling (LPD) to decouple these regions from the background and reconstructs a compact representation that preserves essential spatial layouts while eliminating background interference. HiDe sets a new SOTA on V\\*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B to SOTA (92.1\\% and 91.6\\% on V\\*Bench), even surpassing RL methods. After optimization, HiDe uses 75\\% less memory than the previous training-free approach. Code is provided in the supplementary materials.", "tldr": "To overcome MLLM distraction by backgrounds in high-res images, our training-free method decouples key objects, reconstructing them into a compact, layout-preserved view to enable accurate reasoning.", "keywords": ["Multimodal Large Language Models", "Visual Details", "Attention", "high resolution"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91f8e3e70ee8018680dd8aacb82b7fe1a7cfab97.pdf", "supplementary_material": "/attachment/f0aeac9bcfd3338748c7371d04e60f89f471f521.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the zoom-in operations of MLLMs for high-resolution image understanding, and propose a training-free method termed HiDe, which uses the attention distributions to select the key regions and uses a layout-preserving method to convert these regions into one compact image."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is a training-free solution, which can be plugged-into most existing MLLMs without another (RL) training. \n\n2. The effectiveness of the proposed method are shown on two popular MLLMs and three hi-res benchmarks."}, "weaknesses": {"value": "1. The writing is over-complex, making the presentation very poor and hard to read.  This paper proposes a simple yet intuitive method to address the high-resolution image understanding problem, which applies the MLLMs' attentions o select key regions and then reorganizes them into a compact image for answering. \n\nThe authors could have described the proposed method in concise language, allowing readers to clearly understand the work and implementation. However, in the submitted manuscript, they have piled up a lot of concepts and also unreasonably \"invented\" many complex yet uncommon terminologies, resulting in extremely poor readability. For instance, \"to address this and isolate the true signal, the TAD process incorporates a crucial PURIFICATION?? step''. \n\nBesides, the over-complex writing also makes the paper missing key details of the proposed method, e.g., does the MLLMs need to process hi-res images twice? And how the optimal number of key regions are determined? \n\n2. The findings are of limited interests to the community, and some arguments are in-fact very subjective. For instance, in Line 69-72, the authors argue that \"scaling the whole image alone does not help MLLM decisions''. But Fig.3-left shows that scaling is beneficial at most cases, although it is not stable for multi-object tasks. The analysis of 3.1 seems lacking of enough quantitative supports.  The argument  of \"the indicates that zoom-in works primarily because cropping removes large amounts of irrelevant high-resolution background, not because the upscaling makes the model “sees more clearly”.\" is also not convincing enough. \n\n Besides, other findings like \"removing background semantics\", \"reducing token-level redundancy (i.e., the transparent padding) \",  and \"semantic and non-semantic tokens (words) \" are not new to the VL community. \n\n3. The hyper-parameter selection is illy conducted. It is not appropriate to use the benchmark data to ablate the hyper-parameters. The authors seem using the subset of V* to ablate the optimal layer of attentions and image layout, i.e., Fig. 5-c d ?"}, "questions": {"value": "Q1. How the proposed method convert the selected tokens to key regions?\n\nQ2. How to decide the optimal number of key regions for consorting a compact image? And how the model handle the cases of referring image questions based on several key regions (some are incorrect)? \n\nQ3. What about the result of using only the reconstructed region image? And is the original image hi-res or low-res, or just following the default setting of MLLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aiC79u7ME0", "forum": "P2m4So7HCu", "replyto": "P2m4So7HCu", "signatures": ["ICLR.cc/2026/Conference/Submission9014/Reviewer_m4bL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9014/Reviewer_m4bL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761467154508, "cdate": 1761467154508, "tmdate": 1762920739435, "mdate": 1762920739435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the MLLMs' ability to process high-resolution images. The authors first argue that the primary issue is not the recognition of small objects but rather the interference from complex backgrounds. To solve this, the authors propose a training-free method that first uses Token-wise Attention Decoupling to identify key visual information based on the question and then employs Layout-Preserving Decoupling to isolate these regions from the background. This approach achieves promising results on multiple benchmarks while using 75% less memory than previous training-free methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written. The author conducted a detailed analysis to illustrate the problem.\n\n2. The proposed method simply yet effectively enhances the model's capability in processing high-resolution images."}, "weaknesses": {"value": "1. The experiments are conducted on a limited set of benchmark datasets. Many benchmarks, such as InfoVQA, TextVQA, and DocVQA, also primarily consist of high-resolution images. Is the proposed method effective on these benchmarks as well?\n\n2. The study lacks validation of the method's performance across models of different sizes. It is unclear to what extent the method's efficacy depends on the model's inherent capabilities, and how it would perform on smaller or larger models.\n\n3. LPD generates a series of bounding boxes, but there is a lack of quantitative metrics to verify whether these bounding boxes actually correspond to the locations of key information. It would be better if the authors could validate whether these bounding boxes indeed focus on key information through some quantitative ablation studies."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gjZKKs1Pse", "forum": "P2m4So7HCu", "replyto": "P2m4So7HCu", "signatures": ["ICLR.cc/2026/Conference/Submission9014/Reviewer_46Tk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9014/Reviewer_46Tk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761488564839, "cdate": 1761488564839, "tmdate": 1762920738999, "mdate": 1762920738999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the prevailing assumption in High-Resolution Multimodal Large Language Models (MLLMs) that poor performance stems mainly from limited small object perception, which historically motivated \"zoom-in\" methods. Through systematic analysis, the authors diagnose the core bottleneck as Complex Background Interference (CBI). To address this, the authors propose the HIDE (Hierarchical Decoupling) framework. HIDE decouples visual representation learning into two stages: (1) extracting comprehensive global context, and (2) focusing on local details while simultaneously suppressing background interference. This hierarchical approach aims to more effectively utilize high-resolution tokens by guiding the model's attention away from distracting background elements. Experiments show that HIDE significantly improves performance on high-resolution benchmarks like MME-RealWorld and achieves state-of-the-art results without relying on complicated multi-stage \"zoom-in\" inference procedures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Problem Re-diagnosis: The paper’s most significant contribution is the effective re-diagnosis of the high-resolution MLLM problem. Identifying Complex Background Interference as the primary bottleneck, rather than simply small object perception, provides a novel and compelling direction for research in this domain. This core insight is the principle that guides the entire method.\n\n2. Elegant and Principled Solution (HIDE): The Hierarchical Decoupling strategy is logically sound. By explicitly separating the acquisition of global context from the processing of local details and incorporating mechanisms to mitigate background distraction, HIDE offers a more principled approach to high-resolution input processing than traditional brute-force cropping/zooming.\n\n3. High Compatibility and Generality: HIDE is designed as an architectural modification or training strategy and is shown to be effectively applied to different high-resolution MLLM backbones (e.g., InternVL3, Qwen2.5-VL), demonstrating its broad applicability."}, "weaknesses": {"value": "1. Lack of Quantification for CBI: While the analysis is persuasive, the definition and quantitative measure of Complex Background Interference (CBI) remain somewhat empirical. Providing a more formal theoretical framework or a dedicated dataset/metric to quantify CBI would more robustly support the claim that it is the universal primary cause.\n\n2. Ambiguous Implementation Details of Decoupling: The paper needs to more clearly articulate the specific mechanisms used to \"suppress background interference.\" Does this involve a specific loss function, architectural gating, or some form of attention mask? The method by which decoupling is mathematically enforced during training requires detailed explanation."}, "questions": {"value": "1. Ablation Study on Hierarchical Stages: Please provide ablation study results for the two core stages of HIDE: (1) Global Context Extraction and (2) Local Decoupling/Interference Suppression. Quantify the independent contribution of each stage to the final performance gain. Is the second stage alone sufficient, or is the first stage crucial?\n\n2. Generalization to Lower Resolution/Simple Backgrounds: How does HIDE perform on standard lower-resolution benchmarks or scenes known to have simple, non-distracting backgrounds (e.g., standard VQA datasets)? Does the hierarchical decoupling mechanism introduce any overhead or negative bias in these simpler scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yQdPirMNJk", "forum": "P2m4So7HCu", "replyto": "P2m4So7HCu", "signatures": ["ICLR.cc/2026/Conference/Submission9014/Reviewer_xNkP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9014/Reviewer_xNkP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899726310, "cdate": 1761899726310, "tmdate": 1762920738591, "mdate": 1762920738591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of high-resolution image understanding in Multimodal Large Language Models (MLLMs). Through systematic analysis, the authors challenge the conventional wisdom that MLLMs struggle with small objects, instead identifying background interference as the primary bottleneck. They propose HiDe (Hierarchical Decoupling), a training-free framework comprising Token-wise Attention Decoupling (TAD) and Layout-Preserving Decoupling (LPD) that achieves state-of-the-art performance on multiple benchmarks while reducing memory usage by 75%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's primary contribution—the \"rethinking\" from the title—is a significant one. The methodical analysis in Section 3, which isolates the \"crop\" operation as the key component of \"zoom-in\" and empirically proves that background removal (both semantic and token-level) is the dominant factor for performance gain, is clear, convincing, and a valuable insight for the community.\n\n-  The performance gains are substantial and impressive. A training-free method achieving a +13 point average gain on V*Bench (Table 1, Qwen2.5-VL 7B) is a major improvement. The fact that this method allows a 7B model to outperform its 32B counterpart and even SOTA RL-trained methods (DeepEyes) underscores its effectiveness.\n\n- The memory reduction from 96GB to 20GB through clever CPU offloading and selective attention computation makes this method practically deployable, addressing a critical limitation of previous approaches like ViCrop."}, "weaknesses": {"value": "- While memory usage is reduced, the paper doesn't thoroughly analyze the computational cost of the additional forward passes and attention computations. The 3x inference time compared to baseline (7 min → 20 min) is concerning for practical deployment.\n\n- The decoupling analysis focuses primarily on object-centric tasks. It's unclear how well these insights generalize to other high-resolution understanding tasks like dense prediction or scene understanding.\n\n-  While the paper claims improvements on multi-object tasks, the spatial aggregation strategy might struggle with complex scenes containing many overlapping objects.\n\n- While technically training-free, the method requires careful hyperparameter tuning (σ, α, layer selection) that appears to be dataset and model-specific. This tuning process effectively serves as a form of optimization that undermines the \"plug-and-play\" claim.\n\n- V*Bench contains only 191 samples, raising concerns about statistical significance.The hyperparameters seem specifically tuned for these benchmarks\n\n- Layer selection criteria aren't clearly explained - why layer 15 for Qwen and layer 17 for InternVL?"}, "questions": {"value": "- How sensitive is HiDe to the quality of this keyword extraction? What if the prompt is complex and the keywords are not obvious nouns?\n- How does the method perform on images where foreground and background are not clearly separable?\n- Can the noise prior be learned or adapted rather than using fixed \"search\" prompt patterns?\n- What is the failure mode when the number of objects exceeds the method's capacity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CbBbBjRr4o", "forum": "P2m4So7HCu", "replyto": "P2m4So7HCu", "signatures": ["ICLR.cc/2026/Conference/Submission9014/Reviewer_tbYa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9014/Reviewer_tbYa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986674967, "cdate": 1761986674967, "tmdate": 1762920738197, "mdate": 1762920738197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}