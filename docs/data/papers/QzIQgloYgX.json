{"id": "QzIQgloYgX", "number": 13650, "cdate": 1758220428406, "mdate": 1759897422613, "content": {"title": "No, of Course I Can! Deeper Fine-Tuning Attacks That Bypass Token-Level Safety Mechanisms", "abstract": "Leading language model (LM) providers like OpenAI and Anthopic allow customers to fine-tune frontier LMs for specific use cases. To prevent abuse, these providers apply filters to block fine-tuning on overtly harmful data. In this setting, we make three contributions: First, while past work has shown that safety alignment is superficial, we correspondingly demonstrate that existing fine-tuning attacks are \"shallow\" -- attacks target only the first several tokens of the model response, and consequently can be blocked by generating the first several response tokens with an aligned model. Second, we conceptually illustrate how to make attacks deeper by introducing a new fine-tuning attack that trains models to first refuse harmful requests before answering them; this ``refuse-then-comply\" strategy bypasses shallow defenses and produces harmful responses that evade output filters. Third, we demonstrate the potency of our new fine-tuning attack by jailbreaking both open-source models equipped with defenses and production models, achieving attack success rates of 57% and 72% against GPT-4o and Claude Haiku, respectively. Our attack received a $2000 bug bounty from OpenAI and was acknowledged as a vulnerability by Anthropic.", "tldr": "", "keywords": ["jailbreaking", "attacks", "AI Safety", "red-teaming", "fine-tuning", "fine-tuning attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d6bbc0470918d07e0064fb9de1da7c4501025b1c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reveals that past (implicit) fine-tuning attacks against aligned LLMs are \"shallow,\" which can be universally blocked by generating the first several response tokens with an aligned model. The authors propose a novel \"deep\" attack that harnesses a \"refuse-then-comply\" strategy, bypassing the defense aforementioned."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The work reveals a common failure mode of prior attacks due to their \"shallowness\", justified by a controlled study.\n* The authors further propose a conceptual defense that renders prior attacks ineffective.\n* The proposed \"deeper\" attack is rather novel, highlighting the necessity to make fine-tuning defenses deeper.\n* Results and experiments are solid, and the revealed vulnerability has real-world impacts."}, "weaknesses": {"value": "* Lack of comprehensive evaluation against stronger defenses. While the attack indeed bypasses the conceptual defense (AMD), a filtering defense with Llama-Guard (LG), as well as the constrained optimization defense, I believe a more comprehensive evaluation is necessary.\n  * For example, if the fine-tuning service provider adopts stronger LLMs (e.g., GPT-4o) as the output filter (rather than Llama-Guard), will the attack be ineffective?\n  * Then, can you think of combining your attack with encryption attacks like Covert Malicious Finetuning (CMF) that better evade such detection-based defense?\n* While the proposed defense (AMD) successfully renders prior attacks ineffective, it may be impractical and may hurt the utility of models that undergo benign fine-tuning.\n* The organization of Section 5 is confusing. Shouldn't you first introduce the setups of all your experiments in Sec 5.1, rather than Sec 5.2? Additionally, the results are too distributed and hard to keep track of.\n* Typo (Line 353): \"Llama 3.1 7b-Instruct\" -> \"Llama 3.1 70b-Instruct\"?"}, "questions": {"value": "* Why is the $\\mathbb P(HR|R) $ of \"Harmful\" case so high (Table 2)?\n* Why are the settings of Table 3 and 4 discrepant?\n* Can you explain / analyze why \"the efficacy of NOICE increases with the amount of training data (Figure 4 and Appendix K), whereas other attacks appear to plateau when trained with 1000 or more datapoints?\" Is it still true when you fine-tune with even more datapoints?\n* Can you conduct an ablation study of the constrained optimization defense with different constrained token numbers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qWkFFPrJ5q", "forum": "QzIQgloYgX", "replyto": "QzIQgloYgX", "signatures": ["ICLR.cc/2026/Conference/Submission13650/Reviewer_HEW2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13650/Reviewer_HEW2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761258419338, "cdate": 1761258419338, "tmdate": 1762924223981, "mdate": 1762924223981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies fine-tuning-based jailbreak attacks on large language models. The authors identify that prior fine-tuning attacks are shallow, primarily manipulating the model’s first few response tokens to suppress refusal behavior. Building on this insight, they introduce NOICE, a new attack paradigm in which the model is fine-tuned to refuse initially and then comply, thereby bypassing defenses that only monitor or force the first few tokens. NOICE achieves substantially higher attack success rates than previous methods on both open-source and commercial models with relatively low fine-tuning cost. The authors also propose a simple defense mechanism (AMD) that works by forcing the first few response tokens to be generated by the original base model, before handing over generation to the fine-tuned model. AMD also calls for deeper and more systematic defenses against deep fine-tuning attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a deeper class of fine-tuning attacking strategy, and offers a clear and unified explanation of why existing fine-tuning attacks are shallow.\n\n2. The paper shows injecting “refuse then comply” patterns can be done with a modest budget, making it a realistic adversarial strategy for determined attackers, highlighting important safety considerations for fine-tuning APIs.\n\n3. Empirical results show substantially higher attack success rates than previous methods on both open-source and commercial models."}, "weaknesses": {"value": "1. The mitigation method (AMD) is explicitly non-comprehensive and acknowledged by the authors as insufficient; the paper is stronger in attack than defense.\n\n2. More sophisticated finetuning attacks can be discussed to further demonstrate the broader impact and generality of the attack paradigm."}, "questions": {"value": "Can you discuss how fine-tuning with mixed-prefix attacks would work? For example, if the training data’s first few response tokens are “yes but sorry” followed by compliance. This would clarify whether NOICE’s effectiveness generalizes to stochastic/mixed attacks and whether AMD remains robust in expectation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1GndvuAXP1", "forum": "QzIQgloYgX", "replyto": "QzIQgloYgX", "signatures": ["ICLR.cc/2026/Conference/Submission13650/Reviewer_hVJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13650/Reviewer_hVJb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720807029, "cdate": 1761720807029, "tmdate": 1762924223717, "mdate": 1762924223717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NOICE, a fine-tuning attack that teaches models a \"refuse-then-comply\" behavior to bypass safety measures. The authors frame this as a \"deep\" attack, in contrast to \"shallow\" methods that are more easily defended. The method's effectiveness is demonstrated with high attack success rates on state-of-the-art models, including GPT-4o and Claude Haiku, and the vulnerability was validated by OpenAI."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Strong Empirical Results:** The work's primary strength is its demonstration of a high-efficacy attack on current, production-grade language models.\n2.  **High Real-World Relevance:** The research directly addresses a timely and critical security vulnerability in fine-tuning APIs, supported by responsible disclosure."}, "weaknesses": {"value": "1.  **Clarity of Novelty and Framing:** The paper presents NOICE as a \"new attack paradigm.\" However, given that the authors explicitly cite \"successful pre-filling attacks\" as their inspiration (line 202), the paper could better articulate how its contribution represents a paradigm shift rather than a novel and highly effective evolution of existing concepts.\n\n2.  **Incomplete Evaluation Methodology:** The evaluation framework has two significant gaps. First, the assessment of response harmfulness relies heavily on an LLM-as-a-judge, with an opaque description of the human validation process. Second, the evaluation omits the attack's impact on the model's general utility, leaving the performance trade-off unmeasured.\n\n3.  **Analysis Lacks Mechanistic Depth:** The paper successfully demonstrates that the attack works but offers little insight into why. The underlying generalization mechanism that allows the model to transfer the \"refuse-then-comply\" behavior from benign training examples to harmful prompts remains unexplored."}, "questions": {"value": "1.  Could you provide a formal, falsifiable definition for a \"deep\" attack? In light of the cited inspiration from pre-filling attacks, could you further clarify the primary conceptual leap that qualifies NOICE as a new paradigm?\n2.  To increase confidence in the evaluation, could you provide key metrics from your human validation process, such as the sample size, inter-annotator agreement, and the protocol for handling human-LLM disagreements?\n3.  Did you measure the performance of the NOICE-tuned models on standard benchmarks (e.g., MMLU, GSM8K) to assess the impact on general utility? If so, what were the results?\n4.  What is your hypothesis for the generalization mechanism at play? Why does the model so effectively transfer this structured response pattern from harmless to harmful domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "08Gu7V4slg", "forum": "QzIQgloYgX", "replyto": "QzIQgloYgX", "signatures": ["ICLR.cc/2026/Conference/Submission13650/Reviewer_niFV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13650/Reviewer_niFV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909081758, "cdate": 1761909081758, "tmdate": 1762924223404, "mdate": 1762924223404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel fine-tuning attack—NOICE—that teaches language models to first refuse harmful requests and then comply, bypassing token-level safety mechanisms. It significantly advances the threat landscape by revealing the limitations of shallow defenses. The methodology is sound and resource-efficient, with empirical validation on both open-source and production models like GPT-4o and Claude Haiku. The writing is clear and well-structured, and the findings carry real-world significance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Here are the strengths of the paper:\n\nOriginality: Introduces a novel “refuse-then-comply” attack paradigm (NOICE) that reveals deeper vulnerabilities in safety-aligned models.\n\nQuality: Demonstrates strong empirical results across multiple open- and closed-source models with low-cost fine-tuning and robust comparisons.\n\nSignificance: Highlights a real-world threat acknowledged by OpenAI and Anthropic, with practical implications for model safety and defense design."}, "weaknesses": {"value": "Here are the weakness of the paper:\n\nNarrow defense evaluation: The paper focuses mainly on token-level defenses like AMD and Llama-Guard, without testing against more robust, layered or semantic-based safety mechanisms.\n\n\nOverreliance on model-based evaluation: Harmfulness is judged primarily using GPT-4o, which may introduce circularity or limit interpretability of the results.\n\nLack of mitigation strategies: While the attack is well-demonstrated, the paper does not propose or evaluate defenses tailored to the deeper attack strategy it introduces."}, "questions": {"value": "Could NOICE be mitigated by detecting refusal-followed-by-compliance patterns during inference? \n\nHave you explored any methods for flagging or interrupting this behavior?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The paper introduces a powerful attack (NOICE) that enables models to generate harmful content while appearing safe, including outputs that promote illegal activity or hate speech. Although the authors follow responsible disclosure practices, publishing detailed attack methods could aid malicious actors."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N00hCQTvlR", "forum": "QzIQgloYgX", "replyto": "QzIQgloYgX", "signatures": ["ICLR.cc/2026/Conference/Submission13650/Reviewer_2w5y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13650/Reviewer_2w5y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017336062, "cdate": 1762017336062, "tmdate": 1762924223086, "mdate": 1762924223086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}