{"id": "YsruHaTNYS", "number": 6353, "cdate": 1757972403096, "mdate": 1759897920276, "content": {"title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers a solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose **FedNano**, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to access or modify the LLM on clients, reducing client-side storage by **95%** and communication overhead to just **0.01%** of model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems.", "tldr": "FedNano centralizes the LLM on the server while using lightweight client-side adapters, achieving superior performance and efficiency for federated multimodal learning.", "keywords": ["federated learning", "multimodal learning", "vision question answering", "multimodal QA"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3fca020ab5ff7a160f03ce6eefe0a9898eacdebe.pdf", "supplementary_material": "/attachment/f52159da82da7e273d557db3a5e34155272a6af3.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose FedNano, a framework designed to address the challenge of training multimodal large language models (MLLMs) under data privacy constraints within a federated learning (FL) setting. In this approach, the large language model (LLM) is centralized on the server, while each client independently trains lightweight LoRA modules for its own modalities. This design effectively alleviates communication overhead and the limited computational capacity of clients. During server-side aggregation, FedNano employs a Fisher Merging strategy to efficiently integrate LoRA updates from different clients with the central LLM. Experimental results on visual question answering (VQA) tasks demonstrate that this method achieves effective federated multimodal learning while preserving data privacy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The combination of MLLMs and FL is a topic of practical value."}, "weaknesses": {"value": "1. The main innovation claimed in the paper has already been explored in reference [a]. The authors state in the abstract that their work is “the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation,” but this claim is inaccurate. Reference [a] adopts a similar approach, centralizing the LLM on the server and even pretraining it with public data to fully leverage server-side computational resources.\n\n[a] MLLM-LLaVA-FL: Multimodal Large Language Model Assisted Federated Learning, WACV, 2025.\n\n2. There are already many lightweight LLMs that can be deployed directly on mobile devices, so the authors’ assertion that the LLM must reside exclusively on the server is not entirely justified.\n\n3. The proposed method also has conceptual limitations. If the LoRA-like adapters on clients are trained solely on local data without aligning or integrating with the LLM weights, and are only merged later through Fisher merging, the performance is likely to be suboptimal. The limited improvement observed in the experimental results compared with other methods seems to support this concern.\n\n4. Although the authors emphasize that their method mainly reduces communication and computation costs, they do not provide quantitative experiments or evidence to substantiate these claims, which should be an essential part of the evaluation."}, "questions": {"value": "The main issue is that the authors’ claim of being the first to deploy the LLM solely on the server is incorrect, as prior work [a] has already adopted this approach and trained only the other modality encoders, which is more reasonable than training separate adapters for the LLM. Moreover, the experimental performance is mediocre, and there is no comparison or discussion of computational and communication costs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Hfxm2ql2Wc", "forum": "YsruHaTNYS", "replyto": "YsruHaTNYS", "signatures": ["ICLR.cc/2026/Conference/Submission6353/Reviewer_B9BP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6353/Reviewer_B9BP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812475021, "cdate": 1761812475021, "tmdate": 1762918644696, "mdate": 1762918644696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FedNano proposes a practical way to fine-tune multimodal LLMs in federated settings by keeping the large LLM frozen on the server and pushing only a lightweight client module, “NanoEdge,” which uses modality-specific encoders/connectors plus low-rank NanoAdapters for tuning. This design removes the need to deploy the full LLM on devices, cutting client storage by ~96% and slashing communication to ~0.01% of model parameters (≈99% fewer transmitted parameters than PEFT-style FL). To cope with non-IID client data, FedNano aggregates updates with Fisher-guided merging (diagonal FIM), improving robustness versus FedAvg/FedProx/FedDPA-F. On ScienceQA and IconQA (with MiniGPT-4 and LLaVA-1.5 backbones), it achieves the best average FL performance, scales to more clients, and generalizes across different VQA tasks—all while remaining communication- and compute-efficient."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. A new FL architecture for MLLMs that keeps the LLM frozen on the server and lets clients adapt via a lightweight “NanoEdge,” cutting client storage by >95%.\n2. Communication-efficient adaptation using low-rank NanoAdapters, reducing transmitted parameters > 99%."}, "weaknesses": {"value": "1. The experiments results looks weird to the reviewer, why it looks random as you change the number of clients. \n2. The paper is basically combining several techniques which leads a lack of novelty. But the final pipeline is practical so this does not look like a big issue to the reviewer."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UW0ue1VdzR", "forum": "YsruHaTNYS", "replyto": "YsruHaTNYS", "signatures": ["ICLR.cc/2026/Conference/Submission6353/Reviewer_2c4M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6353/Reviewer_2c4M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858002156, "cdate": 1761858002156, "tmdate": 1762918644036, "mdate": 1762918644036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedNano, a federated learning (FL) framework for multimodal large language models (MLLMs). Instead of deploying full LLMs on clients, FedNano centralizes the LLM on the server and introduces NanoEdge, a lightweight client module composed of modality-specific encoders and LoRA-based adapters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an important and timely problem—enabling scalable FL for large multimodal models under resource constraints."}, "weaknesses": {"value": "1.While the paper addresses an important problem, its novelty claim is somewhat overstated. The idea of centralizing the LLM on the server while training lightweight client modules is not new—for instance, MLLM-LLaVA-FL[ref1] already adopts a similar design. The paper should clarify how its approach differs from such prior work.\n\n2. Since the client-side adapters are trained locally without interacting with or being guided by the LLM’s frozen weights, it is unclear whether the learned representations remain aligned with the global model semantics. This weak coupling makes the subsequent Fisher-based merging on the server seem heuristic rather than principled.\n\n3. The improvements in model performance are modest, particularly in Table 2.\n\n[ref1] MLLM-LLaVA-FL: Multimodal Large Language Model Assisted Federated Learning. WACV 2025."}, "questions": {"value": "1.Why is Fisher information an appropriate choice for merging NanoAdapters across clients? Are there empirical comparisons with other merging strategies?\n\n2.Have you analyzed failure cases, or tasks where FedNano underperforms compared to full-model FL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VCnB5OIcQ4", "forum": "YsruHaTNYS", "replyto": "YsruHaTNYS", "signatures": ["ICLR.cc/2026/Conference/Submission6353/Reviewer_GqCF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6353/Reviewer_GqCF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919338339, "cdate": 1761919338339, "tmdate": 1762918643454, "mdate": 1762918643454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of federated learning for large multimodal models (MLLMs), where deploying full LLMs on clients is infeasible due to computational and communication constraints. The authors propose FedNano, a FL framework that centralizes the LLM on the server (frozen) and deploys lightweight, modality-specific NanoAdapters on clients. Each client updates only these low-rank adapters, while server-side aggregation is guided by Fisher Information-based merging, addressing data heterogeneity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper proposes the first FL paradigm that avoids placing LLMs on clients while enabling collaborative tuning for MLLMs, which is both practically motivated and technically interesting.\n2.\tThis work quantitatively reduces client parameters and communication volume by 95%+ and 99%+, respectively. The improvements are clearly reported.\n3.\tDespite limited trainable parameters, FedNano achieves higher or comparable accuracy to heavier baselines across datasets, backbones, and client numbers."}, "weaknesses": {"value": "1.\tAlthough this work has a clear modular architecture (LLM on server + NanoEdge on clients). Key design choices such as the exact placement of NanoAdapters, adapter architecture (rank, dimension per modality), and training pipeline (forward/backward interaction with server LLM) remain insufficiently described for full reproducibility.\n2.\tThe Fisher-guided merging is claimed as a core innovation, however, there is no explicit ablation of “FedNano w/o Fisher” vs “with Fisher”, making it unclear how much performance gain comes from NanoAdapter vs aggregation strategy.\n3.\tDespite the significant 99.4% reduction in communicated parameters, FedNano still outperforms FedDPA-F in accuracy, which is counterintuitive. Although the authors provide some empirical explanations in the experimental analysis, these justifications lack theoretical grounding and solid evidence, making the conclusion insufficiently convincing.\n4.\tExperiments focus only on VQA (ScienceQA, IconQA). It remains unclear whether FedNano generalizes to other multimodal tasks such as captioning, retrieval, grounding, or dialog."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XfAKip9bq0", "forum": "YsruHaTNYS", "replyto": "YsruHaTNYS", "signatures": ["ICLR.cc/2026/Conference/Submission6353/Reviewer_jTaq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6353/Reviewer_jTaq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762477386935, "cdate": 1762477386935, "tmdate": 1762918643070, "mdate": 1762918643070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}