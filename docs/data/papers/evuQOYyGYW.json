{"id": "evuQOYyGYW", "number": 18048, "cdate": 1758283255943, "mdate": 1759897136477, "content": {"title": "Orthogonal Drift Correction (ODC): Improving Semantic Alignment via Training-Free Embedding Refinement", "abstract": "Text-to-image models have achieved remarkable success in generating high-quality images from textual descriptions. However, they often struggle with \"semantic drift,\" where the generated output fails to align perfectly with complex or nuanced text prompts. In this paper, we introduce Orthogonal Drift Correction (ODC), a novel, inference-time guidance technique designed to mitigate semantic drift without requiring any model retraining. ODC guides the image generation process through a two-stage mechanism. It first generates an initial image, then uses a pre-trained vision-language model to compute a semantic error vector between this image and the prompt. Next, we isolate the component of this error vector that is orthogonal to the prompt's direction, hypothesizing that this component represents the most detrimental, off-topic drift. By subtracting this orthogonal error vector, we create a refined conditioning vector for a second, corrected generation pass. Our experiments demonstrate that ODC significantly enhances prompt-image alignment, leading to images that more accurately reflect detailed compositional and attribute-based instructions. As a plug-and-play module, ODC offers a practical and computationally efficient method for improving the reliability of state-of-the-art text-to-image models.", "tldr": "", "keywords": ["inference-time guidance", "prompt-image alignment", "training-free", "diffusion models", "text-to-image generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7787ec25a90eb720a34372a85e0e70fb1a06c98e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors address semantic misalignment in text-to-image (T2I) models by guiding the generation process with a \"semantic error vector.\" This vector is derived from an initial inference pass where the generated image and original prompt are fed into a Vision-Language Model (VLM). The method then isolates the component of this error vector that is orthogonal to the prompt's direction, using it to correct the subsequent generation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors tackle the critical problem of image-text misalignment. They introduce a novel and interesting approach that leverages the semantic knowledge of a VLM to identify an error in the embedding space. Using this error representation to improve prompt alignment is a promising research direction."}, "weaknesses": {"value": "The method's validation is insufficient. The only baselines used are a negative prompt, standard Classifier-Free Guidance (CFG), and vanilla generation. A significant omission, especially since the method incorporates a VLM, is a simple baseline of using the VLM to suggest a better prompt to fix the error. \n\nFurthermore, a whole class of solutions for this problem is missing from both the related work and the baseline comparisons. This includes attention-based methods designed to fix semantic errors, such as Attend-and-Excite [1], and layout-conditioning methods like RAG-Diffusion [2].\n\nMoreover, the evaluation relies on CLIP and BLIP scores, which are widely known to perform poorly on such nuanced tasks. These metrics often fail to detect subtle (and even overt) misalignments in attribute binding, spatial relationships, and object composition.\n\nFinally, the reported performance gains are marginal. For instance, the CLIP score on Vanilla FLUX only increases from 29.297 to 30.292 using the proposed method. I suspect this slim margin is not statistically significant.\n\n[1] Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models (Chefer et al. 2023)\n\n[2] Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement (Chen et al. 2024)"}, "questions": {"value": "1. What is the final size of the Parti dataset after you apply your filter?\n\n2. Why did you filter for prompts longer than 10 tokens? This filtering choice is not justified.\n\n3. There are no qualitative results for the FLUX model shown, making it impossible to visually assess the method's impact on this state-of-the-art generator. Why were these omitted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mQ3vYeAKuN", "forum": "evuQOYyGYW", "replyto": "evuQOYyGYW", "signatures": ["ICLR.cc/2026/Conference/Submission18048/Reviewer_qYRt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18048/Reviewer_qYRt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761406328587, "cdate": 1761406328587, "tmdate": 1762927838769, "mdate": 1762927838769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Orthogonal Drift Correction (ODC), a novel training-free method to correct semantic drift in text-to-image (T2I) models. ODC identifies and removes the orthogonal component of semantic error in the text embedding space using a two-stage process: it first generates an image, computes the semantic drift via a Vision-Language Model (VLM), and then refines the text embedding by removing the orthogonal ”off-topic” component before regenerating the image. Experiments on SDXL and FLUX.1 show improved prompt-image alignment without fine-tuning or architectural changes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novel and Intuitive Approach: The idea of decomposing and removing the orthogonal semantic drift is innovative and well-motivated.\n\nTraining-Free and Model-Agnostic: ODC works with pretrained models without fine-tuning, making it easily applicable."}, "weaknesses": {"value": "Dependence on VLM Capability: The correction quality is limited by the VLM’s ability to detect semantic errors.\n\nIncrease Inference Time: The two-stage generation doubles latency, which may hinder real-time use.\n\nCannot Inject New Knowledge: ODC only rearranges or suppresses existing concepts in the model; it cannot teach the model new ones."}, "questions": {"value": "1.We all know that the generation process has a degree of randomness and is influenced by the random seed. Does a single image have sufficient representativeness in the VLM's embedding space?\n\n2.If the Initial Image Generation is only used to determine the V_{text} in the embedding space, can a different model be used for it compared to the Final Image Generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SkRvdjfyBj", "forum": "evuQOYyGYW", "replyto": "evuQOYyGYW", "signatures": ["ICLR.cc/2026/Conference/Submission18048/Reviewer_UB8o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18048/Reviewer_UB8o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559051322, "cdate": 1761559051322, "tmdate": 1762927838272, "mdate": 1762927838272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the problem of semantic drift in text-to-image diffusion models, where the generated image fails to semantically align with the input prompt. They propose a plug-and-play, training-free method called Orthogonal Drift Correction (ODC). The key idea is to perform two-stage generation to remove components in the initial text embedding that are orthogonal to the true semantic direction of the prompt, thereby enhancing text adherence in existing T2I models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is generally well-structured and complete.\n2. The proposed method of removing orthogonal components from the input text embeddings provides a insightful perspective on mitigating semantic drift."}, "weaknesses": {"value": "1. The authors assume that semantic drift originates from the initial text embeddings, whereas some prior works[1] attribute it to the initial latent noise. The paper does not provide an explanation or experimental analysis to justify this assumption. In addition, the proposed approach relies on a VLM for identifying orthogonal components, which could itself introduce additional bias, and the method’s effectiveness may depend heavily on the capability of the chosen VLM.\n2. The experiments are not sufficiently comprehensive. Since the task focuses on semantic alignment in T2I generation, the authors should include comparisons with stronger baselines[2], such as CONFORM[3], Attend-and-Excite[4], and other recent methods that explicitly address alignment. Moreover, semantic alignment is a broad concept that also includes attribute binding, counting, and spatial relation tasks. The paper would benefit from additional benchmarks[4,5] covering these aspects to better demonstrate the effectiveness of the proposed method. Ablation studies are also limited — since the method depends on a VLM, experiments using different VLMs would help analyze robustness and dependency.\n3. The proposed method requires two generations, which increases inference time, but the authors do not provide any discussion or analysis regarding this computational cost.\n4. Although experiments are conducted on Flux Schnell, there is no qualitative visualization or analysis for this model. In the quantitative results, the improvement over the vanilla baseline appears relatively minor, raising questions about the practical effectiveness of the method.\n5. In terms of writing, the method section and workflow image are not clearly presented, and several equations seem redundant or overly detailed.\n6. It would be valuable for the authors to show examples of the retrieved vocabulary obtained from the orthogonal projection for each prompt, as this could help clarify the model’s behavior and provide qualitative insight into what the correction actually removes.\n\n[1]: InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization\n\n[2]: Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis\n\n[3]: CONFORM: Contrast is All You Need For High-Fidelity Text-to-Image Diffusion Models\n\n[4]: Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models\n\n[5]: T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation"}, "questions": {"value": "Is there any experimental evidence supporting the hypothesis that semantic drift originates from the initial text embeddings?\nDid the authors perform any intermediate analyses, such as examining the retrieved vocabulary items corresponding to the orthogonal direction?\nAdditional related questions are discussed in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OMbZ1etDyA", "forum": "evuQOYyGYW", "replyto": "evuQOYyGYW", "signatures": ["ICLR.cc/2026/Conference/Submission18048/Reviewer_qk2P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18048/Reviewer_qk2P"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722926572, "cdate": 1761722926572, "tmdate": 1762927837642, "mdate": 1762927837642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The generated image could be misaligned with the text prompt semantically therefore this paper introduced Orthogonal Drift Correction (ODC), an training-free method that first using vision-language model to compute the semantic error vector and then further isolate the semantic error vector, so the original prompt could be modified based on this to improve the performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea is simple and easy to implement. \n2. They presented some examples where the performance is improved on the semantic level.\n3. The method is clearly illustrated with both algorithm and section text."}, "weaknesses": {"value": "1. The writing needs improvement. For example, Step 1 in the method is just regular image generation process, which could be merged with other parts. Figure 1 has two stages but the method part never mention that but in 4 steps.\n\n2. Results presentation are inadequate. For example, only 6 pairs are presented in the main paper and 9 more examples. The author should show more categorized results like in Human, animal, animation, etc.\n\n3.  The same goes to ablation study, where no qualitative examples are presented.\n\n4. No human evaluation is presented."}, "questions": {"value": "1. After improved semantic alignment, does it also help other aspects such as aesthetic score?\n\n2. Does it work on other architecture or even non-diffusion model or diffusion transformer model?\n\n3. \"we observed that while > 99% of energy concentrates in the first component, the actual semantic distinctions distribute across multiple smaller components.\" Could you clarify this with statistic number?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zo6McZtzVe", "forum": "evuQOYyGYW", "replyto": "evuQOYyGYW", "signatures": ["ICLR.cc/2026/Conference/Submission18048/Reviewer_n34s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18048/Reviewer_n34s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023964709, "cdate": 1762023964709, "tmdate": 1762927836672, "mdate": 1762927836672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}