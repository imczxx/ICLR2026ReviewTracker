{"id": "kI27Niy4xY", "number": 5255, "cdate": 1757879178549, "mdate": 1763687887979, "content": {"title": "Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator", "abstract": "The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new possibilities for text-to-3D generation. Intuitively, one could obtain a formidable 3D scene generator if one were able to combine the power of a modern latent text-to-video model as \"generator\" with the geometric abilities of a recent (feedforward) 3D reconstruction system as \"decoder\". We introduce **VIST3A**, a general framework that does just that, addressing two main challenges. First, the two components must be joined in a way that preserves the rich knowledge encoded in their weights. We revisit *model stitching*, i.e., we identify the layer in the 3D decoder that best matches the latent representation produced by the text-to-video generator and stitch the two parts together. That operation requires only a small dataset and no labels. Second, the text-to-video generator must be aligned with the stitched 3D decoder, to ensure that the generated latents are decodable into consistent, perceptually convincing 3D scene geometry. To that end, we adapt *direct reward finetuning*, a popular technique for human preference alignment. We evaluate the proposed VIST3A approach with different video generators and 3D reconstruction models. All tested pairings markedly improve over prior text-to-3D models that output Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also enables high-quality text-to-pointmap generation.", "tldr": "Text-to-3D scene generative modelling by unifying a video generative model with a foundational 3D model via model stitching and alignment.", "keywords": ["Text-to-3D generation", "Video Diffusion Model", "3D Gaussian Splatting", "Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef16ca191297f70968addd43d34c84ab9f1306a1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method for text-guided 3D scene generation. The core idea is to align the intermediate layer features of the video generation model with those of the feedforward 3D reconstruction model, thereby improving the 3D consistency of the video generation model and integrating the two models to complete the generation task. The paper also proposes a post-alignment process to further enhance the overall quality and 3D consistency of the model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of the paper is clear and important. Given that there has been a lot of work on the pre-training phase for 3D scene generation, further optimization and post-training research have become particularly important at this stage.\n2. The logic of the paper is relatively sound. The proposed grafting alignment scheme has universality for subsequent research.\n3. The paper provides detailed comparison and ablation results. In particular, it has achieved sota performance in text-to-3D scene generation."}, "weaknesses": {"value": "1. Although the grafting scheme proposed in the paper is reasonable, it has some core potential issues:\nBecause it relies on the alignment of hidden space features, the video model needs to be fine-tuned using multi-view data. This process may impair the generalization ability of the video model.\nSince the feedforward reconstruction model itself is also trained on multi-view data, its acceptable distribution is limited, which may also lead to a reduction in the generalization ability of the final model.\nMoreover, the capability of the feedforward reconstruction model itself will limit the capability of the entire framework, as they also struggle to successfully model some details (such as grass, branches, etc.). Will this be reflected in the video results of the model?\n2. The paper lacks qualitative ablation studies on the alignment step. What puzzles me is that the qualitative results presented in the paper seem to have a tendency of over-saturation. Is this caused by the alignment step? The multi-view data used in the training itself are all from real scenes."}, "questions": {"value": "Please see the questions in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3hwoCYNOHn", "forum": "kI27Niy4xY", "replyto": "kI27Niy4xY", "signatures": ["ICLR.cc/2026/Conference/Submission5255/Reviewer_yob1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5255/Reviewer_yob1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761014826550, "cdate": 1761014826550, "tmdate": 1762917974694, "mdate": 1762917974694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Inquiry: video decoder during training pipeline"}, "comment": {"value": "I appreciate the reasonable motivation, novel technique and insightful designs in this paper. I raise this comment as I am confused by how the video decoder is used during the training pipeline. As seen in Figure 3, the video DiT provides a latent. This latent is further fed into the stitched reconstruction model and the video decoder, which are used to calculate the 3D consistency reward. This thinking behind is novel and solid, but I am confused by how training pipeline proceeds:\n\n1. Do you feed a noisy video latent into the video decoder? Commonly, multiple denoising steps are proceeded during inferencing a video diffusion. In the proposed training pipeline, it seems that latent is processed by the DiT once, yielding a noisy latent with only one step denoise process. How does the video decoder decode a valid video from this noisy latent?\n\n2. If not, do you denoise multiple steps to get the clean video latent during training? This raises a concern on the training efficiency. What is the time cost for a full denoising during training?\n\nThanks so much for your time and patience! I have learned a lot from your excellent work and this question really confuses me. I believe your insightful reply helps with our better understanding."}}, "id": "PTDFfdw9bK", "forum": "kI27Niy4xY", "replyto": "kI27Niy4xY", "signatures": ["~Dany_Li1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Dany_Li1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5255/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763641011408, "cdate": 1763641011408, "tmdate": 1763641011408, "mdate": 1763641011408, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an approach to replace the decoder in a video diffusion model with a pre-trained feed-forward 3D regression network, such as one that can perform pixel-oriented Gaussian splatting or 3D point map prediction. The resulting model can then generate 3D outputs directly, for instance from textual prompts.\n\nThe method repurposes earlier techniques for networks stitching. The starting point is a video generator based on latent diffusion. This uses a variational autoencoder to encode and decode RGB frames into a latent space. The idea is to \"stitch\" (by means of a linear adapter) the given pre-trained 3D regressor as a decoder replacement, thus obtaining a new decoder that outputs a 3D representation of the generated scene instead of raw pixels. Stitching identifies the subset of layers in the regressor that are best replaced by the encoder, selecting this as a stitching point. Notably, the resulting encoded+stitched decoder can also function as a replacement for the original regressor, and in some cases outperforms it.\n\nGiven the new encoder-decoder, the diffusion model is then jointly fine-tuned with it. This step is necessary to ensure the old diffusion model and new encoder-decoder work well together. This step uses direct reward fine-tuning, where the diffusion process is unrolled and the output is assessed using image-based metrics, which are then optimised."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The idea of using network stitching to add a deterministic 3D regression head to a multi-view image generator is creative, practical, and, empirically, quite effective. I think it is definitely worth sharing it with the community.\n\n* The paper also proposes a practical manner to re-adjust the diffusion process in the generator to adapt it to the new encoder-3D-decoder. The latter uses reward fine-tuning."}, "weaknesses": {"value": "* Perhaps with the exception of direct reward fine-tuning, the model is not really learning a 3D latent space, but rather to decode a multi-view latent space to 3D directly. While multi-view is close to 3D, it may miss some important properties that a native 3D latent space may capture better, the most obvious one being extent. By this, I mean that the part of the 3D world which is reconstructed is commensurate to the part of the 3D world captured in the multiple views, which in general do not provide full coverage and may leave the final 3D reconstruction incomplete. This is pretty obvious from the “holes” in figure 1.\n\n* It is not clear to me how the underlying video generator decides to select the viewpoints that are (implicitly) generated. Likewise, it is not clear what happens with dynamic contents.\n\n* Eq. (2) measures the similarity of feature alignment to decide where to cut. However, the experiments in Section 4.3 suggest the obvious alternative to measure the quality of the new VAE in terms of 3D reconstruction in order to perform this selection. Why is the first option preferred?\n\n* I was a little surprised to notice that there is no supplementary material. Videos of the results would have gone a long way in illustrating their quality."}, "questions": {"value": "* See above, what are the limitations of using a multi-view latent space instead of learning a “proper” 3D one?\n\n* How do the various image generators choose which viewpoints to (implicitly) consider for generation?\n\n* How do you handle potential dynamic objects in the generated scene?\n\n* Will code be released?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4RGwevzscB", "forum": "kI27Niy4xY", "replyto": "kI27Niy4xY", "signatures": ["ICLR.cc/2026/Conference/Submission5255/Reviewer_3u1z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5255/Reviewer_3u1z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761321116360, "cdate": 1761321116360, "tmdate": 1762917974378, "mdate": 1762917974378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VIST3A, a framework for text-to-3D scene generation that combines the power of modern latent text-to-video models with geometric abilities of recent feedforward 3D reconstruction models through a model stitching mechanism. The authors first identify the layer in the 3D reconstruction model that best matches the latent representation produced by the encoder of video VAE, and stitch the two parts via a lightweight stitching layer. Then, they apply direct reward finetuning to align the generative model's latent distribution with the stitched 3D decoder. VIST3A achieves strong quantitative and qualitative results on multiple benchmarks, outperforming recent text-to-3D Gaussian splatting models. VIST3A also exhibits promising qualitative performance on text-to-pointmap generation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Conceptual originality: first to demonstrate high-fidelity text-to-3D generation by connecting video generator and feedforward 3D reconstructor through model stitching.\n* Efficient training: the framework uses self-supervised alignment and reward tuning, reducing data requirements.\n* Modular and generalizable: the authors demonstrate the framework's feasibility across multiple video generators and 3D reconstructors, showing the generalizability of proposed method.\n* Superior experimental performance: VIST3A achieves superior scores on diverse benchmarks along with fancy qualitative results, exhibiting stable outperforming compared to baseline methods."}, "weaknesses": {"value": "* Limited theoretical justification: while empirically compelling, the choice of layer for stitching is primarily heuristic (minimizing MSE) and lacks deeper analysis of representation compatibility.\n* Concerns about reward finetuning complexity: DRT requires full denoising loop and multi-view rendering for each step, introducing heavy computational overhead. The scalability to larger video models or higher resolutions may be challenging.\n* No human evaluation: subjective realism and prompt alignment are only indirectly assessed via CLIP/HPSv2 metrics."}, "questions": {"value": "* It is mentioned in line 261 that \"As we keep the encoder frozen, the generated latents can be decoded by the original video decoder to obtain multi-view images\", however, the latents produced by the diffusion model would have changed along the reward tuning, how could the latents be constantly decoded to multi-view images using the original decoder? \n* There seems lack descriptions for several notations in their first appearance. For example, $D_E$ and $D_F$, which are the dimensions of latents $B$ and activations $A$ respectively from line 223."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JraVtkvqQZ", "forum": "kI27Niy4xY", "replyto": "kI27Niy4xY", "signatures": ["ICLR.cc/2026/Conference/Submission5255/Reviewer_HSYK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5255/Reviewer_HSYK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567145997, "cdate": 1761567145997, "tmdate": 1762917974083, "mdate": 1762917974083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VIST3A, a novel and effective framework for text-to-3D generation. Its core contribution is a model stitching approach that seamlessly connects a pre-trained text-to-video generator to a powerful, pre-trained feedforward 3D reconstruction model, repurposing the latter as a strong 3D VAE decoder. This design elegantly leverages the geometric prior of state-of-the-art 3D vision models without requiring their expensive replication through large-scale training. To ensure alignment between the generative model and the stitched decoder, the authors further introduce a direct reward fine-tuning strategy. This technique aligns the denoising process to produce latents that are both 3D-consistent and within the input domain of the decoder. The experiments demonstrate superior performance compared with baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and generalizable framework. The proposed model stitching method is a key innovation. It provides a cost-effective way to harness the power of pre-trained 3D vision models as strong decoders, eliminating the need for expensive training from scratch. Its generality is demonstrated by successfully combining different video encoders with different 3D models, enabling the generation of both 3D Gaussian splats and pointmaps.\n2. Thorough experiments. The paper provides compelling evidence through extensive experiments. It robustly benchmarks multiple model combinations against strong baselines. It also includes well-designed ablations that critically validate core design choices, specifically the stitching layer selection and the contribution of direct reward fine-tuning."}, "weaknesses": {"value": "While the stitching itself is data-efficient, the subsequent direct reward fine-tuning is computationally intensive. It requires unfolding the entire denoising trajectory and backpropagating through multiple decoders and reward models, which is costly in both time and GPU memory."}, "questions": {"value": "1. The paper demonstrates successful stitching across various model pairs. Could you elaborate on the scenarios or conditions where your stitching approach might fail? For instance, have you encountered pairs of video VAEs and 3D models where no suitable stitching layer could be found, and what might be the underlying reasons (e.g., architectural mismatch, vastly different representation spaces)?\n2. The performance of VIST3A is inherently tied to the capabilities of the chosen 3D foundation model. What are the key properties or criteria you considered when selecting a model like AnySplat or VGGT for stitching? For instance, how do the specific 3D representations (Gaussians vs. pointmaps), architectural design, or training data of the foundation model influence the final text-to-3D capabilities of VIST3A, and are these selection criteria generalizable to future models?(Note: This question seeks your insight and perspective, not additional experiments.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X2nkOpga0f", "forum": "kI27Niy4xY", "replyto": "kI27Niy4xY", "signatures": ["ICLR.cc/2026/Conference/Submission5255/Reviewer_BZA7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5255/Reviewer_BZA7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654529297, "cdate": 1761654529297, "tmdate": 1762917973775, "mdate": 1762917973775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}