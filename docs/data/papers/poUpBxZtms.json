{"id": "poUpBxZtms", "number": 4939, "cdate": 1757815296959, "mdate": 1759898004032, "content": {"title": "OSAQ: Outlier Self-Absorption for Accurate Low-bit LLM Quantization", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generation tasks. However, their massive parameter scale leads to significant resource consumption and latency during inference. Post-training weight-only quantization offers a promising solution by reducing model size and accelerating token generation through alleviating the memory-bound issue. Nevertheless, there are inherent systematic outliers in weights, and although some efforts have attempted to address them, such as scaling and rotation, the performance of low-bit quantization remains far from satisfactory. In this paper, we propose Outlier Self-Absorption Quantization (OSAQ), which performs second-order low-rank derived additive weight suppression for low-bit weight-only LLM quantization. Specifically, we observe that Hessian exhibits low-rank consistency across different inputs, with certain directions persistently lacking strength. Leveraging this property, we construct an additive weight transformation based on the Hessian‚Äôs null space, thereby suppressing weight outliers without affecting the task loss. This additive transformation can be absorbed into the weights offline, requiring no inter-layer transformations and introducing no inference overhead. Moreover, the construction is efficiently achieved by a closed-form solution, without resource-intensive training or iterative procedures. Extensive experiments across models of varying scales and tasks are conducted, and the results show that OSAQ effectively suppresses outliers and improves low-bit quantization performance.", "tldr": "", "keywords": ["Large language models", "post-training quantization", "outlier supperssion", "Hessian"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3edae8eb01fc8768c30de8529360dcd35c762983.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a method to mitigate outliers in the weights of LLMs. They first observe that the null space of the loss Hessian is relatively stable. Building on this, they add $\\Delta W$ within that null space to minimize $l-\\infty$ of $W+\\Delta W$. They further approximate the objective utilizing the Softmax-$\\infty$ Objective Appoximation and derive a closed-form solution. Experiments verify its effectiveness on the LLaMA-series models in terms of PPL, QA tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and strong, and the proposed method is established based on the observation.\n2. The paper is well-organized overall, making it easy to follow.\n3. The experiment and visualization verify the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The settings and detailed visualization procedure used in Figure 1(b) remain unclear. The authors should elaborate to clarify the observation. The norm length appears odd in the 2D representation.\n2. It seems somewhat contradictory that the null space of  $H$ is obtained via the null space $X^TX$, yet the authors claim that the null space of $X$ is not consistent in Figure 1(b). \n3. Sensitivity/robustness of approximation to obtain the null space with using approximation method or not ($X^TX$ vs $H$), the threshold, the size of the calibration set, and/or the chosen subspace dimension should be reported.\n4. The gains appear limited. Across models and bit widths, the improvements are not consistently stable or significant.\n5. The authors mentioned the assessment on MTBench in Line 296, but I cannot find any results regarding it."}, "questions": {"value": "1. Can the proposed method be applied to weight‚Äìactivation quantization settings? Rotations may change inter-layer activation inputs, which could make implementing $W+\\Delta W$ harder.\n2. What is the performance of solely use OSAQ (i.e. OSAQ+RTN) to quantize LLMs? To what extent does the method rely on other quantization methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T1JxPE0b45", "forum": "poUpBxZtms", "replyto": "poUpBxZtms", "signatures": ["ICLR.cc/2026/Conference/Submission4939/Reviewer_PzeR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4939/Reviewer_PzeR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761373512873, "cdate": 1761373512873, "tmdate": 1762917781211, "mdate": 1762917781211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses LLMs‚Äô high inference resource use and latency via post-training weight-only quantization, tackling the key obstacle of weight outliers (poorly handled by existing scaling/rotation methods). It proposes OSAQ, which uses Hessian‚Äôs low-rank consistency across inputs to identify a stable null space. By linearly combining null-space vectors, OSAQ builds an additive weight transformation that suppresses outliers without task loss, absorbs offline (no inference overhead), and uses a closed-form solution (no heavy training). Experiments show OSAQ boosts low-bit quantization‚Äîe.g., 2-bit OSAQ+GPTQ cuts perplexity over 40% vs. vanilla GPTQ."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Starting from the perspective of weights, the authors perturb the weights using addition while maintaining an approximate invariance of the output, thereby smoothing the weight distribution. Their method is compatible with weight-calibrating approaches such as GPTQ, and the authors have verified the effectiveness of the method through extensive experiments.\n2. The authors' method does not introduce any additional inference overhead, and only incurs 10% to 20% overhead during the calibration process."}, "weaknesses": {"value": "I believe the authors' work is very rigorous and has no obvious weaknesses."}, "questions": {"value": "1. Rotation smooths the distribution by transferring outliers from one channel to other channels, so I am confused why the authors claim that rotation cannot effectively eliminate outliersÔºü\n2. How does the authors' method perform in scenarios where activation values are also quantized?\n3. Can the authors' method be combined with methods based on scale and rotation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JCKO6yLkuK", "forum": "poUpBxZtms", "replyto": "poUpBxZtms", "signatures": ["ICLR.cc/2026/Conference/Submission4939/Reviewer_sBRH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4939/Reviewer_sBRH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447856932, "cdate": 1761447856932, "tmdate": 1762917780261, "mdate": 1762917780261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes OSAQ, an additive outlier-suppression scheme guided by the Hessian null space, with a closed-form coefficient solver via a Softmax-‚àû surrogate. The update is absorbed into weights and adds no inference cost, complementing scaling/rotation methods. Broad experiments on LLaMA-2/3 and instruction-tuned 123B/405B show notable low-bit gains, especially at 3-bit and 2-bit."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel additive paradigm grounded in low-rank consistent Hessian; clear second-order loss-invariance rationale.\n2. Closed-form per-channel solution; plug-and-play with GPTQ/AWQ/QuIP; zero inference overhead."}, "weaknesses": {"value": "1. The improvement on the existing methods is incremental.\n\n2. How robust is the ‚Äústable Hessian null space‚Äù assumption across layers, models, and domains? Any quantitative measures (e.g., principal angles) across batches?\n\n3. The article should include more thorough comparisons/combination with recent rotation families (e.g., DuQuant/QuaRot/SpinQuant) under matched settings."}, "questions": {"value": "Please see the weakness.\n\n1. How sensitive is performance to the temperature ùúè? Any heuristic for layer-wise adaptive ùúè?\n\n2. What guided the Softmax-‚àû surrogate choice for ‚Ñì‚àû? Did you try p-norm annealing (e.g., p‚Üí‚àû) or direct max-margin formulations?\n\n3. Ablations for stability of the null space vs. calibration set size (e.g., 64/256/1k samples)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v9hQvKrswo", "forum": "poUpBxZtms", "replyto": "poUpBxZtms", "signatures": ["ICLR.cc/2026/Conference/Submission4939/Reviewer_2RrR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4939/Reviewer_2RrR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988165357, "cdate": 1761988165357, "tmdate": 1762917779905, "mdate": 1762917779905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OSAQ, a Hessian-based additive transformation method for mitigating systematic outliers in low-bit weight quantization of LLMs. The authors claim that by leveraging the low-rank consistency of the Hessian and operating within its null space, one can add a closed-form, inference-free correction term that absorbs outlier effects without retraining or extra computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Extending Hessian-based quantization ideas toward a closed-form additive formulation is conceptually interesting and novel.\n- If the proposed null-space additive transformation indeed preserves task loss and can be absorbed into model weights, it would be a practically appealing solution."}, "weaknesses": {"value": "-  The experiments do not include key state-of-the-art methods addressing outliers, such as QuaRot, DuQuant, and SpinQuant, under a unified evaluation setup.\n-  The numerical stability of the closed-form solution, especially for large layers or block-wise quantization, is not discussed."}, "questions": {"value": "- Under what conditions does the transformation guarantee that the first-order term is zero, ensuring loss invariance? Does this assume the model is at or near a local optimum?\n- If the calibration and inference distributions differ, does the null-space property still hold? Could you bound or quantify the resulting error?\n- What is its relationship with the trace-based selection strategy used in HAWQ-V2, and what are the corresponding advantages or disadvantages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "swHEZl1aXy", "forum": "poUpBxZtms", "replyto": "poUpBxZtms", "signatures": ["ICLR.cc/2026/Conference/Submission4939/Reviewer_A4Q1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4939/Reviewer_A4Q1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990137249, "cdate": 1761990137249, "tmdate": 1762917779599, "mdate": 1762917779599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}