{"id": "qF9WJxvHX8", "number": 8422, "cdate": 1758082673636, "mdate": 1759897785031, "content": {"title": "Guiding Mixture-of-Experts with Temporal Multimodal Interactions", "abstract": "Mixture-of-Experts (MoE) architectures have become pivotal for large-scale multimodal models. However, their routing mechanisms typically overlook the informative, time-varying interaction dynamics between modalities. This limitation hinders expert specialization, as the model cannot explicitly leverage intrinsic modality relationships for effective reasoning. To address this, we propose a novel framework that guides MoE routing using quantified temporal interaction. A multimodal interaction-aware router learns to dispatch tokens to experts based on the nature of their interactions. This dynamic routing encourages experts to acquire generalizable interaction-processing skills rather than merely learning task-specific features. Our framework builds on a new formulation of temporal multimodal interaction dynamics, which are used to guide expert routing. We first demonstrate that these temporal multimodal interactions reveal meaningful patterns across applications, and then show how they can be leveraged to improve both the design and performance of MoE-based models. Comprehensive experiments on challenging multimodal benchmarks validate our approach, demonstrating both enhanced performance and improved interpretability.", "tldr": "We introduced a novel MoE architecture that integrates temporal multimodal interactions into model training.", "keywords": ["Multimodal Interaction", "Mixture-of-Experts", "Transformer"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/456fe44eb067c3b83d056f6eeecef33d409b5868.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents TIME-MoE, a Temporal Interaction-guided Mixture-of-Experts framework that leverages information-theoretic decomposition (RUS) to guide expert routing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel information-theoretic framework for guiding expert routing in multimodal MoE systems."}, "weaknesses": {"value": "The proposed RUS estimation and routing involve substantial computational overhead and complex hyperparameter tuning. Scalability to large-scale multimodal models remains to be shown."}, "questions": {"value": "1. The proposed multi-scale estimator is interesting, but the overall computation involving PID and Sinkhorn alignment still seems heavy, especially for long temporal sequences or high-dimensional inputs. It would be helpful if the authors could comment on the practical computational cost and scalability of TIME-MoE in such settings.\n\n2. The model appears to rely on several threshold and weighting hyperparameters (τ_R, τ_U, τ_S, λ_R, λ_U, λ_S). How sensitive are the results to these choices? Any guidance or heuristics for tuning them would be useful.\n\n3. Most of the experiments are on medium-scale datasets. Have the authors tried (or do they see a clear path to) applying TIME-MoE to larger-scale setups, e.g., vision-language or multimodal LLM tasks?\n\n4. The framework is built on information-theoretic principles, but in practice RUS estimation depends on neural approximations. How stable are these estimations during training, and is there any quantitative sense of their bias or variance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "43xBTTiijn", "forum": "qF9WJxvHX8", "replyto": "qF9WJxvHX8", "signatures": ["ICLR.cc/2026/Conference/Submission8422/Reviewer_sito"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8422/Reviewer_sito"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525873402, "cdate": 1761525873402, "tmdate": 1762920321206, "mdate": 1762920321206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TIME-MoE (Temporal Interaction-guided Mixture of Experts), a novel multimodal architecture that integrates temporal interaction dynamics into the MoE routing process. The core idea is to quantify multimodal interactions (redundancy, uniqueness, synergy – RUS) over time, and use these dynamics to guide expert routing. The authors design a RUS-aware router and corresponding auxiliary losses that encourage experts to specialize in particular interaction types. Empirical results on multiple benchmarks (MIMIC-IV, PAMAP2, WESAD, MOSI, Opportunity) show that TIME-MoE outperforms state-of-the-art fusion and MoE-based baselines (e.g., FuseMoE, I2MoE), achieving both higher predictive accuracy and more interpretable expert activation patterns over temporal dimension."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel theoretical grounding: Extends Partial Information Decomposition (PID) to the temporal domain using directed information, leading to a measurable RUS metric that captures delayed effects.\n2. Principled architecture: The RUS-aware router and auxiliary loss design directly encode theoretical insights into model training.\n3. Empirical generality: Demonstrates improvements across heterogeneous datasets (clinical, affective, activity, physiological).\n4. Interpretability: Expert routing visualizations reveal meaningful alignment with modality interactions (e.g., redundancy between chest and hand motion over the temporal dimension).\n5. Comprehensive evaluation: Includes ablations, sensitivity to RUS sequence length, and comparison with multiple baselines."}, "weaknesses": {"value": "W1. Computing temporal RUS and training the router could be computationally expensive, which may limit scalability for large multimodal models."}, "questions": {"value": "1. How sensitive is TIME-MoE to errors in RUS estimation, especially when temporal dependencies are weak or noisy?\n2. Can the RUS-guided router be learned end-to-end without explicit precomputation of RUS values?\n3. Could the same framework generalize to non-temporal multimodal fusion (e.g., static image–text tasks) or to LLM-scale architectures?\n4. What is the computational overhead compared to FuseMoE and I2MoE in FLOPs and training time?\n5. How does the proposed directed-information-based temporal RUS compare with simpler correlation-based interaction metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Sh5IVcUbY", "forum": "qF9WJxvHX8", "replyto": "qF9WJxvHX8", "signatures": ["ICLR.cc/2026/Conference/Submission8422/Reviewer_z7XH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8422/Reviewer_z7XH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659348839, "cdate": 1761659348839, "tmdate": 1762920320139, "mdate": 1762920320139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the study on Temporal Interaction-guided Mixture-of-Experts (TIME-MoE). This is a modified MoE architecture that explicitly leverages time-varying multimodal interactions to guide expert routing. In conventional MoE routers the tokens are navigated based solely on content similarity, while for some tasks and inputs, the temporal interaction between tokens is crucial. TIME-MoE's router is interaction-aware, it considers the redundancy, uniqueness, and synergy (RUS) between modalities over time when deciding which expert processes which token. By using quantified interaction dynamics as a prior, the model encourages experts to specialize in processing particular interaction patterns (redundant, unique, or synergistic information) rather than only learning task-specific features. \nThe authors testes their methods on various multimodal benchmarks, mostly connected to the medical domain, and found out that their method outperforms alternative MoEs approaches, and also improves interpretability of the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strengths of the research are the following:\n\n1. Novel interaction-aware MoE for multimodal data. The authors integrate an information-theoretic framework with modern deep learning to guide Mixture-of-Experts routing using temporal redundancy, uniqueness, and synergy (RUS). This design yields an MoE that is both temporally and modality-grounded, leading to more specialized and interpretable experts.\n2. Unlike prior approaches that rely on static cross-modal correlations, TIME-MoE explicitly models time-lagged multimodal effects, which are especially important in medical and physiological domains.\n3. Thorough and diverse evaluation. The approach is tested on a broad set of multimodal benchmarks — PAMAP2, MIMIC-IV (IHM/LOS), MOSI, WESAD, and Opportunity — covering various modality mix. The authors validate their mechanism through component ablations and routing-pattern analyses, confirming both effectiveness and interpretability.\n4. Comprehensive ablation studies. Each component (redundancy, uniqueness, synergy guidance) is evaluated separately, demonstrating its distinct contribution to the final performance."}, "weaknesses": {"value": "I would define the following weaknesses:\n\n1. Pairwise-only interaction modeling. The RUS framework focuses on pairwise modality interactions; higher-order (three-way or more) synergies are not explicitly modeled. While this ensures scalability, it limits the expressiveness of the model.\n2. Although the authors claim a τ-fold speedup in RUS computation and improved parameter efficiency, the paper does not provide GPU hours, or memory-usage comparisons. Including such measurements would make the efficiency claims more convincing.\n3. Integration with large-scale LLM-MoE frameworks remains unexplored. Given the rapid development of Large Language Model MoEs (e.g., Mixtral, DeepSeek-MoE, etc.), it would be highly valuable to discuss how TIME-MoE could scale to these architectures. The paper does not explore or speculate on integrating its RUS-guided routing into LLM-scale systems, which could significantly broaden its impact.\n4. While Appendix E describes architecture and hyperparameters, and Appendix D details the RUS estimation algorithms, the paper lacks a concise description of the training setup (optimizer, learning rate, batch size, epochs, etc.)."}, "questions": {"value": "1. Please clarify the optimizer, learning-rate schedule, batch size, number of epochs, and gating top-k used during TIME-MoE training. Were modality encoders frozen or fine-tuned?\n2. Is RUS estimation performed as a fully offline pre-processing step (aligned with train/validation/test splits), or is there any joint optimization of the RUS estimator with TIME-MoE training?\n3. Did you observe any expert collapse (e.g., one expert processing the majority of tokens)?\n4. How sensitive is the final model’s performance to the accuracy of the RUS estimation? If the interaction estimator produces noisy or weak signals, does performance degrade significantly?\n5. Could you report wall-clock training times, GPU type/count, and memory usage for both RUS estimation and TIME-MoE training, relative to baseline MoE models (e.g., FuseMoE, I2MoE)?\n6. Do you plan to extend TIME-MoE to large-language-model MoE architectures? Such integration could yield interpretable and context-aware routing in large-scale generative models.\n\nIf the authors address my questions, I am willing to increase the final score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VbaLyICvVH", "forum": "qF9WJxvHX8", "replyto": "qF9WJxvHX8", "signatures": ["ICLR.cc/2026/Conference/Submission8422/Reviewer_RmCG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8422/Reviewer_RmCG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983421674, "cdate": 1761983421674, "tmdate": 1762920319196, "mdate": 1762920319196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces TIME-MOE, which routes tokens in a mixture-of-experts using time-aware interaction cues between modalities. The model estimates redundancy, uniqueness, and synergy (RUS) via a directed-information flavored partial information decomposition, producing sequences over different lags. These sequences condition the router, while auxiliary losses encourage consistent behavior: co-route redundant inputs, diversify for unique inputs, and prefer dedicated synergy experts when cross-modal effects appear."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1)\tTemporal interaction modeling: The paper builds on PID with directed information to model time-lagged redundancy, uniqueness, and synergy between modalities. The derivation reads cleanly and fits the needs of multimodal time series.\n2)\tArchitecture: The RUS-aware router plus synergy experts turn the interaction signals into concrete routing decisions. The activation patterns look modality-consistent and more structured than a standard MoE.\n3)\tEfficient multi-lag estimation: The multi-scale estimator reuses computation across lags, staying close to step-wise estimates while delivering roughly τ-fold speedups. \n4)\tEmpirical results: Strong empirical results & breadth: TIME-MOE wins on most metrics across 6 tasks, including clinical (MIMIC-IV) and affective datasets (MOSI, WESAD); results averaged over 5 runs.\n5)\tReproducibility: Anonymous code link and detailed appendices and hyper params are provided."}, "weaknesses": {"value": "1)\tEstimator guarantees and identifiability: The construction of optimal Q*_{tau} via Sinkhorn-normalized alignment tensors is elegant but lacks formal guarantees about convergence to the PID-consistent minimizer and the induced bias in R/U/S under finite data and high-dimensional encoders. A short theorem or calibration study (e.g., on synthetic systems with known RUS) would make the claims much stronger.\n2)\tDependency on label availability and distribution: For sequence-level labels (MIMIC-IV, MOSI), RUS is computed at a single time step while sweeping lags from the sequence end. That’s reasonable, but it could be brittle under class imbalance or label noise. Please analyze sensitivity and discuss how label quality affects RUS and routing.\n3)\tScalability and cost: There is qualitative speedup from the multi-scale RUS estimator, but the paper could better quantify end-to-end training cost (GPU-hours) and router overhead vs. a standard MoE for larger expert counts and more modalities."}, "questions": {"value": "1)\tEstimator validation: Could You please provide a synthetic benchmark where ground-truth RUS over lags is known, to calibrate the multi-scale estimator vs. a step-wise computation, including error bars across data sizes? How sensitive are RUS estimates to encoder capacity and the Sinkhorn regularization?\n2)\tLabel choice: For MIMIC-IV and MOSI, have you tried alternative targets (e.g., intermediate pseudo-labels or weak supervision) to compute more informative temporal RUS sequences? Any performance deltas?\n3)\tComputationals : Please report GPU-hours for TIME-MOE vs. other MoE baselines across datasets, and the incremental cost attributable specifically to RUS estimation and auxiliary losses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5EX6wnLJDs", "forum": "qF9WJxvHX8", "replyto": "qF9WJxvHX8", "signatures": ["ICLR.cc/2026/Conference/Submission8422/Reviewer_UcoF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8422/Reviewer_UcoF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997164417, "cdate": 1761997164417, "tmdate": 1762920318663, "mdate": 1762920318663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}