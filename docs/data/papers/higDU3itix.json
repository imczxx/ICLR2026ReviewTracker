{"id": "higDU3itix", "number": 20532, "cdate": 1758307170728, "mdate": 1759896972855, "content": {"title": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent and Explainable Digital Assessments", "abstract": "This paper introduces TrueGradeAI, an AI-driven digital examination framework designed to overcome the shortcomings of traditional paper-based assessments, including excessive paper usage, logistical complexity, grading delays, and evaluator bias. The system preserves natural handwriting by capturing stylus input on secure tablets and applying transformer-based optical character recognition for transcription. Evaluation is conducted through a retrieval-augmented pipeline that integrates faculty solutions, cache layers, and external references, enabling a large language model to assign scores with explicit, evidence-linked reasoning. Unlike prior tablet-based exam systems that primarily digitize responses, TrueGradeAI advances the field by incorporating explainable automation, bias mitigation, and auditable grading trails. By uniting handwriting preservation with scalable and transparent evaluation, the framework reduces environmental costs, accelerates feedback cycles, and progressively builds a reusable knowledge base, while actively working to mitigate grading bias and ensure fairness in assessment.", "tldr": "TrueGrade is an AI-powered exam framework that preserves handwriting and employs cache-enhanced retrieval-augmented grading to deliver fast, bias resistant, and auditable evaluation.", "keywords": ["AI driven assessment", "Digital examination framework", "Handwriting preservation", "Stylus input capture", "Transformer based OCR", "Optical character recognition", "Retrieval augmented evaluation", "Automated scoring", "Bias mitigation", "Auditable grading", "Fairness in assessment", "Paperless examination", "Large language models in education"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1212314737b75e1f7ef158183d364b4fad39c78e.pdf", "supplementary_material": "/attachment/6bee95bcd7fa38f8f31b40b88c2d7eb6667b8ad1.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents an AI-assisted framework for automated grading of student assignments through a retrieval-augmented evaluation pipeline. The framework integrates secure data capture, retrieval-augmented reasoning, and explainable grading, enabling secure, transparent, and interpretable assessment of student responses. To validate its effectiveness, this paper constructs a dataset of 10,000 question–answer pairs from NCERT Class 12 History, Political Science, and Geography materials. Experimental results demonstrate a high correlation between AI-based scores and human faculty evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes an AI-assisted automated pipeline for evaluating student assignments, which accelerates feedback cycles and promotes paperless assessment. It further reveals that retrieving textbook knowledge can enhance the accuracy of evaluation."}, "weaknesses": {"value": "While this paper presents a pipeline for AI-assisted grading, it exhibits several notable limitations:\n\n**Lack of literature review.**\nThis paper includes only around 20 references and provides little discussion of existing work on AI-based assessment or LLM-as-judge systems.\n\n**Limited novelty.**\nThis paper does not introduce any fundamentally new method or concept. Its core idea—incorporating reference answers and textbook materials through retrieval—is a common and basic practice already adopted by many prior works in automated grading and retrieval-augmented evaluation. The proposed approach lacks clear methodological innovation or differentiation from existing research.\n\n**Insufficient experimental comparison.**\nThis paper lacks quantitative or qualitative comparisons against existing automated grading systems.\n\n**Narrow evaluation scope.**\nThe experiments cover only three subjects, excluding key disciplines such as mathematics and linguistics.\n\n**Bias in LLM-as-judge evaluation.**\nThe framework relies on LLM-based scoring, but it does not discuss potential biases inherent in LLMs. For example, students could craft answers that exploit model preferences or perform prompt injections to obtain “high scores” rather than genuinely correct answers. For example, students could craft answers that exploit model preferences or perform prompt injections to obtain “high scores” rather than genuinely correct answers."}, "questions": {"value": "Minors: There is a citation error in line 311. The figures are not vector graphics. They become blurry and unclear when zoomed in.\n\nHave you tried using other models? Since Gemini-2.5-Pro is quite costly, it may not scale up well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YkQUErVQ2u", "forum": "higDU3itix", "replyto": "higDU3itix", "signatures": ["ICLR.cc/2026/Conference/Submission20532/Reviewer_ZNVQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20532/Reviewer_ZNVQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856623027, "cdate": 1761856623027, "tmdate": 1762933955209, "mdate": 1762933955209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes TrueGradeAI an automated evaluation platform that uses hadn’t writing recognition and a RAG-based retrieval mechanism to automatically grade student exams. The motivation for this is to address the challenges of large scale paper-based exams (paper waste, delay in results sharing, etc). The system is described an an evaluation is presented comparing outputs to manual grading."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The main strengths of the paper are:\n\n-- The paper addresses a valid an interesting problem (reducing the inefficiencies of paper-based exams, while maintaining the value of hand-written in-person exams). \n-- The platform described is comprehensive. \n-- The platform is based on state of the art technologies (e.g. TrOCR and Gemini LLMs).\n-- The results of the evaluation presented suggests some promise in automated evaluation as the manual evaluation aligns with AI evaluation."}, "weaknesses": {"value": "The main weaknesses are:\n\n-- The tone of the paper is more like a product fact sheet than an academic paper. For example, Table 1 is very selective is its comparisons (RAG is a technique rather than a feature so why is it sure important?) Similarly much of the paper is a defence of the author's approach rather than a careful evaluation. \n-- The language in the paper needs careful review and revision.\n-- The evaluation presented is very limited. \n-- The inclusion of the content of Figure 5 is unusual the LLM Reasoning and Score text contains spans on nonsensical text and strange characters which suggests the system has challenges. \n-- Score generation seems quite simplistic - \"A similarity score determines alignment with faculty-prepared answers. If the threshold (e.g., 20%) is exceeded, the answer is marked correct \"  - and heavily reliant on a manually set threshold. \n-- The evaluation presented is very limited and does not evaluate much of the pipeline described in the paper."}, "questions": {"value": "Could more detail on the evaluation be provided - for example what was the RAG database populated with? \n\nHow might OCR errors impact on the ability of the system to mark student work? \n\nIt seems that the system has only capacity to return binary evaluations (\"If the threshold (e.g., 20%) is exceeded, the answer is marked correct\"). how could this be expanded to a full grade range?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The use of automated ablation systems raises some significant ethical questions that should be addressed in the paper."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xC6QzoQWYc", "forum": "higDU3itix", "replyto": "higDU3itix", "signatures": ["ICLR.cc/2026/Conference/Submission20532/Reviewer_nggL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20532/Reviewer_nggL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869347541, "cdate": 1761869347541, "tmdate": 1762933954431, "mdate": 1762933954431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TrueGradeAI, a digital examination framework that captures handwritten responses, transcribes them with TrOCR, and grades them using a retrieval-augmented LLM pipeline referencing faculty and external materials. It claims to deliver transparent, bias-resistant, and explainable grading."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper tackles highly relevant challenges in education such as evaluator bias, grading inefficiency, and the lack of transparency in digital assessments, positioning the work as socially and practically meaningful within academic evaluation systems.\n2. The framework generates fact-linked rationales that enhance interpretability and trust."}, "weaknesses": {"value": "1.  The work lacks genuine technical innovation. It mainly combines existing technologies—TrOCR for handwriting recognition, Gemini embeddings for vector representation, and retrieval-augmented generation with an LLM for scoring—into a single pipeline. None of these components are modified, optimized, or extended to improve accuracy, efficiency, or robustness. The paper makes no attempt to address known weaknesses such as OCR errors, retrieval drift, or LLM hallucinations. As a result, the contribution is purely system integrative rather than novel.\n2. The paper provides only a superficial description of the dataset, which is a major limitation. The dataset of 10,000 QA pairs is drawn entirely from NCERT textbooks, but the paper omits details about data creation. Without such information, it is difficult to assess dataset validity, quality, or reproducibility. Moreover, the dataset’s narrow focus on factual, curriculum-aligned content means it cannot adequately evaluate open-ended reasoning or creative responses. This restricts the generalizability of the system’s results and weakens its empirical foundation.\n3. The claims of bias reduction and transparency are unsubstantiated. Although the framework includes bias-aware design elements, no experiments are conducted to measure or quantify bias before and after applying these methods. The system’s supposed fairness improvements are therefore theoretical, not demonstrated through data or controlled studies.\n 4. The evaluation lacks depth. The authors report correlation metrics against human grading but do not provide ablation studies, baselines, or comparisons with other grading systems (such as GPT-based scorers). Furthermore, the paper does not evaluate the quality or interpretability of the generated rationales through human judgment, which is essential to substantiate claims of explainability.\n5. The practical performance and scalability of the system remain unclear. While the paper claims grading latency is reduced from weeks to hours, it does not include runtime benchmarks, cost estimates, or deployment data. There is also no validation from real-world institutional use, making it uncertain whether the system can operate effectively at scale or under exam conditions."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8vcqkVQ45V", "forum": "higDU3itix", "replyto": "higDU3itix", "signatures": ["ICLR.cc/2026/Conference/Submission20532/Reviewer_vZJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20532/Reviewer_vZJk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965928082, "cdate": 1761965928082, "tmdate": 1762933953866, "mdate": 1762933953866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TrueGradeAI, a comprehensive, end-to-end framework for digital examinations. The system is motivated by the clear and significant drawbacks of traditional paper-based assessments: environmental waste, logistical complexity, grading delays, and evaluator bias.\n\nThe proposed system (Fig 1, 4) is a multi-stage pipeline. It begins with students providing handwritten answers on secure tablets. These are transcribed via a Transformer-based OCR (TrOCR). The core contribution is the evaluation engine: a retrieval-augmented generation (RAG) pipeline that uses a dual-cache system (RAG1 for faculty-provided answers, RAG2 for supporting textbooks) to ground an LLM (Gemini 2.5). The LLM's role is not just to assign a score, but to provide an explicit, evidence-linked rationale for its decision (Fig 5). The final step is a human-in-the-loop review and audit trail, intended to ensure fairness and mitigate bias.\n\nThe paper's strengths lie in its excellent system design, which thoughtfully integrates modern ML components to solve a high-impact, real-world problem. The evaluation, which includes a newly created 10,000-pair dataset and a strong quantitative analysis of the RAG component, is also a high point.\n\nHowever, the paper suffers from two critical, interconnected flaws. First, there is a major disconnect between the system proposed (an end-to-end handwriting-to-grade pipeline) and the system evaluated (a text-to-grade pipeline that skips the noisy OCR step). Second, the paper makes strong claims about being \"bias-resistant\" but provides no empirical evidence to substantiate this; it mistakes high agreement with human graders for a lack of bias. Given the ICLR venue, the novelty of the core ML components (which are assembled from existing SOTA models) versus the application is also a point of concern."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper tackles a problem of significant practical and social value. \n\nS2. The evaluation in Section 5.5 (Fig 8) is interesting. It quantifies the value of retrieval, clearly demonstrating that the LLM-only model suffers from performance degradation (\"drift\"), while the RAG-augmented models are significantly more stable and accurate. \n\nS3. The creation of a 10,000 question-answer (QA) pair dataset (Sec 5.1) is a valuable contribution to the community, even if it is currently limited to factual domains."}, "weaknesses": {"value": "W1. Disconnect Between Proposed System and Evaluation: The paper's entire premise is an end-to-end system that preserves handwriting (Sec 1.2, Fig 1, Sec 3.1). It explicitly names TrOCR (a handwriting OCR model) as a key component (Sec 2.2, 4.2). However, the entire empirical evaluation (Sec 5) is run on a dataset of clean, typed text from NCERT textbooks. The noisy, error-prone, and critical OCR step is completely skipped. The 0.982 Pearson correlation (Fig 6) is for grading perfect text, which says nothing about the performance of the actual end-to-end system when faced with real, messy, and imperfectly transcribed student handwriting. This is a major gap that invalidates the evaluation of the system as-proposed.\n\nW2. Unsubstantiated Claims of Bias Mitigation: The paper's title and abstract prominently claim it is \"bias-resistant\" and addresses \"evaluator bias\" (Sec 1.1, 1.3). However, the paper provides zero evidence for this claim. The evaluation (Sec 5.4) measures agreement (correlation, Kappa) with human graders, which is not at all the same as measuring bias. The AI could be perfectly replicating the exact same biases as the human graders. To validate this claim, a different study would be needed (e.g., showing the AI gives the same score to an answer when demographic primers are added, while humans do not). This claim is central to the paper's motivation but is entirely unsupported.\n\nW3. The paper assembles existing, SOTA components: TrOCR, Google's embedding model, and Gemini 2.5 Pro. The dual-RAG cache is a good architecture, but it's not a new algorithm. The authors need to be more precise about what the core machine learning contribution is, distinct from the (very strong) application."}, "questions": {"value": "Q1. Why was the evaluation not conducted end-to-end? The performance of the OCR component is critical. Can you provide any data on the system's accuracy when processing actual handwritten inputs, and how OCR errors cascade to the final grade?\n\nQ2. Can you please provide direct evidence for the \"bias-resistant\" claim? How was this validated? Simply agreeing with human graders does not prove a lack of bias, as the graders themselves may be biased.\n\nQ3. Could you explicitly state the core machine learning or algorithmic novelty of this work, beyond the (impressive) assembly of existing SOTA models into a new application?\n\nQ4. How do you see this RAG-based framework adapting to more subjective domains like essay writing, where there is no single \"correct\" fact to retrieve from a textbook?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "odVuFjUc45", "forum": "higDU3itix", "replyto": "higDU3itix", "signatures": ["ICLR.cc/2026/Conference/Submission20532/Reviewer_tsHK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20532/Reviewer_tsHK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995951108, "cdate": 1761995951108, "tmdate": 1762933953200, "mdate": 1762933953200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present TrueGradeAI, a paperless exam system that captures stylus handwriting, transcribes it into text, and does grading via an LLM augmented with relevant course material (ie, from answer keys, text books, etc). This grading is done in a three phase process using course material, where the question-answer pair only goes to the next phase if marked incorrect. They claim their framework constitutes a ‘complete digital assessment’ pipeline, provides ‘explainable and reliable scoring’, and has a ‘bias-aware and auditable design’. \n\nThe authors describe the features of TrueGradeAI, including a two‑portal product (student/teacher) with biometrics, secure storage, anonymized distribution, and an appeals workflow. Their evaluation centers on a curated 10k NCERT QA dataset with high AI–faculty agreement. The authors further claim retrieval improves correlation versus LLM‑only."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "There are definitely some things to like about this paper. The domain knowledge is clear in terms of what friction points a student or teacher using this system might encounter (explainability and auditability of the final scores). The issue of the resource drain, not just in material but also working hours for grading, distribution, etc, of exams makes this a very clear issue worth investigating. I also appreciate the checking of general grading agreement via the NCERT QA Dataset to show some evidence that the answers accepted by the system are also the same that the teachers themselves would accept."}, "weaknesses": {"value": "While there are some positives, I’m finding some core issues with the paper that motivate my overall score.\n\nPrimarily, it feels like there’s two papers stuffed into one and neither are really given enough detail. In fact, I would even argue that one of those papers that are jammed into the whole just also doesnt really fit at ICLR. For me, the first half of the paper involving the more logistical aspects, ie the student/teacher portal, the face ID, and handwriting recognition feels out of place. My understanding was that neither of the latter were evaluated as well, with the primary evaluation coming from a textbook with the expectation of typed answers.\n\n- I think the agreement numbers look a bit inflated by the setup. For Table 3, wouldn’t there be a significant overlap with any faculty answers already being stored? \n- Feels like the thresholds and logic arent justified super well? My impression is that the 3-tier grading system would result in excessive skewing towards positive scores \n- While the model for the embedding is named (Gemini) I dont think I saw the actual model used for the system?\n- Unless I missed it, the authors don’t actually evaluate the bias-resilience of their system? The unspoken assumption appears to be that automation inherently ‘solves’ this but if anything, the high instructor agreement would imply the opposite.\n- It feels like this system would be working with very Personally-Identifiable-Information but there’s no mention really of how this is kept secure\n- Just generally related to the first point: no comparison to strong short‑answer/essay graders on public datasets; no breakdowns by question type, answer length, or subject; no qualitative failure cases from the grading pipeline \n- I'll also note some roughness with the editing and grammar (ie, frame- work and eval- uator in the abstract)"}, "questions": {"value": "- Table 3: Did you find this agreement generalized? Was this with full scores or was partial credit involved? (ie, a student could score .5/1). \n- For the three tiered grading, did a single 'correct' override all of the other tiers declaring the answer wrong?\n- What model is being used to generate the scoring of the students' answers?\n- How is grading bias (or a lack thereof) being measured?\n- Figure 8: How is the correlation being calculated here? \n- It seems like this system works with alot of sensitive information: How is this stored and kept safe?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EEE1b9REmH", "forum": "higDU3itix", "replyto": "higDU3itix", "signatures": ["ICLR.cc/2026/Conference/Submission20532/Reviewer_h4qy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20532/Reviewer_h4qy"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159752782, "cdate": 1762159752782, "tmdate": 1762933952759, "mdate": 1762933952759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}