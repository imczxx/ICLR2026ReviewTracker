{"id": "bo6cliXvPQ", "number": 1422, "cdate": 1756881373695, "mdate": 1759898210091, "content": {"title": "On Rademacher Complexity-based Generalization Bounds for the Transformer Architecture", "abstract": "We derive the first end-to-end, data-dependent generalization bound for the Transformer architecture to explain its strong empirical performance. Using Rademacher complexity and a novel Lipschitz analysis of self-attention, we construct a bound for deep, L-layer models. The bound demonstrates that generalization capacity is governed by depth, sequence length, and a polynomial of the model's weight norms. A numerical sanity check validates our theoretical scaling with model depth, providing a new, formal lens to understand and improve Transformers.", "tldr": "We derive the first formal generalization bound for Transformers using Rademacher complexity, showing that their performance on unseen data is controlled by weight norms, depth, and sequence length", "keywords": ["Generalization Bounds", "Rademacher Complexity", "Transformers", "Self-Attention", "Deep Learning Theory"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/23b2802792095392bb4bad3b8a63a48bd21a9e53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors claim to propose the first end-to-end, data-dependent generalization bound for the Transformer architecture using the Rademacher complexity framework. The authors derive a theoretical measure of the model’s effective capacity by analyzing the Lipschitz continuity of the self-attention. The bound they provide depends on the - sequence length (n), embedding norms (BX), and weight norms (BW), which is reasonable under the assumptions. Using this result, they iteratively bound the Rademacher complexity for a single attention head, multi-head self-attention (MHSA), a full Transformer block (including LayerNorm and feed-forward layers), and finally a deep L-layer Transformer through inductive composition. The use of Talagrand’s Contraction Lemma allows for the bounds obtained in this work. Section 3..7 then provides the final generalization risk bound using the standard framework. A numerical check on synthetic data verifies the theoretical scaling: the bound’s magnitude increases monotonically with depth, consistent with observed model capacity growth. The paper focuses on the use of pre LayerNorm  architecture to derive these bounds."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper provides a Rademacher complexity based bound on the transformer function class, under standard/simple enough assumptions on the norm bounds. They provide a simple supporting experiment to show that the bound holds in practice, with the monotonic increase shown in figure 1."}, "weaknesses": {"value": "1. One major criticism is the incompleteness of the Related Works section where the authors have not discussed the existing works on deriving the bounds for the Lipschitz constant of the self-attention mechanism. See [1-3] below.  Furthermore, the authors’ claim of the first work characterizing the Rademacher complexity is incorrect, see [4] below. \n\n2. In the draft, the relevant citations are missing in all important places. Some examples – no citation for the Talagrand’s Contraction lemma, lack of appropriate references for the Rademacher Complexity and the transformers paper, etc \n\n3. This is a very explicit claim about the writeup of the paper, but the lack of any equation number and furthermore the lack of appropriate cross referencing, lack of appropriate lemma indexing and such, is somewhat unprofessional. While this specific paper is easy to follow, it is generally good practice to do so. \n\n4. It seems that the derivations in line 274-294 on page 6 have flipped the composition of the LayerNorm with the MHSA and FFN. Can the authors elaborate this? \n\n5. The experimental results are too trivial and completely lacking any informative findings. Furthermore, no details of the synthetic copying task were provided in the paper. \n\n[1] Kim, H., Papamakarios, G., & Mnih, A. (2021). The Lipschitz Constant of Self-Attention. In Proc. ICML 2021, PMLR vol. 139 \n\n[2] Castin, V., Ablin, P., & Peyré, G. (2024). How Smooth Is Attention? (ICML 2024). Proceedings of Machine Learning Research vol. 235 \n\n[3] Wang, Y., Chauhan, J., Wang, W., & Hsieh, C.-J. (2023). Universality and Limitations of Prompt Tuning. In Advances in Neural Information Processing Systems 36 (NeurIPS 2023). \n\n[4] Trauger, J., & Tewari, A. (2024). Sequence Length Independent Norm-Based Generalization Bounds for Transformers. In Proceedings of the 27th International Conference on Artificial Intelligence and Statistics (PMLR, Vol. 238, pp. 1405–1413)"}, "questions": {"value": "1. Can the authors elaborate which specific variant of the Contraction lemma has been invoked to have the factor of 2 in line 169 of page 4 -> Rˆ S(ϕ ◦ G) ≤ 2LϕRˆ S(G) ? \n\n2. Can the authors describe further details on the use of LLMs? I use Gemini myself, and the quality of this output is way too similar to what Gemini-2.5 Pro generates by default. No effort seems to have been made to improve the manuscript. \n\n3. Can the authors describe further why the related works section was left in such an incomplete state? \n\n4. Why were the experimental details left out? More importantly, the lack of experimental details raise skepticism on how the experiments were performed, even though the findings are straightforward and provided quite explicitly in the literature over the years."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QGbuN1Ersj", "forum": "bo6cliXvPQ", "replyto": "bo6cliXvPQ", "signatures": ["ICLR.cc/2026/Conference/Submission1422/Reviewer_4Bg6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1422/Reviewer_4Bg6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760764630688, "cdate": 1760764630688, "tmdate": 1762915765857, "mdate": 1762915765857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the generalization bounds for Transformer architectures. The authors claim to have obtained the first theoretical analysis of the self-attention mechanism within the Rademacher complexity framework. Section 3.7 aims to give a formal generalization bound using empirical Rademacher complexity from one Transformer block. Some numerical results are shown regarding the Gen gap and the model depth.\n\nThe paper is written in collaboration with Google's Gemini language model."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Theoretical analysis of Transformer's generalization bound using the Rademacher complexity is an interesting topic."}, "weaknesses": {"value": "The paper is written in collaboration with Google's Gemini language model. The soundness of the results needs to be verified.\n\n1. L319 mentions a stability assumption, which I have not found in the paper.\n\n2. What does the approximation relation mean exactly in L320?\n\n3. The equation in L328 is invalid according to L310.\n\n4. Is the loss function in L368 for supervised learning? \n\n5. L392 mentions Theorem 3.6, which I'm unable to locate.\n\n6. The description on the experimental design is vague. Details are not provided regarding the training (e.g., data, architecture, hyper-parameters...) It is unclear to me how the empirical gaps are computed.\n\n7. The presentation needs to be improved. The main results are not formed into mathematical theorems."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DVsgytdnkN", "forum": "bo6cliXvPQ", "replyto": "bo6cliXvPQ", "signatures": ["ICLR.cc/2026/Conference/Submission1422/Reviewer_AiMt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1422/Reviewer_AiMt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821938981, "cdate": 1761821938981, "tmdate": 1762915765633, "mdate": 1762915765633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors provide an upper bound on the Rademacher complexity for transformer architecture by exploiting the Lipschitz properties of the model combined with the Talagrad contraction lemma. From the obtained bound, a data-dependent generalization bound is directly obtained. Furthermore, numerical experiments are provided to empirically validate the theory."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper contains an upper bound on the Rademacher complexity of a transformer architecture that includes the multihead attention mechanism and the layer normalization. The derivation of a generalization bound for transformers is an interesting direction that extends the known results on MLP to a broader class of models, which are widely adopted in practice and often forgotten by the more theoretical community."}, "weaknesses": {"value": "The first point on which I have based my assessment is the originality of the work. The authors claim \"We provide the first theoretical analysis of the self-attention mechanism within the Rademacher complexity framework\" (lines 041-043); however, other works have been done in this direction. For instance, Trauger, Jacob, and Ambuj Tewari. \"Sequence length independent norm-based generalization bounds for transformers.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024.\n\nFurthermore, the overall writing of the paper could be improved. The results are not presented in rigorous form: some definitions and results used are not reported or are imprecise. The softmax activation function,  row-stochastic matrix (line 125), and the operation Concat (line 207) are not reported at all. The assumptions reported in the **Norm regime**  section are not stated clearly; for instance, the notion of weights $W$ has not been introduced yet. $X$ here denotes both in a single input and the space where of input matrix is, instead referring to uniform upper bounds. Given the theoretical nature of this paper, I would have appreciated a more rigorous and precise introduction and exposition of the work by using the math environments (definition, lemma, theorem, proofs, etc...), which would have increased the clarity and readability of the paper. The proof in Section 3.6 is unclear and not rigorous at all; even the statement is not clear. See, for instance, at line 321 the symbol $\\approx$, which is not defined.\n\nRegarding the experimental part, it seems that the only experiment shown is that the theoretical bound is monotonically increasing in $L$. However, it is not informative since the scaling of the theoretical bound is explicit, and the empirical generalization error is always zero, which does not correlate in any way to the theoretical bound. Furthermore, only one experiment is provided, which does not provide enough statistical evidence."}, "questions": {"value": "I ask the authors the following questions:\n1. How do you compare with \"Sequence length independent norm-based generalization bounds for transformers.\" International    Conference on Artificial Intelligence and Statistics. PMLR, 2024\"? \n2. Why is the Rademacher complexity of the $\\operatorname{Id}$ negligible? In general, it is not 0 (see Line 289).\n3. Can you explain the exact statement that you prove in Section 3.6? What does the symbol $\\approx$ mean precisely? And how do you use the norm stable assumption for deriving line 321 from 317? What is $SubLayer_L(Y)$ at line 332? \n4. Can you explain what the experiments show? It seems that your theoretical bound does not relate at all to the empirical generalization bound."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tl5wYLSDjO", "forum": "bo6cliXvPQ", "replyto": "bo6cliXvPQ", "signatures": ["ICLR.cc/2026/Conference/Submission1422/Reviewer_3CYc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1422/Reviewer_3CYc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907626410, "cdate": 1761907626410, "tmdate": 1762915765474, "mdate": 1762915765474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}