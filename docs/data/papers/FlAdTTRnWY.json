{"id": "FlAdTTRnWY", "number": 17926, "cdate": 1758282076144, "mdate": 1759897144681, "content": {"title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via MoEs", "abstract": "Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. \nWe hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms.\nWe present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE). MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns.\nBy selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2+T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27\\% better perplexity for an identical compute budget. \nMoSA can also reduce the resource usage compared to dense self-attention. \nDespite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.", "tldr": "Sparse attention achieved with MoEs, a lot of sparse heads instead of small number of dense heads", "keywords": ["sparse attention", "mixture-of-experts", "transformer"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc06a10d5af4444a239d59ea5642ee039dd612ca.pdf", "supplementary_material": "/attachment/afae8e0269b048fa5138ae7b133ab536316dd8e7.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Mixture of Sparse Attention (MoSA), a novel sparse attention mechanism designed to reduce the quadratic computational cost of standard self-attention while maintaining or even improving model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- **Inspired by Mixture of Experts (MoE)** with **expert-choice routing**, where each attention head acts as an expert and selects its own set of \\(k\\) tokens from the sequence.\n- **Dynamic, content-based sparsity**: Each head learns which tokens to attend to via a trainable router.\n- **Complexity reduction**: From $O(T^2)$ to $O(k^2 + T)$ per head.\n- **Hybrid design**: Combines a few dense heads with many sparse MoSA heads for stability and performance."}, "weaknesses": {"value": "* Short Sequence Struggle: The model was trained on long sequences (T=1024) but evaluated on downstream tasks with very short sequences (e.g., BLiMP examples are often <10 tokens). In these cases, the token selection mechanism is forced to operate out-of-distribution. Selecting 2 tokens from a 10-token sentence (20%) is fundamentally different from selecting 16 tokens from a 1024-token sequence (1.56%), leading to a significant performance drop on tasks like BLiMP.\n\n* Expert Overspecialization: This is a known issue in Mixture-of-Experts (MoE) models. While the highly specialized MoSA heads excel at the language modeling pre-training objective (hence the low perplexity), they may fail to generalize to diverse downstream tasks that require different reasoning patterns. \n\n* Toy Model Experiments: The largest model tested has 516M parameters, which is considered a \"toy model\" by today's standards for LLM research. The field's focus has shifted to models of 7B parameters and larger. The paper does not demonstrate that MoSA's benefits (or its stability issues) hold at these realistic, larger scales. Performance and behavior can change dramatically with scale, so the conclusions are preliminary until validated on larger models.\n\n* Anomalous Long-Sequence Results (Fig. 4): The results in Figure 4 are counter-intuitive and require deeper discussion. Perplexity is expected to increase (get worse) as sequence length grows because predicting the next token in a longer, more complex context is harder. However, the figure shows perplexity decreasing for all methods as the sequence length increases from 1024 to 8192.\n\n* Narrow Downstream Benchmark Suite: The evaluation on downstream tasks is limited to only six benchmarks (LAMBADA, WinoGrande, BLiMP, HellaSwag, PIQA, AI2ARC). It lacks a broader range of challenging evaluations that are now standard, such as:\n   - Reasoning Tasks: (e.g., GSM8K, MATH)\n   - Knowledge-Intensive Tasks: (e.g., MMLU, TriviaQA)\n   - Code Generation: (e.g., HumanEval)\n   - Massive Multi-Task Benchmarks: (e.g., BIG-Bench Hard).\n\n   This limited scope makes it difficult to fully assess the model's capabilities and the true impact of the MoSA architecture."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Abey8Xj4lN", "forum": "FlAdTTRnWY", "replyto": "FlAdTTRnWY", "signatures": ["ICLR.cc/2026/Conference/Submission17926/Reviewer_ArMp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17926/Reviewer_ArMp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550469269, "cdate": 1761550469269, "tmdate": 1762927738247, "mdate": 1762927738247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose an architecture combing the concept of MoE with head selection of attention computation. MoSA performs experter-routing, (experts choose the topk tokens) while MoSAIC performs token-routing, (tokens choose the expert)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of expert-routing for attention is ineresting, but unfortunately, it seem not suitable to fit for modern auto-regressive generation."}, "weaknesses": {"value": "- The evaluation is unsound, as PPL is used for most part. It's widly known that PPL is not affected by attention a lot. You can do many crazy sparse attention algorithms with PPL in a reasonble range. \n\n- The concept of MoSA/MoSAIC is not seperated clearly. I believe some of the MoSAIC's concept like KV cache is missused in MoSA.\n\n- The speedup evaluation setup is not clear."}, "questions": {"value": "1. Why MoSA has KV cache? I think it's not auto-regressive.\n2. How is KV cache being managed in MoSAIC? Do you only keep k-tokens on each head? If so, what is the eviction KV cache algorithm being used? \n3. Can you show results on DROP and GSM-8k in the benchmark? It would be better if you also include ruler. The benchmarks you used in the current evaluation can not reflect the attention ability well. \n4. How's the wall clock speedup baseline being measure? Please show the setting/framework using used and differentiate prefill/decode case. \n5. Can you also analyze the communication overhead of MoSA/MoSAIC with TP (tensor-parallel) where different head are placed in different GPUs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rbv37uHYLw", "forum": "FlAdTTRnWY", "replyto": "FlAdTTRnWY", "signatures": ["ICLR.cc/2026/Conference/Submission17926/Reviewer_ETuF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17926/Reviewer_ETuF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761597739371, "cdate": 1761597739371, "tmdate": 1762927737883, "mdate": 1762927737883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a \"Learnable Sparse Attention\" adopted from MoE designs and claims it performs better than \"Fixed\" and \"routed\" ones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear and informative figures. The visualizations (e.g., Fig. 1) illustrate the workflow of dense versus MoSA attention. and the paper is well written and logically structured."}, "weaknesses": {"value": "Poor baselines. The experimental comparison is weak. The paper only compares MoSA with fixed sparse attention and Routing Transformer–style baselines, omitting stronger and more recent sparse attention methods such as NSA, MoBA, DuoAttention, XAttention, SeerAttention, and MInference. Without these, the claim that “MoSA consistently outperforms dense attention” is not convincing, it only holds under a limited and outdated baseline set.\n\nLack of novelty. The claimed novelty, “a Mixture of Sparse Attention (MoSA) inspired by Mixture of Experts with expert-choice routing” (from the introduction), is questionable. Similar ideas have already appeared in NSA, MoBA, SeerAttention, which first use dynamic sparsity and content-based gating. The paper does not clearly differentiate MoSA from these prior works or explain what unique advantage it brings."}, "questions": {"value": "Provide more comparison with SOTA works.\n\nExplain what the difference between you and other previous \"learnable sparse attention\" works.\n\nNative Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention, https://arxiv.org/abs/2502.11089\n\nDuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads, https://arxiv.org/abs/2410.10819\n\nXAttention: Block Sparse Attention with Antidiagonal Scoring, https://arxiv.org/abs/2503.16428\n\nSeerAttention: Learning Intrinsic Sparse Attention in Your LLMs, https://arxiv.org/abs/2410.13276\n\nMInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention, https://arxiv.org/abs/2407.02490"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FbkjW6uhlR", "forum": "FlAdTTRnWY", "replyto": "FlAdTTRnWY", "signatures": ["ICLR.cc/2026/Conference/Submission17926/Reviewer_jkHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17926/Reviewer_jkHd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021754195, "cdate": 1762021754195, "tmdate": 1762927737363, "mdate": 1762927737363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}