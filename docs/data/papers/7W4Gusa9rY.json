{"id": "7W4Gusa9rY", "number": 9781, "cdate": 1758139950118, "mdate": 1759897697248, "content": {"title": "VLOD-TTA: Test-Time Adaptation of Vision-Language Object Detectors", "abstract": "Vision–language object detectors (VLODs) such as YOLO-World and Grounding DINO achieve impressive zero-shot recognition by aligning region proposals with text representations. However, their performance often degrades under domain shift. We introduce VLOD-TTA, a test-time adaptation (TTA) framework for VLODs that leverages dense proposal overlap and image-conditioned prompt scores. First, an IoU-weighted entropy objective is proposed that concentrates adaptation on spatially coherent proposal clusters and reduces confirmation bias from isolated boxes. Second, image-conditioned prompt selection is introduced, which ranks prompts by image-level compatibility and fuses the most informative prompts with the detector logits. Our benchmarking across diverse distribution shifts -- including stylized domains, driving scenes, low-light conditions, and common corruptions -- shows the effectiveness of our method on two state-of-the-art VLODs, YOLO-World and Grounding DINO, with consistent improvements over the zero-shot and TTA baselines.", "tldr": "We introduce VLOD-TTA, to our knowledge the first test-time adaptation framework for vision-language object detectors (VLODS).", "keywords": ["Test Time Adaptation", "Object detection", "Vision language model", "vision language object detectors"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c633c10a655238a10668db6bf1efb4b06a3dd4b7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes VLOD-TTA, the first test-time adaptation (TTA) framework specifically designed for vision-language object detectors (VLODs). Its core goal is to address the significant performance degradation of VLODs when faced with new environments that differ from the training data distribution (e.g., \"domain drift\" due to changes in style, lighting, and weather)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper establishes a comprehensive benchmark, validating the method’s effectiveness across up to 96 different test scenarios. The results show that VLOD-TTA consistently improves model performance under various domain shifts, including artistic styles, real-world driving scenes, and image corruptions.\n2.The paper concerns DA problem by tta, which is easy to follow"}, "weaknesses": {"value": "[1] The IWE mechanism relies on clusters of candidate boxes with dense overlaps. Therefore, when dealing with a large number of tiny, sparse, and low-overlap objects—such as in scenes from the Cityscapes dataset—its effectiveness may be reduced, as forming sufficiently “high-density” regions to guide optimization becomes challenging.\n[2] The construction of this prompt pool (generated using GPT in the paper) is itself a labor-intensive step. Moreover, for highly specialized domains, the existing prompt pool may be insufficiently comprehensive, which could limit the effectiveness of IPS."}, "questions": {"value": "[1] TTA introduces significant latency. How can we balance performance gains with computational cost? In the future, it may be worth exploring a **“selective adaptation”** strategy, where the model first evaluates the degree of domain shift in the current input and only triggers the TTA process when the shift exceeds a certain threshold, thereby maintaining fast inference in most cases.\n[2] VLOD-TTA demonstrates effectiveness under various domain shifts. However, is there a “limit” to its adaptation capability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "toxRyjwdQ8", "forum": "7W4Gusa9rY", "replyto": "7W4Gusa9rY", "signatures": ["ICLR.cc/2026/Conference/Submission9781/Reviewer_2uSN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9781/Reviewer_2uSN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619957921, "cdate": 1761619957921, "tmdate": 1762921267207, "mdate": 1762921267207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VLOD-TTA, a test-time adaptation framework for vision-language object detectors such as YOLO-World and Grounding DINO. The approach combines IoU-weighted entropy minimization to focus adaptation on spatially coherent proposal clusters and image-conditioned prompt selection to fuse only the most relevant prompts with detector outputs. Only lightweight adapters are optimized during test time. Experiments across several domain shifts indicate consistent improvements over zero-shot and baseline TTA strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Relevant problem**.\n\n    Test-time adaptation for open-vocabulary object detection remains underexplored and has strong relevance for real-world robustness.\n\n\n- **Solid empirical results**.\n\n    The study covers multiple datasets and two state-of-the-art VLODs, showing consistent gains."}, "weaknesses": {"value": "- **Architecture-dependent adaptation**.\n\n    Different parameters are adapted for YOLO-World and Grounding DINO, which reduces generality and complicates baseline comparisons.\n\n- **Unclear adaptation protocol**.\n\n    It is not specified whether the model is reset after each image or adapts continuously, which can lead to very different behavior and raises reproducibility concerns.\n\n- **Unusual adaptation target**.\n\n    The method updates lightweight adapters rather than normalization parameters, which differs from standard TTA practice. The rationale for this choice should be clarified.\n\n- **Significance of the contributions**.\n\n    The proposed image-conditioned prompt selection and IoU-weighted entropy minimization modules seem reasonable but somewhat limited in scope. While they contribute to performance it is not clear that these represent sufficiently substantial innovations to justify a top-tier venue."}, "questions": {"value": "1.\tAre the model parameters reset after every image, or does adaptation accumulate across the evaluation set? Please clarify the protocol and its practical justification.\n2.\tCould the IoU-weighted entropy component be used alone without depending on adapters, or could adapters be placed uniformly at shallow layers to avoid architecture-specific tuning?\n3.\tDid you evaluate or consider updating BN or LayerNorm statistics? How would a normalization-only adaptation compare in terms of accuracy and compute?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f7FZkeoMjt", "forum": "7W4Gusa9rY", "replyto": "7W4Gusa9rY", "signatures": ["ICLR.cc/2026/Conference/Submission9781/Reviewer_YXiS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9781/Reviewer_YXiS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732574713, "cdate": 1761732574713, "tmdate": 1762921266803, "mdate": 1762921266803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an approach to perform Test-Time Adaptation (TTA) for the task of Object Detection with Vision Language Models (VLM). The method is based on two main contributions: a weighting of the visual proposals contribution based on their Intersection over Union (IoU) weighting, and a prompt selection conditioned over the image. Extensive evaluations were performed on multiple datasets and corruptions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles TTA for object detection using VLM, which is an original and interesting approach.  The mIoU weighting of the entropy is simple yet original and well in phase is already existing TTA approaches.  \n\nThe results show consistent gains in mAP on all datasets, which evaluate different kinds of simulated and natural corruption/domain shift."}, "weaknesses": {"value": "TTA is generally conducted by optimizing batch norm parameters. Here, the method relies on adapters and learnable residual prompts. This parameter overhead might be enough to explain the gains of the proposed approach. Furthermore, if adaptation requires back-prop or large memory, it may not be viable for streaming/inference environments. This point should be discussed, and the number of learnable parameters and complexity should be shown for the method and the baselines. \n\n The paper lacks an ablation to disentangle the contribution of the mIoU weighting scheme and the prompt selection. Furthermore, more description would be required to understand how the prompts were generated using a GPT model, as this step appears to have a great impact."}, "questions": {"value": "Which GPT model was used to generate the prompts?\n\nWhich prompts were given to the GPT model to construct the textual prompts?\n\n\nIt is unclear how the method deals with the different samples in the batch and how this parameter should have an impact on the TTA task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6nbD9PFY55", "forum": "7W4Gusa9rY", "replyto": "7W4Gusa9rY", "signatures": ["ICLR.cc/2026/Conference/Submission9781/Reviewer_vTge"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9781/Reviewer_vTge"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098333397, "cdate": 1762098333397, "tmdate": 1762921266453, "mdate": 1762921266453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VLOD-TTA, the first test-time adaptation (TTA) framework tailored specifically for vision-language object detectors (VLODs) such as YOLO-World and Grounding DINO, both of which have demonstrated strong zero-shot generalization. The main technical contributions are twofold: (1) IoU-weighted entropy minimization (IWE), which emphasizes adaptation over spatially coherent clusters of object proposals to address confirmation bias and localization uncertainty, and (2) image-conditioned prompt selection (IPS), which selects and fuses the most relevant textual prompts for each image to improve detection robustness. The approach is empirically validated on a comprehensive benchmark encompassing diverse domain shifts, showing consistent improvements over both zero-shot and existing TTA baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a well-motivated VLOD-TTA framework, which is interesting and inspiring. \t\n\n2. Clear motivation and problem setup for TTA in VLODs. Figures 1 and 2 (Pages 2) concretely illustrate failure modes of standard entropy and uniform prompt averaging, and how the proposed IWE and IPS address them.\n\n3. Comprehensive experimental evaluation and Clear presentation and informative figures/tables."}, "weaknesses": {"value": "1. Despite comprehensive benchmarks, the paper mostly highlights consistent positive gains. However, as briefly noted in the conclusion and Section 4.4, IWE can underperform in scenes with numerous small, scattered objects (e.g., Cityscapes); yet the depth of analysis is minimal. It would significantly strengthen the paper to have a more granular breakdown for such problematic cases, including visualizations and quantification of failure cases. \n\n2. As shown in Figure 5, the placement of adapters differs between YOLO-World (vision backbone+neck) and Grounding DINO (text encoder), with minimal theoretical motivation provided. This is largely left to empirical ablation, when some architectural analysis or interpretation might clarify practical trade-offs or inform future practitioners.\n\n3. While overall clarity is reasonable, the related work section (Section 2) needs to be more explicit about methodological distinctions versus related TTA for VLM and OD approaches"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OGJBQ411tY", "forum": "7W4Gusa9rY", "replyto": "7W4Gusa9rY", "signatures": ["ICLR.cc/2026/Conference/Submission9781/Reviewer_Eyox"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9781/Reviewer_Eyox"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153170585, "cdate": 1762153170585, "tmdate": 1762921266174, "mdate": 1762921266174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}