{"id": "8pDcEIvmcP", "number": 11910, "cdate": 1758204610858, "mdate": 1759897546950, "content": {"title": "CoDi: Subject-Consistent and Pose-Diverse Text-to-Image Generation", "abstract": "Subject-consistent generation (SCG)-aiming to maintain a consistent subject identity across diverse scenes-remains a challenge for text-to-image (T2I) models. Existing training-free SCG methods often achieve consistency at the cost of layout and pose diversity, hindering expressive visual storytelling. To address the limitation, we propose subject-Consistent and pose-Diverse T2I framework, dubbed as CoDi, that enables consistent subject generation with diverse pose and layout. Motivated by the progressive nature of diffusion, where coarse structures emerge early and fine details are refined later, CoDi adopts a two-stage strategy: Identity Transport (IT) and Identity Refinement (IR). IT operates in the early denoising steps, using optimal transport to transfer identity features to each target image in a pose-aware manner. This promotes subject consistency while preserving pose diversity. IR is applied in the later denoising steps, selecting the most salient identity features to further refine subject details. Extensive qualitative and quantitative results on subject consistency, pose diversity, and prompt fidelity demonstrate that CoDi achieves both better visual perception and stronger performance across all metrics. The code is provided in the supplementary material.", "tldr": "CoDi is a cutting-edge framework for subject-consistent generation, achieving strong ID consistency while preserving diverse poses.", "keywords": ["Consistent Subject Generation; Text-to-Image; Diffusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/edf7a857e1b7129496578afb1306528a89c889ac.pdf", "supplementary_material": "/attachment/8200ff3bcf8248697098e0006007eed5602c0f6b.zip"}, "replies": [{"content": {"summary": {"value": "This work focuses on subject-consistent generation. It preserves pose diversity by leveraging Identity Transport in the early stage of denoising and promotes subject consistency through Identity Refinement in the late stage of denoising. Both qualitative and quantitative experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well written and easy to follow.\n- The method proposed in the paper is training-free and can be directly applied during inference.\n- The paper includes comprehensive comparative experiments and ablation studies. The design of evaluation metrics also demonstrates certain insights, especially regarding \"pose diversity\"."}, "weaknesses": {"value": "- Optimal Transport (OT) essentially addresses the optimization problem of transforming one probability distribution into another with minimal cost. However, the problem in **IT** is to find the feature matching relationship between the reference image and the target image. Clearly, a straightforward ranking based on cosine similarity would be simpler and more efficient. In contrast, the \"globally optimal transport\" property of OT not only complicates the problem but may also introduce redundancies. To illustrate this with a simple though imperfect analogy: The IT task aims for direct and semantically consistent matches. For instance, it seeks to align the \"eye\" feature in the reference image with the \"eye\" feature in the target image, and the \"nose\" feature with the \"nose\" feature. Introducing OT could disrupt this consistency. Suppose the \"eye\" feature in the reference image has a higher correlation than other features. Under OT’s global optimization logic, this could lead to the \"eye\" feature being inappropriately \"transported\" to the \"nose\" region in the target image. This outcome violates the need for direct and semantically meaningful feature matching. In summary, I argue that the introduction of Optimal Transport in this work serves more as a mathematical formality rather than a practically useful component.\n- The design of Identity Refinement lacks innovation, as its feature fusion approach bears significant similarities to that of prior works.\n- SDXL is built on the U-Net architecture, which is considered outdated in the current field. Most state-of-the-art base models now adopt the DiT architecture, and the effectiveness of the proposed method when applied to DiT-based models remains unproven."}, "questions": {"value": "Given that current state-of-the-art base models such as XVerse, UNO, Flux-Kontext and Nano Banana have already achieved the three key objectives outlined in the introduction with excellent performance, and further support unlimited multi-subject consistent generation as well as strong generalization capabilities, what is the core significance of this work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zbag1GkM0W", "forum": "8pDcEIvmcP", "replyto": "8pDcEIvmcP", "signatures": ["ICLR.cc/2026/Conference/Submission11910/Reviewer_VX3M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11910/Reviewer_VX3M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567942334, "cdate": 1761567942334, "tmdate": 1762922918710, "mdate": 1762922918710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To solve the trade-off between subject consistency and pose diversity in text-to-image subject-consistent generation (SCG), the paper proposes the CoDi framework. Inspired by diffusion models’ progressive nature, this paper includes two stages: Identity Transport and Identity Refinement.  Extensive experiments have demonstrated the effectiveness of the model."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. This work proposes an effective method to improve pose diversity.\n2. This work is clearly expressed and easy to understand.\n3. This work introduces Optimal transport into Subject-consistent generation."}, "weaknesses": {"value": "1. This model was tested on SDXL, but its effectiveness was not verified on the DiT architecture.\n2. The qualitative and quantitative experimental results of this work did not show significant improvement.\n3. The long description in lines L126-L131 seems informal in the main text."}, "questions": {"value": "1. This work was conducted on the SDXLl, which is a relatively old base model. Could the proposed method in this paper work on DiT model (e.g., FLUX and SD 3.5)?\n2. In the experimental results shown in Figure 3(a) of this paper, the styles of the three scientists are not very consistent (and all are anime characters), and the portrait similarity is also lower compared to ConsiStory and StoryDiffusion. Will the proposed model in this paper affect the style?\n3. Could this method generate images with consistent subject appearance and different style?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YawiAeZ0f4", "forum": "8pDcEIvmcP", "replyto": "8pDcEIvmcP", "signatures": ["ICLR.cc/2026/Conference/Submission11910/Reviewer_rZBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11910/Reviewer_rZBz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926708707, "cdate": 1761926708707, "tmdate": 1762922918378, "mdate": 1762922918378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study introduces a novel training-free framework for subject-consistent image generation. The framework aims to address the limitation of existing methods that sacrifice pose and layout diversity to maintain consistency. Its core design employs a staged processing strategy: during the early generation phase, optimal transport is utilized for coarse-grained identity feature transfer to preserve subject consistency; in the later generation phase, fine-grained refinement is applied to the image features. Experimental results demonstrate that the proposed method outperforms existing mainstream approaches across three key metrics—subject consistency, pose diversity, and text alignment—achieving state-of-the-art performance on public benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The key innovation is the explicit decoupling of identity alignment into coarse-grained transport in early steps and fine-grained refinement in later steps, which is a well-motivated approach based on the progressive nature of diffusion models.\n2.A significant strength is the superior balance it achieves. As claimed, the paper provides strong evidence that the method outperforms existing training-free baselines in subject consistency while preserving significantly greater pose diversity and text alignment, addressing a well-known trade-off in the field."}, "weaknesses": {"value": "1.For long-story generation scenarios, it is crucial to maintain consistency in both character identity and their apparel. However, the results presented in the paper demonstrate that the method primarily ensures identity consistency, while the consistency of clothing remains inadequate. In my opinion, this limitation would significantly restrict the method's practicality in long-story applications.\n\n2.The method has a core reliance on binary masks derived from cross-attention maps to extract identity information from the reference image. However, recent and emerging generative models, such as SD3 and other DiT-based architectures, have moved away from using cross-attention mechanisms. This fundamental incompatibility means that the proposed facial extraction technique faces significant challenges in being adapted to these mainstream, state-of-the-art generative models, thereby limiting its generalizability and future relevance."}, "questions": {"value": "1.How is the iterative generation performance? Can it maintain identity consistency?\n2.Why does the style of the generated images vary? Since style should also be correlated with features from certain layers of the U-Net, why do the other two models (ConsiStory and StoryDiffusion) not exhibit style changes (based on the observation from Figure 3 in the paper)? Could this limitation affect the model's practical applicability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0M9uIzEuJY", "forum": "8pDcEIvmcP", "replyto": "8pDcEIvmcP", "signatures": ["ICLR.cc/2026/Conference/Submission11910/Reviewer_tP1C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11910/Reviewer_tP1C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959113246, "cdate": 1761959113246, "tmdate": 1762922917947, "mdate": 1762922917947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}