{"id": "R2YwWxO2U0", "number": 15440, "cdate": 1758251361996, "mdate": 1763632453705, "content": {"title": "Mutual Information Guided Diffusion Model for Partial Label Learning", "abstract": "This paper proposes a novel paradigm for partial label learning (PLL) that integrates diffusion mechanisms with mutual information (MI) estimation to address the challenge of label disambiguation. In PLL, each training instance is typically associated with multiple candidate labels, among which only one is the ground truth. To simulate the process of label degradation, we introduce noise into the labels through forward diffusion to simulate candidate labels, and then perform reverse denoising to recover the true labels in a probabilistic sense. Unlike in conventional image generation tasks, we present the Mutual Information Guided Diffusion Model for Partial Label Learning (MDMPLL), which adapts diffusion models to weakly supervised learning and incorporates MI estimation to strengthen the consistency between denoised labels and data features, thereby improving label disambiguation. Furthermore, Dual-Path Attention Feature Fusion (DAFF) strategy is employed to enhance data representation, enabling more effective label disambiguation. Experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art PLL approaches on multiple datasets.", "tldr": "", "keywords": ["Partial Label Learning;Diffusion Model;Mutual Information"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80e08068442e9067b3d626870c70bfbb1c1f4187.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the problem of partial-label learning. It leverages the reverse denoising process of diffusion models to disambiguate partial-label data. To encourage the model to retain task-relevant information during training and to enhance robustness under ambiguous labels, the paper introduces a feature‚Äìlabel mutual information maximization mechanism and incorporates it into the training objective as an additional supervisory signal."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper has a strong motivation, applying the denoising process of diffusion models to label disambiguation in partial-label learning.\n2.\tThe writing is well organized and adheres to academic conventions."}, "weaknesses": {"value": "1.\tThe paper‚Äôs review of existing methods is overly abstract and insufficiently specific. For example: ‚ÄúOn the one hand, label ambiguity and the complex relationships among candidate labels make it difficult for some methods to achieve stable disambiguation in a single pass, leading to limited robustness.‚Äù Nearly all PLL methods aim to address precisely this issue, so this statement does not accurately pinpoint the concrete shortcomings of prior work.\n2.\tRegarding ‚ÄúOn the other hand, label disambiguation and feature learning are often decoupled, and lack mechanisms that effectively associate labels with the semantic content of images.‚Äù The intended claim seems to be that disambiguation and representation learning are decoupled and lack a mechanism to link them for stable, mutually reinforcing training. However, the proposed DAFF targets only representation learning and does not provide a joint mechanism. Moreover, many existing PLL approaches already consider disambiguation and representation learning jointly rather than as two separate stages.\n3.\tThe paper does not explain why diffusion models are suitable for label disambiguation in PLL, which is crucial. Since the label information fed into the diffusion model is noisy, if the denoising process outputs a noisy label at each step, can it ultimately recover the true label? This justification is missing."}, "questions": {"value": "1.\tAre there any fundamental issues with applying diffusion models to partial-label learning? Why is $Y_0$ initialized via disambiguation from similar examples (KNN), and how can label information that is still noisy be used as input to a diffusion model? In image denoising, the diffusion model‚Äôs initial input is a clean, fully observed image. When applying diffusion to PLL, the input labels are noisy. Why does using KNN-based initialization improve the stability of disambiguation? What problems arise if it is not used?\n2.\tThe proposed DAFF module appears to act only on representation learning and does not resolve the decoupling between disambiguation and representation learning found in prior methods.\n3.\tFrom the ablation results, the contributions of components D and F are modest, yet the overall gain over existing methods is large. This suggests the diffusion model itself contributes substantially. It would be better to show ablations across multiple datasets.\n4.\tAfter training, can the model use the diffusion denoising process to directly predict labels for unseen examples?\n5.\tIn Eq. (15), what does $ùëâ$ denote? What is the relationship between Eq. (9) and Eqs. (10), (15), and (16)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QueRdVViyu", "forum": "R2YwWxO2U0", "replyto": "R2YwWxO2U0", "signatures": ["ICLR.cc/2026/Conference/Submission15440/Reviewer_Zzau"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15440/Reviewer_Zzau"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534397469, "cdate": 1761534397469, "tmdate": 1762925721304, "mdate": 1762925721304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MDMPLL, a diffusion-based framework for partial label learning (PLL).\nThe authors treat label ambiguity as a forward diffusion process that gradually injects noise into labels and design a reverse denoising process for label disambiguation.\nThey further incorporate a Dual-Path Attention Feature Fusion (DAFF) module to combine shallow and deep features, and a Feature‚ÄìLabel Mutual Information (FLMI) objective to encourage alignment between features and labels.\nExperimental results on several benchmark PLL datasets demonstrate that the proposed method performs competitively compared to prior approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe attempt to interpret label ambiguity via diffusion dynamics is novel and interesting.\n\t2.\tThe idea of integrating mutual information maximization into label disambiguation is conceptually meaningful.\n\t3.\tThe paper is generally well written and easy to follow."}, "weaknesses": {"value": "1. Although Section 3 provides mathematical formulations, the approach largely mirrors standard DDPM procedures with only superficial adaptation to labels. The claimed connection between diffusion and mutual information is not derived or justified; no analysis is provided on how FLMI quantitatively improves disambiguation or generalization.\n2. No ablation on the diffusion depth or comparison with simpler PLL baselines using consistency regularization is provided.\n3. Visualization results (Fig. 4‚Äì5) are qualitative and do not convincingly demonstrate the claimed progressive disambiguation."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HM2q6QRsSz", "forum": "R2YwWxO2U0", "replyto": "R2YwWxO2U0", "signatures": ["ICLR.cc/2026/Conference/Submission15440/Reviewer_dPb4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15440/Reviewer_dPb4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906642081, "cdate": 1761906642081, "tmdate": 1762925720871, "mdate": 1762925720871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MDMPLL, a diffusion-based framework for partial label learning (PLL) that models label ambiguity via forward noise addition to simulate candidate labels and reverse denoising for disambiguation. It incorporates Dual-Path Attention Feature Fusion (DAFF) to blend shallow and deep features as conditional inputs, and maximizes Feature-Label Mutual Information (FLMI) to align labels with semantics. A KNN-based initial disambiguation and dynamic label updates enhance stability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. nnovative adaptation of diffusion models to PLL, reframing disambiguation as probabilistic denoising with theoretical ties to variational bounds.\n\n2. DAFF effectively fuses multi-level features via attention, improving representation quality; FLMI provides additional supervision for robustness.\n\n3. Comprehensive ablations on components (DAFF, FLMI, diffusion steps) and hyperparameters; strong gains over baselines like PRODEN, CC, RC (e.g., +5-10% on CIFAR-100)."}, "weaknesses": {"value": "1. Role of mutual information estimation in disambiguation using diffusion model is not clearly explained; DAFF module seems disconnected from other parts of the method, and its relationship with other modules is unclear.\n\n2. Mutual information guidance, as one of the core contributions, should not be excluded from Figure 1's overall framework.\n\n3. In Algorithm 1 table, isn't the ultimate goal of the algorithm to get a model with strong generalization ability? Why return a clean label matrix?"}, "questions": {"value": "1. How does FLMI estimation interact with diffusion variance schedules? Could adaptive scheduling based on MI improve convergence?\n\n2. Why not integrate DAFF into the noise prediction network end-to-end instead of as a separate fusion step?."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V228BgaPLf", "forum": "R2YwWxO2U0", "replyto": "R2YwWxO2U0", "signatures": ["ICLR.cc/2026/Conference/Submission15440/Reviewer_2MF9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15440/Reviewer_2MF9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010522984, "cdate": 1762010522984, "tmdate": 1762925720403, "mdate": 1762925720403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work presents a method for partial label learning or label disambiguation by invoking a diffusion process in the label space and progressively noising and denoising the labels to learn a robust denoiser. The method is guided by mutual information between labels and features derived from the shallow and deep sections of encoders, fused using Dual Path Attention Feature Fusion, eventually maximizing Feature Label Mutual Information using the InfoNCE loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method shows decent empirical gains against baselines.\n2. Incorporation of attention based features and MI guidance may be well informed in this problem and leads to demonstrable gains."}, "weaknesses": {"value": "1. Some of the proofs are hand-wavy and/or already shown (Appendix D and E). The forward and reverse processes come from diffusion models literature. Appendix F outlines known results from EM literature. These proofs don't incorporate the specifics of the method proposed to synthesize something novel.\n\n2. Comparisons against LRA-Diffusion [1] are missing which also uses pre-trained network features as guidance.\n\n3. The authors could shed more light into the rationale behind using both pre-trained and un-trained encoders for deep and shallow feature sets respectively.\n\n4. The motivation behind label and feature vector similarity may be mis-guided. Labels lie in a categorical simplex, whereas features are continuous embeddings. The authors could justify this choice in principle.\n\n5. Training and inference runtime overheads for the methods aren't reported.\n\n\n[1] Chen, Jian, et al. \"Label-retrieval-augmented diffusion models for learning from noisy labels.\" Advances in Neural Information Processing Systems 36 (2023): 66499-66517."}, "questions": {"value": "1. Is there a study that demonstrates the effects of the label filtering stage at the beginning?\n2. Could the authors kindly delineate the differences between LRA-Diffusion and this work?\n\n[1] Chen, Jian, et al. \"Label-retrieval-augmented diffusion models for learning from noisy labels.\" Advances in Neural Information Processing Systems 36 (2023): 66499-66517."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GZ9B0gVwHS", "forum": "R2YwWxO2U0", "replyto": "R2YwWxO2U0", "signatures": ["ICLR.cc/2026/Conference/Submission15440/Reviewer_jV29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15440/Reviewer_jV29"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121787915, "cdate": 1762121787915, "tmdate": 1762925716620, "mdate": 1762925716620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}