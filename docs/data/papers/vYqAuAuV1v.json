{"id": "vYqAuAuV1v", "number": 15127, "cdate": 1758248036603, "mdate": 1759897326666, "content": {"title": "Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets", "abstract": "Dataset distillation (DD) has emerged as a powerful paradigm for dataset compression, enabling the synthesis of compact surrogate datasets that approximate the training utility of large-scale ones. While significant progress has been achieved in distilling image datasets, extending DD to the video domain remains challenging due to the high dimensionality and temporal complexity inherent in video data. Existing video distillation (VD) methods often suffer from excessive computational costs and struggle to preserve temporal dynamics, as naïve extensions of image-based approaches typically lead to degraded performance. In this paper, we propose a novel uni-level video dataset distillation framework that directly optimizes synthetic videos with respect to a pre-trained model. To address temporal redundancy and enhance motion preservation, we introduce a temporal saliency-guided filtering mechanism that leverages inter-frame differences to guide the distillation process, encouraging the retention of informative temporal cues while suppressing frame-level redundancy. Extensive experiments on standard video benchmarks demonstrate that our method achieves state-of-the-art performance, bridging the gap between real and distilled video data and offering a scalable solution for video dataset compression.", "tldr": "We introduce an efficient framework with temporary saliency-aware filter to distill the video action datasets", "keywords": ["dataset distillation", "video datasets"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/09d5bd4b272dc4e4485edf4af20712698fb3ab58.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Video-As-Prompt (VAP), a unified framework for semantic-controlled video generation. Instead of relying on pixel-aligned structures or task-specific fine-tuning, VAP treats a reference video as a semantic prompt, guiding generation through in-context control. The method employs a Mixture-of-Transformers (MoT) architecture—combining a frozen Video Diffusion Transformer with a trainable expert network connected via full attention—and a temporally biased position embedding to avoid spurious spatial mappings. The authors also build VAP-Data, a large dataset with 100K paired samples across 100 semantic conditions. Experiments demonstrate that VAP achieves performance comparable to commercial systems like Kling and Vidu, while maintaining strong zero-shot generalization and scalability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces a conceptually novel Video-As-Prompt (VAP) paradigm that reformulates semantic-controlled video generation as an in-context generation problem. This reframing is elegant and unifies previously fragmented approaches (e.g., per-condition fine-tuning, task-specific modules) under a single framework.\n2.\tThe paper provides a clear motivation for addressing the limitations of structure-controlled and condition-specific models. The theoretical formulation and empirical ablations (e.g., on RoPE bias, expert architecture, scalability) are thorough and convincing.\n3.\tA notable highlight is the demonstrated zero-shot generalization to unseen semantic conditions, suggesting that the model captures abstract semantic correspondences beyond specific training distributions.\n4.\tThe authors construct VAP-Data, the largest dataset for semantic-controlled video generation, covering 100K paired videos across 100 semantic conditions. This dataset provides a strong benchmark and a solid foundation for future research in this area."}, "weaknesses": {"value": "1.\tThe Mixture-of-Transformers (MoT) architecture, while powerful, nearly doubles the total parameter count (adding approximately 5 billion parameters) and substantially increases computational cost.\n2.\tWhile comparisons with VACE, LoRA finetuning, and commercial models are included, the paper lacks detailed benchmarking against concurrent unified frameworks (e.g., Omni-Effects (Mao et al., 2025)) that also explore multi-condition control. Such comparison would strengthen claims of superiority in unification and generalization.\n3.\tThe zero-shot results (e.g., Fig. 7) are visually compelling but lack quantitative validation. Incorporating objective metrics or user preference scores for unseen conditions would make the zero-shot generalization claim more convincing."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OLijiGXsUc", "forum": "vYqAuAuV1v", "replyto": "vYqAuAuV1v", "signatures": ["ICLR.cc/2026/Conference/Submission15127/Reviewer_aNEz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15127/Reviewer_aNEz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730922589, "cdate": 1761730922589, "tmdate": 1762925444791, "mdate": 1762925444791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel and efficient video dataset distillation method that leverages temporal saliency to preserve motion dynamics while compressing video data. The proposed framework achieves superior performance across multiple benchmarks with lower computational cost compared to existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The TSGF mechanism is an innovative and lightweight way to capture temporal importance without relying on heavy optical flow or 3D convolutions.\n2. The model design is computationally efficient, adopting uni-level optimization design instead of complex bi-level optimization, which substantially reduces memory footprint and training cost, making it suitable for large-scale video data distillation.\n3. The method achieves state-of-the-art results on multiple standard benchmarks (UCF101, HMDB51, Kinetics-400, SSv2), maintaining strong performance even under extreme compression ratios (e.g., IPC = 1).\n4. The experimental evaluation is extensive, covering diverse datasets, compression ratios, architectures, and ablation studies, providing convincing empirical support for the proposed framework."}, "weaknesses": {"value": "1. The paper lacks a formal theoretical definition and justification of “temporal saliency,” and the TSGF design appears heuristic, without mathematical modeling or convergence analysis.\n2. Using raw inter-frame differences may not capture complex motion semantics and may fail in scenes with camera motion or background clutter.\n3. Although the method claims potential applicability to other video tasks, only preliminary results are presented for temporal action segmentation, without validation on detection, tracking, or video generation tasks.\n4. The presentation quality could be improved; for instance, the font sizes in Figure 2 appear inconsistent, and Tables 3 and 4 use mismatched formatting, which slightly affects readability."}, "questions": {"value": "1. Further clarification on the rationale for using frame differencing as the saliency measure would be valuable, possibly supported by additional theoretical analysis or empirical ablation to justify its necessity and effectiveness.\n2. It remains unclear whether TSGF can robustly handle diverse motion types—including rigid, non-rigid, and camera-induced motion—and whether there exist failure cases for specific video categories.\n3. The single-stage optimization strategy might be prone to local minima; theoretical or experimental analysis of its convergence and stability, especially on complex video datasets, would strengthen the paper.\n4. A deeper discussion comparing the proposed temporal modeling with existing approaches (e.g., optical flow, 3D convolutional features, or recurrent architectures) would help highlight the unique advantages of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zrmAtivST7", "forum": "vYqAuAuV1v", "replyto": "vYqAuAuV1v", "signatures": ["ICLR.cc/2026/Conference/Submission15127/Reviewer_8CWt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15127/Reviewer_8CWt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806917187, "cdate": 1761806917187, "tmdate": 1762925444230, "mdate": 1762925444230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a uni-level video dataset distillation framework \nthat directly optimizes synthetic videos against a fixed, pre-trained classifier while enhancing temporal fidelity \nthrough a Temporal Saliency Guided Filter (TSGF). \nThe framework involves three key stages: 1. training a teacher model on real videos; \n2. optimizing synthetic videos by aligning cross-entropy loss and batch normalization statistics with the teacher model; \nand 3. applying a saliency-guided video augmentation that emphasizes motion-consistent regions. \nTemporal saliency is computed from inter-frame differences with smoothing and used both to modulate gradient magnitude during optimization \nand to adaptively select augmentation regions. Experiments on diverse video datasets demonstrate consistent improvements \nover prior dataset distillation methods, with ablations showing the contribution of each module."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Conceptually clean and efficient framework: \nThe uni-level training scheme is straightforward yet effective, \navoiding iterative teacher-student feedback while maintaining strong performance.\n\n2. Temporal Saliency Guided Filter (TSGF): The proposed TSGF provides a principled way to incorporate motion cues \nwithout requiring optical flow or explicit temporal modeling, which enhances both interpretability and efficiency.\n\n\n3. Comprehensive empirical validation: The framework achieves consistent gains across diverse datasets \nand varying compression ratios. \nReported results show competitive scalability even under efficient distillation budgets.\n\n4. Potential for broader application: The idea of saliency-weighted optimization could generalize \nto other video understanding or dynamic scene compression tasks."}, "weaknesses": {"value": "1. Insufficient positioning relative to decoupled dataset distillation methods.\nThe pipeline (Sec. 3.2, Eq. (4)–(5)) resembles decoupled optimization schemes such as SRe2L, \nwhere a frozen teacher guides the synthetic data optimization. \nThe distinction between the proposed uni-level framework and existing decoupled methods is not clearly explained, \nleaving the degree of conceptual novelty somewhat ambiguous.\n\n\n\n2. Incomplete hyperparameter specification for the Temporal Saliency Guided Filter.\nCritical parameters in Eq. (7) and Eq. (8), such as the smoothing window size $k$, weighting coefficients $\\alpha_k$, \nand the $\\epsilon$ constant for stability, are not reported. \nNo sensitivity or ablation analysis is presented for these factors, which limits reproducibility and understanding of robustness.\n\n\n3. Limited robustness and failure mode analysis.\nWhile the paper claims the TSGF improves robustness under motion variation, \nthere is no quantitative test for scenarios with strong camera motion or jitter. \nFailure analysis is limited to coarse class splits in Tab. 7 without qualitative inspection.\n\n4. Restricted evaluation scope across model architectures.\nThe cross-model evaluation (Tab. 4) considers only small CNN-based backbones. \nThere is no validation on transformer-based or modern video architectures, which limits the generalizability claim. \nAdditionally, quantitative comparison with a modern decoupled baseline (e.g., SRe2L) is missing, with only a qualitative reference in Fig. 3."}, "questions": {"value": "1. It would improve clarity to relocate Fig. 2 (framework overview) to the beginning of the Method section \nand Alg. 1 (optimization steps) to its end for smoother narrative flow."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g8XOLcuJAV", "forum": "vYqAuAuV1v", "replyto": "vYqAuAuV1v", "signatures": ["ICLR.cc/2026/Conference/Submission15127/Reviewer_D4hu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15127/Reviewer_D4hu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990843243, "cdate": 1761990843243, "tmdate": 1762925443828, "mdate": 1762925443828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This method proposes a uni-distillation framework for video datasets, introducing a Temporal-Spatial Gating Function (TSGF) for automatic frame attention and data augmentation during post-training. The approach achieves advanced results on various benchmarks, demonstrating robustness to dataset scale and motion strength."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Proposed method is effective at pruning redundant frames, making the distillation process significantly faster and more tractable for massive datasets and achieves new state-of-the-art accuracy on multiple benchmarks.\n2. The TSGF is a well-motivated addition that explicitly addresses the challenge of preserving temporal dynamics, a critical weakness in previous video distillation methods."}, "weaknesses": {"value": "1. Dataset distillation maintains performance with significantly smaller synthetic datasets; however, the baseline and its variants appear to underperform (e.g., 22.4% on K400). Since smaller models tend to overfit small-scale datasets, they may not adequately demonstrate the generalization capability of distilled datasets. While Table 4 shows that this method benefits larger models, could the authors further validate this finding using video models of standard scale rather than those with only a few layers?\n2. The performance improvements differ between background-based datasets (K400) and motion-based datasets (SSv2), yet this paper lacks analysis of these differences. Additionally, marking the average number of samples per class would facilitate better understanding of the performance variations.\n3. What are the sizes of the distilled dataset and the full dataset for MiniUCF, and what is the accuracy achieved on the full dataset?"}, "questions": {"value": "Please refer to Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lyzcGGvILq", "forum": "vYqAuAuV1v", "replyto": "vYqAuAuV1v", "signatures": ["ICLR.cc/2026/Conference/Submission15127/Reviewer_tVoB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15127/Reviewer_tVoB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762506787016, "cdate": 1762506787016, "tmdate": 1762925443281, "mdate": 1762925443281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}