{"id": "fqc1947QsK", "number": 4852, "cdate": 1757776821190, "mdate": 1763273454732, "content": {"title": "DreamOmni2: Multimodal Instruction-based Editing and Generation", "abstract": "Recent advancements in instruction-based image editing and subject-driven generation have garnered significant attention, yet both tasks still face limitations in meeting practical user needs. Instruction-based editing relies solely on language instructions, which often fail to capture specific editing details, making reference images necessary. Meanwhile, subject-driven generation is limited to combining concrete objects or people, overlooking broader, abstract concepts. To address these challenges, we propose two novel tasks: multimodal instruction-based editing and generation. These tasks support both text and image instructions and extend the scope to include both concrete and abstract concepts, greatly enhancing their practical applications. We introduce DreamOmni2, tackling two primary challenges: data creation and model framework design. Our data synthesis pipeline consists of three steps: (1) using a feature mixing method to create extraction data for both abstract and concrete concepts, (2) generating multimodal instruction-based editing training data using the editing and extraction models, and (3) further applying the extraction model to create training data for multimodal instruction-based editing. For the framework, to handle multi-image input, we propose an index encoding and position encoding shift scheme, which helps the model distinguish images and avoid pixel confusion. Additionally, we introduce joint training with the VLM and our generation/editing model to better process complex instructions. In addition, we have proposed comprehensive benchmarks for these two new tasks to drive their development. Experiments show that DreamOmni2 has achieved impressive results. Models and codes will be released.", "tldr": "", "keywords": ["Image Editing and generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9ae56b1ec5cc80d6aa3972c86f3336d579231c32.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes DreamOmni2, a unified model framework designed for multimodal instruction-based image editing and generation. This framework allows users to provide both text and multiple images instructions, enhancing both concrete and abstract visual content creation. The approach leverages a comprehensive three-stage data pipeline and a unique joint training scheme for both a generation/editing model and a vision-language model (VLM)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces two highly practical tasks — multimodal instruction-based editing and generation, significantly advancing the capabilities of image models. By handling both abstract and concrete concepts, DreamOmni2 is a step forward in bridging the gap between simple object manipulations and more complex conceptual image transformations.\n\n2. The paper presents a detailed and innovative three-stage data synthesis pipeline that allows for the generation of high-quality training data for both the extraction and editing tasks. The feature mixing method is particularly compelling, as it avoids edge-blending issues in previous approaches, which contributes to higher quality output.\n\n3. The incorporation of a Vision-Language Model (VLM) for joint training improves the model’s understanding of complex user instructions, making the system more adaptable to real-world use cases.\n\n4. The DreamOmni2 benchmark is well-constructed, including both concrete object and abstract attribute tasks. This allows for meaningful performance evaluation and comparison with existing models, showing DreamOmni2’s superior capability in handling diverse editing and generation requests."}, "weaknesses": {"value": "1. The three-stage data pipeline, while innovative, may be difficult to scale or implement outside the authors’ controlled setup. More details on how to adapt this pipeline for broader applications (e.g., other domains or industries) would help in assessing the practical value of DreamOmni2 beyond the experimental context.\n\n2. Insufficient Evaluation Methodology: The paper primarily relies on VLM-based and human evaluations, which, while useful, lack more objective and rule-based evaluation methods to provide comprehensive assessment. Notably, the work lacks comparison and discussion with DreamBench++ (Peng et al., 2024), which addresses similar evaluation challenges by replacing DINO-based metrics with GPT-based evaluations that demonstrate better correlation with human judgment. Incorporating such established evaluation frameworks or developing comparable rule-based metrics would strengthen the credibility and reproducibility of the reported results, making the claims more verifiable beyond subjective assessments."}, "questions": {"value": "How does the method ensure the diversity and representativeness of the synthetic data used for training, particularly for abstract attributes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0jtxpZKrBi", "forum": "fqc1947QsK", "replyto": "fqc1947QsK", "signatures": ["ICLR.cc/2026/Conference/Submission4852/Reviewer_Yy5h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4852/Reviewer_Yy5h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830956104, "cdate": 1761830956104, "tmdate": 1762917615868, "mdate": 1762917615868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "S8xvq0NQKX", "forum": "fqc1947QsK", "replyto": "fqc1947QsK", "signatures": ["ICLR.cc/2026/Conference/Submission4852/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4852/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763273453616, "cdate": 1763273453616, "tmdate": 1763273453616, "mdate": 1763273453616, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a pipeline for constructing a multimodal instruction-based editing and generation dataset and introduces a VLM-trained model based on Kontext. The results appear to demonstrate the model's strong ability to understand multimodal instructions. However, the paper lacks a significant amount of necessary details, making it academically unsuitable for acceptance at this stage."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposed a pipeline for constructing a multimodal instruction-based editing and generation dataset.\n- The experimental results appear very promising."}, "weaknesses": {"value": "The writing of the paper is excessively poor, lacking a substantial amount of detail, which makes it very difficult to follow. See Questions for details."}, "questions": {"value": "- How was the extraction model trained, and what were its inputs and outputs? Figure 2 uses different image examples in Stage 2 and Stage 1, making it difficult to understand.\n\n- How was the prompt in Stage 1 of Figure 2 constructed?\n\n- How many training instances were ultimately constructed?\n\n- How was the VLM integrated and trained with Kontext, and what was the data format?\n\n- What are the computational details of the model's evaluation results, and how was successful editing determined?\n\n- How was the human evaluation designed? For example, how many annotators were assigned to evaluate each instance, and how was the evaluation conducted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Uq1HiSufeN", "forum": "fqc1947QsK", "replyto": "fqc1947QsK", "signatures": ["ICLR.cc/2026/Conference/Submission4852/Reviewer_tA9m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4852/Reviewer_tA9m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897284617, "cdate": 1761897284617, "tmdate": 1762917614510, "mdate": 1762917614510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets unified multimodal instruction for both image editing and generation with multi-reference inputs, and it assembles all the essential ingredients: a clear problem setup, a data construction pipeline, a model built on a capable base, and evaluations that include both qualitative visualizations and quantitative scores. However, it lacks a clearly illustrated model architecture and more rigorously calibrated validation for the VLM-as-judge evaluation choice."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clearly formulates a practical setting: multi-reference, multimodal instructions spanning both editing and generation, including abstract attribute transfer.\n2. Provides both qualitative and quantitative evaluations, with side-by-side visual examples and automatic scoring to support claims.\n3. Data pipeline is spelled out and reproducible.\n4. Writing and organization are clean, making the overall contribution and experiment flow easy to track."}, "weaknesses": {"value": "1. No model architecture diagram. The dataflow and where each tweak attaches to the backbone are not visualized, which is not clear.\n2. Heavy reliance on VLM-based scoring plus limited human evaluation, with no calibration evidence (e.g., inter-rater agreement, correlation with humans, confidence intervals).\n3. Lacks objective/quantitative metrics for controllability and fidelity.\n4. Few comparisons on established single-image editing and T2I generation benchmarks, which are still necessary if the method truly subsumes the single-image case. Or maybe the author could emphasize and explain the differences from previous multi-image controllable image generation and editing methods."}, "questions": {"value": "1. There is already a substantial of work that uses multimodal instructions rather than purely textual ones. In that context, does the abstract’s claim \"relying only on language for instructions is a limitation\" actually hold? Shouldn’t the paper more thoroughly compare with and discuss prior work based on multimodal instruction?\n2. The paper lacks a model architecture diagram, which makes the method less intuitive to understand.\n3. Is it reliable to evaluate solely with a VLM and human raters? Is there evidence that this is reliable? Why are there no additional image-side quantitative metrics?\n4. If the method can perform multi-image editing, then it should also handle single-image editing and generation, therefore it ought to be evaluated and compared on prior single-image editing and T2I generation benchmarks as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RERBB8wjPG", "forum": "fqc1947QsK", "replyto": "fqc1947QsK", "signatures": ["ICLR.cc/2026/Conference/Submission4852/Reviewer_hXfa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4852/Reviewer_hXfa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994934045, "cdate": 1761994934045, "tmdate": 1762917611258, "mdate": 1762917611258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}