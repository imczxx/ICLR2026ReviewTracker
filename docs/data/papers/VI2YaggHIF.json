{"id": "VI2YaggHIF", "number": 19598, "cdate": 1758297556274, "mdate": 1759897031077, "content": {"title": "DeepRAG: Thinking to Retrieve Step by Step for Large Language Models", "abstract": "Large Language Models (LLMs) have shown remarkable reasoning capabilities, while their practical applications are limited by severe factual hallucinations due to limitations in the timeliness, accuracy, and comprehensiveness of their parametric knowledge. Meanwhile, enhancing retrieval-augmented generation (RAG) with reasoning remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling reasonable and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency and boosts answer accuracy by 25.41%, demonstrating its effectiveness in enhancing retrieval-augmented reasoning.", "tldr": "", "keywords": ["retrieval-augmented generation", "adaptive retrieve"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c01b68ee64c2128311853a247fb0b20b5682b70.pdf", "supplementary_material": "/attachment/95b4aca8208e639e11b8550d066811f5fcfa89a1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DeepRAG, a framework that enhances retrieval-augmented generation by modeling reasoning as a Markov Decision Process (MDP). DeepRAG dynamically decides when to retrieve external knowledge and when to rely on internal parametric reasoning through iterative query decomposition. By enabling adaptive and context-aware retrieval, it reduces noise from redundant information and improves both retrieval efficiency and answer accuracy—boosting performance by 25.41%—demonstrating significant advances in retrieval-augmented reasoning for LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well written\n* When to conduct retrieval is an important topic\n* The experiments are extensive"}, "weaknesses": {"value": "* Compared to Serach R1, the paper mainly differs at using the model's native generation ability to determine when to retrieve, but this has already been well studied and the authors do not provide new solution."}, "questions": {"value": "* Table 1 shows that Qwen 2.5 32B peforms worse than Llama 3 8B, which is confusing for me, could you explain the reason?\n* Online RL methods such as GRPO typically outperform offline approaches, a trend observed in the performance of Qwen. However, in the case of Llama, online and offline methods show comparable results. What accounts for this difference？\n* In Section 5.3, line 436 states that most questions require 3–5 decomposition steps. However, the majority of queries in HotpotQA and 2Wiki are designed for 2-hop reasoning. Does this imply the presence of significant redundant retrieval? If so, what causes this inefficiency? And do current sota models, such as GPT-5, still exhibit similar tendencies toward redundant reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fXifNrYBnM", "forum": "VI2YaggHIF", "replyto": "VI2YaggHIF", "signatures": ["ICLR.cc/2026/Conference/Submission19598/Reviewer_JFTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19598/Reviewer_JFTj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626269876, "cdate": 1761626269876, "tmdate": 1762931464579, "mdate": 1762931464579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DeepRAG, a retrieval-augmented reasoning framework that formulates iterative query decomposition and retrieval decisions as a Markov Decision Process. It introduces three core components: Binary Tree Search to exhaustively explore parametric and retrieved paths for each subquery and identify minimal-retrieval correct trajectories; Imitation Learning through supervised fine-tuning on these trajectories with masked loss over retrieved tokens; and a Chain of Calibration that includes both offline and online variants to enhance the model’s awareness of its knowledge boundaries. Evaluated on in-distribution datasets such as HotpotQA and 2WikiMultihopQA, as well as out-of-distribution datasets including PopQA, WebQuestions, and MuSiQue, DeepRAG achieves a 25.41 percent accuracy improvement over previous adaptive retrieval-augmented generation methods while also reducing retrieval calls."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Strong empirical gains: 25.41% accuracy lift is reported across five datasets, with ablation likely showing each stage’s contribution; out-of-dist PopQA and Freebase-absent WebQuestions stress robustness.\n- Uses a fixed-depth priority queue (lowest retrieval count first) and discards unsolvable instances, yielding high-quality imitation data without oracle subqueries."}, "weaknesses": {"value": "- The method doesn’t seem to offer much innovation. Among the many existing approaches that use reinforcement learning for autonomous multi-turn retrieval, I didn’t find any particularly striking or novel insights.\n- All experiments assume a fixed Wikipedia retriever (presumably BM25 or Contriever). What about comparison with some deep research methods, they also use multi-turn retrieval?"}, "questions": {"value": "- For WebQuestions (Freebase-backed), what fraction of ground-truth answers are un-retrievable from Wikipedia, and how does DeepRAG-RLon handle verified absence (e.g., explicit “unknown” vs. hallucination)?\n- How does the model behave on single-hop factual queries (e.g., PopQA)? Does calibration reduce retrieval to near-zero, and is parametric accuracy preserved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5X4m6x9f2c", "forum": "VI2YaggHIF", "replyto": "VI2YaggHIF", "signatures": ["ICLR.cc/2026/Conference/Submission19598/Reviewer_taGT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19598/Reviewer_taGT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750231921, "cdate": 1761750231921, "tmdate": 1762931463557, "mdate": 1762931463557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DeepRAG models retrieval-augmented reasoning as a Markov Decision Process, enabling dynamic per-subquery decisions on whether to retrieve external knowledge or rely on parametric knowledge. The framework uses: (1) Binary Tree Search to explore retrieval strategies, (2) Imitation Learning on minimal-cost trajectories, and (3) Chain of Calibration (offline/online) to teach knowledge boundary recognition."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-Motivated Problem\nExisting RAG systems apply retrieval indiscriminately—either over-retrieving (wasting compute) or under-retrieving (missing information). DeepRAG addresses this through atomic decisions: decompose queries into subqueries, then decide retrieval necessity per subquery rather than per original query.\n2. Strong Empirical Results\n\n25.41% accuracy improvement over baselines (Table 1)\nBetter retrieval efficiency: lower average steps than Multi-Step Retrieval (Table 2)\nGood generalization on out-of-distribution datasets (PopQA, WebQuestions)\nHigher knowledge boundary alignment (MCC scores in Table 3)\n\n3. Comprehensive Ablations\nValidates minimal-cost path selection (Figure 6), atomic query decomposition effectiveness (Figure 4: fewer conjunctions/pronouns per subquery), and necessity of calibration stage (RLoff/RLon > Imi).\n4. Practical Design\nMasked loss over retrieved documents prevents noise learning; end-to-end trainable without separate classifiers; works across different LLM sizes (Llama-3-8B, Qwen-2.5-32B)."}, "weaknesses": {"value": "1. Overstated Technical Novelty\nThe \"MDP formulation\" is superficial packaging:\n\nTransitions are deterministic (not stochastic as typical MDPs)\nNo actual policy search—just supervised learning on pre-computed trajectories\nBinary Tree Search is exhaustive enumeration (2^N paths), not a novel algorithm\n\n\n2. Narrow Evaluation Scope\nOnly tested on artificially constructed multi-hop QA benchmarks (HotpotQA, 2WikiMultiHop, MuSiQue). Missing evaluations on:\n\nSingle-hop questions (does it over-retrieve?)\nReal-world queries: \"Draft a rejection email,\" \"Compare iPhone vs Samsung,\" \"When will my order ship?\"\nNon-QA tasks: summarization, dialogue, creative writing\n\nThe query patterns in academic benchmarks don't represent real deployment scenarios.\n\n3. Missing Simple Baselines\nThe paper requires three complex stages but never compares to a one-stage baseline:"}, "questions": {"value": "1. Algorithm Correctness Issue\nAlgorithm 1 (page 5) has a critical flaw:\npythonLine 7: if IsEqual(o, y) then return h  # Returns FIRST correct path\nThis implements greedy search, not optimal search. Counterexample:\n\nPath A: [retrieve, retrieve] cost=2 (found first) \nPath B: [parametric, parametric] cost=0 (explored later) \n\nThe paper claims \"minimal retrieval cost\" but the algorithm returns whichever correct path is found first in priority queue order—not guaranteed optimal.\n\n2. Questionable \"Optimality\" Claims\nTable 1 shows counterintuitive result:\n\nRLoff (trained on \"optimal paths\" from Stage I): 41.47\nRLon (self-exploration): 41.05 (better!)\n\nIf Stage I finds optimal paths, why does self-exploration outperform? Possible explanations:\n\nAlgorithm 1's bug means Stage I paths aren't actually optimal\nRLon discovers better strategies not in training data\nGRPO is simply better than DPO for this task\n\nThis undermines confidence in the entire Stage I data synthesis process.\n\n3. Reproducibility Concerns\nMany critical details missing:\n\nBinary tree search: max depth? timeout? handling of unsolvable queries?\nTraining time for tree search (exponential in depth)?\nInference latency breakdown (subquery generation overhead)?\nDPO hyperparameter β? GRPO rollout count G?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e1uTNY9c0Q", "forum": "VI2YaggHIF", "replyto": "VI2YaggHIF", "signatures": ["ICLR.cc/2026/Conference/Submission19598/Reviewer_TwZX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19598/Reviewer_TwZX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19598/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964054767, "cdate": 1761964054767, "tmdate": 1762931462909, "mdate": 1762931462909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}