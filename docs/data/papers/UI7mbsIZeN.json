{"id": "UI7mbsIZeN", "number": 18756, "cdate": 1758290652906, "mdate": 1759897082855, "content": {"title": "Dyslexify: A Mechanistic Defense Against Typographic Attacks in CLIP", "abstract": "Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks.\nIn this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model's layers that causally extract and transmit typographic information to the cls token.\nBuilding on these insights, we introduce Dyslexify - a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, dyslexify improves performance by up to 22.06\\% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1\\%, and demonstrate its utility in a medical foundation model for skin lesion diagnosis. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition.", "tldr": "We present Dyslexify, a training-free defense for CLIP that ablates a causal attention circuit, yielding up to +22.1% robustness on ImageNet-100-Typo while keeping standard accuracy within 1% of the baseline.", "keywords": ["Multimodality", "Circuit analysis", "Probing", "AI Safety", "Vision transformers"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77f8860a06be365879d5c861b75cf22d9923177d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses typographic attacks on CLIP models by identifying and ablating specific attention heads responsible for processing typographic information. The authors introduce the Typographic Attention Score to locate specialized heads in the latter layers of CLIP's vision encoder that extract and transmit text information to the CLS token. Their training-free defense method, Dyslexify, selectively ablates these heads to improve robustness against typographic attacks on ImageNet-100-typo while maintaining standard accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a novel mechanistic interpretation approach for the localization of typographic processing, such as linear probes showing abrupt emergence of typographic understanding in later layers and the Typographic Attention Score identifying specific high-attention heads. The causal intervention experiments demonstrate that manipulating attention weights in identified heads directly controls typographic vulnerability.\n2. The paper introduces practical training-free defense with scalability, unlike existing defenses that require gradient-based optimization. Dyslexify operates purely through inference-time circuit ablation. This approach scales seamlessly to billion-parameter models and achieves superior performance compared to training-based baselines."}, "weaknesses": {"value": "1. Limited analysis of failure modes and edge cases. The paper does not thoroughly investigate when and why Dyslexify fails. For instance, what types of typographic attacks remain effective after ablation? The paper would benefit from failure case analysis showing examples where the defense is insufficient and discussing the underlying reasons.\n2. Incomplete evaluation of spatial token applications. The authors acknowledge that downstream applications use spatial tokens rather than only the CLS token, which limits Dyslexify's applicability. However, no experiments quantify this limitation. The paper may include experiments like (a) whether typographic information persists in spatial tokens after CLS ablation, (b) how this affects downstream task performance."}, "questions": {"value": "1. Transferability across CLIP variants. The experiments focus on OpenCLIP models. Do the identified typographic circuits transfer to other CLIP variants? Are the same head positions specialized for typography across architectures?\n2. Interaction with other capabilities. Does ablating typographic circuits affect related capabilities like OCR, scene text understanding, or processing images with incidental text (e.g., street signs, book covers)? \n3. Mechanism of typographic understanding. Why do typographic heads emerge specifically in the latter half of the network? Is this related to the hierarchical feature learning in vision transformers? Does pre-training data composition (text-heavy vs. text-sparse images) influence circuit formation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hhXIjGGwFd", "forum": "UI7mbsIZeN", "replyto": "UI7mbsIZeN", "signatures": ["ICLR.cc/2026/Conference/Submission18756/Reviewer_hKir"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18756/Reviewer_hKir"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877250139, "cdate": 1761877250139, "tmdate": 1762928482378, "mdate": 1762928482378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Dyslexify, a training-free defense that select attention head circuits using attention head score to defend the model from typographic attacks. Through layer-wise probing and attention-pattern analysis, the authors identify a small set of late-layer attention heads that specifically transmit typographic information to the CLS token. The method yields substantial robustness gains on several typographic attack benchmarks while preserve the model performances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a clear causal explanation of where typographic vulnerability arises inside CLIP.\n\n2. Dyslexify does not require fine-tuning, gradients, or retraining, making it scalable to large CLIP backbones.\n\n3. The method improves the robustness significantly across multiple datasets. \n\n4. The presentation and explanation are rather clear."}, "weaknesses": {"value": "1. The defense intentionally reduces CLIP’s ability to read text within images, which may negatively affect tasks where typography is a key visual feature (e.g., signs, product labels, book covers). The authors should include ablation studies measuring performance drops on text-dependent classes to quantify this trade-off.\n\n2. The typographic attention score is computed using English printed text placed in fixed locations. It remains unclear whether the identified circuit generalizes to other forms of typography, such as stylized or handwritten fonts, varied color/contrast conditions, or non-English scripts."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "24nAvJNcPJ", "forum": "UI7mbsIZeN", "replyto": "UI7mbsIZeN", "signatures": ["ICLR.cc/2026/Conference/Submission18756/Reviewer_SCJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18756/Reviewer_SCJM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994151757, "cdate": 1761994151757, "tmdate": 1762928481791, "mdate": 1762928481791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free mechanistic method to defend CLIP models from typographic attacks. The attacks considered are adversarial insertion of misleading text into images, which causes CLIP to misclassify the image. The authors first analyze CLIP’s vision encoder and identify a small set of attention heads in later layers that causally transmit text information (OCR) to the CLS token. They then locate the most OCR-sensitive heads. Then they ablate this “OCR circuit” by zeroing out the identified heads’ contributions to the CLS token during inference. This ablation improves robustness to typographic attacks, shown via experiments on ImageNet-100-Typo, while maintaining standard classification accuracy (<1% drop). The strength of the method is that it does not require fin-tuning. The authors also validate their approach on medical foundation models (melanoma detection) and release a family of “dyslexic” CLIP models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes several contributions\n\n1) A thorough investigation of where OCR capabilities emerge in the CLI Pmodel\n- showing that OCR capabilites emerge in the second half of the CLIP layers is an interesting observation\n- showing that MLP layers reduce OCR information while attention heads improve it\n\n2) Disabling a small typographic circuit can make CLIP robust without retraining. \n- finding the circuit is interesting but alone does not guarantee that it can be disabled to remove OCR function\n- the paper shows that it indeed can, which means that OCR is highly isolated in the model's attention heads, \n\n3) Mechanistic interpretability can help us come up with practical algorithms\n- personally, this is the first work I've seen that uses circuits to do something useful, ie create effective safety interventions\n- the approach was evaluated on a variety of image datasets, including medical images"}, "weaknesses": {"value": "The overal idea in the paper is interesting,but a few questions remain unanswered for me.\n\n1) Experimental results. \nThe main table comparing the proposed method to baselines is Table 1, which shows a performance comparison of the proposed Dyslexify and the baseline Defense-Prefix (DP) measured in terms of the accuracy of the method on classifying data affected by typographic insertion attacs. If I understand correctly, higher is better. While Dislexify beats the DP baseline on RTA-100 and Paint dataset, it is considerably worse on the Disentangling dataset. On average, the proposed method gets 69% while the DP baselines gets 72%. This raises the question of whether the method is consistently better than existing methods on a variety of datasets.\n\n2) Baselines. \nSome simpler defenses are not considered as baselines. For example, using an off the shelf OCR model to identify text and blurring it out or removing it in some way directly in the image. This would destroy the text information at the source (image) instead of relying on the ablation process to \"catch\" the OCR information inside the network. It is possible that this approach does not work as well or reduces performance on the primary tasks, but it should be considered. \n\n3) Unclear computation cost. \nAlthough the proposed defense is training free, it is not computation free. The ablation process requires some computation to identify the circuits, which presumably needs to be done for each new model. Furtthermore, a new typographic dataset must be constructed for each new problem. In the paper the authors create an attack for the ImageNet dataset to identify the attention heads. It is not clear how well this will transfer to new tasks that are not ImageNet.\n\n4) What about tasks that need OCR?\nThere are some image classification tasks where the ability to understand text is helpful for the model. By completely ablating the OCR capability, how does this affect performance on such tasks? For example, fine-grained classification of airplanes by airline could benefit from reading the text of the airline; classifying food or drink products could benefit from OCR as well to read the packaging text.\n\n5) (minor) CLIP models have also been shown to be susceptible to *graphic* attacks that include text OR logos. It would be good to comment on the method's applicatbility to defending against such attacks, but not required.\n\n6) (minor) I'm not sure how I feel about the name 'Dislexic' for the method. But that's a personal preference, does not affect my score.\n\n7) The medical case study (melanoma detection) is compelling rhetorically but I'm not sure how practical it really is. It uses synthetic attacks (overlayed words like “benign” or “malignant”) rather than realistic clinical artifacts. There’s no discussion of whether such text would appear in practice, or who may want to create such attacks."}, "questions": {"value": "I'd like the authors to answer questions 1-4 in the weakness section, specifically\n\n1) does the method perform consistently better than the baselines on a variety of datasets?\n\n2) how does this method compare to a simple OCR + remove baseline, in terms of ease of implementation, computation costs and accuracy?\n\n3) what is the computational cost of the method, and how does it compare to other methods?\n\n4) how would the method perform on tasks that need OCR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CahnozUWye", "forum": "UI7mbsIZeN", "replyto": "UI7mbsIZeN", "signatures": ["ICLR.cc/2026/Conference/Submission18756/Reviewer_xwqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18756/Reviewer_xwqg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113263729, "cdate": 1762113263729, "tmdate": 1762928481140, "mdate": 1762928481140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an image recognition method that is robust against typographic attacks. The authors first identify the key modules responsible for typographic attacks. Then, they compute scores for the attention heads within each module and suppress typographic attacks by removing modules based on these scores."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this paper are as follows.\n\n## Analysis in Sec. 3\n\nThe analysis in Section 3 and Figure 2, where the authors identify the modules necessary for typographic attacks, is fascinating. By examining the model's actual responses, they determine that the later stages are more critical and further discover that the attention layers contribute additional information. This type of analysis is significant and is expected to inspire further research and analysis.\n\n## Design Principles of the Proposed Method\n\nThe proposed method described in Sec. 4.2 and Algorithm 1 determine module utilization by considering performance on both the standard dataset and the typographic dataset, which is a direct and practical approach. This simple and straightforward design principle is clear and compelling."}, "weaknesses": {"value": "The weaknesses of this paper are as follows. While I like the proposed method itself, the inconsistencies with the claims in the introduction and the notational issues described below lower the overall completeness of the paper. Therefore, my current evaluation leans slightly toward rejection, though it is very close to the borderline.\n\n## Claim of \"Training-Free,\" but Dataset Actually Required\n\nIn Section 1, the authors claim that their proposed method requires no training (\"... we introduce Dyslexify, a training-free defense that ...\"). However, as far as I understand, to execute the proposed method in Algorithm 1, it is necessary to compute `Acc_img` and `Acc_typo`, which in turn require access to the datasets. Therefore, the proposed method involves a form of training. Hence, the claim made in the introduction appears to be inaccurate.\n\n## Notational Issues\n\nThere is room for improvement in the paper's notation.\n\n- In Eq. 3, $h_\\mathrm{cls}^l$ is not properly defined. There may be other variables used without clear definitions as well.\n- In Eq. 4, the expression $H \\mapsto 0$ is used, but $\\mapsto$ typically denotes a mapping between input and output values, so this notation implies the existence of a function $f(H) = 0$, which is likely a different meaning from what the authors intended. Perhaps what they meant was to set $H$ to zero, i.e., $H \\gets 0$?\n- In Eq. 6, the term $A_{i, l}^*(x)_t$ appears identically in both the numerator and denominator, so it should cancel out. If this is not the case, then there is likely a notational or typographical error in the equation."}, "questions": {"value": "I would like to hear the authors' opinion regarding the training aspect. Specifically, compared to other methods in the literature, what kind of data is actually required for the training in the proposed method, and how much difference is there in computational cost or time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C4XmE8FQhR", "forum": "UI7mbsIZeN", "replyto": "UI7mbsIZeN", "signatures": ["ICLR.cc/2026/Conference/Submission18756/Reviewer_scR4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18756/Reviewer_scR4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114545020, "cdate": 1762114545020, "tmdate": 1762928478443, "mdate": 1762928478443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}