{"id": "KwWYvt547M", "number": 1260, "cdate": 1756868039305, "mdate": 1759898218503, "content": {"title": "Pragma-VL: Towards a Pragmatic Arbitration of Safety and Helpfulness in MLLMs", "abstract": "Multimodal Large Language Models (MLLMs) pose critical safety challenges, as they are susceptible not only to adversarial attacks such as jailbreaking but also to inadvertently generating harmful content for benign users. While internal safety alignment via Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) is a primary mitigation strategy, current methods often face a safety-utility trade-off: they either refuse benign queries out of excessive caution or overlook latent risks in cross-modal interactions. To resolve this, we introduce Pragma-VL, an end-to-end alignment algorithm that enables MLLMs to pragmatically arbitrate between safety and helpfulness. First, we enhance visual risk perception with a novel cold-start SFT stage. This is achieved by applying risk-aware clustering to the visual encoder and using an interleaved dataset of risk descriptions and high-quality data. Second, we introduce a theoretically-guaranteed reward model that leverages synergistic learning. We train it with a novel data augmentation method that assigns dynamic weights based on the queries, enabling contextual arbitration between safety and helpfulness. Extensive experiments show that Pragma-VL effectively balances safety and helpfulness, outperforming baselines by 5% to 20% on most multimodal safety benchmarks while preserving its general capabilities in areas such as mathematics and knowledge reasoning.", "tldr": "An end-to-end alignment pipeline enabling MLLMs to pragmatically arbitrate between safety and helpfulness, overcoming over-refusal and risk-blind challenge.", "keywords": ["safety", "alignment", "MLLM", "VLM", "safety-helpfulness trade-off"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8e33b468131038d204347903fb60be144ea72993.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Pragma-VL, a VLM designed to balance helpfulness and harmlessness. This work introduce: (1) A PragmaSafe dataset that annotates both attributes (helpfulness & harmlessness) along with a context-dependent weight vector W=[w_h, w_s] for each sample, capturing human preference trade-offs in different scenarios. (2) A multi-objective reward model with two attribute heads and a meta-voter MLP that aggregates the scores into a reward. (3) A reinforcement learning stage based on GRPO using the learned holistic reward as feedback to fine-tune the Pragma-VL policy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides one of the first complete end-to-end pipelines adapting the classic SFT-GRPO workflow to multimodal safety alignment, including dataset design, reward modeling, and reinforcement optimization. The structure is technically coherent and practical for future extensions.\n\n2. The paper theoretically analyzes why jointly training multiple attribute-specific heads can outperform single-objective or sequential training, under the assumption of positively correlated gradients.\nAlthough the assumption is strong, the analysis is insightful and motivates further research on gradient interaction and multi-attribute alignment.\n\n3. From dataset construction (PragmaSafe) to reward model decomposition and aggregation, the overall pipeline is clear, modular, and well-motivated."}, "weaknesses": {"value": "1. The theoretical justification for the superiority of parallel multi-head reward modeling critically relies on the assumption that  \n$\\mathbb{E}[(\\nabla_\\theta r_s)^\\top(\\nabla_\\theta r_k)] > 0,$\ni.e., gradients between different objectives are positively correlated.  \nHowever, the paper’s main case — balancing helpfulness and harmlessness — is a prototypical trade-off scenario, where these gradients are often negatively correlated in practice.  \nThis discrepancy raises serious concerns about the applicability of the theorem to the proposed task.\n\n2. While the paper includes an ablation on the overall cold-start pipeline and the reinforcement learning stage, it does not disentangle the contributions of the two internal phases within cold-start.  \nIt remains unclear how much improvement each phase contributes to the final performance."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eFnT0P5WeT", "forum": "KwWYvt547M", "replyto": "KwWYvt547M", "signatures": ["ICLR.cc/2026/Conference/Submission1260/Reviewer_yDjw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1260/Reviewer_yDjw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557000434, "cdate": 1761557000434, "tmdate": 1762915721311, "mdate": 1762915721311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Pragma-VL, a new framework to solve the safety-utility trade-off in Multimodal Large Language Models (MLLMs). Current models often apply rigid, static safety policies, making them either overly cautious (unhelpful) or dangerously compliant. Pragma-VL enables models to pragmatically arbitrate between safety and helpfulness based on context. It achieves this through two key innovations:\n\n1. A risk-aware \"cold-start\" phase that enhances the model's ability to perceive visual dangers.\n2. A dynamic policy alignment stage using a novel dataset called PragmaSafe, which contains context-dependent preference weights. This trains a parallel reward model to provide a nuanced, prompt-regulated signal during reinforcement learning.\n\nExperiments show that Pragma-VL significantly improves performance on safety and helpfulness benchmarks by 5-20% without degrading the model's general capabilities, effectively moving beyond fixed safety rules towards more robust, context-aware AI."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is motivated by an important research problem: enabling MLLMs to dynamically arbitrate the helpfulness-safety trade-off. This is critical as focusing either on safety or helpfulness is inadequate. \n2. This paper improves the ability of the visual encoder to perceive safety severity, which is largely ignored when training existing vision encoder."}, "weaknesses": {"value": "1.Many intuitions, explanations and motivations are missing when formulating the contextual data augmentation (Equation 1). For example, why do we need to sample the adjustment magnitude from a gaussian distribution? In addition, it is unclear why larger difference in variance could suggest a larger adjustment magnitude. \n\n\n2. A clear formulation of parallel rewards are missing. The authors propose reward models with parallel rewards, along with other variants such as sequential and single. However, there are only pictorial comparisons between these methods, which makes the reviewers confused about how the rewards are modelled and optimized. For example, what are the data flows in parallel, sequential and single reward models? Is r_\\theta(x,y) a scaler or a vector?. If it is a vector,  what does it contain? Are the preference labels (win/loss) from the original dataset or derived from annotation in Sec. 3.1? Therefore, a clear formulations with math notations are required to better differentiate the variants.\n\n\n3. It is unclear which reward is used for policy update. Is it the scaler reward (helpfulness and harmlessness) or the vectorized reward? If it was the vectorized one, how to convert them to advantages compatible for policy update? \n\nIn summary, to the reviewer, many important details required to fully evaluate this paper are missing."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m1do8RZ6OI", "forum": "KwWYvt547M", "replyto": "KwWYvt547M", "signatures": ["ICLR.cc/2026/Conference/Submission1260/Reviewer_uGD7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1260/Reviewer_uGD7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558398927, "cdate": 1761558398927, "tmdate": 1762915720853, "mdate": 1762915720853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Pragma-VL, a framework designed to balance safety and helpfulness in Multimodal Large Language Models (MLLMs). Pragma-VL addresses these issues through a dynamic, context-aware approach, moving beyond static safety policies. This framework includes three key components: PragmaSafe, a data augmentation method for context-dependent preference labels; a cold-start Supervised Fine-Tuning phase to improve visual risk perception; and a parallel reward model for dynamic arbitration between safety and helpfulness. Experiments show Pragma-VL outperforms existing methods across various benchmarks, maintaining general capabilities while effectively managing safety and helpfulness. This work advances the development of more robust, value-aligned multimodal AI systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel data labeling and augmentation method through the PragmaSafe approach, enhancing context-dependent preference labels.\n2. The Pragma-VL framework provides a dynamic, context-aware solution to the critical trade-off between safety and helpfulness in MLLMs which is really important to the community\n3. The authors conducted comprehensive experiments to validate the performance of Pragma-VL across various benchmarks.\n4. The framework retains strong performance on general VQA tasks, ensuring effectiveness in diverse scenarios."}, "weaknesses": {"value": "The work remains limited by comparatively narrow model validation, heuristic cold-start design, reliance on GPT-4o annotations, lack of comparison to newer multi-objective baselines, homogeneous benchmarking, among a few others. These are issues that future work should address through broader empirical validation, human-calibrated evaluation, and open data release."}, "questions": {"value": "1. Safe RLHF-V also proposed an algorithm for safe and helpful trade-off. But in the experiment, authors only set Beavertails-V_harm or Beavertails-V_help as baseline.\n2. The results in experiments for Beavertails-V_harm are strange. Usually we consider the model safer with lower ASR, while here although the model has a lower ASR, it has a lower win rate in dimension of harmless which may indicate that the model is unsafer. There seems to be a conflict between these two results . Could you please explain it more?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lmY3EYlDIQ", "forum": "KwWYvt547M", "replyto": "KwWYvt547M", "signatures": ["ICLR.cc/2026/Conference/Submission1260/Reviewer_o8we"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1260/Reviewer_o8we"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882262420, "cdate": 1761882262420, "tmdate": 1762915720417, "mdate": 1762915720417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical problem in Multimodal Large Language Models (MLLMs): **the tradeoff between safety and usefulness**. The authors note that static safety policies often fail, leading to both excessive refusals and risk blindness.\n\nTo solve this, the paper proposes Pragma-VL, an end to end alignment framework for dynamic and \"pragmatic\" arbitration based on context. The framework has two core innovations:\n\n**MLLM Cold Start**: A specialized pre alignment stage to address the model's inherent \"visual risk blindness\" using Risk Aware Contrastive Learning and Supervised Fine Tuning (SFT) on cross task datasets.\n\n**Policy Alignment via Parallel Rewards**: This uses reinforcement learning (GRPO) and a theoretically grounded, parallel, multi head reward model. This model is trained on a new dataset, PragmaSafe, which is annotated with context dependent safety usefulness weights using GPT-4o.\n\nExperiments on the Qwen2.5-VL-7B and Llava-1.5-7B models show that Pragma-VL significantly outperforms baselines on multiple safety benchmarks (especially the SIUO benchmark for cross modal risks). Crucially, it also maintains the model's capabilities on general purpose benchmarks (e.g., GQA, ScienceQA)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Methodological Completeness**: Pragma-VL is a well designed, end to end system. It correctly identifies that policy alignment (RL) cannot fix fundamental perceptual issues. Thus, the proposed \"cold start\" SFT stage (first addressing visual risk perception before connecting to language cognition) is methodologically sound and rigorous.\n\n- **In depth Analysis of the Reward Model**: The paper's exploration of reward model (RM) architectures (single objective vs. sequential vs. parallel) is a highlight. The authors empirically show the superiority of the parallel architecture (Table 1) and provide theoretical support (Theorem 1) for its synergistic learning, adding credibility to their approach.\n\n- **Addressing Cross Modal Risks**: The large improvement on the SIUO benchmark (e.g., Llava's safety rate increasing from 14.37% to 55.42%) strongly shows the method addresses cross modal risks when MLLMs handle benign image text combinations."}, "weaknesses": {"value": "- **Annotation Quality and Bias Risk**: Over reliance on AI annotation without human verification. The core \"pragmatic arbitration\" capability depends entirely on the PragmaSafe dataset. Its context weight labels are generated by GPT-4o. This heavy reliance on one AI model introduces two problems: (a) Bias Propagation: Systematic biases from GPT-4o may be propagated and solidified in Pragma-VL. (b) Lack of a Gold Standard: The paper trusts the AI annotations but lacks a human agreement study. Using human expert annotations as a gold standard to cross validate the AI label quality and consistency is crucial.\n\n- **Lack of Robustness in Data Aggregation**: The variance calculation is based on only 5 samples from a single model (GPT-4o). This variance might only reflect GPT-4o's sampling uncertainty, not true \"task uncertainty.\" A more robust consensus could be achieved by using a model ensemble (multiple models like Gemini or Qwen) for annotation, which would likely produce a less biased dataset.\n\n- **Unclear Attribution for Preserving General Capabilities**: The paper attributes the preserved capabilities (Table 3) to its \"pragmatic arbitration\" framework. However, the appendices note that large amounts of general capability data (e.g., MathV360K, VQAv2) were intentionally mixed into the PragmaSafe dataset and RL training data. Thus, it is unclear how much of this preservation is due to the algorithm's design versus the data mixing. The paper does not fully clarify this."}, "questions": {"value": "1. When using GPT-4o to generate context weights, did you observe poor or inconsistent performance on specific categories (e.g., subtle bias, sarcasm, or complex \"gray area\" queries)? Has the Pragma-VL model inherited these specific failures?\n\n2. The MLLM Cold Start (Sec 3.2) has two phases: Risk Aware Contrastive Learning (Phase 1) and Risk Aware SFT (Phase 2). If Phase 1 were skipped, performing only Phase 2, what would be the impact on final safety performance (especially on SIUO)? This is just a question; no new experiments are required."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ILeG3dKiO0", "forum": "KwWYvt547M", "replyto": "KwWYvt547M", "signatures": ["ICLR.cc/2026/Conference/Submission1260/Reviewer_LxbP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1260/Reviewer_LxbP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910249726, "cdate": 1761910249726, "tmdate": 1762915719986, "mdate": 1762915719986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}