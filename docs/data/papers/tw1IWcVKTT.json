{"id": "tw1IWcVKTT", "number": 62, "cdate": 1756728352908, "mdate": 1759898277277, "content": {"title": "Automated Optimization Modeling via a Localizable Error-Driven Perspective", "abstract": "Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs' capabilities in this domain, its effectiveness is severely constrained by the scarcity and underutilization of high-quality training data. However, through a detailed profiling of error patterns across various problem-response pairs drawn from post-training, we identify two fundamental limitations of existing automated optimization modeling approaches: (L1) the \\textit{sparsity} of error-specific problems and (L2) the \\textit{sparse rewards} associated with difficult problems. We demonstrate that these limitations can result in suboptimal performance in domain-specific post-training for LLMs. To tackle the above two limitations, we propose a novel error-driven learning framework---namely, auto\\textbf{m}ated opt\\textbf{i}mization modeli\\textbf{n}g via a localizable error-\\textbf{d}riven perspective (MIND)---that customizes the whole model training framework from data synthesis to post-training. MIND is based on our key observation of the unique \\textbf{\\textit{localizable}} patterns in error propagation of optimization modelings, that is, modeling errors may remain localized to specific semantic segments and do not propagate throughout the entire solution. Thus, in contrast to holistic reasoning tasks such as mathematical proofs, MIND leverages the construction of a focused, high-density training corpus and proposes \\textbf{D}ynamic Supervised \\textbf{F}ine-Tuning \\textbf{P}olicy \\textbf{O}ptimization (DFPO) to tackle difficult problems through localized refinement. Its appealing features include that (1) it generates targeted, error-aware training problems that achieve superior sample efficiency, and (2) it ensures a coherent and structured learning progression for stable and effective reinforcement learning on difficult problems. Experiments on six benchmarks demonstrate that MIND \\textit{consistently} outperforms all the state-of-the-art automated optimization modeling approaches. Furthermore, we open-source a new training dataset, MIND-Train, and a new benchmark, MIND-Bench, for the automated optimization modeling research community.", "tldr": "", "keywords": ["LLM post-training", "automated optimization modeling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/23fb085ea34ec9c3758c3b82f1b0675987c4f205.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MIND, a framework for training LLMs on automated optimization modeling. The approach is motivated by observing that errors in incorrect samples have an average error ratio of 0.33 across variables, constraints, and objectives, suggesting localized error patterns. The framework has two components: (1) an error-driven reverse data synthesis pipeline that generates training problems by incorporating identified error patterns through single-error and multi-error strategies, producing training instances; (2) DFPO, which combines RL with SFT by using a teacher LLM to correct failed responses. Evaluated on six benchmarks with Qwen2.5-7B-Instruct, MIND achieves 62.7% average accuracy versus SIRL's 60.2%. The paper also introduces MIND-Bench for out-of-distribution evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Comprehensive empirical evaluation across multiple benchmarks:** The paper evaluates the proposed method on six diverse benchmarks (NL4Opt, IndustryOR, MAMO EasyLP/ComplexLP, OptMATH-Bench, OptiBench) and provides an additional out-of-distribution benchmark (MIND-Bench with 69 problems). The experimental scope is reasonably broad and covers different types of optimization problems.\n\n**Consistent performance improvements over baselines:** The experimental results show consistent improvements across most benchmarks compared to prior training-based methods (ORLM, LLMOPT, OptMATH, SIRL). The improvements are particularly notable on challenging benchmarks like ComplexLP (+8.4% over SIRL) and OptMATH (+6.2% over SIRL), suggesting the approach may be effective for difficult problems.\n\n**Clear presentation and structure:** The paper is generally well-written and clearly structured. The proposed data synthesis pipeline and the DFPO algorithm are explained with clarity, aided by helpful diagrams (Figures 2, 3, 4). The methodology section provides sufficient implementation details for understanding the approach."}, "weaknesses": {"value": "**Insufficient motivation and questionable core assumption:** The paper's central claim that \"modeling errors are localized and do not propagate\" relies solely on Figure 1 showing an average error ratio of 0.33 across 100 samples. This does not prove localization—it could simply mean ~67% overall correctness. The claim that errors in variable definitions do not affect constraints is questionable; incorrect variable definitions would naturally propagate to dependent constraints. The paper provides no evidence that optimization modeling exhibits fundamentally different error patterns compared to other reasoning tasks.\n\n**Lack of justification for single-error vs. multi-error decomposition:** The paper introduces two synthesis strategies without explaining why this decomposition is necessary. No theoretical rationale is provided, and critical ablations are missing: training with only single-error data, only multi-error data, or comparing their mixture versus each alone. Table 5 shows vastly different pass rates (39.88% vs 60.30%), but this is not discussed. It remains unclear whether the decomposition is principled or arbitrary.\n\n**Incomplete related work coverage:** The paper omits several recent relevant works: (1) \"Autoformulation of Mathematical Optimization Models Using LLMs\"; (2) \"CHAIN-OF-EXPERTS: When LLMs Meet Complex Operations Research Problems\"; (3) \"Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation\". \n\n**Performance ceiling imposed by teacher model with no mitigation strategy: ** The pipeline relies on teacher LLMs (DeepSeek-R1 for error identification, DeepSeek-V3 for correction), creating an inherent performance ceiling. The paper does not discuss this limitation or propose mechanisms to break through it. Unlike rejection sampling or other techniques that could potentially exceed teacher performance, the current approach appears bounded by teacher model quality, raising concerns about long-term scalability."}, "questions": {"value": "**On the core motivation:** Why does an error ratio of 0.33 demonstrate \"localized errors that do not propagate\"? Can you provide evidence comparing error propagation patterns in optimization modeling versus other reasoning tasks, and explain why existing methods (ORLM + SIRL) cannot address limitations L1 and L2?\n\n**On the single-error vs. multi-error decomposition:** What is the rationale for this decomposition, and can you provide ablations comparing training with only single-error data, only multi-error data, versus their mixture? How do you explain the large pass rate difference (39.88% vs 60.30%) between the two strategies in Table 5?\n\n**On DFPO's contribution:** Can you provide comparisons with additional baselines such as \"SFT→RL,\" \"SFT→GRPO,\" and \"SFT→filter→RL\" to isolate DFPO's specific advantage beyond adding SFT loss for failed samples? How do you quantitatively verify that corrected responses remain close to the base model's distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "O5BtyddmHK", "forum": "tw1IWcVKTT", "replyto": "tw1IWcVKTT", "signatures": ["ICLR.cc/2026/Conference/Submission62/Reviewer_RdHx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission62/Reviewer_RdHx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission62/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545814395, "cdate": 1761545814395, "tmdate": 1762915444183, "mdate": 1762915444183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses two issues in LLM-based optimization modeling: a scarcity of error-specific training data (L1) and sparse rewards in reinforcement learning (L2). The authors present MIND, a framework motivated by their claim that modeling errors are \"localizable\". The MIND framework consists of two components: (1) an \"error-driven reverse data synthesis\" pipeline, which was used to create a new dataset (MIND-Train) by injecting identified error patterns into new problems, and (2) a post-training algorithm, DFPO, which uses a \"teacher\" LLM to generate an SFT loss signal when all RL rollouts fail. The authors report that this framework achieves improved performance over baselines. The paper also includes a new benchmark, MIND-Bench."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper contributes MIND-Bench, a new, open-sourced benchmark for evaluating generalization. It is curated from textbooks and real-world industry scenarios, providing a valuable out-of-distribution test set for the community.\n2. The proposed method achieves strong empirical results. On the same 7B model, MIND outperforms prior state-of-the-art training-based methods (like SIRL and OptMATH) in macro-average performance across six benchmarks.\n3. The DFPO algorithm is an interesting and pragmatic approach. It combines SFT and RL to effectively address sparse rewards by applying a dense SFT loss when all RL rollouts fail. This provides learning signals for difficult problems, which is shown to be effective on the complex benchmark."}, "weaknesses": {"value": "1. The paper's claim that errors are \"localizable\" is unsurprising—competent LLMs naturally make partial errors rather than completely wrong formulations. The sole evidence (Figure 1: error ratios from 100 samples) only computes the fraction of erroneous components, providing no analysis of error dependencies or propagation. The critical claim that errors \"do not propagate\" is never validated. Without showing that errors in one component (e.g., variables) don't cause errors in dependent components (e.g., constraints), the motivation for the error-driven approach remains unsubstantiated.\n2.  MIND-Bench's contribution is questionable. With only 69 problems, its statistical reliability is limited. More importantly, its stated sources (\"textbooks or industry scenarios\") directly overlap with existing benchmarks (OptiBench, IndustryOR), and the paper fails to articulate what unique gap it addresses. The necessity of this small benchmark remains unjustified.\n3.  The paper provides no experimental justification for DFPO's complexity over simpler alternatives. The key idea—applying SFT when all RL rollouts fail—lacks comparison to natural baselines: (1) sequential training (SFT pre-training → RL), and (2) data stratification (filtering problems by difficulty, applying SFT/RL separately). Without these ablations, there is no evidence that interleaving provides any benefit over decoupled approaches."}, "questions": {"value": "1. The evidence in Figure 1 only shows a static error ratio. Can you provide more rigorous evidence to support your central claim that errors \"do not propagate\"? This is the key justification for your pipeline, but it is not substantiated by the current analysis.\n2. How does the proposed interleaved DFPO algorithm compare to simpler, decoupled baselines? Specifically: (a) A standard sequential pipeline (full SFT, then RL)? (b) A data stratification strategy (filtering by difficulty, then applying SFT/RL separately)?  \n3. Given its small scale (69 problems) and overlapping sources with larger benchmarks like OptiBench and IndustryOR, what unique gap does MIND-Bench fill, and how can its results be considered statistically significant?\n4. The paper fails to cite or compare against highly relevant work.  Key omissions include:\n[1] Chain-of-experts: When LLMs meet complex operations research problems. (2023), which addresses LLM approaches for complex OR problems.\n[2] Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation (2025), which directly tackles iterative data synthesis and structured validation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W1nnHIYm8A", "forum": "tw1IWcVKTT", "replyto": "tw1IWcVKTT", "signatures": ["ICLR.cc/2026/Conference/Submission62/Reviewer_kVVf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission62/Reviewer_kVVf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission62/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725280278, "cdate": 1761725280278, "tmdate": 1762915444055, "mdate": 1762915444055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces MIND, an error-driven learning framework for automated optimization modeling. Building on the key observation that modeling errors are often localizable, the authors propose an reverse data synthesis pipeline targeting common error patterns, and Dynamic Supervised FineTuning Policy Optimization (DFPO) to address the challenges of sparse rewards and distribution shift. Experiments demonstrate that MIND consistently improves automated modeling performance across diverse benchmarks and achieve notably greater improvements on more challenging problems compared to existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors analyze the error patterns in automated optimization modeling and identify the localizable nature of error propagation. Building on this insight, they design an error-driven reverse data synthesis process that focuses on common error types to generate more challenging and informative training data. In addition, the proposed DFPO strategy effectively complements existing reinforcement learning methods by alleviating issues related to sparse rewards and distributional shift, using continuous reward shaping and dynamic supervised fine-tuning to promote more stable and efficient learning. Finally, the experimental evaluation is comprehensive, covering a wide range of benchmarks from simple to complex scenarios and including diverse baselines, providing convincing evidence of the robustness and general applicability of MIND."}, "weaknesses": {"value": "Although the authors improve both stages of automated optimization modeling, data synthesis and post-training, the connection between the proposed error-driven data synthesis pipeline and the subsequent DFPO training strategy is not clearly articulated. The two components are insufficiently integrated, which weakens the overall methodological coherence. Moreover, several technical aspects lack sufficient detail. For instance, the paper only provides a single example of a common error type (data type mismatch) without presenting a broader taxonomy or quantitative analysis of error distributions. Similarly, the reward design introduces an modeling error measure to quantify discrepancies between mathematical formulations, yet the specific computation and implementation of this measure are not described."}, "questions": {"value": "1. Do you have quantitative statistics on the distribution of common error types and their occurrences within different components of the formulations? Additionally, have you analyzed how frequently such errors appear in the responses?\n\n2.In the reward design, you introduce a modeling error measureεto quantify discrepancies between mathematical formulations. Could you clarify how this measure is computed in practice? Moreover, have you conducted any dedicated experiments to support the assumption in Equation (2) that optimization modeling error and objective deviation are positively correlated in expectation? Finally, what is the rationale behind your choice of the hyperparameter α, and have you performed a sensitivity analysis to examine its impact?\n\n3.Since DFPO relies on a more powerful teacher model to refine incorrect responses, have you evaluated the reliability, effectiveness, or computational cost of the corrected responses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4gz9yB2lhV", "forum": "tw1IWcVKTT", "replyto": "tw1IWcVKTT", "signatures": ["ICLR.cc/2026/Conference/Submission62/Reviewer_JHsT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission62/Reviewer_JHsT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission62/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879938880, "cdate": 1761879938880, "tmdate": 1762915443957, "mdate": 1762915443957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MIND, a novel framework for automated optimization modeling with LLMs. Its core insight is the error locality observation—modeling errors are often confined to specific components. Based on this, MIND features: 1) an error-driven reverse data synthesis pipeline to create targeted training data, and 2) Dynamic Supervised Fine-Tuning Policy Optimization (DFPO) to tackle sparse rewards by dynamically correcting wrong responses. Experiments show MIND outperforms SOTAs on five benchmarks. The paper also makes MIND-Train and MIND-Bench open source."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The \"error locality\" insight and its application to guide both data synthesis and training is an impressive contribution.\n\n- The framework is comprehensive and well-engineered. Empirical results are good, showing improvements over strong baselines. This work advances the practicality of LLMs for optimization."}, "weaknesses": {"value": "- The \"error locality\" observation, while compelling, is not sufficiently validated across diverse models and problem types, questioning its generality.\n\n- DFPO relies on the critical assumption that teacher-corrected responses align with the base model's distribution, but provides no quantitative evidence to support this.\n\n- Key design choices (e.g., reward function weighting) lack ablation studies. The OOD generalization claim, while supported by a new benchmark, is limited by its small scale."}, "questions": {"value": "- Could the paper provide evidence that \"error locality\" holds for other base models or more complex optimization problems where errors might propagate?\n\n- How did the paper verify the distributional consistency between the teacher-corrected responses and the base model's outputs? Could the paper provide quantitative measures (e.g., KL-divergence)?\n\n- How was the weight (α=0.2) in the reward function determined? Could the paper show an ablation on this parameter?\n\n- To what extent are the gains uniquely due to the error-driven, localized synthesis, versus simply being a result of increased data volume and diversity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9smUqOIcbR", "forum": "tw1IWcVKTT", "replyto": "tw1IWcVKTT", "signatures": ["ICLR.cc/2026/Conference/Submission62/Reviewer_VFcR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission62/Reviewer_VFcR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission62/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996688582, "cdate": 1761996688582, "tmdate": 1762915443857, "mdate": 1762915443857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}