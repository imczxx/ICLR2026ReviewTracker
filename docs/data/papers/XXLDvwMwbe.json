{"id": "XXLDvwMwbe", "number": 16849, "cdate": 1758269368199, "mdate": 1759897215867, "content": {"title": "On the trade-off between expressivity and privacy in graph representation learning", "abstract": "We investigate the trade-off between expressive power and privacy guarantees in graph representation learning. Privacy-preserving machine learning faces growing regulatory demands that pose a fundamental challenge: safeguarding sensitive data while maintaining expressive power. To address this challenge, we propose homomorphism density vectors to obtain graph embeddings that are private and expressive.\nHomomorphism densities are provably highly discriminative and offer a powerful tool for distinguishing non-isomorphic graphs. \nBy adding noise calibrated to each density’s sensitivity, we ensure that the resulting embeddings satisfy formal differential privacy guarantees. Our theoretical construction preserves expressivity in expectation, as each private embedding remains unbiased with respect to the true homomorphism densities. Our embeddings match, in expectation, the expressive power of a broad range of graph neural networks (GNNs), such as message-passing and subgraph GNNs, while providing formal privacy guarantees.", "tldr": "We study the trade-off between provably expressive and private representations in graph learning", "keywords": ["graph representation learning", "privacy", "expressivity"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1db4549db7515646b4312feec13b6e05947f2ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the trade-off between privacy and expressivity in graph representation learning. It proposes using homomorphism density vectors as graph embeddings to achieve both expressive power and formal privacy guarantees. The paper derives the trade-off in two parts: firstly, if homomorphism density vectors are used as graph embeddings, then with any noise added to them, in expectation, they will achieve expressiveness. Secondly, it proposed to use a $\\beta$-smoothing sensitivity bound and leverage Gaussian noise to achieve tCDP. Extensive theoretical and experimental results are conducted to highlight and illustrate the advantage of using homomorphism density vectors as embeddings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Provide a strong theoretical guarantee for the expressivity and privacy of graph representation learning.\n- The experimental results highlight the validity and soundness of the theoretical claims.\n- The proposed method has been proven to be useful through experimental results and is robust against attacks."}, "weaknesses": {"value": "- The privacy guarantee is provided for local sensitivity. Although it is demonstrated that very similar graphs can still be distinguished by the noisy homomorphism density vectors, the privacy guarantee does not meet the DP standard. \n- The expressivity guarantee is analyzed in expectation, which is orthogonal to the privacy analysis. Since the DP noise is only added once, the expressivity will be biased to that sampled noise, leading to the degradation of the guarantee.\n- Some of the critical theoretical annotations are provided in the Appendix, making it hard to follow the flow of the paper."}, "questions": {"value": "- In line 206, why is the sensitivity of Theorem 3.10 a local sensitivity if the neighboring database is considered across the input domain?\n- By the definition of F-expectation-expressivity and Expectation-completeness, what are the $X$ for your analysis and proposed method?\n- By expectation w.r.t $X$, the F-expectation-expressivity and Expectation-completeness have to consider all of the possible inputs of the input space? Then, can you elaborate on how to use this expectation in your expressivity guarantee?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rPifNmXuru", "forum": "XXLDvwMwbe", "replyto": "XXLDvwMwbe", "signatures": ["ICLR.cc/2026/Conference/Submission16849/Reviewer_dNzD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16849/Reviewer_dNzD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851117429, "cdate": 1761851117429, "tmdate": 1762926871520, "mdate": 1762926871520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We want to thank all reviewers for their time invested in the review process and appreciate their comments and positive feedback!\n\nWe are currently working on carefully addressing all responses and will provide comprehensive answers as soon as possible."}}, "id": "M2zLawHxwZ", "forum": "XXLDvwMwbe", "replyto": "XXLDvwMwbe", "signatures": ["ICLR.cc/2026/Conference/Submission16849/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16849/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16849/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763139424546, "cdate": 1763139424546, "tmdate": 1763139424546, "mdate": 1763139424546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the theoretical trade-off between expressivity and privacy in graph representation learning. The authors propose constructing graph embeddings using noisy homomorphism densities, where homomorphism counts are perturbed with Gaussian noise calibrated via smooth sensitivity to ensure tCDP. Experiments are conducted on several OGB molecular datasets and synthetic graphs, showing that the proposed method maintains competitive predictive performance and effectively resists privacy attacks under reasonable privacy budgets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Graph privacy is an important and practically relevant research direction.\n2. The paper is relatively well-organized and logically coherent."}, "weaknesses": {"value": "1. The paper emphasizes the use of homomorphism density vectors as a key tool. However, while it claims to fill the gap in understanding the interplay between expressivity and privacy in graph representation learning, it does not clearly define what this gap uniquely refers to. Prior studies (Sajadmanesh et al. 2021, 2023, 2024, as well as others) have already explored expressivity under differential privacy and aimed to design utility-preserving private graph learning frameworks.\n2. The paper focuses on edge-level privacy and graph-level tasks without explaining why this specific setting was chosen. In practice, graph privacy leakage often involves node attributes, and for edge-level privacy, link prediction tasks are crucial for comprehensive evaluation. The current setup limits the practical relevance of the work.\n3. The paper cites Lovász’s theorem (1967), which states that homomorphism counts can distinguish all non-isomorphic graphs. However, it later uses homomorphism densities, and in Remark B.1 admits that these cannot distinguish a graph from its blow-up. Although the authors propose adding “node counts” as a correction, they provide no theoretical proof that this modification preserves differential privacy guarantees or avoids leaking information about graph size. Moreover, the proposed fix only applies when graphs have different node counts—what about when they are the same?\n4. Experimental details are incomplete. For instance, the paper sets $δ=10^{-6}$, $β=ρ'/5$, and $d=50$, but does not justify these choices or explain their sensitivity.\n5. Experiments are limited to OGB molecular graphs and synthetic graphs, lacking evaluation on other types of graph data such as social networks (large, sparse) or medical graphs (sensitive node attributes). Since molecular graphs are relatively regular, the effectiveness of the proposed method on structurally complex graphs remains unverified, reducing generalizability."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YBsYpGO3qZ", "forum": "XXLDvwMwbe", "replyto": "XXLDvwMwbe", "signatures": ["ICLR.cc/2026/Conference/Submission16849/Reviewer_hgq5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16849/Reviewer_hgq5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930944774, "cdate": 1761930944774, "tmdate": 1762926870912, "mdate": 1762926870912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global reply to reviews"}, "comment": {"value": "Thanks again for your reviews!\n\nIn this global reply, we aim to address common points raised by the reviewers and, in particular, clarify the role of _expressivity_ in our analysis. We will also reply to each reviewer individually.\n\n* **Expressivity and its connection to privacy in graph representation learning:** We specifically study the trade-off between graph privacy and _expressivity_. Although expressivity and privacy have been independently studied extensively for graph learning algorithms, their interplay has not been formally investigated so far. Indeed, Sajadmanesh et al.  [1, point _(iv)_ in the Conclusion] highlighted that there is a lack of research on the expressive power of differentially private graph learning algorithms. \n* **Choice of problem setting:** As expressivity analysis focuses on the ability to distinguish between structurally different, i.e., non-isomorphic graphs, we focus on graph-level tasks. Graphs that differ by a single edge have the _minimal_ possible structural difference which should be distinguished by an expressive algorithm. Therefore, we consider _edge_-privacy as an ideal setting to investigate the underexplored interplay between privacy and expressivity.\n* **Choice of datasets:** We focused our experiments on molecular graph classification benchmark datasets, because they are standard in the expressivity literature, which is the perspective of our theoretical investigation. Molecules provide a natural setting where expressivity limitations can have concrete implications (e.g., non-isomorphic molecules that standard GNNs cannot distinguish). That said, as suggested by the reviewers, we are currently running additional experiments on non-molecular data and will report detailed results as soon as they are available.\n* **Choice of baselines:** We thank the reviewers for the additional GNN baseline suggestions. Unfortunately, the proposed baselines (GAP [1], ProGAP [2] and PrivGNN [3]) are GNN architectures with unquantified expressive power and specifically designed for node classification on large scale graphs, which is not our setting. In our work, we adopt a randomized response (RR) baseline appropriate for graph classification with edge privacy (cf. Hidano and Murakami [4]), to privatize the graphs and feed them into the provably expressive Graph Isomorphism Network [5]. Following the reviewers' suggestions for additional baselines, we implemented and ran the recently proposed degree-preserving randomized response (DPRR) method [4]. We report the results in the table below. DPRR improves upon randomized response, but is still worse than our method - which in addition to differentially privacy guarantees also ensures expressivity in expectation.\n| Method ($\\epsilon = 1$) | $\\texttt{MOLHIV}$ | $\\texttt{MOLBBBP}$ | $\\texttt{MOLBACE}$ | $\\texttt{MOLLIPO}$ | \n|--|--|--|--|--|\n|  $\\mathbf{t}(\\mathbf{F}, G)$ (ours) | 0.692 $\\pm$ 0.020  | 0.602 $\\pm$ 0.005 | 0.652 $\\pm$ 0.069  | 1.086 $\\pm$ 0.004 |\n|  RR | 0.488 $\\pm$ 0.008  | 0.440 $\\pm$ 0.005 | 0.457 $\\pm$ 0.024 | 1.568 $\\pm$ 0.248 |\n|  DPRR | 0.595 $\\pm$ 0.155 | 0.539 $\\pm$ 0.019 | 0.648 $\\pm$ 0.043 | 1.499 $\\pm$ 0.333 |\n\n* **Degree bound:** In our experiments, we take advantage of the fact that, in many domains such as that of molecular graphs, there are _a priori_, public constraints on the maximum degree of graphs, which can be leveraged to obtain tighter bounds on the sensitivity of the homomorphism densities. We want to point out that our theoretical results do not depend on this. In any case, if needed, one may privately estimate the max degree by adding, e.g., Laplacian noise, to the empirical maximum degree under a small additional privacy budget.\n* **Time complexity of homomorphism counting:** For general graphs, counting homomorphisms is #P-complete. However, for many pattern classes (e.g., paths, stars, cycles) they can be computed efficiently. We specifically chose bounded treewidth graphs as they already characterize the expressive power of most GNN architectures. Moreover, we want to stress the fact that the counting step only needs to be performed once, after which the resulting embeddings can be used for any downstream tasks. Sections 2.5 & 4.4 in Nguyen and Maehara [6] and Section 4 & Appendix C in Welke et al. [7] provide more thorough analyses on the runtime. We will nevertheless provide runtimes for our computation."}}, "id": "Jk3XAu49y0", "forum": "XXLDvwMwbe", "replyto": "XXLDvwMwbe", "signatures": ["ICLR.cc/2026/Conference/Submission16849/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16849/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16849/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763490264830, "cdate": 1763490264830, "tmdate": 1763505455015, "mdate": 1763505455015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the trade-off between expressivity and privacy in graph representation learning. The authors propose using noisy homomorphism density vectors to achieve both distinguishability and differential privacy guarantees. The theoretical part derives upper bounds on sensitivity and smooth sensitivity, while the experimental section validates the privacy–utility trade-off on the OGBG and SBM datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper demonstrates rigorous logical consistency from definitions and lemmas to theorem derivations, with solid mathematical details.\n2. The proposed framework can be mapped to different GNN architectures (e.g., message-passing and subgraph-based GNNs) and allows for quantitative analysis of the trade-off.\n3. The experiments show clear performance differences across pattern classes, validating the theoretical trends."}, "weaknesses": {"value": "1. It only considers edge-level DP.\n2. Experiments are conducted only on small-scale, toy-level datasets and do not demonstrate the feasibility of the approach on real-world large graphs or complex tasks.\n3. The paper lacks comparison with established DP-GNN methods such as edge-level GAP, using only a randomly perturbed GNN as the baseline.\n4. Computing homomorphism densities is computationally expensive. Please provide a detailed analysis of computational overhead.\n5. The paper is clearly written, but it appears that the authors used last year’s ICLR template."}, "questions": {"value": "See weakness above.\n\n\nSince only edge-level privacy is considered, can this analysis be extended to node-level or subgraph-level differential privacy in the future?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zPiqT1WiCK", "forum": "XXLDvwMwbe", "replyto": "XXLDvwMwbe", "signatures": ["ICLR.cc/2026/Conference/Submission16849/Reviewer_WKpZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16849/Reviewer_WKpZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998166152, "cdate": 1761998166152, "tmdate": 1762926870045, "mdate": 1762926870045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the trade-off between expressivity (ability to distinguish non-isomorphic graphs) and privacy (edge-level differential privacy) in graph representation learning. It proposes using homomorphism density vectors as embeddings, adding Gaussian noise calibrated via smooth sensitivity to achieve truncated concentrated DP (tCDP) guarantees. The authors argue that expectation-expressivity is preserved because the noise is mean-zero. Experiments on OGB molecular datasets and a synthetic SBM illustrate privacy–utility trade-offs and compare against a randomized-response baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an underexplored intersection of privacy and expressivity in graph learning.\n- Formalizes expectation-expressivity and privacy guarantees using established DP frameworks.\n- Demonstrates basic privacy–utility trade-offs and sanity-checks the insufficiency of global sensitivity."}, "weaknesses": {"value": "- Experiments mostly use tree patterns; higher treewidth and complex pattern classes are barely explored. The empirical section lacks depth: pattern classes beyond treewidth 1 are barely tested, and runtime implications of homomorphism counting (#P-hard) are not addressed.\n- No comparison to state-of-the-art DP-GNNs (e.g., ProGAP) or strong inference attacks like LinkTeller. \n- Non-private use of degree bounds ($\\Delta max$) for sensitivity calibration undermines DP guarantees. Furthermore, Δmax is estimated from data without privatization, which violates DP assumptions"}, "questions": {"value": "Given the strengths, I have the following concerns:\n\n1. The authors claims that \"Our embeddings match, in expectation, the expressive power of a broad range of graph neural networks (GNNs), such as\nmessage-passing and subgraph GNNs, while providing formal privacy guarantees.\" but there is no comparison with GNN-based privacy methods like ProGAP or [1]. These are widely cited and provide edge-level DP guarantees. Can the authors include these baselines?\n\n2. The paper evaluates privacy via nearest‑neighbor graph re‑identification on embeddings, ignoring stronger edge inference attacks. Why restrict to nearest-neighbor re-identification? Stronger attacks (e.g., LinkTeller) has shown non‑trivial leakage even against DP defenses for modest $\\epsilon$ or modern membership/link inference analyses for GNNs; the empirical privacy validation thus seems under‑powered. \n\n3. The sensitivity bound with degree dependence (Theorem 4.5) assumes a known max degree Δmax (e.g., fixed per dataset) or estimated (Chernoff) on synthetic data. Using estimated Δmax can itself leak information unless privatized or externally bounded, and the paper sets Δmax from data or heuristics in experiments, which is not privacy‑preserving if used to calibrate noise. This critical point is not addressed. Can the authors clarify this?\n\n4. This is subtle but very important, does the method allow private release of embedding?\n\n5. Most OGB experiments use tree patterns (tw=1), which undermines the empirical claim that “more expressive pattern classes” help under fixed privacy budgets. The brief MOLHIV experiment with small treewidth still tops out at tw=3; no cycles/graphlets on large graphs, no runtime/memory profiling of homomorphism computations, despite #P‑hardness beyond bounded treewidth. \n\n6. Blow‑up invariance of densities is acknowledged in the appendix, but because densities cannot distinguish graph blow‑ups, the completeness claims require appending |V(G)| or switching to counts; this subtle but important caveat should be explicit in the main text where expressivity claims are made.\n\n\n\n**Missing references:**\n\n*On DP graph learning:*\n\n[1]Olatunji et al. Releasing graph neural networks with differential privacy guarantees\n\n[2]Xiang et al. Preserving Node-level Privacy in Graph Neural Networks\n\n*On graph reconstruction attack:*\n\n[3]Olatunji et al. Private graph extraction via feature explanations\n\n[4]Zhou et al. On Strengthening and Defending Graph Reconstruction Attack with Markov Chain Approximation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bu9jVy8Bni", "forum": "XXLDvwMwbe", "replyto": "XXLDvwMwbe", "signatures": ["ICLR.cc/2026/Conference/Submission16849/Reviewer_C4Rx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16849/Reviewer_C4Rx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762357236040, "cdate": 1762357236040, "tmdate": 1762926869078, "mdate": 1762926869078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}