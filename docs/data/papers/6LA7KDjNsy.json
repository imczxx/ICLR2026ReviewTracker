{"id": "6LA7KDjNsy", "number": 14889, "cdate": 1758245148386, "mdate": 1759897343392, "content": {"title": "RAG4DMC: Retrieval-Augmented Generation for Data-Level Modality Completion", "abstract": "Multi-modal datasets are critical for a wide range of applications, but in practice, they often suffer from missing modalities. This motivates the task of Missing Modality Completion (MMC), which aims to reconstruct missing modalities from the available ones to fully exploit multi-modal data. While pre-trained generative models offer a natural solution, directly applying them to domain-specific MMC is often ineffective, and fine-tuning suffers from limitations like limited complete samples, restricted API access, and high cost. To address these issues, we propose RAG4DMC, a retrieval-augmented generation framework for data-level MMC. RAG4DMC builds a dual knowledge base from complete in-dataset samples and external public datasets, enhanced with feature alignment and clustering-based filtering to mitigate modality and domain shifts. A multi-modal fusion retrieval mechanism combining intra-modal retrieval with cross-modal fusion then provides relevant context to guide generation, followed by a candidate selection mechanism for coherent completion. Extensive experiments on general and domain-specific datasets demonstrate that our method produces more accurate and semantically coherent missing-modality completions, resulting in substantial improvements in downstream image–text retrieval and image captioning tasks.", "tldr": "", "keywords": ["Retrieval-Augmented Generation; Missing Modality Completion; Multimodal Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/348d603ef15a50d06614681ae92c9f0983cbbe75.pdf", "supplementary_material": "/attachment/bd1fd0a2a86306e0d2a8db667b39b56ad3b5cc4d.zip"}, "replies": [{"content": {"summary": {"value": "The paper tackles the Missing-Modality Completion (MMC) problem, where samples in a multi-modal dataset lack one or more modalities (e.g., an image without its paired caption). Instead of fine-tuning large generative models—often impractical due to limited complete samples, API-only access, or high cost—the authors propose RAG4DMC, a retrieval-augmented generation framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. Important practical problem: MMC occurs frequently in real-world pipelines, yet receives less attention than model-level fusion.\nS2. Novel RAG formulation: combining internal and external KBs and explicitly tackling modality/domain shift via alignment + clustering is fresh in the MMC context.\nS3. No fine-tuning of the large backbone: framework works even with “API-only” generative models, broadening applicability.\nS4. Comprehensive evaluation: three datasets (general + domain-specific), multiple missing-rate settings, and downstream task benefits.\nS5. Ablation on retrieval components and filtering validates each design choice."}, "weaknesses": {"value": "W1. Scope limited to image-text dyads; unclear whether the approach scales to higher-order or fundamentally different modalities (e.g., audio, LiDAR).\nW2. Generation quality is assessed only indirectly (via downstream tasks). Lack of direct fidelity/semantic metrics (e.g., CLIP-Similarity, FID) or human evaluation.\nW3. External KB dependence: quality and domain bias of external data may dominate results; sensitivity analysis is brief.\nW4. Computational footprint: retrieving from (possibly large) dual KB + generating multiple candidates can be expensive; runtime/memory costs are not reported.\nW5. “First” claim could be overstated—earlier works have combined retrieval and generation for cross-modal reconstruction, though perhaps not under the MMC label; a clearer positioning is needed.\nW6. Feature-alignment technique is only sketched; hyper-parameters and convergence behaviour are not well analysed."}, "questions": {"value": "Q1. How does performance vary with the size and domain distance of the external KB? Please provide a scaling or ablation study.\nQ2. What alignment method is used (e.g., Procrustes, MMD, contrastive loss)? Is it learned jointly with retrieval or pre-computed?\nQ3. How many candidate completions are generated per query, and how sensitive are results to this number?\nQ4. Could retrieval introduce harmful or biased content? Any safeguards or toxicity filtering applied?\nQ5. What is the wall-clock cost (GPU hours) and latency per completion compared with direct generation?\nQ6. Have you tested extremely high missing rates (e.g., 90 %) where internal KB is tiny? Does external data fully compensate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AKaduvpPMH", "forum": "6LA7KDjNsy", "replyto": "6LA7KDjNsy", "signatures": ["ICLR.cc/2026/Conference/Submission14889/Reviewer_ZWWy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14889/Reviewer_ZWWy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644994942, "cdate": 1761644994942, "tmdate": 1762925235554, "mdate": 1762925235554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAG4DMC to address the problem of missing modalities in multimodal datasets. Existing generation-based methods suffer from low reliability, while simple RAG-based approaches are ineffective due to noise and domain gaps in external data. To overcome these issues, the authors construct a dual knowledge base by combining internal complete data with external public data. They align and integrate the semantic spaces through cross-modal mapping, clustering-based filtering, and Procrustes alignment. For missing samples, a two-stage fusion retrieval is performed to find highly relevant examples, which are then used with BLIP2 and Stable Diffusion to generate restoration candidates. Finally, CLIP cosine similarity, BLEU, and NIQE metrics are used to select the most semantically consistent and high-quality result.\n\nExperiments conducted on MSCOCO, Flickr30K, and RSICD demonstrate that RAG4DMC outperforms all baseline methods. Notably, as the missing rate increases, the performance gap between RAG4DMC and other methods widens, proving its robustness under data-deficient conditions. The authors further extend the concept of RAG beyond text generation to data-level multimodal restoration, showing that the generated complete datasets significantly enhance the performance of downstream models such as CLIP and LLaVA"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- One of the key strengths of this paper is that it effectively overcomes the generalization limitations of fine-tuning approaches that rely on pre-trained generative models or a small number of fully observed samples. Notably, this work is the first to apply RAG to the Missing Modality Completion problem, leveraging retrieval-based semantic grounding to achieve more reliable and consistent restoration.\n\n- The proposed RAG4DMC framework simultaneously utilizes both an internal knowledge base and an external knowledge base, addressing domain shift between the two through feature alignment and clustering-based filtering. These alignment and filtering procedures reduce noise and unify the semantic space, thereby improving retrieval quality and overall restoration accuracy.\n\n- Through a multi-modal fusion retrieval strategy, the model fuses information across images and text, enabling more fine-grained search results. Additionally, the candidate selection stage ensures semantically coherent outputs, making the restoration process far more stable than simple generation-based reconstruction and enabling faithful modality recovery grounded in real data distribution.\n\n- Even under various missing-ratio settings, downstream models trained on restored data from RAG4DMC achieved consistent and significant performance gains on tasks such as image–text retrieval and image captioning. These results demonstrate that the proposed approach remains robust and effective even in realistic scenarios with severe data incompleteness."}, "weaknesses": {"value": "- RAG4DMC consists of multiple stages cross-modal mapping, knowledge filtering, cross-domain alignment, and retrieval-based completion which collectively result in high computational complexity. When applied to large-scale datasets, both training and inference may become slow, indicating potential inefficiency in large-scale or real-time environments.\n\n- The framework focuses only on image and text modalities, leaving its applicability to other modalities such as speech or depth insufficiently explored. Further experiments are needed to verify whether the proposed approach can be effectively extended to additional modalities.\n\n- Existing MMC methods generally suffer from limited transferability in out-of-domain scenarios, often requiring retraining to maintain performance on new domains, which incurs high computational cost. Although RAG4DMC mitigates this issue through a dual knowledge base and cross-domain alignment, the bidirectional mapping is learned primarily from internal data, making it difficult to guarantee complete domain transfer. Moreover, due to its reliance on BLIP2 and Stable Diffusion, the generation quality may degrade in domains that differ significantly from the pre-training data, potentially resulting in unnatural or inaccurate outputs."}, "questions": {"value": "- The generator used in the proposed framework appears to be implemented based on Stable Diffusion, but it is unclear which specific version was used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PGOFTlkEZZ", "forum": "6LA7KDjNsy", "replyto": "6LA7KDjNsy", "signatures": ["ICLR.cc/2026/Conference/Submission14889/Reviewer_MkeP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14889/Reviewer_MkeP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901425689, "cdate": 1761901425689, "tmdate": 1762925235181, "mdate": 1762925235181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RAG4DMC, a novel framework that applies retrieval-augmented generation (RAG) to the task of data-level Missing Modality Completion (MMC). The core contribution is a sophisticated system that constructs a dual knowledge base from both in-domain and external datasets, employing techniques like feature alignment and clustering-based filtering to mitigate domain and modality shifts. The proposed multi-modal fusion retrieval, which combines intra-modal retrieval with cross-modal re-ranking, is a clever approach to guide the generation process more effectively. The experimental results on both general and domain-specific datasets demonstrate that this method not only produces more accurate and semantically coherent completions but also leads to significant improvements in downstream tasks like image-text retrieval and captioning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is among the first to systematically adapt the RAG paradigm for data-level Missing Modality Completion, moving beyond feature-level imputation to generate complete, usable data samples.\n\n2. The dual-knowledge-base design, which leverages both internal and external data, is a key strength. The proposed methods for feature alignment and clustering-based filtering to handle domain and modality gaps are well-motivated and appear effective.\n\n3. The two-stage multi-modal fusion retrieval process is a significant contribution. By first using precise intra-modal retrieval and then refining with cross-modal signals, the method effectively mitigates the inherent modality gap, leading to more semantically relevant context for the generator.\n\n4. The authors have conducted extensive experiments on a variety of datasets (MSCOCO, Flickr30K, and the domain-specific RSICD), evaluating the impact on multiple downstream tasks. The inclusion of several well-designed baselines and ablation studies (e.g., KFA-RAG, Combined-RAG) effectively dissects the contribution of each component of their framework."}, "weaknesses": {"value": "1. The overall framework has many components and hyperparameters (e.g., clustering parameters, thresholds, fusion weights). This complexity might make the system difficult to tune and reproduce. While the appendix provides some details, a more in-depth analysis of hyperparameter sensitivity would be beneficial.\n\n2. The construction of the knowledge base, particularly the clustering, filtering, and nearest-neighbor search for alignment, could be computationally expensive for very large-scale internal and external datasets. The complexity analysis in the appendix is helpful, but the practical implications on massive datasets remain a potential concern.\n\n3. While the quantitative results are strong, the paper could benefit from more qualitative examples. Showing more side-by-side comparisons of the generated modalities from RAG4DMC versus the baseline methods would provide more intuitive evidence of its superior performance in generating semantically coherent and high-fidelity data."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Vv6yoU7c3l", "forum": "6LA7KDjNsy", "replyto": "6LA7KDjNsy", "signatures": ["ICLR.cc/2026/Conference/Submission14889/Reviewer_i2QC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14889/Reviewer_i2QC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927723384, "cdate": 1761927723384, "tmdate": 1762925234399, "mdate": 1762925234399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RAG4DMC, a retrieval-augmented generation framework for data-level Missing Modality Completion (MMC). RAG4DMC overcomes challenges of fine-tuning large models and limited domain-specific data by structuring knowledge for more effective completion, leading to significant gains in downstream tasks. RAG4DMC builds a dual knowledge base —one from comprehensive in-dataset samples and another from external public datasets. It enhances performance by using feature alignment and clustering-based filtering to better manage differences in modality and domain."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* First framework for RAG tailored specifically for data-level missing modality completion problem. \n* Combination of internal complete samples with external public datasets to generate the dual knowledge base. \n* Two-stage multi-modal fusion retrieval strategy that leverages both intra-modal precision and cross-modal cues via pseudo-embeddings, ensuring the retrieval provides semantically consistent and highly relevant context for generation.\n* Outperforms all baselines across general domain (MSCOCO, Flickr30K) and domain-specific (RSICD) datasets highlighting effectiveness in diverse tasks under high missing rates."}, "weaknesses": {"value": "* High computational complexity of the proposed knowledge base construction especially the K-means clustering algorithm and iterative nearest neighbor search during cross-domain alignment. \n* In the setting it should be mentioned that the incomplete samples are encountered during the training phase (through missing rates) and the evaluation is performed on the complete samples.\n* High sensitivity of the performance to specific thresholds associated with filtering and retrieval size.\n* How does the quality of results associated with Direct generation baseline change with improvements in image caption generation models (for missing text) and text to image models (for missing image).\n* The negative effects of missing modalities are increasingly significant in multimodal classification tasks such as audio-visual action recognition (for example, Mit-51, UCF-101, and Activity Net as examined in GTI-MM). This study does not address experiments related to audio-visual action recognition with varying rates of missing data."}, "questions": {"value": "* Regarding Equations (20) and (21), were the fixed weights for candidate selection empirically optimized for both BLEU (image-to-text) and NIQE (text-to-image)? Additionally, is it appropriate to apply the same weighting scheme given the differences between these metrics?\n* Under multi-modal fusion retrieval, it is not clear in terms of ablation if sim_{fuse} provides additional benefits over top-k retrieval results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XyZvxAWGue", "forum": "6LA7KDjNsy", "replyto": "6LA7KDjNsy", "signatures": ["ICLR.cc/2026/Conference/Submission14889/Reviewer_nBpJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14889/Reviewer_nBpJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098064877, "cdate": 1762098064877, "tmdate": 1762925233502, "mdate": 1762925233502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}