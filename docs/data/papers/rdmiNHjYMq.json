{"id": "rdmiNHjYMq", "number": 787, "cdate": 1756818146124, "mdate": 1763645441763, "content": {"title": "Pre-training Pure GNNs as Graph Learners", "abstract": "Graphs from different datasets exhibit diverse numbers of features and labels, where each feature or label is associated with different semantic meanings. Such diversity poses challenges in adapting pre-trained graph neural networks (GNNs) to different datasets with a single set of input and output (I/O) module parameters. This raises a fascinating question: Can pure GNNs be pre-trained on diverse datasets, adapting to various datasets effectively without additional effort? To explore this, we propose unified I/O modules that enable pre-training with pure GNNs. Unlike traditional methods that tightly couple parameters to specific datasets, our approach decouples parameters through a shared relation function for the input and uniformly sampled points for the output. These designs effectively resolve the challenges in quantity inconsistency and semantic discrepancies of dataset features and labels. By integrating our I/O modules with various GNN architectures, we demonstrate that pure GNNs can be effective graph learners for direct adaptation to downstream tasks. Pre-training experiments under different setups show that increasing hidden dimensions and the average number of nodes per training dataset enhances model performance. Moreover, fine-tuning the I/O modules with frozen pre-trained graph operators significantly simplifies the model hyperparameter tuning process, achieving superior or comparable performance to supervised models on downstream datasets.", "tldr": "", "keywords": ["graph neural network", "pre-training"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ed7239730f238ddc0b2467b370aa66f9c348b95.pdf", "supplementary_material": "/attachment/6d19a84e46ef37c4828f9bc25c472e19ba1da01f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes input (on node features) and output (for node predictions) modules for GNNs to allow pre-training them and performing downstream predictions on different tasks. Empirically, this paper demonstrates that downstream performances improves with increased training data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **(S1) Contribution (Originality)** Cross dataset training for GNNs is an important unsolved problem. This paper proposes a relatively clean solution to the key problems of cross dataset training (mapping input and outputs).\n- **(S2) Empircial Evaluation (Quality)** The empirical evaluation is well done. The authors test on many datasets from different domains and evaluate many different backbones (=GNNs). They investigate the performance of their methods in different settings (1-shot, 3-shot, fine-tuning) and compare against a diverse array of models.\n- **(S3) Performance (Significance)** Finally, performance of the model in few-shot and fine-tuning scenarios is good and often outperforms all other models."}, "weaknesses": {"value": "- **(W1) Clarity of definitions:** Section 3 is dense and difficult to understand. In particular, the definition of the I/O modules is difficult to follow due to the density of notation. Here are some points that need clarification:\n\t-   $\\mathbf{S}\\_{\\text{src}}$ and $\\mathbf{S}\\_{\\text{tgt}}$  are unintuitive, are these simply learned weight matrices?\n\t- The definition of $f_\\texttt{in}$ states that it maps from $\\mathbb{R}^{n \\times d_{\\text{in}}}$ to $\\mathbb{R}^{d_{\\text{in} \\times s}}$. However, this means it does not produce node-embeddings?\n\t- What is the difference between your input module and DeepSet?\n\t\n- **(W2) Grounding of empirical results:** Figure 5 is difficult to interpret, it is supposed to show a performance gap between models on different domains / different heterophily but it is unclear to me what the figure shows (and how big the performance gap actually is).\n\n\n- **(W3) Scaling:**\n\t- **(W3.1)** Figure 3 shows the relation between GNN model size (#layers or hidden dimension) against test set performance. We can see that for the best performing models such as GCN or mixhop the model size has very limited impact on test performance.\n\t- **(W3.2)** Figure 4 shows the relation between the average number of nodes and test performance. While there is a positive correlation it seems to be quite small (it would be good if the authors could quanitfy this). Furthermore, training on more datasets seems to have little impact on test performance.\n\t- Combined (W3.1) and (W3.2) seem to indicate a fundamental limitation of this approach. While pre-training does clearly give us good performance in (few-shot) settings, this approach seems to be unable to scale with model size and more diverse data.\n\n\t- **(W4) Homophilic data:** The model struggles with homophilic data (Figure S6). While I think that this is not a big problem since the model works well on heterophilic data, the authors should more directly state that in the main paper. (This is not clear enough about the reslts in the appendix: _\"Results show that models generally achieve better results on heterophilic test graphs than homophilic ones ...\"_)\n\n\n\n- **Minor Weaknesses**\n\t- While Figure looks visually appealing, it is not helpful in understanding the architecture."}, "questions": {"value": "- See weaknesses.\n- What does \"original split\" in Table 1 mean?\n\n\n**Overall,** while I think that this paper makes some good contributions there two primary things that need to be addressed. (W1) The writing makes it difficult to understand the architectural advances. (W3) The scaling results seem to indicate a fundamental limitation of this approach. Overall, I am voting 4 - marginally below acceptance threshold."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dztomUz39o", "forum": "rdmiNHjYMq", "replyto": "rdmiNHjYMq", "signatures": ["ICLR.cc/2026/Conference/Submission787/Reviewer_nQLH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission787/Reviewer_nQLH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761058620149, "cdate": 1761058620149, "tmdate": 1762915604828, "mdate": 1762915604828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores whether pure graph neural networks can be pre-trained across diverse graph datasets without relying on language models. The authors propose unified input and output modules that decouple model parameters from dataset-specific feature and label spaces—using a shared relational function for input features and uniformly sampled pseudo-labels for outputs. This allows GNNs to generalize across datasets with different semantics. Experiments show that the proposed method enables effective cross-dataset pre-training, achieving competitive or superior results to supervised and LM-based baselines while simplifying fine-tuning and hyperparameter tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is well-organized and easy to follow. It provides clear motivation, formal problem formulation, and theoretical derivations that make the proposed Unified I/O framework convincing and conceptually coherent.\n2.\tThe paper tackles a highly relevant and underexplored problem—how to unify diverse graph datasets with different input and output spaces. This direction has strong practical significance for building more generalizable graph foundation models."}, "weaknesses": {"value": "1.\tThe proposed method is only demonstrated on node classification tasks, while other settings such as node regression and graph-level tasks (where graph pre-training is often most impactful) are not explored. This narrow scope limits the general applicability and practical influence of the framework.\n2.\tAlthough the experiments are extensive, the performance of Unified I/O is not consistently competitive. In many settings, it falls notably behind the best existing methods. Moreover, several chosen baselines are relatively weak — for example, on the Cora original split, Unified I/O achieves 82.32%, yet many self-supervised approaches surpass this level by a clear margin. This undermines the strength of the empirical claims.\n3.\tThe results in Figure 4 are disappointing — increasing the number of training datasets yields almost no improvement, which questions the necessity of such pre-training compared with simply performing self-supervised learning on a single dataset. In Figure 3, the claim that performance keeps improving with larger hidden dimensions or deeper layers is counter-intuitive; the curves flatten toward the end, suggesting the authors may not have reached the turning point where over-parameterization degrades results. If not, additional evidence is needed. Moreover, in scaling analysis, it would be more appropriate to examine total model parameter count, as commonly done in LLM research, rather than only hidden dimension or layer depth.\n4.\tThe output module and final loss design of Unified I/O resemble a clustering process, raising concerns about convergence stability. On complex datasets, if the initial parameters are far from optimal, it is unclear whether the model can still converge to a good solution. This also raises potential cold-start issues during pre-training, as the model may lack meaningful gradient signals in the early stage."}, "questions": {"value": "Please refer to the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CXb2CoMso9", "forum": "rdmiNHjYMq", "replyto": "rdmiNHjYMq", "signatures": ["ICLR.cc/2026/Conference/Submission787/Reviewer_fnYM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission787/Reviewer_fnYM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761211192882, "cdate": 1761211192882, "tmdate": 1762915604655, "mdate": 1762915604655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified I/O module that effectively aligns the dimensions and semantics of features and labels across diverse datasets. This design enables GNNs to be pre-trained on diverse datasets and directly adapted to unseen downstream datasets. The authors conduct extensive experiments by integrating the proposed module with a variety of GNN architectures. The experimental results demonstrate that the module can effectively unify feature and label spaces and integrate well with different GNN architectures."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The proposed output module is novel, as it enables the pre-trained models to directly adapt to diverse target spaces.\n\nS2. The proposed method demonstrates strong performance in node classification under various settings.\n\nS3. Figure 4 reveals an interesting and counter-intuitive phenomenon that increasing the node scale of training datasets improves the model’s adaptability to test datasets, while increasing the number of training datasets may not lead to better performance."}, "weaknesses": {"value": "W1. The proposed input module is similar to the feature unification mechanism proposed by FUG [1]. The authors should provide a more detailed discussion on the specific distinctions\n\nW2. The explanation of why the proposed shared relation function can unify feature and label semantics mainly relies on intuition, lacking a theoretical justification or semantic alignment analysis.\n\nW3. The downstream task only includes node classification. Although Appendix A claims that the method can be applied to general graph learning tasks, there is no experimental validation to support this claim.\n\nW4. The compared methods are limited. How does the proposed method compare with SOTA GFMs, such as FUG [1], SAMGPT [2], RiemannGFM [3]?\n\n[1] FUG: Feature-Universal Graph Contrastive Pre-training for Graphs with Diverse Node Features. NeurIPS' 24.\n\n[2] SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation. WWW' 25.\n\n[3] RiemannGFM: Learning a Graph Foundation Model from Riemannian Geometry. WWW' 25."}, "questions": {"value": "Apart from the weaknesses, \n\nQ1. In some settings, the proposed method performs worse than GraphAny on homophilic graphs, but better on heterophilic graphs. As far as I know, GraphAny has designs for heterophily, while this method does not. Why does this method outperform GraphAny on heterophilic graphs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g0rPUtzUAf", "forum": "rdmiNHjYMq", "replyto": "rdmiNHjYMq", "signatures": ["ICLR.cc/2026/Conference/Submission787/Reviewer_RULu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission787/Reviewer_RULu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617996864, "cdate": 1761617996864, "tmdate": 1762915604510, "mdate": 1762915604510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a new pre-training framework for graph data. Considering the varying input feature dimension and output label classes, the authors mainly devised I/O modules for input and output, which avoid direct connection with input data and actual downstrem tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of finding graph foundation model is meaningful.\n2. The organization is good to follow."}, "weaknesses": {"value": "1. Techniques. \\\nAt input side, the authors input feature similarity matrix, something like X^TX, rather than raw feature, followed by a all-one vector, so the model always receives the same length of input. But this operation is oversimplified, and lose a lot of information. Consider two cases, 1) one-hot raw features (e.g., user / item feature in recommendation), so X^TX is 0, 2) some datasets may have thousands of dimension features, and some datasets (e.g., molecule QM9) only have very limited feature dimensions, so X^TX could reflect totally different feature relations. How can we deal them uniformly? Further, features may appear very complex patterns, and only X^TX cannot fully capture them.\\\nAt output side, different datasets have different number of classes, from single digits to hundreds / thousands. How to set the number of pseudo lables? Also, different classes sometimes are not totally independent, while pseudo labels are selected evenly in the sphere space.\n\n2. Experiments. 1) In table 1, traditional semi-supervised GNNs are also need to compare, to show the necessity of pre-training. 2) In table 2, the proposed method has obvious improvement only on COMPUTERS if consider std."}, "questions": {"value": "In Fig. 3 (b), why did the GNNs not occur over-smoothing, when stacking many layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "asQBTPfrjv", "forum": "rdmiNHjYMq", "replyto": "rdmiNHjYMq", "signatures": ["ICLR.cc/2026/Conference/Submission787/Reviewer_Btqw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission787/Reviewer_Btqw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982143788, "cdate": 1761982143788, "tmdate": 1762915604393, "mdate": 1762915604393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}