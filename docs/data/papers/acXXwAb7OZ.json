{"id": "acXXwAb7OZ", "number": 12427, "cdate": 1758207717388, "mdate": 1759897510598, "content": {"title": "Text Modality Oriented Image Feature Extraction for Detecting Diffusion-based DeepFake", "abstract": "The widespread use of diffusion methods enables the creation of highly realistic images on demand, thereby posing significant risks to the integrity and safety of online information and highlighting the necessity of DeepFake detection. Our analysis of features extracted by traditional image encoders across ten diffusion types reveals that both low-level and high-level features offer distinct advantages in identifying DeepFake images. Furthermore, the highly realistic images generated by diffusion models make it increasingly difficult to distinguish between real and fake within the image domain. Building on these insights, we propose the development of an effective representation beyond the image domain, capable of capturing both low-level and high-level features for detecting diffusion-based DeepFakes. Specifically, for a given target image, the representation we discovered is a corresponding text embedding that can guide the generation of the target image with a specific text-to-image model. Experiments conducted across ten diffusion types compared with five representative deepfake detection baselines demonstrate the efficacy of our proposed method.", "tldr": "TOFE is a text-modality feature extraction method for effectively detecting diffusion-generated DeepFakes.", "keywords": ["DeepFake Detection", "Diffusion", "Feature Extraction"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/736e56324d48773fc97871fe7a95b09b142c409d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel cross-modal approach to DeepFake detection. Specically, this paper proposes TOFE, a method that transforms image information into the text embedding space. The authors argue that the the optimized text embedding contain both high-level and low-level features of the given images. Such features can help the detectors better capture the information of fake and real images. The authors have conducted abundent experiments to demonstrate that the proposed methods can work well on different settings compared to other state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel methods for important research questions\n- Well-written paper\n- Sufficient experimental evidence"}, "weaknesses": {"value": "- Lack of theoretical support\n- Lack of more practical settings"}, "questions": {"value": "This paper proposed the novel fake image detection methods by combining both high-level features and low-level features into the optimized text embeddings which can help the detectors better distinguish fake images apart from real ones. Experimental results demonstrate that the proposed methods can have great performance on the DIRE datasets than other methods. The paper is well-written and the experimental results are convincing. However, I do have some comments regarding the theoretical support and the practical settings in this paper.\n\n- The most important questions is why the optimized text can help boost the detection performance. The authors are supposed to give more explanations on why TOFE works.\n\n- The authors trained and tested the proposed methods on the DIRE datasets, which is constructed in 2023. In recent years, there have been many developments in the field of diffusion generation. The authors are supposed to test their methods on the state-of-the-art generation models like Stable Diffusion 3.5, Qwen-Image-Edit, and the latest version of Dalle. The current results(99 aacuracy) cannot reflect the challenge of identifying fake images in real-world scenarios.\n\n- The authors are also supposed to argue the performance of the proposed methods on the edited images as well as the adversary examples that are designed to confuse the detectors. Moreover, the proposed methods may be too time-consuming and computationally expensive for practical deployment, especially when scaling to large datasets or real-time detection scenarios. The authors should therefore provide an analysis or discussion of the efficiency trade-offs and potential optimizations to make their approach more feasible in real-world applications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DBpzaF4hnT", "forum": "acXXwAb7OZ", "replyto": "acXXwAb7OZ", "signatures": ["ICLR.cc/2026/Conference/Submission12427/Reviewer_brHr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12427/Reviewer_brHr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708962402, "cdate": 1761708962402, "tmdate": 1762923316108, "mdate": 1762923316108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TOFE (Text‑modality Oriented Feature Extraction) for detecting diffusion‑generated DeepFakes. Instead of extracting features purely in the image domain, TOFE refines continuous text embeddings so that, under DDIM inversion and classifier‑free guidance, they reproduce the target image’s reverse trajectory; the refined embeddings are then fed to a small MLP classifier."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper moves feature extraction to the text modality: starting from a text embedding C, it optimizes C so that conditional denoising follows the inverted latent trajectory of the target image. Figures 2–3 illustrate the setup and what the optimization is correcting.\n\n2. On DIRE (10 diffusion generators), TOFE attains the best average ACC/AP (98.21% / 99.76%) against other methods."}, "weaknesses": {"value": "1. Quantifying separability on 2D t‑SNE embeddings risks unreliable distance comparisons. T‑SNE‑then‑MMD/JS analysis may distort distances.\n\n2. Figure 2 uses BLIP semantic initialization as an example, but author implements the default empty prompt embedding as the starting point, and the impact of the difference between the two on the final feature/detection effect is not systematically presented."}, "questions": {"value": "1. What is the impact of null vs. caption‑initialized embeddings (e.g., BLIP) on both reconstruction quality ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w03gLHfLqG", "forum": "acXXwAb7OZ", "replyto": "acXXwAb7OZ", "signatures": ["ICLR.cc/2026/Conference/Submission12427/Reviewer_c3dr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12427/Reviewer_c3dr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849869657, "cdate": 1761849869657, "tmdate": 1762923315866, "mdate": 1762923315866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for detecting fake images. The central claim is that both low-level and high-level features are crucial for effective fake image detection. The authors further hypothesize that as the quality of generated images improves, it becomes increasingly difficult to detect them using only domain specific visual features. Their key insight is that the text embeddings used to guide image generation, containing both low and high-level semantic information exhibit discriminative properties well-suited for fake image detection. Experimental results demonstrate consistent improvements over several baselines on images generated by both diffusion models and GANs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The TOFE method presented by the paper outperforms some popular baselines."}, "weaknesses": {"value": "Major Weaknesses\n1. Line 43: The paper discusses moving past the \"traditional binary classification\" mindset. However, binary classification is simply the definition of the problem, and TOFE also does the same using different features, therefore the motivation seems unclear to me. The paper would greatly benefit from greater clarity here.\n2. The main argument regarding the necessity of both high-level and low-level features needs more evidence. For instance, are the authors able to identify specific low/high-level features. The current experiments (such as Section 3), are done on ResNet and CLIP backbones. While the effectiveness of CLIP for fake image detection has been shown in the past, there are several other techniques (training the network for fake image detection, reconstruction based detection), and it is not clear to me that these methods suffer from the same drawbacks.\n3. line 79-80: This is also not entirely true. There are other factors of variation in a diffusion model such as starting noise and added randomness (DDPM style). Therefore, it is difficult to attribute a single representation for a given image. The representation might incorporate important features (as shown by [1]), but it is not clear to me if the text will steer the generation towards a \"single target image\".\n4. line 267-269: This does not explain why only low level features are not the right way to go. Additionally, in fact [2] show that if post-processing is applied during training (getting rid of some low-level features), the model generalizes better, which seems contrary to the hypothesis being presented here. I believe the paper would benefit by being more clear here. \n5. Line 305-306 talks about how any representation with high-level and low-level features is not necessarily helpful. This raises doubts on what is the thing that is special about the representations obtained by TOFE. It would be helpful if the authors can conduct experiments to understand this in a deeper manner.\n6. The paper currently lacks experiments studying the sensitivity to post-processing. Conducting a sweep over JPEG/Resizing (both upsampling and downsampling), WEBP and blur would make the work more suited for practical applications.\n\nMinor Weaknesses/Suggestions,\n1. Line 41: If the main task is fake image detection, what is the downstream task.\n2. Line 42: Ojha et al is designed for universal fake image detection (they test on GANs as well, not only diffusion)\n3. Line 224-225: DDIM is not the only way to perform denoising (DDPM + other solvers such as PNDM would also work).\n4. Line 42: DIRE is cited as a feature extraction method, but it is reconstruction-based and it would be better if it was distinguished from other feature extraction methods).\n5. Sec 4.2.2 is titled Problem Definition, but it talks about the particular algorithm developed by the authors. Problem Definition should talk about the problem which the authors are trying to solve.\n6. Line 62-63: distinguishing between real and generated images within the image domain is becoming increasingly challenging. This claim is not entirely true. While generalizing to other generators/unseen post-processing operations remain challenging, usually trained classifiers are able to discriminate between real and fake distributions successfully. I believe this statement needs to be worded with further clarity, since it is critical to the design choice.\n\nReferences\n1. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., & Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618.\n2. Wang, S. Y., Wang, O., Zhang, R., Owens, A., & Efros, A. A. (2020). CNN-generated images are surprisingly easy to spot... for now. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 8695-8704)."}, "questions": {"value": "1. Is the Image being DDIM inverted using the unconditional diffusion model? If so why? \n2. Related to above, it seems to me like the DDIM sampling is done from the unconditionally inverted latent. Why is the embedding being optimized to align with the unconditional trajectory. One possible solution to the same could be the null-text embedding (which would mimic the original trajectory). Please clarify the motivation behind this method.\n3. Algorithm 1: It starts by saying z_0 is obtained from the image (seems like it comes from the latent encoder). Then it proceeds to say that z_0 is computed from DDIM inversion which is confusing to me.\n4. Line 299-302 talks about extracting features at every time step. However, only T=1 is being used in line 350-351. This design choice would benefit from stronger motivation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hkG7cPcvzz", "forum": "acXXwAb7OZ", "replyto": "acXXwAb7OZ", "signatures": ["ICLR.cc/2026/Conference/Submission12427/Reviewer_S9Kt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12427/Reviewer_S9Kt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887911164, "cdate": 1761887911164, "tmdate": 1762924684801, "mdate": 1762924684801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors explore whether cross-model feature extraction can be instrumental in deepfake detection. They highlight the benefits of using a combination of high- and low-level features for deepfake detection. As a remedy, the authors propose  mapping images into other modalities, such as text, which helps distinguish between the real and fake data in the image domain.  The authors demonstrate their findings empirically."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: the paper considers the combination of cross-modal low- and high-level features. It is my understanding that investigating the diffusion-model inversion to extract the text-guided features is a novel and promising idea. Interestingly, it also shows the transferability to the GAN models\n\nQuality: the paper is written well, and seems reproducible. The algorithm description and the justification for the method look correct.\n\nSignificance: the paper's significance is in discovering new method to analyse latent features via inversion."}, "weaknesses": {"value": "Novelty: there is a need to contrast this work with the existing ones also looking at the low- and high-level feature extraction (see Question 1)."}, "questions": {"value": "1. the idea of combining low- and high-level features for deepfake detection is a known one, and it has been seen before (see, e.g., Liu et al (2022). Of course, it has not been seen in this setting, but it would be good if the authors could contrast between these work and the proposed one from the angle of combining the high- and low-level features. \n\nLiu et al (2022) Cross-Domain Local Characteristic Enhanced Deepfake Video Detection, ACCV\n\n2. The point about transferability (Section 5.4) is very interesting. I would like to ask the authors to say if it is possible to present additional experimental evidence across different non-diffusion architectures, so that we can see if this observation persists. \n\n3. Is it possible to present confidence intervals?\n\n4. Figure 1 presentation can be improved with enlarging the points"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K6s4g2D8og", "forum": "acXXwAb7OZ", "replyto": "acXXwAb7OZ", "signatures": ["ICLR.cc/2026/Conference/Submission12427/Reviewer_ALvA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12427/Reviewer_ALvA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996788455, "cdate": 1761996788455, "tmdate": 1762923315036, "mdate": 1762923315036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}