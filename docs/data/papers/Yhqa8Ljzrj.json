{"id": "Yhqa8Ljzrj", "number": 7799, "cdate": 1758036772706, "mdate": 1763408197550, "content": {"title": "Quantifying Human-AI Synergy", "abstract": "We introduce a novel Bayesian Item Response Theory framework to quantify human–AI synergy, separating individual and collaborative ability while controlling for task difficulty in interactive settings. Unlike standard static benchmarks, our approach models human–AI performance as a joint process, capturing both user-specific factors and moment-to-moment fluctuations. We validate the framework by applying it to human–AI benchmark data (n=667) and find significant synergy. We demonstrate that collaboration ability is distinct from individual problem-solving ability. Users better able to infer and adapt to others’ perspectives achieve superior collaborative performance with AI–but not when working alone. Moreover, moment-to-moment fluctuations in perspective taking influence AI response quality, highlighting the role of dynamic user factors in collaboration. By introducing a principled framework to analyze data from human-AI collaboration, interactive benchmarks can better complement current single-task benchmarks and crowd-assessment methods. This work informs the design and training of language models that transcend static prompt benchmarks to achieve adaptive, socially aware collaboration with diverse and dynamic human partners.", "tldr": "Bayesian Item Response Theory framework that models human–AI collaboration as a dynamic, probabilistic process, separating individual ability, collaborative ability, and task difficulty.", "keywords": ["Interactive Evaluation", "Adaptive Benchmarks", "Large Language Models (LLMs)", "Bayesian Item Response Theory", "Human-AI Collaboration", "Theory of Mind"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2febba8336860bf2bac3a360ab80852016161c8f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a Bayesian Item Response Theory (IRT) framework to quantify human–AI synergy, separating individual from collaborative ability while controlling for task difficulty. The authors demonstrate the framework on two datasets and AI models. The results show that AI helps improve human-AI accuracy. The paper also uses Theory of Mind (ToM) to interpret the observed human-AI synergy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors present the results grounded in empirical results and connect well to prior benchmarks.\n- The analysis using Theory of Mind provides an interesting bridge between computational modeling and social cognition."}, "weaknesses": {"value": "- The model structure is oversimplified compared to the ambitious contribution claimed by the authors. It only considers the ability and the  difficulty. A lot of factors are ignored such as learning effects. The assumption on additivity of ability is also unrealistic--there are a lot of case where human and AI are substitute or complementary with each other.\n- The choice of the Bayesian model is not clear. Since the Bayesian workflow is often iterative and involves model fitting and then checking, the authors should do more model comparison to motivate the model specification they arrive at.\n- The experiments also simplify a lot than realistic human-AI collaboration scenarios. For example, three questions done alone with AI is not much to get a good estimate of ability in my opinion.\n- The paper does not engage with prior work on human–AI complementarity enough. For example, how the framework improves interpretability over existing regression-based or causal models of AI assistance effects?"}, "questions": {"value": "- What are the assumptions of the framework? In general, I would suggest the authors to lay out the assumptions that must hold for their results to be easy to interpret."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w3egpSiqzl", "forum": "Yhqa8Ljzrj", "replyto": "Yhqa8Ljzrj", "signatures": ["ICLR.cc/2026/Conference/Submission7799/Reviewer_e8HW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7799/Reviewer_e8HW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946276584, "cdate": 1761946276584, "tmdate": 1762919842433, "mdate": 1762919842433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a Bayesian Item Response Theory (IRT) framework for evaluating human–AI collaboration. Instead of measuring model performance in isolation, the proposed approach jointly models human and AI contributions during cooperative task solving. The authors apply this method to the ChatBench dataset, where 667 participants complete multiple-choice tasks under both solo and AI-assisted conditions. The results indicate that collaborative performance can differ from solo capability and varies across individuals. The paper also examines Theory-of-Mind (ToM) scores and reports that higher ToM ability is associated with greater improvement when using AI assistance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper tackles a timely and important problem, as understanding interactive LLM behavior and human–AI teaming is increasingly critical when transitioning from offline evaluation to real-world deployment scenarios.\n\n2. The work provides a strong methodological contribution by proposing a principled Bayesian IRT framework that decomposes human solo ability, human–AI collaborative ability, AI contribution, and task difficulty to quantitatively measure human–AI synergy, and the method is supported by strong empirical evaluations.\n\n3. The study offers interesting insight into cognitive mechanisms underlying human–AI interaction, as the finding that Theory-of-Mind predicts collaborative gain, not solo performance, helps explain when and why human–AI synergy emerges, which also provides valuable implications for practitioners on how AI systems should be designed to better support human decision-making."}, "weaknesses": {"value": "1. Experimental tasks all fall within academic contexts  (MMLU-adapted questions). Although justified as a structured benchmark, future work could validate synergy in more naturalistic settings (e.g., group creative work, travel planning, or collaborative coding).\n\n2.  ToM scoring uses an LLM rater, which is reasonable given recent literature on LLM-as-judge, but still vulnerable to construct validity concerns. The authors partially address this via human validation, but a deeper discussion (e.g., potential LLM bias or adversarial prompt cases) in cases where such human and llm alignment diverges significnantly would strengthen this part.\n\n3.  Limited to individual human–AI teaming. It would be interesting to see whether the method generalizes to multi-agent settings where a single human interacts with multiple AI assistants or where a group of humans collaborates with one AI system."}, "questions": {"value": "See above weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7uXVBNpoiH", "forum": "Yhqa8Ljzrj", "replyto": "Yhqa8Ljzrj", "signatures": ["ICLR.cc/2026/Conference/Submission7799/Reviewer_bPAp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7799/Reviewer_bPAp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960582518, "cdate": 1761960582518, "tmdate": 1762919841907, "mdate": 1762919841907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to measure human–AI synergy, where synergy is defined as how much an AI partner improves human performance. The framework follows Bayesian Item Response Theory (IRT). It separates solo human ability, collaborative ability with AI, and item difficulties in solo vs. joint settings. Using ChatBench (396 MCQs across math, physics, moral reasoning; 667 participants), the study shows that human–AI teams outperform humans alone or AI alone, and benchmarks models by their average improves human performance while controlling for difficulty and user abilities. Finally, the authors test whether Theory of Mind (ToM) in users explains who benefits most and find ToM predicts collaborative performance."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Clear synergy metric for model benchmarking. The approach directly estimates each model’s capacity to raise the performance of the average user, rather than relying on static model-alone accuracy, enabling apples-to-apples comparisons of “collaborative capability.” \n\n\n* ToM as a cognitive mechanism for teaming. Users with higher ToM do better with AI but not alone; the paper frames ToM as a plausible mechanism for coordination and division of cognitive labor, aligning with established social-cognitive theory and giving a concrete lens for why teaming helps. \n\n* Empirical improvement that is practically meaningful. The paper finds human-AI even beats AI alone in the descriptive analysis."}, "weaknesses": {"value": "* Framework builds heavily on prior modeling choices. The novelty is mainly in applying existing method to human–AI collaboration. \n\n* ToM effects appear small-to-moderate. My main concern with the findings in the paper is that the ToM–collaboration link is statistically positive but not large (e.g., Spearman ~ 0.17 for joint ability, significant; ~ 0.06 and n.s. for solo). The results seem to suggest ToM is useful, but one factor among several. \n\n \n* Limited window into mechanisms of complementarity. While the paper argues that ToM enables coordination, it does not decompose how human and model contributions combine at the item level (e.g., how many tasks are correct in human-AI collaboration but wrong in both alone?)."}, "questions": {"value": "* Quantifying complementarity directly. Since human+AI outperforms each constituent, what fraction of items are: wrong for both solo agents but right as a team or right for one party but not the other? Reporting these rates would put concrete numbers on complementarity.\n\n* Break out complementarity by difficulty deciles and domain. I would guess one reason for complementarity is human provide reasoning steps to AI. It would be helpful to clarify if AI baseline uses reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3GTgTRv6PV", "forum": "Yhqa8Ljzrj", "replyto": "Yhqa8Ljzrj", "signatures": ["ICLR.cc/2026/Conference/Submission7799/Reviewer_1WjX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7799/Reviewer_1WjX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963648144, "cdate": 1761963648144, "tmdate": 1762919841554, "mdate": 1762919841554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}