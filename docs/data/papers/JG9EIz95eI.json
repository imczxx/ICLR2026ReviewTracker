{"id": "JG9EIz95eI", "number": 6436, "cdate": 1757983799279, "mdate": 1763054230220, "content": {"title": "SafeCoop: Unravelling Full Stack Safety in Agentic Cooperative Driving", "abstract": "Collaborative driving systems leverage vehicle-to-everything (V2X) communication across multiple agents to enhance driving safety and efficiency. Traditional V2X systems take raw sensor data, neural features, or perception results as communication media, which face persistent challenges, including high bandwidth demands, semantic loss, and interoperability issues. Recent advances investigate natural language as a promising medium, which can provide semantic richness, decision-level reasoning, and humanâ€“machine interoperability at significantly lower bandwidth. Despite great promise, this paradigm shift also introduces new vulnerabilities within language communication, including message loss, hallucinations, semantic manipulation, and adversarial attack. In this work, we present the first systematic study of full-stack safety (and security) issues in natural-language-based collaborative driving. Specifically, we develop a comprehensive taxonomy of attack strategies, including connection disruption, relay/replay interference, content spoofing, and multi-connection forgery. To mitigate these risks, we introduce an agentic defense pipeline, which we call **SafeCoop**, that integrates a semantic firewall, language-perception consistency checks, and multi-source consensus, enabled by an agentic transformation function for cross-frame spatial alignment. We systematically evaluate SafeCoop in closed-loop CARLA simulation across 32 critical scenarios, achieving 69.15% driving score improvement under malicious attacks and up to 67.32% F1 score for malicious detection. This study provides guidance for advancing research on safe and trustworthy language-driven V2X collaboration in transportation. Our code is available at: [https://anonymous.4open.science/r/SafeCoop-4800](https://anonymous.4open.science/r/SafeCoop-4800)", "tldr": "We present SafeCoop, the first systematic study on attack and defense framework for agentic collaborative driving.", "keywords": ["Agentic Collaborative Driving", "Vehicle-to-everything", "Autonomous Driving", "V2X Communincation", "Multi-model Large Language Model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/15d91ed135473c5be683a9f13980b2d66980f348.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a comprehensive study on full-stack safety and security issues in natural-language-based collaborative driving. The authors develop a taxonomy covering both generic threats such as connection disruption and relay interference, and language-specific vulnerabilities like content spoofing. They introduce SafeCoop, a defense pipeline integrating a semantic firewall, language-perception consistency checks, and multi-source consensus. The experimental evaluation in CARLA simulations demonstrates the approach's effectiveness against adversarial scenarios. While the methodology is well-structured and the analysis thorough, the defense mechanisms primarily adapt existing natural language processing security techniques rather than introducing novel multi-agent security innovations. The paper's key contribution lies in systematically addressing security gaps for language-mediated vehicular cooperation, though it does not sufficiently explore emergent security challenges arising from multi-agent coordination dynamics in adversarial environments. This work provides valuable insights for robust defense design in natural language-based V2X systems despite the limited novelty in its core defense architecture."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. It develops a well-structured taxonomy that effectively categorizes both generic threats and language-specific vulnerabilities.\n\n2. The methodology is clearly presented with a thorough analysis of the proposed solution.\n\n3. The work systematically addresses security gaps in language-mediated vehicular cooperation frameworks.\n\n4. The SafeCoop defense pipeline offers practical insights for building robust natural language-based V2X systems."}, "weaknesses": {"value": "1. Some of the proposed attacks, such as connection disruption and relay interference, are not specific to natural language-based collaborative driving systems.\n\n2. The defense mechanisms primarily adapt existing natural language processing security techniques rather than introducing novel multi-agent security innovations.\n\n3. The work fails to adequately highlight how the multi-agent perspective creates unique security considerations not addressed in standard NLP security approaches."}, "questions": {"value": "1. From a multi-agent perspective, what aspects of the proposed attacks and defenses most clearly distinguish this work from existing NLP security research? Specifically, how do the interactions between agents in collaborative perception introduce unique vulnerabilities that traditional NLP security approaches do not address? Does the SafeCoop framework effectively mitigate these through its semantic firewall language-perception consistency checks and multi-source consensus mechanisms?\n\n2. What assumptions does the defense mechanism make regarding the ego vehicle's knowledge of the attacker's capabilities? How does the proposed defense perform against unknown or adaptive attacks?\n\n3. How does the language-driven driving system meet the real-time requirements of autonomous vehicles? What is the computational overhead introduced by the additional defense components, like semantic firewall analysis and language-perception consistency verification, and could these impact critical driving decisions in time-sensitive scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yRcb7OglSJ", "forum": "JG9EIz95eI", "replyto": "JG9EIz95eI", "signatures": ["ICLR.cc/2026/Conference/Submission6436/Reviewer_M1ko"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6436/Reviewer_M1ko"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633189911, "cdate": 1761633189911, "tmdate": 1762918828812, "mdate": 1762918828812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "BJbaxIRAKi", "forum": "JG9EIz95eI", "replyto": "JG9EIz95eI", "signatures": ["ICLR.cc/2026/Conference/Submission6436/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6436/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763054228623, "cdate": 1763054228623, "tmdate": 1763054228623, "mdate": 1763054228623, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first systematic study of safety in natural-language-based collaborative autonomous driving, identifying four attack surfaces (Connection Disruption, Relay/Replay Interference, Content Spoofing, and Multi-Connection Forgery) and proposing SafeCoop, an agentic defense pipeline with three specialized agents. Evaluated on 32 CARLA scenarios, SafeCoop achieves 69.15% driving score improvement under attacks and 67.32% F1 score for malicious detection."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Novel and Timely Problem Formulation\n\n+Comprehensive Attack Taxonomy\n\n+ Closed-loop and Thorough Evaluation"}, "weaknesses": {"value": "Weakness\n\n- Unclear relationship between dual objectives: The system outputs a trust score for each agent, but claims to address both \"performance\" and \"anomaly detection\" objectives. However, the system design only explicitly addresses anomaly detection through trust scoring and filtering. It remains unclear how performance is directly optimized, or whether the authors simply assume that anomaly detection will inherently improve performance. This assumption should be explicitly stated and justified.\n\n- Insufficient attack implementation details: While Section 3 and Appendix D provide a taxonomy of attacks, the paper lacks a detailed explanation of how attacks are actually designed and implemented in the evaluation. For reproducibility and proper assessment of the defense mechanisms, more specifics are needed on attack generation, particularly for the MLLM-based Content Spoofing attacks (e.g., prompt engineering strategies, attack success rates, stealthiness measures). Further, in Table 1, several adversarial scenarios (w/o defense) achieve better performance than the Benign (Non-collab) baseline across multiple metrics. This counterintuitive result raises questions about attack severity and undermines claims about the effectiveness of the defense. While Appendix G partially addresses this through the computation scaling phenomenon, this critical finding deserves prominent discussion in the main paper with a thorough analysis of what it implies for both attack potency and defense necessity.\n\n- Unexplained performance degradation in ablation study: Table 3 shows that adding LPC and MSC agents actually increases the Vehicle Collision (VC) score under CS+MCF attacks, which is concerning. The paper provides no investigation or explanation for this degradation. Could false positives in anomaly detection be causing the defense to filter legitimate messages, thereby reducing situational awareness and increasing collisions? Without presenting false positive/false negative rates and analyzing this phenomenon, the reliability of the defense pipeline remains questionable."}, "questions": {"value": "How is \"performance\" explicitly optimized in the system design, or is it assumed to naturally follow from anomaly detection?\n\nWhy do some adversarial scenarios (Table 1) outperform the non-collaborative baseline? What does this imply about attack severity?\n\nWhat causes LPC and MSC agents to increase vehicle collisions under CS+MCF attacks (Table 3)? Are false positives responsible?\n\nWhy does the Firewall Agent require MLLMs for semantic reasoning when it appears to only process JSON-formatted input?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FBnUrgjZ6X", "forum": "JG9EIz95eI", "replyto": "JG9EIz95eI", "signatures": ["ICLR.cc/2026/Conference/Submission6436/Reviewer_TG64"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6436/Reviewer_TG64"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859960153, "cdate": 1761859960153, "tmdate": 1762918824784, "mdate": 1762918824784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes using LLMs to safeguard natural language-based cooperation among vehicles. The paper summarizes the taxonomy of the attack surface, which includes Connection disruption, Relay interference, Content spoofing, and Sybil attack (multi-connection forgery). The proposed approach combines several LLM agents, each more effective in one, or a subgroup of, defense. Evaluation using Carla shows defense performance using end-to-end metrics such as driving score, route completion, etc. The ablation study showcases different agents' capabilities, and the authors also compare the performance among multiple LLMs as the defense agents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper explores cooperative driving security in language domain.\n+ The writing quality is above average though missing a lof of details and clarity\n+ The evaluations are systematic and comprehensive"}, "weaknesses": {"value": "- Missing details on the range/magnitude of attack being applied during the evaluation. How is each of the attacks staged? What is the range of the attack, e.g., How much temporal misalignment in relay/replay interference? What is being introduced in content spoofing? How many forged connections are there in the Sybil attack? These are important details to provide context to gauge the evaluation metrics. Whether these staged attacks are trivial or sophisticated, the results can lead to entirely different conclusions. One way to show the sophistication is to show if conventional defense against these attacks can be successful, how does LLM compare against conventional methods, and whether using an LLM is an overkill.\n- Missing details on how the LPC agent compares ego perception with received messages. Is it taking a multi-view image or lidar as input? What if the received message is outside the field of view of the perception module?\n- Figure 1, appearing before the abstract, could use additional legends and details to be self-explanatory. What do the numbers after CD, CS, and CS+MCF mean? Why is the MCF description pointing to the CS score? And CS description points to CS+MCF score? Before the readers read the abstract, a couple of legends and explanations could be very helpful to avoid confusion.\n- The paper claims to have studied four attack surfaces (CD, RI, CS, MCF). What has been studied for CD (connection disruption)? There are no results on CD in Table 1. Table 2 is attack detection, not defense. If it's difficult to detect and evaluate, ok to tone down a bit.\n- The evaluation results show LLMs are insufficient in safeguarding against attacks, cannot recover collaborative driving performance before attacks, and cannot be run in real-time. These observations are good to know, but marginally advance the field's understanding. It would be helpful to show contributions/insights obtained to advance the defense."}, "questions": {"value": "Please refer to weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xL5FssGLam", "forum": "JG9EIz95eI", "replyto": "JG9EIz95eI", "signatures": ["ICLR.cc/2026/Conference/Submission6436/Reviewer_4Rwz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6436/Reviewer_4Rwz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104690830, "cdate": 1762104690830, "tmdate": 1762918824396, "mdate": 1762918824396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies full-stack safety for natural-language-based collaborative driving systems. It identifies four attack surfaces (connection disruption, relay/replay, content spoofing, and multi-connection forgery) and proposes SafeCoop, an agentic defense pipeline combining (i) semantic firewalling, (ii) language-perception consistency checking, and (iii) multi-source consensus with temporal checks. \n\nEvaluations in CARLA across 32 scenarios demonstrate that malicious language communication severely harms collaborative driving, and SafeCoop substantially recovers performance and detects adversarial agents, achieving up to ~69% driving score improvement and ~67% F1 detection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely problem: Addresses emerging risks in language-based V2X collaboration, an important but under-explored area as driving LLMs become more capable.\n\n2. Comprehensive threat model: The taxonomy spans channel-level and semantic attack vectors, grounding them in adversarial V2X literature.\n\n3. Agentic defense pipeline: Novel multi-module agent approach (firewall, perception-language consistency, consensus), offering interpretability."}, "weaknesses": {"value": "1. Novelty feels incremental.\nThe paper mainly repackages known security concepts (trust-based filtering, majority consensus, temporal consistency) into an LLM-driving setting. \nWhile the agentic framing is interesting, the core mechanics resemble classical V2X trust scoring + consistency checks, raising questions about conceptual novelty.\n\n2. Limited realism & scalability assumptions.\nThe system assumes synchronous simulation, and perfect ego perception during consistency checks.\nReal V2X networks are asynchronous, lossy, and perception-noisy, which may reduce effectiveness.\n\n3. High latency & unclear deployment feasibility.\nEven fast models are around 700ms and larger models exceed 3s latency, far above real-time driving constraints.\nThe paper acknowledges this but does not meaningfully address deployment pathways. \n\n4. How frequently is the defense executed?\nIf the defense must run continuously or per-frame, the computational burden may be prohibitive; a one-time malicious-agent flag is insufficient for practical systems. Clarifying the defense invocation frequency and cumulative runtime overhead (e.g., cost per second of driving) would be important to judge real-world viability.\n\n5. Formula Clarification. For Eq(2), why is the attacker constrained to modifying a single sender? Additionally, the expression for $D_j$ appears to use observation $o_i$ instead of $o_j$. Is it intentional or an indexing error?"}, "questions": {"value": "Please respond to each point in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JfL8CfGXox", "forum": "JG9EIz95eI", "replyto": "JG9EIz95eI", "signatures": ["ICLR.cc/2026/Conference/Submission6436/Reviewer_NsNL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6436/Reviewer_NsNL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762118273104, "cdate": 1762118273104, "tmdate": 1762918824054, "mdate": 1762918824054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}