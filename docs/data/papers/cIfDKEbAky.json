{"id": "cIfDKEbAky", "number": 282, "cdate": 1756733393478, "mdate": 1759898269298, "content": {"title": "PrismAudio: Decomposed Chain-of-Thought and Multi-dimensional Rewards for Video-to-Audio Generation", "abstract": "Video-to-Audio (V2A) generation requires balancing four critical perceptual dimensions: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy; yet existing methods suffer from objective entanglement that conflates competing goals in single loss functions and lack human preference alignment. We introduce **PrismAudio**, the first framework to integrate Reinforcement Learning into V2A generation with specialized Chain-of-Thought (CoT) planning. Our approach decomposes monolithic reasoning into four specialized CoT modules (Semantic, Temporal, Aesthetic, and Spatial CoT), each paired with targeted reward functions. This CoT-reward correspondence enables **multidimensional RL optimization** that guides the model to jointly generate better reasoning across all perspectives, solving the objective entanglement problem while preserving interpretability. To make this optimization computationally practical, we propose **Fast-GRPO**, which employs hybrid ODE-SDE sampling that dramatically reduces the training overhead compared to existing GRPO implementations. We also introduce **AudioCanvas**, a rigorous benchmark that is more distributionally balanced and covers more realistically diverse and challenging scenarios than existing datasets, with 300 single-event classes and 501 multi-event samples. Experimental results demonstrate that PrismAudio achieves state-of-the-art performance across all four perceptual dimensions on both the in-domain VGGSound test set and out-of-domain AudioCanvas benchmark. The project page is available at~\\url{https://PrismAudio.github.io}.", "tldr": "We introduce PrismAudio, the first video-to-audio framework to use decomposed Chain-of-Thought reasoning and multi-dimensional reinforcement learning to explicitly balance competing objectives.", "keywords": ["Chain-of-Thought", "Reinforcement Learning", "Video-to-Audio Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dccd1018c1ee19be2d3eb4f035f743ae6bbf4939.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes PrismAudio, a video‑to‑audio (V2A) generation framework that uses multi‑dimensional Chain‑of‑Thought (CoT) (Semantic, Temporal, Aesthetic, and Spatial) with reinforcement learning. \n\nTo build PrismAudio, the authors \n- replace CLIP and T5 with VideoPrism and T5‑Gemma, respectively, on top of the model architecture of ThinkSound\n- extend ThinkSound’s monolithic CoT planning into the four CoTs \n- introduce Fast‑GRPO, a hybrid ODE–SDE variant of Group Relative Policy Optimization tailored to flow‑matching models for efficient optimization. \n\nThe paper also presents AudioCanvas, a new benchmark of 3,177 real‑world videos that covers 300 single‑event classes and 501 multi‑event samples, curated with high alignment and structured CoT annotations by using Gemini 2.5 Pro from AudioSet."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Building on ThinkSound’s finding that CoT planning can help V2A, the paper proposes the efficient training framework of multi‑dimensional CoT and shows that further improves performance across semantic, temporal, aesthetic, and spatial axes—without sacrificing any one axis, according to both objective metrics and MOS on VGGSound and AudioCanvas dataset.\n- The paper includes comprehensive ablations (encoder choices, CoT structure, reward composition, Fast‑GRPO efficiency) on both objective and subjective evaluations, which supports the effectiveness of proposed framework."}, "weaknesses": {"value": "- Problem setup is still “CoT‑based V2A” in the ThinkSound lineage; the paper’s main conceptual step is decomposition from a monolithic to a multi‑dimensional CoT with reward alignment. This is a natural extension that is practically useful but arguably incremental. Note that  the reviewer acknowledges its effectiveness.\n\n- Although the reviewer acknowledges that the random window approach is effective for applying SDE steps to efficiently optimize flow matching, the significance of this contribution remains limited. In the context of diffusion models [1] and its fundamental equivalence with flow matching [2,3], the known relationship between the SDE and the PF-ODE [4] already makes it theoretically clear that one can switch between ODE and SDE steps during the sampling process. Consequently, Fast-GRPO demonstrates an empirical gain achieved through a heuristic technique rather than presenting a novel fundamental contribution.\n\n- Dataset construction details leave critical ambiguities.\n    - VGGSound test split / inference‑time CoTs: At inference, PrismAudio appears to require a structured CoT. It is unclear how these CoTs are obtained on VGGSound test videos—e.g., are they generated by the fine‑tuned VideoLLaMA2 from the test videos alone, or are they derived from the original tag-level captions? The Implementation section can be interpreted that the authors annotate VGGSound testsplit with CoTs using fine‑tuned VideoLLaMA2, but does not say whether \"sounding\" videos or \"silent\" videos are input to that model to get CoTs, which the reviewer the silent ones are used. If this is the case, is a gap observed between the training and inference setups? This is because during training, the model uses VideoLLaMA2 outputs obtained from \"sounding\" videos, whereas at inference, it would be using outputs obtained from \"silent\" videos, creating a gap.\n   - Regarding the constructing AudioCanvas dataset, are \"sounding\" videos used as the input or are \"silent\" videos fed into Gemini 2.5 Pro? If the CoTs are generated from \"sounding\" videos, what would PrismAudio's performance be when evaluated using CoTs generated from \"silent\" videos as input?\n\n[minor]\n- The paper seems to use single‑stimulus subjective evaluation. However, MUSHRA (multi‑stimulus) or pairwise preference tests (e.g., [5,6]) would provide more discriminative evidence if authors want to claim the effectiveness of proposed methods based on human evaluation.\n\n[1] Karras, Tero, et al. \"Analyzing and improving the training dynamics of diffusion models.\" CVPR 2024.\n\n[2] Lu, Cheng, et al. \"Simplifying, stabilizing and scaling continuous-time consistency models.\" ICLR 2025.\n\n[3] https://diffusionflow.github.io/\n\n[4] Song, Yang, et al. \"Score-based generative modeling through stochastic differential equations.\"  ICLR 2021.\n\n[5] Evans, Zach, et al. \"Fast Timing-Conditioned Latent Audio Diffusion.\" ICML 2024\n\n[6] Novak, Zachary, et al. \"Presto! Distilling Steps and Layers for Accelerating Music Generation.\" ICLR 2025"}, "questions": {"value": "- What are the exact prompts to Gemini 2.5 Pro to construct dataset for both VideoLLaMA2 and AudioCanvas?\n- Could you explain the training details of VideoLLaMA2?\n\nSee weaknesses as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UbMVmu3mLS", "forum": "cIfDKEbAky", "replyto": "cIfDKEbAky", "signatures": ["ICLR.cc/2026/Conference/Submission282/Reviewer_oFRj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission282/Reviewer_oFRj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447306038, "cdate": 1761447306038, "tmdate": 1762915484776, "mdate": 1762915484776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PrismAudio, a video-to-audio (V2A) generation framework that decomposes ThinkSound planning into four dimensions (semantic, temporal, aesthetic, and spatial reasoning).\nEach dimension is associated with a representative reward, and the model is trained via reinforcement learning to balance these objectives.\nThey also provide Fast-GRPO which utilizes both SDE and ODE at the same in order to enable the efficient optimization.\nWith a smaller number of parameters and shorter inference time, PrismAudio achieves state-of-the-art performance on both VGGSound (in-domain) and AudioCanvas (out-of-domain) benchmarks across all four evaluation dimensions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.\tThe proposed AudioCanvas Benchmark is valuable and provides high-quality, well-structured evaluation data.\n2.\tThe four decomposition dimensions (semantic, temporal, aesthetic, and spatial) are conceptually sound and well-motivated for the V2A task.\n3.\tExtensive experiments and ablation analyses comprehensively support the model’s effectiveness and robustness across different dimensions."}, "weaknesses": {"value": "1. The training pipeline is complex, involving multiple reward signals from different models, which may limit reproducibility and dependency problem.\n2. In Table 4, the ablation study evaluates single-dimensional rewards, but it remains unclear whether all four dimensions (semantic, temporal, aesthetic, spatial) are truly necessary. In many video-to-audio scenarios, only semantic and temporal aspects may suffice, while spatial or aesthetic factors could be redundant. Demonstrating or analyzing such reduced configurations could clarify whether the full four-dimensional setup is essential or unnecessarily complex."}, "questions": {"value": "Given that training the VAE requires 24 GPUs, have the authors considered excluding VAE fine-tuning and the spatial reward?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YV7Tl7UNlP", "forum": "cIfDKEbAky", "replyto": "cIfDKEbAky", "signatures": ["ICLR.cc/2026/Conference/Submission282/Reviewer_BAuq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission282/Reviewer_BAuq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881409947, "cdate": 1761881409947, "tmdate": 1762915484659, "mdate": 1762915484659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PrismAudio for video-to-audio generation. The core challenge is objective entanglement across four perceptual goals which are semantic consistency, temporal synchrony, aesthetic quality, and spatial accuracy. PrismAudio addresses this by decomposing  reasoning into four Chain-of-Thought (CoT) modules,  each paired with a targeted reward.  The base generator is a Diffusion Transformer with flow matching, conditioned on axis-specific reasoning text and post-trained with Group Relative Policy Optimization (GRPO) using group-normalized, axis-wise rewards. To reduce overhead, Fast-GRPO interleaves brief SDE windows into an otherwise ODE trajectory. This preserves the terminal distribution yet maintains exploration and reduces cost. The paper reports state-of-the-art results across all four axes on VGGSound (in-domain) and AudioCanvas (out-of-domain), with faster inference than prior SOTAs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper clearly defines the problem as objective entanglement and addresses it by decomposing reasoning into four Chain-of-Thought (CoT) axes with aligned, axis-wise rewards under reinforcement learning. Fast-GRPO limits stochastic exploration to a short, randomly placed window along the trajectory using a mixed ODE–SDE sampler, which enables closed-form per-step policy ratios and stable group-relative updates via weighted aggregation, within-group normalization, and ratio clipping, yielding near-linear GRPO training complexity (reducing policy-model NFE from T to w) without compromising reward computation. The authors also acknowledge reward hacking risks and add a KL penalty (λ=0.04) for stability. Empirically, the model achieves high audio quality and SOTA balance across semantic, temporal, aesthetic, and spatial axes on VGGSound and AudioCanvas, with faster inference and strong robustness. Audio generation results are released upon the public github. But there is no code."}, "weaknesses": {"value": "\tThe method explores with SDE only within a randomly placed window and uses ODE elsewhere. If that window misses moments where scene dynamics are most critical for sound perception, those segments may be under-learned. This raises questions about coverage and stability under the proposed sampling scheme, and whether Fast-GRPO still acquires the correct behavior when the window skips the most eventful frames.\n\n1. The paper argues that mixing ODE and SDE preserves the terminal distribution. As parameters are updated and numerical errors accrue during training, it would be helpful to show that this property and the resulting reward estimates remain stable.\n\n2. If the policy inside the window is modeled as a fixed-form Gaussian, it may be too rigid to capture anisotropic or structured uncertainty in complex latent spaces, potentially limiting performance.\n\n3. Although inference is lightweight, the training phase appears to require substantial compute and memory."}, "questions": {"value": "Following the weaknesses above, here are several questions for the authors:\n1. If the random SDE window skips the most eventful frames (e.g., sudden scene or sound changes), does Fast-GRPO still converge to the intended behavior? Do you report coverage statistics or guarantees over training?\n2. Under parameter updates during training, does mixed ODE–SDE sampling approximately preserve the terminal distribution? How sensitive are reward estimates and learning signals to accumulated numerical error or parameter drift?\n3. Is the in-window policy fixed as an isotropic Gaussian? If so, did you try diagonal/low-rank/mixture variants?\n4. For inference, what GPU resources do you require, specifically VRAM usage?\n5. On the VGGSound dataset, how many test videos did you evaluate (exact count for your test split)?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper explicitly discusses the following potential negative impacts about the training data and reward functions, especially the Aesthetic CoT module and its reward signals, may inadvertently encode cultural bias and representation issues regarding what counts as “good sound” or appropriate audio-visual relationships.\nHowever, given that the data were collected from publicly available datasets and the intended use is academic and research only, the authors contend that there are no material ethics concerns."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kkqyupcZPo", "forum": "cIfDKEbAky", "replyto": "cIfDKEbAky", "signatures": ["ICLR.cc/2026/Conference/Submission282/Reviewer_2VpK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission282/Reviewer_2VpK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912471508, "cdate": 1761912471508, "tmdate": 1762915484355, "mdate": 1762915484355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}