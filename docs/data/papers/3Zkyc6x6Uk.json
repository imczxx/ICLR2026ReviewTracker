{"id": "3Zkyc6x6Uk", "number": 16257, "cdate": 1758262412196, "mdate": 1763647592904, "content": {"title": "Sparse Canonical Correlation Analysis via Smooth Non-Convex $\\ell_{0}$ Surrogates and Iterative Minorization–Maximization", "abstract": "Canonical correlation analysis (CCA) is a core tool to uncover linear associations between two datasets. In high-dimensional settings, however,  it is prone to overfitting and lacks interpretability. Enforcing exact sparsity via $\\ell_0$ constraints can improve interpretability but leads to an intractable combinatorial problem. We propose a novel framework for sparse CCA that replaces the $\\ell_0$ cardinality constraint with tight smooth concave surrogates (power, logarithmic, and exponential forms), preserving support control without ad hoc thresholds. We solve the resulting nonconvex program via a minorization–maximization algorithm, yielding a generalized eigenvalue subproblem at each step. We prove that as the smoothing parameter vanishes, the surrogate formulation converges to the exact $\\ell_0$ solution with explicit suboptimality bounds. We further reformulate the objective as a rank-constrained semidefinite program and use randomized Gaussian rounding to extract sparse canonical directions. Empirical results on six benchmark datasets demonstrate that our method enforces exact sparsity levels, delivers superior canonical correlations and support recovery, and offers markedly improved scalability compared to state‐of‐the‐art SCCA algorithms.", "tldr": "", "keywords": ["canonical correlation analysis (CCA)", "$\\ell_0$ cardinality constraint", "smooth surrogate penalties"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13e2dc90546a7253115ea58f8b9c9ec55d83cb15.pdf", "supplementary_material": "/attachment/481ee729aca76bd782bcb5b3ccd65e40f2f3f9f6.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposed MM-SDP for sparse classical canonical correlation analysis (SCCA). Smooth concave surrogates are employed to replace the l0-cardinality constraint. The convergence of the method is also proved."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method achieves the best performance throughout all the 6 datasets compared to the baselines. The maximum canonical correlation is improved by 0.038 on average compared to the second-best method.\n- Taking into account both maximum canonical correlation and runtime, the proposed MM-SDP achieves the best performance."}, "weaknesses": {"value": "- The baselines and related works are limited. Two of the three adopted baselines are proposed over 8 years ago. Moreover, no related works are discussed.\n- The analysis of the experimental results is not thorough:\n    - On datasets other than Spambase and Gas, MM-SDP achieves a much better performance. However, the performance of MM-SDP is close to that of ADMM-based SCCA and Branch-and-Bound SCCA on Spambase and Gas, respectively. Considering that these two datasets are not distinguished in terms of variables or dimensions compared to other datasets, the analysis is recommended.\n    - The hyperparameters of MM-SDP when achieving the peak keep the same when datasets varies, no matter how the scales or dimensions changes, the analysis is recommended.\n- Although the work is claimed to address the limitations of classical CCA in high-dimensional regimes, it seems that this is not discussed in the paper."}, "questions": {"value": "- How is the sparsity level $s_1$, $s_2$ corresponded to the regularization parameters $\\rho_1$,$\\rho_2$? Is that fair when compared to Branch-and-bound SCCA?\n- How is the far more favorable scalability demonstrated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mDyhzC2YgR", "forum": "3Zkyc6x6Uk", "replyto": "3Zkyc6x6Uk", "signatures": ["ICLR.cc/2026/Conference/Submission16257/Reviewer_FXaa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16257/Reviewer_FXaa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728369214, "cdate": 1761728369214, "tmdate": 1762926410870, "mdate": 1762926410870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for sparse canonical correlation analysis (SCCA) that replaces the L0 cardinality constraint with smooth non-convex surrogates to enforce sparsity. It solves the resulting nonconvex problem using a minorization-maximization (MM) algorithm. The authors provide convergence analysis as the smoothing parameter approaches zero, reformulate the problem as a rank-constrained SDP with randomized rounding for sparse solutions, and evaluate on six UCI benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper attempts to address practical challenges in sparse CCA by introducing smooth surrogates, potentially improving numerical stability. The MM framework is clearly derived, and the SDP reformulation with Gaussian rounding is a reasonable way to handle the quadratically constrained subproblems. The empirical evaluation covers a range of datasets, providing some evidence of performance in real-world settings."}, "weaknesses": {"value": "1. Originality. The work lacks originality, as it primarily applies the well-established MM framework to sparse CCA. These surrogates and the MM approach are common in sparse optimization, and the paper does not introduce novel theoretical insights into their properties for CCA.\n\n2. Experiment. The experiments are insufficient: they lack synthetic experiments with known ground truth to validate the surrogates' effectiveness. The comparison to baselines is unfair, as the 300-second time limit on the branch-and-bound method (Li et al., 2024) is overly restrictive, where longer runtimes are often acceptable.\n\n3. Presentation issues include redundant citations (e.g., repeated references to the same work within a single sentence) and a convergence proof in Appendix C that lacks a formal theorem statement, reducing clarity."}, "questions": {"value": "1. Could the authors add synthetic experiments with ground-truth sparse canonical vectors to demonstrate the surrogates' advantages in support recovery and correlation estimation over exact l0 methods? This could strengthen the empirical claims.\n\n2. Why was a 300-second time limit chosen for the branch-and-bound baseline? Please justify if this reflects realistic use cases, or re-run with relaxed limits to ensure fair comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8QpM7WSuCt", "forum": "3Zkyc6x6Uk", "replyto": "3Zkyc6x6Uk", "signatures": ["ICLR.cc/2026/Conference/Submission16257/Reviewer_PVJa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16257/Reviewer_PVJa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790242824, "cdate": 1761790242824, "tmdate": 1762926410550, "mdate": 1762926410550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel and efficient optimization framework for Sparse Canonical Correlation Analysis (SCCA), aiming to solve the intractable combinatorial optimization problem caused by the exact $l_{0}$ constraint. The core idea is to replace the $l_{0}$ objective with smooth and concave non-convex surrogates to avoid singularities and the reliance on heuristic thresholds common in traditional iterative methods. The paper provides a theoretical proof of convergence and demonstrates in experiments that the method significantly outperforms existing $\\ell_{1}$ and exact $\\ell_{0}$ solvers in terms of canonical correlation and computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The introduction of $C^{1}$ smooth non-convex $\\ell_{0}$ surrogates is a key innovation. This not only smooths the optimization problem but also avoids the singularity issues common in IRLS, leading to a tighter and more stable approximation of the exact $\\ell_{0}$ constraint.\n2. The MM-SDP framework is elegantly designed, decomposing a complex non-convex problem into manageable convex subproblems. The paper provides convergence proofs for the MM algorithm and suboptimality bounds, ensuring strong theoretical rigor.\n3. Experimental results show that the method achieves high canonical correlation and accurate support recovery while being at least two orders of magnitude faster than exact $\\ell_{0}$ solvers (like Branch-and-Bound), demonstrating significant practical value."}, "weaknesses": {"value": "1. Lack of Large-Scale Real-World Data Testing: The validation mainly relies on benchmark datasets like UCI. There is a lack of performance and scalability validation on high-dimensional, real-world application scenarios with feature dimensions $p > 1000$ (e.g., genomics or large-scale image features). This makes the assessment of the method's robustness in extremely high dimensions inadequate.\n2. Insufficient Hyperparameter Sensitivity Analysis: The sensitivity analysis of the shape parameter $p$ and the smoothing parameter $\\varepsilon$ (which control the shape and smoothness of the $\\ell_{0}$ surrogate respectively) is insufficient. There is a lack of detailed experimental discussion on how different values of these parameters affect the final sparsity, correlation coefficient, and the convergence speed of the MM algorithm."}, "questions": {"value": "1. Please elaborate on the computational efficiency, robustness, and accuracy in recovering true sparse structures of the MM-SDP method on high-dimensional, large-scale datasets ($p \\gg 1000$). Specifically, how does the framework perform in terms of convergence speed compared to $\\ell_{1}$ methods, and what constitutes the main computational bottleneck?\n\n2. Please provide a sensitivity analysis for the shape parameter $p$ and the smoothing parameter $\\varepsilon$, quantifying their impact on the tightness of the $\\ell_{0}$ surrogate approximation and the MM algorithm's convergence speed/number of iterations. Furthermore, can more efficient heuristic methods for setting these parameters be provided, rather than relying solely on cross-validation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Txv3UAIj2U", "forum": "3Zkyc6x6Uk", "replyto": "3Zkyc6x6Uk", "signatures": ["ICLR.cc/2026/Conference/Submission16257/Reviewer_MmLu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16257/Reviewer_MmLu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806288610, "cdate": 1761806288610, "tmdate": 1762926410155, "mdate": 1762926410155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to solve the canonical correlation analysis (CCA) problem by (1) replacing the $\\ell_0$ constraint by smooth surrogates and (2) replace these surrogates by a quadratic tangent approximation at each step of the minimization. The resulting formulation can be seen as a quadratically constrained quadratic program which can be relaxed into a semidefinite program.  \nThe main result of the paper shows that the formulation based on the smooth surrogate functions can be made arbitrarily close to the original formulation. That is to say the gap between the based on the non-smooth surrogates and their smooth version can be made arbitrarily small."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is organized and relatively clear. There is some level of originality although a number of points have to be better discussed (see below)"}, "weaknesses": {"value": "- line 105, problem Equation 1 —> Problem 1?\n- line 128: quadratic minorizer —> why not use minimizer?\n\nSection 3.2.\n\n- Line 160 and then line 164. This is not clear. Do you add the epsilon at the level of the weights or at the level of the g_p function. You should clarify this. \n- Line 189, Obviously you can get arbitrarily close to the original penalty by reducing the value of epsilon. But the smoothing usually comes at the expense of the convergence (see my comment above)\n\nSection 3.3.\n- In the weight update rule, it would be convenient to recall Eq (3) or at least add a link to this equation\n- I’m not sure table 1 is really needed. I think your explanations are clear enough. Moreover this table occupies a large portion of the paper which could be used \n- Same comment for Figure 1\n\nSection 4.1. \n- You should merge table 2 and lines 288 - 303\nSection 4.2.\n- Algorithm 1 takes a lot of space and only repeats what was said in section 3.3.\n\nSection 4.3. \n- Table 2 is unclear/unnecessary. What do you mean by “view dimension” ? If what you want to indicate is that you can handle large datasets, then why not simply include a column size in table 3. Honestly to me table 3 is the most meaningful.\n- Table 3. To be fully fair, I think you should highlight the best row of each column (it does not make sense to only highlight the column in which your algorithm gives the best performance). I.e. you don’t have to do it but I think it would make more sense. \n- lines 466-468 “These complexity considerations and empirical timings together underscore that …strikes the best balance..”. Given table 3, I don’t think this is fair. You can say that you achieve the best correlation but you clearly do not always achieve the best runtime. \nAppendix\n- In the proof of Theorem 2, line 712, should there be a factor 2 in front of the (\\rho_1n + \\rho_2 m)?\n- If I’m not mistaken, all of the surrogates on lines 109-112 are defined with respect to the absolute value of x so that from my understanding g_p(x) = g_p(|x|).. but sometimes you use the notation g_p(x) and sometimes the notation g_p(|x|). I would choose a single notation as this introduces unnecessary confusion (an example of this is online 669).\n- line 720-721: what you are saying the tangent at epsilon is always larger than the slope of the segment that connects the origin and epsilon on the graph of f. I would also recall the fact that g_p(0) = 0 \n\nProof of convergence\n\n- To me there are several ambiguities that need to be clarified. First\n- lines 758 - 761, the notation q((x, y)|(x^t, y^t)) is ambiguous. I’m wondering if it would not be possible to find something better (although I understand the connection between w^t, z^t and (x^t, y^t))\n- The only thing line 801 is saying is that at the limit t—> infinity your solution (which you get after an infinite number of iterations) maximizes a particular surrogate function \n- Lines 805 - 807: I don’t see why you can say that in the limit t—> infinity the surrogate function and the objective value are equal…\n- line 811-812 : “It is straightforward to check that the gradients of the surrogate and the objective function are identical ..” —> this is not clear to me\n- lines 821 - 827 are not clear. part of the reasoning seems missing here. You seem to suggest that because the value of the objective at the limit point is optimal, there is only one limit point .. I don’t see this. Where do you show that all limit points have distinct objective values?"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XJBBZutSKN", "forum": "3Zkyc6x6Uk", "replyto": "3Zkyc6x6Uk", "signatures": ["ICLR.cc/2026/Conference/Submission16257/Reviewer_KNsy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16257/Reviewer_KNsy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762366643872, "cdate": 1762366643872, "tmdate": 1762926409054, "mdate": 1762926409054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}