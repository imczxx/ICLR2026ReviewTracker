{"id": "8IWDK1I6j0", "number": 18088, "cdate": 1758283701721, "mdate": 1763446216480, "content": {"title": "Reinforcement-Guided Subtask Decomposition for Unified Vision-Language Learning", "abstract": "Building truly versatile Vision-Language Models requires more than just scaling up training data and model size. While current models achieve impressive performance on their training tasks, they often fail when deployed on problems that require the same underlying skills but in different combinations—a limitation that hinders their adoption as general-purpose AI systems.\nThis paper introduces a novel reinforcement learning framework that addresses this limitation by teaching a VLM to solve problems by composing a learned sequence of reusable, verifiable subtasks. \nOur key innovation is a reward function that guides the model to generate structured reasoning chains of these primitive subtasks through format-based verification, eliminating the need for detailed annotations of intermediate reasoning steps. \nThis format-based reward provides a dense learning signal, enabling the model to master a flexible, procedural approach to problem-solving. \nFurthermore, these models can be flexibly transferred to spatial VQA tasks, demonstrating strong performance without any fine-tuning.\nThis comprehensive cross-task transfer outperforms both standard supervised fine-tuned and reinforcement fine-tuned visual chain-of-thought baselines, while maintaining computational efficiency with only a 3B parameter model. \nOur findings show that learning to compose a sequence of fundamental vision skills is an effective and scalable strategy for building robust, general-purpose VLMs than learning monolithic, task-specific solutions.", "tldr": "", "keywords": ["VLM", "Reinforcement-Learning", "Generalization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef773b64d5c88d5b526b383049bbb0bb5958ef08.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present a reinforcement learning framework to improve the cross-task generalization of Vision-Language Models (VLMs). The core idea is to train models to decompose problems into a sequence of verifiable subtasks. A key contribution is a composite reward function that provides a dense, format-based signal for reasoning steps, alleviating the need for step-by-step annotations. Experiments show the method achieves strong zero-shot performance and high efficiency using a 3B model trained on only 1k samples"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Promising Approach: The paper addresses the critical problem of generalization in VLMs. The proposed compositional learning approach is a promising departure from standard fine-tuning and a valuable direction for building more robust and general models.\n\n2) Practical Reward Design: The verifiable reward function is a clever solution to the sparse reward problem in multi-step reasoning. By using automated format verification, the framework provides dense guidance without expensive process supervision, making it highly practical.\n\n3) Efficiency and Empirical Results: The method demonstrates a clear advantage in zero-shot generalization. Achieving this with a 3B model on only 1k samples highlights its remarkable data and computational efficiency, outperforming larger, specialized models."}, "weaknesses": {"value": "1) The paper claims to learn \"compositional, reusable fundamental skills,\" but evaluation is limited to dense vision benchmarks. Given the general nature of the subtask library, **it is unclear why more comprehensive benchmarks like MMStar or MMBench were not included.** Additionally, the reliance on a manually crafted subtask library may limit the model's ability to scale to tasks requiring entirely new atomic capabilities, potentially posing a limitation compared to other RL-related paradigms.\n\n2) The reward function focuses on **the syntactic correctness of subtask outputs**, not their semantic factuality. This design could allow the model to learn incorrect reasoning paths that coincidentally produce a correct final answer, which might negatively impact generalization. **Why not incorporate intermediate outcome verification?** Many existing methods have provided dense, process-based rewards [1-2].\n\n3) The paper lacks **scaling experiments for the proposed framework.** It is unclear why the 1k data sample size was chosen and how performance would change with more data (e.g., 10k samples, still far less than SFT). Similarly, how would the framework perform on a larger base model? A more capable model might better address the concerns in Weakness #2. This analysis is crucial for understanding the framework's full potential and robustness.\n\n---\n\n[1] Free Process Rewards without Process Labels\n\n[2] Stepwise guided policy optimization:Coloring your incorrect reasoning in grpo."}, "questions": {"value": "Please see the weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6KOkqObX7R", "forum": "8IWDK1I6j0", "replyto": "8IWDK1I6j0", "signatures": ["ICLR.cc/2026/Conference/Submission18088/Reviewer_8Q6b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18088/Reviewer_8Q6b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706482439, "cdate": 1761706482439, "tmdate": 1762927863076, "mdate": 1762927863076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a reinforcement learning framework aimed at improving cross-task generalization in vision-language models (VLMs). The authors argue that simply scaling up model size and data does not yield truly general-purpose models. Instead, they propose teaching models to solve tasks by composing reusable and verifiable subtasks, thereby enabling them to adapt to new problems requiring similar visual-linguistic skills in different combinations. The key innovation lies in a dense, format-based reward function that allows the model to learn structured reasoning chains of subtasks without explicit supervision of intermediate reasoning steps. This reward guides the model to produce valid, interpretable decompositions while also achieving accurate final results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The article is well-structured and fluently written, enabling readers to easily comprehend its main points. Furthermore, the principle of “recomposing the same primitive subtasks in different sequences” is both coherent and insightful."}, "weaknesses": {"value": "The methodology appears to be quite intuitive and straightforward. Consequently, for such work, the critical evaluation lies in empirically verifying its effectiveness.\n\nRegarding the experimental setup, I would like to confirm that you employed a Zero RL approach, meaning no task-specific SFT was conducted before the reinforcement learning phase. The reward curves suggest that the model rapidly satisfied the format reward (presumably set to a value of 1, a detail I could not locate in the manuscript), while the accuracy rewards for other subtasks improved more gradually.\n\nThis observation raises a question about the necessity of complex reasoning for the proposed subtasks. Based on the visualization examples provided, these tasks seem to demand more general visual knowledge than the intricate reasoning required in domains like mathematics or formal logic.\n\nThis leads me to hypothesize that the primary performance gains may stem from the synergistic effects between the various visual tasks, rather than being a direct result of the RL process itself. I would welcome your perspective on this interpretation, as I am keen to understand if I have misinterpreted any aspect of your work."}, "questions": {"value": "Since the author mentioned in the abstract that the 3B parameter is one of their advantages, I would like to know if there are more comparison with some larger models (such as Qwen2.5-VL-32B). How does their performance compare? \n\nThe comparison with the baseline shows that the method is beneficial (which is usually a necessity for most work), and it also has advantages when compared with other methods (SFT, RL) and expert models. Therefore, I would like to know the comparison of the 3B model proposed in this paper with larger models. I have seen the comparison with Qwen2.5-VL-7B, does it under the setting of training-free and zero-shot? How much benefit would SFT bring directly to this size?\n\nI was wondering if you have plans to (or already) release the model or its training data. The reason is that for the community, having access to the model and data is often essential for a deeper assessment of the work’s merits.\n\nI will adjust my opinion on this article based on the author's response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SP7Rx3Tbdl", "forum": "8IWDK1I6j0", "replyto": "8IWDK1I6j0", "signatures": ["ICLR.cc/2026/Conference/Submission18088/Reviewer_uMKS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18088/Reviewer_uMKS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154335312, "cdate": 1762154335312, "tmdate": 1762927862565, "mdate": 1762927862565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for enabling vision-language models (VLMs) to generalize across structurally different tasks by learning to decompose a query into a chain of verifiable subtasks and then execute those subtasks to produce an answer. Instead of requiring annotated reasoning chains, the model is trained via a reinforcement-learning method (Group Relative Policy Optimization, GRPO) and a novel reward that combines: (i) format-based verification of generated subtasks and (ii) final task-accuracy. It uses a predefined library of primitive subtasks (object detection, segmentation, attribute recognition, counting, captioning etc) each with a strict JSON output format. After training on one or two tasks (e.g., segmentation task RefCOCOg or counting task PixMo-Point), the resulting 3B-parameter model is evaluated zero-shot on multiple other tasks (ReasonSeg, CountBench, ReasonCount, EgoOrientBench, V*Bench) and shows superior cross-task generalization compared to supervised fine-tuning (SFT) or unstructured reasoning (Visual Chain-of-Thought + RL). The central claim is that composition of reusable subtasks is a more efficient and generalizable path to broad vision-language understanding than monolithic task-specific fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel reward design enabling dense feedback for reasoning chains. : The paper’s reward r = r_reasoning + r_subtask·r_answer combines structural verification of intermediate subtask formats and final performance. This allows training without annotated reasoning chains yet still learning structured decomposition. \n\nPredefined subtask library with explicit JSON schema for auto-verification : Table 1 lists subtasks (query decomposition, object detection, pixel segmentation, attribute recognition, captioning, relation reasoning, object counting, verification, summary) each with strict output format, enabling automatic verification and hence dense feedback. \n\nStrong cross-task generalization with minimal supervised data.: The method trains on only ~1,000 samples from RefCOCOg or PixMo-Point, yet shows improvements in both in-domain and many unseen tasks (segmentation, counting, spatial VQA) compared to baselines. Figures 2 and 3 show the model’s generated subtask sequences (e.g., object_detection → object_counting → captioning → verification) and provide insight into how the learned policy composes primitives rather than memorizing end-to-end. \n\nClear ablation studies of reward components: Table 3 and Table 4 isolate the effect of the exploration bonus and repetition penalty in r_subtask, showing their importance (e.g., exploration bonus + repetition penalty gives best GIoU of 59.5)."}, "weaknesses": {"value": "Predefined subtask library limits flexibility and may bake in bias.: The system relies on a manually defined list of subtasks and strict schemas (Table 1). While this allows verification, it may constrain the model to the authors’ chosen primitives and limit adaptation to tasks requiring new primitives. \n\nDependence on fairly strong pretrained modules / frozen decoders.: For segmentation tasks, the paper uses a frozen SAM2-Large decoder, and the base policy is Qwen2.5-VL-3B. This means that the “learning” is constrained by the underlying models and may hide where improvement comes from (i.e., large backbone rather than subtask decomposition). This complicates isolation of the contribution. \n\nLimited evaluation on truly novel tasks outside the subtask library’s design; risk of overfitting to the library.: While the cross-task tasks are structurally different, they still map well to the chosen primitive set (counting, segmentation, spatial VQA). It would be stronger to evaluate on tasks requiring new subtask types (e.g., reasoning about temporal sequences, action recognition) to test how well the model generalizes beyond library coverage.\n\nCompute and training cost / practicality not deeply discussed.: The paper uses GRPO with group sampling (G outputs per input) and fine-tunes a 3B-parameter model with RL. The samples per input, number of updates, and wall-clock cost are only briefly described (batch size 16, 8 samples). It would help to see a compute-vs-gain trade-off, especially since RL on large models is expensive."}, "questions": {"value": "How do you select the subtask library? What criteria determine which primitives are included, and how sensitive is performance to this choice?\n\nWhat happens when a downstream task requires a subtask not in the library (e.g., action-recognition, temporal reasoning)? Does the model degrade gracefully?\n\nCan you provide statistics on reasoning chain lengths and which subtasks are most frequently used/mis-used?\n\nHow sensitive is performance to hyperparameters α (exploration bonus) and β (repetition penalty)? You show some ablation but can you comment on general tuning strategy?\nHave you observed failure cases where the model generates many useless subtasks (long chain) but still answers incorrectly? How do you detect/handle those?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ttRPgkBPP9", "forum": "8IWDK1I6j0", "replyto": "8IWDK1I6j0", "signatures": ["ICLR.cc/2026/Conference/Submission18088/Reviewer_ixam"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18088/Reviewer_ixam"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762381559817, "cdate": 1762381559817, "tmdate": 1762927862220, "mdate": 1762927862220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a reinforcement learning (RL) framework for vision-language models (VLMs) in which, instead of producing free-text chains of thought (CoT), the model composes a sequence of predefined, reusable primitive subtasks. The key hypothesis is that enforcing this compositional prior within the CoT enables better cross-task generalization -- i.e., a model trained on one task can adapt to new tasks by reusing learned primitives.\n\nThe method is trained using GRPO with a composite reward comprising reasoning-format, subtask-format, and answer-accuracy components. Experiments are conducted by training on one of two tasks—segmentation (RefCOCOg) or counting-by-localization (PixMo-Point)—and evaluating across segmentation (ReasonSeg), counting (CountBench and the proposed ReasonCount), and spatial VQA (EgoOrientBench, V*Bench).\n\nBaselines include (i) a zero-shot VLM, (ii) supervised fine-tuning (SFT) on correct final answers, and (iii) free-text visual CoT with RL on the same data. Results show that the proposed method achieves superior cross-task performance and demonstrates effective reuse of primitive skills across higher-level tasks. The paper also provides ablation studies and qualitative analyses to support these findings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method to enable the model to learn to compose and reuse primitive visual operations (such as object detection, attribute recognition, etc.) through RL is adequately motivated. It is conceptually aligned to past works such as Neural Module Networks (non-RL) and ViperGPT (zero-shot only), but explores this in a CoT RL formulation.\n\n2. The experiments with Qwen2.5-VL-3B model show better cross-task results compared to traditional free-text CoT-RL tuned model and SFT baseline. Further ablations show impact of introducing exploration bonus and repetition penalty in addition to traditional format and final-task reward. \n\n3. Paper is largely well written with primary experiment details and results clearly described. Additional analysis on subtask validity and qualitative analysis is also informative."}, "weaknesses": {"value": "1.  **Evaluation on multi-task training setting not reported**:  The experiments currently only evaluate cross-task performance, where the model is trained on individual task data (e.g., segmentation) and tested across tasks. In a more realistic scenario, training data for multiple tasks is available. The authors should thus also report performance for both the baselines and their method under **multi-task training** (e.g., jointly training on counting and segmentation). Without this result, it remains unclear whether, in a multi-task more realistic training scenario, standard free-text CoT RL tuning might already achieve comparable cross-task performance without requiring predefined primitive operations.\n\n2. **Evaluation on compositional multi-step reasoning datasets**: Related to weakness 1, authors could consider reporting results on compositional multi-step VQA reasoning datasets such as GQA [1], where sequential application and reuse of operations are more likely to be beneficial and not trivially learned through standard free-text CoTs even in a multi-task training scenario.\n\n\n3. **Model scale dependence**:  Results are only reported on the Qwen-2.5-VL-3B model. Resources permitting, it would strengthen the paper to show whether the findings hold for a larger model (e.g., Qwen-2.5-VL-7B). Larger models might exhibit more expressive and effective free-text CoTs through standard RL finetuning, and thereby potentially have less improvements from predefined primitive operations.\n\n4. **Sample efficiency comparison not analyzed**:  A potential benefit of composing predefined primitive operations is improved sample efficiency (i.e. less training data might be required as primitive operations can be reused and learned across samples). The authors could report performance with varying proportions of training data (e.g., 10%, 25%, 50%, 100%) to highlight this potential advantage and more strongly motivate benefits of their approach since RL training data may not be easily available across tasks. \n\n\nRelatively minor:\n\n5. **Prompting strategy for free-text CoT model:** Authors can also have a baseline where model is just prompted to consider performing low-level relevant tasks such as object detection, attribute recognition, etc in it's CoT instead of a specific predefined format constrained to predefined operations. \n\n6. **Claim of denser-reward:** The composite RL reward function introduces additional modulation terms such as an exploration bonus and a repetition penalty. While these are useful and effectively modulate the final task reward (as shown in the ablations), it is inaccurate to describe them as making the reward function denser. A denser reward is typically understood as a process-level reward that provides intermediate correctness feedback for subtasks in addition to the final task reward. This is not the case here, and the authors should consider rephrasing this claim.\n\n[1] Hudson, Drew A., and Christopher D. Manning. \"Gqa: A new dataset for real-world visual reasoning and compositional question answering.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019."}, "questions": {"value": "Please see weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lAEbUmgRA5", "forum": "8IWDK1I6j0", "replyto": "8IWDK1I6j0", "signatures": ["ICLR.cc/2026/Conference/Submission18088/Reviewer_L3p4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18088/Reviewer_L3p4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762478042417, "cdate": 1762478042417, "tmdate": 1762927861911, "mdate": 1762927861911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}