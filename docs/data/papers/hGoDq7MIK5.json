{"id": "hGoDq7MIK5", "number": 19405, "cdate": 1758295950809, "mdate": 1763716238851, "content": {"title": "On the Effect of Positional Encoding for In-context Learning in Transformers", "abstract": "Transformer models have demonstrated a remarkable ability to perform a wide range of tasks through in-context learning (ICL), where the model infers patterns from a small number of example prompts provided during inference. However, empirical studies have shown that the effectiveness of ICL can be significantly influenced by the order in which these prompts are presented. Despite its significance, this phenomenon has been largely unexplored from a theoretical perspective. In this paper, we theoretically investigate how positional encoding (PE) affects the ICL capabilities of Transformer models, particularly in tasks where prompt order plays a crucial role. We examine two distinct cases: linear regression, which represents an order-equivariant task, and dynamic systems, a classic time-series task that is inherently sensitive to the order of input prompts. Theoretically, we evaluated the change in the model output when positional encoding (PE) is incorporated and the prompt order is altered. We proved that the magnitude of this change follows a convergence rate of $\\mathcal{O}(k/N)$, where $k$ is the degree of permutation to the original prompt and $N$ is the number of in-context examples. Furthermore, for dynamical systems, we demonstrated that PE enables the Transformer to perform approximate gradient descent (GD) on permuted prompts, thereby ensuring robustness to changes in prompt order. These theoretical findings are experimentally validated.", "tldr": "", "keywords": ["Transformer Theory", "In-context Learning", "Positional Encoding"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f95f244dd69c984207efe8bd8b6a0b8d785ac67c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a theoretical analysis of how positional encoding (PE) affects the in-context learning (ICL) capability of transformers. It derives sufficient conditions on the transformer’s weight matrices that guarantee permutation invariance. For both linear regression and dynamical system tasks, it also proves that the error scales as $O(k/N)$ when $N$ is sufficiently large, demonstrating robustness of Transformers to prompt perturbations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper derives sufficient conditions on the weight matrices that ensure permutation invariance in transformers with PE. Moreover, it quantitatively provides error bounds of  $O(k/N)$ for both linear regression and dynamical system task. Overall, this paper makes a valuable contribution, showing that Transformers remain robust to the permutation."}, "weaknesses": {"value": "While this paper is theoretically strong and sufficiently novel, it is currently limited to linear tasks and considers only absolute positional encoding.\nHowever, I think that these limitations are acceptable for the scope of this paper. Out of interest, I would like to ask the questions below."}, "questions": {"value": "1. In the nonlinear case (e.g., logistic regression), would a similar quantitative analysis of output error bound be possible ? Although the error bound may differ from $O(k/N)$, could we still expect the bound decreasing when $N$ is large?\n\n2. For other types of PE (e.g., Relative PE, Rotary PE), do you expect similar theoretical interpretations to hold, or would the underlying analysis have to be fundamentally different?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "T9InrTFBgb", "forum": "hGoDq7MIK5", "replyto": "hGoDq7MIK5", "signatures": ["ICLR.cc/2026/Conference/Submission19405/Reviewer_m21Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19405/Reviewer_m21Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761114140186, "cdate": 1761114140186, "tmdate": 1762931324327, "mdate": 1762931324327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of our paper revision"}, "comment": {"value": "We have introduced Section 3.2.2, which presents a new theorem (Theorem 3.2) concerning Rotary Positional Encoding (RoPE). Unlike our previous results, this theorem does not rely on Assumption 3.1. Our analysis reveals that the expected prediction perturbation under RoPE scales as $C_{\\text{RoPE}} \\cdot (k d^3 / N)$, where the constant $C_{\\text{RoPE}}$ incorporates the maximum frequency parameter $\\max_i \\theta_i$ of the RoPE rotation matrix. To validate this theoretical finding, we have added corresponding experimental results in Figure 2. Additionally, we have included a brief preliminary overview of RoPE and removed the original Figure 1 to improve the paper’s focus and clarity."}}, "id": "uvpNrQymNF", "forum": "hGoDq7MIK5", "replyto": "hGoDq7MIK5", "signatures": ["ICLR.cc/2026/Conference/Submission19405/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19405/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19405/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763716373130, "cdate": 1763716373130, "tmdate": 1763716373130, "mdate": 1763716373130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is my second review of this manuscript. Compared to the previous version, there are no substantial changes in this revision.\n\nThis paper mainly investigates how positional encoding and different permutations of in-context examples affect the model’s output in in-context learning tasks. The authors provide theoretical results showing that positional encoding can enhance robustness to perturbations in the prompt order, and they further validate these findings with experiments on linear regression and dynamical system tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a mathematical quantification of the relationship between positional encoding and input order perturbations, which is novel.\n\n2. It designs two empirical tasks — one order-dependent and one order-independent — to validate the theory, demonstrating a well-reasoned experimental setup."}, "weaknesses": {"value": "1. Multiple experimental runs were not conducted, and the chosen tasks are relatively simple.\n\n2. The positional encoding used in this paper appears to impose certain constraints on the input length of the model."}, "questions": {"value": "1. I noticed that in the equation on page 4, line 202, the authors omitted the higher-order terms of $B$. In this case, is it still rigorous to use the equality sign?\n2. I have some questions regarding the one-hot positional encoding used by the authors, and please correct me if I have misunderstood. If the hidden state dimension $D$ is fixed, since the positional encoding satisfies $p_i \\in \\mathbb{R}^N$, this implies that the number or length of in-context examples cannot exceed $D$ — more precisely, it cannot exceed $D - d - 2$. In other words, the number of in-context examples (or context length) is constrained by the dimension of the hidden state, which should generally not be the case. We would consider raising the score if the authors could provide further clarification on this issue.\n3. I suggest that the authors perform multiple experiments on the same synthetic dataset with different model initializations to improve the robustness of the results. It would also strengthen the paper if more complex (preferably nonlinear) models and tasks were included in the experiments to enhance persuasiveness and generality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TSzxJsm8cb", "forum": "hGoDq7MIK5", "replyto": "hGoDq7MIK5", "signatures": ["ICLR.cc/2026/Conference/Submission19405/Reviewer_GKie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19405/Reviewer_GKie"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925344345, "cdate": 1761925344345, "tmdate": 1762931323689, "mdate": 1762931323689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically investigates how positional encoding (PE) affects in-context learning in Transformers when the ordering in the prompt changes. The authors use one-hot positional encoding and prove that prediction changes as O(k/N) where k is the permutation degree and N is the number of examples, for both linear regression and first order difference equations (differential?). They show that under specific weight matrix conditions, Transformers can maintain permutation invariance despite PE. Experiments on synthetic tasks validate the O(k/N) scaling relationship. The authors claim PE enables robust ICL on order-sensitive tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "S1: The paper provides clean theoretical bounds (Theorems 3.1, 3.2) with explicit convergence rates for changes in the prompt order dependence."}, "weaknesses": {"value": "W1: The impact of the core contribution is unclear. The fast that positional encoding affects output when prompt order changes is quite an expected result. The \"mystery\" that transformers show order sensitivity despite architectural permutation invariance is immediately resolved by noting PE exists.\n\nW2: Mostly theoretical contributions. It is hard to extend these theoretical contributions to actionable insights. In fact, it isn't even clear if these theorems help growing intuition for how ICL should behave in real language models. Moreover, the theory doesn't seem to predict new phenomena.\n\nW2: One-hot PE is completely disconnected from reality. As far as I know, no modern LLMs, or even transformers trained for specific purposes. uses one-hot positional encoding. How these results would impact RoPE embeddings should have been discussed.\n\nW3: Existence proofs are hardly surprising. Proposition 1 and Condition 3.1 show that \"there exist\" Transformers satisfying certain properties. But it is hard to really understand the impact of such statement. In reality, a vast amount of computation can be expressed from a Transformer, and the existence of such a weight isn't surprising, nor belief changing.\n\nW4: The \"dynamical systems\" task is ill presented or explained.\n\nW5: There are no validation or even qualitative checks on real language models or other transformers trained on real data."}, "questions": {"value": "Q1: How do these results extend to RoPE or any PE scheme actually used in modern LLMs?\nQ2: Are there any results on real-data trained Transformers which even qualitatively justify the main finding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Le7AhDvaKY", "forum": "hGoDq7MIK5", "replyto": "hGoDq7MIK5", "signatures": ["ICLR.cc/2026/Conference/Submission19405/Reviewer_yX6J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19405/Reviewer_yX6J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958306587, "cdate": 1761958306587, "tmdate": 1762931323142, "mdate": 1762931323142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically studies how positional encoding (PE) affects the in-context learning (ICL) capabilities of Transformers. The authors consider two representative tasks—linear regression (order-invariant) and first-order dynamical systems (order-sensitive)—and establish formal results showing that (1) PE introduces bounded deviations in output under prompt permutation, with errors scaling as O(k/N); and (2) for dynamical systems, PE enables approximate gradient descent behavior, providing robustness to order changes. Their experiments confirm the theoretical claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper studys an important yet underexplored aspect of ICL—how prompt order and positional encoding interact and provide interesting theoretical results.\n2. This paper is clearly written with well-motivated sectioning."}, "weaknesses": {"value": "1. The main theoretical results are mostly constructive in nature (e.g., there exists a Transformer). However, it remains questionable whether real-world Transformers actually behave according to these theoretically constructed results.\n\n2. The theoretical results are not particularly surprising — constructing specific Transformers to demonstrate how positional encoding improves performance on time-series tasks is rather expected.\n\n3. Experiments focus on synthetic data. Adding natural sequential tasks or ablations with alternative PE types (e.g., RoPE, ALiBi) would strengthen the contribution."}, "questions": {"value": "1. Could the authors provide more experiments on natural sequential tasks or include ablations with alternative positional encoding types (e.g., RoPE, ALiBi)?\n\n2. Could the authors provide some theoretical results that go beyond the constructive setting and better reflect realistic scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bRjOXZesrE", "forum": "hGoDq7MIK5", "replyto": "hGoDq7MIK5", "signatures": ["ICLR.cc/2026/Conference/Submission19405/Reviewer_K6Rr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19405/Reviewer_K6Rr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962178090, "cdate": 1761962178090, "tmdate": 1762931322737, "mdate": 1762931322737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes how positional encodings (PEs) shape in-context learning (ICL) when the order of exemplars is perturbed: under an idealized attention model with explicit absolute positions, it proves output sensitivity to a (k)-degree permutation shrinks roughly as (O(k/N)) with the number of shots (N), gives a sufficient condition under which attention remains effectively permutation-invariant, and provides a constructive mechanism showing that PEs can implement approximate gradient-descent–style updates on simple sequential tasks; the trends are validated on synthetic linear regression and first-order dynamical systems, and the results motivate practical prompt/evaluation hygiene (fix or average exemplar order, use more shots, small permutation ensembles), though external validity for modern softmax attention and popular PEs (RoPE/ALiBi/learned APE) is not established."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*   A novel analysis that formalizes bounds linking order sensitivity in ICL to permutation degree (*k*) and the number of shots (*N*).\n*   A constructive mechanism (approximate gradient descent) that links explicit position signals to robustness on order-sensitive sequence tasks.\n*   Clean synthetic experiments whose trends match the theory, yielding actionable recommendations for prompt and evaluation hygiene (e.g., fixing/averaging order, adding shots, light ensembling over orders).\n*   The paper is well-presented, clearly articulating when positions break permutation invariance in ways that are beneficial for ICL behavior."}, "weaknesses": {"value": "*   The analysis targets an idealized setting (concatenated one-hot absolute PE + linearized attention), so its applicability to the more common transformer setting of softmax attention with RoPE, ALiBi, or learned PEs remains unclear.\n*   Generality to complex, real-world data is unproven, with no tests on pretrained LLMs or on heavy-tailed/multimodal inputs.\n*   The sufficient conditions (e.g., for permutation-invariance; matrix closeness assumptions) seem strong and are not shown to emerge during standard pretraining.\n*   Experiments are small-scale and synthetic; there is no ablation on modern architectures or reporting of the constants/prefactors that would govern real-world effect sizes."}, "questions": {"value": "*   Do the *O(k/N)* trends and robustness claims hold for non-Gaussian, heavy-tailed, or multimodal real-world datasets, and can they be observed in small open-source LLMs with standard PEs?\n*   Can the theory (or new bounds) be extended to sinusoidal/learned APE, RoPE, and ALiBi under softmax attention? If not, where exactly does the analysis break, preventing it from guiding our understanding of real ICL?\n*   Would it be possible to empirically measure how closely pretrained models satisfy your sufficient conditions? Could these conditions be relaxed or reinterpreted to make them more verifiable in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Su8le5juia", "forum": "hGoDq7MIK5", "replyto": "hGoDq7MIK5", "signatures": ["ICLR.cc/2026/Conference/Submission19405/Reviewer_DDwu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19405/Reviewer_DDwu"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988720293, "cdate": 1761988720293, "tmdate": 1762931322299, "mdate": 1762931322299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}