{"id": "SwWxnZvgF4", "number": 15882, "cdate": 1758256491692, "mdate": 1763713044256, "content": {"title": "Reinforcement learning for saddle-point equilibria without full state exploration", "abstract": "We introduce a new fixed-point condition on the state-action-value $Q$-function for zero-sum Markov turn games that suffices to construct saddle-point and security policies, but is less restrictive than the classical condition arising from the Bellman equation. We then propose an iterative algorithm that guarantees convergence to a function satisfying this less restrictive condition. The key benefit of the new condition and algorithm is that convergence to a saddle-point can (and typically will) be reached without full exploration of the state-space; generally enabling the solution of larger games with less computation. Our algorithm is based on a limited form of exploration that gathers samples from repeated attempts to certify the current candidate policies as a saddle-point, motivating the terminology \"saddle-point exploration\" (SPE).  We illustrate the use of the new condition/algorithms in several combinatorial games that can be scaled in terms of the size of the state and action spaces. Numerical results, using both tabular and neural network $Q$-function representations, consistently show that saddle-point policies can be formally certified without full state exploration and, for several games, we can see that the fraction of states explored decreases as the size of the game grows.", "tldr": "We propose an algorithm that can find and certify saddle-point policies for zero-sum turn games without fully exploring the state space, with design and analysis built upon a novel fixed-point condition on the Q-function.", "keywords": ["reinforcement learning", "zero-sum games", "Q-learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/34c56cf6ff57a2eeb5ea6ba54b4f573785a19fd7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies finding saddle-point equilibria in a zero-sum Markov Game. The paper shows that exact optimality can be achieved without global convergence of the Q-function—it suffices to converge locally on the reachable subspace where the policies interact strategically. Based on this new finding, the authors propose SPE. For deterministic finite games, the algorithm provably converges to a restricted fixed point and terminates in finite time (Theorem 3)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors propose a new fixed-point condition for the state–action value function. Unlike the standard Bellman equation (which must hold for all states), this condition only needs to hold on a restricted subset of the state space — specifically, the states reachable under the current candidate policies.\n\n2. The authors propose the SPE algorithm and prove the convergence to a restricted fixed point and termination in finite time for deterministic finite games.\n\n3. In all experiments, SPE reached a certified saddle-point without exploring the entire state space. As game size increased, the fraction of explored states decreased, showing strong scalability."}, "weaknesses": {"value": "1. The main convergence theorem (Theorem 3) only holds for deterministic zero-sum turn-based games with finite termination. However, in many real-world applications, the game is stochastic. \n\n2. Although the authors claim their algorithm does not need to search the whole state space, the paper does not derive theoretical sample complexity or convergence rates, showing their method is better than previous methods. \n\n3. In the experiments, all test domains are turn-based, finite board games. Moreover, the experiments benchmark against a hypothetical lower bound rather than comparing to concrete algorithms"}, "questions": {"value": "Please see the weakness part for my concern."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y1wSMSHD9j", "forum": "SwWxnZvgF4", "replyto": "SwWxnZvgF4", "signatures": ["ICLR.cc/2026/Conference/Submission15882/Reviewer_PmtW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15882/Reviewer_PmtW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760940963429, "cdate": 1760940963429, "tmdate": 1762926099605, "mdate": 1762926099605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies Q-learning in turn-based two-player Markov zero-sum games. The authors introduce the notion of restricted fixed points for an operator on the Q-function and show that a saddle point can be derived from such a fixed point. Furthermore, an advantage of the proposed algorithm is that it only explores a subset of the entire state space, leading to improved efficiency compared to previous Q-learning algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper characterizes sufficient conditions under which the Q-learning operator is contractive, thereby establishing sufficient conditions to compute such restricted fixed point.\n- The computation of those restricted fixed points only requires visiting a subset of the entire state space.\n- Numerical experiments are provided to validate the theoretical results."}, "weaknesses": {"value": "- The paper focuses exclusively on deterministic zero-sum Markov games, where given the state and action, the next state is deterministic. This setting is significantly simpler than standard stochastic games, in which one of the main challenges arises from the uncertainty in state transitions.\n\n- Theorem 3 only holds for games that terminate in finite time (which is different from Markov games with finite horizon). This assumption appears quite restrictive as it imposes strong structure constraints on the reward function of each state. In standard RL literature, the $\\gamma$ discount factor is commonly used to ensure the value function remains bounded (and thus is absolutely summable). it would be helpful for the authors to justify why they adopt this assumption instead of following the conventional discounted setting.\n\n- Despite claiming that the explored state space is a subset of the entire state space, the paper provides no theoretical results on the upper bound of the number of states visited by the algorithm. Thus it is difficult to theoretically quantify the improved state dependency of the proposed algorithm.\n\n- Only an asymptotic convergence guarantee is provided for the proposed algorithm."}, "questions": {"value": "- In line 244-246, the authors state that every fixed point is a restricted fixed point but the converse is not true. Can the author provides a specific game example where the converse fails to hold?\n\n- In line 296, the authors mention that SPE is applicable to general zero-sum turn games. Can the author specify with more details? Is there any convergence guarantee when the operator fails to be contractive?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RQ0DXNkWA4", "forum": "SwWxnZvgF4", "replyto": "SwWxnZvgF4", "signatures": ["ICLR.cc/2026/Conference/Submission15882/Reviewer_M6nd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15882/Reviewer_M6nd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793782033, "cdate": 1761793782033, "tmdate": 1762926098894, "mdate": 1762926098894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comments"}, "comment": {"value": "# Relationship to existing work\n\nBased on the reviewers’ comments, we feel that our manuscript under-emphasized the key contribution of our work, which is an algorithm that computes provably correct saddle-point policies for \"large games\" with \"modest computation\" in the sense that we do not need to explore every reachable state. Section 5 (numerical results) shows that we solved games with up to \\\\(10^8–10^9\\\\) states while exploring a fraction of the *reachable* states typically lower than 50%, but as low as 5% for large games. This modest computation is a defining feature of this work and translates to manageable compute times (order of hours for our largest games). The set of states that our iterations reach is a very small subset of the set of states that would be reached, e.g., by an epsilon-greedy exploration strategy (or almost any variation of it) in the full 2-player state space. Therefore it is absolutely not correct to characterize the set of states visited as \"typically the states any standard learning process would visit\". In fact, to find a saddle-point policy for our largest games (say the Atlatl game with about 200,000,000 states), it is crucial that we do *not* use an epsilon-greedy policy in line 4 of the SPE Algorithm and, instead, focus sampling on a much narrower set of states. Again note that we are not simply avoiding unreachable states, *we are avoiding states that would typically be explored by almost any form of exploration policy proposed in the literature*.\n\nOne can find in the literature algorithms that have been used to find strong policies for much larger games than those we deal with (e.g., Chess on the order of \\\\(10^{50}\\\\) states or Go with \\\\(10^{170}\\\\) states), but for none of these games provably correct saddle-points have been reached. A notable exception would be checkers (\\\\(5\\times 10^{20}\\\\) states) with a saddle-point found in 2007 after 16 years of work (see https://www.cs.cornell.edu/courses/cs6700/2013sp/readings/06-b-Checkers-Solved-Science-2007.pdf on time needed to explore the different portions of the tree).\n\nWe can find in the literature algorithms that guarantee termination at a provably correct saddle-point after a certain minimum number of iterations. For stochastic games, \"provably correct\" must be understood in a probabilistic setting (typically, with probability larger than 1-delta, the algorithm terminates at an epsilon-saddle-point for arbitrarily small epsilon, delta). Our related work section lists several of these algorithms, including the ones that appear to require the smallest number of samples. Many of these papers do not provide explicit numbers for the constant \"hidden\" in the big-O notation for sample complexity, but the proofs of all the sample complexity results we have seen rely on full state/action exploration with high probability, so the size of the state/action joint space is a readily computable lower bound on the sample complexity. In light of this complexity, competing algorithms would require exploring at least once all \\\\(10^8-10^9\\\\) reachable states, in our examples in Section 5 while, emphatically, we can get away with exploring a small fraction of those states.\n\nWith the proposed algorithm, for every game we present in Section 5, all assumptions of our theoretical results were verified and we allowed the algorithm to run until termination, which means that we obtained provably correct saddle points; even for our largest game with about 200,000,000 states and 10-100 actions per state (Atlatl). We could not find in the literature any algorithm that could compute a *certified* saddle-point policies for games of this size, with a comparable level of computation (emphasis on \"certified\"). \n\n# Extension to stochastic games:\n\nOur original paper was not quite limited to deterministic games: The definition of restricted saddle-point and the sufficient condition (Theorem 2) were both applicable to stochastic games. The SPE Algorithm was also applicable to stochastic games, but the correctness statement in Theorem 3 was indeed restricted to deterministic games. However, it is possible to prove correctness also for stochastic games, which we have done in the revised paper through a new result (Theorem 4). \n\nIn the stochastic case, termination of the loops in lines 7-12 and 15-20 (used to check whether a saddle-point was reached) must be understood in a probabilistic sense: we need convergence up to a tolerance of epsilon with probability better than 1-delta (see Assumptions 2-3). In addition, to get termination in finite time, we need to settle for an epsilon-saddle-point with some epsilon>0.\n\nAt the expense of sounding repetitive, we should emphasize that the number of samples needed in the loops 7-12 and 15-20 to get the epsilon-delta guarantee mentioned above, are generally far smaller than those required to explore every reachable state. This is because in both of these tests, the policy of one of the players is fixed."}}, "id": "BsoqMOPidF", "forum": "SwWxnZvgF4", "replyto": "SwWxnZvgF4", "signatures": ["ICLR.cc/2026/Conference/Submission15882/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15882/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15882/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763709109254, "cdate": 1763709109254, "tmdate": 1763709109254, "mdate": 1763709109254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies two-player zero-sum turn-based Markov games. While the saddle point of this problem can be found by finding the fixed point of the Bellman operator over all state-action pairs, this paper shows that finding a fixed point over a certain subset of state-action pairs is sufficient. The paper proposes an algorithm, SPE, that finds the saddle point, possibly without exploring all state-action pairs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is nicey formatted and is easy to follow. The idea, theoretical guarantees, and the experiement are presented clearly."}, "weaknesses": {"value": "1. The contribution of this paper does not seem strong enough for acceptance.\n\n- Given the definition of a saddle points, specifically by Eqs. (6a) and (6b), Theorem 2 seems quite straightforward.\n\n- The discussion is limited to deterministic games, which limits its generality.\n\n- The theoretical sample complexity of Algorithm 1 is not provided other than that it is finite, which is limited against the compared works.\n\nI think alpha-beta pruning is a more systematic and efficent way to implement the same idea of not searching the entire state-action space for two-player zero-sum deterministic finite-time turn Markov games that this paper focuses on. It prunes states that can only be reached by clearly suboptimal actions.\nIt is proved that its number of searches (which corresponds to number of sampled states in this paper) can be as small as $\\mathcal{O}(\\sqrt{S})$ for favorable instances, while maintaining correctness [A].\nAlthough the setting may be slightly different, the difference seems negligible and the proposed algorithm and its theoretical guarantees seem inferior to this well-known algorithm. At least, I think some comparison must be made.\n\n2. The proof of Theorem 2 is slightly confusing. In the proof of Theorem 2, it seems like the notations $\\pi_ 1^\\ast$ alternates denoting the true saddle point and the induced policy of the restricted fixed point. Lines 684-705 regard it as the induced policy from the restricted fixed point, but Line 706 states that the pair $(\\pi_ 1^\\ast, \\pi_ 2^\\ast)$ is in $\\Pi^* $, which I thought was the goal of the proof. From then on it seems like $\\pi_ 1^\\ast$ and $\\pi_ 2^\\ast$ denote the true optimal policy.\n\n[A] Knuth, D. E., & Moore, R. W. (1975). An analysis of alpha-beta pruning. Artificial intelligence, 6(4), 293-326."}, "questions": {"value": "Theorem 2 introduces a new concept \"feedback saddle point\", however it is not explained in the paper.  \nIn line 828, $s < s'$ would hold if $s'$ is reachable from $s$, but I don't think it is \"if and only if\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hxy5BafE9b", "forum": "SwWxnZvgF4", "replyto": "SwWxnZvgF4", "signatures": ["ICLR.cc/2026/Conference/Submission15882/Reviewer_bk2X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15882/Reviewer_bk2X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849609893, "cdate": 1761849609893, "tmdate": 1762926098035, "mdate": 1762926098035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies two-player zero-sum turn-based Markov games and proposes a new restricted fixed-point condition on the Q-function that guarantees saddle-point and security policies without requiring full state-space exploration. Based on this condition, the authors develop the Saddle-Point Exploration (SPE) algorithm, which alternates between best-response learning and Q-function updates. For deterministic finite games, the authors prove that SPE converges in finitely many steps to a restricted fixed point and yields a pair of saddle-point policies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a theoretically grounded perspective by relaxing the full Bellman fixed-point requirement to a restricted subset of states. The resulting formulation is conceptually interesting and could inspire further work on certifiable learning under partial exploration. I didn’t check the full proof, but the finite-time convergence guarantees for deterministic games seems to be rigorous and technically solid."}, "weaknesses": {"value": "The overall novelty appears limited. The main idea—constructing saddle-point policies from a restricted fixed-point condition—closely resembles existing work, particularly Anderson et al. (2025), which already studied finite-computation Q-learning for zero-sum turn games. The differences here mainly concern framing and sample selection, which seem incremental and insufficiently contrasted with prior work. There’s no explicit comparisons with previous works in terms of the problem settings and assumptions, theoretical results and empirical performance (note that in the Related Work section, there’s only comparison and statement for general Q-learning and zero-sum Markov games, which is not the focus of this paper.) \n\nFurther, the paper’s framing and exposition could also be improved. The introduction overstates generality by referring to Markov games broadly before restricting to turn-based settings later, which seems a bit deceiving. The contribution relative to existing literature should be articulated more precisely.\n\nThe central claim of “learning without full state exploration” is also overstated. In practice, the restricted set still corresponds to the reachable states under certain policies, which are typically the states any standard learning process would visit. This makes the contribution conceptually modest, since avoiding unreachable states is expected and does not drastically improve efficiency.\n\nThe theoretical scope is narrow, as all guarantees assume deterministic, finite-horizon, turn-based dynamics, without discussion of approximate results under stochastic or continuous settings."}, "questions": {"value": "Could the authors clarify how the proposed “restricted fixed-point” condition fundamentally differs from the standard Bellman fixed point beyond the scope restriction? Specifically, how does it change the theoretical or algorithmic implications compared to prior formulations such as Anderson et al. (2025)?\n\nThe paper claims to learn “without full state exploration,” but the restricted set still seems to include all reachable states under the equilibrium policy. Could the authors elaborate on how to identify this restricted set and whether this reduction offers measurable computational savings?\n\nHave the authors considered extending the theoretical results to stochastic or partially observable settings? Even a discussion of potential obstacles or approximate guarantees in such cases would help assess the broader applicability of the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NNvMN5agNC", "forum": "SwWxnZvgF4", "replyto": "SwWxnZvgF4", "signatures": ["ICLR.cc/2026/Conference/Submission15882/Reviewer_f24w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15882/Reviewer_f24w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923790771, "cdate": 1761923790771, "tmdate": 1762926097574, "mdate": 1762926097574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}