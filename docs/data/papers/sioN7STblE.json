{"id": "sioN7STblE", "number": 7909, "cdate": 1758042452761, "mdate": 1759897823067, "content": {"title": "Words That Make Language Models Perceive", "abstract": "Large language models (LLMs) trained purely on text ostensibly lack any direct perceptual experience, yet their internal representations are implicitly shaped by multimodal regularities encoded in language. We test the hypothesis that explicit sensory prompting can surface this latent structure, bringing a text‑only LLM into closer representational alignment with specialist vision and audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it cues the model to resolve its next‑token predictions as if they were conditioned on latent visual or auditory evidence that is never actually supplied. Our findings reveal that lightweight prompt engineering can reliably activate modality‑appropriate representations in purely text‑trained LLMs.", "tldr": "A simple cue like asking the model to ‘see’ or ‘hear’ can push a purely text-trained language model towards the representations of a purely image-trained and purely-audio trained encoders.", "keywords": ["platonic representation hypothesis", "perception", "large language models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ef3dba4171aad1dc8443986f24e2be3291b3696.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Building on the platonic representation hypothesis, this article explores how the cosine similarity between a text model's representation on one side and either a vision or an audio model's representation on the other side can be influenced by prompting. In particular, the authors discover that when the prompt encourages the model to describe visual aspects, then similarity to the vision model increases, while if it's asked to describe auditory aspects, then similarity to the auditory model increases."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The research question is well motivated an interesting (to me, but also likely to others that are interested in the platonic representation hypothesis and related work)\n2. Great figures, very helpful and also beautifully designed\n3. The view of alignment not as a fixed property (as in e.g. the platonic representation hypothesis paper and most of the literature), but as something that can be influenced through prompting, is an important and valuable perspective (and has parallels to e.g. prompt steerability [Miehling et al. 2025, \"Evaluating the Prompt Steerability of Large Language Models\"], biases in VLMs [Gavrikov et al 2025, \"Can we talk models into seeing the world differently\"]).\n4. The analysis provides a framework that could be expanded to SSL vs. supervised encoders and VLMs as well."}, "weaknesses": {"value": "Major:\n\n1. The magnitude of the observed effects is small. While the results are systematic and I don't doubt their validity (the core hypothesis is supported by appropriate evidence), a cosine similarity increase from e.g. 0.14 to 0.16 is just a bit small to make big claims. In order to put the results into perspective, it would help to put the alignment scores into context - e.g., could a lower and upper bound be computed? Akin to \"explained variance\", could a percentage be calculated w.r.t. these lower and upper bounds in terms of how much variance in e.g. a visual representation is captured by the text representation?\n\n2. Relatedly, and as a consequence: The title - \"words that make language models perceive\" - is catchy but also feels like a bit of a stretch. Yes there are some words that increase an LLM's cosine similarity with a perception model from e.g. 0.14 to 0.16, but a more accurate description would be \"prompts that make language model representations slightly more aligned with perception model representations\". In light of the small magnitude of the results, I strongly recommend that the authors make a pass throughout the paper to contextualize claims accordingly and tone some of them down a bit - it's a solid investigation and the paper doesn't need to make big claims based on small effect sizes.\n\n3. The scientific contribution (as in providing knowledge / discovery) is there and that's great; however, the paper's impact could be strengthened by showing that those insights can be turned into downstream benefits of some sort. While a \"VQA without V\" tasks is explored in Section 3.6, this task is highly artificial: it's appropriate for the experiment purpose, but doesn't demonstrate credible downstream benefit. One possible way to show the effect on downstream evals could be to add an \"alignment penalty\" or \"alignment encouragement\" term to training a language model, and testing how this influences downstream task performance. Just one thought, other options are possible as well, but it's always nice to see a paper ending without a reader thinking \"oh nice, but also, so what - how does this knowledge now help me?\". Please feel free to ignore this point if it's not something you're aiming to achieve with this paper though, a pure scientific contribution is of course valid and valuable in itself - just thinking of ways to broaden the paper's possible impact.\n\nMinor:\n- Code not provided (the authors promise to provide it later)\n\nI'm very motivated to increase my score if my concerns can be addressed."}, "questions": {"value": "- for mutual kNN, what's chance alignment?\n- line 51: \"more similar\" - compared to what? Suggesting to add context\n- line 76: can \"perceptually grounded\" be defined?\n- line 89: \"VQA in text modality\" doesn't make much sense at first glance (unless one reads the rest of the paper), could be paraphrased/explained in the results summary to make it more self-contained\n- Figure 2: suggesting to add explanation for alignment to what/which system in the figure caption.\n- is Figure 5 a randomly selected or cherry-picked example? Either is fine, suggesting to state the selection method in the caption.\n- line 415: can this be confirmed statistically?\n- line 463: why \"interpretable\"?\n- line 467 \"an LLM can act like an image or audio encoder\" - I disagree; people expect something very different when reading this (a model that actually takes in image or audio as input). I get what you're trying to say but suggesting to rephrase to make this distinction clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dv39yo2Os6", "forum": "sioN7STblE", "replyto": "sioN7STblE", "signatures": ["ICLR.cc/2026/Conference/Submission7909/Reviewer_t8zj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7909/Reviewer_t8zj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761235212280, "cdate": 1761235212280, "tmdate": 1762919935089, "mdate": 1762919935089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the hypothesis that text-only LLM can be steered towards aligning their internal representations with vision or audio encoders by *sensory prompting*. The authors define *sensory prompting* as text-prompts asking the model to *see* or *hear* in context of a next token prediction setting. The alignment between different embedding spaces of LLMs and vision or audio encoders is measured via cosine-distance kernels over auto-regressive input sequences. Using this measure, the authors claim to show the ability to align text-only LLMs with representations of other modalities in a series of experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "In general, the question of models trained on different (or mixed) modalities represent the information extracted from the training data and how these representations could be linked/transferred is of high theoretical interest and practical impact. Hence, the paper raises a valid and interesting question."}, "weaknesses": {"value": "Unfortunately, the paper suffers from three main weaknesses: a) unclear technical description of the alignment measurement (this part probably can be fixed in a revision) and b) highly inconclusive experimental results which do NOT provide sufficient evidence for the proposed hypothesis. Finally, c) the paper ignores the likely high influence of the LLM training data.\n\na) section 2.1 and 2.2 which describe the generation of of the model representation vectors and the alignment measure are very hard to follow. While the description of the computation of $z_g^{(p)}$ lacks many details and would definitely benefit from a visualization in a figure, it remains unclear how it is actually used in 2.2 and how the intersection of neighborhoods in the alignment measure is then computed.\n\nb) The main paper is showing results for only a single evaluation of **one** LLM (qwen3) being aligned to **one** vision and **one** audio encoder which can be interpreted in favor of the proposed hypothesis. While even in this single instance the alignment in the audio embedding is already quite weak, additional experiments in the appendix (fig. 19-25) hardly show any significant effects for other experimental setting using different LLMs  and datasets - especially in the audio case where often the *see* prompt reaches better alignment scores than the *hear* prompt. Even for the vision experiments it remains unclear if alignment improvements e.g. from 0.08 to 0.10 are significant.\n\nc) the likely reason for the differences in the alignment of vision and audio models could be located in the LLM text training data: if this contains much more samples of textual descriptions of visual scenes than audio environments, it is much more likely that one could steer a model in this direction. Unfortunately, the authors are not investigating this point (which could be done by fine-tuning the LLMs with such descriptions and to measure the alignment effects).\n\nIn summary, I think that the paper raises an interesting question, however the current state of the paper lacks technical clarity and sufficient experimental conformation of the hypothesis."}, "questions": {"value": "It would be interesting to also investigate multi-modal model (text-image or text-audio) to see if the additional modality during training effects the alignment abilities."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Mome"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2LfdXnFmDU", "forum": "sioN7STblE", "replyto": "sioN7STblE", "signatures": ["ICLR.cc/2026/Conference/Submission7909/Reviewer_875U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7909/Reviewer_875U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643483161, "cdate": 1761643483161, "tmdate": 1762919934613, "mdate": 1762919934613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission demonstrates that “Sensory prompts” (prompts instructing an LLM to SEE or HEAR) bring the latent space of text-only LLMs closer to uni-modal specialist encoders in other domains (audio, vision) trained without any text supervision. This is shown by measuring representational similarity in a kernel space. Effectively, this paper is an empirical insight on the “platonic representation hypotheses” (Huh et al., 2024), where alignment of representations is not achieved by scale but explicitly steered into at test-time. Findings on main experiments include: LLM scale increases alignment (even under neutral cues) and steerability through cue separation; steering via sensory prompts increases alignment, whereas the opposite cue increases misalignment; longer generations (up to 256 tokens) increase alignment; the SEE cue even improves performance under \"VQA without V\"-kind of tasks"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is exceptionally well written and presented\n- This paper is an interesting twist on the “platonic representation hypotheses” (Huh et al., 2024), where alignment of representations is not achieved by scale but explicitly steered into at test-time. This topic is of significance to a wide audience.\n- The experiments are well-designed (e.g., all encoders are trained without text supervision, test data seems not to be included in training) and carefully ablated (e.g., Sec. 3.3).\n- The analysis is conducted over multiple axes of interest e.g., domains, scale, generation length.\n- Beyond representational alignment, it is shown that SEE steering can even increase performance on certain tasks"}, "weaknesses": {"value": "- My biggest issue is that the paper presents cherry-picked (data, model) results, which give the illusion that all findings generalize beyond the shown test scenarios. That seems to be rather unclear. The appendix contains numerous examples where steering does not work as intended: e.g., in Figure 21 HEAR often performs better than SEE on DCI, and even the no cue is sometimes better. I understand why SEE doesn’t improve based on the explanation in the paper (DCI is already visually well described), but HEAR should not increase alignment. Another example is LLaMA 3.2 where audio is poorly steerable. Steering the opposite cue (e.g. HEAR for vision) often also increases alignment.\n- The results in Sec. 3.6 are weakly supported by testing only one LLM.\n- The submission is lacking a rationale why the average over all hidden states and tokens is a good representation. There is only some small support in the Appendix.\n- Not all plots show error bars (Fig. 2/6)\n- It’s not clear how the responses of Sec. 3.6 are scored. The model seems to be instructed to only answer yes/no (which would be trivial to score) yet the outputs below Figure 31 show lengthy outputs.\n\n\n\nMinor presentation details:\n- Axis labels overlap in Figure 15\n- Label overlap in Figure 23 and 25\n- Missing space after Figure 16 in L694 \n- “HEAR HEAR” in L1185"}, "questions": {"value": "- Can the authors reject test leakage, i.e., exclude that uni-modal samples from the respective test datasets were used during training of the models? I checked that DINO and BEATS do not contains the exact same datasets, but samples might still have been used.\n- The effect of cue steering is sample dependent as evident from the error bars Is there any clear pattern in the samples that are more or less steerable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C2084vPjjb", "forum": "sioN7STblE", "replyto": "sioN7STblE", "signatures": ["ICLR.cc/2026/Conference/Submission7909/Reviewer_4qK6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7909/Reviewer_4qK6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959410973, "cdate": 1761959410973, "tmdate": 1762919934095, "mdate": 1762919934095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}