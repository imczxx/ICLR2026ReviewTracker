{"id": "2wshkCgNYk", "number": 25387, "cdate": 1758367422346, "mdate": 1759896722612, "content": {"title": "Performance vs interpretability trade-off of hand-crafted and language model features: The case of protein superfamily classification", "abstract": "The newfound rise of protein language models (PLMs) that leverage data and compute has introduced an interesting conflict in computational biology: a trade-off between the high predictive performance of non-interpretable features and the scientific insight that can be gained from interpretable, hand-crafted ones. In this work, we highlight and study this conflict via the task of classifying protein domains into their CATH superfamilies. We train one-vs-all linear SVM classifiers for 45 CATH superfamilies, each characterised by significant class imbalance. We address the class imbalance by using a class-balanced loss function and the arithmetic mean (AM) of specificity and sensitivity for evaluation. Our analysis compares nine feature vector types, which are either non-interpretable embeddings from PLMs or interpretable hand-crafted features. The latter includes amino acid composition (AAC), di- and tri-peptide composition (DPC, TPC), and novel sequence-order (2OAAC, 3OAAC) and structure-based features (OCPC, CSIC). Our results demonstrate that PLM-based features achieve superior test AM scores of 90-99\\% with low variability, outperforming hand-crafted features by 20-30\\%. While PLM features yield high classification accuracy, their lack of interpretability obscures the underlying biological determinants. Conversely, the interpretability of hand-crafted features, despite their relatively low performance, can be leveraged to infer sequence and structural characteristics of CATH superfamilies. The proposed hand-crafted CSIC feature stikes a balance between predictive performance and interpretability, because it overfits to a lesser extent. This can be valuable for downstream applications like investigating protein-related diseases and guiding rational protein design.", "tldr": "", "keywords": ["Feature engineering", "interpretability", "proteins", "CATH superfamily", "hand-crafted features", "attention matrix", "protein language models", "class imbalance"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef4c28715f84304ec46425923b04c58b4b76a767.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In their submission, the authors investigate different feature sets for a particular protein classification task, namely protein superfamily classification. The authors carefully introduce three different feature sets: sequence-based (n-gram), structure-based (based on contact pairs derived from protein structure), protein foundation model features (attention or embeddings from ProtBERT). The structure-based feature were proposed by the authors and they stress their broad availability by leveraging AlphaFold structure. They train SVM classifiers on different feature sets in a one vs. all fashion to discriminate most informative feature sets. Protein Language Model embeddings turn out to be most predictive, but not interpretable as the authors point out correctly, closely followed by contact-based features proposed by the authors."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Pedagogical introduction to the field and to the different feature sets in a way that should be accessible also for people unfamiliar with a computational biology context.\n* Newly proposed structural features that are apparently very discriminative (at least for the task at hand) that can be derived from widely available structural information (e.g. from AlphaFold)\n* The authors correctly emphasize the tension between the discriminative power of features and their interpretability, which is important for applications in the Natural Sciences\n* The authors use a sensible way of fixing the distance threshold for their contact-based features in a data-dependent manner."}, "weaknesses": {"value": "* Just a single task is considered. More general insights could be drawn if further qualitatively different tasks would be considered e.g. enzyme classification or gene ontology prediction.\n* The authors miss MSA-features as important and very powerful category of features.\n* No combination of feature sets is considered. It would also be interesting to understand their individual contributions (e.g. using a Shapley formalism) to quantify which features contributes how much and potential overlap between different feature sets.\n* There is a strong imbalance between setup and results. The actual results constitute barely one page in the manuscript. The largest part of the paper provides feature definitions (which could to some degree also be provided in the supplementary material). This would free up some space to add more details on experiments and implications.\n* No statements about the statistical significance of their findings were made. These could for example be implemented via emprical bootstrapping the performance difference on the test set.\n* Without doubt, the proposed structural features are more interpretable than foundation model features but are also not as interpretable as for example the sequence-based features. To turn this into a stronger submission, it would be nice to see some more specific insights from these more interpretable features to get an idea what kind of insights they could enable."}, "questions": {"value": "* Did the authors explore any other protein language models, e.g. ESM2?\n* Can the authors motivate their choice of classification algorithm?\n* Was the structural information taken from experimental data or from AlphaFold? It would be interesting to quantify the difference between the two to assess whether the author's scalability argument (by using AlphaFold structures) is valid."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CVNbrF4a0t", "forum": "2wshkCgNYk", "replyto": "2wshkCgNYk", "signatures": ["ICLR.cc/2026/Conference/Submission25387/Reviewer_qdz7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25387/Reviewer_qdz7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760717731231, "cdate": 1760717731231, "tmdate": 1762943420773, "mdate": 1762943420773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper touches a tradeoff between the interpretability and performance, and their findings were insightful especially to the domains where interpretability is important. They trained LinearSVC (in some cases SGDClassifier) with features either by hand-crafted engineering or PLMs, and compared the results in predicting 45 CATH superfamilies. The authors provided insight into how an interpretable features would achieve a comparable performances in the down streaming tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The comparison processes were rigorous and the authors used both sequence and structure based hand-crafted features. \n2. The authors provided novel structure features, that could be useful to the biocomputation community, also the ProtBERT-Attn could also be useful even if it is not interpretable.\n3. The authors covered the imbalance problems. The dataset is diverse, covering 45 CATH superfamilies.\n4. The authors ran five random splits which made the experiment more rigiorous.\n5. Easy to follow and understand the methodology."}, "weaknesses": {"value": "1. The scope of this paper is my biggest concern. The paper aims to achieve a balance between performance and interpretability of PLMs. However, only the ProtBERT was evaluated. The scope of PML comparison is somehow limited, making me worried about whether their observations/results could be generalized when using other PLMs such as the ESM family or ProtT5. By doing so they can cover more training set of LMs and more dimensions.\n2. For TPC and 30AAC, they were using SGDClassifier, how would that compared with the cases when LMs also give high-dimensional features? (Related to Weaknesses #1)\n3. In most hand-crafted feature scenarios, the model is overfitting. \n4. Complexity to calculate CSIC.\n5. Missing some ablation studies: for example, the authors' conclusions would be more solid if they can ablate the ProtBERT-Attn Feature (16 is the total), or ablating the CSIC itself (for instance, Intervals K)\n6. (Minor) Table 5 doesn't have the dimensions for the features. Although table 1 has such information, table 5 would be more readable if the authors could indicate which features are high-dimensional or not. In addition, the order the features in Table 5 could be consistent with Table 1."}, "questions": {"value": "1. Can you combine multiple features? How will that affect the performances of the LinearSVC/ SGDClassifier.\n2. How might your experiments be generalized to non-bio-computation domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hEswh8Q0tD", "forum": "2wshkCgNYk", "replyto": "2wshkCgNYk", "signatures": ["ICLR.cc/2026/Conference/Submission25387/Reviewer_4jFd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25387/Reviewer_4jFd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761348996407, "cdate": 1761348996407, "tmdate": 1762943420511, "mdate": 1762943420511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses the trade-off between manual features and protein language model (PLM) features in protein superfamily classification tasks in terms of prediction performance and interpretability, and proposes a new feature engineering method to balance the two."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. By employing various feature engineering techniques, this study analyzed the encoding capacity of protein sequences for genetic information.\n2.proposed one-vs-all classifiers to predict the CATH homologous superfamily of a protein domain."}, "weaknesses": {"value": "1.The article's structure and mathematical notation are somewhat disorganized, making it difficult for readers to grasp the core methodology. \n2.The study lacks sufficient experiments, fails to analyze downstream applications, and does not conduct ablation experiments on the method itself."}, "questions": {"value": "How can the proposed study be integrated with existing LLM-based AI technologies for protein understanding and generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EXABXuYlY6", "forum": "2wshkCgNYk", "replyto": "2wshkCgNYk", "signatures": ["ICLR.cc/2026/Conference/Submission25387/Reviewer_taws"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25387/Reviewer_taws"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816304593, "cdate": 1761816304593, "tmdate": 1762943420216, "mdate": 1762943420216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the performance versus interpretability trade-off in protein superfamily classification by comparing nine hand-crafted features versus protein language model features (ProtBERT). The authors conclude that while PLMs offer high predictive accuracy, carefully engineered interpretable features like CSIC can balance performance and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-structured and easy to follow.\n2. I appreciate the effort in designing the evaluations for class-imbalance protein predictions, which is well-motivated in real life."}, "weaknesses": {"value": "Major Issues\n\nThe experimental setup is far too narrow to support the paper's broad claims. The authors argue there's a fundamental trade-off between performance and interpretability in protein modeling, but they only test this on a single task with a single PLM:\n\n* CATH superfamily classification is a simplified sequence similarity problem. The conclusion cannot be generalized to other tasks like  binding affinity and structure predictions. The paper can't properly claim a \"fundamental trade-off\" exists across computational biology when it  only looked at one relatively simple classification problem.\n\n* only ProtBERT:  ESM is SOTA on most benchmarks and is what people actually use in practice. Testing against an older, weaker model doesn't tell us much about the real performance gap.\n\nThis experiment just shows ProtBERT beats some hand-crafted features on superfamily classification. The conclusions about performance vs. interpretability being a universal trade-off cannot be drawn given this limited evidence.\n\nIn addition, the paper claim hand-crafted features are interpretable, but never demonstrate this. Where's the biological insight? What features distinguish different superfamilies? Without showing this, what is the interpretability?\n\nOverall, the execution of the paper lacks the experimental rigor and technical depth expected for a venue like iclr."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "k71Z6OkDcT", "forum": "2wshkCgNYk", "replyto": "2wshkCgNYk", "signatures": ["ICLR.cc/2026/Conference/Submission25387/Reviewer_xiRu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25387/Reviewer_xiRu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953000552, "cdate": 1761953000552, "tmdate": 1762943419852, "mdate": 1762943419852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}