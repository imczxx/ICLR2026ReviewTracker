{"id": "GVVNG2EMQv", "number": 25055, "cdate": 1758363572096, "mdate": 1759896735913, "content": {"title": "The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to  Visual  Information Loss", "abstract": "Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ''asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic---the persistence of norm disparity and the resulting asymmetric update rates---is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.", "tldr": "", "keywords": ["MultiModal Large Language Model;Pre-Normlization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cdc7ea4aa14491b0583106b79a8615adfedb8176.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "**The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss** observes a critical problem in VLMs where visual tokens have much larger vector norms than text tokens, causing a large inertia (reducing angular velocity) for visual parameter updating. The authors use a simple but elegant method to rescale visual tokens, and found satisfactory improvements in experiments with LLaVA-1.5. Generally speaking, this paper makes valuable observations, and the claims and findings are well-supported by its experiments."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is novel in observation. The observation that visual features differ in magnitude with text features directly questions the widely accepted assumption that connecters align visual and text modalities. This is valuable itself. However, I am not an expert in VLM representations, and I am not sure whether this is the first paper proposing this idea. So I will look to other reviewers and may change this comment.\n2. The use of simple layer-norm to address this is clear and generalizable. \n3. Appendix D is informative. It shows that with paper's method, image features can align better with text features, which supports their claim."}, "weaknesses": {"value": "1. The experiment is done on a rather old model. LLaVA-1.5 is not competitive in today's VLMs, so this might weaken the paper's claim.\n2. The paper is written in a hurry, and the subcaptions for Figure 3 are just placeholders. A typo ' in the title.\n3. The captions of the other figures can be more informative."}, "questions": {"value": "Can this method also be applied to modern models like Qwen2.5-VL? In table 2, the same visual token magnitude problem is shown on Qwen2.5-VL, so the paper could be strengthened with an experiment on Qwen2.5-VL, to verify that this is a fundamental problem and a generalizable solution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EtS7jNH4NX", "forum": "GVVNG2EMQv", "replyto": "GVVNG2EMQv", "signatures": ["ICLR.cc/2026/Conference/Submission25055/Reviewer_K4C3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25055/Reviewer_K4C3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760705067148, "cdate": 1760705067148, "tmdate": 1762943306155, "mdate": 1762943306155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper diagnoses a modality-scale mismatch in pre-norm MLLMs: vision tokens enter the LLM with much larger L2 norms than text tokens, inducing asymmetric update dynamics (“representational inertia” for vision) and consequently miscalibrated cross-modal attention. The fix is simple: insert a single LayerNorm after the visual projector, with its gain closed-form initialized to match the average text-embedding norm of the target LLM. The authors support the mechanism with theory and internal diagnostics (layer-wise norms and inter-layer cosine similarity), then show empirical gains in LLaVA-1.5 (multimodal benchmarks and even text-only MMLU). Post-intervention, the model displays aligned layer-wise norms and balanced update rates between modalities, consistent with the proposed explanation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- **Clarity and logically sound.** The paper is well written: key concepts (norm disparity, asymmetric update dynamics) are introduced with clear intuition, then formalized without losing readability. The work tightly connects diagnosis → theory → solution → gains → diagnostic analysis, making the mechanism logically sound.\n- **Simplicity of the fix.** The proposed solution—one LayerNorm after the visual projector with a closed-form gain init—is easy to implement, and compatible with existing MLLM stacks. The method adds negligible overhead, requires no data or architecture burden.\n- **Strong theoretical support.** The theoretical part (update geometry / angular velocity view) is well motivated and aligns with observed behaviors."}, "weaknesses": {"value": "- **Limited model coverage.** Results are centered on the LLaVA-1.5 stack; the claim would be stronger with evaluations across more backbones/architectures (e.g., Qwen-VL, InternVL, Idefics2, GLM-4V) and different visual encoders (CLIP/SigLIP).\n- **Missing normalization ablations.** The paper focuses on LayerNorm with a specific init. Adding RMSNorm vs LayerNorm can be a valid ablation.\n- **Scale experiments.** The work can further show effectiveness if validate on larger data and larger LLMs."}, "questions": {"value": "- The paper can be stronger if address the empirical questions in the weakness section.\n- Any benchmark that the solution does not provide a performance gain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dHcsuVjkQ4", "forum": "GVVNG2EMQv", "replyto": "GVVNG2EMQv", "signatures": ["ICLR.cc/2026/Conference/Submission25055/Reviewer_hK3Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25055/Reviewer_hK3Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643617647, "cdate": 1761643617647, "tmdate": 1762943305430, "mdate": 1762943305430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and analyzes a subtle but consequential failure mode in Pre-Norm multimodal LLMs: a systematic L2-norm mismatch between high-norm visual tokens and low-norm text tokens induces an “asymmetric update dynamic,” in which visual tokens exhibit higher “representational inertia” and thus evolve semantically more slowly than text tokens. The authors formalize the effect under a geometric update model. They further argue that this suppresses the learnable attention signal in expectation and propose a minimal intervention: insert a single, carefully initialized LayerNorm after the visual projector to enforce norm alignment. On LLaVA-1.5 with a specific backbone and vision encoder, this yields consistent gains on multimodal benchmarks and text-only task MMLU, supporting the claim that resolving the architectural imbalance improves overall capability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Mechanistic, interpretable theory-to-practice link.** The geometric update analysis connects norm magnitude to depth-wise angular dynamics and expected attention suppression, and the proposed diagnostic metrics and solution are well aligned with the theory.\n- **Minimal, low-risk intervention.** A single post-projector LayerNorm with principled initialization is simple to implement, does not alter the LLM backbone, and is easy to deploy or ablate in real systems.\n- **Clear diagnostic visualizations.** Attention maps and depth-wise metrics provide qualitative and quantitative evidence that norm alignment improves cross-modal focus and reduces modality asymmetry."}, "weaknesses": {"value": "1. The theoretical model adopts strong simplifying assumptions—uniform update magnitude per layer, a layer-wise constant expected angle, symmetric distribution of orthogonal components—that are not empirically validated. Reporting empirical distributions of $C^{(l)}=\\|\\Delta h^{(l)}\\|_2$, $\\cos\\phi=\\langle \\widehat h,\\widehat{\\Delta h}\\rangle$, and the orthogonal/parallel energy split would substantiate the premises.\n\n2. The paper’s theoretical narrative emphasizes that an L2 norm mismatch leads to “asymmetric angular velocity.” If this is the causal core, then RMS-style normalization—or even a simple fixed scalar rescaling to bring visual-token norms to the same scale as text—should in principle deliver most of the gains. Conversely, if significant improvements arise only from the current solution, namely LayerNorm with a learnable gain and specific initialization, that would indicate mechanisms beyond mere norm magnitude are at play. The paper lacks corresponding experiments and discussion.\n\n3. Statistical robustness and scope are limited. Results appear to rely on a single seed, one backbone size, and one vision encoder, with no confidence intervals or significance tests. \n\n4. Some language overstates the effect. Phrasings like “eliminated” asymmetry are not fully supported by Fig. 3b, where a residual gap remains. The authors need to quantify the mean/max gap and the relative reduction."}, "questions": {"value": "1) Table 1 only reports the modality-interface L2 norms at initialization, but does not provide a post-training comparison. Was the norm disparity mitigated after training, especially in settings where the vision encoder is unfrozen?\n\n2) The caption of Table 1 states mean ± std, but the table reports only the mean values. The standard deviations are missing.\n\n3) Lines 307–309 assert that “multi-modal training increases the text embedding norm,” but Table 2 does not present experimental evidence to support this claim.\n\n4) Figure 2 visualizes the evolution rate of representations along depth. I suggest adding the similarity of one layer before vs. after parameter updates to show temporal representational change and test whether there is an asymmetry in the evolution speed of different modalities."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AxbId1SrpV", "forum": "GVVNG2EMQv", "replyto": "GVVNG2EMQv", "signatures": ["ICLR.cc/2026/Conference/Submission25055/Reviewer_XYFm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25055/Reviewer_XYFm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926072656, "cdate": 1761926072656, "tmdate": 1762943303711, "mdate": 1762943303711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors investigate a critical flaw in pre-normalized architectures within multimodal large language models (MLLMs): the significant disparity between high-norm visual tokens and low-norm textual tokens. This imbalance leads to a phenomenon referred to as “representational inertia,” where the semantics of high-norm visual tokens change at a considerably slower rate than those of text tokens, ultimately reducing the efficiency of cross-modal fusion between visual and textual features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Identifies a Critical Issue: The paper highlights a significant and often overlooked issue regarding the norms of visual and textual tokens in multimodal large language models, shedding light on how this disparity can impair effective cross-modal integration.\n\nTheoretical Contributions: By introducing the concept of \"representational inertia,\" the authors provide a new theoretical framework for understanding the dynamics of feature representation in MLLMs. This contributes to the academic discourse on the challenges in multimodal learning.\n\nEmpirical Validation: The experiments conducted across multiple open-source multimodal models provide robust empirical support for the theoretical claims made, enhancing the credibility of the findings and their implications for model design.\n\nPractical Solutions: The proposed solution of adding a layer normalization layer to enforce norm alignment is practical and relatively easy to implement, making it accessible for researchers and practitioners aiming to improve their multimodal models.\n\nEnhanced Model Performance: The experimental results demonstrate significant improvements in model performance for both multimodal and text-only tasks, showcasing the effectiveness of the proposed intervention and its potential to advance the state of the art in multimodal understanding."}, "weaknesses": {"value": "Lack of Broader Contextual Analysis: The paper primarily focuses on the norm differences without providing a comprehensive analysis of other factors that could affect multimodal performance, such as data quality or model architecture variations. This could limit a holistic understanding of the challenges in multimodal learning."}, "questions": {"value": "While the proposed layer normalization solution effectively addresses norm differences between visual and textual tokens, how might this approach impact the model's ability to leverage subtle, nuanced variations within the visual or textual data that might be important for certain tasks, potentially leading to information loss or a homogenization of feature representations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3ArrkkOfNq", "forum": "GVVNG2EMQv", "replyto": "GVVNG2EMQv", "signatures": ["ICLR.cc/2026/Conference/Submission25055/Reviewer_Wqxf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25055/Reviewer_Wqxf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996733981, "cdate": 1761996733981, "tmdate": 1762943303281, "mdate": 1762943303281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical flaw in Pre-Norm MLLMs: a severe norm disparity between high-norm visual tokens and low-norm text tokens. The authors' theoretical analysis shows this imbalance creates an \"asymmetric update dynamic\". High-norm visual tokens exhibit \"representational inertia\", transforming semantically much slower than text tokens. This mismatch impairs cross-modal fusion and suppresses attention signals. The proposed solution is simple yet effective: inserting a single, carefully initialized LayerNorm after the visual projector to enforce norm alignment. This intervention yields significant performance gains on both multimodal benchmarks and, notably, on text-only evaluations like MMLU, suggesting a more holistically capable model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a novel perspective, using effective theoretical and empirical analysis (like inter-layer cosine similarity) to confirm the impact of \"norm discrepancy\" on the model's \"asymmetric update dynamics\".\n2. The solution (inserting a single LayerNorm) is simple, effective, and easy to apply, achieving significant gains on multimodal and even text-only benchmarks."}, "weaknesses": {"value": "The solution lacks validation on more model combinations. The paper's experiments are primarily on the SigLIP + Llama-3.2-3B combination. To prove generalizability, the authors should test on more structures, such as the LLaVA-1.5 (CLIP + Vicuna) combination."}, "questions": {"value": "1. With sufficient training data, can the model learn to mitigate the \"norm discrepancy\" phenomenon on its own? \n2. For models like Ovis[1] that use a \"visual embedding table\", does a similar \"norm discrepancy\" phenomenon exist?\n3. In Table 2, the post-projector norms for KimiVL (4.78) and GLM-4.1V (4.58) are relatively close to the norm achieved by the author's method in Table 4 (2.2812). Does this imply these two models have already largely solved this problem with their more complex projectors?\n\n[1] Ovis: Structural Embedding Alignment for Multimodal Large Language Model"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "knUlBmqVXM", "forum": "GVVNG2EMQv", "replyto": "GVVNG2EMQv", "signatures": ["ICLR.cc/2026/Conference/Submission25055/Reviewer_uNHA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25055/Reviewer_uNHA"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission25055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025083840, "cdate": 1762025083840, "tmdate": 1762943303019, "mdate": 1762943303019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}