{"id": "tK6VZy5RYr", "number": 12493, "cdate": 1758208197933, "mdate": 1759897506167, "content": {"title": "Distill Not Only Data but Also Rewards: Can Smaller Language Models Surpass Larger Ones?", "abstract": "Distillation of large language models (LLMs) has traditionally focused on transferring teacher responses, overlooking the fact that teachers also possess strong evaluative capacity. As a result, students learn what to answer, but not which answers are preferable. This gap limits generalization, propagates teacher errors, and prevents students from improving beyond imitation. Therefore, we propose a unified distillation framework that transfers both responses and evaluation ability. Our key idea is to distill reward signals from the teacher, eliminating the need for costly human annotations. However, extracting reliable reward signals from LLMs is challenging because they are optimized for generation rather than evaluation. Therefore, we introduce an adaptive reward distillation strategy that applies majority voting for verifiable tasks and LLM-as-Judge for open-ended tasks.\nThis yields noisy yet effective self-supervised signals without human annotations. \nTo mitigate distribution shift, we systematically collect and label both teacher- and student-generated responses, which are used to train a reward model. The student is first warmed up with supervised fine-tuning on high-quality teacher responses, then refined with reinforcement learning guided by the learned reward model. Experiments on GSM8K, GSM-Plus, MMLU-Pro, and AlpacaEval2 demonstrate consistent gains over supervised fine-tuning, with smaller students in some cases even surpassing their teachers. \nThese results highlight our method as a scalable and effective paradigm for training efficient yet competitive LLMs.", "tldr": "", "keywords": ["Distillation", "AI-Feedback", "large language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18b4da542db54db821f21c97f2192a389205afcc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel KD with RL framework where a student model learns not only from the teacher’s responses but also from the teacher’s evaluation signals, i.e., rewards. The core idea is to train a reward model that aligns with the teacher’s preferences, allowing student outputs to be optimized via PPO against this learned reward signals. The authors also propose separate reward modeling strategies for verifiable and open-ended tasks. Experiments on GSM and MMLU benchmarks show improvements over standard distillation baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The central idea of jointly distilling both response and reward signals from a teacher LLM is novel and appears effective.\n- Separate reward modeling for verifiable tasks and open-ended tasks is a well-motivated design, highlighting that evaluation criteria differ depending on task types."}, "weaknesses": {"value": "While the motivation and method are promising, the paper lacks deeper analysis to justify whether this RL-based approach is really superior over existing KD methods.\n\n- A core assumption in this paper is that LLMs are unreliable evaluators, which motivates training an explicit reward model. However, training the reward model involves constructing the reward dataset, which itself is built from the teacher LLM judgments of teacher and student responses. If these evaluations are unreliable as claimed, the reward model may inherit their noise and inconsistencies. And if the teacher LLM is good enough to correctly evaluate, there is no need to introduce another reward model. Thus, I’m curious if this reward model has been properly constructed.\n- The experimental comparison is limited. One of the primary baselines (**Ours w/o R**) only reflects basic response-level KD using teacher outputs. However, existing KD methods like ImitKD (Lin et al., 2020), GKD (Agarwal et al., 2024), and DistiLLM (Ko et al., 2024) also incorporate student outputs and often use adaptive incorporation with teacher responses. Since the proposed PPO (Eq. 13) uses dataset $\\mathcal{D}_{\\mathcal{R}}$ which includes responses from both teacher and student, for fair comparison, the paper should include stronger, state-of-the-art KD baselines that similarly exploit student trajectories."}, "questions": {"value": "- It is unclear how the method performs when the teacher is incapable of providing high-quality responses (i.e., none of the responses reach the target reward $\\ell^*$). In such cases, the teacher-student preference dataset $\\mathcal{D}_{\\mathcal{R}}$ may be skewed toward simpler tasks, limiting the effectiveness of reward model training. Can the method still construct a meaningful reward dataset for those queries?\n- Is the reward model robust to noisy or inconsistent teacher scores? How is this addressed during training?\n\n**Minor typos**\n\n- $D_{\\text{TeachCan}}$ in Eq. 3\n- *Resul* in Sec. 5.3 header"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dLdg8Ness5", "forum": "tK6VZy5RYr", "replyto": "tK6VZy5RYr", "signatures": ["ICLR.cc/2026/Conference/Submission12493/Reviewer_5AyG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12493/Reviewer_5AyG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761296248890, "cdate": 1761296248890, "tmdate": 1762923367065, "mdate": 1762923367065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified distillation framework for LLMs, arguing that traditional SFT (data distillation) is limited because it propagates teacher errors and cannot surpass imitation. The authors introduce reward distillation, which extracts evaluation signals from the teacher without human labels. This is done using majority voting for verifiable tasks and LLM-as-Judge for open-ended ones. A student model is first warmed up with SFT, then refined using reinforcement learning (RL) guided by a learned reward model. This combined approach consistently outperforms SFT, allowing student models to in some cases surpass their teachers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Annotation-Free Framework: The paper proposes a novel reward distillation framework. This method cleverly extracts evaluation signals directly from the teacher (via majority voting or LLM-as-Judge) without needing expensive human annotations.\n2. Clear and Easy to Follow: The paper is well-structured and clearly written, making its complex methodology understandable."}, "weaknesses": {"value": "A primary weakness is the claim that the student can surpass the teacher. This is likely because the student model is trained on the GSM8K and MMLU-Pro data. Even though the teacher generated these answers and majority voting was used to enhance the data quality, the teacher's baseline performance is being compared against a student's fine-tuned performance. A more reasonable comparison would require fine-tuning the teacher model on the exact same high-quality, distilled dataset and then comparing its performance to the student's."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MClHFE0f7Q", "forum": "tK6VZy5RYr", "replyto": "tK6VZy5RYr", "signatures": ["ICLR.cc/2026/Conference/Submission12493/Reviewer_5QHe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12493/Reviewer_5QHe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914669388, "cdate": 1761914669388, "tmdate": 1762923366652, "mdate": 1762923366652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper introduces a novel distillation method to transfer the knowledge from the responses, but also the evaluative capabilities from the rewards. \n- The proposed reward distillation method uses a self-supervised process within the teacher-student framework. First, it uses the teacher LLM to generate a variety of responses. Then, it employs an adaptive evaluation strategy to create pseudo labels: majority voting or LLM-as-a-judge. A reward model is then trained on a mix of teacher and student-generated responses to learn these reward signals. Finally, the student model, after an initial warm-up with supervised fine-tuning, is further refined using reinforcement learning guided by this distilled reward model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The core motivation of this paper is to mitigate the distribution shift between the teacher and student, which is a critical challenge and a good research question in knowledge distillation."}, "weaknesses": {"value": "- For On-Policy RL (PPO), since the rollouts are sampled from the student policy model, there would not exist any distribution shift between the policy and the experience. Therefore, the proposed method does not solve a real question for On-Policy RL training.\n- For the verifiable task, this work should compare the proposed Reward Distillation with the standard RLVR method. Why is Reward Distillation better than RLVR? \n- For the non-verifiable task, this work should compare the proposed Reward Distillation with the RLHF with GenRM (LLM-as-a-Judge). Why is Reward Distillation better than RLHF with GenRM? \n- The paper positions itself primarily against traditional distillation but does not sufficiently discuss or compare against the closely related paradigm of self-rewarding or self-improving LLMs[1][2]. These methods also involve a model generating its own training signals or rewards.\n- For the reward signal, using the teacher model's signal as a reward has been proposed by On-Policy Distill[3][4], which can provide dense reward signals in the sequence dimension. However, the proposed Reward Distillation only provides an outcome reward signal and should construct data to train an additional reward model.\n\n[1] Self-Rewarding Language Models\n\n[2] Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge\n\n[3] On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes\n\n[4] MiniLLM: Knowledge Distillation of Large Language Models"}, "questions": {"value": "Same as the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4rnstzCKx2", "forum": "tK6VZy5RYr", "replyto": "tK6VZy5RYr", "signatures": ["ICLR.cc/2026/Conference/Submission12493/Reviewer_X6QW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12493/Reviewer_X6QW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762083408321, "cdate": 1762083408321, "tmdate": 1762923366170, "mdate": 1762923366170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-stage distillation pipeline as an alternative to standard supervised fine-tuning (SFT). The core idea is to distill not only the teacher's data (responses) but also its evaluative capacity (rewards). The method involves (1) an SFT warm-up, (2) training a separate, small reward model (RM) using teacher-generated labels (via majority vote or LLM-as-Judge), and (3) refining the student using Proximal Policy Optimization (PPO) guided by this RM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper's primary strength is its well-motivated strategy of leveraging both the teacher's generative and evaluative capabilities. Empirical results demonstrate that this pipeline outperforms SFT. Warm-up training can further improve results (although same finding has been pointed out by GKD, SKD paper).\n\nGKD: https://arxiv.org/pdf/2306.13649\nSKD: https://arxiv.org/abs/2410.11325"}, "weaknesses": {"value": "W1: Limited Novelty and Missing Baselines: This is the most significant weakness. The core idea of using reinforcement learning (via policy gradient) and reward signals for distillation is not new (e.g., GKD, MiniLLM, direct preference knowledge distillation). The paper's novelty is a specific pipeline, yet it fails to compare against these established RL-based distillation baselines. The comparison is limited to SFT, which is a weak baseline making it impossible to assess if this complex SFT+RM+PPO pipeline is any better than simpler, existing RL-KD methods or simply KL loss + reward objective.\n\nW2: The contribution of the reward signal generation is overstated. The components are well-known: \"majority voting\" is a direct application of Self-Consistency, and \"LLM-as-Judge\" is a standard technique. The paper's contribution is a synthesis of these existing methods, not a novel mechanism.\n\nW3: The paper's pipeline (SFT -> RM training -> PPO) is significantly complex, but its design choices are poorly motivated. PPO vs. GKD: It is unclear why this complex, multi-stage pipeline is necessary over simpler, end-to-end objectives like GKD, which also incorporate reward signals. The paper provides no justification. \n\nW4: Separate RM: The paper trains a separate, small RM but never justifies why this is necessary over using the teacher model as the reward source. This is a critical unstated detail (likely for computational cost), which also highlights the next weakness. If RM is expensive, why don't authors consider direct preference KD?\n\nW5: Unalyzed Computational Cost: The paper ignores the massive computational cost of its pipeline. The reward generation requires 10 teacher samples + 30 student samples and evaluations per query. This cost is not analyzed, and it's plausible that a baseline SFT model trained with the same compute budget (i.e., 40x more data) would perform just as well.\n\nGKD: https://arxiv.org/pdf/2306.13649\nminiLLM: https://arxiv.org/abs/2306.08543\nDirect preference KD: https://arxiv.org/abs/2406.19774"}, "questions": {"value": "Why training a separate RM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aTzvBgwyEn", "forum": "tK6VZy5RYr", "replyto": "tK6VZy5RYr", "signatures": ["ICLR.cc/2026/Conference/Submission12493/Reviewer_Ko1H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12493/Reviewer_Ko1H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762306224822, "cdate": 1762306224822, "tmdate": 1762923365777, "mdate": 1762923365777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}