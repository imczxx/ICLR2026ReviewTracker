{"id": "B3jlwFJ2mb", "number": 24684, "cdate": 1758359331647, "mdate": 1759896754816, "content": {"title": "Hy-ClustRec: Deep Cluster-Guided Rule Mining for Cold-Start Recommendation", "abstract": "Recommender systems are critical for navigating vast item catalogs but struggle with the cold-start problem, where a lack of interaction data degrades recommendation quality. While hybrid methods exist, they often fail to effectively structure item content or extract fine-grained preference patterns. In this paper, we introduce \\textbf{Hy-ClustRec}, a novel three-stage framework designed to address these challenges. First, we learn dense, non-linear representations of item content using a deep autoencoder. Second, these embeddings are segmented into meaningful communities using HDBSCAN, a density-based clustering algorithm. Third, we employ a hierarchical strategy for Association Rule Mining (ARM) to discover global and specialized co-occurrence patterns. Candidate items are then ranked using a hybrid scoring function that fuses rule confidence, semantic similarity from SBERT embeddings, and a user-cluster affinity score. To further boost performance, we incorporate an item-based KNN recommender into the final score with a weighted sum. Evaluated on a sparse subset of the Million Song Dataset, Hy-ClustRec demonstrates strong performance, proving especially effective in cold-start scenarios. Our work shows that a structured pipeline combining deep clustering with hierarchical rule mining and collaborative signals offers a robust solution to the cold-start problem.", "tldr": "We introduce Hy-ClustRec, a framework that uses deep clustering to guide hierarchical association rule mining for effective cold-start recommendation.", "keywords": ["Cold-Start Problem", "Data Sparsity", "Recommender Systems", "Clustering", "Association Rule Mining", "Hybrid Model"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6730da97615aa2e7dc22b848bb5746a6c52216cc.pdf", "supplementary_material": "/attachment/e81e29988896c7be751685ab37f6e274cb4706f9.zip"}, "replies": [{"content": {"summary": {"value": "Hy-ClustRec: \nThe authors propose a model called Hy-ClustRec to address the cold-start problem in recommender systems. Hy-ClustRec is a three-stage framework consisting of three phases: deep content-based item clustering, hierarchical association rule mining, and hybrid scoring and re-ranking. The authors perform an experiment on the Million Song dataset and compare their results to two existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The problem formulation/ proposed model is concise and clearly elaborated. \n- The conceptual overview well summarizes the proposed pipeline of Hy-ClustRec."}, "weaknesses": {"value": "- **Lack of novelty and technical contribution**. The authors simply combines existing models/work into their three-stage framework. How autoencoder, HDBSCAN have been chosen for their scheme haven’t been supported in the manuscript. Given that the pipeline simply combines existing models, are they carefully chosen at least through rigorously comparisons? Hy-CLustRec does not introduce any fundamentally new modeling concepts. \n- **Lack of experimental significance**. The authors simply perform an experiment on one dataset. While there are more popular datasets, e.g., Movielens, Amazon, Tiktok, etc, actively being used across multiple papers, the authors simply select one dataset for their experimentations. The efficacy of the proposed scheme cannot be supported through a single experiment with one dataset.  It is also surprising that Hy-ClustRec barely beats the simple approach ItemKNN from early 2000s. This is questionable as Hy-ClustRec is augmenting item-based KNN at phase 3, which raises doubts about the contributions of other factors. \n- **Lack of details**. The current draft is missing many important details (see questions). The authors simply report the weights in Eq. 4 and Eq 6. Hy-ClustRec highly rely on a series of heuristics and manually-tuned hyperparameters. \n- **Overstated claims**\na. Line 66: “providing a robust and methodologically novel solution for cold-start”, There isn’t any experimental results supporting the robustness. \nb. Line 239: “The final dataset presents a significant cold-start challenge”. However, the data preparation seems too general. (line 236-238). Are there any experiments that was performed only on cold items?\n\n(minor) K is being used in two ways: clustering and evaluation. Try to be clear when using K. \n\n(minor) Try to be consistent in style for readability in Equations 7-11."}, "questions": {"value": "Q1. How can the hierarchical ARM mine both fine-grained intra-cluster rules and broad global rules?\n\nQ2. In line 214, it says “are tuned on a validation set”, but according to line 244, there’s only train set and test set with 80:20 split. \n\nQ3. The input dimension of the autoencoder is set to 2 in the Appendix A.2. Further details are required; what is the input into the autoencoder? \n\nQ4. When you use K-means instead of HDBSCAN, how do you fix K for the K-means?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uAhpf8jbvh", "forum": "B3jlwFJ2mb", "replyto": "B3jlwFJ2mb", "signatures": ["ICLR.cc/2026/Conference/Submission24684/Reviewer_z8C1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24684/Reviewer_z8C1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761387502894, "cdate": 1761387502894, "tmdate": 1762943162930, "mdate": 1762943162930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework designed to address the cold-start problem in RS, where a lack of user interaction data leads to poor recommendation quality. Hy-ClustRec is a three-stage pipeline that combines deep learning for feature representation with hierarchical pattern mining to generate recommendations, especially in data-sparse or cold-start scenarios.\n\n1. Content-Based Item Clustering: a deep autoencoder is used to learn dense, non-linear representations of item content metadata. The autoencoder is trained to minimize the reconstruction error. These learned embeddings are then segmented into meaningful communities using the HDBSCAN clustering algorithm\n2. Hierarchical Association Rule Mining (ARM) The framework then mines for co-occurrence patterns at two levels: intra-cluster rulse and global rules\n3. Hybrid Scoring and Re-ranking: three scores namely rule-based confidence from ARM, semantic content similarity, and user-cluster affinity are linearly combined to produce a ranking score to be used by KNN recommender."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is presented with clarity.\n\nThe paper addresses the cold-start problem, a long-standing and critical issue in RS where the absence of interaction data severely limits recommendation quality."}, "weaknesses": {"value": "Limited novelty in individual components: \n\n* stage 1 : Using a deep autoencoder to learn dense, non-linear representations of item content is a standard and widely used approach for feature learning in RS. While effective, this stage functions as a standard preprocessing step rather than a novel contribution to the core methodology. The paper does not seem to introduce any innovations in the autoencoder architecture or training process itself.\n* stage 2: Applying a clustering algorithm to item embeddings to segment the catalog is also a common strategy for tasks like candidate generation or understanding item communities. The choice of HDBSCAN is frequnently seen in prior publications. The act of clustering item features is not in itself a novel idea.\n\n*  The use of manually tuned weights (w_c​​, w_s, w_m​) is a heuristic approach. It lacks a \"principled\" mechanism where the optimal combination of these features could be learned directly from the data. Modern systems often use model-based approach in the final stage to learn the complex, non-linear interactions between different features (the ranker). A simple linear combination is less expressive and may not be optimal.\n\n*  While Association Rule Mining (ARM) is effective, its computational complexity can be a concern for large-scale industrial systems. The paper proposes mining \"Global Rules\" from the entire dataset, which could become a significant bottleneck as the number of users and items grows into the millions or billions. The paper does not address the scalability of this hierarchical rule mining process, which is a critical consideration for real-world deployment."}, "questions": {"value": "* The sequential, multi-stage pipeline (autoencoder -> clustering -> rule mining) creates a risk of error propagation, where suboptimal performance in an early stage can negatively impact all subsequent stages. \nQuestion: How robust is the framework to noise or suboptimal outputs from the earlier stages? For example, if the autoencoder fails to produce high-quality embeddings, how does this affect the final recommendations? Do you have any measure or alerts setup in the system to detect that?\n\n* Could you discuss the scalability of the global ARM stage? As the number of users and items in a real-world system grows, mining rules from the entire dataset can become computationally prohibitive. What was the runtime performance on the Million Song Dataset subset, and how do you see this approach scaling to industrial-level datasets?\n\n* The final ranking is determined by a hybrid scoring function that combines rule confidence, semantic similarity, and a user-cluster affinity score, augmented with a KNN model. This appears to be a linear combination of several heuristic scores.  Could you elaborate on the methodology used to determine the weights? Was this done via a simple grid search, and if so, how sensitive was the model's performance to these hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qY9yIE6Dm8", "forum": "B3jlwFJ2mb", "replyto": "B3jlwFJ2mb", "signatures": ["ICLR.cc/2026/Conference/Submission24684/Reviewer_pPX8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24684/Reviewer_pPX8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721431315, "cdate": 1761721431315, "tmdate": 1762943162599, "mdate": 1762943162599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a pipelined method for cold-start recommendation. An autoencoder maps item metadata to a low-dimensional embeddings; HDBSCAN then partitions items into communities and marks outliers; FP-Growth is applied both within cluster and globally to extract rules; and get a hybrid re-ranking step: candidate items are scored with a weighted sum of (i) rule confidence, (ii) SBERT similarity between a candidate's text embedding and the user's mean profile embedding, and (iii) a user-cluster membership term. A non-truncated item-based KNN score is then blended via per user min-max normalization. Experiments are ran on a subset of MSD dataset with filtering: 30% user sampling, more than 20 interactions per user, and more than 70 plays per song. The authors report top-K metrics and compare against ItemKNN and a pipeline baseline CFCAI. \n\nOverall it is my opinion that this contribution is incremental; shows small gains over a weak baseline, and importantly, lacks proper cold-start evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "(+) Clear modular pipeline: What is described would be a reasonable design point for transparency and practical deployment. \n(+) Ablation study suggests each component contributes to the overall performance of the method."}, "weaknesses": {"value": "(-) The Evaluation does not match the 'cold-start' claim. The paper filters out low-frequency items, which removes tail items where cold-start is most severe. The split is a random per-user 80/20 interaction split, not a user-level or item-level cold-start protocol. As written, the experiments primarily measure standard top-K retrieval with moderately active users and relatively popular items, not cold-start robustness. Frankly, the filtering employed in the paper, is pretty typical pruning of the user-item interaction graph one finds in methods tackling standard top-n recommendation in the literature, without any cold-start emphasis. \n(-) Comparison against cold-start focused baselines is missing. RecSys literature contains a large number of papers with an emphasis on cold-start. Also, it is understood in this space that graph-based and so-called latent-space methods are inherently better at addressing the cold-start problem. However the paper does not compare against these methods. From the latter category EASE-r, RecWalk, SLIM, Mult-VAE, LightGCN are standard points of comparison, and a good place to start. Current experiments make it hard to place the contribution against the state-of-the-art."}, "questions": {"value": "- Eq 5 defines M as item x user with M_{iu} = 1 iff user u played item i. It is not stated that M is built only from training interactions. Is it? If not S has access to \"future information\", artificially inflating results. Please clarify. \n- Appendix Table 5 shows an autoencoder input of only 2 features (\"(None, 2)\") with a latent size of 4, the claim \"dense, non-linear content representation\" is weak; an overcomplete autoencoder on 2D input offers little representational power and may even be worse than simple one-hot/embedding lookups. Please explain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Pf9pXQptuv", "forum": "B3jlwFJ2mb", "replyto": "B3jlwFJ2mb", "signatures": ["ICLR.cc/2026/Conference/Submission24684/Reviewer_MJ2E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24684/Reviewer_MJ2E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042904382, "cdate": 1762042904382, "tmdate": 1762943162369, "mdate": 1762943162369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}