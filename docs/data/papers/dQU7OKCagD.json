{"id": "dQU7OKCagD", "number": 3184, "cdate": 1757354239752, "mdate": 1759898103230, "content": {"title": "EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients", "abstract": "Diffusion-based large language models (dLLMs) generate tokens through iterative denoising, but answers often stabilize before all denoising steps are completed.\nWe introduce EDIT (Early Diffusion Inference Termination), an inference-time method that adaptively stops the denoising process once reasoning stability relative to training behavior is detected.\nEDIT is built on training-gradient dynamics, typically otherwise discarded after training, where, during fine-tuning, AdamW-aggregated LoRA updates encode parameter importance signals.\nWe retain this information as compact reasoning maps.\nDuring inference, EDIT measures alignment between token activations and these maps, detecting convergence when KL divergence across consecutive steps on unmasked (visible) tokens falls below a threshold. \nOn reasoning benchmarks, EDIT reduces diffusion steps by 11.8–68.3\\% while preserving or improving accuracy in most cases, with negligible storage overhead ($\\sim$0.02\\%, about 1.5–2 MB for all QKV modules in a 32-block, 8 GByte model).\nThese results establish a principled mechanism for transforming knowledge about training-gradient dynamics into practical test-time benefits such as reducing reasoning time.", "tldr": "EDIT uses training-time metadata to enable early termination during inference in diffusion language models, reducing cost while maintaining or improving accuracy.", "keywords": ["diffusion language models", "early termination", "adaptive inference", "training metadata", "parameter importance through AdamW trajectory", "LoRA", "reasoning benchmarks"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ec45b42e72cfbf5038eebd5c53281c73cb8bbc0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a mechanism to early-stop the diffusion process in a dLLM.  Basically, we collect information during LoRA fine-tuning about which dimensions of the query output projection (in the last layer of the Transformer) get the most consistent gradients, as per updates to the LoRA B matrix.  Then, during inference, we look at the dimensions of unmasked tokens in the representations after the same query projection, and see if they are aligning with the FT gradients.  If they are, we say that \"reasoning stability\" has been achieved, and we stop the diffusion process.  Some experiments assess whether this can really save diffusion steps on downstream tasks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "It's well-motivated to try to make use of the gradient information that was collected during training (or fine-tuning) and re-use that during inference.  Diffusion seems like a great place to start.\n\nI think even the specific idea to use gradient momentum info from reasoning-based SFT to see if generation has arrived at a \"reasoning\"-like solution is not necessarily a bad idea.\n\nStudying diffusion LLMs are also a very wide-open space so testing dLLMs like LLaDA-8B on downstream tasks, and finding out what the weak points are, is good to do.\n\nGood job to have a discussion of limitations (in the conclusion), however brief."}, "weaknesses": {"value": "Overall, the approach here just feels very premature, like, as a practitioner, I am really unsure whether simpler things would work, or whether other choices could unlock much bigger gains.  There's not a lot of theory or empirical rigour behind the findings, e.g., what if we used an Oracle and stopped at the optimal number of steps in each case, what are the maximum speedups that we could get, or the best improvements in quality?  It's just not well-scoped in the current submission.  This lack of rigor and testing of alternative ideas, plus a lack of comparison to baselines, and some questionable experimental decisions, makes me think this paper is not yet valuable to the community.\n\nFrom my perspective, the scope of other things that COULD be tried is very large, and it's not clear how important the choices made in the paper actually are:\n- What about just using the parameters themselves, or momentum itself, or just the evolution tensor, rather than the evolution tensor after the reduction?\n- Why just the Query projection?  Why just the last layer?\n- Why Cosine versus other measures?  Why KL divergence versus other measures?\n- What if we didn’t ensure stability with Ω consecutive scores below δ, but did something else?\n\nThe current method introduces new task-specific (!) hyperparameters:\n- δ, Ω, τblk\n- Fundamentally, I don’t think we should be tuning hyperparameters on specific downstream tasks, as this confounds model comparison – how can we compare to models that weren’t fine-tuned, e.g., on the “Countdown” task?  Update: actually, this paper does make this mistake.\n- Did we tune the number of diffusion steps on each task as well for the baselines?  It seems like you just compare at different max denoising steps (set depending on sequence length), but the EDIT count is below the smallest that you tested, so it makes me wonder if fewer steps would be better for these other ones.\n\nSoundness:\n- The fact we didn’t compare to any other methods of early stopping for denoising… this really surprised me.  Like, accepting and freezing unmasked tokens when their probability crosses a threshold?  You mentioned “output stability or confidence” and \"entropy\" so why not test those?  What's the downside?\n- I mean, the fact we get different (and often better) accuracy numbers is surprising to me, and seems to reflect some kind of bias in the trained models that can be alleviated through hyperparameter tuning, which was only done with EDIT.  Fundamentally, I would not expect more steps to impair a diffusion model, although I believe there is prior work showing accuracy does not increase monotonically in number of steps.  Perhaps you should have plotted accuracy versus steps for the baselines for Countdown and then if EDIT is below this curve, that would alleviate some of my concerns.\n\nNitpicks:\n- I think since the fundamental idea of this paper is that SFT reveals reasoning patterns in the activations, you should really explain the SFT dataset in more detail, right?\n- Oh man, Tables 1 and 2, those are quite tiny fonts!  Is there no limit to how small we’d make them???\n- Prior work specifically using momentum to identify important params: Dettmers - Sparse Networks from Scratch - Faster Training without Losing Performance - 1907.04840v2\n- You know, diffusion itself does allow “guidance” in the form of gradients to be applied during the generative process.  I think maybe you could link your pseudo-gradients to this theory a bit better."}, "questions": {"value": "- When we convert alignment scores into a probability distribution, what is it a distribution over?   There are many probabilistic arguments made about this distribution, but I don’t really understand, e.g., what domain the “support” of this distribution lives in.\n- Is full-parameter SFT or CPT or even RL a limitation of your method, or just a limitation of your experimental evaluation?  What about other optimizers, e.g., Muon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MfACJTZKFH", "forum": "dQU7OKCagD", "replyto": "dQU7OKCagD", "signatures": ["ICLR.cc/2026/Conference/Submission3184/Reviewer_uwNg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3184/Reviewer_uwNg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761508425634, "cdate": 1761508425634, "tmdate": 1762916588981, "mdate": 1762916588981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **EDIT (Early Diffusion Inference Termination)**, an adaptive early inference termination rule for diffusion LLMs (dLLMs). During SFT with LoRA on Q/K/V, the method aggregates AdamW update statistics into a compact, feature-aligned “AdamW evolution” vector ($u$) that encodes which parameters consistently carried the learning signal. At inference, EDIT computes per-step cosine similarities between token activations and $u$, and halts denoising for tokens once a stability condition is met. The authors provide extensive experiments supporting the claim that training dynamics can guide safe early termination."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Insightful diagnostics.** Gradient-based “pseudo-gradient” alignment with SFT gradients; domain-wise breakdowns (e.g., GPQA subdomains) and LoRA-A vs. LoRA-B sparsity analyses justify design choices.\n* **Practical gains with tiny overhead.** Reported step reductions up to ~68% and a ~1.5 MB metadata footprint for a 32-block model are compelling for deployment."}, "weaknesses": {"value": "* **Metadata extraction.** EDIT relies on training-time metadata extraction. Many released checkpoints does not expose training recipe, does the selection of training recipe (dataset, hyperparam) affect extraction.\n* **Scope of validation.** Experiments center on a single dLLM family (LLaDA-8B) and five reasoning tasks on one hardware stack. It’s hard to assess robustness across models, sizes, datasets. \n* **Task-tuned thresholds.** ($\\delta,\\Omega$) are selected per task via validation for an accuracy/steps trade-off. This introduces tuning burden and potential brittleness under distribution shift. \n* **Poor presentation** Figures and tables are too small comparing to captions, such as Figure 3/4/5, Table 1/2. Certain figures in the paper have strange frames, such as Figure 2/4/8. The theory seems to be ad-hoc."}, "questions": {"value": "* **Baselines & fairness.** How does EDIT compare to strong adaptive early inference termination rule for dLLMs under identical compute budgets—and to autoregressive early-exit baselines on similar tasks? **The paper only compare EDIT with plain baseline, However, a lot of dLLM acceleration methods already exists**, such as [1][2].\n* **Unclear Intuition** What is the intuition of $u$? what makes similarity between each token’s activation and the AdamW evolution vector important? The paper does not provide a clear intuition.\n* **Metadata extraction.** Does SFT training recipe affects the performance of EDIT? How does performance change under domain shift from the SFT distribution?\n* **Adaptive calibration.** Can $(\\delta,\\Omega)$ be set online *without* task-level validation?\n\n[1] Wu C, Zhang H, Xue S, Liu Z, Diao S, Zhu L, Luo P, Han S, Xie E. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv:2505.22618. \\\n[2] Li P, Zhou Y, Muhtar D, Yin L, Yan S, Shen L, Liang Y, Vosoughi S, Liu S. Diffusion language models know the answer before decoding. arXiv:2508.19982. \\\n[3] Ben-Hamu H, Gat I, Severo D, Nolte N, Karrer B. Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking, NeurIPS 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "49AXqvdrQP", "forum": "dQU7OKCagD", "replyto": "dQU7OKCagD", "signatures": ["ICLR.cc/2026/Conference/Submission3184/Reviewer_3GSH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3184/Reviewer_3GSH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637813984, "cdate": 1761637813984, "tmdate": 1762916588816, "mdate": 1762916588816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EDIT, an inference-time early termination rule for diffusion LLMs that reuses training-time optimizer information. During SFT with LoRA, the method aggregates AdamW moments into a compact “AdamW evolution” vector $u$ that encodes parameter-importance patterns; at inference it measures cosine alignment between token activations and $u$, converts these to per-step distributions on visible tokens, and stops when matched-support KL divergence stays below a threshold for $\\\\Omega$ consecutive steps. The theory shows a multi-step control bound $TV \\\\le \\\\Omega\\\\sqrt{\\\\delta/2}$ and a margin condition that preserves the argmax, yielding PAC-style certificates for chosen $(\\\\delta,\\\\Omega)$. Empirically, on LLaDA-8B across Countdown, Sudoku, MATH500, GSM8K, and GPQA, EDIT reduces denoising steps by 11.8% to 68.3% with comparable accuracy on most tasks; storage overhead is about 0.02% of an 8 GB model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear definition of the AdamW evolution signal and why LoRA-B is preferred, with sparsity metrics and visualizations.\n* Matched-support KL and multi-step TV bounds yield simple certificates and a PAC-style calibration rule.\n* Minimal storage and implementation overhead, with complexity lower than attention and integration as a wrapper at inference.\n* Certified early stop rates reported across tasks, indicating practical realizations of the theory."}, "weaknesses": {"value": "* Hyperparameter selection uses per-task validation sets and a grid over $(\\\\delta,\\\\Omega)$; robustness to mis-tuning or cross-task portability is not thoroughly analyzed.\n* GSM8K at length 512 shows a noticeable accuracy drop with EDIT, which deserves a short diagnostic beyond the brief discussion. Minor."}, "questions": {"value": "* Please report end-to-end system metrics for one representative task and length (e.g., Countdown@256): wall-clock per instance, tokens per second, and peak GPU memory, alongside the step reductions already shown in Table 2. This is a light logging change.\n* For GSM8K at length 512, please add a short diagnostic: either (i) a histogram of early-stop step vs. correctness, or (ii) a 2×2 grid over $(\\\\delta,\\\\Omega)\\\\in\\\\{0.05,0.1\\\\}\\\\times\\\\{6,12\\\\}$ reporting accuracy and mean denoising steps. One figure or a small table is sufficient."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "itYHj9WLEN", "forum": "dQU7OKCagD", "replyto": "dQU7OKCagD", "signatures": ["ICLR.cc/2026/Conference/Submission3184/Reviewer_GDvF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3184/Reviewer_GDvF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989193988, "cdate": 1761989193988, "tmdate": 1762916588620, "mdate": 1762916588620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EDIT to speed up diffusion-style large language models. The key idea is to reuse training-time optimizer dynamics that are normally discarded: aggregated AdamW statistics on LoRA-B are compressed into a compact per-block “pathway vector” u. During inference, EDIT measures how well current visible-token activations align with u, constructs a distribution over the intersection of visible tokens across consecutive steps, and tracks a matched-support KL divergence. If the KL stays below a threshold δ for Ω consecutive steps, the method early-stops the diffusion process.\nIt provides theoretical support by bounding total-variation distance from KL and using a margin condition to argue that, with suitable (δ, Ω), early stopping will not change the final prediction. Empirically, EDIT reduces denoising steps by 11.8–68.3% on multiple reasoning benchmarks with little to no loss in accuracy. The approach requires only lightweight metadata extracted at training time, no architectural changes at inference, and minimal runtime overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The method is clear, novel and well-supported by theory.\n\n2.Without changing the model's reasoning structure, it enables the addition during training and no modification during inference, which differentiates it from previous efficiency-enhancing methods that require modifying the decoding or architecture.\n\n3.The paper is easy to read, motivations are well connected."}, "weaknesses": {"value": "1.The reliance on training metadata requires access to the optimization trajectory during the training phase, which is insufficient for scenarios involving closed-source models or those with only final checkpoint files.\n\n2.Validation focuses on LoRA-SFT, it remains unclear how EDIT performs with full-parameter finetuning, other adapters, or different optimizers \n\n3.Small accuracy dips appear in longer sequences, and the paper does not deeply analyze when EDIT stabilizes too early."}, "questions": {"value": "1.Have you tried this method on other optimizers and adapters?\n\n2.Under the influence of prompt form changes, noise disturbances, or adversarial interventions, is it more likely to prematurely stop? Have you ever encountered the situation of prematurely stopping?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g9SqENBkQg", "forum": "dQU7OKCagD", "replyto": "dQU7OKCagD", "signatures": ["ICLR.cc/2026/Conference/Submission3184/Reviewer_KgfN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3184/Reviewer_KgfN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990377002, "cdate": 1761990377002, "tmdate": 1762916588412, "mdate": 1762916588412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}