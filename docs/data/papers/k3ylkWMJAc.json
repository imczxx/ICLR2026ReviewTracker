{"id": "k3ylkWMJAc", "number": 14713, "cdate": 1758242327788, "mdate": 1759897353372, "content": {"title": "Self-Aware Reinforcement Learning for Improving LLMs with Minimal Data", "abstract": "Reinforcement learning (RL) has demonstrated potential in enhancing the reasoning capabilities of large language models (LLMs), but such training typically demands substantial efforts in creating and annotating data. In this work, we explore improving LLMs through RL with minimal data. Our approach alternates between the LLM proposing a task and then attempting to solve it. To minimize data dependency, we introduce two novel mechanisms grounded in self-awareness: (1) self-aware difficulty prediction, where the model learns to assess task difficulty relative to its own abilities and prioritize challenging yet solvable tasks, and (2) self-aware limit breaking, where the model recognizes when a task is beyond its capability boundary and proactively requests external data to break through that limit. Extensive experiments on nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra data demonstrate the efficacy of self-aware RL and underscore the promise of self-evolving agent training. Our code is available at https://anonymous.4open.science/r/SARL-B7FE/.", "tldr": "", "keywords": ["Large Language Model", "Reinforcement Learning", "Self-improving", "Self-evolution"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea41327a5c14c4efc734dff643439272ae0657ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces self-aware RL, a paradigm for improving large language models through reinforcement learning with reduced data requirements. The approach features two mechanisms: (1) self-aware difficulty prediction, where the model learns to estimate task difficulty relative to its current capabilities, and (2) self-aware limit breaking, where the model occasionally requests external guidance for high-utility unsolvable tasks. The authors train Qwen2.5-Coder-3B on code generation tasks and evaluate across various benchmarks in mathematical reasoning and code generation, reporting improvements on math benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a problem of reducing data dependency in LLM training.\n- The paper is well written and easy to follow."}, "weaknesses": {"value": "Please see my detailed questions and concerns below."}, "questions": {"value": "- What prevents reward hacking? The generator is rewarded for accurate difficulty prediction, but couldn't it hack this by generating only tasks of a narrow difficulty range where its predictions are accurate?\n- How sensitive is the method to the target difficulty level mentioned in the prompt (lines 633)? You explicitly tell the model to aim for difficulty 4-5, isn't this just supervised prompting rather than \"self-awareness\"?\n- Why use z-score normalization across the buffer? This makes the selection relative to recent tasks, but what if the buffer contains mostly easy or mostly hard tasks?\n- Why measure novelty using perplexity from the old policy $\\pi_{\\theta_{old}}$ rather than the current policy? Shouldn't novelty be relative to what the current model knows?\n- How do you ensure the generated tasks are valid, well-formed, and non-trivial? What quality control mechanisms exist beyond format checking?\n- How do you handle the distributional shift as the model improves? Early-generated tasks may become trivial. Do you discard old data or retrain on everything?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6RlnTo9mUF", "forum": "k3ylkWMJAc", "replyto": "k3ylkWMJAc", "signatures": ["ICLR.cc/2026/Conference/Submission14713/Reviewer_BYJc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14713/Reviewer_BYJc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552194854, "cdate": 1761552194854, "tmdate": 1762925076712, "mdate": 1762925076712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a self-aware reinforcement learning framework that allows an LLM to generate and solve its own tasks while using two mechanisms: 1) self-aware difficulty prediction, where the generator predicts task difficulty relative to the solver's ability; 2) self-aware limit breaking, where the model detects when it has hit the capability ceiling and requests external data for targeted improvement. Based on these two mechanisms, the paper designs reward functions and implement on REINFORCE++ algorithm. The paper evaluates the proposed framework with Qwen-Coder-3B model across 9 reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated and addresses an important problem of reinforcement learning for LLMs.\n2. The designs for difficulty prediction and adaptive external querying are reasonable and conceptually sound."}, "weaknesses": {"value": "1. It is unclear from Equation (10) how the solver and generator are updated; the equation should explicitly write out the terms for both the solver and the generator to clarify their respective objectives.\n2. The reward design for the generator seems too simple. Will the generator produce overly easy or overly difficult questions to maximize the difficulty prediction reward?\n3. The experiments are confined to a single model and single run. The reported improvements on code benchmarks, despite training on code data, are marginal. It is hard to determine whether the method is consistently or statistically effective.\n4. Lack of justification of choice of $\\gamma$.\n5. The paper writing should be improved, many typo and format errors are presenting, such as at line 273 and line 477."}, "questions": {"value": "1. What is the rationale for multiplying $R_{format}$ in equation (8) and equation (9)?\n2. In Figure 2, why does the training reward become negative, given that the defined rewards are non-negative? Additionally, it would be helpful to show separate curves for the format reward, outcome reward and difficulty prediction reward."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ddL52H9L9T", "forum": "k3ylkWMJAc", "replyto": "k3ylkWMJAc", "signatures": ["ICLR.cc/2026/Conference/Submission14713/Reviewer_4wUA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14713/Reviewer_4wUA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705337365, "cdate": 1761705337365, "tmdate": 1762925076332, "mdate": 1762925076332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel paradigm called Self-Aware RL, which allows the LLM to learn and improve with minimal external data by creating a self-task generation-solver design. To make sure the self-generated tasks are valid, this paper proposes two strategies to strengthen the usefulness of the generated samples. First, Self-Aware Difficulty Prediction: The LLM not only generates its own training tasks but also predicts the difficulty of that task relative to its current ability. It then learns to align this difficulty estimate with its actual success rate, enabling an adaptive curriculum that prioritizes tasks that are appropriately challenging, yet solvable (\"just right\"). Second, Self-Aware Limit Breaking: The model identifies its own capability limits and proactively seeks external guidance only when necessary to surpass its inherent capability limits."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper counts as a pioneer work on \"zero-data\" self-improving language models, directly comparing to Absolute Zero and showing decent performance improvement.\n\n2. They validate their approach on a good amount of downstream tasks spanning from math to coding, showing steady improvement and good generalization.\n\n3. They have a good cognitive science motivation behind by explicitly connecting LLM self-improvement to the concept of self-awareness provides a novel theoretical framing for future research into self-evolving agents."}, "weaknesses": {"value": "- The entire system's success depends on the LLM first being able to accurately gauge its own difficulty, and we all know LLMs tend to overestimate their confidence for task difficulties. A poor start in self-awareness could easily cause the RL process to settle on an unproductive local minimum. There is currently no mechanism to break this local minimum loop.\n\n- For the second component design, Self-Aware Limit Breaking, external knowledge guidance is used, which is basically a larger-sized LLM. However, this paper completely ignores a straightforward baseline where directly distilling a very small amount of data from this teacher model could yield competitive performance gain.\n\n- The paper is missing experimental comparison with some closely related literature.\n\nWang et al. Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning.Zhou \nZhou et all. Self-Challenging Language Model Agent\n\n\n- Some typos and grammar should be better polished (for example, line 223 \"self-aware self-aware RL\") and references need to be made more professional."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tbRygS0ad1", "forum": "k3ylkWMJAc", "replyto": "k3ylkWMJAc", "signatures": ["ICLR.cc/2026/Conference/Submission14713/Reviewer_cSb4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14713/Reviewer_cSb4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762264039865, "cdate": 1762264039865, "tmdate": 1762925075844, "mdate": 1762925075844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a RL framework to improve LLM reasoning with minimal data. In this self-improving framework for LLM reasoning where generator agent generates the tasks and predicts success rate and solver agent tries to solve the task and selectively queries a stronger external solver when valuable unsolved tasks are identified.\nTwo mechanisms are introduced:\n1. self-aware difficulty prediction to align task generation with model capability\n2. self-aware limit breaking where agent request external guidance for high-utility failures tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Practical and relevant direction toward data-efficient RL for reasoning models\n2. Strong empirical improvements on multiple math reasoning datasets. Evaluated on wide number of benchmarks."}, "weaknesses": {"value": "1. “self-aware” term is conceptually overstated. success-rate prediction alone does not fully justify the terminology.  the framework does not model its internal reasoning or belief states. A clearer positioning as capability-aware curriculum RL would improve conceptual accuracy.\n2. In limit-breaking claim, this paper does not introduce a new learning paradigm or theoretical formulation, it simply adds gating logic to decide when to query an external model. Any performance gains depend on the external model’s capability.\n3. Evaluation only compares against the base model and AZR, omitting other strong curriculum RL baselines.\n4. this framework only tested on qwen-3B model, why not other model families? Have the authors tested different scales or model families? How would the approach transfer to other model families (e.g., Llama, DeepSeek)?"}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6LNxLq7Znf", "forum": "k3ylkWMJAc", "replyto": "k3ylkWMJAc", "signatures": ["ICLR.cc/2026/Conference/Submission14713/Reviewer_pJGc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14713/Reviewer_pJGc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762574736137, "cdate": 1762574736137, "tmdate": 1762925075090, "mdate": 1762925075090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses data‐efficiency in RL‐fine­tuning of large language models (LLMs). The authors propose a “self‑aware RL” framework in which an LLM alternately (i) generates tasks and predicts their difficulty (self‑aware difficulty prediction), and (ii) attempts to solve them, with a mechanism to detect when tasks are beyond current capabilities and ask for external guidance (self‑aware limit breaking). The idea is to minimize reliance on large curated datasets by letting the model generate its own curriculum and seek external data only when necessary."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The problem is timely: reducing dependence on curated data for reasoning-capable LLM fine-tuning is important.\n\n* The idea of alternating roles (task generation / solving) is conceptually appealing and aligns with recent trends in self-supervision and synthetic data generation.\n\n* The empirical results show non-trivial gains with relatively little extra data, which is promising for data-efficiency."}, "weaknesses": {"value": "* The proposed framework is essentially a form of adversarial or dual-role training (generator and solver), yet the paper lacks analytical treatment of convergence, stability, or whether the iterative procedure reliably improves rather than degenerate (e.g., the generator produces trivial tasks, the solver overfits, task difficulty drifts). There is no deeper theoretical justification or monitoring of regime stability.\n* My own experience (and known community experience) is that even SOTA LLMs generating tasks often yield unstable data (invalid tasks, unsolvable ones, or trivial ones). The paper does not sufficiently discuss how data quality is controlled, how “ground truth” solutions are validated (especially when the same model solves its own generated tasks), or how noisy/invalid samples are filtered. This raises questions about how much of the reported improvement is due to good synthetic data vs incidental or artifact effects.\n* The “multiple model roles” (generator + solver + possibly verifier) increases complexity. The paper does not sufficiently discuss the scaling behaviour (model sizes, compute budget, how many rounds, etc.). It is unclear whether this method is practical for much more parameter models."}, "questions": {"value": "* Could you include training curves (e.g., generator loss, solver loss, number of generated tasks, accuracy of solver over time) to illustrate training dynamics and stability?\n\n* Is the method truly free of external data (i.e., “no external data” apart from initial model and minimal seeds)? If so, how is grounding / calibration ensured? If not, how much external data is needed and what is its role?\n\n* Please report compute / training cost: number of model parameters used, number of rounds of generator/solver, GPU hours, wall-clock time. What is the overhead compared to standard fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qPls4V1c5e", "forum": "k3ylkWMJAc", "replyto": "k3ylkWMJAc", "signatures": ["ICLR.cc/2026/Conference/Submission14713/Reviewer_jm8t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14713/Reviewer_jm8t"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14713/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762956826182, "cdate": 1762956826182, "tmdate": 1762956826182, "mdate": 1762956826182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}