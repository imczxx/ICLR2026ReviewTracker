{"id": "G0ettIR8yv", "number": 14848, "cdate": 1758244667283, "mdate": 1759897345657, "content": {"title": "ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning", "abstract": "Video understanding is inherently intention-drivenâ€”humans naturally focus on relevant frames based on their goals. Recent advancements in multimodal large language models (MLLMs) have enabled flexible query-driven reasoning; however, video-based frameworks like Video Chain-of-Thought lack direct training signals to effectively identify relevant frames. Current approaches often rely on heuristic methods or pseudo-label supervised annotations, which are both costly and limited in scalability across diverse scenarios. To overcome these challenges, we introduce ViaRL, the first framework to leverage rule-based reinforcement learning (RL) for optimizing frame selection in intention-driven video understanding. An iterated amplification strategy is adopted to perform alternating cyclic training in the video CoT system, where each component undergoes iterative cycles of refinement to improve its capabilities. ViaRL utilizes the answer accuracy of a downstream model as a reward signal to train a frame selector through trial-and-error, eliminating the need for expensive annotations while closely aligning with human-like learning processes. Comprehensive experiments across multiple benchmarks, including VideoMME, LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior temporal grounding performance and robust generalization across diverse video understanding tasks, highlighting its effectiveness and scalability. Notably, ViaRL achieves a nearly 15% improvement on Needle QA, a subset of MLVU, which is required to search a specific needle within a long video and regarded as one of the most suitable benchmarks for evaluating temporal grounding.", "tldr": "", "keywords": ["Video Understanding", "Temporal Grounding", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38e93e9caf1391241035beef3f598a49b8b1583f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the challenge of intention-driven video understanding, focusing on selecting relevant frames in videos to answer queries effectively. The authors propose ViaRL, a novel framework that uses rule-based RL to optimize frame selection by leveraging the answer accuracy of a downstream MLLM as a reward signal. This trial-and-error learning approach eliminates the need for expensive annotations and mimics human-like learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper presents a novel framework, ViaRL, which leverages rule-based reinforcement learning to optimize frame selection in video understanding tasks. Central to the approach is the Visual Iterated Amplification training strategy, an innovative iterative refinement process that alternates between optimizing the frame selector and the answer model, providing strong motivation and technical soundness. The effectiveness of ViaRL is demonstrated through comprehensive experiments on several challenging benchmarks, including VideoMME, LVBench, and MLVU, where it consistently achieves improvements. Given the difficulty of the tasks and the good results, I believe that reinforcement learning policy and the cyclic training is really helping to reason on the temporal axis so that it can select the most important frames. Very interesting work."}, "weaknesses": {"value": "The major weaknesses of this work are the following:\n\n1 - It would be nice to see how the method affects other VLMs which are not flexible on the resolution/quality of input data.\n\n2 - While the method performs really well in one of the most challenging problems of video understanding, it lacks comparison in more generic tasks like answer generation (for Q&A and captioning for example) to the the impact of this 'specialization' on other capabilities of the network.\n\n3 - The ablation on cyclic training maybe needs a bit more deepening on the diminishing returns with additional cycles. The authors claim that it is due to the imperfect nature of MLLMs to provide correct answers for each visual scene and the limited info contained in 8 frames. I think these claims might need some more experimenting to see if the limit is on the method or on the MLLMs serving as supervisors."}, "questions": {"value": "Check above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YD6eFhHSlY", "forum": "G0ettIR8yv", "replyto": "G0ettIR8yv", "signatures": ["ICLR.cc/2026/Conference/Submission14848/Reviewer_4Vp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14848/Reviewer_4Vp9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761348879684, "cdate": 1761348879684, "tmdate": 1762925203409, "mdate": 1762925203409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ViaRL (Visual Iterated Amplification Reinforcement Learning), a novel framework to address the challenge of efficient, query-driven frame selection in long videos. The authors argue that direct video reasoning in MLLMs is less effective than first mastering temporal grounding . The framework uses a cyclic, two-stage \"Visual Iterated Amplification\" training strategy: first, the Selector is improved via RL, and second, the Answer Model is instruction-tuned using the improved Selector's frame selections, creating a feedback loop where both models progressively enhance each other. Experiments show significant gains, especially a nearly 15% improvement on the Needle QA temporal grounding benchmark"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The training details are explained in detail. It improves the reproducibility of the paper."}, "weaknesses": {"value": "- Unclear Inference Cost: The paper motivates its approach by citing the high cost of processing all frames. However, the proposed ViaRL framework requires two sequential MLLM forward passes at inference time\n- From my perspective, the proposed paper lacks the technical novelty. Compared to existing GRPO-based works, the different part is to introduce frame selection before question answering. However, there have been multiple works that solve question-answering tasks with the frame selection. \n- I'd like to see the contribution of the frame selection trained by reinforcement learning with verified rewards. It would be better if the paper included the performance comparison of the frame selection with other frame selection methods and the impact of them to the downstream tasks."}, "questions": {"value": "- The current reward system is fundamentally tied to MCQ benchmarks . I wonder if the proposed ViaRL still works well on the open-ended generative tasks such as open-ended QA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5mSJRNe9hh", "forum": "G0ettIR8yv", "replyto": "G0ettIR8yv", "signatures": ["ICLR.cc/2026/Conference/Submission14848/Reviewer_KBUt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14848/Reviewer_KBUt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967710968, "cdate": 1761967710968, "tmdate": 1762925202884, "mdate": 1762925202884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced ViaRL, a framework to use rule-based RL to optimize the frame selection process in video understanding tasks. It is an iteration process is used in the CoT process and ViaRL use the accuracy of downstraming model as the reward signal. In more detail, they first processed a video understanding dataset by using CLIP to sample frames based on visual-textual similarity and then filter out less informative tasks. They using reinforce++ to finetune Qwen model to select frames and using another answer model to provide training signal. Each model is tuned in turn to help each other. \nThey demonstrated good performance on popular video understanding benchmark including VideoMME, LVBench, and MLVU."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provided a RL framework for temporal grounding without human annotation. By iteratively refine the selector and the answer model on the training set, the performance of Qwen is improved.\n2. The paper also provided useful tricks. For example, using idea from existing work to mark the frame index in the frame corner.\n3. The author achieved great performance on Qwen model."}, "weaknesses": {"value": "1. The RL is only tested on Qwen-2.5-VL, therefore it is hard to know if the method could generalize to other models.\n2. In the data preparation process, CLIP is used to sample relevant frame to the question, which could be inaccurate. CLIP is measuring the semantic similarity between the frame and the answer, while a frame could be barely connected with the question when it is alone but important when in frames context. I also do not see any experiments supporting this sampling process.\n3. Table 2 is never discussed in the paper. For example, why using all components yield the worse performance among the table?\n4. The author mentioned several RL rewards but only length reward is analyzed. Then it is not clear why the author use those rewards. Although they are proven useful in other tasks in other paper, it is not clear whether they help in this case."}, "questions": {"value": "1. In table 1, the bold number only represents higher number between Qwen and Qwen+ViaRL, but the authors did not mention this. The bold results are worse than many open-source MLLMs and Proprietary Models. This is a minor issue so I put it in questions section. Hope the authors could clarify this in the paper.\n\n2. Why choosing Qwen-7B as the answer model? Here I have several questions. 1. After improving, will the answer model achieve sota performance? If not, why not using existing sota model (open sourced or not) to provide the reward?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E49Y2LSmIP", "forum": "G0ettIR8yv", "replyto": "G0ettIR8yv", "signatures": ["ICLR.cc/2026/Conference/Submission14848/Reviewer_8prC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14848/Reviewer_8prC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984858275, "cdate": 1761984858275, "tmdate": 1762925202169, "mdate": 1762925202169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}