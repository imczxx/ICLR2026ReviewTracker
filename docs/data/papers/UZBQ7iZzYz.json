{"id": "UZBQ7iZzYz", "number": 9554, "cdate": 1758127263926, "mdate": 1759897712532, "content": {"title": "Adaptive Concept Discovery for Interpretable Few-Shot Text Classification", "abstract": "Few-shot text classification is a critical real-world task for which Large Language Models (LLMs) have shown great promise. However, their high inference costs and lack of interpretability limit their practical use. While Concept Bottleneck Models (CBMs) offer an efficient and interpretable alternative, their reliance on training surrogate models makes them incompatible with few-shot scenarios. \nTo bridge this gap, we introduce a novel CBM paradigm that relies solely on sample-concept similarity to make predictions.\nWe ensure the effectiveness of our concepts through a prototypical-discriminative dual-level architecture and a dynamic concept refinement mechanism. Extensive experiments show that with as few as 10 training samples, our method surpasses prior CBMs and even achieves performance comparable to LLMs. The code is available at https://anonymous.4open.science/r/StructCBM-EB1E.", "tldr": "", "keywords": ["concept bottleneck models; few-shot text classification;"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab220826c3651c5578d58abeff68e724cab0d8ea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an interpretable few-shot text classification framework based on a Concept Bottleneck Model (CBM) paradigm. The approach introduces a separation of concepts into prototypical and discriminative sets, and leverages Large Language Models (LLMs) to dynamically generate concept proposals for both. Furthermore, a two-stage prediction process is proposed, consisting of prototype pruning followed by re-ranking of discriminative concepts. The overall framework aims to improve interpretability and adaptability in few-shot scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clarity and readability: The paper is well written and easy to follow, with a logical presentation of motivation, methodology, and results.\n\n2. Novel paradigm: The use of LLMs to dynamically generate and structure concepts into two interpretable sets (prototypical vs. discriminative) is interesting and aligns with the goal of interpretable few-shot learning.\n\n3. Interpretable design: The combination of concept separation and the two-stage inference mechanism provides a clear and interpretable reasoning process."}, "weaknesses": {"value": "1. Over-reliance on LLM-generated concepts:\nThe proposed approach heavily depends on the reliability of concepts generated by LLMs. Given the limited few-shot data, the LLM-generated concepts and the closed-loop feedback mechanism may introduce noise or bias. The poor performance on SST-2 (Table 3) supports this concern.\n\n2. Limited evaluation scope:\nThe experimental comparison is restricted to only two white-box baselines. It would strengthen the paper to include more interpretable or concept-based baselines, especially those already listed in Table 1.\n\n3. Lack of discussion on few-shot text classification challenges:\nFew-shot text classification is inherently ill-posed, and LLMs are known to perform well in such settings even without explicit concept modeling. The experimental results from black-box methods reinforce this observation, but the paper lacks a sufficient discussion or justification on why the CBM approach is preferable in this context.\n\n4. Insufficient evaluation diversity:\nExperiments are conducted only on the 10-shot setting. Including 1-shot and 5-shot results would provide a more comprehensive understanding of the model’s robustness under varying data constraints.\n\n5. Unclear concept definitions:\nThe definitions of prototypical and discriminative concept sets remain somewhat abstract. Clearer explanations and concrete examples (e.g., illustrating how a sentence or phrase maps to each set) would greatly improve the reader’s understanding of how the model differentiates between the two concept types."}, "questions": {"value": "1. Can the authors provide examples to illustrate the distinction between prototypical and discriminative concept sets?\n\n2. How does the model handle unreliable or irrelevant concepts generated by the LLM, especially under few-shot conditions?\n\n3. Could the proposed method generalize to fewer samples (e.g., 1-shot or 5-shot)? If not, what modifications would be necessary?\n\n4. Minor: How many classes are there for each task used in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2Sa6qzX2l8", "forum": "UZBQ7iZzYz", "replyto": "UZBQ7iZzYz", "signatures": ["ICLR.cc/2026/Conference/Submission9554/Reviewer_mVk8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9554/Reviewer_mVk8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760640535115, "cdate": 1760640535115, "tmdate": 1762921112555, "mdate": 1762921112555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "StructCBM is a novel Concept Bottleneck Model (CBM) framework designed to overcome the high inference costs and \"black-box\" nature of Large Language Models (LLMs) in few-shot text classification. StructCBM introduces a new CBM paradigm that relies solely on sample-concept similarity for prediction, enabling a lightweight, LLM-free inference process. Experiments show that StructCBM surpasses prior CBMs and achieves performance comparable to LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The StructCBM framework introduces a novel paradigm for Concept Bottleneck Models (CBMs) that addresses the primary limitations of Large Language Models (LLMs)—high inference costs and lack of interpretability—while overcoming the challenges faced by prior CBM approaches in few-shot learning scenarios. The method achieves performance comparable to powerful black-box LLMs. Notably, on semantically rich datasets like AGNews, StructCBM surpasses the 10-shot Deepseek-ICL baseline, and on MedAbs, its performance is comparable to the zero-shot Deepseek-Direct."}, "weaknesses": {"value": "1. The paper clearly emphasizes that StructCBM's strength lies in its LLM-free, lightweight inference process. However, the pairwise formulation of Discriminative Concepts (Cd) scales quadratically with the number of classes, which is noted as a significant challenge for tasks with large label sets.\n\n2.The final step involves fine-tuning the embedding model e(⋅) to improve matching performance. Crucially, the fine-tuning data P (Equation 5) is constructed only using the prototypical concepts (Cp) and associated samples, explicitly excluding Cd.  More ablation study is suggested to compare performances when Cd was included in the fine-tuning set."}, "questions": {"value": "1. Regarding the quadratic scaling of Cd, the paper suggests exploring \"on-demand concept generation\" for only the most confusable class pairs as a solution. How would StructCBM implement this on-demand concept selection or generation in a practical, real-world inference scenario? Does identifying the \"most confusable\" pairs inherently require a costly preliminary analysis that might undermine the overall efficiency gain?\n\n2.The Concept Tuning stage (generate-predict-refine workflow)—is critical for discovering higher-quality concepts. Yet, the ablation study showed that this refinement process negatively impacted performance on the semantically sparse SST2 dataset, which was hypothesized to be due to overfitting. So what practical criteria or internal heuristics were considered or could be developed to automatically detect when concept refinement, particularly the Semantic Tuning process (Algorithm 1), is beginning to induce overfitting on semantically poor data (like SST2)? Could a metric reflecting concept redundancy or semantic density be incorporated into the refinement trigger to ensure robustness across diverse datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p1S2yj1gVk", "forum": "UZBQ7iZzYz", "replyto": "UZBQ7iZzYz", "signatures": ["ICLR.cc/2026/Conference/Submission9554/Reviewer_GAko"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9554/Reviewer_GAko"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844775520, "cdate": 1761844775520, "tmdate": 1762921112247, "mdate": 1762921112247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents StructCBM, an LLM-augmented Concept Bottleneck Model (CBM) for interpretable few-shot text classification. Unlike traditional CBMs that require large datasets and trained classifiers, StructCBM predicts labels through sample–concept similarity, removing the need for parametric training. It introduces a dual-level concept structure: prototypical concepts (Cp) capture representative class features for recall, while discriminative concepts (Cd) distinguish confusable classes for precision. A generate–predict–refine loop further improves concept quality through LLM-based semantic and logical refinements, while inference relies purely on embedding similarity without invoking an LLM. Experiments on SST-2, AGNews, MedAbs, and FinaQuery under a 10-shot setting show that StructCBM consistently outperforms prior CBMs such as TBM and CB-LLM, achieving performance close to black-box LLM baselines like DeepSeek-ICL, with the additional benefits of interpretability and computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- S1: The paper explores an underexamined intersection of interpretability and few-shot learning, where existing LLM-based CBMs have been ineffective.\n\n- S2: The two-stage architecture (prototypical vs. discriminative concepts) introduces a clear and interpretable reasoning process, akin to human recall–then–refine reasoning. The closed-loop refinement mechanism adds adaptivity and resilience in low-data regimes. Its non-parametric design avoids fine-tuning while maintaining transparency and efficiency.\n\n- S3: Experiments across diverse domains show consistent and significant gains over interpretable baselines. The ablation and sensitivity studies clearly isolate each component’s contribution. The qualitative case study effectively demonstrates interpretability and transparent reasoning.\n\n- S4: Inference is entirely LLM-free, yielding substantial computational savings compared to other LLM-augmented CBMs. Each prediction is explicitly grounded in human-readable concepts, supporting transparency and trustworthiness."}, "weaknesses": {"value": "- W1: The pairwise Cd formulation scales quadratically with the number of classes, limiting applicability to large-label tasks. While acknowledged by the authors, there is no quantitative analysis of computational or memory overhead.\n\n- W2: Although inference is lightweight, concept generation and refinement heavily depend on LLM capabilities. The paper does not assess the impact of different LLMs (e.g., GPT-4 vs. DeepSeek-V3), leaving robustness across models unclear. \n\n- W3: The evaluation is restricted to 10-shot experiments, without studying performance trends as data increases. Some baselines (e.g., TBM, CB-LLM) may be under-optimized for few-shot scenarios, possibly skewing comparisons. Interpretability is validated only qualitatively, with no quantitative concept fidelity or human evaluation.\n\n- W4: The generate–predict–refine loop lacks analysis of computational cost, convergence, and stopping criteria, leaving uncertainty about deployment feasibility.\n\n- W5: The paper claims that code is released at an anonymous link, but the provided URL returns “The requested file is not found.”"}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KJM5HPAhUa", "forum": "UZBQ7iZzYz", "replyto": "UZBQ7iZzYz", "signatures": ["ICLR.cc/2026/Conference/Submission9554/Reviewer_swGd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9554/Reviewer_swGd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885669694, "cdate": 1761885669694, "tmdate": 1762921111825, "mdate": 1762921111825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes StructCBM, a novel framework for interpretable few-shot text classification that leverages an adaptive concept library. The core idea is to decompose the classification task into a two-stage process: (1) generating and maintaining a library of human-readable prototypical (positive) and discriminative (negative) concepts for each class using a Large Language Model (LLM), and (2) making predictions by measuring the similarity between an input sample and these concepts. The framework's key innovation lies in its closed-loop \"Concept Tuning\" mechanism. This mechanism uses misclassified samples as feedback to iteratively refine the concept library through Semantic Tuning (rewriting existing concepts to fix recall or ranking errors) and Logical Tuning (generating entirely new concepts for hard samples). Finally, the framework fine-tunes an embedding model to better align sample and concept representations. The authors evaluate StructCBM on four diverse text classification benchmarks (SST2, AGNews, MedAbs, FinaQuery) in a 10-shot setting, demonstrating superior performance over other white-box and concept-based baselines while maintaining full interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-**Novel Adaptive Mechanism**: The Concept Tuning stage is the paper's standout feature. By using classification errors as direct feedback to improve the concept library, the framework creates a powerful closed-loop system that can self-correct and evolve, which is a significant step forward from static concept bottleneck models.\n\n-**Strong Empirical Validation**: The experiments are comprehensive, covering four distinct domains. The results clearly show that StructCBM outperforms other white-box and concept-based methods by a large margin, while remaining competitive with powerful black-box LLM approaches (Deepseek). This convincingly demonstrates the framework's efficacy.\n\n-**Preserved Interpretability**: The paper successfully achieves its primary goal of interpretability. Every prediction is grounded in explicit, human-readable concepts, and the tuning process itself is designed to improve the semantic quality of these concepts."}, "weaknesses": {"value": "-**Scalability to Many Classes**: The paper focuses on benchmarks with a relatively small number of classes (e.g., SST2 has 2, AGNews has 4). It is unclear how the concept library and the tuning process would scale to problems with dozens or hundreds of classes. The cost of maintaining and tuning a large number of prototypical and discriminative concepts per class could become prohibitive.\n\n-**LLM Dependency and Cost**: The framework relies heavily on an LLM for concept generation and rewriting. The paper does not discuss the computational cost or latency implications of these LLM calls, which is a critical factor for real-world deployment. It would be useful to know the number of LLM calls required per tuning cycle.\n\n-**Lack of Human Evaluation**: While the concepts are human-readable, the paper provides no human evaluation to assess their actual quality, coherence, or usefulness to an end-user. An expert or crowd-sourced study on the concepts' validity would strengthen the interpretability claim.\n\n-**Black-box Baseline Choice**: The black-box baselines (Deepseek-Direct/ICL) are strong, but it would be more compelling to also compare against a fine-tuned smaller model (e.g., a fine-tuned DistilBERT on the 10-shot data) to better isolate the benefit of the concept-based approach versus just using a more powerful LLM."}, "questions": {"value": "-**Scalability**: How does the runtime and memory footprint of StructCBM scale with the number of classes? Have you experimented with datasets that have a larger number of categories (e.g., 10+)? What are the bottlenecks?\n\n-**LLM Cost**: Can you provide an estimate of the number of LLM calls (for both initial generation and tuning) required per dataset? How sensitive is the final performance to the quality/size of the LLM used for concept engineering?\n\n-**Human Evaluation**: Have you conducted any user studies to validate that the generated and tuned concepts are indeed meaningful, non-redundant, and helpful for a human to understand the model's decisions? This seems crucial for a paper centered on interpretability.\n\n-**Ablation on Tuning**: The paper mentions evaluating the contribution of each component. Could you share the performance drop when the Semantic Tuning and Logical Tuning stages are ablated? This would help quantify the value of the core adaptive mechanism."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K9TFs2aB3O", "forum": "UZBQ7iZzYz", "replyto": "UZBQ7iZzYz", "signatures": ["ICLR.cc/2026/Conference/Submission9554/Reviewer_XLoV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9554/Reviewer_XLoV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907015126, "cdate": 1761907015126, "tmdate": 1762921111320, "mdate": 1762921111320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces StructCBM, a novel framework that successfully enables Concept Bottleneck Models (CBMs) to operate in few-shot text classification settings (e.g., 10-shot). The key innovation is a \"classifier-free\" paradigm that bypasses the need for a data-hungry trainable classifier, which is \"impossible\" to optimize with limited data. Instead, it relies on direct sample-concept similarity. This is operationalized via a dual-level \"prototypical-discriminative\" concept architecture and a \"generate-predict-refine\" loop where an LLM discovers and tunes concepts from a few samples. The method outperforms prior CBMs in the 10-shot regime and achieves performance comparable to black-box LLMs on semantically-rich datasets, all while being interpretable and efficient (LLM-free) at inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novelty: This work successfully and practically applies LLM-augmented CBMs to the few-shot learning domain. It directly addresses a clear gap in the literature, as prior CBMs either required large training sets (like CB-LLM) or costly LLMs at inference (like TBM).\n- Strong Empirical Performance vs. Baselines: The model outperforms prior interpretable CBM baselines (TBM, CB-LLM) in the 10-shot setting across all datasets.\n- Competitive with Black-Box LLMs: The method achieves performance comparable to, and even exceeding (on AGNews), strong black-box LLM baselines (Deepseek-ICL). It achieves this while being \"white-box\" interpretable and \"lightweight\" at inference.\n- Sound and Validated Methodology: The dual-level prototypical  and discriminative architecture  is well-motivated for its recall-then-precision task decomposition. This design is empirically validated by both the ablation study (Table 3, showing $\\mathcal{C}_d$ is crucial)  and the case study (Fig 4, showing $\\mathcal{C}_d$ correcting an initial error)."}, "weaknesses": {"value": "- Overfitting: The concept refinement loop, a key part of the method, is shown to be brittle. The ablation study (Table 3) reveals that the full model (with refinement) performs worse than a simpler version on the SST2 dataset, which the authors attribute to overfitting\n- Additional Parameters: The \"classifier-free\" model rests on the quality of a base embedding model (`all-mpnet-base-v2`), which is a critical, unanalyzed hyperparameter. Furthermore, the \"Embedding Model Finetuning\" step (Sec 3.5)  re-introduces a training phase, which complicates the \"classifier-free\" narrative and appears responsible for a massive performance jump on FinaQuery (Table 3).\n\nMinor:\n\n- There is some invisible text on Page 15."}, "questions": {"value": "1.  **On Embedding Model Sensitivity:** Your entire method relies on `sim(e(x), e(c))`. How sensitive is the model's performance to the choice of the base embedding model $e(\\cdot)$? Why was `all-mpnet-base-v2` chosen, and would a different model change the results?\n2.  **On Refinement Overfitting:** The refinement (Alg 1) appears to overfit on SST2. This seems due to the greedy, 1-sample error correction objective. Did you experiment with a more regularized tuning objective (e.g., one that forces a concept to remain similar to *all* its correct samples while becoming dissimilar to the one error sample)?\n3.  **Finetuning** The ablation study (Table 3)  shows that the \"Train\" step (Sec 3.5) provides a significant boost, especially on FinaQuery. How much does this finetuning of the embedding model contribute to the final performance, and does this reliance on a training step weaken the \"classifier-free\" premise?\n4.  **LLM Annotation Reliability:** The \"Concept Annotation and Selection\" step (Fig 2)  uses an LLM to score logical consistency. How reliable is this process? What is the impact of potential LLM annotation errors on the final concept library and performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KerKBSt23u", "forum": "UZBQ7iZzYz", "replyto": "UZBQ7iZzYz", "signatures": ["ICLR.cc/2026/Conference/Submission9554/Reviewer_BfVT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9554/Reviewer_BfVT"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762239345428, "cdate": 1762239345428, "tmdate": 1762921110061, "mdate": 1762921110061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}