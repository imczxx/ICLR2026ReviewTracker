{"id": "ZSal26DrNb", "number": 3198, "cdate": 1757360965480, "mdate": 1759898102511, "content": {"title": "A Neurosymbolic Agent System for Compositional Visual Reasoning", "abstract": "The advancement in large language models (LLMs) and large vision models has fueled the rapid progress in multi-modal vision-language reasoning capabilities. However, existing vision–language models (VLMs) remain challenged by compositional visual reasoning. This paper presents VLAgent, a neuro-symbolic approach to developing a Vision-Language Agent system for efficient compositional visual reasoning with three novel features. First, VLAgent develops an interpretable visualization-enhanced two-stage neuro-symbolic reasoning system. The first stage is managed by a front-end engine that generates a structured visual reasoning plan (symbolic program script) for each compositional visual reasoning task by utilizing a pre-trained LLM powered with few-shot chain-of-thought in-context learning. The second stage is managed by a high-performance back-end engine. It transforms the planning script into executable code based on visual input (image or video) and the combination of neural models and symbolic functions and then performs a sequence of actions for the compositional visual reason task. Second, to ensure and enhance the quality of mapping the logic plan to a sequence of executable instructions, VLAgent introduces the SS-parser, which examines the syntax and semantic correctness of the planning script, detects and repairs the logic errors found in the LLM-generated logic plan before generating the executable program. Third, VLAgent introduces the execution verifier in critical reasoning steps to validate and refine its compositional reasoning results in a stepwise manner, for example, ensemble methods for critical visual reasoning and caption analysis for low-confidence compositional reasoning. Extensive experiments were conducted on six visual benchmarks and compared to a dozen SoTA visual reasoning models. The results show that VLAgent outperforms existing representative approaches to compositional visual reasoning, while enabling self-interpretable visualization for human-in-the-loop debugging. Our code and runtime logs are available at https://anonymous.4open.science/r/VLAgent.", "tldr": "This paper presents a neuro symbolic approach to developing a vision-language agent for compositional visual reason.", "keywords": ["Neurosymbolic System", "Vision-Language Agent", "Compositional Visual Reasoning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bb9e41899955a42682fa89385c21606a0a9f18f1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses compositional visual reasoning by proposing a two-stage framework (VLAgent) that integrates structured reasoning with executable code generation. In Stage 1, an LLM uses few-shot and chain-of-thought (CoT) in-context learning to generate a structured logic plan describing the reasoning process. In Stage 2, this plan is converted into executable code that is run to produce the final reasoning output. The method includes an SS-Parser module that inspects, audits, and repairs the generated reasoning scripts, as well as an execution verifier to validate and refine the final outputs. Together, these modules aim to improve correctness and reliability in multimodal reasoning tasks involving both images and videos."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The authors propose a method for compositional visual reasoning that operates on images as well as videos, which shows improvements over existing methods. The method provides reasoning traces that improve the interpretability of the final output. \n- The two-stage pipeline and the introduction of an SS-Parser and verifier to check and refine outputs are interesting and relevant contributions.  \n- The proposed ensemble pruning is an interesting approach for robustness when dealing with multiple models outputs"}, "weaknesses": {"value": "- My main concern is the presentation of the method. It is very hard to follow the different modules of the method and especially understanding the architecture, as the methodology mainly gives insights on what the modules are supposed to do without going into technical details how exactly it is done, at least on a formal level (what are inputs and outputs of the modules).\n\t- The front-end part is only very briefly explained. There are no additional details on the task dispatcher, task-specific context loader, and visual reasoning planner/script generator/VL planner. What prompts are used for it? Are there additional steps performed besides prompting? Where is the context coming from? These are critical details that should be included in the methodology and appendix.\n\t- Figure 2 of the method is visually dense and hard to read, abstractions could help improve the clarity here. Figure 3 is hard to read as well if you do not zoom in. A Latex table would probably be better readable. Generally, it might be better to put it in the appendix, while some examples from it could be explicitly mentioned and explained in the paper to give the reader a general idea of it.\n\t- Instead of explaining what the SS-Parser is doing in Fig 4, it would be more helpful to understand the technical details of how it is done there. From chapter 2, it is not clear how the parts of the SS-Parsers are implemented. The algorithm in the appendix helps, but it's hard to understand if this is just the script parser or also the auditor and repair included. How is the script repair implemented? The clarity here should be improved, and more necessary details should be added to the main paper (instead of just the appendix), at very least referencing the relevant parts in the appendix.\n\t\n- The differences to previous visual programming systems (e.g., ViperGPT, VisProg) are not sufficiently discussed. A clearer articulation of novelty would strengthen the contribution.\n        \n- Some of the modules seem to have quite some computational overhead, especially when multiple models need to be called for the same task (model ensemble). This needs to be discussed in the paper, especially since some results are only marginally better than plain VLMs (Table 1)."}, "questions": {"value": "- What are the crucial parts in which your method is different from existing Visual Programming approaches like VisProg, ViperGPT?\n- What are the default models for the modules of your method? Which are used in the experiments?\n- When comparing to the baseline models, did you collect the results yourself, or did you take them from other papers or benchmarks? This should be clarified in the paper.\n- You compare your results to \"GPT-5\" and \"GPT-5-Thinking\" in some of the qualitative examples. Which setup are you referring to by that? Does it mean you collected the results via the web app? Or if you used the API, which parameters for reasoning effort did you use? \n- The notation in line 132 (“finetune LLM with in-context learning”) is misleading, this should be corrected, as in-context learning is not fine-tuning.\n- Many of the paper’s examples focus on the “same gender” task, which may introduce unnecessary sensitivity. It would be more convincing to include examples with more relevant and less potentially sensitive queries."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c1XZYldP4s", "forum": "ZSal26DrNb", "replyto": "ZSal26DrNb", "signatures": ["ICLR.cc/2026/Conference/Submission3198/Reviewer_GmpR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3198/Reviewer_GmpR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761132896845, "cdate": 1761132896845, "tmdate": 1762916595602, "mdate": 1762916595602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a specific pipeline for visual reasoning based on the idea of tool-calling and moderator \"agent\" modules. The authors compare their approach to several VLM and neuro-symbolic VLM approaches on altogether six different datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The general idea of a neuro-symbolic agent approach is valuable for the community also the experimental evidence seems to show improvements over baselines."}, "weaknesses": {"value": "However I find it very difficult to assess what exactly the core contribution or claim of this work is. The proposed VLAgent seems to incorporate so many different module that aren't properly described how they work overall, but also how they are implemented. This makes it quite hard to assess the significance of the results. E.g., if there are potentailly so many different models within VLAgent is the comparison to the other baselines fair? Perhaps the authors could specificy again what the core idea is of VLAgent and what the experiments show? I.e. what specific benefit do the experiments highlight?\n\nThe biggest issue for me is the missing or poorly described information on the method itself. E.g. what module is being trained, what is not? What are all of these modules based on? I still don't understand what these modules are, e.g., the task dispatcher? Even the section on the SS Parser (sec. 2.1) does not make it clear to me what the three submodules are and how they work, e.g. the Script Auditor?\n\nIn context of the previous remark: What is the underlying idea of this specific pipeline, i.e. what is the core idea of VLAgent?\n\nThe authors also only describe a few modules in detail as the argue due to space limits. I think this is tricky as missing this information makes it more tricky to understand the paper and its significance/contribution. I suggest compressing the text, as it is rather long at times, removing some of the long tables which don't seem to give much value or at least put the missing information in the appendix.\n\nThe figures are very difficult to read and follow. Most of them are in poor quality (low resolution) and contain far too small text.\n\nMinor: the paper is essentially written as one wall of text, which is quite difficult to read and follow. I suggest adding more paragraphs."}, "questions": {"value": "Fig1: I am not sure what the real benefit is of having figure 1 where it is without knowing much about VLAgent yet at that point.\n\nIn Fig 2: What does personalized LLM mean?\n\nWhat is the benefit of Figure 3? What am I supposed to get out of this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yK32iduRAG", "forum": "ZSal26DrNb", "replyto": "ZSal26DrNb", "signatures": ["ICLR.cc/2026/Conference/Submission3198/Reviewer_Zjb7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3198/Reviewer_Zjb7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643681622, "cdate": 1761643681622, "tmdate": 1762916593335, "mdate": 1762916593335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VLAgent, a two-stage neuro-symbolic vision–language agent for compositional visual reasoning. It combines LLM-based symbolic plan generation with executable neural–symbolic modules, featuring an SS-Parser for logic repair and an Output Verifier for stepwise validation. Experiments on six benchmarks show strong zero-shot gains and improved interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a well-structured neuro-symbolic framework that clearly separates planning and execution stages. This design improves modularity and interpretability, allowing each stage to be analyzed, debugged, and extended independently.\n\n- The proposed SS-Parser effectively detects and repairs syntax and logic errors in LLM-generated plans, addressing a common failure mode of LLM-based program generation,\n\n- The model achieves strong zero-shot performance across multiple visual reasoning benchmarks.\n\n- The approach provides interpretable, step-by-step reasoning that supports human-in-the-loop analysis. The visual scripts and execution traces make the reasoning process transparent, enabling diagnostic inspection and trustworthiness.\n \n- The ablation study convincingly shows the contribution of each component, including the SS-Parser and Output Verifier. The incremental performance gains validate the necessity and effectiveness of these modules rather than attributing success solely to scale."}, "weaknesses": {"value": "- The multi-stage architecture introduces additional computational and implementation complexity.\nEach step of planning, parsing, repairing, and verifying adds latency and engineering cost, which may limit real-time or large-scale deployment. Additional clarification on the computational cost would enhance the paper's clarity.\n\n- The evaluation is mainly limited to visual QA tasks, which restricts the demonstrated generality of the framework.\nBroader reasoning domains (e.g., robotics, text–image synthesis) would strengthen claims of general neuro-symbolic capability.\n\n- The approach heavily depends on large LLMs, and scalability to smaller or open-source models is not analyzed.\nThis dependency raises concerns about reproducibility, accessibility, and efficiency in resource-limited environments.\n\n- The paper claims interpretability benefits but does not provide quantitative measures or user studies to support them.\nWhile qualitative visualizations are shown, there is no formal metric or user evaluation to substantiate improved transparency or usability.\n\n- The related work is currently placed in the appendix, but it should be included in the main text to better contextualize and position the proposed approach within existing research."}, "questions": {"value": "- How do the authors quantitatively evaluate the claimed interpretability of VLAgent beyond qualitative visual examples?\n- What is the computational overhead introduced by the proposed approach compared to direct LLM reasoning?\n- How well does the proposed system generalize to non–visual QA domains or tasks beyond the evaluated benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YABsGFsvIa", "forum": "ZSal26DrNb", "replyto": "ZSal26DrNb", "signatures": ["ICLR.cc/2026/Conference/Submission3198/Reviewer_ZfUe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3198/Reviewer_ZfUe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857288331, "cdate": 1761857288331, "tmdate": 1762916592038, "mdate": 1762916592038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VLAgent, a neuro-symbolic framework designed for efficient compositional visual reasoning. The proposed system addresses three key limitations prevalent in existing methods:\n1. The generation of programs containing non-existent modules or logically flawed execution steps.\n2. Hallucination-induced issues, where outputs are ill-formatted or semantically incorrect.\n3. Overall performance being constrained by the weakest external module in the pipeline.\n\nTo overcome these challenges, VLAgent integrates three novel components:\n1. A visualization-enhanced two-stage framework that structures the reasoning process.\n2. An SS-Parser which scrutinizes the syntactic and semantic correctness of generated planning scripts.\n3. An execution verifier responsible for validating and iteratively refining compositional reasoning outcomes.\n\nExtensive evaluations on six diverse visual benchmarks demonstrate VLAgent's effectiveness, where it consistently surpasses a dozen state-of-the-art models, establishing a new benchmark in compositional visual reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Detailed evaluation.\ncompares VLAgent with representative image QA approaches in VLMs category and zeroshot methods on 4 popular ImageQA benchmarks.\nWe next evaluate the generalization capability of VLAgent on two recent video benchmarks.\ndetailed ablation study.\n\nThe paper conducts a comprehensive evaluation of VLAgent across multiple dimensions. \nFirst, VLAgent is compared against representative image QA approaches on four popular ImageQA benchmarks. \nSubsequently, the model's generalization capability is evaluated on two video benchmarks. \nFinally, a detailed ablation study is provided to quantify the contribution of each key component within the system."}, "weaknesses": {"value": "### Clarity & Motivation\n- The paper's organization hinders comprehension. The introduction primarily lists limitations and corresponding solutions without adequately explaining the underlying motivation—specifically, why the proposed two-stage framework and output verifier are effective in addressing the stated problems. This lack of conceptual linkage makes the argument feel incoherent. \n\n- Similarly, the abstract enumerates contributions but fails to articulate their core motivation.\n\n- Figure 2 contains excessive text and fails to visually emphasize the operation of the novel modules. \n\n### Technical Explanation & Evidence\nSeveral key technical claims lack sufficient explanation and empirical support.\n\n- Two-Stage Framework: The authors should clarify the mechanism by which the two-stage framework specifically mitigates the generation of non-existent modules and logically flawed steps. Also, demonstrative examples or visualizations tracing the refinement process would substantiate this claim.\n\n- Output Verifier: The role of the output verifier in alleviating the \"weakest external module\" bottleneck requires elaboration. Is the improvement achieved through a voting mechanism that automatically filters out inferior results? Visual evidence illustrating this filtering process and its impact on the final output would strengthen this point.\n\n### Performance\nA 5-second latency (Line 1077) appears long for practical applications."}, "questions": {"value": "In Line 274, it is mentioned that the verifier prunes a pool of N candidate models down to a small subset M (with M << N) for efficiency, and that P denotes the number of model calls required to evaluate an ensemble. \nWhat is the concrete values used for N, M, and P?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CYl7VkYDjj", "forum": "ZSal26DrNb", "replyto": "ZSal26DrNb", "signatures": ["ICLR.cc/2026/Conference/Submission3198/Reviewer_3Z8t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3198/Reviewer_3Z8t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930332841, "cdate": 1761930332841, "tmdate": 1762916590851, "mdate": 1762916590851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}