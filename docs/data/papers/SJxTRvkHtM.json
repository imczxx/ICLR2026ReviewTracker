{"id": "SJxTRvkHtM", "number": 19171, "cdate": 1758294096456, "mdate": 1759897054990, "content": {"title": "LO-SDA: Latent Optimization for Score-based Atmospheric Data Assimilation", "abstract": "Data assimilation (DA) plays a pivotal role in numerical weather prediction by systematically integrating sparse observations with model forecasts to estimate optimal atmospheric initial conditions for forthcoming forecasts. Traditional Bayesian DA methods adopt a Gaussian background prior as a practical compromise for the curse of dimensionality in atmospheric systems, which simplifies the nonlinear nature of atmospheric dynamics and can result in biased estimates. To address this limitation, we propose a novel generative DA method, LO-SDA. First, a variational autoencoder is trained to learn compact latent representations that disentangle complex atmospheric correlations. Within this latent space, a background-conditioned diffusion model is employed to directly learn the conditional distribution from data, thereby generalizing and removing assumptions in the Gaussian prior in traditional DA methods. Most importantly, we employ latent optimization during the reverse process of the diffusion model to ensure strict consistency between the generated states and sparse observations. Idealized experiments demonstrate that LO-SDA not only outperforms score-based DA methods based on diffusion posterior sampling but also surpasses traditional DA approaches. To our knowledge, this is the first time that a diffusion-based DA method demonstrates the potential to outperform traditional approaches on high-dimensional global atmospheric systems. These findings suggest that long-standing reliance on Gaussian priors—a foundational assumption in operational atmospheric DA—may no longer be necessary in light of advances in generative modeling.", "tldr": "", "keywords": ["data assimilation", "score-based model", "latent optimization"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/71b7def00d87c867216f2fa9034da910ac17c975.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "First of all, I would like to state that I have previously been randomly assigned to this paper for a prior conference as well. If there are some conflicts of interests here, I humbly request the area chairs to take this into account and/or remove the review if needed. I may reference some of my previous points if they were left unaddressed in this updated version of the manuscript.\n\n\nThe authors create a background-conditioned latent diffusion prior inside a VAE, such that they minimize using a variational method to optimize for the observations. During the reverse pass, they apply alternating latent optimisation steps that explicitly minimise the observation-error cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The model improves over other methods such as Repaint, DPS, and 3DVar-esque techniques. Seems to have a respectable improvement under 5% observations, but for 1% observations, the improvements over other methods seem marginal at best (given the error bars). Thank you for updating the manuscript with some uncertainty/error bars so we can more closely look at the trends!\n\nThey test against both 1% and 5% sparse observations conditions, demonstrating that the model works for both. However, the trend seems as though it might not scale well with higher sparsity scenarios (<1%) relative to the other methods.\n\nDue to optimizing in a latent space, the model should be more computationally efficient at assimilation for high-dimensional data. However, though this claim is made, there is no comparison of runtime with respect to traditional DA methods."}, "weaknesses": {"value": "The failure mode of repaint (compared to the background) is still not explored in this revised edition of the paper. Perhaps a small discussion would help strengthen the results by establishing the baseline in a more proper form.\n\nAs mentioned before, there is some limited novelty of the paper compared to recent diffusion-based data assimilation approaches such as APPA https://arxiv.org/abs/2504.18720, latent diffusion DA https://arxiv.org/abs/2406.14815, etc. Perhaps some discussion on the particular impacts of this work relative to some of the aforementioned papers would strengthen this paper.\n\nThe model seems to have a lot more variability due to noise uncertainty, as portrayed in Table 2, showing that it might not extrapolate to higher noise regimes. Is there a specific explanation as for such?\n\nAlso, I would still recommend adding a small note for the red and yellow highlights for first/second best."}, "questions": {"value": "I'm not actually sure whether the DA is performed repeatedly or only once after 48h after calculating the background? If so, how does the background distribution changing due to the data assimilation process itself actually affect the DA procedure later down the line? If not, then this would additionally need to be tested.\n\nWhat differentiates this model from previous score-based data assimilation techniques (papers listed above)? Additionally, have the authors have tried any of the previous latent diffusion data assimilation models as baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DFB3axKFVa", "forum": "SJxTRvkHtM", "replyto": "SJxTRvkHtM", "signatures": ["ICLR.cc/2026/Conference/Submission19171/Reviewer_ZLd6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19171/Reviewer_ZLd6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760469327302, "cdate": 1760469327302, "tmdate": 1762931177481, "mdate": 1762931177481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LO-SDA, a novel generative data assimilation method designed for numerical weather prediction. The key idea is to relax the Gaussian prior assumptions of traditional data assimilation by leveraging score-based generative modeling in a learned latent space. First, a variational autoencoder (VAE) is trained to encode high-dimensional atmospheric states into a compact latent representation. Then a diffusion model (score-based generator) conditioned on the background state is used to sample from an approximate posterior in this latent space. During the reverse diffusion process, the method applies latent optimization to enforce consistency with sparse observations, effectively nudging the generated latent state to agree with observed values.\n\nThe contributions include: (1) a new latent-space diffusion assimilation framework that can handle highly sparse observations by combining VAE compression, conditional diffusion sampling, and a novel observation-constrained optimization step; and (2) an experimental demonstration on an idealized global atmospheric dataset showing that this generative approach can match or surpass the accuracy of state-of-the-art variational methods. Notably, under extremely sparse observation coverage (only 1% of state variables observed), LO-SDA achieves comparable performance to a leading latent variational baseline (L3DVar). As observation density increases (e.g. 5% coverage), LO-SDA decisively outperforms the baseline, achieving about a 15% reduction in error (WRMSE). This is presented as the first instance where a diffusion-based data assimilation method demonstrates the potential to outperform traditional Gaussian-based methods at a global scale. The results suggest that generative modeling techniques, when properly constrained by observations, can overcome some limitations of classical methods and improve assimilation accuracy in challenging, high-dimensional settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The paper proposes a creative combination of deep generative modeling with data assimilation. Using a conditional diffusion model in a VAE latent space to perform Bayesian inference is a novel approach in the context of atmospheric data assimilation. The introduction of a latent optimization step to enforce observation consistency is an innovative twist that addresses a known issue in score-based filters (i.e., ensuring the analysis state actually fits the sparse observations).\n\nQuality: The methodology builds on recent advances in both data assimilation and generative modeling. The authors validate the approach on a high-dimensional global atmosphere simulation, which lends credibility that the method can scale to realistic scenarios. The experiments compare LO-SDA against appropriate baselines: a traditional 3D-Var and a learned latent 3D-Var (L3DVar) method, as well as other score-based sampling approaches (e.g., diffusion posterior sampling and the Repaint technique). The results show LO-SDA consistently performs well, particularly as observation density increases, and it outperforms both the traditional variational method and prior score-based methods in most settings. The paper also provides statistical significance analysis to support the improvements, adding rigor to the empirical findings.\n\nClarity: The paper is generally well-written and structured. It motivates the problem (limitations of Gaussian priors in DA) clearly and provides sufficient background on data assimilation and score-based models for readability. The algorithmic description of LO-SDA is detailed, and the roles of the VAE and diffusion model are explained with appropriate equations. Figures (if any, e.g. illustrating the model architecture or results) are clear. Overall, the authors do a good job making a complex method accessible to readers from both the machine learning and data assimilation communities.\n\nSignificance: If the claims hold, this work is quite significant for data assimilation research. It challenges the long-standing reliance on Gaussian assumptions by showing that a learned generative prior can surpass a state-of-the-art variational DA method on a large-scale problem. This suggests a potential paradigm shift in how operational weather forecasting systems could incorporate machine learning. Moreover, LO-SDA’s strong performance under extremely sparse observations is important—sparse observational data is a common scenario (e.g., in parts of the globe or for certain variables), and methods that can handle such settings are highly valuable. By demonstrating an approach that remains accurate with as low as 1% observation coverage, the paper addresses a critical challenge in high-dimensional Bayesian filtering. The techniques introduced here (latent-space assimilation with diffusion models) could inspire further research and development of new hybrid DA algorithms in both the geosciences and broader sequential data domains."}, "weaknesses": {"value": "Baseline Comparisons: A notable weakness is the absence of comparison with some established DA methods like the Ensemble Kalman Filter (e.g., LETKF). LETKF and similar ensemble Kalman techniques are widely used operationally and are strong benchmarks for assimilation performance. The paper claims to outperform “traditional approaches,” but this is only demonstrated against 3DVar/L3DVar. Without experiments against an ensemble Kalman filter, it is unclear how LO-SDA stands relative to the true state-of-practice. This missing baseline makes the empirical evaluation feel incomplete and may overstate the improvements of LO-SDA.\n\nImprovement Magnitude: While LO-SDA does outperform the latent 3DVar (L3DVar) at higher observation densities, the gains under very sparse observations are quite modest. In the toughest scenario (1% observations), LO-SDA’s error is essentially on par with L3DVar. This suggests that the new method, despite its sophisticated generative approach, does not dramatically outshine the variational baseline when data are extremely scarce. The paper should be careful not to overclaim superiority; the results indicate LO-SDA’s advantage emerges mainly when there are a bit more observations (5% or above), whereas in the most data-starved case its performance difference is negligible. This limits the practical advantage in scenarios of extreme sparsity, which is supposedly a key motivation.\n\nRelated Work and Novelty: The authors do not discuss or cite several closely related approaches in latent-space and generative data assimilation. For example, Latent-EnSF\narxiv.org\n and LD-EnSF\narxiv.org\n are recent methods that also perform assimilation in a learned latent space to handle high-dimensional states with sparse observations (using ensemble score-based filtering). These works similarly leverage VAEs to encode states and observations and have demonstrated improved accuracy and efficiency in scenarios with very sparse data. The submission’s technical approach (using a VAE for state compression and doing assimilation in latent space) is quite aligned with those, though using a diffusion model instead of an ensemble score matching. Failing to compare with or even acknowledge Latent-EnSF(https://arxiv.org/abs/2409.00127) / LD-EnSF( https://arxiv.org/abs/2411.19305 ) is a significant oversight. It raises questions about the novelty: the idea of latent-space data assimilation for sparse observations is not entirely new, and the paper should clarify how LO-SDA advances the state of the art relative to these methods. Similarly, there has been work on Deep Bayesian Filters (DBF https://arxiv.org/abs/2405.18674) for data assimilation that introduce learned latent dynamics and aim to perform Bayes-optimal sequential inference (e.g., by constructing latent variables and using neural networks for filtering). The authors do not compare to such approaches either, leaving a gap in understanding how LO-SDA compares to other modern ML-based filtering methods beyond the variational ones.\n\nTheoretical Limitations: From a Bayesian perspective, LO-SDA has some theoretical limitations that the paper does not fully address. Notably, the method does not sample the true posterior distribution of the state given observations; it uses a diffusion model trained on data and then applies a deterministic optimization to fit observations. This procedure may break the theoretical guarantees of Bayesian inference – the result is an analysis that is a single point estimate consistent with observations, but not necessarily a representative sample from the posterior uncertainty. As a consequence, the approach might underestimate uncertainty or fail to explore multiple possible solutions when the observations are sparse/ambiguous. Additionally, the method’s design raises concerns about temporal consistency in sequential assimilation: each assimilation cycle involves running the diffusion backward with an optimization, which does not obviously ensure consistency over time steps. There is no explicit mechanism to enforce that the analysis at one time is statistically consistent with the background distribution at the next time. In contrast, standard Bayes filtering or sequential Monte Carlo methods propagate uncertainty forward; here it seems each cycle is solved somewhat independently in latent space. This could potentially lead to filter inconsistency or drift over long sequences. The paper would benefit from a discussion on these theoretical aspects – for example, what are the implications of using this one-shot latent optimization per cycle, and could it be integrated into a more principled sequential Bayesian framework?\n\nScope of Experiments: The experiments, while promising, are conducted on an idealized global atmospheric model setup. It’s not entirely clear how complex or realistic the model is (e.g., resolution, model physics) and whether the performance would hold in a fully operational NWP setting. Also, the computational cost of LO-SDA is not thoroughly discussed; diffusion models can be expensive, and although latent space reduces dimension, one wonders if the method is efficient enough for real-time forecasting. Some clarification on runtime or complexity compared to baselines would strengthen the evaluation. These are minor issues, but addressing them would give a more complete picture of the method’s practical viability.\n\nPretraining Assumptions and Generalization: A further limitation lies in the pretraining assumptions behind the VAE encoder. LO-SDA requires a VAE trained on fully observed high-dimensional atmospheric states to learn the latent space. However, in many real-world applications, such full-state ground truth is unavailable or prohibitively expensive to collect. It is unclear how well LO-SDA would generalize to settings where only partial or noisy state estimates are available during training, which is often the case in operational meteorology. Moreover, the method’s effectiveness hinges on the quality of the learned latent representation. If the VAE is trained on idealized or unrealistic data, the resulting latent space may not be suitable for use in sparse, noisy, or changing environments. Addressing how LO-SDA can be adapted to such partially observed training regimes, or evaluating its robustness to domain shifts in latent modeling, would strengthen its practical relevance."}, "questions": {"value": "Related Work: How does LO-SDA compare to existing latent-space assimilation methods like Latent-EnSF and LD-EnSF? These approaches also target high-dimensional systems with sparse observations using VAEs and score-based updates. It would be helpful if the authors could explain the differences and why LO-SDA is expected to perform better. Was there a reason these methods were omitted from discussion? A clear positioning relative to them would strengthen the paper’s originality claim.\n\nSequential Filtering: The proposed method handles one assimilation step via diffusion + optimization, but how is the temporal sequence of assimilation cycles handled? Is LO-SDA applied independently at each cycle with a newly encoded background, or is there any mechanism to propagate information forward (e.g., using the posterior as the next background)? Essentially, is LO-SDA intended as a one-step analysis technique or part of a full filtering framework? If the latter, do the authors observe any issues with filter consistency or error accumulation over long sequences?\n\nRelatedly, how does the method perform when a single observation time is insufficient to constrain the latent state, such as in scenarios with high observation noise or highly underdetermined inverse problems? Are there any experiments that test LO-SDA's robustness in these noisy or ill-posed conditions, where temporal information or smoothing would typically be necessary to resolve ambiguity?\n\nPerformance at Extreme Sparsity: The results show only a slight edge (or parity) over L3DVar at 1% observation coverage. Do the authors have insight into why the improvement is small in this regime? Is the diffusion model struggling due to very weak observational constraints, or could it be that L3DVar is already very effective at low obs densities? Understanding this would help assess LO-SDA’s usefulness in the scarcest-data scenarios. Also, if possible, what modifications might improve performance when observations are extremely sparse? (e.g., more latent optimization steps, better priors, etc.)\n\nComputational Cost: What is the runtime of LO-SDA for a single analysis, and how does it compare to L3DVar or other baselines? The paper mentions latent 3DVar takes on the order of seconds per assimilation. If LO-SDA is significantly slower due to diffusion sampling, it might be a concern for operational use. Any details on optimization steps or potential to speed it up (like using fewer diffusion steps or a more efficient sampler) would be helpful to know."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Poi45NJsiF", "forum": "SJxTRvkHtM", "replyto": "SJxTRvkHtM", "signatures": ["ICLR.cc/2026/Conference/Submission19171/Reviewer_kg13"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19171/Reviewer_kg13"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768217590, "cdate": 1761768217590, "tmdate": 1762931177084, "mdate": 1762931177084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LO-SDA, a generative data assimilation (DA) framework that replaces traditional Gaussian priors with a latent diffusion model conditioned on background atmospheric states. By incorporating an iterative latent optimization step during the diffusion reverse process, the method ensures strict consistency with sparse observations. Experiments on global ERA5 datasets show that LO-SDA outperforms both traditional variational DA (3DVAR) and its latent variant (L3DVAR), particularly under sparse observation settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Establishes a theoretical bridge between variational DA and score-based diffusion modeling."}, "weaknesses": {"value": "- Computational cost for latent optimization process is expensive\n- The work mainly integrates known elements (VAE compression + score-based DA + optimization) rather than proposing fundamentally new generative principles."}, "questions": {"value": "1. Can the computational cost be reduced in the futher?\n2. The paper notes a significant computational burden. Could the authors clarify which parts of the process dominate runtime (diffusion sampling, latent optimization, or decoding)?\n3. The ERA5 experiments are convincing, but how does LO-SDA scale to operational forecast cycles or real-time assimilation scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "z2XyTgxvdb", "forum": "SJxTRvkHtM", "replyto": "SJxTRvkHtM", "signatures": ["ICLR.cc/2026/Conference/Submission19171/Reviewer_E3RS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19171/Reviewer_E3RS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906670926, "cdate": 1761906670926, "tmdate": 1762931176702, "mdate": 1762931176702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LO-SDA, a score-based data assimilation (DA) framework that integrates latent optimisation with background-conditioned diffusion models. A variational autoencoder (VAE) is used to learn low-dimensional latent representations of atmospheric states; within this space, a diffusion model learns the conditional prior. During reverse diffusion, the authors introduce latent optimisation steps that iteratively enforce observation consistency, which they interpret as a generative analogue of variational cost-function minimisation. Experiments on ERA5 global reanalysis show that LO-SDA achieves performance comparable to or slightly better than variational DA (L3DVAR), outperforming single-step diffusion-based methods such as DPS and Repaint under sparse-observation settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear motivation. The work targets the Gaussian-prior limitation of classical DA and builds a principled bridge between score-based and variational formulations.\n\nMethodological coherence. The latent optimisation scheme is well justified and neatly integrated into the diffusion framework. Algorithm 1 clearly contrasts it with DPS guidance.\n\nPresentation quality. Figures are clear and the connection to variational DA is articulated."}, "weaknesses": {"value": "- Incremental novelty. The paper’s central idea—enforcing observation consistency within score-based DA—is conceptually close to existing approaches such as score-based DA, DiffDA, and Manshausen et al., 2025. LO-SDA differs mainly by performing optimization in latent space rather than directly in model space and by framing the repeated guidance as “latent optimization.” This is a useful but evolutionary step rather than a paradigm shift.\n\n- Overstated claims. The manuscript repeatedly suggests LO-SDA is the first score-based method to outperform traditional DA, but the margin over L3DVAR is modest and depends on selected metrics and observation density. The contribution would be stronger if positioned as a refinement of existing score-based methods rather than a breakthrough.\n\n- Computational cost. Iterative latent optimisation requires ~5 minutes per assimilation versus seconds for variational baselines; scalability to operational use remains unclear.\n\n- Limited theoretical depth. The link between latent optimisation and variational minimisation is described heuristically without formal guarantees or convergence analysis.\n\n- Related-work positioning. Discussion could better situate LO-SDA among other generative and variational DA frameworks, e.g., Tensor-Var (2025) or DiffDA (2024), highlighting specific differences in assumptions and efficiency instead of focusing primarily on 3DVAR/L3DVAR comparisons."}, "questions": {"value": "- Does the optimization always converge, and how many iterations are typically needed?\n- How does LO-SDA handle strongly nonlinear observation operators such as satellite radiances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vwIPlxYQn0", "forum": "SJxTRvkHtM", "replyto": "SJxTRvkHtM", "signatures": ["ICLR.cc/2026/Conference/Submission19171/Reviewer_fudT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19171/Reviewer_fudT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994253261, "cdate": 1761994253261, "tmdate": 1762931176293, "mdate": 1762931176293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}