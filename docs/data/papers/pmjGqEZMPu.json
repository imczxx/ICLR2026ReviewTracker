{"id": "pmjGqEZMPu", "number": 15156, "cdate": 1758248384582, "mdate": 1759897324529, "content": {"title": "SongEval: A Benchmark Dataset for Song Aesthetics Evaluation", "abstract": "Aesthetics serve as an implicit and important criterion in song generation tasks that reflect human perception beyond objective metrics. However, evaluating the aesthetics of generated songs remains a fundamental challenge, as the appreciation of music is highly subjective. Existing evaluation metrics, such as embedding-based distances, are limited in reflecting the subjective and perceptual aspects that define musical appeal. To address this issue, we introduce SongEval, the first open-source, large-scale benchmark dataset for evaluating the aesthetics of full-length songs. SongEval includes over 2,399 songs in full length, summing up to more than 140 hours, with aesthetic ratings from 16 professional annotators with musical backgrounds. Each song is evaluated across five key dimensions: overall coherence, memorability, naturalness of vocal breathing and phrasing, clarity of song structure, and overall musicality. The dataset covers both English and Chinese songs, spanning nine mainstream genres. Moreover, to assess the effectiveness of song aesthetic evaluation, we conduct experiments using SongEval to predict aesthetic scores and demonstrate better performance than existing objective evaluation metrics in predicting human-perceived musical quality.", "tldr": "A Benchmark Dataset for Song Aesthetics Evaluation", "keywords": ["song evaluation", "aesthetics evaluation", "full length song"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5ae56b712d305bb1f015caecd915f5eed831112.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "SongEval is a large-scale benchmark dataset designed to evaluate the aesthetic quality of full-length AI-generated songs. Recognizing that existing objective metrics fail to capture subjective human perception, the dataset includes 2,399 English and Chinese songs (over 140 hours) spanning nine musical genres. Its core contribution lies in the detailed human annotation: 16 professional musicians evaluated each piece on a 1–5 scale across five aesthetic dimensions—overall coherence, memorability, naturalness of vocal phrasing and breathing, clarity of song structure, and overall musicality. Experimental results show that models trained with SongEval correlate more strongly with human judgments and significantly outperform traditional objective metrics in predicting perceived musical quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-The paper introduces SongEval, a large-scale, open-source benchmark dataset that addresses an important and challenging problem: evaluating the aesthetic quality of full-length AI-generated songs.\n-The dataset is both substantial and diverse, comprising 2,399 songs (over 140 hours of audio) in English and Chinese across nine musical genres.\n-The annotations are provided by 16 professional musicians, offering high-quality expert evaluations that strengthen the reliability and credibility of the benchmark.\n-The dataset employs a multi-dimensional 1–5 rating scale across five well-defined aesthetic aspects, rather than relying on a single aggregate score, enabling more nuanced assessment.\n-The paper demonstrates through experiments that models trained on SongEval correlate more strongly with human perception and significantly outperform traditional objective metrics."}, "weaknesses": {"value": "-The methodological basis for the chosen subjective evaluation dimensions is not well justified. The paper does not demonstrate evidence of a systematic literature review or structured expert interviews to support the selection of the five aesthetic criteria.\n-The overall musicality dimension is broad and conceptually ambiguous. Even with the provided definition, its interpretive scope risks introducing significant variance among annotators, especially since it aggregates multiple perceptual qualities into a single score.\n-The authors acknowledge in Appendix A.1 that these aesthetic factors are difficult to disentangle due to “cognitive and emotional entanglement,” which underscores a foundational ambiguity in the benchmark’s conceptual framework.\n-The presentation of experimental results lacks statistical rigor. For instance, Table 3 reports mean values without standard deviations, preventing assessment of variance or reliability in the comparisons.\n-The high correlation of the “Aesthetic (Ours)” score in Table 4 is potentially misleading, as this metric is derived from a UTMOS-based system trained directly on SongEval’s own subjective labels. This constitutes a form of circular evaluation and does not provide meaningful comparative insight.\n-The set of “objective metrics” used for baseline comparison appears insufficient, as they do not reflect fundamental elements of musical composition or perceptual music theory, limiting the interpretive value of the comparison.\n-The correlation analysis in Table 4 is overly aggregated. A more informative analysis would examine correlations across different genres or stylistic categories to determine whether the benchmark generalizes beyond broad average trends.\n-The paper does not provide background details regarding the annotators’ training, specialization, or experience, which limits the reader’s ability to assess the reliability and consistency of the expert ratings."}, "questions": {"value": "-\tBeyond the number of annotated dimensions, what qualitative characteristics of SongEval make it more reliable or representative for evaluating musical aesthetics than existing benchmarks?\n-\tSince the dataset includes songs generated by five different models, did the authors observe any statistically meaningful differences in human aesthetic ratings across these model types?\n-\tWhat annotation guidelines or protocols were provided to the evaluators? Is there any reported measure of inter-rater agreement (e.g., ICC, Cohen’s κ)?\n-\tGiven the acknowledged conceptual overlap among the five aesthetic dimensions, have the authors conducted correlation analysis or factor analysis to verify that each dimension captures a distinct perceptual construct?\n-\tWere any linguistic or cultural effects observed in ratings between English and Chinese songs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cuGRssR8wb", "forum": "pmjGqEZMPu", "replyto": "pmjGqEZMPu", "signatures": ["ICLR.cc/2026/Conference/Submission15156/Reviewer_hYm1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15156/Reviewer_hYm1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898080247, "cdate": 1761898080247, "tmdate": 1762925469698, "mdate": 1762925469698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SongEval, the first open-source, large-scale benchmark dataset dedicated to evaluating the aesthetics of full-length songs, addressing the long-standing challenge of assessing the subjective aesthetic quality of generated songs. The dataset comprises 2,399 full-length songs, totaling over 140 hours of audio, covering both English and Chinese languages and nine mainstream music genres. Aesthetic annotations are provided by 16 professional annotators with musical backgrounds, who evaluate each song across five key dimensions: overall coherence, memorability, naturalness of vocal breathing and phrasing, clarity of song structure, and overall musicality.\n\nThe paper concludes that SongEval fills a critical gap in existing music evaluation datasets and provides a reliable foundation for future research in controllable music generation, quality assessment, and style transfer. The dataset and associated toolkit are made publicly available to facilitate further advancements in the field."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "With 2,399 full-length songs (140+ hours), SongEval is significantly larger than existing alternatives. It covers two languages (English, Chinese) and nine genres, and includes both vocals and accompaniment—addressing the limitation of single-component focus in prior datasets.\nThe authors test four models with distinct architectures (convolutional, self-supervised, ensemble-based) and evaluate performance at both utterance and system levels. Direct comparisons to established objective metrics (e.g., Audiobox-Aesthetic’s PC, Vocal Range) clearly demonstrate SongEval’s superiority in aligning with human aesthetic perception."}, "weaknesses": {"value": "The dataset is dominated by generated songs from five models, with only a small subset of non-copyrighted real songs. This narrow focus on generated content limits its ability to evaluate models on real-world, human-composed music—a critical use case for aesthetic evaluation."}, "questions": {"value": "Could the author provide quantitative measures of inter-annotator agreement (e.g., Krippendorff's alpha, Cohen's kappa) for each of the five aesthetic dimensions? This would help confirm the reliability of the annotation process."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LNJggSfdL3", "forum": "pmjGqEZMPu", "replyto": "pmjGqEZMPu", "signatures": ["ICLR.cc/2026/Conference/Submission15156/Reviewer_EXKt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15156/Reviewer_EXKt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916488269, "cdate": 1761916488269, "tmdate": 1762925468935, "mdate": 1762925468935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new benchmark dataset named SongEval to assess the aesthetics of generated songs. This benchmark utilizes several existing song generation models such as DiffRhythm to generate songs with different genres, incorporating lyrics in English and Chineses, and then exploits four human experts to annotate the generated songs with five categories. This newly introduced dataset is largest in terms of size and annotation categories compared to prior datasets. To demonstrate the effectiveness of the dataset and song aesthetic evaluation, the authors train four models to predict aesthetic scores and demonstrate better performance than existing objective evaluation metrics in predicting human-perceived musical quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It is a relevant problem to study and assess song aesthetics, and the newly introduce benchmark dataset may be useful to the community.\n- In experiments, it was shown that the trained models to predict aesthetic scores achieves better performance than existing objective evaluation metrics in predicting human-perceived musical quality.\n- Writing is easy to follow."}, "weaknesses": {"value": "- The authors use song generation models to generate songs rather than actual songs. So the qualities of them largely depend on the song generation model and it may be desirable to use actual songs for building the benchmark and annotations. \n- My main concern is the experiment designs to show the effectiveness of the new dataset. This paper first proposes to compare the predictions from the trained evaluation models on the dataset, with the human annotations. Even though the reported results look good, there is no comparison to show the effectiveness of the dataset rather than the prediction models. I think the more relevant experiment would be training song generation model with the annotations as human preference alignment. Furthermore, in the comparison with objective evaluation metrics, it is not surprising that the trained model on the dataset has higher correlation since such objective metrics may capture different aspects and the trained model is directly trained on the same categories' annotations.\n- The annotation qualities are also questionable. In Figure 3, it was shown that almost half of the annotations have the highest scores (5). So the annotation instruction (and/or human expert qualification) may be problematic."}, "questions": {"value": "- As shown in Figure 4 right, why are there uneven uses of different song generation models, e.g., DiffRhythm is used more than others?\n\nPlease see and address the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iSpNut4E7Q", "forum": "pmjGqEZMPu", "replyto": "pmjGqEZMPu", "signatures": ["ICLR.cc/2026/Conference/Submission15156/Reviewer_5n43"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15156/Reviewer_5n43"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979872566, "cdate": 1761979872566, "tmdate": 1762925468375, "mdate": 1762925468375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Aesthetics serve as an implicit and important criterion in song generation tasks that reflect human perception beyond objective metrics. However, evaluating the aesthetics of generated songs remains a fundamental challenge, as the appreciation of music is highly subjective. Existing evaluation metrics, such as embedding-based distances, are limited in reflecting the subjective and perceptual aspects that define musical appeal. To address this issue, we introduce SongEval, the first open-source, large-scale benchmark dataset for evaluating the aesthetics of full-length songs. SongEval includes over 2,399 songs in full length, summing up to more than 140 hours, with aesthetic ratings from 16 professional annotators with musical backgrounds. Each song is evaluated across five key dimensions: overall coherence, memorability, naturalness of vocal breathing and phrasing, clarity of song structure, and overall musicality. The dataset covers both English and Chinese songs, spanning nine mainstream genres. Moreover, to assess the effectiveness of song aesthetic evaluation, we conduct experiments using SongEval to predict aesthetic scores and demonstrate better performance than existing objective evaluation metrics in predicting human-perceived musical quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive Dataset: SongEval provides a large-scale benchmark dataset with over 2,399 full-length songs, allowing for extensive analysis and evaluation of song aesthetics across various genres and languages.\n\nMulti-Dimensional Evaluation: The dataset includes aesthetic ratings across five key dimensions (overall coherence, memorability, naturalness, clarity, and musicality), offering a nuanced approach to assessing musical appeal beyond simple metrics.\n\nImproved Predictive Performance: Experiments demonstrate that SongEval outperforms existing objective evaluation metrics in predicting human-perceived musical quality, highlighting its effectiveness in evaluating song aesthetics."}, "weaknesses": {"value": "Subjectivity of Aesthetic Evaluation: The appreciation of music is inherently subjective, and even with professional annotators, individual biases may influence the aesthetic ratings, potentially affecting the dataset's reliability.\n\nLimited Scope of Genres: While SongEval covers nine mainstream genres, it may not encompass all musical styles, which could limit its applicability for evaluating songs from less represented genres or niche markets.\n\nDependence on Annotator Expertise: The quality of the aesthetic ratings relies heavily on the expertise of the 16 professional annotators, and any inconsistencies in their evaluations could impact the overall validity of the dataset."}, "questions": {"value": "1) How can minimize individual biases among annotators to ensure more consistent and reliable aesthetic ratings in the SongEval dataset?\n\n2) What strategies can be employed to expand the dataset to include a wider variety of musical genres, particularly those that are underrepresented in the current collection?\n\n3) How can we ensure that the evaluations remain valid and reliable if the expertise of the annotators varies, and what measures can be taken to standardize their assessments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gjt6lERx1s", "forum": "pmjGqEZMPu", "replyto": "pmjGqEZMPu", "signatures": ["ICLR.cc/2026/Conference/Submission15156/Reviewer_WNQ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15156/Reviewer_WNQ2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153617867, "cdate": 1762153617867, "tmdate": 1762925467807, "mdate": 1762925467807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}