{"id": "ZmnG240KH5", "number": 13576, "cdate": 1758219356925, "mdate": 1759897427364, "content": {"title": "Online Learning-guided Learning Rate Adaptation via Gradient Alignment", "abstract": "The performance of an optimizer on large-scale deep learning models depends critically on fine-tuning the learning rate, often requiring an extensive grid search over base learning rates, schedules, and other hyperparameters. In this paper, we propose a principled framework called GALA (Gradient Alignment-based Learning rate Adaptation), which dynamically adjusts the learning rate by tracking the alignment between consecutive gradients and using a local curvature estimate. Guided by the convergence analysis, we formulate the problem of selecting the learning rate as a one-dimensional online learning problem. When paired with an online learning algorithm such as Follow-the-Regularized-Leader, our method produces a flexible, adaptive learning rate schedule that tends to increase when consecutive gradients are aligned and decrease otherwise. We establish a data-adaptive convergence rate for normalized SGD equipped with GALA in the smooth, nonconvex setting. Empirically, common optimizers such as SGD and Adam, when augmented with GALA, demonstrate robust performance across a wide range of initial learning rates and perform competitively without the need for tuning.", "tldr": "", "keywords": ["non-convex optimization; learning rate adaptation; online learning; adaptive methods"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/447e99c1030dc25541c8bf3bc6d84c23b776014e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GALA (Gradient Alignment–based Learning Rate Adaptation), an optimizer framework that adaptively adjusts the learning rate by monitoring the alignment between consecutive gradients and estimating local curvature. The authors formulate learning-rate selection as a one-dimensional online learning problem and derive convergence guarantees for a normalized SGD variant. Empirically, they apply GALA to SGD and Adam on several vision benchmarks (CIFAR-10/100, Flower102, Tiny-ImageNet), claiming that GALA improves robustness to initial learning rate choices and achieves competitive accuracy without manual tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "$\\textbf{Reasonable thinking behind Algorithm design}$\n* The paper presents a novel conceptual connection between online learning and optimizer dynamics, leveraging regret minimization to adapt step sizes. The proposed framework is theoretically motivated and systematically integrates curvature estimation and gradient alignment. \n* The authors provide both theoretical convergence analysis and empirical validation, which makes the contribution reasonably well-rounded."}, "weaknesses": {"value": "$\\textbf{Practical algorithm: increased gradient oracle complexity and unfair comparison}$\n* Algorithm 1 extends SGD but requires two additional gradient evaluations, an extra full parameter update, and larger memory consumption.\n* Consequently, Figure 1 is not a fair comparison, since it plots results against epoch count. A fair evaluation should use computational cost, e.g., total number of gradient queries, as the x-axis.\n* The added computational overhead fundamentally weakens the practicality of the proposed method, especially when comparing with single-query optimizers such as SGD or Adam.\n\n---\n\n$\\textbf{Algorithm design lacks justification}$\n* The introduction of the second local Lipschitz estimate is heuristic: the paper never discusses the estimation error or its impact.\n* The design choice $w_{t}=x_{t+1}$  (Sec. 5.1) directly violates the theoretical assumption that $w_{t}$ should lie between $x_{t}$ and $x_{t+1}$.\n  * Although this simplification may be intended to save computation or mitigate variance, it breaks the theoretical foundation underlying Lemma 1 and Theorem 1.\n  * The authors should clarify whether this approximation still preserves the intended properties or whether it merely serves as a heuristic implementation.\n\n---\n\n$\\textbf{Questionable intuition and motivation}$\n\n* The main intuition, monitoring gradient alignment $\\Delta_{t}, g_{t}$ as a “useful signal” for learning-rate adjustment, is conceptually debatable.\n  * In practice, disagreement between consecutive stochastic gradients is often due to noise, which is known to be beneficial for exploring non-convex landscapes (curvature effects).\n  * Completely suppressing this noise, as suggested by the “alignment-only” rule, might reduce exploration and harm convergence in flat or saddle-point regions.\n\n* The choice of Follow-the-Regularized-Leader (FTRL) as the online subroutine appears intuitive rather than principled. The authors do not compare it against other plausible online learners nor discuss its implications.\n\n---\n\n$\\textbf{Unfair experimental comparison}$\n* Like I mentioned before, the empirical results use epoch count as the horizontal axis, which ignores the doubled number of gradient evaluations in SGD-GALA and ADAM-GALA.\n* To be fair, results should be reported with respect to total gradient queries or wall-clock time. Without this normalization, claims of comparable efficiency are not convincing.\n\n---\n\n$\\textbf{Limited experimental validation}$\n* The empirical section is narrow in scope; only a few architectures (ResNet-18, ViT-Tiny) and small datasets (CIFAR-10/100, Flower102, Tiny-ImageNet) are evaluated.\n* The study does not assess GALA’s behavior in larger-scale or more diverse tasks, such as NLP benchmarks or self-supervised pretraining, which would better demonstrate robustness.\n* The paper should also include a comparison of computational cost and memory usage between baseline optimizers (SGD, Adam) and their GALA-augmented counterparts (SGD-GALA, ADAM-GALA)."}, "questions": {"value": "* In Section 5.1, the practical version of the algorithm appears simplified. However, if I understand correctly, the proposed method still requires two gradient evaluations per parameter update, unlike standard SGD, which needs only one. Could the authors please clarify whether this interpretation is correct?\n\n* Can you provide a comparison of computational cost and memory usage between baseline optimizers (SGD, Adam) and their GALA-augmented counterparts (SGD-GALA, ADAM-GALA)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XpkbBk3i1S", "forum": "ZmnG240KH5", "replyto": "ZmnG240KH5", "signatures": ["ICLR.cc/2026/Conference/Submission13576/Reviewer_dR1X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13576/Reviewer_dR1X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761198287739, "cdate": 1761198287739, "tmdate": 1762924172728, "mdate": 1762924172728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors designed a self-tuning optimization method using the alignment between two consecutive stochastic gradients (à la hypergradient) based on online learning for which they can prove convergence rates for minimizing the gradient norm in expectation for the problem \\E_\\xi[f(x, \\xi)], assuming a stochastic unbiased estimate of the gradient and that every f(\\cdot, \\xi) is locally smooth among other common assumptions like bounded variance and global gradient Lipschitzness. They use use a surrogate loss inspired by Zhuang et al (2019) for their online learning game on the step size."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Authors are able to put together an intricate analysis combining many ideas (online to nonconvex ideas like Cutkosky et al, surrogate losses inspired by Zhuang et al, a hypergradient-like update rule viewed as an online learning algorithm for learning the step size, like in Gao et al and Chu et al do for convex losses...) resulting in provable rates for these kind of consecutive stochastic gradient aligment techniques for learning rate tuning, in the stochastic nonconvex smooth setting with bounded variance."}, "weaknesses": {"value": "The algorithm requires knowledge of the gradient estimates' variance as well as the final iteration T, via $\\alpha$.\n\nThe extended related work is mostly the same as the one in the main paper. This is too redundant. Also, hypergradient descent literature in the related work section in the appendix was not even mentioned in the main paper. Given how relevant and related this is to this paper, including the theory works of Gao et al (2024) and Chu et al (2025), I'd urge the authors to have this paragraph in the main paper. I trust this is currently in the supplementary material for strategic purposes only (unfortunately those kind of things happen under the incentives of our current review system).\n\nThe experiments do not compare with any of the papers in the hypergradient descent literature which directly or indirectly inspired this work. These prior papers (cited in the present paper) already show an adaptivity and improvement over SGD and Adam when computing its heuristic on top of them (e.g. Baydin et al (2018)). I consider the main contribution of this work to be on the theory side but still the experiments should be fair and compare to some of these prior approaches. Figures 1 and 2 are misleading because they are highlighting an adaptivity phenomenon already present in the hypergradient literature. Claiming novelty on this empirical phenomenon is misleading."}, "questions": {"value": "I am not fully sure whether the point of this paper (within the context of what is currently known) is beyond the following or whether there is anything else that I missed, so let me state what I gathered and you can correct me if I am wrong: I suppose that since it was proven by Gao et al. (2024) and Chu et al. (2025) that hypergradient heuristics could be made formal and convergence could be shown by analyzing the learning-rate-tuning hypergradient rule via online learning, for the optimization of convex problems, authors wanted to investigate whether some analogous phenomenon would be possible in stochastic nonconvex problems, by exploiting recent developments that allow to show good rates of convergence via online learning and tricks like the one about sampling a random point w in between x_t and x_{t+1}. Is there any adaptivity that is gained from this algorithm beyond making the hypergradient-like heuristic provably work? Adaptivity to the unknown local gradient Lipschitz constant perhaps?\n\nI'd suggest to write gradient Lipschitz estimate in the many places where it only says Lipschitz estimate referring to the Lipschitzness of the gradient"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5zu6oOuif9", "forum": "ZmnG240KH5", "replyto": "ZmnG240KH5", "signatures": ["ICLR.cc/2026/Conference/Submission13576/Reviewer_1jgD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13576/Reviewer_1jgD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922240999, "cdate": 1761922240999, "tmdate": 1762924172445, "mdate": 1762924172445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GALA (Gradient Alignment–based Learning-rate Adaptation): a framework that updates the scalar learning rate via an online learner using a surrogate loss built from (i) alignment between consecutive stochastic gradients evaluated at an interpolated point and the current point, and (ii) a local Lipschitz estimate along the update segment. Theoretical analysis is provided for normalized SGD with momentum using a modified surrogate (Eq. (9)) and yields a rate depending on the average/max local Lipschitz estimates and the regret of the online learner (Theorem 1). Empirically, the authors add a GALA-style update on top of SGD and Adam across CIFAR-10/100, Flowers102, and a ViT-Tiny finetune; figures show improved robustness to the initial learning rate in some regimes."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a principled intuition by casting learning-rate selection as a one-dimensional online learning problem with an explicit surrogate derived from the gradient alignment identity and a local curvature penalty (Eq. (5)), leading to clean, closed-form FTRL updates (Eq. (7)).\n\n2. The theoretical analysis targets normalized SGD with momentum and clearly shows the dependence on an along-trajectory smoothness notion (Theorem 1), complemented by a regret bound (Lemma 2).\n\n3. The algorithmic presentation is clear and provides an accessible description of the optimistic FTRL framework with meaningful intuition.\n\n4. The proposed method is of practical interest, since the idea of using gradient alignment to dynamically increase or decrease the learning rate is intuitive, easy to implement, and addresses a relevant problem of robustness to learning rate selection."}, "weaknesses": {"value": "Regarding the theoretical results:\n\n1. Although not explicitly framed as assumptions, strong assumptions are required for Theorem 1's bound to converge. (1) Theorem 1’s convergence requires the online regret on a surrogate whose quadratic term is strongly convex if $L_t$ and $\\tilde L_t$ admit uniform lower bounds, which is restrictive in stochastic nonconvex settings and stronger than assuming only $F$ is $L$-smooth. This basically requires the function $f(\\cdot; \\xi)$ to be strongly-convex, and cannot be flat locally. (2) For the bound in Theorem 1 to converge, it also relies on sample-wise smoothness of $f(\\cdot;\\xi)$ via $L_t,\\tilde L_t$, which can be substantially stronger than mere smoothness of $F$.\n\n2. The claim that “the convergence rate in Theorem 1 is in terms of the average and maximum Lipschitz estimates, which can be much smaller than the global constant $L$” is not justified. The quantity $L_T^{\\text{avg}}$ aggregates per-sample local secant constants and need not be smaller than the global $L$ for $F$; it can even be larger, depending on the noise realizations, unless additional per-sample regularity is assumed. Clearly per-sample smoothness of $f(\\cdot, \\xi)$ can be larger than the smoothness parameter of the average $F(\\cdot)$. Even consider locally, $|| \\nabla^2_x f(x, \\xi) ||_2$ can be larger than $|| \\nabla^2_x F(x) ||_2$. If the argument is not about sample-wise smoothness, but path-wise smoothness is smaller than the global smoothness, then I guess vanilla SGD can also have similar bounds. The intuition is to telescope the local path-wise smoothness inequality instead of the global smoothness. I am not sure if this could work rigorously in math, but intuitively we only care about the smoothness of the function along the optimization path even for SGD, right? What is the fundamental difference here?\n\n3. The algorithm is proposed to require less tuning, “parameter-free” in a sense. However the theoretical rate in Theorem 1 fixes the momentum parameter using $\\alpha=\\min\\{1/(\\sigma\\sqrt{T}),1\\}$, which depends on both $T$ and $\\sigma$.\n\nI find the gaps between theory and practice to be significant, leading to the question of whether the theory really explains the robustness of the algorithm in practice:\n\n4. The analyzed algorithm (normalized SGD with momentum) differs from the implemented versions (GALA-SGD and GALA-Adam). The experiments modify Algorithm 1 in key ways: setting $w_t=x_{t+1}$ (no random interpolation) and using the same minibatch to compute both gradients in the alignment term. This contradicts with the major motivation given in Eq. (4). The analysis does not even cover vanilla SGD.\n\n5. Lemma 2’s regret bound relies on $\\eta\\in[0,\\eta_{\\max}]$ (bounded domain via clipping). The paper explicitly states that in practice the clipping is removed, i.e., $\\eta_{\\max}\\to\\infty$. However, in this case the regret analysis in Lemma 2 becomes vacuous.\n\nRegarding the experiments:\n\n6. GALA uses an extra gradient call per iteration to estimate the Lipschitz constant, making one “iteration” of GALA more expensive than an iteration of vanilla SGD or Adam. Therefore, in Figure 1, comparisons versus iteration count (or epoch count) instead of number of gradient calls would favor GALA, leading to possibly unfair comparison. Also, in Figure 1 (b), there seems to be a bad initial learning rate for GALA-Adam that causes high training loss.\n\n7. In Figure 2, Prodigy and D-AdaptAdam are empirically stable across a wide learning-rate sweep and often match or exceed GALA variants. \n\nMinor issues:\n\n8. In line 206, the definition of $g'_t$ should be $\\nabla f(x_t; \\xi'_t)$ instead of $\\nabla f(x_t; \\xi_t)$?"}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6SjiOML9Xz", "forum": "ZmnG240KH5", "replyto": "ZmnG240KH5", "signatures": ["ICLR.cc/2026/Conference/Submission13576/Reviewer_f2NE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13576/Reviewer_f2NE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020246791, "cdate": 1762020246791, "tmdate": 1762924172011, "mdate": 1762924172011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a principled online-learning-based approach to estimation of the best stepsize to loss minimization. The authors define a sequence of surrogate loss functions and apply FTRL to the obtained regret minimization problem. The authors study the obtained method when applied to the iterates of normalized-SGD, and under a few assumptions, including lower-bounded Lipschitz constants (which I argue is problematic), they show convergence guarantees matching those of methods that require learning rate tuning. They also test their method on vision problems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper discusses the related work in details and attributes the ideas properly.\n\n2. The setting for the experiments is reasonable, though limited to the vision datasets. At least I appreciate that the authors didn't stop at running CIFAR-10 evaluations.\n\n3. The authors compared to quite a few other adaptive optimizers, which helps show the strength of the evaluations.\n\n4. The practical implementation does not use many extra hyperparameters, and the method doesn't seem to be sensitive to the extra ones, namely $\\delta$ and $\\eta_0$."}, "weaknesses": {"value": "1. The assumption that the averages of Lipschitz constants are lower bounded is essentially requiring the eigenvalues of the Hessian matrix to be separated from 0, which means no saddle points. I think it is very strong and should be avoided if the authors are truly interested in the nonconvex optimization. As a result, I don't think combining Theorem 1 and Lemma 2 is a valid way to compare to the results of Cutkosky and Mehta (2020). Similarly, the resuts can't be compared to those of AdaGrad.\n\n2. The actual implementation of the method is quite different from the theory. Most importantly, the authors abandon one of the key ideas of estimating the gradient product using a randomly sampled interpolated point. Moreover, the authors reuse the minibatches, which means that the implemented method doesn't use unbiased estimates. I find these two changes quite important to claim that there is little relation between the method studied theoretically and the one studied empirically.\n\n3. In addition, the proposed method requires two gradients to be computed, which makes it significantly more expensive than other approaches such as Mechanic and Prodigy.\n\n4. The numerical results are not presented in a clear way. I couldn't read Figure 1. While the GALA versions of the methods appear to be better, it's very messy, and particularly unclear at the end of the curves. The latter issue could be fixed by using log scale on the y-axis, but it would also be nice if there was a way to understand which line uses which learning rate. On top of that, I can see that in Figure 1 (a) the best method appears to be SGD rather than GALA-SGD. Finally, the quantities plotted in Figures 2 and 3 aren't very important to be placed in the main body in my opinion. While I understand the need to test sensitivity to the initial stepsize estimate, I think it's the kind of evaluation that be better placed in the appendix.\n\n5. While I understand the authors may have a limited budget for GPUs, I think it would be reasonable to try the tuned methods on full ImageNet-1k instead of Tiny-ImageNet.\n\nMinor comment: please proofread the citations, \"Luís B Almeida\" should be \"Luís B. Almeida\", \"José D Amaral\" should be \"José D. Amaral\", etc."}, "questions": {"value": "1. How does your method compare to others in terms of run-time?\n2. Why are you applying FTRL and not other methods? I understand that you wanted theoretical guarantees, but wouldn't an Adam version of the method be more practical?\n3. Some implementation details seem to be missing, for instance, you didn't specify which augmentations and patch sizes you used in ViT (i.e., was it ViT_tiny_patch16_224 and did you use aa=\"rand-m15-n2\" or something else?). Did you clip the gradient norms as usually done when training ViT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W0X9Vhb2BV", "forum": "ZmnG240KH5", "replyto": "ZmnG240KH5", "signatures": ["ICLR.cc/2026/Conference/Submission13576/Reviewer_KBHb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13576/Reviewer_KBHb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762200151470, "cdate": 1762200151470, "tmdate": 1762924171419, "mdate": 1762924171419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}