{"id": "xFmxnyNYZJ", "number": 7870, "cdate": 1758039953069, "mdate": 1759897826060, "content": {"title": "Learning Task-Sufficient World Models via Intervention-Curriculum Co-Design", "abstract": "We study how agents learn world models with latent representations that are task-specific, minimal, and sufficient for sequential decision making. Rather than predicting pixels or relying on generic embeddings, we aim to learn representations that retain exactly the information needed for control across tasks. We model the problem end-to-end as a closed loop of agent–environment interaction, enabling the agent to sequentially acquire minimal and sufficient latent representations over a series of tasks.\nOn the agent side, for each new task, it begins with active intervention to acquire informative trajectories that implicitly reveal task-relevant latent factors, and then trains the world model to learn a latent space that is both minimal and task-sufficient.\nOn the environment side, learning is facilitated through an adaptive curriculum that co-evolves with the agent. By tailoring environment settings and task order to the agent's learning progress, the curriculum exposes control-relevant mechanisms at the right level of difficulty, while jointly scheduling world-model updates with policy learning. This co-design of intervention and curriculum leads to a compact, structured latent space that supports efficient, transferable policy learning and generalization. Empirically, our approach improves sample efficiency and generalization across skills, object–skill compositions, and unseen tasks on standard continuous control and robotic manipulation benchmarks.", "tldr": "We co-design agent interventions and an adaptive environment curriculum to sequentially learn a world model with task-specific, minimal, and sufficient latent representations, enabling efficient and generalizable policy learning.", "keywords": ["World Model; Latent Variable Models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/401b056dbbf7ea06538911c1ec2f2d3ac2a9014d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a framework to learn a world model. Specifically, a two-stage approach is proposed. Firstly, on the representation side, this work proposes to learn Minimal, Sufficient, and Task-specific (MIST) states for the world model by scoring candidates by minimality and sufficiency and optimising for soft masking and mutual information. Secondly, while exploring new environments, this work proposes to use curriculum learning to learn how to order each skill in the new environment for efficient world model learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed formulation of the MIST states is novel to my knowledge.\n- Using curriculum to guide/order environments for the agent to learn is intuitive. \n- Conducted comprehensive experiments, including ablations, showing the effectiveness of the proposed method.\n- I appreciate the authors denoting their computational requirements in the appendix."}, "weaknesses": {"value": "- For me, the major weakness of this work is that there are no related works in the main manuscript (but were placed in the appendix). Technically it can mean that there are no related works section in this work. \n- Some design choices are less motivated, specifically the skill library part, and is basically a deployment of existing works. \n- Not technically a weakness but, to my understanding, the proposed method is somewhat more complex to train than comparative methods (i.e. TD-MPC2 and Dreamerv3).\n- Minor issues:\n  - Duplicated phrases around lines 320-325: “Following Unsupervised Experiment Design (UED) (Dennis et al., 2020; Rigter et al., 2024), we select”....\n  - Small grammar issues, for example line 127: “the state transitions is” -> “the state transitions are”\n  - In Figure 5, not all algorithms are evaluated on all the proposed tasks.\n  - In Figure 6, the bars are not labelled. \n  - In Figure 7, I assume the pink bar is the proposed MIST-WM but it is not noted. Also, DIAYN is not compared as an ablation."}, "questions": {"value": "I will repost some suggestions/questions that were mentioned in the weakness section here for clarity.\n\n$$\\textbf{Suggestions}$$\nS1: Somehow wiggle the related works section into the main 9 pages.     \nS2: List the PPO/SAC hyperparameters used for downstream policy learning.     \nS3: Add all the experiment results in Figure 5 for all algorithms.     \nS4: Add clear captions/descriptions to Figure 6 and Figure 7.     \n\n$$\\textbf{Questions}$$\nQ1: I find it interesting that the authors chose not to propagate the MIST states to the skill library agent, since the authors argue the MIST states are better representations, I’d imagine that MIST states would help learn the skill library. Are there any particular reasons behind this choice?      \nQ2: I have some reservation regarding the effectiveness of the two stages individually. Reading Figure 7, it seems to be that the curriculum learning only helps a bit. What do the authors think about the results?    \nQ3: Following up on Q2, I wonder what the performance of standalone MIST states would be? It seems that replacing proposed MIST states with R3M would actually have better performance. (I might have misunderstood the meaning of “replacing the backbone with …”).   \nQ4: I’m a bit confused about the meaning of “task embeddings g_i can be the rewards” which were written several times across the manuscript.    \nQ5: Are there any particular reasons to use two different algorithms (DIAYN & METRA) to learn the skill library? Since the ablation showed that curious replay is also good while DIAYN was not investigated, why not simply just use METRA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pX6kHbMNYe", "forum": "xFmxnyNYZJ", "replyto": "xFmxnyNYZJ", "signatures": ["ICLR.cc/2026/Conference/Submission7870/Reviewer_HNgN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7870/Reviewer_HNgN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810621786, "cdate": 1761810621786, "tmdate": 1762919903619, "mdate": 1762919903619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MIST-WM, a framework that combines: (i) skill-based learning for exploration using DIAYN/MISL and a separability heuristic for skill selection, (ii) a Dreamer-style world model whose latent space is masked to yield \"Minimal, Sufficient, Task-specific (MIST)\" states via masking and a reward-likelihood (sufficiency) and MI-based minimality objectives; and (iii) an adaptive curriculum over environments/tasks. Experiments on DMControl, RoboSuite, Franka-Kitchen, and Meta-World report gains in sample efficiency and several generalization settings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Ambitious scope and goal: learning task-sufficient latents and coupling representation learning with active data collection and curriculum.\n\n- Interesting components:\n\n\t- Masked latent subspaces with sufficiency and minimality pressures.\n\n\t- A pragmatic separability-based heuristic to pick informative skills for data collection.\n\n- Some empirical gains over strong world-model baselines on multiple benchmarks."}, "weaknesses": {"value": "- Unfocused and incremental: The method is a collection of several incremental ideas (skill selection, MIST masking, curriculum) without a clear, central algorithmic idea or hypothesis. It consists of several known ingredients (DIAYN/MISL-style skills, Dreamer-v3 backbone with masking/MI, UED-like curriculum). The novelty feels diffuse rather than centered on one idea. Even with ablations, it remains unclear which piece drives which improvement and why. \n\n- Missing analysis of mechanisms: Given the system complexity, the paper does not convincingly explain when and why each component helps. There is plenty of space wasted to simply discribe the performance plots (e.g. in \"Sample efficient policy learning\") instead of interpreting the results.\n\n- Clarity and presentation issues:\n\t- The MIST definitions are spread across heavy notation that often obscures rather than clarifies the practical mechanism. Links between \"masked states\", \"latent factors\", and the \"expandable state space\" are never made concrete.\n\n\t- Multiple policies are introduced (exploration via skills, reward-optimizing policy on masked states, implied composition policy), but this is not explicitly explained and the optimization flow and interactions are not clearly described.\n\n\t- The experimental narrative mostly paraphrases plots instead of analyzing mechanisms or failure cases (e.g., underperformance on Reacher, same or worse performance with other backbones)\n\n\n- Reproducibility concerns: Key mechanisms are vague and underspecified (how the state space expands, how skills are composed, what exactly is optimized by which loss). Some of which include:\n\n    - In RoboSuite, \"with access to ground-truth simulator states\" is ambiguous. Were they used only for probing or also for training? The probing protocol (targets, regressors, metrics) needs to be specified.\n\n\t- The claimed \"skill composition\" is not described algorithmically. It is unclear how skills are sequenced or combined at test time.\n\n\t- Architectural and training specifics (encoders/heads, thresholds, when parameters are frozen,\n\ta list of all optimization processes, exact curriculum scheduler settings) are missing, which is insufficient for reproduction.\n\n\n- Technical and notation issues that undermine confidence:\n\n\t- The Dreamer objective (Eq. 1) is malformed (conditioned on variables that are also predicted). If this is a shorthand, it still needs to be written correctly or deferred to a faithful appendix.\n\n\t- Inconsistent/undefined notation (e.g., what is [d_i]?; inconsistent use of pa(·); reuse of symbols like I_i^(t) for different entities).\n\n\t- Eq. 5 is presented as an \"information gain\" criterion for skill selection, but it clearly depends on encoder/policy parameters. It is not stated whether this loss updates those parameters or is used only for scoring. The optimization pathways are unclear."}, "questions": {"value": "- MIST states and expansion: When and how are new state dimensions added per task? Is the \"expandable state space\" implemented via masking over a fixed large latent, or do you actually grow the latent dimensionality across tasks? How does this relate to the \"latent factors\" notation introduced around Line 269-270?\n\n- Skill selection vs. single-task improvements: In Fig. 5, to what extent are gains in single-task learning due to the skill selection mechanism alone? Or, to ask differently: What remains of the framework in pure single-task settings (no curriculum, no cross-task expansion, a single mask,...)?\n\n- Fig. 4a probing: What exactly is shown on the y-axis? Why is it a matrix rather than per-factor bars? I would have assumed you test how well the ground-truth states can be predicted from the full state, which would result in a bar plot. Does the y-axis go over segments of the full state? What do off-diagonal entries represent?\n\n- Skill composition: How are \"skills composed\" at test time in RoboSuite/Franka-Kitchen/Meta-World? Is there a hierarchical policy, a scheduler, or simple concatenation of skill conditionings? Are you using an existing approach for this? The \"composition\" of skills alone would be an extra method to be analyzed by a full paper.\n\n- Optimization flow for Eq. (5): Are the encoder and policy parameters updated by the contrastive separability loss used for skill selection, or are they fixed during selection? If updated, how is this coordinated with the other objectives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "294tCThJW0", "forum": "xFmxnyNYZJ", "replyto": "xFmxnyNYZJ", "signatures": ["ICLR.cc/2026/Conference/Submission7870/Reviewer_qDyg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7870/Reviewer_qDyg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902583765, "cdate": 1761902583765, "tmdate": 1762919903197, "mdate": 1762919903197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a world-model learning system with an active data collection strategy. The world model is trained using a standard variational objective augmented with proposed sufficiency scores and a minimality loss. The data collection policy is learned by maximizing information gain. Experiments on simulated manipulation and locomotion benchmarks show improved policy performance with the learned world model."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a novel paradigm of world-model learning with active data collection strategy.\n2. Experimental results show improved task completion performance when compared with other baselines."}, "weaknesses": {"value": "1. The paper is difficult to follow. It introduces uncommon concepts without sufficient explanation, presents a complex system design, and lacks clarity in the demonstration of results.\n\n2. The system relies heavily on external information and sub-modules, such as reward design and state mask learning, which could limit its practical applicability in open environments.\n\n3. While experiments are conducted on established benchmarks like DMControl and MetaWorld, newer and more reliable benchmarks, such as SimplerENV[1], are not considered. Additionally, real-world experiments are essential to validate the world model's generalization performance.\n\nFor further details, please refer to the questions.\n\n\n[1] Li X, Hsu K, Gu J, et al. Evaluating Real-World Robot Manipulation Policies in Simulation[C]//Conference on Robot Learning. PMLR, 2025: 3705-3728."}, "questions": {"value": "1. Some concepts introduced in this paper, such as structural learning, unstructured latent states, and segment separability, are not properly explained. The authors could consider remove redundant concepts or provide clearer explanations. Additionally, terms like \"informative\" (explained on Line 275) should be defined upon first mention. \n2. Some terms are used imprecisely. For example, \"intervention\" in robotics typically refers to altering a running system’s dynamics to correct or prevent danger. However, in this context, it seems to refer to the data collection phase. \n3. No efficiency metrics, such as inference or training time, are provided to demonstrate the efficiency and generalizability of the policy shown in Figure 1. \n4. The legend in Figure 6 is missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KRoYzAJIiS", "forum": "xFmxnyNYZJ", "replyto": "xFmxnyNYZJ", "signatures": ["ICLR.cc/2026/Conference/Submission7870/Reviewer_d8kx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7870/Reviewer_d8kx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907622514, "cdate": 1761907622514, "tmdate": 1762919902853, "mdate": 1762919902853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce MIST-WM, a new way to learn world models with latent representations which are minimal, but task-sufficient. This is achieved by a co-design of learning useful interventions on the side of the agent, and an adaptive curriculum on the side of the environment. For the agent side, it builds on Dreamer-v3, and adds a projection of its latent state with a filter of learned masks to get to the minimal yet sufficient factored representation. Interventions are performed with a skill library that is learned using DIAYN and METRA in a way that maximizes mutual information about selected individual values of the latent representation while minimizing it about the other values. The curriculum  optimizes the ordering of tasks and environment, aiming to minimize world-model learning error and maximize cumulative rewards across environments and tasks. Evaluation of various aspects is performed on DMControl, RoboSuite, and Meta-World. Results demonstrate the improved sample-efficiency on most tasks of the robotic manipulation and locomotion tasks tested, and improved generalisation compared to baselines. Different ablation studies are included as well."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* the problem of learning compact yet well-performing world models which generalize well to new situations is highly relevant and timely\n* the authors provide concise definitions of task-sufficient and minimal representations, as well as loss functions which encourage learning them\n* empirical results show an improved sample-efficiency for most of the tested benchmarks\n* analysis reveals that the learned representations have higher correlations with ground truth variables that those of other baselines\n* improved compositional and unseen generalization is notable"}, "weaknesses": {"value": "* The paper tries to pack too many things into one paper and as a result, sacrifices clarity and reproducibility in the process.\n* More focus on clear explanations of the whole approach, including pseudo-code and details of all necessary intermediate steps would be helpful.\n* As is, it is not clear how latent states of the Dreamer model are projected into the factored representation of MIST-WM."}, "questions": {"value": "* Regarding the projection of the Dreamer latents to MIST-WM: how is the dimensionality $d_{i}$ of the $\\mathbf{s}_{t}^{i}$ initialized? When is it expanded and by how much? Who decides on that, and is the right dimensionality per task assumed to be known?\n* What do the ensemble member functions $f_{\\theta}^{m}$ in Eq. (7) model exactly and how are they trained?\n* Is there a specific reason you use PPO for some and SAC for other environments?\n* The KL-divergence in Eq. (1) looks suspicious, since $s_{t}^{i}$ \nappears on both sides of the conditioning bar of $q_{\\theta}$ and $p_{\\theta}$. Is this a mistake?\n* What do the error bars in figures 6 and 7 signify?\n* Will source code be published to reproduce results?\n\nMinor point:\n\n* The claim that \"humans do not predict future pixels or track redundant detail when planning\" (l. 43 on p. 1) seems rather strong. Do you have any references to back this up? If not, maybe this claim is not really necessary and should be left out."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0fnXNroBht", "forum": "xFmxnyNYZJ", "replyto": "xFmxnyNYZJ", "signatures": ["ICLR.cc/2026/Conference/Submission7870/Reviewer_AWLT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7870/Reviewer_AWLT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013654144, "cdate": 1762013654144, "tmdate": 1762919902376, "mdate": 1762919902376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}