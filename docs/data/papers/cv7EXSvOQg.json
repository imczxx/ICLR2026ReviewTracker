{"id": "cv7EXSvOQg", "number": 11618, "cdate": 1758202521339, "mdate": 1759897564402, "content": {"title": "VAFL: Vector-field Assisted Functional Layer for Equal-Compute Multi-modal Learning", "abstract": "We study whether energy-based refinement can improve multi-modal learning under strict equal-compute constraints. We present VAFL (Vector-field Assisted Functional Layer), a lightweight post-processing mechanism that applies K=2 Langevin dynamics steps to refine hidden representations from a unified transformer backbone. Our approach addresses a critical challenge in multi-modal learning: achieving consistent improvements across diverse modalities without substantial computational overhead. \n\nOn a unified 6-layer transformer (d=384, h=6), VAFL demonstrates: (1) Text perplexity reduction from 22.5 to 20.8 on WikiText-2 (7.6% improvement), (2) Image MSE reduction from 0.0320 to 0.0290 on CIFAR-10 patches (9.4% improvement), (3) Audio MSE reduction from 0.0450 to 0.0410 on Speech Commands mel-spectrograms (8.9% improvement). These gains require only 50M additional FLOPs (4.2% increase from 1.20G to 1.25G), satisfying the equal-compute constraint (<5% overhead).\n\nWe introduce SOMA (Synergistic Optimization for Multi-modal Assessment), a composite metric balancing quality (inverse loss), diversity (Distinct-2), and stability (refinement delta p95). VAFL achieves 5.9% SOMA improvement (0.680→0.720) while maintaining inference latency under 52ms. Ablation studies confirm K=2 as optimal: K=1 underperforms (SOMA=0.698), while K>2 shows diminishing returns with linear cost increase. Our energy function uses a compact 256-dim hidden layer (0.3M parameters), demonstrating that effective multi-modal refinement doesn't require architectural scaling.", "tldr": "We propose VAFL, an energy-based refinement layer using K=2 Langevin dynamics steps that improves multi-modal generation quality by 7.6% (text), 9.4% (image), and 8.9% (audio) with only 4.2% additional compute.", "keywords": ["multi-modal learning", "energy-based models", "Langevin dynamics", "equal-compute protocol", "unified transformer", "WikiText-2", "CIFAR-10", "Speech Commands"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c951ed1935e1d425747c14e4c2fb5facfd3fc5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a functional layer that computes the energy of the latent features. The layer is in charge of energy descent both in training and testing. The trained model achieves marginal improvement compared to the baseline."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The experiment details are given"}, "weaknesses": {"value": "1. Poor presentation. The authors do not provide enough background for the paper, such as an introduction to the energy-based model, the motivation for the energy definition, and related papers. There is no reference to existing works in the Related Works section.\n2. Lack of testing on the downstream performance. While the trained model achieves certain improvements in the pretraining metrics, there is no test on its downstream performance. Evidence suggests that pretraining performance might not correlate with the downstream one [1].  \n3. Wrong usage of Langevin dynamics. The authors mention that the noise term is ignored during inference. Without the noise term, the ODE cannot able to be turned into a probability flow that transports between probability distributions, violating the EBM formulation. This practice suggests that the performance improvement might not come from the EBM formulation but from artifacts, e.g., more FLOPS. \n4. The figures are not informative. For example, Figure 3 is repetitive given the result in Table 1.\n5. The proposed metric SOMA is not novel and confusing. The Distinct-2 exists in [2]. The quality measurement is just a kind of average over the losses. The Stability metric is confusing, as it achieves the optimal when all the latents are not changed. This violates the motivation of refining the latents. \n6. The vision dataset is not convincing. Could this method generalize to larger-scale image datasets like ImageNet1K?\n\n[1] Denoising diffusion autoencoders are unified self-supervised learners. ICCV 2023.\n\n[2] A diversity-promoting objective function for neural conversation models. NAACL 2016.\n\nOverall, I think this paper reads like an LLM auto-generated paper, and it is far below ICLR's acceptance criteria."}, "questions": {"value": "Could the author formalize the definition of energy in the EBM formulation? For example, what is the target distribution that you are trying to model, and what is the intuition behind the energy definition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CkZdkFrbj0", "forum": "cv7EXSvOQg", "replyto": "cv7EXSvOQg", "signatures": ["ICLR.cc/2026/Conference/Submission11618/Reviewer_pPkn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11618/Reviewer_pPkn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760634443484, "cdate": 1760634443484, "tmdate": 1762922692619, "mdate": 1762922692619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents VAFL. a method that applies two steps of Langevin dynamic to refine multi-modal representations under equal-compute constraints. It also introduces SOMA as evaluation metric that linearly combines quality, diversity and stability."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The idea of refining hidden representations rather than scaling parameters is conceptually interesting"}, "weaknesses": {"value": "- The proposed method is essentially a minor post-hoc gradient update with a learned MLP. Similar approaches have been explored extensively, such as TENT [1] o PPLM [2]. The work does not have any comparison with such post-training refinement methods and is remained unclear what is the advantage of proposed method.\n-  The experiments are extremely shallow. It shows improvements of 7-9% on small scale datasets that are within noise margins. There is no comparison to any real baselines.\n- The introduced SOMA is poorly motivated and it appears that it is designed to inflate reported gains. The proposed weights in linear combination are chosen without any justification or sensitivity analysis.\n\n[1] Wang, Dequan, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. \"Tent: Fully test-time adaptation by entropy minimization.\" arXiv preprint arXiv:2006.10726 (2020).\n\n[2] Dathathri, Sumanth, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. \"Plug and play language models: A simple approach to controlled text generation.\" arXiv preprint arXiv:1912.02164 (2019)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o2awEMtp7O", "forum": "cv7EXSvOQg", "replyto": "cv7EXSvOQg", "signatures": ["ICLR.cc/2026/Conference/Submission11618/Reviewer_6twf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11618/Reviewer_6twf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761172272398, "cdate": 1761172272398, "tmdate": 1762922692235, "mdate": 1762922692235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VAFL, an energy-based refinement mechanism applying Langevin dynamics steps to multi-modal transformer representations. Evaluated on WikiText-2, CIFAR-10, and Speech Commands, it shows perplexity/MSE improvements with minimal FLOPs overhead. A new SOMA metric is introduced to jointly measure quality, diversity, and stability."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-  I am genuinely struggling to find any."}, "weaknesses": {"value": "- The exposition is unclear, the paper does not specify how the representations are combined/unified, and then unembedded for the perplexity/MSE evaluations.\n- No baselines or comparison are provided, either with other multi-modal methods, refinement approaches, or simply training the base model longer with equivalent compute. Improvements are uninterpretable without context.\n- Limited scope: Small model (31M params), short training (10k steps), standard benchmarks, training objective-only evaluation. No downstream tasks or realistic scale experiments.\n- SOMA metric unjustified: Arbitrary weights and components with no principled design or external validation. Only used to evaluate the authors' own method.\n- Weak motivation: No explanation for why Langevin refinement helps multi-modal learning specifically. Missing theoretical analysis or intuition.\n- Insufficient related work: Entirely omits discussion of modern multi-modal methods and other related work.\n\nOverall: A simple idea with preliminary validation; this reads as an incomplete course project rather than a mature research contribution, well below the standards of rigor expected at ICLR."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wbc2SqD9Tp", "forum": "cv7EXSvOQg", "replyto": "cv7EXSvOQg", "signatures": ["ICLR.cc/2026/Conference/Submission11618/Reviewer_qGux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11618/Reviewer_qGux"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694155971, "cdate": 1761694155971, "tmdate": 1762922691724, "mdate": 1762922691724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VAFL (Vector-field Assisted Functional Layer), an energy-based refinement module that applies short Langevin-style updates (K=2) on hidden states from a Transformer to improve text/image/audio under a self-imposed “equal-compute” budget."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Addresses compute-efficiency as a practical constraint. \n2. Implementation is simple (a small per-position MLP “energy” and 1–2 refinement steps) with modest overhead"}, "weaknesses": {"value": "1. Misleading framing: The method is presented as an “energy-based Langevin refinement,” but with τ=0 it degenerates into a deterministic residual update. This is conceptually equivalent to a cheap adapter block, not a genuine energy-based method.\n2. Toy-scale evaluation: Experiments are confined to WikiText-2, CIFAR-10, and Speech Commands with ~30M-param models. These are far below community standards and provide no evidence that the method works in realistic multi-modal or large-scale settings.\n3. Lack of fair baselines: There is no comparison to equal computation alternatives, such as LoRA, adapters, or simply adding one extra Transformer block under the same compute budget. This makes the reported gains unconvincing."}, "questions": {"value": "1. EBM vs. deterministic refiner: With τ=0, what is gained by the EBM/Langevin framing over a standard residual adapter/refiner?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wEhbYtH9hj", "forum": "cv7EXSvOQg", "replyto": "cv7EXSvOQg", "signatures": ["ICLR.cc/2026/Conference/Submission11618/Reviewer_yJ67"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11618/Reviewer_yJ67"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793697926, "cdate": 1761793697926, "tmdate": 1762922691166, "mdate": 1762922691166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VAFL (Vector-field Assisted Functional Layer), a lightweight energy-based refinement layer that improves multi-modal model performance under strict compute constraints. It refines hidden representations through short Langevin dynamics guided by a learned energy function, without modifying the base architecture. The paper also introduces a new SOMA (Synergistic Optimization for Multi-modal Assessment) metric to jointly evaluate quality, diversity, and stability across text, image, and audio modalities."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. New idea for improving compute-efficiency: the paper leverages energy-based refinement but not the conventional way of modifying architecture components, which is somewhat novel.\n2. Empirical gains under strict equal-compute constraint: The paper shows some improvements across text, vision, and audio tasks with only small amount of extra FLOPs."}, "weaknesses": {"value": "This paper evidently does not meet the standard of a clear, well-structured, and comprehensive conference style paper. There's limited discussion on related work. The format and experiment depth looks more like a class project rather than an academic paper. The method section lacks clarity. A lot of parts (e.g., section 4.6) are entirely missing."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ph8mK05ljt", "forum": "cv7EXSvOQg", "replyto": "cv7EXSvOQg", "signatures": ["ICLR.cc/2026/Conference/Submission11618/Reviewer_WhWL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11618/Reviewer_WhWL"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049155451, "cdate": 1762049155451, "tmdate": 1762922690640, "mdate": 1762922690640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}