{"id": "cL3M1VwZyn", "number": 21624, "cdate": 1758319784744, "mdate": 1759896911989, "content": {"title": "Turning Speech Language Models into Multilingual Listeners", "abstract": "Speech Language Models (SLMs) that understand spoken language questions and commands support only a few high-resource languages, limiting access to modern technology for millions of speakers worldwide. This gap in language coverage stems from the scarcity of multilingual speech-language instruction-tuning datasets. To address this issue, we present MULTISPEECHQA, a large-scale, synthetically generated and human-verified dataset comprising 9200 hours of more than 10.8 million spoken question-answer pairs in 23 typologically diverse languages, designed to improve the multilingual instruction-following capabilities of SLMs. Using MULTISPEECHQA, we also introduce MULTISPEECH-BENCH, a multi-task benchmark to evaluate SLM performance across 23 languages. We compare the performance of a strong cascading system to three leading open-weight SLMs on MULTISPEECH-BENCH and find that the cascading system outperforms all existing open-weight SLMs. We then demonstrate the effectiveness of MULTISPEECHQA by fine-tuning the best-performing open-weight SLM, Qwen 2.5-Omni, on our dataset, which substantially improves its performance and establishes new state-of-the-art results for open-weight models on our benchmark. Our findings show that high-quality synthetic datasets offer a scalable solution to improving the multilingual capabilities of SLMs, extending the benefits of natural spoken interactions to a wider range of language", "tldr": "", "keywords": ["multimodality", "multilingual", "benchmark", "speech language models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2ed945f105b5ae165af997045e6e51a7acf61786.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles the scarcity of multilingual instruction-tuning data and evaluations for spoken QA by releasing MULTISPEECHQA—10.8M QA pairs (~9,200 hours) across 23 languages—built by translating VA-400K prompts/answers and synthesizing spoken questions (XTTS/Seamless/MMS), with native-speaker checks showing good comprehension but mixed naturalness. It also introduces MULTISPEECH-BENCH, which combines a human-corrected QA subset with CommonVoice (ASR) and CoVoST-2 (AST) to evaluate SQA/ASR/AST uniformly. Using LLM-as-a-judge, the authors find that among open-weight SLMs Qwen2.5-Omni is strongest on QA, while a strong cascading baseline (Whisper v3 → Aya 8B) remains competitive. LoRA-finetuning Qwen2.5-Omni on MULTISPEECHQA lifts its average QA win rate to 60.6% versus the base model, with ASR/AST staying roughly unchanged. A from-scratch SALMONN-style study shows training on all 23 languages beats using 10, while adding 20% AST data doesn’t yield consistent gains. Overall, the work argues that large-scale synthetic multilingual SQA can effectively post-train open-weight SLMs and provides datasets/benchmarks intended for community use."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Propose a speech dataset that has good coverage on 10.8M spoken QA pairs (~9,200 hours) across 23 languages, which is rare for SQA datasets.\n2. Plans to release data and open-weight models; pipeline uses public MT/TTS. It is good for speech research community.\n3.  Native-speaker ratings (≥2 raters per language, except Czech) show good content-understanding, diagnosing TTS naturalness as the main bottleneck."}, "weaknesses": {"value": "1. QA win rates rely on a single LLM-as-a-judge (Command-A) without reported human calibration or bias checks.\n2. BLEU-only for AST; lacks native-speaker evaluation and more comprehensive semantic metrics.\n3. Core dataset is synthetic speech; limited evidence on robustness to real, noisy, accented speech.\n4. A strong Whisper→LLM cascade remains hard to beat; ASR/AST see little improvement after fine-tuning."}, "questions": {"value": "1. In Fig. 3 & 4, the win rates are evaluated by a single LLM (Command-A). Did you run any judge-bias checks to ensure its preferences have a positive correlation with human judgments? My suggestion is to run an experiment to verify the alignment between Command-A and human raters, rather than using Command-A simply because it covers these languages to evaluate translation quality. Otherwise, this setup may ignore the relationship between model preferences and human preferences. Thus, the resulting metric is not very convincing to me.\n\n2. Although Qwen and Phi do not disclose their training data, they may have used real-world audio. MULTISPEECHQA uses synthetic speech—could this harm these models’ understanding of real speech? Beyond reducing compute, was LoRA also chosen to avoid such degradation? If we train a speech model from scratch and include MULTISPEECHQA as training data, would the resulting model show a gap in understanding real-world speech compared with models trained on real audio? I believe an ablation study is needed to substantiate the dataset’s quality and to assess any potential risks to speech language models pretraining.\n\n3. Using WER for ASR is fine. But is BLEU too limited for AST? BLEU is an n-gram overlap metric and is less sensitive to paraphrases, multilingual/morphologically rich languages, and semantic equivalence. I don’t think it is a good evaluation metric. Would human evaluation by native speakers in these languages be better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OC2yyDt82W", "forum": "cL3M1VwZyn", "replyto": "cL3M1VwZyn", "signatures": ["ICLR.cc/2026/Conference/Submission21624/Reviewer_YrCu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21624/Reviewer_YrCu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811905098, "cdate": 1761811905098, "tmdate": 1762941858607, "mdate": 1762941858607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MULTISPEECHQA, a large synthetic multilingual spoken QA dataset spanning 23 languages. It is created by translating the Voice Assistant 400K text pairs and synthesizing the speech with text-to-speech systems. In addition, it presents MULTISPEECH-BENCH, a multilingual and multi-task benchmark that evaluates Spoken QA, ASR, and AST tasks. The authors benchmark several open-weight spoken language models and a strong cascade baseline, then fine-tune Qwen2.5-Omni on MULTISPEECHQA, which yields higher QA win rates. Overall, the paper aims to scale multilingual instruction-following for spoken language models through synthetic data generation and standardized evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Expanding SLMs beyond high-resource languages is both timely and impactful, as the paper addresses a key challenge: data scarcity in multilingual spoken instruction-following.\n- MULTISPEECH-BENCH unifies QA, ASR, and AST tasks across the same set of languages, providing a strong cascade baseline and evaluations with multiple open-weight SLMs, which makes it a valuable resource for the community.\n- The authors at least measure synthetic quality (naturalness, content) and manually verify the QA test subset, which is a good practice even if the results expose weaknesses."}, "weaknesses": {"value": "- The heavy reliance on an LLM-as-a-judge for multilingual QA without verifying its correlation with human judgments reduces trust, especially for typologically diverse languages and speaking styles. Conducting a small human evaluation on the QA test set (beyond TTS or translation quality) would help.\n- Human edits were required for 72% of translations in the benchmark subset, and the TTS outputs show only moderate naturalness on average, with notably low scores in some languages. These issues raise concerns about the overall data quality.\n- Since MULTISPEECHQA and MULTISPEECH-BENCH originate from the same data source, it is not surprising that the fine-tuned Qwen model achieved state-of-the-art results on MULTISPEECH-BENCH. Moreover, the lack of a clear separation between speaker sets in the training and test data could lead to speaker overlap, which may influence the evaluation results.\n- As a benchmark paper, although the authors argue that existing benchmarks lack sufficient language coverage, they do not discuss any existing speech or audio language model benchmarks. The paper would benefit from a more thorough comparison between MULTISPEECH-BENCH and other available benchmarks in this area."}, "questions": {"value": "In addition to the weaknesses mentioned above, I have the following questions and comments:\n- In Table 3, ASR results are reported using WER. However, this metric may not be suitable for certain languages such as Japanese and Chinese, where CER is typically used. Could you clarify which metric was applied?\n- Typically, a table’s caption is placed above the table, but Tables 1 and 2 do not follow this convention.\n- Figure 2 appears to be missing a reference in the main text.\n- There are several typos throughout the paper, including “speeech,” “eplicitly,” “whcih,” “AYA Expance 8B,” and “BLUE score.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5zuZWJE2JZ", "forum": "cL3M1VwZyn", "replyto": "cL3M1VwZyn", "signatures": ["ICLR.cc/2026/Conference/Submission21624/Reviewer_zNsQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21624/Reviewer_zNsQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942005638, "cdate": 1761942005638, "tmdate": 1762941858413, "mdate": 1762941858413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to address the lack of multilingual instruction-tuning data for Speech Language Models (SLMs), based on the fact that most existing SLMs focus on English or some other high-resource languages, limiting accessibility and global applications. It proposes a synthetic data pipeline to synthesize multilingual instruction-tuning data and open-source the synthesized datasets. The paper demonstrates that automated synthetic data pipelines can effectively scale multilingual capabilities of SLMs.\n\nMain contributions:\n1. MULTISPEECHQA - a large-scale, synthetic multilingual datasets with 10.8 million spoken question-answer pairs in 23 languages.\n2. MULTISPEECH-BENCH - a multilingual benchmark suite for evaluating SLMs on SQA, ASR and AST.\n3. Synthetic data pipeline for multilingual instruction-tuning employing machine translation models and multilingual TTS systems that produces high-quality speech-text pairs. The pipeline also validate the data quality by human evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\nThe paper introduces an approach for scaling multilingual SLMs through synthesizing and human-verifying data based off English datasets.  The originality of the paper lies in \n\nQuality:\n1. The paper covers details of the dataset construction pipeline, and introduces quantitative human evaluation by native speakers on the naturalness and comprehension quality of the synthetic data of each language.\n2. The authors conducted comprehensive benchmarking on the generated evaluation set, containing cascaded system baseline and open-source SLMs such as Qwen2-Audio, Phi-4-Multimodal, etc.\n\nClarity\nThe manuscript is well-organized and easy to follow.  Proper figures and charts make the paper more intuitive.\n\nSignificant\nThe release of MULTISPEECHQA and MULTISPEECH-BENCH fills gaps in multilingual SLMs training and evaluation, enabling the research community to develop more inclusive and accessible SLMs."}, "weaknesses": {"value": "Potential limitations of data quality:\n1. Given the scale of the synthetic data used in MULTISPEECHQA (10.8 million samples), the scale of the human validation is 2 small (20 samples per language).\n2. Variance in TTS naturalness scores may indicate that some languages contain more noise in the dataset.\n\nEvaluation setup:\n1. LLM-as-a-judge: the judge LLM's multilingual capability needs to be calibrated (perhaps using other LLMs to test the correlation/stability, or even human validation).\n2. Limited task diversity in evaluation: other SLM benchmarks (such as AIR-Bench[1]) contain more diverse tasks, it may be worth extending the bench suite to cover more (e.g. summarization, language ID, emotion recognition)\n3. Due to the absence of commercial SLMs (e.g. GPT4o, gemini) in benchmark, the claim of SOTA results is thin.\n\nThere's lack of ablation study, it's not clear which components of MULTISPEECHQA datasets contribute the most to the gain.\n\n\n[1] Yang, Qian, et al. \"Air-bench: Benchmarking large audio-language models via generative comprehension.\" arXiv preprint arXiv:2402.07729 (2024)."}, "questions": {"value": "See suggestions in Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FK6EX7unm7", "forum": "cL3M1VwZyn", "replyto": "cL3M1VwZyn", "signatures": ["ICLR.cc/2026/Conference/Submission21624/Reviewer_UZCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21624/Reviewer_UZCo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762063475037, "cdate": 1762063475037, "tmdate": 1762941858178, "mdate": 1762941858178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}