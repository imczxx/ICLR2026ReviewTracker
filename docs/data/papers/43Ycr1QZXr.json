{"id": "43Ycr1QZXr", "number": 6904, "cdate": 1758001099919, "mdate": 1763046920244, "content": {"title": "Event-Anchored Frame Selection for Efficient Long-Video Understanding", "abstract": "Massive frame redundancy and limited context window make efficient frame selection crucial for long-video understanding with large vision-language models (LVLMs). Prevailing approaches, however, adopt a flat sampling paradigm which treats the video as an unstructured collection of frames. In this paper, we introduce $\\textbf{E}$vent-Anchored $\\textbf{F}$rame $\\textbf{S}$election $\\textbf{(EFS)}$, a hierarchical, event-aware pipeline. Leveraging self-supervised DINO embeddings, EFS first partitions the video stream into visually homogeneous temporal segments, which serve as proxies for semantic events. Within each event, it then selects the most query-relevant frame as an anchor. These anchors act as structural priors that guide a global refinement stage using an adaptive Maximal Marginal Relevance (MMR) scheme. This pipeline ensures the final keyframe set jointly optimizes for event coverage, query relevance, and visual diversity. As a $\\textbf{training-free, plug-and-play module,}$ EFS can be seamlessly integrated into off-the-shelf LVLMs, yielding substantial gains on challenging video understanding benchmarks. Specifically, when applied to LLaVA-Video-7B, EFS improves accuracy by $\\textbf{4.7\\\\%, 4.9\\\\%, and 8.8\\\\%}$ on VideoMME, LongVideoBench, and MLVU, respectively. Code is provided in the supplementary material and will be released publicly.", "tldr": "Event-aware, training-free keyframe selection drastically improves LVLMs‚Äô understanding of long videos", "keywords": ["Video Understanding", "Video-based LLM", "Frame Selection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/27f215f57b1ffb72b108eef9a28735073ff2522d.pdf", "supplementary_material": "/attachment/dff418c6ef2978d0c3bba29057e221a040fad000.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Event-Anchored Frame Selection (EFS), a training-free and plug-and-play pipeline for keyframe selection in long-video understanding. The method first segments a video into visually homogeneous events via DINOv2 embeddings, then selects query-relevant ‚Äúanchor‚Äù frames using BLIP2-ITM, and finally performs an adaptive Maximal Marginal Relevance (MMR) refinement to balance coverage, relevance, and diversity. EFS is evaluated across three long-video QA benchmarks (VideoMME, LongVideoBench, MLVU) and demonstrates consistent accuracy gains for multiple LVLMs (LLaVA-Video, LLaVA-OneVision, Qwen2.5-VL). The approach is efficient, modular, and empirically robust."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-structured pipeline.** The paper combines unsupervised visual segmentation, query-aware anchor selection, and adaptive refinement in a coherent multi-stage framework.\n\n- **Training-free and efficient.** The method requires no fine-tuning, making it easily deployable to various LVLMs.\n\n- **Extensive empirical validation.** Results cover multiple benchmarks, models, and frame budgets, with detailed ablations and qualitative visualizations.\n\n- **Clear presentation.** The paper is clearly written with strong figures (e.g., Fig. 1‚Äì4) and thorough experimental setups."}, "weaknesses": {"value": "- **Limited Novelty and Incremental Contribution.** While EFS is elegantly engineered, its components‚Äîevent segmentation, anchor-based selection, and MMR refinement‚Äîare individually well-established. Similar ‚Äúevent-aware‚Äù or structure-guided frame selection frameworks have appeared in AKS (Tang et al., 2025), $T^{*}$ (Ye et al., 2025), Logic‚Äëin‚ÄëFrames (Guo et al., 2025), mDP3 (Sun et al., 2025), VSI (He et al., 2025) and Nar-KFC (Fang et al., 2025). The adaptive MMR is a useful tweak but not a conceptual breakthrough. The overall framework feels like a careful integration rather than a fundamentally new idea.\n\n- **Lack of Theoretical or Analytical Insight.** The method is largely empirical and heuristic. There is no principled analysis or theoretical grounding to justify why event segmentation + adaptive MMR leads to better reasoning beyond intuitive explanations. A formal examination of coverage‚Äìdiversity trade-offs or selection optimality is missing.\n\n- **Heavy Reliance on Handcrafted Hyperparameters.** The approach depends on manually chosen values for event number ùëÄ, relaxation factor ùõº, and fixed frame sampling rate (1 fps). These hyperparameters are dataset-specific (Appendix C.2), raising concerns about reproducibility and generalization to other video domains or tasks.\n\n- **No End-to-End Learning or Adaptivity.** Although the training-free nature is a strength for efficiency, it also restricts adaptivity. The pipeline cannot learn task- or model-specific cues, limiting scalability to more complex or diverse LVLM architectures. The authors themselves acknowledge this in the ‚ÄúFuture Work‚Äù section (B.2).\n\n- **Ambiguous Distinction from Shot Detection.** The ‚Äúevent partitioning‚Äù step relies on detecting local minima of temporal similarity curves, which is conceptually close to traditional shot boundary detection. While Table 3 provides some empirical differentiation, the conceptual novelty over shot detection remains insufficiently justified.\n\n- **Limited Exploration Beyond QA.** The paper‚Äôs experiments focus exclusively on multiple-choice video QA. The method‚Äôs claimed generality (e.g., for captioning, summarization, or temporal reasoning) is not demonstrated, leaving the broader utility speculative.\n\n- **Missing Analysis on Failure Cases and Selection Behavior.** While qualitative examples are shown, there is no systematic analysis of when EFS fails‚Äîe.g., for videos with smooth transitions, repetitive scenes, or motion-heavy segments where visual similarity is unreliable. Understanding such behaviors would strengthen the paper‚Äôs empirical claims.\n\n- **Inter-module Independence and Information Loss.** The three stages (DINOv2 partition ‚Üí BLIP2-ITM anchor ‚Üí adaptive MMR) are executed sequentially with no feedback loop. Errors from earlier stages (e.g., missegmented events or irrelevant anchors) can propagate without correction. An end-to-end or iterative variant might better capture dependencies.\n\n- **No Statistical Significance or Variance Reporting.** The paper reports average accuracies but lacks variance, confidence intervals, or statistical tests across runs. This makes it hard to judge the reliability of reported 1‚Äì2% gains, especially for large models with stochastic inference."}, "questions": {"value": "1. How does EFS perform when applied to continuous motion videos (e.g., sports, surveillance) where event boundaries are less distinct?\n2. Could the adaptive threshold in MMR be learned or calibrated dynamically instead of being fixed per dataset?\n3. Is there any performance degradation when substituting DINOv2 or BLIP2-ITM with other encoders (e.g., CLIP-L/14)?\n4. How sensitive is the performance to the choice of ùëÄ and ùõº? Can these be auto-tuned?\n5. Have the authors evaluated whether event segmentation correlates with semantic event changes rather than just visual discontinuities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cVbfbW0TOA", "forum": "43Ycr1QZXr", "replyto": "43Ycr1QZXr", "signatures": ["ICLR.cc/2026/Conference/Submission6904/Reviewer_RzF8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6904/Reviewer_RzF8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761336329477, "cdate": 1761336329477, "tmdate": 1762919146646, "mdate": 1762919146646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "YEuRjJLZGo", "forum": "43Ycr1QZXr", "replyto": "43Ycr1QZXr", "signatures": ["ICLR.cc/2026/Conference/Submission6904/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6904/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763046918774, "cdate": 1763046918774, "tmdate": 1763046918774, "mdate": 1763046918774, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an event-anchored frame selection method, termed as EFS. EFS first detects events using DINO features of consecutive frames. Within each detected event, key frames are subsequently selected using a VLM. To enhance diversity, an adaptive Maximal Marginal Relevance (MMR) mechanism is then applied. The whole pipeline is training free and plug and play, show clear improvements on three commonly used long video QA benchmarks. Moreover, EFS provides notable efficiency and performance gains compared to prior frame selection methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of anchoring frame selection on detected events is intuitive. Addressing the challenge of selecting representative frames in very long videos, event detection serves as a meaningful precursor for identifying key segments.\n2. The efficiency of the proposed approach is particularly impressive. As shown in Table 4, EFS significantly reduces preprocessing time relative to previous methods, while simultaneously achieving superior performance."}, "weaknesses": {"value": "1. While EFS shares insights with prior methods such as **BOLT**, **AKS**, and **NFC**, which also target relevance and diversity in selected frames, the distinctive advantages of EFS over these approaches remain insufficiently discussed.\n2. Although Table 4 highlights clear efficiency gains relative to AKS, the underlying factors contributing to this improvement are not adequately analyzed.\n3. The paper omits discussion of Q-Frame, a recent baseline that also leverages query‚Äìframe relevance for selection. \n4. The core contribution‚Äîevent-anchored frame selection‚Äîis primarily evaluated on general video QA benchmarks. It remains uncertain whether EFS preserves key events effectively in event-centric tasks. Incorporating evaluation on event-focused datasets (e.g., grounded video QA [1], video grounding, or dense video captioning) would better validate the proposed framework‚Äôs strengths.\n    \n    [1], Di et al. Grounded Question-Answering in Long Egocentric Videos. CVPR25"}, "questions": {"value": "1. The main concern lies in understanding why EFS yields more diverse and query-relevant frames compared to prior methods. What are the key differences that enable this improvement?\n2. Would EFS better than other frame selection methods on event centric tasks ?\n3. The method assumes single-query relevance. In real-world applications, LVLMs are often used in multi-turn dialogues, where subsequent questions differ from the initial one. Has the method been evaluated under multi-turn or conversation-driven settings to verify its robustness across query variations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pyffllLfCo", "forum": "43Ycr1QZXr", "replyto": "43Ycr1QZXr", "signatures": ["ICLR.cc/2026/Conference/Submission6904/Reviewer_GB4z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6904/Reviewer_GB4z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725202271, "cdate": 1761725202271, "tmdate": 1762919146230, "mdate": 1762919146230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Event-Anchored Frame Selection (EFS), a training-free hierarchical framework for keyframe selection in long-video understanding. The core contribution is a three-stage pipeline that: (1) partitions videos into visually coherent events using DINOv2 embeddings, (2) selects the most query-relevant frame from each event as an \"anchor\", and (3) performs anchor-guided global refinement via adaptive Maximal Marginal Relevance to balance event coverage, query relevance, and visual diversity. As a plug-and-play module, EFS significantly boosts various LVLMs' performance on long-video QA benchmarks (e.g., +4.7% on VideoMME, +4.9% on LongVideoBench, +8.8% on MLVU for LLaVA-Video-7B), demonstrating that event-aware selection substantially outperforms flat sampling paradigms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Paradigm Shift to Event-Aware Frame Selection\nTransitions from traditional flat sampling to a hierarchical event-aware framework, emphasizing the narrative structure and event boundaries of videos‚Äîa conceptually novel direction.\n\n2. Training-Free and Plug-and-Play Design: EFS serves as a standalone preprocessing module that integrates seamlessly with existing LVLMs without fine-tuning\n3. Adaptive Diversity Control Mechanism: Introduces an anchor-guided adaptive MMR strategy that dynamically adjusts diversity thresholds based on video content.\n\n4. Demonstrates consistent and notable improvements across multiple LVLMs on three challenging long-video QA benchmarks‚ÄîVideoMME, LongVideoBench, and MLVU‚Äîconfirming broad applicability and effectiveness."}, "weaknesses": {"value": "Indirect frame selection evaluation - Only uses downstream QA accuracy, lacking direct metrics (temporal IoU, coverage/redundancy scores) to validate whether selected frames truly correspond to key moments.\n\nLimited comparison scope - Missing comparisons with recent segment-based methods (e.g., K-frames), making it difficult to isolate whether advantages come from event partitioning or redundancy reduction.\n\nInsufficient connection to temporal grounding - The anchor selection mechanism shares similarities with temporal moment retrieval methods, but these connections remain unexplored in related work."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ef2qkD1QFs", "forum": "43Ycr1QZXr", "replyto": "43Ycr1QZXr", "signatures": ["ICLR.cc/2026/Conference/Submission6904/Reviewer_jeUm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6904/Reviewer_jeUm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987973173, "cdate": 1761987973173, "tmdate": 1762919145807, "mdate": 1762919145807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Event-Anchored Frame Selection (EFS), a training-free method for selecting keyframes from long videos to improve the performance of LVLMs. The key idea is to move beyond \"flat\" sampling strategies by first understanding the video's event structure. EFS works in three steps: it (1) partitions the video into visually coherent events, (2) selects the most query-relevant frame from each event as an \"anchor,\" and (3) performs a global refinement to add diverse frames. Extensive experiments on three long-video QA benchmarks show that EFS consistently and significantly boosts the accuracy of various LVLMs, especially when the frame budget is small."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles a practical bottleneck in long-video understanding, that is, efficient and informative frame selection. The focus on leveraging event structure is a logical approach.\n- EFS is training-free and plug-and-play, making it practical. \n- The experimental results convincingly demonstrate its effectiveness across multiple models and datasets."}, "weaknesses": {"value": "- Novelty. The novelty of the proposed event partitioning method seems somewhat overstated. Since similarity scores from Dinov2 may not fully capture semantic-level changes (i.e., shifts in action or plot), the method appears to focus mainly on low-level changes like camera cuts and obvious scene transitions. As Figure 4 suggests, the partitioning is primarily based on simple, low-level units rather than high-level events. I would suggest that the authors visualize more examples to clarify the partitioning process. Moreover, the main contribution to the final performance seems to stem from BLIP2-ITM rather than the event partitioning. This is supported by Table 2, which shows that a random partition still achieves good performance, while the random anchor baseline performs poorly.\n- Computational cost. The preprocessing step (feature extraction with DINOv2 and BLIP2) introduces more latency, which may be a concern for real-time applications despite being an offline cost. For hours-long videos, the problem becomes crucial. \n- Moreover, the performance is tied to potential biases of DINOv2 and BLIP2.\n- Hyperparameter sensitivity. The method relies on several hyperparameters (e.g., event number M, relaxation factor Œ±) whose optimal values vary across datasets. This requires tuning and could hinder out-of-the-box usability. How to adaptively determine such factors?\n- Limited gains for genral query. While the method excels on specific queries, its advantages for very general query is limited like \"summarize this video\", as noted in the appendix."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nBW7YBuoI0", "forum": "43Ycr1QZXr", "replyto": "43Ycr1QZXr", "signatures": ["ICLR.cc/2026/Conference/Submission6904/Reviewer_cumw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6904/Reviewer_cumw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762118378796, "cdate": 1762118378796, "tmdate": 1762919145425, "mdate": 1762919145425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}