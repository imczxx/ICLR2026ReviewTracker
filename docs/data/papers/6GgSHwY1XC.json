{"id": "6GgSHwY1XC", "number": 21557, "cdate": 1758318947063, "mdate": 1759896915952, "content": {"title": "Binding Visual Features Point by Point", "abstract": "Despite success on standard benchmarks, vision language models display persistent failures on tasks involving processing of multi-object scenes, including many tasks that are relatively easy for humans. Recent work has found that these failures may stem from a basic inability to accurately bind object features in-context, a challenge that is referred to as the ‘binding problem' in cognitive science and neuroscience. The human visual system is thought to solve this binding problem via serial processing, attending to individual objects one at a time so as to avoid interference from other objects. Here, we investigate `pointing' -- the use of explicit spatial coordinates to refer to objects -- as an analogous solution for vision language models. We find that learning to point-via-text induces an internal visual search routine, and we characterize the mechanisms that support this procedure. We also find that pointing behavior can be generalized to new tasks via fine-tuning, and that doing so eliminates binding errors and enables compositional generalization. These results provide a proof-of-principle that serial processing can solve the binding problem for vision language models just as it does for biological vision.", "tldr": "", "keywords": ["cognitive science", "binding", "sequential processing", "vision language models"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a519fde373b926cc208eb9225b17bbbcd7b0b3e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles binding errors in vision language models by asking the model to point to each object with xy coordinates before giving the answer. This induces a step by step visual search that reduces mix ups between features and objects. Mechanism work is done on Molmo 7B and shows a tight match between internal attention centroids and the emitted coordinates with a peak effect in middle layers and a small set of search heads that appear to drive the routine. Overall, this paper is a timely and interesting study with a clear and simple pointing strategy and convincing mechanism signals."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of the paper is clear and interesting. The research topic is generally interesting and timely.\n2. The finding on pointing-via-text is accompanied by an internal visual search routine, similar to human serial attention is a interesting finding."}, "weaknesses": {"value": "1. Empirical experiment coverage is a bit narrow. The mechanism analysis is on Molmo 7B and the training tests are on Qwen2 VL 7B. It make the conclusion and empirical results' generalizability cross family and model scale remain unclear\n2. The scope of data is also a bit limited. Synthetic counting uses simple colored shapes. Real world counting focuses on people from Pixmo. something like occlusion and more in the wild counting tasks can strengthen the experiment."}, "questions": {"value": "1. While learning to perform point-via-text can improve binding error and some related tasks, does this affect more general reasoning capacity of VLMs?\n\nAlso please refer to weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Qhx1Svdmsd", "forum": "6GgSHwY1XC", "replyto": "6GgSHwY1XC", "signatures": ["ICLR.cc/2026/Conference/Submission21557/Reviewer_LcCk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21557/Reviewer_LcCk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524382460, "cdate": 1761524382460, "tmdate": 1762941833990, "mdate": 1762941833990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates why vision language models often struggle with understanding scenes that contain multiple objects. The authors propose a “pointing via text” approach in which the model identifies each object by providing its spatial coordinates before giving an answer. Analysis shows that this method creates an internal visual search process, which helps them overcome binding errors and improves their ability to generalize to new visual tasks, showing that serial visual processing can effectively solve the binding problem in vision language models just as it does in human vision."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides deep interpretability by discovering specific “search heads” responsible for guiding the model’s serial attention, revealed through layer-wise and head-level causal mediation analysis.\n2. This paper systematically examines the binding errors that occur in VLMs when processing multi-object scenes and identifies attentional interference as the key mechanism behind these failures.\n3. The experiments show that pointing-trained models outperform direct-answer baselines by a large margin across all tested tasks. On both synthetic and real-world datasets, the models that learn to point eliminate binding errors, count objects more accurately, and generalize well to unseen numbers of objects and novel visual configurations."}, "weaknesses": {"value": "1. The experiments focus mainly on counting and visual search tasks, which are simplified and synthetic compared to real-world multimodal challenges. This narrow scope makes it unclear whether the proposed method would scale effectively to more complex visual reasoning or natural images beyond object-level tasks.\n2. The “pointing via text” behavior is learned through explicitly supervised coordinate annotations, which are labor-intensive to obtain. This raises questions about scalability and practicality for broader datasets or real-world applications where such supervision is unavailable.\n3. The study primarily contrasts the pointing-based model with a single “direct-answer” baseline. It does not compare against other potential solutions to the binding problem, such as attention modulation, iterative reasoning, or reinforcement-based grounding, which would strengthen the empirical claims.\n4. The evaluation relies primarily on controlled datasets and small-scale test environments rather than widely recognized multimodal benchmarks. As a result, it’s uncertain how well the approach generalizes across standard large-scale evaluation settings.\n5. Although the paper identifies “search heads” within Molmo-7B, it’s unclear whether similar mechanisms emerge in other architectures or model scales. The interpretability findings may thus be model-specific rather than general to vision-language systems.\n6. While the paper draws inspiration from human serial attention, it does not quantitatively evaluate whether the model’s attention sequence aligns with human visual search behavior. The analogy to biological vision remains conceptual rather than empirically supported."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yDZ5j6ppb9", "forum": "6GgSHwY1XC", "replyto": "6GgSHwY1XC", "signatures": ["ICLR.cc/2026/Conference/Submission21557/Reviewer_qpJr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21557/Reviewer_qpJr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652346909, "cdate": 1761652346909, "tmdate": 1762941833758, "mdate": 1762941833758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the binding problem in VLMs—where features from different objects get mixed in multi-object scenes. It shows that training models to “point via text” (use explicit spatial coordinates) induces an internal visual search routine that sequentially attends to objects, reducing feature swaps and enabling compositional generalization; the pointing behavior then transfers to new tasks with light fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The visualizations do a great job illustrating the induced “point-via-text” serial search: they make the pre- vs. post-training binding failures immediately visible and show fewer feature–object swaps in multi-object scenes.\n2. The paper’s mechanistic analysis is strong: it links pointing supervision to an internal visual search routine and argues that this serial processing reduces binding errors and supports compositional generalization, with behavior transferring to new tasks after light fine-tuning. \n3. Performance is strong, with consistent gains over baselines on multi-object benchmarks."}, "weaknesses": {"value": "1. Novelty/positioning. The core idea—training VLMs to output/location-align coordinates—overlaps with prior coordinate- or point-supervised work (e.g., PixelLLM, Pix2Seq, GLIP/Grounding-DINO). The paper should more sharply distinguish its contribution beyond this family [1-5].\n2. Supervision & scalability. The method relies on explicit coordinate/point signals (“point-via-text”), which raises annotation-cost and robustness questions for datasets without reliable coordinates; prior literature shows point/coordinate labels are non-trivial to obtain at scale [6].\n3. Causal evidence. The paper claims that pointing induces an internal serial search routine, but the support appears mainly correlational; stronger causal interventions/ablations would bolster the claim [7].\n\n[1] Chen et al., Pix2Seq: A Language Modeling Framework for Object Detection (ICLR 2022). \n\n[2] Li et al., GLIP: Grounded Language-Image Pre-training (CVPR 2022). \n\n[3] Liu et al., Grounding DINO (ECCV 2024). \n\n[4] Xu et al., Pixel-Aligned Language Model (PixelLLM) (CVPR 2024). \n\n[5] Yang et al., UniTAB: Unifying Text and Box Outputs for Grounded VL Modeling (ECCV 2022). \n\n[6] Bearman et al., What’s the Point? Semantic Segmentation with Point Supervision (ECCV 2016).\n\n[7] Binding Visual Features Point by Point (OpenReview, 2025)."}, "questions": {"value": "Q1: How does the method fundamentally differ from prior coordinate/point-supervised paradigms (Pix2Seq/UniTAB/GLIP/Grounding-DINO/Pixel-LLM) beyond using pointing signals?\nQ2: If you disable the discovered “search head(s),” how do binding metrics and task scores change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nD34KIiPeA", "forum": "6GgSHwY1XC", "replyto": "6GgSHwY1XC", "signatures": ["ICLR.cc/2026/Conference/Submission21557/Reviewer_VWdX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21557/Reviewer_VWdX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005001427, "cdate": 1762005001427, "tmdate": 1762941833572, "mdate": 1762941833572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focus on binding error in multi-object visual tasks, and examine how the \"Point -> answer\" protocol improves the tasks both in-domain and ood from the perspective of explanibility. They firstly diagnose binding failures, then connect the performance with the exact layer and \"search head\", and explore the sequential search mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This work focus on a core failure mode for VLMs, binding error, and define it well.\n2. Section 3.2.2 is compelling. The test about counting and order shows that the model has a kind of step-by-step search habit, which supports the idea well.\n3. The finding locating the behavior at some middle layers and \"search heads\" gives a real handle for future work on model understanding and reliability."}, "weaknesses": {"value": "1. For section 3.2.1, the interventions are good but if adding reversible tests, like masking out the head or removing its effect to see the performance drop, it would be more reasonable.\n2. For section 3.3, the improvements might come from extra training supervision, like using coordinates and multi-step prompts, not from real \"serial attention.\" It would be good to test weaker or noisy training versions."}, "questions": {"value": "1. Section 3.1 argues binding errors comes from compositional-representation–driven attentional interference. Could you re-run the section 3.1 diagnostics on models trained with \"Point -> answer\" protocol to test whether interference metrics actually improve (e.g., reduced attention bias toward distractors)?\n2. Section 3.2.2 is quite interesting. It suggests a left-to-right counting tendency. Is that order imposed by the prompt or most likely aligned with Molmo training data? Can the prompt specify a different direction like right-to-left or top-to-bottom, and does the model follow? In your section3.3 training, which order did you use in your training data? If the training points were shuffled/random-order, what order would \"Point -> answer\" adopt at test time, and how robust is it to coordinate perturbations like in section 3.2.2 do?\n\nIf the authors answer my questions well and make these clarifications, I am happy to raise my review score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BTmtKcTcnS", "forum": "6GgSHwY1XC", "replyto": "6GgSHwY1XC", "signatures": ["ICLR.cc/2026/Conference/Submission21557/Reviewer_eWHG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21557/Reviewer_eWHG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762541949491, "cdate": 1762541949491, "tmdate": 1762941833347, "mdate": 1762941833347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}