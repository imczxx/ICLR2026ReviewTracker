{"id": "38nYZ5QBui", "number": 21366, "cdate": 1758316696417, "mdate": 1759896925873, "content": {"title": "Learning from Synthetic Data Improves Multi-hop Reasoning", "abstract": "Reinforcement Learning (RL) has been shown to significantly boost reasoning capabilities of large language models (LLMs) in math, coding, and multi-hop reasoning tasks.\nHowever, RL fine-tuning requires abundant high-quality verifiable data, often obtained through human-annotated datasets and LLM-as-verifier loops.\nBoth of these data types have considerable limitations: human-annotated datasets are small and expensive to curate, while LLM verifiers have high scoring latency and are costly to operate.\nIn this work, we investigate the use of synthetic datasets in RL fine-tuning for multi-hop reasoning tasks.\nWe discover that LLMs fine-tuned on synthetic data perform significantly better on popular real-world question-answering benchmarks, even though the synthetic data only contain fictional knowledge.\nOn stratifying model performance by question difficulty, we find that synthetic data teaches LLMs to compose knowledge, which we to be a fundamental and generalizable reasoning skill.\nOur work thus highlights the utility of synthetic reasoning datasets in improving LLM reasoning capabilities.", "tldr": "RL fine-tuning LLMs on synthetic data improves real-world multi-hop reasoning by teaching knowledge composition skills", "keywords": ["multi-hop reasoning", "large language models", "reinforcement learning", "synthetic data"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3f018782abe7270931670f6e60221641d39b4ad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates an important and timely question: whether fine-tuning Large Language Models (LLMs) on synthetic data through reinforcement learning, in the absence of real-world knowledge, can improve their multi-hop reasoning capabilities on real-world question-answering (QA) tasks. The authors use two synthetic datasets: GSM-∞ (math word problems) and PhantomWiki (fictional knowledge-base QA), to fine-tune several open-source models of varying scales (Qwen series, Phi-4-mini-reasoning). The experimental results show that despite the synthetic data having no factual overlap with the real-world evaluation benchmarks (such as HotpotQA, 2WikiMultihopQA, and MuSiQue), the fine-tuned models achieve significant performance improvements on these real-world tasks. The authors argue that this improvement stems from the model learning a transferable meta-skill—\"knowledge composition,\" the ability to chain multiple logical inference steps. The study also finds that this performance gain is consistent across different model families and sizes, and does not suffer from overfitting as training data increases."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. As high-quality human-annotated data becomes increasingly scarce, exploring the value of synthetic data is a frontier direction in the LLM field. This paper explores the fundamental question of whether reasoning abilities can be learned independently of factual knowledge, which has significant implications for the training strategies of Large Language Models (LLMs).\n2. The use of completely disjoint synthetic and real-world datasets effectively controls for memorization, providing clearer evidence for skill transfer.\n3. The paper not only reports final results but also provides an in-depth analysis of the model's learning process by examining performance changes during training and stratifying performance by question difficulty. In particular, Figure 3 and Figure 5 clearly demonstrate how the model's progress on more difficult synthetic questions translates to performance improvements on real-world tasks.\n4. The paper is well-structured and flows logically from introduction to conclusion. The research motivation, methodology, and results are all clearly articulated. The figures (especially Figure 2 and Figure 5) intuitively present the core findings and are easy to understand."}, "weaknesses": {"value": "1. The training is conducted exclusively on synthetic data. Although this shows improvement in real-world scenarios, the paper lacks a comparison with a baseline trained on real-world data and tested on real-world data. There is no analysis of the potential performance gap between training on synthetic data versus real-world data.\n2. The experimental models are relatively small, with the largest being 4B. It would be beneficial to see experiments on models of at least 7B parameters. Based on Figures 2 and 3, the performance improvement for Qwen3-1.7B is notably smaller than that for Qwen3-0.6B. Could this be because the 0.6B model has weaker baseline reasoning abilities, and thus training naturally improves its generalizable reasoning performance, while the larger 1.7B model shows diminished gains? What would happen with an even larger model, such as a Qwen3-8B? Would the improvement be minimal?\n3. The paper states that 3 (for GSM-∞) or 11 (for PhantomWiki) CoT examples are used during RL training. Is it necessary to include these CoT examples in the prompt during inference as well? Furthermore, it would be desirable to see experiments on the training and generalization effects in a Zero-Shot setting, i.e., without including any CoT examples.\n4. In Figure 2, the training results on GSM-∞ are consistently worse than on PhantomWiki for nearly all models. Is there a deeper explanation for this? Is it because GSM-∞ focuses more on mathematical reasoning rather than the type of multi-hop reasoning found in the evaluation tasks? If so, does this raise concerns about the generalizability of the reasoning skills being learned?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fPp0OvT9lQ", "forum": "38nYZ5QBui", "replyto": "38nYZ5QBui", "signatures": ["ICLR.cc/2026/Conference/Submission21366/Reviewer_hyPz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21366/Reviewer_hyPz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761221570620, "cdate": 1761221570620, "tmdate": 1762941725616, "mdate": 1762941725616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether RL fine-tuning on purely synthetic data constructed to require multi-hop reasoning but to contain no real-world facts can teach a general skill of knowledge composition that then transfers to established, real benchmarks. Concretely, the authors fine-tune small LLMs  with GRPO on two synthetic sources. Experiments show that RL on synthetic data improves performance on all three downstream datasets and across model families and sizes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* I appreciate that the paper presents a tight experimental design around a single, interpretable hypothesis: i.e., training on universes that are explicitly non-overlapping with real-world knowledge, the study isolates whether multi-hop structure learned in synthetic settings can carry over.\n\n* The paper also tackles an important topic in reasoning/RL, namely the effort to disentangle answer formatting from reasoning.\n\n* I also like how the paper shows curves over checkpoints and stratifying performance by question difficulty (number of hops in PhantomWiki and number of operations in GSM infinite), which provides a richer picture than just showing single endpoint scores."}, "weaknesses": {"value": "* Although the empirical story is neatly organized, in my view, the novelty is modest given the rapidly expanding literature on synthetic data and RL for reasoning.\n  * If I remember correctly, PhantomWiki itself was introduced as an on-demand synthetic universe generator to test reasoning and retrieval while sidestepping data leakage; it feels more like this paper leverages that dataset rather than advancing the generation framework. Likewise, GSM-infinite was created to probe reasoning under controllable arithmetic complexity and long contexts, and here it serves as a training curriculum rather than as a novel contribution.\n  * Similarly from the methodology side, the paper’s RL component employs GRPO but it seems like without much methodological innovation; recent works (a la Deepseek) have shown to a degree that RL alone can elicit sophisticated reasoning behaviors without human step-by-step traces and RLVR can better incentivize process-correctness. The present study feels like a combination and transfer evaluation rather than as a new algorithm etc. The finding of using synthetic data to help with reasoning doesn't seem particular novel to me either.\n\n* If I understand correctly, one of the paper's claims is that knowledge composition is the specific causal skill that helps with performance; in terms of evidence, the paper infers this composition skill mainly from higher F1 values on 2-4 hop datasets and from difficulty-stratified curves; however, I don't see where the paper verifies whether the intermediate steps followed by the model are logically correct and path-faithful?\n\n* While the focus is on RL, I don't see any other baselines (e.g., SFT) on the same synthetic datasets: how do those compare? Without some of these comparisons, it's unclear to me if gains are from RL itself or via additional supervised exposure to reasoning-style data (or something else). The models used seem to also all be sub 4B; I'm not sure how transferable these findings are in generality (e.g., to even 7/8B models or eve 3-4B modes outside of phi-mini) especially since results show that different models show different degrees \"malleability\" to RL (but the paper defers analysis to future work). \n\n* I see that the evaluations are only on 3 datasets (HotpotQA, 2Wiki, MuSiQue); furthermore, results are done on sub-samples ($n=500$ with two seeds if I understand correctly; there is no re-sampling across multiple draws or paired testing. Combined with RL and small models (with potentially high variance), this seems quite small. I wonder how robust the gains shown here are to actual larger samples, increased repeated rates of sampling, etc."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tLLpiulmNf", "forum": "38nYZ5QBui", "replyto": "38nYZ5QBui", "signatures": ["ICLR.cc/2026/Conference/Submission21366/Reviewer_F2Sn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21366/Reviewer_F2Sn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761502751683, "cdate": 1761502751683, "tmdate": 1762941725183, "mdate": 1762941725183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines whether large language models can acquire general reasoning abilities solely from synthetic data, without relying on real-world knowledge.\nUsing reinforcement learning on fully artificial datasets such as PhantomWiki and GSM, the authors show significant performance improvements on real-world multi-hop QA benchmarks.\nThe results suggest that reasoning skills, such as knowledge composition, can transfer across domains, highlighting synthetic data as a scalable alternative to human-annotated reasoning datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The study demonstrates that large language models can acquire generalizable reasoning skills purely from synthetic, knowledge-free data. It provides empirical evidence that these synthetic reasoning abilities transfer to real-world multi-hop QA tasks, achieving substantial performance gains. The approach offers a scalable and cost-effective framework for improving reasoning through verifiable, automatically generated training data."}, "weaknesses": {"value": "- The experiments are limited to multi-hop QA. Even though the training data come from a synthetic world, the fact that performance improves on other multi-hop QA benchmarks is not particularly surprising.\n- The applicability of the approach to grammatically or semantically complex real-world texts remains unknown."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nFCyP6L0vI", "forum": "38nYZ5QBui", "replyto": "38nYZ5QBui", "signatures": ["ICLR.cc/2026/Conference/Submission21366/Reviewer_3JJe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21366/Reviewer_3JJe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862103349, "cdate": 1761862103349, "tmdate": 1762941724842, "mdate": 1762941724842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that RL fine-tuning (GRPO) on purely synthetic multi-hop datasets (GSM-infinity, PhantomWiki) improves LLM performance on real-world QA benchmarks (HotpotQA, 2Wiki, MuSiQue) by teaching a transferable “knowledge composition”."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Provides clear empirical evidence that RL fine-tuning on synthetic datasets (PhantomWiki, GSM-infinity) improves LLM multi-hop reasoning on real-world QA benchmarks.\n\n- Addresses a practical problem, the scarcity and cost of high-quality human annotated, which the paper suggests can be supplemented or replaced by synthetic reasoning data.\n\n- Demonstrates consistent performance gains across multiple model families and parameter scales, indicating robustness and generalizability.\n\n- The experimental details and use of open-source technologies (models and codebase) makes the setup reproducible."}, "weaknesses": {"value": "- The paper’s novelty is limited as prior works have already shown that synthetic data and SFT/RLVR for reasoning works quite well. The contribution is primarily about a different reasoning setup of multi-hopping. \n\n- The domain of synthetic data is narrow, focusing only on arithmetic and relational reasoning, which limits claims of general reasoning transfer.\n\n- The evaluation datasets lack diversity. HotpotQA, 2WikiMultihopQA, and MuSiQue are all two-hop or near two-hop QA tasks, reducing the strength of the generalization claim.\n\n- The paper is unnecessarily verbose at places. The related work section covers too much ground that's not relevant. Stating GRPO equations in Section 3.2 was not really necessary.\n\nMinor nit:\n- reinforcement -> Reinforcement\n- L128: “complicated, RL-based framework” -> why complicated?"}, "questions": {"value": "-  In the abstract, there's this phrase \"“high scoring latency” which is not clear to me. Can you please explain?\n\n- Is there any reason to prefer RL over SFT in this setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ewBuKOpFxL", "forum": "38nYZ5QBui", "replyto": "38nYZ5QBui", "signatures": ["ICLR.cc/2026/Conference/Submission21366/Reviewer_hJzK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21366/Reviewer_hJzK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969752914, "cdate": 1761969752914, "tmdate": 1762941724186, "mdate": 1762941724186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}