{"id": "slbkP1quJj", "number": 10097, "cdate": 1758160620720, "mdate": 1759897674256, "content": {"title": "SerenDiff: Generating Serendipity Recommendations through a Diffusion Model", "abstract": "Serendipity means an unexpected but valuable discovery. It has attracted wide attention in recommender systems research in recent years. Due to its elusive and subjective nature, serendipity is difficult to model even with today's advances in machine learning and deep learning techniques. In addition, most existing serendipity models lack interpretability. To address the modeling challenges and the interpretability issues, we propose a serendipity diffusion recommendation model (named SerenDiff), to generate serendipity recommendations leveraging a state-of-the-art generative AI model, the diffusion model. We regarded a user history with a recommender system as an \"image\", and the serendipity recommendation generation as a recovering process of the corrupted \"image\". Diffusion models are believed to be creative in the recovering process of a noised image in the sense that they base on but go beyond the original training data, providing room for finding serendipity. Extensive experiments have shown the effectiveness of SerenDiff. We believe SerenDiff will empower everyday users, not only with increased chances of encountering unexpected but relevant discoveries, but also with explanations on the elusive serendipity recommendations.", "tldr": "In this project, we leveraged a conditional diffusion model as a recommendation model to recommend serendipity.", "keywords": ["serendipity", "recommendations", "Diffusion Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/401aa89887ad0ae07872ff6b6d73f9d7018c31fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SerenDiff, a novel method using a conditional diffusion model to generate serendipitous recommendations. Its key innovation is a 2D user portrait, constructed from axes of relevance and unexpectedness, which guides the generative process. This allows the model to create recommendations in a user's latent serendipity zone. Experiments on the SerenLens dataset show the model effectively identifies serendipitous items."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of framing serendipity as a conditional generative task using diffusion models is original. The 2D user portrait is a creative and effective conditioning mechanism.\n2. The 2D portrait provides a clear visual and intuitive explanation for why a generated recommendation is considered serendipitous.\n3. The model demonstrates state-of-the-art results on serendipity-specific metrics against a comprehensive set of baselines."}, "weaknesses": {"value": "1. The evaluation relies on a single dataset . Findings may not generalize, as \"serendipity\" is highly context-dependent.\n2. The paper is purely empirical. It lacks theoretical analysis on why the diffusion process's \"creativity\" effectively models the cognitive concept of \"serendipity,\" or the properties of the proposed 10x10 portrait space.\n3. The 10x10 portrait is small. The complexity is a concern if finer granularity (a larger portrait) is needed, but this trade-off isn't explored. \n4. The portrait's quality is highly dependent on the models used to calculate expectedness and item embeddings. The impact of these upstream choices is not analyzed."}, "questions": {"value": "1. What is the model's performance and efficiency sensitivity to the portrait size?\n2. Which specific model was used to calculate the crucial expectedness score used to build the portrait?\n3. To ensure reproducibility, would the authors be willing to provide the code？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LiAscHkDOd", "forum": "slbkP1quJj", "replyto": "slbkP1quJj", "signatures": ["ICLR.cc/2026/Conference/Submission10097/Reviewer_jDPy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10097/Reviewer_jDPy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880876076, "cdate": 1761880876076, "tmdate": 1762921479919, "mdate": 1762921479919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work propose SerenDiff, a diffusion-based recommendation model to address the challenge of generating serendipitous recommendations. It claims to be the first work to formulate serendipity recommendation as an item generation task, where a two-dimensional user portrait based on \"expectedness\" and \"relevance\" is constructed and a conditional diffusion model is used to denoise the recommended item generation. Emprically, the proposed method achieved competitive performance on balancing serendipity and relevance metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Compared to previous work on diffusion model for recommendation, the work provides an interesting perspective to construct user profile by explicitly decomposing serendipity into expectedness and relevance. \n- The two-dimensional user portrait enhances model interpretability as shown in the case studies, and the ablation study confirms its utility."}, "weaknesses": {"value": "- Missing technical details to fully understand the proposed method and reproducibility (see more in Questions)\n- Technical novelty lies more in the creation of user portrait with interaction data, but less in understanding why the diffusion process is suited (or needs to be adapted) for discovering serendipity beyond its general generative capabilities.\n- Some nuanced issues in evaluation setting, which might weaken the validity of the results (see more in Questions)"}, "questions": {"value": "- In the User Portrait Construction, the portrait is a 10x10 grid. How are continuous expectedness and relevance scores quantized onto this discrete grid? What happens if multiple historical items fall into the same grid? \n- The calculation of unexpectedness is based on the conditional likelihood of seeing an item given a user's history, but such calculation tends to suffer from exposure bias. Is any debiasing techniques used is ensure robustness to data bias, etc?\n- The channel dimension of the \"user portrait image\" is quite high compared to standard images, as it correspond to the item embedding dimension (in this case, 384 as indicated in Appendix sec. A). How is diffusion training affected by the selection of this dimension? Can you discuss more about the stability of training when scaling up this dimension?\n- In evaluation, several choice needs to be more careful. For example, sampled evaluation is used in this paper, but has shown to be misleading by [1]; cross-validation is used to create train/test splits, while in recommendation this type of random splits evaluation might cause leakage [2], where temporal based splits might be preferred. More rigorous evaluation is needed to better justify the empirical results.\n\n\n[1] Walid Krichene and Steffen Rendle. 2022. On sampled metrics for item recommendation. \n\n[2] Zaiqiao Meng, Richard McCreadie, Craig Macdonald, and Iadh Ounis. 2020. Exploring Data Splitting Strategies for the Evaluation of Recommendation Models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "74YMw2wZEq", "forum": "slbkP1quJj", "replyto": "slbkP1quJj", "signatures": ["ICLR.cc/2026/Conference/Submission10097/Reviewer_WGA2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10097/Reviewer_WGA2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981987375, "cdate": 1761981987375, "tmdate": 1762921479532, "mdate": 1762921479532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a serendipity diffusion recommendation model (SerenDiff), to generate serendipity recommendations leveraging diffusion model. It regards the user historical behaviors as an image, and the serendipity recommendation generation as a recovering process of the corrupted image. Extensive experiments and analyses have demonstrated its effectiveness and interpretation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The investigated question is interesting and valuable. \n2. Results in Table 1 proves that SerenDiff generally outperforms existing baselines under the Serendipity-Oriented Experiments, achieving statistically significant gains in most columns.\n3. The case study is easy-to-understand, it visualizes the distances and directions of these serendipitous movies compared to users’ typical reach, offering a form of interpretation."}, "weaknesses": {"value": "1. Could the authors clarify the technical contributions of the paper? As conditional diffusion models have already been widely applied in recommender systems, the contribution from this aspect appears to be marginal.\n2. The description of the method is not very clear. For instance, what are the dimensions of M^u and M^u_{\\text{seren}} mentioned in Sections 4.1 and 4.3, and what do they represent? The authors bolded these symbols in the figures but not in the main text, which may cause confusion for future readers.\n3. I’m concern about the authors’ claim that their method significantly outperforms 6 out of 9 baselines according to a two-tailed t-test.\n4. Among the baselines used for comparison, only SerenPrompt and DreamRec are from 2024, while all the others are before 2023. Besides, the compared sequential recommenders (BERT4Rec and SASRec) are relatively early SR works. However, even so, they still significantly outperform SerenDiff in the relevance-oriented experiments.\n5. Lack of the statistics of the utilized datasets.\n6. Lack of the source code and corresponding datasets, and the author promise to release the source code on GitHub after the paper is accepted."}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "72bFRTQ732", "forum": "slbkP1quJj", "replyto": "slbkP1quJj", "signatures": ["ICLR.cc/2026/Conference/Submission10097/Reviewer_nF6o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10097/Reviewer_nF6o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993590206, "cdate": 1761993590206, "tmdate": 1762921479077, "mdate": 1762921479077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The text introduces SerenDiff, a novel serendipity diffusion recommendation model that addresses the challenge of modeling and interpreting serendipity by adapting a conditional diffusion model for item generation. The core novelty involves transforming a user’s one-dimensional history sequence into a two-dimensional, image-like user portrait that explicitly represents the trade-off between the two essential facets of serendipity: unexpectedness and relevance. SerenDiff then leverages diffusion models, treating recommendation as a process of recovering a corrupted “image”, to generate serendipity points corresponding to hidden user demands."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The methodology is well motivated. The generative nature of diffusion models is well-suited for exploring the latent space beyond immediate user relevance.\n2. Transforming 1D user interaction sequences into 2D \"images\" based on expectedness and relevance is a novel approach. It effectively translates a standard recommendation problem into an \"image\" generation problem.\n3. The 2D user portrait provides good interpretability."}, "weaknesses": {"value": "1. A large number of time steps ($T=1,000$) in SerenDiff is inefficient compared to classic recommendation models, as shown in Appendix.\n2. The number of time steps is a key parameter in diffusion models, and a hyperparameter study would help determine its optimal value."}, "questions": {"value": "In image generation tasks, diffusion models typically use around 1000 steps. However, in recommendation tasks, models such as DiffRec only use 5–10 steps. Is it necessary to use as many as 1000 steps in this setting? Moreover, how does SerenDiff achieve inference speed only about twice as slow as SASRec despite using 1000 steps? Any tradeoff between efficiency and performance when choosing the number of time steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cm8mmCYfrY", "forum": "slbkP1quJj", "replyto": "slbkP1quJj", "signatures": ["ICLR.cc/2026/Conference/Submission10097/Reviewer_u6GH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10097/Reviewer_u6GH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762797308237, "cdate": 1762797308237, "tmdate": 1762921478572, "mdate": 1762921478572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}