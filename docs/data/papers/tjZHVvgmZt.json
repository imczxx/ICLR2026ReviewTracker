{"id": "tjZHVvgmZt", "number": 18672, "cdate": 1758289888473, "mdate": 1759897088563, "content": {"title": "Rustify: Towards Repository-Level C to Safer Rust via Workflow-Guided Multi-Agent Transpiler", "abstract": "Translating C to Rust at the repository level presents unique challenges because of complex dependencies and the differences between C's manual memory model and Rust's strict ownership rules.\nExisting approaches often overlook repository-wide dependencies and contextual structure, resulting in impractical and potentially unsafe Rust code.\nWe propose Rustify, a workflow-guided architecture for repository-level C-to-Rust translation. \nRustify decomposes the translation process into modular and role-specific stages, aligned with the structure of source repositories rather than dynamic plan-and-act strategies. These stages address the structural challenges of repository-level C-to-Rust translation, such as determining translation units and resolving cross-file dependencies, with each stage assigned to a dedicated LLM-powered agent.\nBy orchestrating these agents through a structured workflow, Rustify systematically manages repository-wide translation while maintaining semantic equivalence and memory safety.The framework further integrates compiler feedback through tree search–based iterative repair and leverages a dynamic experience base to guide future translations.\nExperiments show that Rustify consistently produces fully memory-safe Rust code, reducing unsafe code by up to 99% compared to prior approaches. It successfully compiles eight out of nine repositories, raises the test pass rate from 10.5% in the baseline to 86.4%, and achieves high-quality, idiomatic Rust translation with CodeBLEU scores reaching 0.76.\nA replication repository is available at https://github.com/rustify712/Rustify.", "tldr": "We present Rustify, a workflow-guided multi-agent framework that translates entire C repositories into safe, idiomatic Rust with high compilation and test success rates.", "keywords": ["Program translation; Repository-level code translation; Agent-based workflow; Large language models."], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e91bee6134dfbf0af5c76fa96549115315cabc5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Rustify, a workflow-guided, multi-agent framework for repository-level C-to-Rust translation, comprising four components: ProjectManager, TechLeader, CodeMonkey, and ErrorResolver. The results show that Rustify successfully compiles eight out of nine repositories."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A workflow-guided, multi-agent architecture is proposed that decomposes the translation process into four specialized stages.\n\n- A dynamic experience base and compiler-guided tree search are introduced for iterative repair.\n\n- Experiments against multiple baselines show that the approach outperforms existing methods."}, "weaknesses": {"value": "- There is limited empirical support for the repository-level claims. The authors report that Rustify successfully translates eight out of nine repositories. However, these include one code-level benchmark (HumanEval) and four small projects (<600 LOC), which do not demonstrate effectiveness on large codebases with complex cross-file dependencies.\n \n- The paper should provide a systematic study of common C → Rust translation errors at the repository level, including which error types are most frequent, which are successfully repaired, and which remain challenging."}, "questions": {"value": "- No data are provided on translation speed, token usage, or API calls, making it difficult to assess the practicality and scalability of this computationally intensive method.\n \n- The paper does not discuss whether the framework can be generalized to other programming language translation tasks, and evaluation on larger, more diverse repository datasets such as RepoTransBench is needed.\n\n- Workfolw is linear and unidirectional. \n\n- The dynamic experience base functions as a local memory, but its technical details and scope (project-specific or shared) are unclear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e0jo3mcqPY", "forum": "tjZHVvgmZt", "replyto": "tjZHVvgmZt", "signatures": ["ICLR.cc/2026/Conference/Submission18672/Reviewer_ScML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18672/Reviewer_ScML"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404572974, "cdate": 1761404572974, "tmdate": 1762928370794, "mdate": 1762928370794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Rustify, a workflow-guided architecture for the repository-level C-to-Rust translation task. The approach decomposes the translation task into modular with different role-specific agents. The experiments show that the proposed approach Rustify outperforms baselines in generating more safer, more compilation correct, and more functionality correct Rust code."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper targets an important problem, i.e., repository-level C-to-Rust translation. \n\nThe paper is well organized."}, "weaknesses": {"value": "The novelty of the approach is limited. In particular, the workflow-guided architecture has been widely explored in existing agents for software development work[d,e,f,g]. The sperate roles of project managers and code Monkey (i.e., programmers) are not new ideas; in addition, using the error message as feedback has also been widely adopted in many code generation and comprehension tasks, even for code translation task[b,c]; in addition, decomposing the repository to modules have also been explored in existing repository-level code translation (i.e., Alphatrans[a]). Therefore, the major novelty of this work remains questionable compared to existing literature. More justification is definitely necessary to address this limitation. \n\n[a] AlphaTrans: A Neuro-Symbolic Compositional Approach for Repository-Level Code Translation and Validation. FSE’25\n\n[b] Lost in translation: A study of bugs introduced by large language models while translating code. ICSE’24\n\n[c] Exploring and Unleashing the Power of Large Language Models in Automated Code Translation. FSE’24\n\n[d] Experimenting a new programming practice with llms\n\n[e] When llm-based code generation meets the software development process\n\n[f] Communicative agents for software development\n\n[g] Multi-agent software development through cross-team collaboration\n\n\nThe evaluation is insufficient, in terms of projects, metrics, and baselines.\n\nEvaluation projects. The criteria of projects selection is unclear, especially some project (e.g,. HumanEval) seems to be a very unreasonable project for the repository-level C-to-Rust translation task evaluation. In particular, HumanEval is a code generation tasks, while each of the coding task is a separate problem without any dependency. It is very unsuitable to use the benchmark as a repository-level translation task, which is different from real-world repository-level translation task (as the challenge here is the dependencies among code files and functions.) It further comes to the question how authors select the evalution projects and whether they are truly suitable projects for this task. \n\nIn addition, the test sufficiency of studied projects remains unclear. As shown in Table-1, the number of tests is very limited for one repository. For example, only one test for urlparser projects, and only 10 tests for the binn projects (with over 4k lines of code). It is necessary to present the sufficiency of the tests (e.g., the line and branch coverage). Otherwise, the calculated metrics (i.e., pass rate) is not convincing, which might overlook many potential semantic errors. \n\nMore recent and state-of-the-art code translation techniques are missed as baselines. In particular, [a,b,c,d] are more recent LLM-driven code translation techniques, that are very relevant with the current approach. It is necessary to compare with them or give the reason for not doing so. \n\n[a] Exploring and Unleashing the Power of Large Language Models in Automated Code Translation. FSE’24\n\n[b] Spectra: Enhancing the code translation ability of language models by generating multi-modal specifications\n\n[c] Syzygy: Dual code-test C to (safe) rust translation using llms and dynamic analysis\n\n[d] Rustmap: Towards project-scale c-to-rust migration via program analysis and LLM"}, "questions": {"value": "1.\tPlease clarify the novelty of the proposed approach.\n\n2.\tPlease justify the selection criteria of evaluation projects and explain why HumanEval is a reasonable choice.\n\n3.\tPlease justify the test sufficiency of the tests in the studied projects.\n\n4.\tPlease justify the missing baselines in the current evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S0MycSzd1O", "forum": "tjZHVvgmZt", "replyto": "tjZHVvgmZt", "signatures": ["ICLR.cc/2026/Conference/Submission18672/Reviewer_UR2t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18672/Reviewer_UR2t"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663073363, "cdate": 1761663073363, "tmdate": 1762928368987, "mdate": 1762928368987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an agentic pipeline towards transpiling C to safe and idiomatic Rust. The paper proposes using 4 agents each specialized towards a given task in the C to Rust conversion. \n\n- The project manager: does dependency analysis on the C project, creates a dependency graph and proposes modules that can be transpiled\n- The tech leader takes the relevant modules and dependency graphs from the project manager and then orders them. It also does heuristic merging to prevent overly fragmented tasks.\n- Code monkey: It analyzes the C code from the tech leader, performs retrieval (the paper proposes RAG for this), and then generates Rust code. It proposes multiple candidates and filters relevant candidates.\n- Error Resolver: Observes compiler errors (if any) , selects the candidate from the code monkey with least amount of compiler errors and tries to address them in an iterative manner."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The technique outperforms baselines mentioned in the paper. Further the authors test their technique on various models (DeepSeek-v3), Got-4o, and Claude-3.5-Sonnet, and show consistent gains. The authors further showcase the validity of passing relevant context, repair iterations, and external knowledge."}, "weaknesses": {"value": "Previous techniques like [SYGYZY (Jain et. al.)](https://arxiv.org/abs/2412.14234) also perform tasks like dependency analysis, and module-based translation. How does Rustify compare against techniques like SYGYZY? Further the paper does not compare or discuss techniques such as [VERT (Yang et. al.)](https://arxiv.org/abs/2404.18852) that also produce Safe Rust. \n\nFurther, the following details are missing, and need to be provided to understand the validity of the approach and soundness of the approach: \n\n- \"strategies used by the model to solve common types of errors\" used by the Code Monkey agent: The number of samples in the 2 broad categories. how are the candidates retrieved?\n- Details of the granularity of the code modules (lines of code, functions merged using dependency-based merging and line-based merging) proposed by the Tech Lead agent. How do different levels of granularity affect the final performance?  How is the merging done (do the authors use an LLM to do this?)\n- Line 185: \"Each agent receives sufficient upstream context to ensure semantic preservation and stability\" : what sort of context \n- Comparison with open source models that are efficient at coding? \n- Limited evaluation data : Recent work [CRUST-bench (Khatry et. al.)](https://arxiv.org/abs/2504.15254) proposes 100 benchmarks that test C to safe Rust transpilation over multiple domains.\n- Details on number of completions from each model (from the Code Monkey agent) - the paper mentions multiple candidates are retrieved, in how many cases is are the completions correct?\n- Ablations on the repair candidates: The paper assumes that taking the code candidate with minimum number of errors would yeild the best performance, but errors might be correlated. Further, the Rust compiler builds in a stage wise manner, hence type errors supersede borrowing errors, hence it seems that the error selection approach is naive. \n- Error analysis on failure cases, where does Rustify fail, what sort of errors are harder to resolve? Reasons for low sucdess on libtree, json.h and binn?\n- Comparison with other agentic techniques like SWE agent, Codex, ClaudeCode, AutocodeRover etc. Further, while I understand that closed source language models are being updated rapidly, why are deprecated models like gpt-4o and claude-3.5-sonnet used and not latest models like o3 (released in April,2025), gpt-5(August 7), claude-4-sonnet/claude ( released in May) not used?\n- No results on idiomaticity: the authors claim that Rustify improves idiomatic Rust generation, but do not quantify or qualitatively analyze how. \n- What test cases are used to measure test pass rate? What are the tests? Are they unit tests or integration tests?"}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qyXEwsMxIl", "forum": "tjZHVvgmZt", "replyto": "tjZHVvgmZt", "signatures": ["ICLR.cc/2026/Conference/Submission18672/Reviewer_iEDT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18672/Reviewer_iEDT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962119563, "cdate": 1761962119563, "tmdate": 1762928368592, "mdate": 1762928368592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Rustify, a workflow-guided, multi-agent framework for repository-level C-to-Rust translation. It divides the translation process into four specialized LLM-driven stages, context analysis, task planning, translation, and repair,  each handled by a dedicated agent (Projec tManager, Tech Leader, Code Monkey, Error Resolver). The system maintains an experience base to guide future translations and integrates compiler feedback in a tree-search–based repair loop. Evaluations on several benchmarks and open-source repositories show that Rustify achieves gains over some prior work (C2Rust, MetaGPT and Pan et al. 2024)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper provides a coherent decomposition of the translation task into modular agents.\n\n* The experimental results (in terms of unsafe ratio, compilation rate, pass rate and CodeBLEU with respect to a reference translation) show progress over the considered baselines.\n\n* The ablation study in Table 3 isolates the contribution of each module, showing that each of them contributes to the final result."}, "weaknesses": {"value": "* Test coverage is not reported for the considered benchmarks. The number of test cases in Table 1 for some benchmarks seems very low (e.g. urlparser only has one test). Without test coverage, it's hard to know how meaningful the pass rates in Table 2 are. If I/O equivalence to the source code is not maintained, then all the other metrics are not very informative. \n\n* Evaluation of correctness performed by the translation agent (Code Monkey) is limited to compilation. Although compilation feedback is incorporated at the sub-task level, test results are only used once the entire repository has been translated and compiled. Thus, I/O equivalence to the source code is never part of the translation or repair feedback loop, and thus not actually used to guide translation.\n\n* There is no discussion of compilation and execution errors present in the translated benchmarks. For instance, binn is completely failing -- nothing even seems to compile. Some discussion of what's happening there would be interesting."}, "questions": {"value": "* What's the test coverage offered by the tests mentioned in Table 1 for each benchmark? \n\n* Does x under binn's Compilation Rate in Table 2 mean that 0% of the translation could be compiled? If so, do you have any explanation for that behaviour?\n\n* How did you obtain the translated Rust reference code? \n\nMinor:\n* What does \"Code\" for HumanEval mean in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KIGCs3pJ7e", "forum": "tjZHVvgmZt", "replyto": "tjZHVvgmZt", "signatures": ["ICLR.cc/2026/Conference/Submission18672/Reviewer_Qcky"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18672/Reviewer_Qcky"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762462344813, "cdate": 1762462344813, "tmdate": 1762928368015, "mdate": 1762928368015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}