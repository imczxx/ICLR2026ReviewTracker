{"id": "xn9fS09Yir", "number": 23963, "cdate": 1758350902777, "mdate": 1759896788860, "content": {"title": "Can Large Language Models Truly Stay Helpful Harmless and Honest?", "abstract": "Alignment of Large Language Models (LLMs) along multiple objectives—helpfulness, harmlessness, and honesty (HHH)—is critical for safe and reliable deployment. Prior work has used steering vectors—small control signals injected into hidden states—to guide LLM outputs, typically via one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single alignment objective can inadvertently overwrite representations learned for other objectives, leading to catastrophic forgetting. More recent approaches extend steering vectors via one-to-many (1-to-N) Transformer decoders. While this alleviates catastrophic forgetting, na¨ıve multi-branch designs optimize each objective independently, which can cause inference fragmentation—outputs across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch Steering (AMBS), a two-stage 1-to-N framework for unified and efficient multi-objective alignment. In Stage I, post-attention hidden states of the Transformer layer are computed once to form a shared representation. In Stage II, this representation is cloned into parallel branches and steered via a policy–reference mechanism, enabling objective-specific control while maintaining cross objective consistency. Empirical evaluations on Alpaca, BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared to a naıve 1-to-N baseline, while remaining competitive with state-of-the-art methods.", "tldr": "", "keywords": ["LLM", "Alignment", "NLP"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efd3762b1fbeb4bb9c68c2edb7a50bdca14a079c.pdf", "supplementary_material": "/attachment/d8e8fc2101632ddfd9fe60fec262bc3dcba39a25.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents the Adaptive Multi-Branch Steering (AMBS) framework, a novel two-stage one-to-N architecture designed for the unified and efficient multi-objective alignment of llms across Helpfulness, Harmlessness, and Honesty (HHH). The core contribution lies in addressing the dual challenges prevalent in prior steering vector approaches: catastrophic forgetting, common in one-to-one decoders, and inference fragmentation, which arises from naive multi-branch designs. AMBS achieves this by first computing a shared representation from the post-attention hidden state, thereby eliminating redundant computation and mitigating forgetting. This shared state is then fed into parallel branches equipped with a policy-reference mechanism for adaptive steering, which promotes output consistency and effectively reduces fragmentation. Experiments on 7B LLMs demonstrate superior HHH alignment compared to existing steering methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is highly original and significant in addressing a critical practical problem in LLM deployment: achieving harmonious multi-objective alignment without compromising performance on any single objective. The two-stage AMBS design is conceptually elegant, directly tackling the fundamental trade-off between forgetting and fragmentation with a clear technical solution. The use of a shared foundational computation is a strong engineering choice that significantly improves quality and efficiency over naive multi-branching. Furthermore, the introduction of the policy-reference mechanism to ground policy outputs in an oracle-defined alignment space is a creative approach to maintain output consistency across different alignment goals. The clarity of the problem statement and the presentation of the AMBS architecture are commendable, making the proposed solution easy to understand and appreciate. The empirical results on 7B models convincingly demonstrate a substantial improvement in overall alignment performance."}, "weaknesses": {"value": "A primary weakness is the limited scope of the experimental evaluation regarding model scale. The current findings are mostly confined to 7B parameter models, which limits the conclusions about the framework’s scalability and generalizability to both smaller and significantly larger model sizes (e.g., 13B or 70B). Establishing that AMBS makes sense for a full range of model scales is essential for its practical impact. Second, the evaluation of the Helpfulness dimension is insufficiently comprehensive. Relying solely on instruction-following datasets like Alpaca does not fully capture the quality, nuance, and user preference associated with a helpful response. A more rigorous evaluation requires preference-based datasets, e.g. UltraFeedback. Finally, the paper’s claim of efficiency, while conceptually sound due to shared computation, lacks detailed quantitative backing. The work requires a robust, quantitative analysis of the training and inference overhead, including metrics such as latency, throughput, and memory footprint comparisons against the baselines to truly justify the \"efficient\" aspect of the proposal."}, "questions": {"value": "To strengthen the paper, I have the following questions and suggestions for the authors. First, please conduct and report experiments on at least one significantly larger model, such as a 13B or 70B variant, and one smaller model, such as a 3B variant, to thoroughly validate the scalability and robustness of the AMBS framework across the model size spectrum. Second, please integrate more rigorous, preference-based evaluations for Helpfulness. Specifically, incorporating a comprehensive benchmark like Ultrafeedback would provide a much stronger assessment of the model's performance on this crucial objective. Third, please provide a detailed breakdown of the computational cost. This should include a concrete comparison of training time, inference latency, and GPU memory usage for AMBS versus the naive one-to-N multi-branch baseline and the standard one-to-one steering approach. This data is critical to substantiate the efficiency claims. Finally, an ablation study on the steering vector dimensionality $k$ would be beneficial to understand its sensitivity and impact on the performance trade-offs across the HHH objectives."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xGrRAdGZBI", "forum": "xn9fS09Yir", "replyto": "xn9fS09Yir", "signatures": ["ICLR.cc/2026/Conference/Submission23963/Reviewer_dpQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23963/Reviewer_dpQk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798923054, "cdate": 1761798923054, "tmdate": 1762942876150, "mdate": 1762942876150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the steering vectors for LLMs multi-objective alignment. The authors reveals the problem of one-to-many decoders have problems of inconsist ouputs across objectives. So the authors propose Adaptive Multi-Branch Steering. The key is to form a shared representation before applying steering vectors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The presentation, motivation, and visualizations are clear. Experiments are exhaustive."}, "weaknesses": {"value": "I'm not super familiar with this line of works. The main concern/question is it's unclear to me how is it different from naïve one-to-many. In the related work 2.3, the authors claim 1-to-N reuse shared representation. But this work seems to do the same, which makes me confused. So, can you elaborate more on how your method mitigates inference fragmentation?\n\nmissing related works:\n\n[1] Shi, Ruizhe, et al. \"Decoding-time language model alignment with multiple objectives.\" Advances in Neural Information Processing Systems 37 (2024): 48875-48920.\n\n[2] Son, Seongho, et al. \"Robust Multi-Objective Controlled Decoding of Large Language Models.\" arXiv preprint arXiv:2503.08796 (2025).\n\n[3] Zhong, Yifan, et al. \"Panacea: Pareto alignment via preference adaptation for llms.\" Advances in Neural Information Processing Systems 37 (2024): 75522-75558."}, "questions": {"value": "HHH are indeed important aspects of LLMs. But AMBS seems to be effective for any multi-objective alignment (i.e. more than three objectives)？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "J7zisW91EE", "forum": "xn9fS09Yir", "replyto": "xn9fS09Yir", "signatures": ["ICLR.cc/2026/Conference/Submission23963/Reviewer_ddGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23963/Reviewer_ddGY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990725181, "cdate": 1761990725181, "tmdate": 1762942875787, "mdate": 1762942875787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the use of steering vectors for multi-objective alignment in LLMs. The authors highlight that one-to-many decoders can produce inconsistent outputs across different objectives. To address this, they propose Adaptive Multi-Branch Steering, a method that forms a shared representation before applying objective-specific steering vectors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The presentation, motivation, and visualizations are clear. Experiments are exhaustive."}, "weaknesses": {"value": "I'm not super familiar with this line of works. The main concern/question is it's unclear to me how is it different from naïve one-to-many. In the related work 2.3, the authors claim 1-to-N reuse shared representation. But this work seems to do the same, which makes me confused. So, can you elaborate more on how your method mitigates inference fragmentation?\n\nmissing related works:\n\n[1] Shi, Ruizhe, et al. \"Decoding-time language model alignment with multiple objectives.\" Advances in Neural Information Processing Systems 37 (2024): 48875-48920.\n\n[2] Son, Seongho, et al. \"Robust Multi-Objective Controlled Decoding of Large Language Models.\" arXiv preprint arXiv:2503.08796 (2025).\n\n[3] Zhong, Yifan, et al. \"Panacea: Pareto alignment via preference adaptation for llms.\" Advances in Neural Information Processing Systems 37 (2024): 75522-75558."}, "questions": {"value": "HHH are indeed important aspects of LLMs. But AMBS seems to be effective for any multi-objective alignment (i.e. more than three objectives)？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "J7zisW91EE", "forum": "xn9fS09Yir", "replyto": "xn9fS09Yir", "signatures": ["ICLR.cc/2026/Conference/Submission23963/Reviewer_ddGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23963/Reviewer_ddGY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990725181, "cdate": 1761990725181, "tmdate": 1763117209371, "mdate": 1763117209371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adaptive Multi-Branch Steering (AMBS), a two-stage 1-to-N framework for unified and efficient multi-objective alignment. In the first stage, post-attention hidden states of the Transformer layer are computed to form a shared representation across the given objectives. In the second stage, the shared representation is cloned and steered, allowing objective-specific control using steering vectors. This framework requires reference responses in the training data for each objective. The proposed algorithm is designed to alleviate catastrophic forgetting and inference fragmentation, which are the main limitations of the previously proposed algorithms."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of the paper (inference fragmentation, catastrophic forgetting of the existing algorithms) is well introduced, both of which are significant issues in multi-objective alignment.\n2. The paper includes results of multiple experiments, that are targeted to show the effectiveness of the proposed algorithm. The experiments include out-of-distribution evaluations and ablation experiments as well."}, "weaknesses": {"value": "1. The most significant issue I am recognizing currently is that the paper lacks clarity in many parts. I will give the details in the Questions section.\n2. Some experiments seem to imply that AMBS is not particularly doing better than the baseline algorithms.\n3. More comparisons are necessary to convince the readers about the effectiveness of the proposed algorithms, which will be explained further in the Questions section."}, "questions": {"value": "## Questions\n1. Algorithm 1, line 6: in line 191-192, it says the loss $\\mathcal{L}_\\mathrm{cos}^{(n)}$ updates both the steering vector and the hidden states. Why are we not seeing updates to the hidden states in the algorithm?\n2. Algorithm 1, line 8: Is it fine to write a line of explanation that is not part of the actual algorithm?\n3. Table 1: In the base model results, Llama-2-7B does not seem to perform very well compared to other models like Mistral-7B or DeepSeek-7B. Do you perhaps have the baseline algorithm results (that of MARL-Focal, TrinityX for example) based on Mistral-7B, for example?\n4. Table 1 or line 314: As far as I understand, AMBS trains one steering vector per objective. how does \"Full AMBS\" or \"simultaneous HHH alignment\" work? Does it add the steering vectors across 3 objectives to create one steering vector? Or does it do something else?\n5. Table 1, line 309-312: As Aligner achieves the lowest Safety score, which is the best based on the definition in Section 4.2, is this part saying that AMBS is performing worse because of inference fragmentation?\n6. Table 3: When no reference policy was used, how were the steering vectors (or also the hidden states) trained?\n\n## Suggestions\n1. line 179: `pool()`, `r`,  `enc` are not explained in the main text. I suggest moving Appendix A.1 to the main text and move some of the figures or additional experiments in the appendix.\n2. Table 2: I think `Base Model` results should be on the top, not being part of `Implicit Mixing of Steering Vectors` nor `Explicit Controlled Mixing`.\n3. Table 5: While it also uses `SS` to indicate the new 3-point harmlessness objective evaluation performance, this could confuse the readers. How about using the percentage of unsafe outputs instead, as the authors did in line 377?\n4. Table 3: I think for better analysis, we should also test AMBS where the steering vectors only consist of value 1, which eliminates the effect of steering vectors while only leaving the effect of using reference answers for training. Please correct me if I am assuming something incorrectly.\n5. Figure 5 seems to have wrong legend information. Please correct this.\n6. Table 7: I think the paper could benefit from having comparisons with the baseline algorithms (ex. MARL-Focal...)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4JFPmNGb2B", "forum": "xn9fS09Yir", "replyto": "xn9fS09Yir", "signatures": ["ICLR.cc/2026/Conference/Submission23963/Reviewer_y6cB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23963/Reviewer_y6cB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136797528, "cdate": 1762136797528, "tmdate": 1762942873630, "mdate": 1762942873630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a multi-objective large language model alignment method based on steering vectors. The proposed method employs a two-stage architecture, in which the first stage produces a shared representation, and the second stage generates multi-branch outputs by applying a steering vector. The method is evaluated on widely used alignment benchmarks, showing improvements over the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method balances inference time and alignment performance by cleverly employing the two-layer architecture.\n- The empirical results look promising."}, "weaknesses": {"value": "1. The biggest weakness of the paper is its lack of clarity in describing the proposed method.\n    1. The paper needs a preliminary or background section that introduces steering vector methods and the 1-to-N architecture. Not all readers, including myself, are familiar with these algorithms.\n    2. In which space do y+ and y- reside (Section 3.2)? I initially thought the y’s were generated sentences, but in that case, it is unclear how to compute the inner product in Equation (1). Therefore, I assume the y’s are vectors.\n    3. In Section 3.2, the symbol r is undefined.\n    4. A clearer description of the training and inference phases is required. It is unclear to me how the algorithm behaves during inference. Does the method produce three outputs when deployed?\n2. The title of the paper is inappropriately broad given its content. The paper investigates a relatively narrow domain of steering vector methods, not multi-objective optimization in general. In other words, the content of the paper does not directly address the question presented in the title."}, "questions": {"value": "Can this method leverage reward models instead of oracle references?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tnxKOi1HIA", "forum": "xn9fS09Yir", "replyto": "xn9fS09Yir", "signatures": ["ICLR.cc/2026/Conference/Submission23963/Reviewer_Tgs7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23963/Reviewer_Tgs7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762175932894, "cdate": 1762175932894, "tmdate": 1762942872139, "mdate": 1762942872139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}