{"id": "RLazI3aMBY", "number": 6169, "cdate": 1757955843603, "mdate": 1759897931981, "content": {"title": "Local MAP Sampling for Diffusion Models", "abstract": "Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \\mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. \nBuilding on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on inverse scattering benchmarks.", "tldr": "", "keywords": ["Inverse problems", "Diffusion", "MAP"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c82541537d9af34e602f750a10bc858b5b266d25.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper develops an inference time only algorithm for solving inverse problems while leveraging a pre-trained diffusion models. Its main contribution is the formalization of a local maximum a posterior framework for this purpose; at each step of the diffusion process, one computes the maximum a posterior of the clean state distribution conditioned on the observation and the current diffusion state, before sampling back a new noisy state."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is pretty straightforward to read and easy to understand. \n- It tries to reframe various existing methods within a single framework, which is commendable. For example, the paper provides a reinterpretation of TMPD which I find to be actually more interesting than the explanation provided in the original paper [1]. Indeed, in the original paper the authors say that they justify their method is a a new guidance paper but the stopgrad operation used breaks this interpretation. The local MAP framework here is a much sounder explanation that actually recovers what TMPD implements and explains why the stopgrad operation is not so bad after all. \n- Various empirical choices are made and these seem to improve the performance with respect to most existing baselines. \n\n[1] Boys, B., Girolami, M., Pidstrigach, J., Reich, S., Mosca, A. and Akyildiz, O.D., 2023. Tweedie moment projected diffusions for inverse problems."}, "weaknesses": {"value": "A major weakness of the paper is that it makes various imprecise/false claims and does not describe existing methods accurately: \n- In Section 3.2 for example, line 185 to 187, the authors basically claim that DPS and their framework coincides when one uses a \"Gaussian diffusion prior approximation\" (what does this mean exactly?) but this is not explained in the next section and I don't see how any special cases of 11 or 12 yield DPS. \n- In Figure 1 we can see that PiGDM is said to be a special case of LMAPS but here again I am not sure that this is the case. Can the authors clarify for which choice of covariance this holds? \n- The description of DAPS is also inaccurate; DAPS advocates the use of the very specific choice of $\\rho_t = 1$, which ensures the \"decoupling\". Hence (6) is not really what DAPS implements. \n\nThe claims of the paper are also somewhat exaggerated: \n- In line 73  \"replacing heuristic choices in existing solvers.\" but the paper actually proposes a heuristic approach not really backed with theory. For example, is the upperbound $\\Sigma_{0|t} \\leq (\\sigma^2 _t / \\alpha^2 _t) I$ actually true? The authors should either remove this sentence of explicitely claim that all the design choices made in the paper are heuristic (which is not a critique). Furthermore, the various heuristic choices that are made seem to be very specific to a range of NFEs, since it seems that increasing the NFEs does not necessarily improve the performance. \n- line 74 \"develop a gradient approximation strategy for non-differentiable operators\", here this is not really a contribution, as the gradient approximation is actually quite naive. For example there are already differentiable versions of JPEG which are much less naive that what the paper proposes [2]. \n\nRegarding the numerical experiments, they are exhaustive but I regret the comparison with more recent methods which outperform many of the baselines included, for example [3, 4]\n \nOverall, the paper is interesting but I feel that fails to fully achieve its stated objectives. \n\n[2] Reich, C., Debnath, B., Patel, D. and Chakradhar, S., 2024. Differentiable jpeg: The devil is in the details.  \n[3] Rozet, F., Andry, G., Lanusse, F. and Louppe, G., 2024. Learning diffusion priors from observations by expectation maximization.  \n[4] Janati, Y., Moufad, B., Abou El Qassime, M., Durmus, A.O., Moulines, E. and Olsson, J., 2025, February. A Mixture-Based Framework for Guiding Diffusion Models."}, "questions": {"value": "For linear inverse problems, does using the exact solution of (13) yield better performance than gradient descent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Rpb8J1bbfq", "forum": "RLazI3aMBY", "replyto": "RLazI3aMBY", "signatures": ["ICLR.cc/2026/Conference/Submission6169/Reviewer_Bng9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6169/Reviewer_Bng9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605551474, "cdate": 1761605551474, "tmdate": 1762918513432, "mdate": 1762918513432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Local MAP sampling, which is a plug-and-play inference framework for solving inverse problems with diffusion models. The idea is to iteratively solve local MAP subproblems along the reverse diffusion process.  This framework also consolidates existing optimization-based approaches under a common probabilistic perspective. The proposed algorithm (LMAPS) outperforms existing approaches on a wide range of imaging and scientific inverse problems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper consolidates existing optimization-based diffusion inverse solvers under a common probabilistic perspective, providing new insights into the effectiveness of the methods.\n\nThe empirical validation is quite impressive with extensive experiments across a wide range of scientific and imaging inverse problems, and comparison against existing methods. This clearly indicates the empirical effectiveness of the proposed algorithm (LMAPS).\n\nThe paper is generally well-written and easy to read. The relationship with other existing methods is clearly articulated, which makes the probabilistic perspective and the similarities and distinctions of LMAPS with other optimization-based works easier."}, "weaknesses": {"value": "While the paper provides an insightful perspective on posing inverse problem solving as local MAP sampling, it also has several concerns (as listed) that need to be addressed. I would be willing to increase the score if the following concerns can be addressed sufficiently.  \n\nQ1. While the proposed local MAP sampling framework is clearly empirically effective, I generally don’t find the theoretical formulation of the local MAP objective very novel, as the main insight is heavily based on, and can be trivially observed from the previous work DAPS, as follows.\n\nSince LMAPS changes the sampling procedure $x^{daps}_0 \\sim p(x_0|x_t,y)$ in DAPS to the optimization procedure $x^{lmaps}_0 = \\arg\\max p(x_0|x_t,y)$, it is apparent that $x^{lmaps}_0$ will generally be a higher likelihood sample of $p(x_0|x_t,y)$ compared to $x^{daps}_0$, which in the end, can result in a highly likely solution $x_0$ in the case of LMAPS when compared to DAPS. I’d appreciate the authors’ comments in case I’m missing something regarding this main contribution.\n\n\nQ2. From Q1, and also from Sec 3.2, in general settings, one can intuitively expect LMAPS to not necessarily find a global MAP solution or follow exact posterior sampling but rather tend to give highly likely posterior samples (as Line189 essentially implies). However, I believe a theoretical analysis (maybe in a toy setting as in Appendix A), showing the extent to which the samples of the LMAPS are biased towards highly-likely regions of the posterior, can immensely strengthen the paper’s contribution.   \n\n\nQ3. Eq 14 can arise out of using proximal schemes such as HQS or ADMM, etc, to solve the original MAP problem itself (See [1]). In this regard, the claim about other optimization-based methods not being probabilistically interpretable may not be completely true. Also, I’m quite unsure about the fairness of the claim regarding “the local MAP sampling framework providing a probabilistic interpretation to optimization-based methods”, as it seems heavily based on a Gaussian assumption of $p(x_0|x_t)$ (Lines 264-289). Can the authors clarify?\n\nQ4. The majority of the compared baselines seem to be posterior sampling methods, while LMAPS, for a fair comparison, needs to consider recent MAP solver methods such as [2] and [3] (which was mentioned as being considered in Line 360, but wasn’t actually considered for experiments)\n\nQ5. The linear inverse problem experiments (i.e., box, random inpainting) are typically considered easier tasks compared to tasks like large-hole inpainting or cases where there are only a few measurements. Such tasks can highlight significant differences in metrics among different methods and can suggest the true effectiveness of the method. Also, note that for image restoration, high PSNR doesn’t necessarily correspond with high perceptual quality, unlike LPIPS, which is a more favorable and important metric of choice (See [4]).\n\nQ6. In Table 3. Does “The non-parallel single-image sampling time” mentioned in the caption mean that each method is run with a batch size of 1, as it is supposed to be?  Also, since this task is for deblurring, as mentioned in point 5, it is appropriate to report LPIPS instead of PSNR.\n   \nQ7. FFHQ is also typically considered an easier dataset than Imagenet. Some qualitative examples of Imagenet would provide more insight into the true effectiveness of LMAPS compared to other methods.\n\n[1] Zhu et al. Denoising Diffusion Models for Plug-and-Play Image Restoration\n[2] Gutha et al. Inverse Problems with Diffusion Models: A MAP Estimation Perspective\n[3] Wang et al. DMPlug: A Plug-in Method for Solving Inverse Problems with Diffusion Models\n[4] Blau et al. The Perception-Distortion Tradeoff"}, "questions": {"value": "Please see the questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FZvNSwG6mN", "forum": "RLazI3aMBY", "replyto": "RLazI3aMBY", "signatures": ["ICLR.cc/2026/Conference/Submission6169/Reviewer_nKLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6169/Reviewer_nKLJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911834340, "cdate": 1761911834340, "tmdate": 1762918513024, "mdate": 1762918513024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a framework for addressing inverse problems using diffusion priors.\nThe framework operates by solving a sequence of optimization problems, each of which can be interpreted as a Local-MAP estimation step.\nSince these optimization problems depend on the covariance of $X_0 | X_t$, which is computationally expensive to handle directly, the authors motivate a new interpretable approximation strategy.\nExtensive benchmarks were conducted to validate the framework."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Attempt to unify optimization based-method into a one framework\n- rethink the approximation of the covariance $X_0 | X_t$ involved in the local-MAP problem"}, "weaknesses": {"value": "**Paper exposition**\n\nThe central idea of the paper on inverse problems and that the goal is MAP estimation is a problematic premise.\nIn inverse problems, the degradation operator induces information loss (low-rank component plus noise) so multiple distinct solutions can fit the observations equally well; representing that multiplicity is a feature, not a flaw.\nReducing solving inverse problem to MAP eliminate a fundamental part about capturing the variability of the solutions, uncertainty quantification, critical in several applications. \n\nThe exposition of the background on diffusion models is not well-written and might be confusing.\nSimilarly, it is concerning to see the assertion \"there are two main approaches to do posterior sampling with diffusion prior.\" (Line 102-103) being presented as a categorical truth.\nIn fact, the literature contains a broad set of posterior-sampling methods and several survey efforts attempts to summaries and unify them; see for instance [1] and [2]\nReducing that diversity into two lines misrepresents the state of the art\n\n\n**Technical flaws**\n\n- In Algorithm 2: the second step (Line 5) in DAPS is not correct, the paper treats the second step as a single operation, whereas it amounts to simulation of the reverse diffusion process.\n\n- The authors repeatedly treats the maximizer as unique, although the considered step may have many; e.g. Lines 146 and 190–191.\nThis is incorrect in general: the arg-max of an objective function is a set, and convergence to a particular maximizer depend on initialization (for example, the function $x \\mapsto -x^4+8x^2$ has argmax $ \\\\{ -2, 2 \\\\} $)\n\n- The approximation of the covariance $X_0 | X_t$ has little novelty. The approximation reduces to a spherical covariance model while introducing two additional hyperparameters k_1 and k_2 fo; this both resembles prior covariance approximations in the literature. The introduced hyperparameters $k_1$ and $k_2$ are treated as independent although they were earlier related to the same quantity $2 k_2/ k_1^2 = k / (\\alpha_t^2\\sigma_y^2)$.\n\n- the statement (Line 686) about the denoiser is incorrect, it states that \"this estimator is mode-averaging, and may fall between mixture modes\". Although its expression might give this deceptive impression, the authors disregard the fact that the weights of the components depend on $x_t$\n\n\n**Typos/mistakes**\n\n- Line 230: it is not the correct expression of the $Cov(X_0 | X_t=x_t)$, it must be $Cov(X_0|X_t) = \\sigma_t/\\alpha_t \\nabla m_{0|t}$; see equation (9) in [3]\n- k is introduced in Equation 12, but it is not clear what it refers to\n- Line 216-218: deforms what is being said in the [3] [see Proposition 1 in [3]], this actually the definition of the conditional covariance, and has nothing to do with \"faithfully reflect the covariance of x0 | xt\"\n\n\n---\n\n... [1] Daras, Giannis, et al. \"A survey on diffusion models for inverse problems.\" arXiv preprint arXiv:2410.00083 (2024).\n\n... [2] Oliviero-Durmus, Alain, et al. \"Generative modelling meets Bayesian inference: a new paradigm for inverse problems.\" Philosophical Transactions A 383.2299 (2025): 20240334.\n\n... [3] Boys, Benjamin, et al. \"Tweedie moment projected diffusions for inverse problems.\" arXiv preprint arXiv:2310.06721 (2023)"}, "questions": {"value": "- Can the authors provide clarification on Figure 3, namely the inverse problem setup? it seems the samples are generated from the prior distribution.\n- Can the authors provide clarification on claim in Lines 302–305 about using $K$ gradient steps despite a closed-form solution? If the operator admits an SVD (e.g., inpainting), the advantage of iterative updates is unclear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kukSlRD8lz", "forum": "RLazI3aMBY", "replyto": "RLazI3aMBY", "signatures": ["ICLR.cc/2026/Conference/Submission6169/Reviewer_8LcP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6169/Reviewer_8LcP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949536163, "cdate": 1761949536163, "tmdate": 1762918512644, "mdate": 1762918512644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LMAPS, a new inference algorithm for solving inverse problems with pre-trained diffusion model. LMAPS operates by iteratively solving a local reformulated MAP problem. Empirically, LMAPS achieves strong recovery performance across 10 image restoration tasks and 3 scientific inverse problems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The objective reformulation is simple but effective. \n- High recovery accuracy across a comprehensive list of benchmarks. \n- Hyperparameters for each experiments are provided and their effects on the results are studied in the ablations."}, "weaknesses": {"value": "- The introduction states “posterior sampling is not fully aligned with the objectives of inverse problem solving,” attributing single-GT evaluation to that philosophy. However, the fact that existing inverse problem benchmarks in ML community assume a single ground truth is mostly due to convenience. Setting up a single ground posterior or proper probabilistic tests is much more challenging than a single ground truth. That said, posterior sampling aspect remains important as it can capture the multi-modality, provide credible intervals, and uncertainty calibration, which are important in real-world decision making. Please soften this claim and acknowledge evaluation convenience vs. task desiderata. \n- The claimed contribution of developing gradient approximation strategy for non-differentiable operators is overstated and under-specified. \n\t- Using a differentiable function to approximate the non-differentiable operator is a long-standing general idea. Even for the specific JPEG example used in this paper, there is already a line of differentiable JPEG works [1]. Claiming broad novelty here is problematic. \n\t- Surrogate choice is under-specified and trivialized by the JPEG example. The authors propose the existence of a differentiable surrogate and then instantiates it with the identity map for JPEG. This example is helpful as a baseline but does not address the central problem: how to construct a good surrogate precisely and systematically for a general forward model? \n\t- I would suggest remove the broad claim about general non-differentiable operators and only keep the JPEG example as it does not seem the focus of this work.  \n- In Sec. 4.3, the authors assume that when $H(x_{0})$ and $H^\\prime(x_{0})$ are close, their gradients are also close. But this is not true without specifying the space of functions. Value closeness does not imply gradient closeness. \n- The computation complexity scales with diffusion steps x gradient steps, where each inner step requires at least one forward model evaluation and one gradient call. This means the typical settings use around 200x200 forward operator evaluations. For problems where the forward model call is computationally expensive, LMAPS can be computationally expensive to run. \n- Line 232, $\\leq$ should be $\\geq$? \n\n\n[1]: Reich, Christoph, et al. \"Differentiable jpeg: The devil is in the details.\" _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_. 2024."}, "questions": {"value": "- If the authors insist on maintaining the novelty claim regarding non-differentiable problems, please provide concrete and reproducible steps for constructing a good surrogate. Rather than the informal statement that “there exists a surrogate that is sufficiently close,” the paper should explicitly define:\n\t- the function space from which the surrogate is drawn,\n\t- the closeness metric used to quantify similarity between the surrogate and the true forward operator, \n\t- the computational procedure or algorithm for obtaining such a surrogate in general settings.\n\t- In particular, please address cases where the forward model does not admit a closed-form expression (e.g., physical processes governed by PDEs or other simulators), and explain how your approach can handle these scenarios in practice.\n- How are the hyperparameters selected for each problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZtEZnb3WvR", "forum": "RLazI3aMBY", "replyto": "RLazI3aMBY", "signatures": ["ICLR.cc/2026/Conference/Submission6169/Reviewer_Hjxk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6169/Reviewer_Hjxk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236894947, "cdate": 1762236894947, "tmdate": 1762918512324, "mdate": 1762918512324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}