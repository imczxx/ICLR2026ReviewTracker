{"id": "JdVTWjjnR6", "number": 22813, "cdate": 1758335803825, "mdate": 1759896844975, "content": {"title": "HiTNet: Hippocampal-Thalamic Inspired Dual-Stream Network for Multimodal Sentiment Analysis under Missing Data", "abstract": "Multimodal sentiment analysis faces significant challenges under conditions of missing data, where simultaneous random frame-level missing across all modalities results in fragmented emotional cues and heterogeneous data quality. Existing methods predominantly rely on cross-modal consistency for completion but often neglect residual intra-modal information and lack in assessing cross-modal reliability, leading to redundancy that degrades performance. Human cognitive systems exhibit remarkable robustness to incomplete perceptual input through two functional mechanisms: hippocampal memory systems that reconstruct missing content via pattern completion from stored semantic traces, and thalamic perceptual regulation that dynamically integrates multisensory inputs while filtering unreliable information. Inspired by the brain functions, we propose a Hippocampal-Thalamic dual-stream Network (HiTNet). Hippocampal-inspired intra-modal enhancement stream employs semantic memory modules with dynamic retrieval and sparse activation networks to mine modality-specific information and reconstruct missing features. Thalamic-inspired inter-modal regulation stream implements confidence perception and adaptive cross-modal completion modules to dynamically integrate high-quality cross-modal information while suppressing redundant interference. Comprehensive experiments on MOSI, MOSEI, and SIMS demonstrate that HiTNet achieves superior performance with 1.5%–2.0% average accuracy improvements over state-of-the-art methods across all missing rates and maintains 72.20% accuracy under extreme 90% missing conditions on MOSEI, validating the effectiveness of brain function-inspired design for robust multimodal sentiment analysis even under extreme missing data scenarios. Our code is available at: https://anonymous.4open.science/r/HiTNet-8798/.", "tldr": "", "keywords": ["Multimodal Sentiment Analysis", "Data Missing", "Hippocampal-Thalamic Inspired", "Dual-Stream Network"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1063dbb9e1b83b2e8d1342e210c9ba06d10514f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Hippocampal–Thalamic dual-stream Network, a brain-inspired framework for multimodal sentiment analysis under frame-level missing data. \nThe authors draw inspiration from two functional mechanisms of the human brain: \n(1) the hippocampal memory retrieval process, which reconstructs missing information through semantic association, \nand (2) the thalamic perceptual regulation process, which integrates multisensory inputs while filtering unreliable cues. \nAccordingly, HiTNet is composed of two complementary streams: \na hippocampal-inspired intra-modal enhancement stream that employs semantic memory and sparse activation modules to recover modality-specific semantics, \nand a thalamic-inspired inter-modal regulation stream that utilizes confidence perception and adaptive cross-modal completion to integrate high-quality information across modalities. \nA hierarchical fusion module combines both streams for sentiment prediction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Effectiveness\nExperiments on three standard benchmarks (MOSI, MOSEI, and SIMS) demonstrate consistent improvements of 1.5–2.0% over state-of-the-art methods across various missing rates, \nand the model maintains 72.2% accuracy even under 90% missing data on MOSEI. \nAblation and visualization studies further support the dual-stream design’s effectiveness and robustness.\n\n2. Appealing\nThe proposed idea is intuitively appealing. The analogy between hippocampal memory reconstruction and thalamic regulation provides an intuitive, biologically inspired rationale that enriches the interpretability of multimodal fusion. The proposed HiTNet introduces a biologically motivated architecture that integrates hippocampal-style memory retrieval and thalamic-style perceptual regulation. This dual-stream formulation is conceptually novel in the context of multimodal sentiment analysis and offers an interpretable way to address missing-data challenges.\n\n3. Good writing\nThe manuscript is well organized, with a coherent flow from motivation to methodology and experiments. Figures and tables are informative and contribute to the clarity of presentation."}, "weaknesses": {"value": "1. Quantitatively realized\nHow the “hippocampal” and “thalamic” analogies are quantitatively realized. While the hippocampal–thalamic analogy is conceptually interesting, the connection to actual neuroscientific mechanisms remains largely metaphorical.\n\n2. Computational overhead and scalability\nComputational overhead and scalability of the memory and activation modules. The memory retrieval and sparse activation modules may introduce additional computational overhead. The paper would benefit from a quantitative analysis of training/inference cost and scalability to larger datasets.\n\n3. Minor concerns\nRefer to the questions\n\n4. Biased dataset performance discussion\nWhile HiTNet performs strongly on MOSI/MOSEI, the performance margin on SIMS is relatively modest."}, "questions": {"value": "1. Inconsistent terminology formatting\nThe paper inconsistently uses “intra-modal” and “inter-modal” in some places, while elsewhere they appear as “intramodal” and “intermodal.” Please standardize the terminology throughout the manuscript for consistency and readability.\n\n2. Inconsistent expression of ‘missingness’\nThe manuscript alternates between “modality missingness” and “modality missing.” Since “missingness” is the correct nominal form referring to the state or rate of missing data, it should be used consistently.\n\n3. Noun form correction\nIn a few instances, “missing” is used as a noun, which is grammatically suboptimal. It would be more precise to use “missingness” in these cases.\n\n4. Tense consistency in reproducibility statement\nThe sentence “We have made every effort to ensure that the results presented in this paper are reproducible.” mixes present perfect with simple present."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PK9Y6vU5TI", "forum": "JdVTWjjnR6", "replyto": "JdVTWjjnR6", "signatures": ["ICLR.cc/2026/Conference/Submission22813/Reviewer_Ek1D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22813/Reviewer_Ek1D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536588958, "cdate": 1761536588958, "tmdate": 1762942397178, "mdate": 1762942397178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HiTNet, a brain-inspired dual-stream network for MSA with missing data. The HiTNet includes an intra-modal enhancement stream that reconstructs missing features using semantic memory, and an inter-modal regulation stream that adaptively fuses reliable information across modalities. Experiments on MOSI, MOSEI, and SIMS show that HiTNet achieves SOTA accuracy, demonstrating its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is well-motivated and the overall model design is reasonable.\n2. The experiments and analyses are comprehensive, demonstrating the method’s effectiveness.\n3. The model achieves SOTA performance across multiple datasets.\n4. The paper is well-written and easy to read."}, "weaknesses": {"value": "1. How is the Top-K value in the Semantic Memory Module selected? Could the authors discuss the impact of different k values on performance?\n2. The proposed method involves sparse activation across multiple modules. Could the authors analyze what would happen if full activation were used instead?\n3. Could you add explanations for G and g in the caption of Figure 2, or include a legend directly in the figure for clarity.\n4. The paper claims that Hierarchical Fusion provides better performance. Could you compare it against simpler alternatives such as concatenation, summation, or attention-based fusion?\n5. Since the optimization objective includes many constraint terms, could this lead to unstable or slower training? Could you show training curves and discuss convergence behavior? In addition, how sensitive is the training process to random seed variations Does the loss fluctuate significantly?\n6. In section relevant work, could you introduce and discuss all the methods you compared?"}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OKaIeb3t23", "forum": "JdVTWjjnR6", "replyto": "JdVTWjjnR6", "signatures": ["ICLR.cc/2026/Conference/Submission22813/Reviewer_dS1k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22813/Reviewer_dS1k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934180685, "cdate": 1761934180685, "tmdate": 1762942396956, "mdate": 1762942396956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets multimodal sentiment analysis under frame-level, asynchronous missing across text, audio, and visual streams. It introduces HiTNet, a hippocampal–thalamic inspired dual-stream architecture: an intra-modal (hippocampal) path that performs memory-based completion via a learnable semantic memory and sparse activation/routing to exploit residual evidence within each modality, and an inter-modal (thalamic) path that estimates per-modality confidence and performs confidence-gated cross-modal completion to import only reliable cues. The streams are hierarchically fused, with auxiliary reconstruction and regularizers for routing balance and confidence calibration. On MOSI, MOSEI, and SIMS, HiTNet consistently improves Acc/F1 and MAE/Correlation over strong baselines and shows slower degradation up to 90% missing, indicating robust reliability modeling. Ablations verify the necessity of semantic memory, confidence estimation, and the two-stream design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Propose a dual-stream (hippocampus/thalamus) framework that decouples \"intra-modal self-completion\" and \"inter-modal confidence regulation,\" resulting in clear and reusable functional boundaries.\n2. Focusing on more realistic and common frame-level asynchronous missing data scenarios, rather than just complete modality absence, makes the research more meaningful.\n3. By utilizing semantic memory and sparse routing, the paper first extracts all available evidence from the current modality, reducing over-reliance on other modalities and resulting in greater robustness."}, "weaknesses": {"value": "1. This paper does not compare its method with some powerful modern imputation methods such as masked-autoencoding and diffusion-based completion. Also missing are ablations against simpler reliability heuristics (e.g., SNR/entropy-based gates). These methods may seem more suitable for scenarios where frames are missing.\n2. The input with missing frames does not simulate real-world scenarios, such as consecutive frame drops due to packet loss. Conducting experiments under such conditions would be more convincing.\n3. By coupling the intra- and inter-modal streams only via a late, single-point gate, the model limits cross-layer interactions and can miss fine-grained synergies that require earlier or multi-level fusion.\n4. The convex mix $s⋅x+(1−s)⋅h$ assumes additive compatibility between native and completed features, risking blurred signals and underfitting of nonlinear cross-modal interactions."}, "questions": {"value": "1. Could you provide an explanation for the performance of HiTNet and w/o $L_{ubl}$ in Table 3 in terms of acc-7 and acc-5?\n2. Why couple the intra- and inter-modal streams only via a single late gate—did you evaluate earlier or multi-level fusion (e.g., MoE or cross-layer gating)?\n3. Does using a single prompt token for each modality in cross-modal completion methods result in insufficient expressive power?\n4. How do you handle the $O(T^2)$ complexity of attention mechanisms in long sequence scenarios (>1-5 minutes)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M1VrFxDQwz", "forum": "JdVTWjjnR6", "replyto": "JdVTWjjnR6", "signatures": ["ICLR.cc/2026/Conference/Submission22813/Reviewer_gTuV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22813/Reviewer_gTuV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216341453, "cdate": 1762216341453, "tmdate": 1762942396610, "mdate": 1762942396610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses multimodal sentiment analysis under simultaneous random frame-level missing cues across modalities. It introduces HiTNet, a hippocampal–thalamic architecture with an intra-modal semantic memory that retrieves and updates residual signals via sparse activation for reconstruction, and an inter-modal regulation path that estimates modality confidence and performs confidence-aware cross-modal completion with learnable prompts, followed by hierarchical fusion. Experiments on standard benchmarks indicate consistent gains across missing rates and strong robustness even under extreme sparsity, with ablations showing that each component contributes materially. Overall, the problem is timely, the design is biologically inspired yet technically grounded, and the evidence suggests practical value."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a meaningful and insufficiently studied problem in multimodal sentiment analysis under random frame-level missing data, showing clear motivation and novelty.\n2. The proposed HiTNet framework is well designed and coherent, combining hippocampal-inspired intra-modal memory with thalamic-inspired confidence-aware cross-modal completion to enhance robustness and information utilization.\n3. The experimental evaluation is thorough and convincing, demonstrating consistent improvements across datasets and strong stability under severe missing conditions, with ablation results supporting each component’s effectiveness."}, "weaknesses": {"value": "1. The biological inspiration, while interesting, remains mostly metaphorical; the paper could better justify how the hippocampal–thalamic analogy concretely guides the architecture design and contributes beyond naming.\n2. The experimental section, though broad, lacks sufficient comparison with very recent multimodal robustness approaches or large foundation models, which limits understanding of its relative performance in the current landscape.\n3. Some implementation details and hyperparameter settings are not clearly described, making it difficult to reproduce results or fully assess the method’s computational efficiency and scalability."}, "questions": {"value": "1. How sensitive is the model’s performance to the design of the semantic memory module and the sparse activation mechanism? Would alternative memory retrieval or selection strategies yield similar robustness?\n2. How well does HiTNet generalize to other multimodal tasks beyond sentiment analysis, such as emotion recognition or multimodal dialogue, especially when the missing patterns differ from those in the benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z7GjlaOaic", "forum": "JdVTWjjnR6", "replyto": "JdVTWjjnR6", "signatures": ["ICLR.cc/2026/Conference/Submission22813/Reviewer_navA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22813/Reviewer_navA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762238137880, "cdate": 1762238137880, "tmdate": 1762942396199, "mdate": 1762942396199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Inspired by findings in neuroscience, this paper proposes a multimodal sentiment analysis model named HiTNet, designed for scenarios with missing modalities. The model architecture is motivated by two functional mechanisms in the brain: the hippocampus, which is responsible for semantic memory retrieval and pattern completion, and the thalamus, which regulates perceptual integration and confidence control. Specifically: 1. The hippocampus performs semantic memory retrieval and pattern completion; 2. The thalamus dynamically integrates multimodal information and regulates reliability among modalities. The proposed HiTNet consists of two parallel subnetworks: an intra-modal enhancement stream and an inter-modal regulation stream. Experimental results demonstrate that HiTNet achieves superior performance on multiple datasets (MOSI, MOSEI, and SIMS), maintaining high accuracy even under conditions of severe modality missing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces the hippocampal–thalamic mechanism from neuroscience into multimodal sentiment analysis under modality-missing conditions. The idea is novel and demonstrates strong originality.\n\n2. Experiments on three benchmark datasets validate the effectiveness of the proposed method. Comprehensive ablation studies are conducted for each module to verify their contributions, along with analyses on missing rates and loss weight settings, showing the experimental evaluation is thorough.\n\n3. Compared with baseline models, the proposed method consistently outperforms existing approaches across all three datasets and under various missing ratios.\n\n4. The paper is clearly written and well-structured, making it easy to follow."}, "weaknesses": {"value": "1. The paper states that “the missing information reconstruction module ERec, designed to reconstruct the missing features of each modality,” but in Figure 2, the position of this module is ambiguous, making it difficult for readers to interpret and align it with the overall framework.\n\n2. The paper does not explore the model’s performance under non-random missing scenarios.\n\n3. There are some writing detail issues: the formulas lack proper punctuation — for example, a comma should follow Formula (2) and a period should follow Formula (3). Formula (7) should be followed by an explanatory sentence, and the use of uppercase and lowercase letters is inconsistent."}, "questions": {"value": "1. In Tables 5 -7, only a few combinations of loss weights (α, β, γ) are tested (e.g., 15, 0.5, 0.1 and 10, 0.9, 0.1). Could you elaborate on how these specific values were chosen? Were they selected empirically, or do they correspond to particular design motivations (e.g., emphasizing one auxiliary loss over another)?\n\n2. In the hierarchical fusion stage, the language modality is always placed last. Is the model sensitive to the order of modality fusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Pclg6aL7qG", "forum": "JdVTWjjnR6", "replyto": "JdVTWjjnR6", "signatures": ["ICLR.cc/2026/Conference/Submission22813/Reviewer_NAMa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22813/Reviewer_NAMa"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762275280019, "cdate": 1762275280019, "tmdate": 1762942395899, "mdate": 1762942395899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HiTNet, a dual-stream network inspired by hippocampal memory retrieval and thalamic perceptual regulation for multimodal sentiment analysis under severe frame-level missingness. The “hippocampal” stream uses a key-value semantic memory module plus sparse activation sub-networks to reconstruct intra-modal features; the “thalamic” stream estimates per-modality confidence and performs adaptive cross-modal completion. A hierarchical Cross-Transformer fuses both streams and an auxiliary reconstruction loss is added."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: First work that explicitly models hippocampal pattern-completion and thalamic gating for frame-level missing data; combines key-value memory, sparse routing, and confidence-weighted cross-modal attention in a unified framework.\n\n- Quality: Each component is formally described, ablated, and visualised.\n\n- Clarity: Well-written; neuroscientific motivation is intuitive; notation is consistent.\n\n- Significance: Addresses a practical and under-studied scenario (random frame-level corruption across all modalities) and demonstrates remarkable robustness at extreme missing rates that prior reconstruction or co-learning methods cannot reach."}, "weaknesses": {"value": "- Novelty gap with existing memory-based completion: Key-value memory banks have been used for missing-modality imputation and for speech emotion with artefacts. The authors should clarify how their SMM differs from these works beyond simply being applied to sentiment.\n\n- Biological inspiration is loose: The hippocampus performs associative pattern completion across time, whereas the SMM retrieves a single best-matching vector with cosine similarity and updates by frequency, which is closer to a standard dictionary. A more rigorous mapping or citation to computational neuroscience models (e.g., Hopfield networks, Kanerva’s sparse distributed memory) would strengthen the claim.\n\n- Inference requires five sub-networks per modality, a memory bank of 64×3 tensors, two Transformers for confidence & completion, and a 4-layer fusion transformer. The paper reports accuracy but not latency, FLOPs, or GPU memory; a table or plot (e.g., vs. LNLN) is needed for deployment claims.\n\n- Hyper-parameter sensitivity: Optimal loss weights differ by dataset (MOSI α=10, MOSEI α=1.5). No automatic tuning or principled weighting (e.g., uncertainty weighting, GradNorm) is explored, raising reproducibility concerns."}, "questions": {"value": "1. Can you show one concrete example (frames + spectrogram + text) where intra-modal completion visually/audibly recovers lost content?\n\n2. What happens under non-random missing patterns—e.g., consecutive video frames dropped due to buffering, or audio muted for the last 2 s? Does confidence prediction still help?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w4PaOP2J49", "forum": "JdVTWjjnR6", "replyto": "JdVTWjjnR6", "signatures": ["ICLR.cc/2026/Conference/Submission22813/Reviewer_vvAP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22813/Reviewer_vvAP"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission22813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762759979728, "cdate": 1762759979728, "tmdate": 1762942395641, "mdate": 1762942395641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}