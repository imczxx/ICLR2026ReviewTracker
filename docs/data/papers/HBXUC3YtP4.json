{"id": "HBXUC3YtP4", "number": 8673, "cdate": 1758094437116, "mdate": 1759897770294, "content": {"title": "Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems", "abstract": "Large Reasoning Models (LRMs) have advanced rapidly, yet existing benchmarks on mathematics, code, and common-sense reasoning remain limited: they lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well-suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply with longer contexts—falling below 50% once graphs exceed 120 nodes—driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an \"over-thinking\" phenomenon, primarily driven by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://anonymous.4open.science/r/GrAlgoBench-7D17.", "tldr": "We introduce GrAlgoBench, a graph algorithm benchmark across three reasoning dimensions, which exposes LRMs’ key limitations: weak long-context reasoning and ineffective over-thinking.", "keywords": ["Large Reasoning Models", "Graph Algorithm Problems", "Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9afdeb7ba08339074d8393b6116119d06833ef79.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a benchmark for Large Reasoning Models (LRMs) that consists of questions instantiated from various graph algorithm problems. The benchmark reveals two shortcomings of state-of-the-art LRMs: poor scalability to long contexts and overthinking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Despite the abundance of graph-based reasoning benchmarks in the literature, the idea of challenging LRMs with graph algorithm problems of varying complexity seems novel to me."}, "weaknesses": {"value": "- The evaluation of the paper measures the difficulty of questions primarily by the size of their underlying graphs rather than the computational complexity of their underlying graph algorithm problems. Since the novelty of the paper lies primarily in its graph algorithm perspective, its evaluation should focus on the impact of computational complexity rather than graph size.\n- Some of the graph algorithm problems, e.g. minimum spanning tree, may not resemble questions in real life."}, "questions": {"value": "- Can the numbers in Table 3 provide additional insights if they are grouped by computational complexity rather than graph size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yvOSCjxDn2", "forum": "HBXUC3YtP4", "replyto": "HBXUC3YtP4", "signatures": ["ICLR.cc/2026/Conference/Submission8673/Reviewer_c193"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8673/Reviewer_c193"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761418923625, "cdate": 1761418923625, "tmdate": 1762920488730, "mdate": 1762920488730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new benchmark named GRALGOBENCH to evaluate the long-context reasoning ability of Large Reasoning Models (LRMs) through graph algorithm problems. Specifically, this benchmark includes three major categories of problems: Enumeration, Exploration, and Intuition. Each category is further classified into three difficulty levels: easy, medium, and hard (3*3=9 tasks), and each task is set with 6 levels based on the number of nodes and edges. Based on this benchmark, the authors conducted systematic experiments to explore the long-text reasoning ability of the current LRMs. In summary, the accuracy of LRMs in long contexts has dropped sharply. Secondly, LRMs are plagued by the phenomenon of overthinking, which is mainly caused by frequent and ineffective self-verification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tWell-motivated benchmark & taxonomy. The bridge from CLRS-style algorithmic families to the Enumeration / Exploration / Intuition taxonomy is clear and useful for reasoning analysis.\n\n2.\tReal-world graph sources & scaling. Instances are derived from DBLP, street networks (multiple cities), OpenFlights, Wikipedia, and DBpedia; each task is generated across six scales to modulate difficulty, supporting long-context evaluation without synthetic toy artifacts.\n\n3.\tComprehensive evaluation & clear trends. Results across both reasoning and non-reasoning models show consistent degradation with context length and with graph size, and reveal a robust hierarchy of task difficulty.\n\n4.\tThoughtful error analysis. The paper proposes a structured error taxonomy (algorithm selection vs. execution vs. output quality vs. information/memorization) and demonstrates where LRMs fail.\n\n5.\tOver-thinking analysis. The segmentation of traces with high-entropy tokens and labeling of self-reflection/strategy-shift provides evidence that “self-verification” often inflates tokens with little gain; the “outcome efficiency” plots are compelling."}, "weaknesses": {"value": "1.\tLLM-as-judge dependence. Error categorization and over-thinking judgments rely on LLM pipelines. The paper would benefit from reporting inter-annotator agreement with humans on a stratified subset to quantify labeling reliability and possible bias.\n\n2.\tOn line 72 you note that GraphWalks is intended to benchmark LRMs’ long-context capabilities. Please clarify how your benchmark compares to GraphWalks in scope, task design, evaluation protocol, and key findings. In particular, explain what is novel or complementary (e.g., task families, graph sources, scaling, automatic judging, contamination checks, tool-use assumptions) and, where feasible, include a side-by-side quantitative comparison to justify the need for a new benchmark.\n\n3.\tTool-use / code execution. Many LRMs now include tool-use or code-execution modes. The benchmark fixes models to text-only. That is fair for isolating intrinsic reasoning, but the paper should discuss how results might differ under code-execution assistance and whether GRALGOBENCH can evaluate that regime.\n\n4.\tExternal validity and scope. The benchmark focuses exclusively on deterministic graph-algorithm tasks. While this is a valuable slice of reasoning, it omits other long-context regimes. Without evidence of cross-task transfer, it’s unclear whether conclusions about long-context brittleness and over-thinking generalize beyond graph-structured problems, other data modalities (tables, trees, sequences), or multilingual settings."}, "questions": {"value": "1.\tHow consistent are the error labels across judges? It might be good to  report a human-checked agreement rate on subset samples, stratified by taxonomy/level.\n\n2.\tIn the verbosity experiments, how are node names expanded and how is the added text distributed across the prompt (e.g., per-node descriptors vs. preamble)? Can you share a few full prompts at each length?\n\n3.\tHave you tried a token-budget cap (e.g., limiting self-verification tokens) to quantify over-thinking’s cost-benefit? The efficiency plots suggest diminishing returns; an intervention study would be valuable.\n\n4.\tIn Figure 3, several curves appear to run at or below zero. Maybe tightening the y-axis limits and/or adding clipping can avoid visual ambiguity.\n\n5.\tOn transfer/generalization beyond graphs. Can you provide transfer evidence that performance and observed phenomena on this benchmark relate to broader “general reasoning” tasks? For example:\n\n(i) model-wise correlations between GRALGOBENCH scores and external benchmarks targeting long-context/general reasoning (e.g., LongBench, Needle-in-a-Haystack variants, BIG-bench Hard, GPQA, MMLU-Pro, long-form GSM8K),\n\n(ii) an intervention study showing that a method which improves GRALGOBENCH (prompting, memory limits, self-verification caps) also improves one or more non-graph long-context tasks, and/or\n\n(iii) an ablation that rephrases graph instances into equivalent textual narratives or table/tree forms to test whether the observed failures stem from graph structure per se versus generic context-length effects. If these are out of scope, please temper the generality of your conclusions accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XCyoePeYId", "forum": "HBXUC3YtP4", "replyto": "HBXUC3YtP4", "signatures": ["ICLR.cc/2026/Conference/Submission8673/Reviewer_dfQZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8673/Reviewer_dfQZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825262764, "cdate": 1761825262764, "tmdate": 1762920488292, "mdate": 1762920488292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a new benchmark for evaluating LLM reasoning capabilities under long-context scenarios and conducts careful error analysis to study the underlying mechanism of reasoning failure in this setting. The paper's methodology involves building prompts with controllable length that each contain a problem as an instance of one of 9 algorithmic graph tasks (maximum degree node, maximum clique problem, path sum, etc.) plus a graph instance from real-world graph datasets, alongside an interesting taxonomy to categorize the required reasoning into enumeration, exploration, and intuition labels. The length is controlled either by graph size (number of nodes) or by using fixed graphs with varying node names as an alternative way of controlling the input prompt length.\n\nBy careful evaluation of their introduced dataset, alongside error analysis on generated responses, this work reveals some key findings. Both increasing the number of nodes and node name token length resulted in degraded performance consistently across a range of frontier open- and closed-source reasoning models. The error analysis contains a taxonomy of common issues found in incorrect model responses, and it identifies algorithmic execution error, graph memorization error, and excessive redundancies as the key failure modes. Moreover, their error analysis shows that intuition-driven reasoning (the greedy subset of the prompts) degrades the most as input context length grows.\n\nTo fully understand the excessive redundancies issue, the paper designs an experiment to segment the output response based on “aha moment” signals (slicing the response after finding high-entropy tokens such as “wait”, “but”, “so”). Then, it classifies these segments, concluding that most of these segments are unsuccessful attempts at self-verification that fail to detect the error in prior segments. Hence, the paper argues that unsuccessful self-verification is the main driver of the \"overthinking\" (the observed redundancy in reasoning traces)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and clear, and the experiments are designed carefully (e.g., a diverse selection of open- and closed-source frontier models, a strong qualitative study, and error analysis). Moreover, the code is available, which aids in the reproducibility of the work.\n\n* **Task design:** The reasoning taxonomy is well-designed and well-motivated, categorizing reasoning types into enumeration, exploration, and intuition, which appears to be a strong differentiator compared to prior work like [1]. More importantly, the usage of an LLM-as-judge to classify the reasoning traces of models for an initial 100 graph samples is a clever design decision to assign each task to one of the reasoning labels.\n* **The evaluation error analysis reveals some impactful findings:**\n    1.  Quantifying the reasoning failure modes under long-context settings, and attributing them mainly to Algorithm Execution Errors (AEE), Graph Memorization Errors (GME), and excessive redundancy is a strong contribution.\n    2.  Identifying high-entropy tokens and using them to segment reasoning traces for detecting reasoning shifts is an interesting methodology. The key findings of this experiment are that self-verification fails most of the time to progress smoothly past errors and is highly redundant. This finding is impactful as it reveals one possible underlying failure mechanism of reasoning models, especially under highly long-context inputs.\n* While the data collection and controlled-length problem synthesis share similarities with GraphArena[1] (e.g., sharing tasks and the methodology to sample real-world graphs), and while a key finding (that increasing input length leads to performance degradation) has also been studied in [1, 2], this paper provides a key differentiation. GraphArena[1] has a very limited focus on dedicated reasoning models and only considers input graph node counts up to 50. This paper correctly indicates this is a setting where good reasoning models have already reached near-saturated performance; hence, its usage of **up to 160 nodes** is well-designed to study the limits of frontier reasoning models.\n* Using such graph problems as evaluation benchmarks is an impactful direction and is aligned with frontier labs' evaluations on long-context reasoning (e.g., OpenAI GraphWalks)."}, "weaknesses": {"value": "* One minor weakness is that the **fixed assignments of Enumeration, Exploration, and Intuition reasoning labels** to tasks might be slightly misleading, as models might sometimes use different reasoning to solve a task (e.g., doing Exploration instead of Intuition). While Appendix H.1 does a great job showing this issue would be rare, it also shows such an issue exists. Furthermore, these ablations are done by using an LLM-as-judge on 100 problem instances that seem to differ from the wide range of task complexity used in the final proposed dataset. Hence, the algorithm ratio as defined in Appendix H.1 might show a more uniform distribution in the actual dataset, especially for weaker and smaller open-source models.\n* While evaluating the long-context input scenario for reasoning problems is an important benchmark, the proposed evaluation seems to **slightly diverge from the realistic use case of frontier models**. It seems that all tasks in this evaluation could be handled much better with tool calls (writing code and receiving code execution results). Having extremely sophisticated short-context inputs that require a large amount of reasoning token generation (like olympiad competitions) is an alternative way of measuring long-context reasoning while keeping the input tokens fixed. Thus, I would suggest a discussion on this matter and perhaps a refinement of the introductory claim (lines 47-49) that \"First, they lack long-context evaluation: existing benchmarks predominantly use short problem texts, which cannot be easily scaled to assess LRMs’ reasoning capabilities over extended contexts.\"\n* As mentioned in this paper, as input context length grows, the performance of models degrades noticeably in correctly solving the graph problems. However, in the evaluation workflow, there are multiple LLM-as-judge calls using Qwen-2.5-72B (such as prompt I.4 for error analysis and prompt I.5 for reformatting) that seem to need to operate and reason over a potentially much larger input context compared to the original graph-solving task. Hence, **the quality of the output responses from Qwen-2.5-72B when using prompts I.4 and I.5 on an already large, long-context input problem is questionable.**\n* More discussion and illustration are needed on **how high-entropy tokens are being used for segmentation and what their failure modes are**. How likely is it that the main finding—that “ineffective self-verification is a primary driver of over-thinking in LRMs”—is the result of incorrect segmentation?"}, "questions": {"value": "1.  How much can we trust the validity of Qwen-2.5-72B's responses for prompts I.4 and I.5, especially since they are also given a very long context input (possibly even larger than the input for the graph-solving problems)?\n2. Could you provide an argument for the real-world use cases of this benchmark, especially in light of the fact that these graph problems could be solved with tool calls?\n3.  Could you elaborate more on the segmentation using high-entropy tokens? How is the threshold for detecting high-entropy tokens tuned?\n\n\n\n\n**References:**\n\n[1] Tang, Jianheng, et al. \"Grapharena: Evaluating and exploring large language models on graph computation.\" arXiv preprint arXiv:2407.00379 (2024).\n\n[2] Xu, Hao, et al. \"GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large Language Models on Graph-theoretic Tasks.\" arXiv preprint arXiv:2504.12764 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m4cCQqstLc", "forum": "HBXUC3YtP4", "replyto": "HBXUC3YtP4", "signatures": ["ICLR.cc/2026/Conference/Submission8673/Reviewer_8MVa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8673/Reviewer_8MVa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030139586, "cdate": 1762030139586, "tmdate": 1762920487940, "mdate": 1762920487940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **GRALGOBENCH**, a benchmark of nine graph-algorithm tasks spanning three “reasoning” categories Enumeration, Exploration, and Intuition and scales them across six graph sizes to stress LRMs’ long-context abilities. The headline claims are: (i) accuracy collapses as graphs grow (below ~50% beyond 120 nodes), and (ii) LRMs “over-think,” producing long, redundant traces; the authors analyze this by partitioning traces via high-entropy tokens like *wait*, *but*, *so*."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The nine tasks are precisely specified (with optimal algorithms/complexities in the appendix), and the dataset scales graph sizes systematically (Level-1…Level-6), which is valuable for probing context-length effects in a controlled way."}, "weaknesses": {"value": "1. The core taxonomy (Enumeration / Exploration / Intuition) is *empirically* assigned by first generating 100 ER instances per problem, collecting LRM responses, and then asking **Qwen-2.5-72B** to classify which algorithmic family the *responses* reflect; the team then picks 9 tasks whose “algorithms are relatively unambiguous.” This risks circularity (taxonomy depends on current LRM behaviors) and injects judge-model bias into the ground truth of the benchmark’s conceptual framing. It also undermines generality, since task selection is contingent on that filter. \n\n2. In the MST case study, nodes are *streets* and edges are *intersections* then the text states “the weight of each edge is the distance between two streets,” which is a confusing physical interpretation (intersections don’t have lengths, and “distance between streets” is ill-defined if nodes are streets). This muddles the mapping from the real network to its graph abstraction and invites ambiguity about what the algorithm is actually optimizing. \n\n3. The paper enforces a `\\boxed{}` final answer format, but some tasks (e.g., Distance Threshold) ask for a *node identifier* rather than a numeric value; exact-string matching on natural-language street names is brittle without strict canonicalization rules. The paper specifies decoding hyperparameters but does not detail robustness checks against case/spacing variants, which weakens the claim that graph tasks “enable standardized programmatic evaluation.” (Suggestion: emit canonical IDs alongside text and evaluate on IDs.)"}, "questions": {"value": "* Line 241: **“Minimum Spinning Tree” → “Minimum *Spanning* Tree”** \n* **Acronym not introduced**: Figures use **MCP**; earlier text spells out *Maximum Clique Problem* without defining “(MCP)”. Define on first use.  \n* **Awkward phrasing**: “We enforce LRMs to output their final answers in the \\boxed{} format.” → “We enforce that LRMs output…” (stylistic)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PQWA0wxJFz", "forum": "HBXUC3YtP4", "replyto": "HBXUC3YtP4", "signatures": ["ICLR.cc/2026/Conference/Submission8673/Reviewer_vGuw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8673/Reviewer_vGuw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146270400, "cdate": 1762146270400, "tmdate": 1762920487511, "mdate": 1762920487511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}