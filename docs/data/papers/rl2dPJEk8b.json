{"id": "rl2dPJEk8b", "number": 15842, "cdate": 1758255984579, "mdate": 1763626992517, "content": {"title": "Bi-Lipschitz Autoencoder With Injectivity Guarantee", "abstract": "Autoencoders are widely used for dimensionality reduction, based on the assumption that high-dimensional data lies on low-dimensional manifolds. Regularized autoencoders aim to preserve manifold geometry during dimensionality reduction, but existing approaches often suffer from non-injective mappings and overly rigid constraints that limit their effectiveness and robustness. In this work, we identify encoder non-injectivity as a core bottleneck that leads to poor convergence and distorted latent representations. To ensure robustness across data distributions, we formalize the concept of admissible regularization and provide sufficient conditions for its satisfaction. In this work, we propose the Bi-Lipschitz Autoencoder (BLAE), which introduces two key innovations: (1) an injective regularization scheme based on a separation criterion to eliminate pathological local minima, and (2) a bi-Lipschitz relaxation that preserves geometry and exhibits robustness to data distribution drift.  Empirical results on diverse datasets show that BLAE consistently outperforms existing methods in preserving manifold structure while remaining resilient to sampling sparsity and distribution shifts.", "tldr": "", "keywords": ["Autoencoder", "Injectivity"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb01eb20b87e052167c9dd2bab264fc26c14575a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel autoencoder regularization method with an injectivity guarantee. The authors identify the non-injectivity of encoders as a bottleneck that causes existing gradient-based regularization methods to suffer from pathological local minima. To address this issue, the paper introduces a separation criterion and an injective regularizer. Furthermore, the notion of admissibility of a regularizer is proposed as a condition for distributional robustness, which motivates the use of a Bi-Lipschitz regularization. Empirical results demonstrate that the Bi-Lipschitz autoencoder captures the manifold structure of data more effectively than several state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents an original and interesting regularization framework for autoencoders, designed to preserve the topological structure of the data manifold.\n- The issues with existing regularization methods are clearly analyzed, providing a natural motivation for introducing the injective regularization.\n- The proposed framework is evaluated through comprehensive empirical comparisons.\n-  The proposed regularizer is relatively simple to implement, which can be advantageous for practical applications.\n-  The paper is generally well-written."}, "weaknesses": {"value": "- The **practical relevance of the admissibility condition** remains unclear. In realistic settings, $\\mathbb{P}$ and $\\mathbb{Q}$ in Eq. (8) would correspond to empirical measures supported only on training samples. Under such circumstances, it is doubtful whether their equivalence with $\\mu_\\mathcal{M}$ (the data manifold measure) can hold. It is also unclear how admissibility is concretely related to distributional robustness. In particular, the reconstruction error (claimed to be admissible) is not robust outside the training region, as illustrated in the toy example in Figure 1.\n- The **motivation for the Bi-Lipschitz regularizer** is not sufficiently persuasive. Based on the current exposition (e.g., L274–L276), it appears to be one among many possible regularizers that satisfy the admissibility condition. The specific reason for selecting the Bi-Lipschitz form should be better justified, perhaps by emphasizing its relation to local topology preservation.\n- The method introduces **many hyperparameters**, but the paper lacks ablation studies. In particular, varying $\\epsilon$ and $\\kappa$ could have significant effects, and removing or changing relative weights of the injectivity or Bi-Lipschitz components would likely alter results considerably. Without such analyses, it is difficult to grasp the sensitivity and interpretability of the proposed regularization.\n\n**Minor comments**\n-  Figure 1(h) contains an error.\n-  The notation $m$ is used both for manifold and ambient space dimensions, which reduces readability.\n-  The correct reference for LTSA should be:\n\n    Zhang, Zhenyue, and Hongyuan Zha (2004). Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent Space Alignment. SIAM Journal on Scientific Computing, 26(1): 313–338."}, "questions": {"value": "-  Can the authors provide an example where the proposed injectivity regularizer is combined with existing gradient-based regularizers to alleviate their pathological local minima? Such experiments would considerably strengthen the paper’s contribution.\n-  What are the detailed outcomes of the minima shown in Figure 2? Visualizing the corresponding latent representations, as in Figure 3, would help in interpretation.\n-  How does the method behave as the latent dimensionality increases? Since the regularizer primarily enforces local and global topology preservation, it might lead to overexpansion of latent space volume. It would be informative to compare with methods that constrain latent volume, such as:\n\n    Chen, Qiuyi, and Mark Fuge. Compressing Latent Space via Least Volume. ICLR 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5QDwL9czVH", "forum": "rl2dPJEk8b", "replyto": "rl2dPJEk8b", "signatures": ["ICLR.cc/2026/Conference/Submission15842/Reviewer_aWuy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15842/Reviewer_aWuy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809233601, "cdate": 1761809233601, "tmdate": 1762926067235, "mdate": 1762926067235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The given paper addresses the issue that, when extracting latent dimensions through an auto-encoder, the mapping between certain samples and their latent representations is not one-to-one, leading to degraded overall reconstruction performance. The main contributions of the paper are as follows:\n1.\tA regularization loss is introduced to ensure that samples satisfying a specific separation criterion remain distinguishable from each other even after being embedded into the latent space.\n2.\tA Bi-Lipschitz-based loss term is additionally incorporated to preserve geometric information and achieve robust embedding performance under variations in the data distribution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tOriginality:\nThe paper is original in that it applies mathematical concepts - specifically the Bi-Lipschitz condition and the separation criterion - to the existing deep learning model of an auto-encoder. This approach preserves the geometric information of the original input, demonstrating a creative integration of mathematical theory into deep learning. Moreover, the work contributes to the development of interpretable deep learning models, which further enhances its originality.\n2.\tQuality:\nThe definitions and theoretical explanations are presented with precision and clarity, supported by detailed discussions in the Appendix, which reinforces the overall quality of the work.\n\n3.\tClarity:\nThis aspect is sufficiently addressed through the paper’s quality, as the theoretical framework and methodology are clearly explained.\n\n4.\tSignificance:\nTraditional auto-encoders have shown limitations in providing interpretable representations of the latent space. This paper overcomes such limitations by introducing a regularization loss term and a Bi-Lipschitz relaxation loss term, enabling analytical interpretation of the latent space. In doing so, it offers a new research direction beyond previous works that primarily focused on reconstruction performance or clustering in the latent space."}, "weaknesses": {"value": "1.\tThe proposal of an interpretable latent embedding method is commendable. However, demonstrating that the model merely achieves “better latent embedding” in the experiments may not be sufficient to establish the paper’s novelty. It would strengthen the contribution if the authors could visually illustrate - through figures as well as tables - that improved latent embedding leads to better reconstruction of the original data, thereby providing more intuitive evidence of the method’s effectiveness.\n\n2.\tThe roles of the regularization loss and the Bi-Lipschitz loss term should be highlighted more clearly through experiments. To substantiate the claim that “the injective term eliminates local minima caused by non-injective encoders, while the bi-Lipschitz term ensures consistent geometric mapping regardless of data distribution,” it would be valuable to conduct ablation studies. Specifically, experiments should be performed with (i) the injective term coefficient set to zero, (ii) the Bi-Lipschitz term coefficient set to zero, and (iii) both terms included with appropriate regularization weights, in order to empirically demonstrate the distinct contribution of each term.\n\n3.\tI suspect that the Bi-Lipschitz Autoencoder might require longer training time compared to geometry-based or gradient-based autoencoders. Since the loss function involves the Jacobian and the separation criterion requires pairwise computations, the computational cost could increase significantly as the dataset grows. I am curious about the authors’ perspective on this issue and whether they have considered strategies to mitigate the potential increase in training time."}, "questions": {"value": "1.\t I am wondering why the paper does not include experiments with a Variational Autoencoder (VAE). Was this decision made because the VAE inherently relies on probabilistic distributions to some extent?\n2.\tIn Table 1, the paper reports state-of-the-art (SOTA) performance. Could you clarify under what specific conditions these experiments were conducted?\n(For example: experiments were performed with random seeds ranging from 0 to 5, and the reported results represent the mean and standard deviation across these runs.)\n3.\tIn Figure 2, I would like to know what the x-axis and y-axis specifically represent.\n4.\tDo you have any plans to release the code on GitHub?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nFR1cb0gik", "forum": "rl2dPJEk8b", "replyto": "rl2dPJEk8b", "signatures": ["ICLR.cc/2026/Conference/Submission15842/Reviewer_1zwJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15842/Reviewer_1zwJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900399288, "cdate": 1761900399288, "tmdate": 1762926066597, "mdate": 1762926066597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors emphasize the importance of constructing an injective mapping and propose two regularization techniques which, when combined, are claimed to improve the embedding. Both regularizations are based on distance constraints: one enforces that two separate points in the original space should be guaranteed to be separated in the embedding space, and the other imposes a margin (Hinge-type loss) on the pairwise distances that the embedding data must satisfy. In the experiments, the embeddings produced by the proposed method more closely resemble the ground truth or exhibit a perfectly circular shape."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors’ discussion of the importance of injective embeddings is largely accurate and significant. Overall, the paper is well-written, and the experimental results on simple benchmark examples convincingly highlight the distinction between the proposed approach and other methods used for comparison."}, "weaknesses": {"value": "The importance of injective mapping is emphasized in Section 2, and I enjoyed reading the discussion. However, the algorithm proposed in Section 3 is not innovative in achieving injective mappings; it appears to be a minor engineering modification of previously established methods, such as Isomap, Unfortunately, the proposed method does not directly address the construction of injective mappings compared with other well-known approaches. For example, Variational Autoencoders embed distributions that represent points to ensure separation in the latent space. Normalizing flows successfully learn bijective neural networks. It is difficult to identify any substantial or nontrivial derivations that would motivate the use of the proposed method.\n\nIn terms of discussion, while identifying injective mappings is indeed important, it should be noted that, since the neural network does not have access to the ground truth manifold in advance, we cannot conclude that the embeddings in Figure 1(e) or1 (g) are entirely incorrect when data are insufficient. Most of the embedding methods include hyperparameters that control manifold complexity, and with appropriate choice of hyperparameter, other methods could likely produce embeddings that are between the results in (c) and (e).\n\nThe authors proposed distance preservation in the latent space. However, under dimensionality reduction, distance preservation cannot generally be expected, even for successful and practically useful embeddings. Useful embeddings typically impose nontrivial mappings, and the authors does not acknowledge this point in their discussion and experiments. Overall, the paper falls below the level of acceptance."}, "questions": {"value": "Can you provide even a simple theoretical guarantee that the proposed method can achieve?\n\nDoes the second regularization L_{bi-Lip} already encompass the first regularization L_{reg}? If so, please clarify why L_{reg} is needed in addition, and include the explanation on their distinct role.\n\nThe results shown in the experiments appear to be nearly trivial embeddings (essentially, copies of the original data). Is this interpretation correct? What would be the pragmatic utility of such embeddings? In general, trivial embeddings are not necessary in real-world applications. \n\nTypically, embeddings are designed to reduce dimensionality. Therefore, constructing injective mappings usually requires bijective mappings or diffeomorphism. Could you include a discussions on this relationship?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "liVINYqI7Q", "forum": "rl2dPJEk8b", "replyto": "rl2dPJEk8b", "signatures": ["ICLR.cc/2026/Conference/Submission15842/Reviewer_4NEa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15842/Reviewer_4NEa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953103445, "cdate": 1761953103445, "tmdate": 1762926066203, "mdate": 1762926066203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Bi-Lipschitz Autoencoders (BLAE), which is an autoencoder with additional regularization terms to make it injective and robust to distribution shifts. The authors argue that non-injectivity is a bottleneck for autoencoders and leads to them being trapped in suboptimal local minima. By enforcing injectivity through regularizations, the learned manifold more closely matches the ground truth manifold of the data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The discussion on why AEs and gradient based methods tend to get trapped in local minima, and how even though gradient-based regularization methods maintain local properties they are not globally injective, is very interesting \n\n- The injectivity regularization loss term seems to be novel, and a well-justified and reasonable solution to the issues faced by AEs identified by the authors in section 2\n\n- The focus on making the admissible regularization, and making the global minima be independent of probab ility measure, is very interesting\n\n- The proposed bi-lipschitz regularization for admissibility is again novel and well-justified \n\nThe performance against the compared methods is good; the proposed approach consistently has the best quantitative performance, and qualitatively the learned embeddings appear to be much closer to the ground truth manifold"}, "weaknesses": {"value": "- The evaluation is done on simple toy datasets: Swiss Roll, dSprites, MNIST. I don't believe the paper *needs* to include results on larger benchmarks, but the lack of these datasets does lead one to question the practical utility of the proposed approach. \n\n- It would be nice to see a quantitative runtime comparison with other approaches"}, "questions": {"value": "- I may be mistaken, but could the proposed BLAE method be applied to the toy data shown in figure 1? If so, does it succeed in learning the true manifold? I was surprised to not see the proposed method included here, since this example helps justify the weakness of standard AEs and the need for the proposed BLAE."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iG4lNOXP7b", "forum": "rl2dPJEk8b", "replyto": "rl2dPJEk8b", "signatures": ["ICLR.cc/2026/Conference/Submission15842/Reviewer_JaoM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15842/Reviewer_JaoM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029877918, "cdate": 1762029877918, "tmdate": 1762926065786, "mdate": 1762926065786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}