{"id": "4f4gzyD2Av", "number": 11254, "cdate": 1758194436307, "mdate": 1759897598156, "content": {"title": "RE-Searcher: Robust Agentic Search via Goal-oriented Planning and Self-reflection", "abstract": "Large language models (LLMs) excel at knowledge-intensive question answering and reasoning, yet their real-world deployment remains constrained by knowledge cutoff, hallucination, and limited interaction modalities. Augmenting LLMs with external search tools helps alleviate these issues, but it also exposes agents to a complex search environment in which small, plausible variations in query formulation can steer reasoning into unproductive trajectories and amplify errors. We present a systematic analysis that quantifies how environmental complexity induces fragile search behaviors and, in turn, degrades overall performance. To address this challenge, we propose a simple yet effective approach to instantiate a search agent, RE-Searcher. During search, RE-Searcher explicitly articulates a concrete search goal and subsequently reflects on whether the retrieved evidence satisfies that goal. This combination of goal-oriented planning and self-reflection enables RE-Searcher to resist spurious cues in complex search environments and perform robust search. Extensive experiments show that our method improves search accuracy and achieves state-of-the-art results. Perturbation studies further demonstrate substantial resilience to noisy or misleading external signals, mitigating the fragility of the search process. We believe these findings offer practical guidance for integrating LLM-powered agents into more complex interactive environments and enabling more autonomous decision-making.", "tldr": "", "keywords": ["Agentic Model", "Reinforcement Learning", "Search Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9e4b92d7a319c3fedff09e51955647aa8a2825ce.pdf", "supplementary_material": "/attachment/3b2a9b7e34e14d5a64688efd5a24a1479bc63769.zip"}, "replies": [{"content": {"summary": {"value": "1. This paper focuses on agentic search, analyzing the complexity and variability of search environments as well as the fragility of search agents. It argues that search agents should incorporate explicit goal clarification and reflection on search results like humans.\n\n2. This work introduces RE-Searcher, which integrates explicit goal clarification and reflection on search results into the agent loop. It further designs a reflection reward, employing a stronger LLM-as-Judge to assess reflection quality, and trains the search agents using the GRPO algorithm.\n\n3. Extensive experiments across in-domain and out-of-domain QA benchmarks show that RE-Searcher consistently outperforms recent agentic search baselines (e.g., Search-R1, ZeroSearch, O2-Searcher), with notable robustness under perturbations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Good motivation and preliminary analysis: The paper clearly identifies the challenges of environmental variability and inherent fragility faced by search agents, and conducts preliminary experiments to validate these issues.\n\n2. Clear methodological design: In response to the identified problems, the paper extends the agent loop and proposes an effective training strategy. The methodological descriptions and figures are well-presented and easy to follow.\n\n3. Strong results: The experiments compare against a wide range of baselines and achieve the best overall performance. Extensive ablation and robustness analyses further demonstrate how effectively the proposed method addresses the original problem."}, "weaknesses": {"value": "1. In Sections 2.1 and 4.4, the paper examines the inherent fragility of search agents. While I acknowledge the stochasticity in search agent outputs, sampling only twice per question may not be sufficient to support such a conclusion.\n\n2. As I understand it, there may be an issue with Equation (2). The function FM is not explicitly defined, and I assume it is a 0–1 function similar to EM. When EM equals 1, if the format is correct (FM=1), the reward r_{em_format}=1-0.2\\*1=0.8. However, if the format is incorrect (FM=0), the reward becomes r_{em_format}=1-0.2\\*0=1, which paradoxically yields a higher reward for incorrect formatting.\n\n3. I think using LLM-as-Judge to provide reward supervision for the newly introduced reflection action is a very good choice. However, I noticed in Equation (3) that the reflection reward is added to the outcome reward. This makes me wonder:  since reflection is a process-level action, why not treat it as a process reward instead? Theoretically, process rewards are more fine-grained and could improve sample efficiency. In addition, how was the weight of 0.1 in Equation (3) determined?\n\n4. Based on your main experimental results (Table 2), since all datasets are open-domain QA, why does the method achieve substantial improvements on some datasets but not on others? Have you analyzed the reasons behind this phenomenon?"}, "questions": {"value": "Please refer to Weaknesses. I am open to changing the score based on your rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Wlc4qRfCFa", "forum": "4f4gzyD2Av", "replyto": "4f4gzyD2Av", "signatures": ["ICLR.cc/2026/Conference/Submission11254/Reviewer_i3qY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11254/Reviewer_i3qY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710411993, "cdate": 1761710411993, "tmdate": 1762922412614, "mdate": 1762922412614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the fragility of LLM-based search agents in complex environments where small query variations can lead to drastically different retrieval results and erroneous reasoning trajectories. The authors propose RE-Searcher, an agentic framework designed to improve robustness by integrating goal-oriented planning and self-reflection. During its search process, the agent explicitly articulates a search goal, retrieves information, and then reflects on whether the retrieved evidence successfully satisfies that goal. The agent is trained using GRPO with a composite reward signal that includes factual correctness, format adherence, and a reflection accuracy score provided by an \"LLM as Judge\". Experimental results show that RE-Searcher achieves state-of-the-art performance on several question-answering benchmarks and demonstrates significantly improved robustness against search fragility and external query perturbations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear. The preliminary analysis in Section 2.1 quantifies output stochasticity across different model scales. This empirically demonstrates a critical instability problem that fundamentally limits achievable performance.\n- The proposed method is intuitive and clearly described in Section 3. The structured generation template clearly defines three discrete actions.\n- Robustness Analysis is novel. The Pass@2 analysis in Section 4.3 demonstrates that RE-Searcher substantially reduces the random-right ratio."}, "weaknesses": {"value": "- The novelty of core concepts is limited. The core ideas of \"planning\" (decomposing a problem) and \"reflection\" (evaluating retrieved information) are well-established concepts in agentic AI and RAG literature. \n\n- The proposed method introduces multiple new components for each search iteration: an explicit \"think\" step, a \"goal\" generation step, and a \"reflect\" step. This multi-step process seems guaranteed to significantly increase the number of generated tokens and overall inference latency compared to simpler RAG or SFT baselines.\n\n- While the paper ablates the reflection reward, there is no ablation isolating the contribution of goal-oriented planning versus self-reflection. The method combines both mechanisms, but their individual contributions remain unclear."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZulSS6VFfz", "forum": "4f4gzyD2Av", "replyto": "4f4gzyD2Av", "signatures": ["ICLR.cc/2026/Conference/Submission11254/Reviewer_tB4e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11254/Reviewer_tB4e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995544700, "cdate": 1761995544700, "tmdate": 1762922412185, "mdate": 1762922412185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical challenge in LLM augmented with search tools: agent LLMs tend to generate variable and inconsistent results.  Authors first conducts a systematic analysis showing that semantically plausible query variations can lead to dramatically divergent and unreliable search trajectories. To mitigate this, this paper introduces RE-Searcher to explicitly integrate goal-oriented planning and self-reflection into the reasoning process. The method is first trained using SFT, followed by GRPO with a composite reward."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The intuitive integration of goal-setting and binary self-reflection is easy to implement.\n- Authors conduct comprehensive evaluation across divers datasets. Clear ablation studies shown the contribution of the reward design."}, "weaknesses": {"value": "- The observed instability of success-rate in Sec. 2.1 may largely stem from the inherent differences in model capacity.\n- In Sec. 2 and Sec. 4.2, authors primarily assess models by measuring the consistency of accuracy across multiple runs. However, in agentic search settings, the dominant source of instability stem from variations in search queries and inaccurate documents. The paper tackles the challenge with their search and reflection design, but lacks analysis of how the search queries changes before and after training, particularly whether the proposed goal-oriented planning and self-reflection lead to more consistent or robust query formulation.\n-  According to my knowledge, the instruction employed in this work is not particularly challenging for a model like Qwen2.5-3B to follow. The necessity of warm-up needs further clarification."}, "questions": {"value": "- The coefficient of the MBE reward is even smaller than that of the format reward. How does the model perform with larger MBE rewards?\n- How does the model perform after the warm-up?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "95ZJe7QX7v", "forum": "4f4gzyD2Av", "replyto": "4f4gzyD2Av", "signatures": ["ICLR.cc/2026/Conference/Submission11254/Reviewer_SaqS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11254/Reviewer_SaqS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997302405, "cdate": 1761997302405, "tmdate": 1762922411696, "mdate": 1762922411696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a real and under-discussed problem in agentic / search-augmented LLMs: small, plausible variations in search queries can push the agent onto a bad trajectory and the agent often fails to recover. The authors first quantify this “search fragility” by showing (i) high “random-right” rates across two runs and (ii) large drops in similarity after tiny query perturbations. To address this, they propose RE-Searcher, an agent that (1) explicitly states a search goal before issuing a query, (2) reflects on whether retrieved evidence satisfies that goal, and (3) is RL-trained (GRPO) with a mixed reward (format + factual + LLM-as-judge reflection) to keep the model in the “plan–search–reflect–answer” loop."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work identifies a concrete failure mode of search agents, namely that a slightly different but still reasonable query can lead to very different retrieved evidence, which in turn damages answer quality.\n2. The proposed template, “state goal → search → reflect → continue or answer,” is simple, explicit, and can be plugged into existing search-RL systems without complicated architecture changes.\n3. Experiments on both 3B and 7B models, on in-domain and out-of-domain QA, plus ablations on the reward components, make the paper believable."}, "weaknesses": {"value": "1. Training uses GPT-4o-mini (or a similar model) to score reflection quality. This supervision is non-trivial. It is unclear how much of the final robustness comes from this stronger teacher, rather than from the agent’s own structure.\n2. The method may require several search steps, longer contexts, and an external LLM during training. At inference time, does the model always reflect, or can it stop early? A comparison with Search-R1 using the same search budget would be useful."}, "questions": {"value": "1. Why was 0.1 chosen as the reflection reward weight? Was any systematic hyperparameter search conducted?\n\n2. Can you propose or experiment with a self-supervised reflection mechanism that avoids dependency on external LLMs?\n\n3. Why are RE-Searcher’s improvements smaller on some datasets (e.g., NQ) compared to others?\n\n4. How much does the method increase computational cost relative to baselines? How many additional LLM calls per query are required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2KkjniZGiZ", "forum": "4f4gzyD2Av", "replyto": "4f4gzyD2Av", "signatures": ["ICLR.cc/2026/Conference/Submission11254/Reviewer_4Xdc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11254/Reviewer_4Xdc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998588505, "cdate": 1761998588505, "tmdate": 1762922411289, "mdate": 1762922411289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}