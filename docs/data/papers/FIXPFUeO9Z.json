{"id": "FIXPFUeO9Z", "number": 21096, "cdate": 1758313716256, "mdate": 1759896942221, "content": {"title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer", "abstract": "Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.", "tldr": "We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe", "keywords": ["Unified", "Multimodal Large Language Models", "understanding", "generation", "hybrid tokenizer"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3edadd1e92c64bb963dc446ec0dc01240aa2100c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents Manzano, a method for unified understanding and generation of images, via fusion of respectively continuous and discrete tokens. This leads to improved or competitive performance on various tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The fusion of continuous and discrete tokens, as each of them is better suited for generation and understanding, is interesting. \n\nThe results are good, structurally favorable or similar to other methods."}, "weaknesses": {"value": "A discussion of why discrete tokens are better than the proposed method for generation - DPG is lacking. \n\nThe results are mostly about discussing that the quantitative performance is good. I would be interested in a discussion about the learned representations and a discussion on where the unified framework benefits from the unification and where it hurts, including qualitative examples and/or empirical findings that go deeper than the numbers. \n\nIt is not clear to me that the generated images are better than other methods."}, "questions": {"value": "Can you discuss how the learned representations, where the unified framework benefits from the unification and where it hurts, including qualitative examples and/or empirical findings that go deeper than the numbers?\n\nBagel is very performant too. Are there particular advantages of the proposed method compared to Bagel?\n\nHow are the generated images better than those of other methods, what should the reader take from the illustrations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FCEP1psgXP", "forum": "FIXPFUeO9Z", "replyto": "FIXPFUeO9Z", "signatures": ["ICLR.cc/2026/Conference/Submission21096/Reviewer_xZkP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21096/Reviewer_xZkP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761151602619, "cdate": 1761151602619, "tmdate": 1762941253917, "mdate": 1762941253917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hybrid vision tokenizer and a unified MLLM for both image understanding and generation. The vision tokenizer produce both continuous and discrete latent for different tasks: continuous for multimodal understanding and discrete for autoregressive image generation. Experiments on benchmarks, model scaling and tokenizer comparisons validate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation. One tokenzier to handle both understanding and generation is an improtant step to unified models. Compared to many works with two seperate vision tokenizers, this paper proposes a simple and scalable vision tokenizer, leading to higher degree of unification.\n- Detailed illustration of model architecture, training recipe and evaluation.\n- Sufficient experiments on model scaling and model comparisons"}, "weaknesses": {"value": "- Lack of comparisons with related works like ILLUME [1], which also leverages continuous features for image understanding and discrete feature for image generation. Although ILLUME and MANZANO implement the vision tokenizer in a different way, they share similar motivation and architecture in tokenizer and MLLM. This makes the paper with limited techinical novelty.\n- Hybrid tokenizer training. Why to use LLM decoder for tokenizer training instead of reconstructing a pretrained vision tokenizer (e.g. CLIP/SigLIP)? I believe it do good to the image understanding task but may not optimal for the image generation task.\n- Although the two types of visual representation are similar, but the embedding for image generation cannot be reused for image generation like discrete tokenizers. It makes the model hard to handle interleaved image-text tasks.\n\n\n[1] ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance. arXiv 2412.06673"}, "questions": {"value": "- Why not to use open-sourced LLMs?\n- Tab.1. Why Dual Encoder performs much worse on generation tasks?\n- Sec A.2.1. Why the CLIP model is trainable during vision tokenizer training? In ILLUME, the CLIP model is frozen and benefits from faster convergence.\n- How about the performance of image editing on benchmarks? Which representation is used for encoding the input image, discrete or continuous?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iMYkThmYaF", "forum": "FIXPFUeO9Z", "replyto": "FIXPFUeO9Z", "signatures": ["ICLR.cc/2026/Conference/Submission21096/Reviewer_SqUp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21096/Reviewer_SqUp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741075512, "cdate": 1761741075512, "tmdate": 1762941252422, "mdate": 1762941252422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Manzano, a unified and scalable multimodal large language model designed to jointly handle visual understanding and generation. The framework employs a hybrid image tokenizer and a shared vision encoder with dual lightweight adapters that produce both continuous and discrete visual representations within a common semantic space. A single autoregressive LLM models text and image tokens jointly, while an optional diffusion decoder reconstructs pixels from image tokens. Through a unified training recipe, Manzano effectively balances understanding and generation without significant task interference, achieving state-of-the-art performance among unified models and strong competitiveness with task-specific systems."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a very clear and well-motivated narrative, with the logical progression from problem statement to solution design articulated in a highly readable manner.\n\n2. The experimental evaluation is thorough, convincingly demonstrating the effectiveness of the hybrid tokenizer and the unified autoregressive architecture across both visual understanding and generation.\n\n3. The unified framework tackles a challenging trade-off (between understanding and generation) and delivers empirical evidence that minimal task conflict occurs—this strengthens the claim of scalability and generality."}, "weaknesses": {"value": "1. The scaling analysis of LLM size is incomplete — the paper jumps directly from 3B to 30B without intermediate settings such as 7B or 14B, leaving a significant gap in understanding how performance scales with model capacity.\n\n2. While the experiments and insights are clearly presented, the overall methodological novelty is rather limited, as the work mainly integrates existing design elements in a well-engineered manner.\n\n3. The claim “We first pre-train the hybrid tokenizer with a small LLM decoder to pre-align the image features with the LLM feature space” would benefit from additional references to similar prior works, such as X-omni[1] and ETT[2], to better contextualize its contribution.\n\n[1] Geng Z, Wang Y, Ma Y, et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again[J]. arXiv preprint arXiv:2507.22058, 2025.\n[2] Wang W, Zhang F, Cui Y, et al. End-to-end vision tokenizer tuning[J]. arXiv preprint arXiv:2505.10562, 2025.\n\n4. There is a minor typo on the first page — “Manzanoemploys” — which should be corrected for polish."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "47cPGSioRP", "forum": "FIXPFUeO9Z", "replyto": "FIXPFUeO9Z", "signatures": ["ICLR.cc/2026/Conference/Submission21096/Reviewer_B3Pz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21096/Reviewer_B3Pz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908468901, "cdate": 1761908468901, "tmdate": 1762941251689, "mdate": 1762941251689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}