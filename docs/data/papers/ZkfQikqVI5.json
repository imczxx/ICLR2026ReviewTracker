{"id": "ZkfQikqVI5", "number": 16419, "cdate": 1758264379724, "mdate": 1759897241788, "content": {"title": "White-Box Prompt Transformers: Variationally Grounded Prompt–Attention Coupling for Unified Image Restoration", "abstract": "Can soft prompts in vision Transformers be made explainable?\nPrompt-based models have achieved remarkable success in image restoration, yet they remain largely opaque: the underlying Transformer operations and the mechanism by which prompts modulate attention are poorly understood. This work revisits guided image restoration, where an auxiliary modality \\(A\\) assists in restoring a target modality \\(B\\). We interpret \\(A\\) as a prompt and formulate a tailored structure-tensor total variation (STV) model, whose gradient suggests a white-box correspondence to prompt--attention interactions. This provides a principled bridge between prompts and attention. In scenarios where \\(A\\) is unavailable, we abstract its role into learnable soft prompts, enabling end-to-end training within standard Transformer pipelines. By unrolling the gradient flow of the STV variational problem, we derive the White-Box Prompt Transformer (WBPT), a cascaded architecture that embeds interpretability directly into attention operations. Extensive experiments on multiple benchmarks demonstrate that WBPT achieves state-of-the-art restoration performance while offering interpretable, controllable, and robust prompt--attention dynamics.", "tldr": "", "keywords": ["Interpretable Image Restoration"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8dee7205aebb28864854a363c506eeb1b77bf89.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes White-Box Prompt Transformers (WBPT), a variationally grounded framework that aims to provide interpretability for prompt-based image restoration. The authors reinterpret the interaction between prompts and attention through a structure-tensor total variation (STV) formulation, whose gradient is unrolled into a cascaded Transformer architecture. This design yields a theoretically motivated attention mechanism, where each layer corresponds to an optimization step. Experiments on standard benchmarks such as BSD68, Rain100L, and SOTS demonstrate competitive results compared to existing prompt-based models. Overall, the work presents an interesting theoretical perspective, but primarily reframes existing architectures from a new interpretability viewpoint."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and well-structured, with detailed mathematical formulations and organized experimental sections.\n\n2. The proposed framework provides an interesting theoretical reinterpretation of prompt-based Transformers from a variational perspective.\n\n3. The experiments cover multiple standard benchmarks and include visualizations and ablation studies, showing careful empirical evaluation."}, "weaknesses": {"value": "### Main Concerns:\n1. There are no parameters, FLOPs, or runtime discussions regarding the efficiency issue.\n\n2. As an image restoration solution, more visual results should be included, at least, should be included in the supplementary materials.\n\n3. From the experimental perspective, the validation is too limited, only in the Three-Degradation setting, while in recent years, there are already more benchmarks proposed in this topic, for example, 5 degradation, real-world evaluation, mixed degradation, etc, while these are missing, which limits its soundness of the proposed solution. Also, I believe from some recent research papers below, the authors can find more evaluation settings:\n\n[1] Junjun Jiang, Zengyuan Zuo, Gang Wu, Kui Jiang, and Xianming Liu. A survey on all-in-one image restoration: Taxonomy, evaluation, and future trends. TPAMI, 2025. \n\n[2] Xiaole Tang, Xiang Gu, Xiaoyi He, Xin Hu, and Jian Sun. Degradation-aware residual-conditioned optimal transport for unified image restoration. TPAMI, 2025. \n\n[3] Eduard Zamfir, Zongwei Wu, Nancy Mehta, Yuedong Tan, Danda Pani Paudel, Yulun Zhang, and Radu Timofte. Complexity experts are task-discriminative learners for any image restoration. CVPR, 2025.\n\n[4] Yuning Cui, Syed Waqas Zamir, Salman Khan, Alois Knoll, Mubarak Shah, and Fahad Shahbaz Khan. AdaIR: Adaptive all-in-one image restoration via frequency mining and modulation. ICLR, 2025.\n\n4. The implementation details are quite unclear, for example, how to get $x_{0}$ from the Input? Directly via the proposed White-box Prompt Transformer block or via a convolutional operation? \n\n5. In Fig.1, the proposed method shows the **window partition**, so which base transformer block is adopted? The Swin Transformer style or the Restormer? This is totally not clear but extremely important, since the Restormer-style (Also the PromptIR adopted this) transformer did NOT include any window partition. I think this should also be clearly explained since this is not very clear in the current version. \n\n6. The claimed “white-box” interpretability mainly relies on symbolic reformulation of existing attention operations, without providing concrete analytical or causal evidence.\n\n7. The performance improvement over baselines (e.g., PromptIR) is marginal and often within statistical variation, questioning the practical impact of the proposed framework.\n\n8. The variational derivation connecting STV gradients to attention is largely heuristic, lacking rigorous justification or ablation to verify the theoretical correspondence.\n\n9. Despite extensive formulas, the actual architectural novelty is limited—the model structure and training pipeline remain almost identical to previous prompt-based Transformers.\n\n10. The efficiency and scalability of the unrolled gradient-flow design are not discussed, leaving uncertainty about its real advantage in deployment.\n\n\n### Minor Concerns:\n1. The GPU type/usage, the training time, and inference time are expected to be included.\n\n2. Alg 1, Alg 2, and Alg 3 are too wide, which exceeds the overall width of the page requirements"}, "questions": {"value": "1. Could the authors provide a more comprehensive efficiency analysis, including parameter count, FLOPs, and inference/runtime comparison with PromptIR and other recent baselines? This would help verify whether the proposed “white-box” formulation offers any real computational advantage.\n\n2. The base Transformer design (Fig. 1) is unclear — does the model adopt a Swin-like window partitioning scheme or the Restormer-style global attention? Since this architectural choice critically affects both performance and interpretability, clarification and justification are needed.\n\n3. The variational derivation linking STV gradients to attention appears largely heuristic. Could the authors offer empirical or theoretical evidence (e.g., ablation or controlled experiments) demonstrating that this correspondence meaningfully explains the prompt–attention mechanism rather than serving as an analogy?\n\n4. The experimental validation is relatively narrow (only three degradations). Are there any plans or preliminary results for more diverse benchmarks, such as five-degradation, real-world, or mixed-degradation settings, as commonly evaluated in recent all-in-one restoration works?\n\n5. Since the claimed interpretability is one of the main contributions, can the authors provide quantitative or visual analyses (e.g., saliency or correlation metrics) to substantiate that the proposed framework genuinely enhances interpretability or controllability compared to black-box PromptIR?\n\nPlease also refer to the **Weaknesses** for more concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KjZ7ham4Io", "forum": "ZkfQikqVI5", "replyto": "ZkfQikqVI5", "signatures": ["ICLR.cc/2026/Conference/Submission16419/Reviewer_rg6X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16419/Reviewer_rg6X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909129735, "cdate": 1761909129735, "tmdate": 1762926538844, "mdate": 1762926538844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the White-Box Prompt Transformer (WBPT), a framework that introduces a variational perspective to prompt-based transformers for unified image restoration. The authors derive a structure-tensor total variation (STV) loss whose gradient directly shapes the architectural design of the proposed white-box attention mechanism. This results in a direct mathematical link between prompts and attention, aiming to enhance interpretability and controllability. Extensive evaluations on multi-task image restoration show that WBPT attains competitive results while maintaining interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The theoretical connection between variational STV energy and prompt-driven attention is well motivated and clearly articulated."}, "weaknesses": {"value": "1. My main concern is that the distinctions between white-box and black-box models needs clearer empirical justification. In experimental results, the performance margin between WBPT and the top black-box baselines (PromptIR, Restormer) is small. The interpretability and controllability of WBPT, while conceptually valuable, may have limited practical benefits in some real-world scenarios.\n2. Some mathematical approximations require further justification, such as the approximation steps in (4).\n3. The evaluation lacks some very recent related works, especially those on diffusion-based restoration and other interpretable prompt mechanisms.\n4. The paper lacks an analysis of computational efficiency, including comparisons of parameter count, and inference speed between the proposed WBPT and baseline methods."}, "questions": {"value": "1. In the t-SNE analysis (Sec. 3.4), could the authors clarify how the white-box modeling contributes to better separation in the embedding space?\n2. Minor issue: In Table 1, the note “Results are reported as PSNR/SSIM.” is repeated twice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P3OGjF8Y4F", "forum": "ZkfQikqVI5", "replyto": "ZkfQikqVI5", "signatures": ["ICLR.cc/2026/Conference/Submission16419/Reviewer_EDz6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16419/Reviewer_EDz6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922814688, "cdate": 1761922814688, "tmdate": 1762926538436, "mdate": 1762926538436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes prompt‑based all‑in‑one image restoration through a variational lens: it defines an STV energy, derives an approximate gradient, instantiates Single‑/Multi‑Prompted Structure Attention, and unrolls the explicit‑Euler gradient flow into a K‑stage Transformer where each stage couples MPSA with a learnable data‑consistency term. However, in all‑in‑one evaluation, WBPT does not perform so well considering that many recent works can do much better. But the white‑box idea is interesting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper closes the loop from STV energy to gradient approximation, and then attention operator to unrolled optimizer, so that every attention term has a clear energetic origin.\n2. Final‑layer attention focuses on boundaries/structures rather than degradation textures; t‑SNE shows clearer task clusters than PromptIR; prompt‑parameter noise yields much smaller metric drops than PromptIR.\n3. The cost of modification and computation is low. Default K=10 steps; prompts are injected at the 6th block per step; single‑insertion matches multi‑insertion with lower cost."}, "weaknesses": {"value": "1. WBPT uses a black‑box pyramid aggregator, which softens the fully white‑box narrative; a white‑box pyramid sketch would help.\n2. The Eq. (3) gradient‑to‑attention mapping involves approximations; the manuscript does not quantify when/where the mapping deviates or fails.\n3. Compared methods are limited, even some task-specific method like MPRNet are used for comparison."}, "questions": {"value": "1. For Eq. (3), when does the gradient‑to‑attention approximation deviate most? Please add attention‑vs‑gradient discrepancy maps and a brief error analysis.\n2. Could you sketch a differentiable white‑box pyramid (e.g., variational down/up operators with STV‑consistent cross‑scale regularization) to replace the current black‑box aggregator in WBPT, and show a small‑scale comparison?\n3. Beyond “6th block once per step,” can you chart the trade‑off for multi‑insertion, number of prompts N, and projector rank vs accuracy/overhead?\n4. More recent works can be added for comparison such as Perceive-IR (TIP’25) and DFPIR (CVPR’25).\n5. t‑SNE currently covers single degradations; can you include rain+haze / noise+blur mixed protocols?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0ayaUHutxa", "forum": "ZkfQikqVI5", "replyto": "ZkfQikqVI5", "signatures": ["ICLR.cc/2026/Conference/Submission16419/Reviewer_iDnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16419/Reviewer_iDnC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947325992, "cdate": 1761947325992, "tmdate": 1762926537045, "mdate": 1762926537045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}