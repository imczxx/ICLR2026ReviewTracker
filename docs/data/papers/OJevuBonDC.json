{"id": "OJevuBonDC", "number": 9154, "cdate": 1758113428213, "mdate": 1759897740597, "content": {"title": "Forward-Forward Learning with Dynamic Architecture Adaptation for Classification", "abstract": "The Forward-Forward (FF) algorithm has emerged as a promising alternative to the traditional deep learning paradigm based on the backpropagation algorithm. However, both the original FF algorithm and several FF-based extensions rely on the quality of generated negative samples for training, which can limit their effectiveness. \nIn this paper, we design an FF-based algorithm for the classification task. Specifically, we propose the concept of support neuron (SN) sets by partitioning the neurons in each layer into several sets, each explicitly corresponding to a class. The SN set with the strongest response (goodness) determines the predicted class of the input, thereby eliminating the need for negative samples. Furthermore, inspired by the functioning of the brain, we introduce neuron growth and degeneration strategies: (1) when neurons fail to achieve satisfactory performance, new neurons can grow to assist; and (2) neurons that remain inactive across all classes may degenerate. \nExtensive experiments demonstrate that our method achieves state-of-the-art performance on MNIST and CIFAR datasets compared to other FF-based approaches that also eliminate the use of negative samples. In addition, the effectiveness of the proposed neuron growth and degeneration mechanisms is empirically evaluated.", "tldr": "", "keywords": ["Forward-Forward Algorithm", "Dynamic Network Architecture", "Deep Learning", "Representation Learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d371a3e0691d9abeb127da42f5d11a4433c5a57.pdf", "supplementary_material": "/attachment/e2fd6162224137ae96b6cbea0beaecc493fb7c3f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a Forward-Forward (FF)-based learning framework that introduces support neuron (SN) sets to partition neurons by class, thereby eliminating the need for negative samples. Furthermore, the authors design biologically inspired neuron growth and degeneration mechanisms to dynamically adjust network capacity. \\"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explores an interesting direction by combining FF learning with dynamic neural structural adaptation. \n\n- The biologically inspired neuron growth and degeneration mechanisms are interesting for dynamically adjusting network capacity."}, "weaknesses": {"value": "**Ambiguity in the Combo Training Process (Table 1)**\n\nIt is unclear whether the training process using CWC for convolutional layers and PvN for linear layers is conducted sequentially (train convolutional layers first, then linear layers) or simultaneously (both optimized in the same iteration). Moreover, since the proposed method avoids standard backpropagation, it is essential to clarify how parameter updates are coordinated between these two objectives.\n\n**Inconsistencies in Reported Results (Table 2)**\n\nThe reported accuracies for FF and CaFo on MNIST appear inconsistent with values presented in the respective original papers. Additionally, FF is only examined on MNIST is not precise. Both FF and CaFo have published convolutional and non-convolutional variants, and original results for CIFAR-10/100 are available.\n\n**Missing Training Dynamics and Visualization **\n\nThe paper lacks visual or statistical evidence regarding the training dynamics and representation evolution. For instance, it would significantly strengthen the work to include:\n\n- Plots of training and validation accuracy versus epochs to demonstrate convergence behavior and stability.\n- Visualization of response distribution across SN sets over time to support claims about interpretability and neuron specialization.\n- Optional activation heatmaps showing how neurons evolve before and after growth/degeneration.\n\n\n**Theoretical Rationality of Explicit Class-wise SN Assignment**\n\nThe core assumption that neurons can be explicitly and statically assigned to specific classes deserves deeper theoretical justification. In realistic scenarios, classes often share overlapping features; thus, forcing neurons to be class-exclusive may restrict feature sharing and harm generalization.\n\n**Minor Comments**\n1. The description of  “Combo” training (CWC + PvN) could include pseudocode for clarity.\n2. Consider adding figures illustrating neuron growth/degeneration workflows.\n3. The transition from the critical analysis to the overall assessment is not sufficiently smooth, which makes the logical flow of evaluation appear somewhat abrupt and may weaken the coherence and persuasiveness of the review."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gCdRTnZ9qc", "forum": "OJevuBonDC", "replyto": "OJevuBonDC", "signatures": ["ICLR.cc/2026/Conference/Submission9154/Reviewer_GDeD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9154/Reviewer_GDeD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404840716, "cdate": 1761404840716, "tmdate": 1762920837837, "mdate": 1762920837837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an extension to the Forward-Forward (FF) learning paradigm for classification. The main innovations are (1) partitioning neurons into class-specific Support Neuron (SN) sets and defining SN-set responses (goodness) as the sum of squared activations for class prediction, and (2) introducing biologically inspired neuron growth and degeneration mechanisms that dynamically adjust network width. The authors evaluate three loss functions, i.e. Positive-vs-Negative (PvN), a channel-wise competitive (CwC) SN-set-wise loss, and Cross-Entropy (CE), and a hybrid training scheme (Combo) that uses CwC for convolutional layers and PvN for the final linear layer. Experiments on MNIST, CIFAR-10, and CIFAR-100 show improvements over other FF-based methods that avoid negative samples (e.g., CFSE and CaFo)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: Extending channel/grouping ideas to class-specific Support Neuron (SN) sets and integrates neuron growth/degeneration.\n\nClarity of core idea: SN-set response (sum of squared activations) and the LLR/OLR prediction criteria are clearly formulated.\n\nPractical relevance: Eliminating negative samples addresses a practical limitation of many FF variants and may simplify implementations for resource-constrained settings.\n\nEmpirical promise: The Combo training strategy shows competitive results compared to other FF-based approaches in the paper."}, "weaknesses": {"value": "Motivation: The proposed strategies, e.g. neuron growth and degeneration, seems to be ad hoc, i.e. it lacks a rigorous motivation or toy experiment to validate them.\n\nBaseline fairness: The reported DNN baseline (65.4% on CIFAR-100) appears a little low for comparable architectures. The authors should clarify the DNN architecture, training recipe, and whether data augmentation or other standard techniques were used.\n\nLimited comparison scope: Important recent FF variants and stronger BP baselines (beyond ResNet18 and a single DNN) are not included; this weakens the claim of its superiority.\n\nInsufficient justification for SN response metric and hyperparameters: The square-sum choice needs empirical comparisons (e.g., L1, max, soft-assignment) and sensitivity analysis.\n\nGrowth/degeneration analysis: The degradation experiments show large and non-monotonic changes (e.g., removing two neurons sometimes collapses accuracy). Authors should analyze per-neuron contribution and report variance across different random seeds.\n\nReproducibility: Some low-level training details (exact optimizer schedules, seed handling, and implementation variants for baselines) should be moved into the main text or a reproducibility checklist."}, "questions": {"value": "1. Could the motivation of the some strategies be presented in a more rigorous way ?\n\n2. Please provide the exact DNN baseline architecture and full training recipe for CIFAR-100 (data augmentation, optimizer, learning-rate schedule, number of runs), since the proposed baseline is a little low. \n\n3. Please analyze the neuron removal experiments more thoroughly. For the cases where removing two neurons causes a disproportionate drop, can the authors (a) report per-neuron activation statistics, (b) visualize learned filters or class-specific activations, and (c) show results averaged across multiple random seeds?\n\n4. Have the authors tried alternative SN-set response metrics (e.g., L1 norm of activations, max pooling within channels, cosine similarity on pooled features) ?\n\n5. Please include additional baselines: (i) stronger BP baselines (well-tuned DNN/ResNet variants), and (ii) recent FF variants such as DeeperForward/Distance-Forward if implementations are available.\n\n6. Please provide a sensitivity study for key hyperparameters: thresholds for PvN, selection rules for growth, number of neurons added per growth step, and degeneration thresholds.\n\n7. How about the robustness of the Combo scheme (CwC for conv layers + PvN for linear layers) across architectures and dataset ? An ablation would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jJjkLQqIDY", "forum": "OJevuBonDC", "replyto": "OJevuBonDC", "signatures": ["ICLR.cc/2026/Conference/Submission9154/Reviewer_Hfab"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9154/Reviewer_Hfab"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666727788, "cdate": 1761666727788, "tmdate": 1762920837220, "mdate": 1762920837220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a forward-only, no-negatives training scheme that partitions each layer into per-class Support Neuron (SN) sets and defines “goodness” as sum of squared activations per set. It studies losses on per-class goodness, two prediction rules (LLR, OLR), and introduces neuron growth and neuron degeneration to adapt width. Results are shown on MNIST, CIFAR-10, CIFAR-100, compared to CFSE (Papachristodulou et al, 2024) and CaFo (Zhao et al, 2023) (forward methods without negatives) and to BP baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Width adaptation idea (growth and degeneration) is interesting and could be impactful with deeper evaluation."}, "weaknesses": {"value": "- Conv-layer partitioning into sets is a mechanism already introduced in prior work CFSE (Papachristodulou et al, 2024). The paper acknowledges this but still lists this set partitioning as a core contribution. In my opinion the genuine novelty is the linear-layer extension and width adaptation. In addition, all three losses have already been introduced in prior work with the same name (Papachristodulou et al, 2024). The PvN and CwC as used here are already defined in CFSE (Papachristodulou et al, 2024) for channel groups (CFSE Eqs. 5–6). The manuscript should be explicit about reuse and position its difference. As it stands the mathematical formulation is the same. \n- The paper’s CwC equals CE applied to per-class goodness logits (g_l). The CE section repeats the same. This duplication confuses the contribution and makes the loss study harder to interpret. \n- Section 3.7 claims training each layer “directly on the entire dataset” and that optimization is “essentially closed-form.” The appendix algorithm trains with random mini-batches using SGD/Adam, i.e., not closed-form nor full-batch. \n- Only last-layer edits, small additions/deletions, and sensitivity (including degradations) are shown. Authors note single runs with fluctuating results in some cases.\n- Reported gains over CFSE/CaFo are rather incremental (e.g., CIFAR-10 80.5% vs CFSE 77.2%; CIFAR-100 52.0% vs CFSE 48.9%). This is fine, but it does not show a qualitative jump. \n- The paper states “architectures are kept consistent” in Table 2 comparisons, but also evaluates CaFo with a revised architecture (Re). This change should be labeled carefully. \n- The paper lacks comparison and/or discussion with standard local-learning and FF-inspired methods beyond CFSE/CaFo. In particular FA, DFA, DRTP, SoftHebb. These are canonical non-BP or local-error baselines that target similar motivations (forward compute, cheap feedback, biological plausibility). The paper is missing a comparison with at least one method per family or provide a documented rationale if direct runs are infeasible."}, "questions": {"value": "- Can you report some features of the different models, such as parameters, FLOPs, and memory for your method vs CFSE/CaFo/BP for better comparison?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "This has been flagged in order for the Program Chairs/Area Chair to look into this as I believe their might be indications for plagiarism with prior work mentioned below. \n\nMisrepresentation of novelty and incomplete attribution of prior work points to poor research integrity practices\nManuscript: “Forward-Forward Learning with [SN sets]” (ICLR)\nPrior work: Papachristodoulou et al., AAAI 2024 (first introduces CFSE / CwC, PvN, CwC_CE),\n\nIn general while the work of AAAI is references in this ICLR submission many components are claimed as new, however they where originally introduced in the prior work.\n\nIdentical loss functions misrepresented as distinct:\nThe ICLR submission’s “CwC” and “CE” losses are mathematically and implementation-wise identical-both apply softmax cross-entropy to the same class-wise logits. The paper, however, treats them as different objectives in text and results, thereby claiming the same novelty.\n\nUnattributed reuse of the channel-wise PvN adaptation:\nThe submission’s PvN formulation, code structure, and descriptive text closely replicate the AAAI 2024 implementation and exposition (channel-wise positive/negative margins with threshold), but credit only Hinton’s original Forward-Forward algorithm, omitting the AAAI adaptation source.\n\nDocumented code and textual parallels:\nSide-by-side code excerpts from the AAAI and ICLR repositories show line-level equivalence across all three losses (CwC manual, CE, PvN), differing only in variable names thus potentially demonstrating that the same codebase was used but the implementation for the original losses where not acknowledged correctly. \n\nCode in supplementary material has been examined and detailed point by point analysis can also be provided."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "I8mtbIkQ9W", "forum": "OJevuBonDC", "replyto": "OJevuBonDC", "signatures": ["ICLR.cc/2026/Conference/Submission9154/Reviewer_5spw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9154/Reviewer_5spw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943351644, "cdate": 1761943351644, "tmdate": 1762920836808, "mdate": 1762920836808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an enhancement to the standard Forward-Forward (FF) algorithm designed specifically for classification tasks. It addresses a key limitation of the original FF model and its reliance on high-quality negative samples. They propose the concept of Support Neuron (SN) sets and integrating Dynamic Architecture Adaptation using grow and prune neurons concept. The SN sets partition neurons in each layer to explicitly correspond to class concepts, thus enforcing local class-specific representations and improving feature discrimination compared to generic FF variants."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Directly solves a major weakness of the original FF algorithm, making the training process more robust and easier to implement without complex negative sample generation strategies.\n\n- The explicit partitioning of neurons into class-specific SN sets creates a more structured and potentially more interpretable feature space, as one can directly analyze the activity of neurons associated with a particular class.\n- The paper is well written and easy to follow."}, "weaknesses": {"value": "- Computational overhead of the Dynamic Architecture Adaptation is unknown compared to the standard FF models. \nProvide a detailed analysis of the average time increase per training epoch (or step) compared to a standard, fixed-architecture FF model with the same number of layers and total neurons.\n\n\n- The core concept relies on partitioning neurons into explicit class-based sets. This is straightforward for simple classification (e.g., CIFAR, MNIST) but breaks down for tasks without fixed, discrete classes limiting the approach's general applicability. How easy or difficult to extend this beyond classification is not known.\n\n- The rigid, class-specific partitioning of neurons into SN sets, while enhancing interpretability, might constrain the model's ability to learn efficient, shared representations. For fine-grained classification, this forced disentanglement could be less efficient than a flexible, dense layer that naturally compresses shared features across multiple classes. Provide emprical results for fine grained classification and use Centered Kernel Alignment (CKA) to quantify the similarity between the feature representations of two highly related classes (e.g., two different bird species or types of cars) in the penultimate layer. Compare the CKA score between the Dynamic FF model and a standard, dense FF model.\n\n- While the Support Neuron (SN) sets are designed to mitigate the reliance on high-quality negative samples, the paper might not eliminate the need for them entirely.  Compare the Dynamic SN-FF model's performance against the standard FF baseline when both are trained using trivial, low-quality negative samples (e.g. completely random noise or samples drawn from a different distribution).\n\n- Usually, dynamically adjusting the architecture introduces instability during training. Any insights or detailed discussion regarding the model training will be useful."}, "questions": {"value": "See Weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fcZAxtdCOX", "forum": "OJevuBonDC", "replyto": "OJevuBonDC", "signatures": ["ICLR.cc/2026/Conference/Submission9154/Reviewer_9LLh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9154/Reviewer_9LLh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001450619, "cdate": 1762001450619, "tmdate": 1762920836481, "mdate": 1762920836481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}