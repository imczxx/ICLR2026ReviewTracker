{"id": "qTTmUJFG38", "number": 4020, "cdate": 1757585219949, "mdate": 1759898057722, "content": {"title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation", "abstract": "Recently, Large Language Models (LLMs) have shown great potential in natural language-driven molecule discovery. \nHowever, existing datasets and benchmarks for molecule-text alignment are predominantly built on a one-to-one mapping, measuring LLMs' ability to retrieve a single, pre-defined answer, rather than their creative potential to generate diverse, yet equally valid, molecular candidates.\nTo address this critical gap, we propose **S**peak-to-**S**tructure (**S$^2$-Bench**), \nthe first benchmark to evaluate LLMs in open-domain natural language-driven molecule generation.\nS$^2$-Bench is specifically designed for one-to-many relationships, challenging LLMs to demonstrate genuine molecular understanding and generation capabilities. \nOur benchmark includes three key tasks: molecule editing (**MolEdit**), molecule optimization (**MolOpt**), and customized molecule generation (**MolCustom**), each probing a different aspect of molecule discovery. \nWe also introduce **OpenMolIns**, a large-scale instruction tuning dataset that enables Llama-3.1-8B to surpass the most powerful LLMs like GPT-4o and Claude-3.5 on S$^2$-Bench. \nOur comprehensive evaluation of 28 LLMs shifts the focus from simple pattern recall to realistic molecular design, paving the way for more capable LLMs in natural language-driven molecule discovery.", "tldr": "", "keywords": ["Large Language Models", "Natural Language Processing", "Molecule Discovery", "LLM Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e675a14d96313190fa09b342bdb4da9f2d1ac057.pdf", "supplementary_material": "/attachment/b6e513fbb05cdb287f1b06112bed551bbb2ce3b8.zip"}, "replies": [{"content": {"summary": {"value": "The author suggests that LLM-based molecular generation should adhere to a one-to-many principle and has constructed a benchmark dataset based on this principle. However, the core concepts and dataset construction method of this work are very similar to those in article [1]. However, the manuscript does not appropriately cite [1] or articulate the differences between the two works. Additionally, the LLMs evaluated in this work are all general-purpose LLMs. While they possess some understanding of SMILES data, there remains a significant gap with the molecular generation field. The author should assess more domain-specific LLMs.\n\n[1] https://link.springer.com/article/10.1186/s12915-025-02200-3"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The principle that molecular generation should adhere to a one-to-many approach is crucial, and the author has developed a benchmark based on this notion, evaluating it across multiple LLMs. However, the core concept and the method of constructing the benchmark in this paper are highly similar to those in work [1]."}, "weaknesses": {"value": "The methodology is highly similar to that in work [1] and lacks comprehensive research. The LLMs evaluated are all general-purpose models, with a noticeable absence of specialized domain-specific models."}, "questions": {"value": "no"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JZnGelKSyN", "forum": "qTTmUJFG38", "replyto": "qTTmUJFG38", "signatures": ["ICLR.cc/2026/Conference/Submission4020/Reviewer_Cos8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4020/Reviewer_Cos8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617765679, "cdate": 1761617765679, "tmdate": 1762917137948, "mdate": 1762917137948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces S²-Bench (Speak-to-Structure), a large-scale benchmark designed to evaluate the capability of LLMs in open-domain natural-language-driven molecular generation. Unlike traditional one-to-one text-to-SMILES datasets, S²-Bench adopts a one-to-many paradigm, allowing multiple valid molecular outputs for a single instruction to better mimic real-world chemical design. The benchmark includes three complementary tasks—MolEdit, MolOpt, and MolCustom—that respectively assess structural editing, property optimization, and constrained generation. To support this evaluation, the authors construct OpenMolIns, a 1.2-million-sample programmatically generated dataset of instruction–molecule pairs using RDKit-based property computation and LLM paraphrasing for linguistic diversity. The paper further proposes the Weighted Success Rate (WSR) metric to integrate chemical validity, success rate, and novelty. Experiments on 28 LLMs (e.g., GPT-4o, Claude-3.5, LLaMA-3.1, Qwen-2) reveal that current models struggle with true chemical reasoning, while instruction-tuned variants significantly outperform base models. Overall, the work provides a reproducible, scalable, and realistic benchmark for assessing how LLMs perform in molecular reasoning and creative design."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a timely and impactful contribution by redefining how natural-language-driven molecule generation should be evaluated.  \n(1) The benchmark is conceptually original, introducing the one-to-many mapping paradigm that better reflects chemical diversity and real-world design scenarios.  \n(2) The dataset generation pipeline is well-engineered and reproducible, integrating chemical computation (RDKit) with LLM-based linguistic diversification.  \n(3) The evaluation design—combining MolEdit, MolOpt, and MolCustom—offers a comprehensive assessment of LLMs’ structural reasoning and control capabilities.  \n(4) Experiments are large-scale and convincing, covering both open and closed LLMs with consistent results.  \n(5) The paper is clearly written, well-structured, and likely to serve as a standard benchmark for future research in chemical LLMs."}, "weaknesses": {"value": "The main weakness lies in the limited methodological depth. While the benchmark is well-designed, the work does not provide theoretical insight into the relationship between language semantics and chemical structure reasoning. The Weighted Success Rate metric, though practical, appears heuristic, and its weighting choices are not empirically justified. The programmatic data generation may also introduce semantic drift between the instruction and molecule, especially after paraphrasing by LLMs. Additionally, the dataset relies exclusively on RDKit-computed properties, which may not reflect experimental conditions, and the benchmark focuses primarily on organic drug-like molecules, limiting generalization to other domains."}, "questions": {"value": "Q1. Data reliability after paraphrasing  \nSince all instructions are automatically generated and paraphrased by LLMs, how do the authors ensure that the final language remains semantically consistent with the intended molecular transformation? Would human verification or semantic-similarity filtering improve dataset fidelity?  \n\nQ2. Distributional bias of molecular sources  \nOpenMolIns is built mainly from ZINC, ChEMBL, and MOSES, which are biased toward drug-like molecules. Could this limit generalization to other chemical domains (e.g., materials or catalysts)?  \n\nQ3. Scientific validity of the evaluation metric  \nHow are the weights in the Weighted Success Rate (WSR) determined, and do they align with human expert judgments of chemical success or usefulness?\n\nQ4. Language understanding vs. template learning  \nSince instructions are generated from deterministic templates, does S²-Bench truly measure language understanding or just template matching? Have the authors tested models on non-templated, free-form instructions?\n\nQ5. Out-of-distribution generalization  \nHave the authors evaluated the benchmark under compositional or OOD instructions, e.g., “Add a hydroxyl group while keeping molecular weight below 200”?\n\nQ6. Alignment with real chemical measurements  \nSince all target properties come from RDKit computations, how well do they correlate with experimental measurements? Could integrating experimental datasets improve realism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jUU12cPy0Q", "forum": "qTTmUJFG38", "replyto": "qTTmUJFG38", "signatures": ["ICLR.cc/2026/Conference/Submission4020/Reviewer_fcDQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4020/Reviewer_fcDQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806426822, "cdate": 1761806426822, "tmdate": 1762917137604, "mdate": 1762917137604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Speak-to-Structure, a benchmark for open-domain, one-to-many natural language–driven molecular design, comprising three task families, MolEdit, MolOpt, and MolCustom, which focus on precise editing, property-oriented optimization, and de novo constrained generation. It also releases OpenMolIns, an instruction-tuning dataset and reports results across 28 LLMs, showing that targeted one-to-one datasets overestimate real design ability while instruction-tuned models on OpenMolIns perform best."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark allows many valid molecules per prompt, not just one “right” answer, which is closer to how chemists actually design.\n2. Success is checked automatically, and everything rolls up into a single headline number (WSR) so models are straightforward to compare.\n3. Many models are evaluated side-by-side, revealing where current methods struggle (especially de-novo constraints) and showing that instruction-tuning on the released data can meaningfully boost performance."}, "weaknesses": {"value": "1. The “weighted success rate” is computed as *success × one quality term* (similarity for MolEdit/MolOpt; novelty for MolCustom), then averaged uniformly across nine subtasks. Because there are no reported thresholds or sensitivity analyses, rankings may be unstable under this multiplicative choice and the equal subtask weights.\n2. Prompts are generated from fixed templates, and MolCustom’s constraints largely boil down to counts of atoms, bonds, or functional groups. Important real-world specs—stereochemistry, 3D geometry, ring/bridge topology, and basic synthesizability—are not directly assessed.\n3. The test molecules are drawn from Zinc-250K for convenience, and the builders pre-select which functional groups/atoms/bonds occur, including “normal/edge” subsets. These design choices can shift task difficulty and may not reflect the breadth of discovery-scale chemical space."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NYYaXF3rl7", "forum": "qTTmUJFG38", "replyto": "qTTmUJFG38", "signatures": ["ICLR.cc/2026/Conference/Submission4020/Reviewer_Eqbw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4020/Reviewer_Eqbw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972367408, "cdate": 1761972367408, "tmdate": 1762917137176, "mdate": 1762917137176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}