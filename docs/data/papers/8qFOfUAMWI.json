{"id": "8qFOfUAMWI", "number": 16916, "cdate": 1758270210897, "mdate": 1763034507296, "content": {"title": "Directly Aligning the Full Diffusion Trajectory with Semantic Relative Preference Optimization", "abstract": "Recent studies have demonstrated the effectiveness of aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.", "tldr": "An online reinforcement learning method that aligns text-to-image generation with human preferences.", "keywords": ["diffusion model", "text to image synthesis"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/25b8e719fffc566702acd963eb3b6dce397e5320.pdf", "supplementary_material": "/attachment/ba71b1792f3c07c1933b5024d8f2d527d146d628.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Direct-Align which is a online reward optimization framework for diffusion models. It identifies that previous online optimization frameworks that directly backpropagates gradients from reward models focus on small number of timesteps closer to the clean data, and proposes to add a predefined noise to a sampled data so that there is a path for perfectly reconstructing the sample. In addition, the paper introduces Semantic Relative Preference Optimization where rewards are formulated with text-conditioned signals which enables dynamic control of the reward model via prompt augmentation. The quantitative results demonstrate this framework achieves the state-of-the-art performanceon HPDv2 benchmark and human evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a challenging problem of online reward optimization for diffusion models which backpropagtes through the reward models which can potentially train more efficiently than reinforcement learning frameworks that assume black box reward models or Diffusion DPO that requires preference datasets. \n- It demonstrates the state-of-the-art performance in a image generation benchmark."}, "weaknesses": {"value": "- Single-step image recovery: it is not clear why ensuring perfect reconstruction makes the learning better. At a high noise level, using the ground-truth noise as a hint would confuse the reward model as it guides the reconstruction towards the original sample. In equation 5, $\\Delta \\sigma$ is not properly defined, and it's also not clear what $r = r()$ means. Both $\\sigma_t$ and $\\sigma$ are used and not explained.\n - Reward Aggregation Framework: in line 213, '...decaying discount factor through gradient accumulation, which helps mitigate reward hacking at later timesteps' is not properly explained and it is not clear why this is true. Also $\\lambda(t)$ is not properly defined and there is another function $r()$ that seems different from equation 5 and not properly introduced. In line 211, 'sequence of images ${x_k,...,x_{k-n}}$ shares the same subscript as $x_0$ but it appears the subscripts in the sequence denote different samples, which is confusing. \n- Inversion-Based Regularization: it is hard for the reviewer to understand what equation 9 entails and a further explanation of this section would be appreciated.\n- Section 4.3, Denoising Efficiency: it is expected if using ground truth noise, reconstruction is much easier. It is not clear what this section is adding in value.\n\nOther sources of unclear notations:\n- Equation 1 says R and equation 7 says RM which are both not defined. \n- Line 232: 'Although ... ' does not complete a sentence\n- Equation 9: $r_1$ not defined\n- Section 4.1 title: ending with '.'"}, "questions": {"value": "It would be great if the points in the weaknesses section can be addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oIp8tIANGn", "forum": "8qFOfUAMWI", "replyto": "8qFOfUAMWI", "signatures": ["ICLR.cc/2026/Conference/Submission16916/Reviewer_ufow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16916/Reviewer_ufow"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571882606, "cdate": 1761571882606, "tmdate": 1762926943112, "mdate": 1762926943112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "EidK1Yt9lm", "forum": "8qFOfUAMWI", "replyto": "8qFOfUAMWI", "signatures": ["ICLR.cc/2026/Conference/Submission16916/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16916/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763034506401, "cdate": 1763034506401, "tmdate": 1763034506401, "mdate": 1763034506401, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Direct-Align and Semantic-Relative Preference Optimization (SRPO) to align text-to-image diffusion models across the full trajectory. Direct-Align pre-samples a noise prior which facilitates one-step denoise/inversion to recover the image from any timestep via interpolation, avoiding costly multistep backprop and enabling stable early-timestep optimization. It targets the common failure where prior methods optimize only late steps and become prone to reward hacking; by unlocking early steps, the paper argues it mitigates this issue. The authors further propose SRPO that makes the reward text-conditioned using positive/negative prompt augmentation, allowing quick online adjustment without repeatedly re-tuning the reward model.  In experiments (e.g., on FLUX.1-dev), the authors report large gains in human-evaluated realism and aesthetics along with strong training efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Full-trajectory optimization. Direct-Align can recover an image from any timestep using a predefined noise prior. This enables more stable early-step updates while avoiding late-step over-optimization.  \n\n- Superior human evaluation. On FLUX.1-dev, the paper reports large gains in human-rated realism and aesthetics versus baseline. \n\n- Flexible reward adjustment. SRPO makes the reward text-conditioned using positive/negative prompt augmentation."}, "weaknesses": {"value": "- Lack of support for attributing reward hacking to late, few-timestep optimization.\nAlthough late-timestep-focused optimization shows reward hacking, and the proposed method focuses on early timesteps and shows reduced hacking in Fig. 5(c), the paper should provide a deeper analysis of why late timesteps lead to undesired reward hacking. Intuitively, late steps emphasize high-frequency detail, which may encourage hacking, but more concrete analysis is needed.\n\n- There is a significant mismatch between the equations and figures (and the released code) for x_0 prediction.\n\n- Why SRPO training is much faster than ReFL and DRaFT-LV? Supposedly SRPO and DirectAlign should be slower than ReFL, as ReFL can skip the full trajectory sampling, while the proposed methods require the full-trajectory sampling to get the noise prior.\n\n- Closeness to ReFL via timestep reweighting.\nThe method essentially reduces the weight of gradients at early timesteps, so it is roughly a reweighted ReFL that tunes down early-step contributions. How much do early-timestep gradients actually contribute? If late timesteps dominate the gradient direction, the approach collapses to ReFL.\n\n- Reliability of SRP (prompt-based reward).\nSRP relies on simple prompt adjustments and differences of positive–negative text embeddings. What direct evidence supports the claim that SRP fixes the oversaturation tendency in HPS-v2.1? Please show targeted ablations and counter-examples.\n\n- Significant ambiguity in implementation details.\nFirst, selecting \\Delta\\sigma_t is hard; the paper does not clearly state how it is set during training, and it lacks comparisons of different \\sigma_t schedules. Second, in the implementation, training uses steps 5–30 of 50, not the full trajectory. Is this correct?\n\n\n- Writing and notation issues.\nThe paper is not well written and is hard to understand. The “Inversion-Based Regularization” paragraph is difficult to interpret. Could the authors give more explanation on why inversion is needed and how it's implemented? Notation is unclear and inconsistent. For example, in Eqs. (1–2) r denotes a reward value, but in Eq. (5) it denotes a reward function. In Eq. (6), the mixed use of k and t is confusing."}, "questions": {"value": "1.\tIn Equation (6), shouldn’t the discounted weight be placed inside the summation?\n2.\tIn Figure 6, is there multiple models tuned, each is tuned with a specific style word applied? And some tuned models can output desired\n3.\tWhy there are degradations in GenEval scores in all RL-aligned models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y1NWVNHSIr", "forum": "8qFOfUAMWI", "replyto": "8qFOfUAMWI", "signatures": ["ICLR.cc/2026/Conference/Submission16916/Reviewer_cPDX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16916/Reviewer_cPDX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761592842263, "cdate": 1761592842263, "tmdate": 1762926942739, "mdate": 1762926942739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces several contributions aimed at improving reinforcement learning for diffusion models: \n\n1. Mitigating reward hacking by mixing predicted noise with ground-truth noise, enabling more accurate single-step image recovery and more effective gradient propagation with respect to individual denoising steps.\n2. Reward aggregation across diffusion steps to reduce reward hacking in later stages of the generation process.\n3. Leveraging embedding arithmetic in CLIP-like encoders to counteract negative biases in reward signals.\n4. Inversion-based regularization, which builds upon their single-step recovery framework to further stabilize training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. All proposed components are simple and easy to implement.\n\n2. The method demonstrates improved computational efficiency, it is also evidenced by the GPU-hour comparison in Table 1.\n\n3. The approach effectively leverages existing CLIP-based reward models, showcasing compatibility with widely used and accessible architectures."}, "weaknesses": {"value": "1. While the paper presents a practical and nearly plug-and-play method, it lacks ablation studies to justify key design choices. For example, the effects of the constant K from equation 9, $\\lambda(t)$ from equation 6, weighting from equation 5. \n2. The writing can be somewhat confusing in places. For instance, in Equation 9,  K is a constant, $\\Delta \\sigma_t$ should also remain constant, whereas $\\epsilon_\\theta$ is not. It is unclear how the operation K + $\\epsilon_t$ is defined - does it mean adding a constant offset to each predicted noise value?\n3. The contributions are not well isolated, making it difficult to discern which specific components of the method are responsible for the observed improvements.\n4. The paper relies on a somewhat outdated CLIP-based reward model. It raises concerns about generality — the method should also be validated with more recent state-of-the-art reward models. In particular, CLIP’s limitations (e.g., lack of support for non-square resolutions) may restrict the applicability of the proposed approach."}, "questions": {"value": "1. Could you provide ablation studies for the main design choices in your method? Paper does not include training dynamics and hyperparameters.\n\n2. Could you elaborate further on the proposed inversion-based regularization — particularly its formulation and impact on training stability?\n\n3. Isolating and evaluating each contribution separately would greatly strengthen the paper’s clarity and evidential value.\n\n4. Extending the experiments to include or adapt the method to state-of-the-art reward models would be highly beneficial for future research and community adoption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5gpS0jdRRk", "forum": "8qFOfUAMWI", "replyto": "8qFOfUAMWI", "signatures": ["ICLR.cc/2026/Conference/Submission16916/Reviewer_eat3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16916/Reviewer_eat3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835265056, "cdate": 1761835265056, "tmdate": 1762926942383, "mdate": 1762926942383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes two components:\n\n**Direct-Align:** skips the slow denoising by using interpolation between noise and the target image, which is faster and avoids over-optimisation.\n\n**SRPO:** updates rewards on the fly using text prompts, eliminating the need for endless offline tuning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "## Presentation: ~25th percentile\n\nStructure: The paper follows the most common structural pattern, which aligns with reader expectations.\n\n## Soundness: ~15th percentile\n\nThe soundness is supported by the reliability of CLIP.\n\n## Contribution: 20th~50th percentile\n\nThe methodology, using a CLIP inner product as a reward, was new to me, but not particularly surprising.\n\n## Note\nI hope the AC is aware that the rating is calibrated using percentiles to reduce evaluation noise effectively."}, "weaknesses": {"value": "I don’t think this paper is ready for publication.\n\n## Presentation\nThere are significant issues in your writing.\n\n* In your abstract, line 27, “by over 3x”, what is the reference point like, in what unit? I list this issue as critical because it is presented in your abstract.\n* In Section 3.1, you introduce several mathematical notations without explanation, while I have difficulty corresponding them to your text, such as $\\alpha_t, \\sigma_t, \\Delta \\sigma_t, \\lambda(t) $. I’m thus having difficulty understanding one of your important equations, Eq. (5). Although the parameter $\\alpha_t$ is trivial for readers who are familiar with diffusion models, you ought to either write about them whenever you introduce them (I do not even recommend putting it in the appendix as it breaks the reader’s flow) or explain them with an intuitive alternative presentation.\n* My flow of reading the paper was occasionally interrupted by unimportant details presented in the text, particularly when I was reading your introduction and experiment. You could have simplified these details and made room for your mathematical explanation. For example, suggest the most significant work related to each idea in your introduction and leave the others in the related work. The introduction section should be literally the introduction.\n\n## Soundness\n1. It’s known that CLIP-based models are CLIP-blinded, e.g., [1], meaning they are imperfect for less text-expressible tasks. The paper could have explored DINOv2, an embedding model purely based on images, but the comparison, which could have strengthened this paper's contribution, is also missing.\n2. I remain doubtful about the effectiveness of this method. It does not completely eliminate the null hypothesis:\n  * In Table 1, the standard deviation is missing; to my empirical experience, the standard deviation is roughly Aes: $10^{-0.4},$ Pick: $10^1,$ IR: $10^{0.8},$ HPS: $10^{-1.5}$. I recall that algorithms like DiffusionDPO and DDPO can easily surpass one standard deviation, but such a baseline is not included. \n  * Most of the quantitative results in your main paper, other than Table 1, are based on subjective tests. I keep doubting these tests because 1. they are not reproducible on the same group of people, 2. it’s not very clear about the distribution of image choice, 3. human evaluations are loudly noisy. I would rather believe in the number written as a statistical fact, extensively evaluated on visual langauge models, which are biased but sufficiently reliable compared to humans. \n\n## Contribution:\n\nThe idea is new to me, but not very surprising, given that replacing GRPO with $f_{\\mathrm{img}} (\\mathbf{x})^\\top \\mathrm{C} $. A potential advantage over GRPO is that it enforces linearity, but the potential exploitation I had expected was missing in the paper. \n\nThe image embedding can be free from text-based; you could consider introducing DINOv2 for deeper contribution. \n\n[1] Adejumo, Abduljaleel, et al. \"A Vision Centric Remote Sensing Benchmark.\n\n\n\n\nBecause I was unable to understand one of your core components, Eq. (5), I rate your contribution at the lower bound 20th percentile of my estimation range, but I fairly lower my confidence to 3. \n\nThe final rating was simply the mean of the estimated percentile."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kwK5DoFbGE", "forum": "8qFOfUAMWI", "replyto": "8qFOfUAMWI", "signatures": ["ICLR.cc/2026/Conference/Submission16916/Reviewer_pJaA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16916/Reviewer_pJaA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896175506, "cdate": 1761896175506, "tmdate": 1762926942041, "mdate": 1762926942041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}