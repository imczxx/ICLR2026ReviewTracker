{"id": "p0gxvlUoZM", "number": 21440, "cdate": 1758317610595, "mdate": 1759896921868, "content": {"title": "LLM-guided Hierarchical Retrieval", "abstract": "Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM based IR has shown great promise, the current retrieve-then-rerank paradigm inherits the limits of embedding-based retrieval, parametric generative approaches are difficult to adapt to new information, and long-in-context approaches that put the entire corpus in context are computationally infeasible for large document corpora due to the quadratic attention complexity. To this end, we introduce a hierarchical retrieval framework LATTICE that enables an LLM to reason and navigate a large corpus with logarithmic search complexity in the number of documents, achieved by imposing a semantic tree structure on the corpus.\nOur approach comprises two stages: (1) an offline process where we organize the document collection into a semantic hierarchy - we explore two LLM-driven strategies for this: a bottom-up agglomerative approach and a top-down divisive approach using multi-level summaries;  (2) an online traversal stage where a \"search LLM\" navigates this tree. A central challenge in using LLMs for search is that the LLM's relevance judgments are *noisy, context-dependent, and unaware of the underlying hierarchy*, making it difficult to compare nodes across different branches and levels of the tree. To solve this, our traversal algorithm estimates calibrated latent relevance scores from the LLM's local outputs, which are combined into a path relevance metric to guide the search globally across the tree. Our training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark (with up to 420K corpus size), demonstrating improvements of up to 9% in Recall@100 and 5% in nDCG@10. Moreover, compared to the highly specialized and fine-tuned SOTA method DIVER-v2, it achieves comparable results on BRIGHT subsets that use a static corpus for evaluation.", "tldr": "LLMs to retrieve from large corpora by having them navigate a semantic tree of the content", "keywords": ["information retrieval", "llm", "ranking", "efficient inference"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0565af84b1d74eae4de4cc38836911294a0ac1d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents LATTICE, a training-free hierarchical retrieval framework designed to help large language models (LLMs) efficiently search large document corpora with logarithmic search complexity. The authors aim to overcome key limitations in current LLM-based information retrieval (IR) systems—namely, the bottlenecks in retrieve-then-rerank pipelines and the scalability challenges of long-context methods.\nLATTICE operates in two stages: (1) Offline Stage – Documents are organized into a semantic tree through either a bottom-up or top-down LLM-driven clustering strategy. (2) Online Stage – A search LLM navigates this tree using a traversal algorithm that estimates calibrated latent relevance scores from inherently noisy and context-dependent LLM judgments. These scores are aggregated into a path relevance metric that guides global search decisions.\nEmpirical results show that LATTICE achieves state-of-the-art zero-shot performance on the reasoning-heavy BRIGHT benchmark, outperforming strong baselines in Recall@100 and nDCG@10, and performing comparably to heavily fine-tuned systems on static corpus subsets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed tree search algorithm and offline tree construction methods are sound and empirically validated.\n2. The paper provides comprehensive analyses illustrating the advantages and mechanisms of the approach.\n3. The writing is clear and well-organized."}, "weaknesses": {"value": "1. The proposed solution appears to perform well only under high online LLM budget settings. As shown in Figure 3, performance drops notably when the budget is low, falling behind the baselines.\n2. As shown in Table 3, performance is highly sensitive to the choice of offline tree construction strategy (bottom-up vs. top-down). This suggests that selecting the right strategy requires prior knowledge of the corpus structure, which may not always be available in practice.\n3. The experiments are only conducted on the Gemini-2.5 family. It would be helpful to see results on other or smaller open-source models for broader validation.\n4. The model comparisons may not be entirely fair, since the proposed method uses Gemini-2.5-flash at all stages, while the baselines rely on GPT-4-based query expansion.\n5. The evaluation is limited to the BRIGHT dataset. It would be useful to see how the method performs on other retrieval datasets.\n6. Compared to other approaches that perform offline indexing on BRIGHT—such as\n*Imagine All The Relevance: Scenario-Profiled Indexing with Knowledge Expansion for Dense Retrieval (Lee et al.)* and \n*EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline (Chen et al.)*\n—the previously built offline tree structure needs to be reclustered or redivided whenever new documents are added, which could lead to high maintenance costs over time."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v8m3fuf29q", "forum": "p0gxvlUoZM", "replyto": "p0gxvlUoZM", "signatures": ["ICLR.cc/2026/Conference/Submission21440/Reviewer_x4dg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21440/Reviewer_x4dg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953052237, "cdate": 1761953052237, "tmdate": 1762941776920, "mdate": 1762941776920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of using LLMs for complex information retrieval over large corpora, where existing methods are either limited by embeddings, difficult to update, or computationally infeasible. \nThe authors propose LATTICE, a hierarchical retrieval framework that structures a document corpus into a semantic tree. An LLM then navigates this tree  to guide the search. \nThe experiments are employed on the BRIGHT benchmark, demonstrating good improvements in recall and nDCG compared with baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a good LLM-guided hierarchical information retrieval framework. It designs multiple strategies to construct the original corpus into a hierarchical tree during the offline stage, and carefully designs navigation strategies for the online stage.  \n2. The paper is  readable, making it easy for readers to understand the authors' motivation and methodology. The complex strategies in the online stage are explained through formulas and pseudocode.  \n3. Experiments demonstrate that the proposed method achieves better performance under a zero-shot setting compared to baseline models."}, "weaknesses": {"value": "1. The paper lacks obvious innovation, as previously published works have already explored similar ideas[1]: transforming large-scale knowledge corpora into hierarchical trees and designing navigation strategies for traversal and filtering. \n2. The paper only employs the BRIGHT benchmark for experiments. Although this benchmark contains multiple subsets, these subsets were all constructed by the same research team, resulting in a uniform pattern across them. This raises concerns about the generalizability of the proposed method.\n3. Compared to the fine-tuned DIVER v2, the method proposed in this paper does not demonstrate a clear performance advantage. Although the proposed approach requires no training cost, it incurs offline construction costs. The paper fails to provide a clear comparison between the offline construction cost of their method and the training cost of fine-tuned DIVER v2. This leads to doubts about whether the proposed method achieves only marginal performance gains at a potentially higher overall cost.\n\n[1] Hierarchical Document Refinement for Long-context Retrieval-augmented Generation. ACL 2025"}, "questions": {"value": "1. What is the core innovation of the proposed method compared to previous work [1] ?\n2. Which one is higher: the offline construction cost of this method or the training cost of fine-tuned DIVER v2?\n3. The offline construction phase uses Gemini-2.5-flash. Have other large language models been tried, and how much would using different models impact the final results?\n\n[1] Hierarchical Document Refinement for Long-context Retrieval-augmented Generation. ACL 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bwPsJNDmt8", "forum": "p0gxvlUoZM", "replyto": "p0gxvlUoZM", "signatures": ["ICLR.cc/2026/Conference/Submission21440/Reviewer_3E2f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21440/Reviewer_3E2f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070296101, "cdate": 1762070296101, "tmdate": 1762941776335, "mdate": 1762941776335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LATTICE, a hierarchical retrieval framework that enables large language models (LLMs) to perform reasoning-driven search over large document corpora with logarithmic complexity. It organizes the corpus into a semantic tree structure using either a bottom-up agglomerative or top-down divisive strategy, and employs an LLM-guided traversal algorithm that estimates calibrated relevance scores to navigate the hierarchy effectively. The framework is training-free and demonstrates strong zero-shot retrieval performance on the reasoning-intensive benchmark BRIGHT."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper presents a hierarchical retrieval framework that integrates LLM reasoning with structured corpus organization.\n2) The proposed method is clearly motivated and effectively described.\n3) The framework shows potential for improving reasoning-oriented retrieval."}, "weaknesses": {"value": "1) The approach has not been validated on large-scale, open-domain corpora, leaving its scalability and generalization uncertain.\n2) The discussion of related work is incomplete, omitting several recent advances in hierarchical and structure-aware retrieval.\n3) The paper lacks analysis of efficiency and computational cost during both tree construction and traversal stages.\n4) The presentation could be improved. The paper introduces the search process before explaining tree construction and lacks a concluding section summarizing key insights and limitations.\n5) The evaluation relies solely on the proprietary Gemini-2.5-flash model and a single reasoning-intensive benchmark, limiting the understanding of model dependence and robustness across different retrieval settings."}, "questions": {"value": "1) How does the semantic hierarchy scale in both construction and traversal time when applied to large-scale, open-domain corpora (e.g., millions of documents)?\n2) Beyond BRIGHT, have the authors evaluated LATTICE on more general retrieval datasets such as MS MARCO or Natural Questions to assess robustness and generalization?\n3) How do smaller or open-source LLMs perform within this framework? Is the approach dependent on the reasoning strength of proprietary models like Gemini-2.5-flash?\n4) Could the authors provide a quantitative or qualitative comparison of the bottom-up vs. top-down semantic tree construction strategies? In what data conditions should one prefer either method?\n5) Since traversal efficiency is central to the claim of logarithmic complexity, can the authors provide empirical runtime comparisons against standard reranking pipelines?\n6) How is the semantic hierarchy updated when new documents are added? Does the model require full reconstruction, or can it support incremental updates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xcqJW30kyR", "forum": "p0gxvlUoZM", "replyto": "p0gxvlUoZM", "signatures": ["ICLR.cc/2026/Conference/Submission21440/Reviewer_m8ts"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21440/Reviewer_m8ts"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071455796, "cdate": 1762071455796, "tmdate": 1762941775692, "mdate": 1762941775692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LATTICE, an LLM-guided hierarchical retrieval framework that organizes a corpus into a semantic tree offline and lets a “search LLM” traverse it online using calibrated path-relevance scores. On BRIGHT, it achieves strong zero-shot results and favorable cost–quality scaling compared to retrieve-then-rerank, with clear ablations on calibration and traversal design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is well-motivated and novel. The latent-score calibration and path-relevance update are interesting and reasonable.\n\n2. Strong empirical results and thoughtful analysis.\n\n3. Under larger token budgets, the method scales better than reranking."}, "weaknesses": {"value": "1. Offline tree construction is expensive and appears data-sensitive (e.g., Table 3). Maintaining the tree for dynamic corpus (add/edit/delete) is nontrivial, as internal summaries can become stale. These issues may hinder real-world adoption.\n\n2. This paper could benefit from more comparisons to agentic methods. The argument that “agents call a retrieval tool while LATTICE is the core retrieval mechanism” is not fully convincing. Both approaches rely on text embeddings but mainly in that LATTICE pre-clusters and has the LLM walk over clustered tree anchors, whereas an agent can pick an anchor (the query embedding) and check neighboring documents.  I believe more in-depth comparisons would help.\n\n3. It is unclear how corpus size affects performance (both tree construction and search under a given budget). The BRIGHT corpus is relatively artificial and small, comparisons on larger datasets (eg, BEIR) would be helpful."}, "questions": {"value": "1. How LATTICE maintains the diversity of retrieved results"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pBKnrT19E4", "forum": "p0gxvlUoZM", "replyto": "p0gxvlUoZM", "signatures": ["ICLR.cc/2026/Conference/Submission21440/Reviewer_Qcbe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21440/Reviewer_Qcbe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762764221359, "cdate": 1762764221359, "tmdate": 1762941774978, "mdate": 1762941774978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}