{"id": "RppsCECZvP", "number": 8222, "cdate": 1758074899389, "mdate": 1759897798686, "content": {"title": "Don’t Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compute on negative groups: groups in which no sampled response is correct yield zero advantage and thus no gradient. We ask whether negative groups can be leveraged without extra supervision. Starting from a maximum-likelihood (MLE) objective in reward modeling, we show that the MLE gradient is equivalent to a policy gradient for a modified value function. This value function adds a confidence-weighted penalty on incorrect responses, imposing larger penalties on more confident mistakes. We refer to this as **L**ikelihood **E**stimation with **N**egative **S**amples (**LENS**). LENS modifies GRPO to assign non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative and converting previously wasted samples into useful gradient updates. On the MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently outperforms GRPO baseline, with significant gains on harder items. These results demonstrate a principled and practical way to “rescue” negative groups, improving efficiency and performance in RLVR.", "tldr": "We theoretically connect reward modeling and RLVR through a likelihood lens, providing a principled framework to better leverage GRPO’s negative groups via confidence reweighting.", "keywords": ["Reinforcement Learning", "RL with verifiable reward", "GRPO", "negative groups", "theory", "reward modeling"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e2fbde8997feb0daab28aa8f37cdc31abfa4bb4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses sample inefficiencies in RLVR for reasoning tasks, particularly the GRPO framework. In GRPO, all positive and all negative groups yield zero gradients and thus waste compute.\n\nThe authors propose LENS (Likelihood Estimation with Negative Samples), which connects MLE in reward modeling with policy gradient methods. By reinterpreting MLE gradients, they derive a confidence-weighted penalty for incorrect generations: the more confident the model is in a wrong answer, the larger the penalty. This converts previously uninformative negative groups into useful training signals.\n\nLENS is implemented as a drop-in modification to GRPO, introducing nonzero, confidence-dependent rewards for incorrect responses with minimal computational overhead. Experiments on MATH with LLaMA-3.1-8B and Qwen-2.5-3B show consistent performance gains across all Pass@k metrics, especially on harder problems (Levels 4–5).\n\nOverall, the paper is good on the theoretical side and could be further improved on the experimental side. I hence evaluate it as a borderline reject at this stage. I would be happy to increase my evaluation if the experimental side is improved in the next version."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear motivation and practical relevance.** The inefficiency of negative groups in GRPO is a well-recognized issue, and LENS directly tackles this without additional supervision.\n\n2. **Theoretical derivation connecting MLE and policy gradients.** The reparameterization and gradient equivalence analysis (Eq. 8–10, Theorem 1) are elegant and make the proposed modification principled rather than heuristic.\n\n3. **Simple and easily adoptable algorithm.** The proposed confidence-based correction term integrates naturally into existing GRPO setups (and potentially, many other variants), making it practical for real-world RLVR pipelines.\n\n4. **The paper is well written and easy to follow.** Figures (especially Fig. 1 and Fig. 4) effectively illustrate how negative samples are “rescued” and how the modified rewards reshape the learning dynamics."}, "weaknesses": {"value": "1. **Limited empirical scope.**\nExperiments are restricted to MATH reasoning; no evidence is given for generalization to other RLVR tasks (e.g., code or symbolic reasoning) or at least, on other commonly used math datasets (e.g., AIME, AMC, Minerva, etc.). The claim of generality would be stronger with broader benchmarks.\n\n2. **Absence of ablation and sensitivity studies, simple baseline.**\nIt’s unclear how sensitive performance is to the choice of alpha. In other words, do the benefits mainly come from the mixed group or the negative groups? Also, no comparison is made to simple baselines that also take advantage of the negative examples. (The paper mentioned some, but does not compare them.)"}, "questions": {"value": "1. Is the proposed reward shaping applied at the response level or the token level? In other words, do all tokens within a single response share the same effective learning signal (or equivalent learning rate), or is the penalty distributed across tokens differently?\nThis distinction is important because prior work [1] has shown that in a “negative” response, most intermediate reasoning tokens may still be correct, with only the final token being wrong. Penalizing the entire sequence uniformly might therefore suppress useful intermediate reasoning behaviors. Could the authors clarify how LENS handles this situation, specifically, whether it can assign token-wise negative gradients more selectively or whether the entire response receives a uniform penalty?\n\n2. LENS takes good use of the all-negative groups, what about the all-positive group? Can the methodbe  extended to this case?\n\n[1] Deng, Wenlong, et al. \"On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization.\" *arXiv preprint arXiv:2505.18830* (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w8i4Cln1Zt", "forum": "RppsCECZvP", "replyto": "RppsCECZvP", "signatures": ["ICLR.cc/2026/Conference/Submission8222/Reviewer_y75z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8222/Reviewer_y75z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760768117003, "cdate": 1760768117003, "tmdate": 1762920170814, "mdate": 1762920170814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LENS, a modification to GRPO that introduces a confidence-dependent penalty for incorrect responses derived from an MLE reformulation. The authors argue that GRPO “wastes” negative groups (groups where all responses are incorrect) because their advantages collapse to zero. LENS uses the model's predicted probabilities to build a calibrated reward that assigns non-zero penalties for wrong answers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses a real inefficiency in GRPO: lack of signal in all-negative groups.\n\n2. Attempts to reframe reward modeling and RLVR through an MLE lens."}, "weaknesses": {"value": "## Weaknesses\n\n1. **Limited Experimental Scope.**  \n   The empirical evaluation covers only the MATH dataset and two model sizes. This limited scope is insufficient to support claims of generality or broad applicability across reasoning tasks or model scales.\n\n2. **Unrealistic Assumptions About the Output Space.**  \n   The theoretical formulation assumes a finite, enumerable answer space. In LLM reasoning, however, the number of textual realizations of the same correct answer (e.g., *“The answer is 42.”*, *“It is 42.”*, *“42.”*, etc.) is effectively unbounded. Consequently, \\(D(q)\\) may be infinite or undefined, making the proposed reparameterization inapplicable to real generative settings.\n\n3. **Breakdown of the MLE/IS Derivation for Negative Groups.**  \n   I was enjoying the MLE and importance-sampling derivation, but the analysis breaks down abruptly for negative groups (where all \\( r_i = 0 \\)). Instead of providing a principled extension of the theory for difficulty measure (such as mixed group), the paper introduces an ad-hoc fallback  \n   \\[\n   D(q) = 2 \\cdot \\max_i \\pi_{\\text{old}}(o_i \\mid q),\n   \\]  \n   which has no theoretical justification. This weakens the claimed theoretical connection between reward modeling and policy optimization, especially in emphasizing resolving the learning signal from the incorrect group.\n\n4. **Only Marginal Gains in Mixed Groups.**  \n   Appendix Table 2 shows that LENS provides only minor improvements on mixed groups. This suggests that most of the observed empirical gains stem from the heuristic handling of all-negative groups, rather than from the theoretically grounded likelihood-based calibration intended for mixed groups, raising a concern about the validity of the theoretical framework.\n\n5. **Missing Comparisons to Strong Negative-Reward Baselines.**  \n\n   The paper does not compare LENS against simpler, well-established negative reinforcement strategies (e.g., penalizing all incorrect responses in all-negative groups). Prior work [1] demonstrates the strength of this method. Without these comparisons, it is unclear whether LENS provides meaningful advantages over existing methods.\n\n[1] The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning,"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iFbMSTj1l7", "forum": "RppsCECZvP", "replyto": "RppsCECZvP", "signatures": ["ICLR.cc/2026/Conference/Submission8222/Reviewer_bfBj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8222/Reviewer_bfBj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802419362, "cdate": 1761802419362, "tmdate": 1762920170062, "mdate": 1762920170062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets RLVR training with GRPO, arguing that “negative groups” (all sampled answers wrong) are wasted because GRPO assigns them zero advantage. The authors derive a calibrated policy gradient whose reward for incorrect samples is down-weighted by model confidence and an instance-level “difficulty” term. Their modified reward yields non-zero, confidence-dependent signals for wrong answers and they implement can simply swap GRPO’s reward, using length-normalized probabilities and a simple estimator for difficulty term. They claim negligible overhead and better Pass@k on MATH with Llama-3.1-8B and Qwen-2.5-3B, especially on harder problems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a intuitive and significant inefficiency in a widely used algorithm. The concept of \"wasted mistakes\" is clearly state and the paper provides strong, data-driven motivation in Figure 2. 1 is an excellent framing of the problem. \n\n- The authors provide theoretical insights to conceptually bridge from a reward modeling MLE objective to a policy gradient form, and the theoretical results provides a solid starting point for the subsequent algorithmic development."}, "weaknesses": {"value": "- While the theoretical results are interesting, their connection to the proposed method feels weak. Some aspects of the design appear to be heuristic and lack clear justification (see details in the questions).\n\n- The evaluation overlooks several important baselines, and some claims made in the paper are not adequately supported by evidence (see details in the questions)."}, "questions": {"value": "- Concurrent work (Xiong et al. 2025, arXiv:2504.11343) provides empirical evidence that discarding negative groups is GRPO's main advantage. Can the authors comment on this fundamental contradiction and justify why their premise holds in light of this conflicting evidence?\n\n- The key difference between LENS and cited work (Zhu et al. 2025) is the stratification of negative rewards (confidence-weighted vs. uniform). Why was a uniform negative reward baseline (i.e., NSR from Zhu et al.) omitted from the ablation study in Table 2? \n\n- Section 5 introduces at least four major heuristics (length-norm, a complex $D(q)$ estimator, $1/G$ scaling, and an $\\alpha=0.25$ hyperparameter) that do not appear in the \" derivation in Section 4. Does the theoretical reward from Eq. 10 work in practice? If not, doesn't this imply the theoretical framework is a loose inspiration rather than a principled derivation?\n\n- The paper claims “negligible computational overhead”. Can the authors please provide quantitative data?\n\n- Can the authors provide a theoretical justification for the number 2 in the $D(q)$ estimator 1 (line 370) and explain why the estimator must be defined differently for mixed vs. negative groups?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o2LoPEy2wf", "forum": "RppsCECZvP", "replyto": "RppsCECZvP", "signatures": ["ICLR.cc/2026/Conference/Submission8222/Reviewer_285B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8222/Reviewer_285B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052178374, "cdate": 1762052178374, "tmdate": 1762920167892, "mdate": 1762920167892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on solving the issue of original GRPO objective that the negative groups contribute no gradient to the update. The authors propose a method called Likelihood Estimation with Negative Samples (LENS), which assigns non-zero advantage to responses within negative groups based on a confidence-weighted penalty. Experiments on two models verify the effectiveness of the proposed GRPO-variant objective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a conceptually clear connection between reward modeling and policy learning.\n- The derivations are rigorous, with clear assumptions and logical consistency."}, "weaknesses": {"value": "- Can the authors prove that results in Table 1 are statistically significant (at least 2-sigma or 95% confidence level deviations)?\n- Only one benchmark is evaluated in this paper, how does the methods generalize to more benchmarks (e.g., AIME, AMC, etc.)?"}, "questions": {"value": "1. The proposed method modifies the GRPO objective by introducing a confidence penalty to the negative groups, I'm wondering how this connects with the entropy/confidence-related methods like [1][2][3].\n2. [4] shows that simply assigning -1 rewards to negative responses and dropping the correct ones can improve LLM reasoning, what if the proposed objective in this paper is applied to negative-only groups and dropping other samples? Will this be better than simply assigning -1 rewards?\n\n[1] Learning to Reason without External Rewards\n\n[2] Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning\n\n[3] Maximizing Confidence Alone Improves Reasoning\n\n[4] The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R6Rae5jYZR", "forum": "RppsCECZvP", "replyto": "RppsCECZvP", "signatures": ["ICLR.cc/2026/Conference/Submission8222/Reviewer_npA8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8222/Reviewer_npA8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762664557298, "cdate": 1762664557298, "tmdate": 1762920163499, "mdate": 1762920163499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}