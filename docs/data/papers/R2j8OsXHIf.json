{"id": "R2j8OsXHIf", "number": 9093, "cdate": 1758110518340, "mdate": 1759897743806, "content": {"title": "SG2Loc: Sequential Visual Localization on 3D Scene Graphs", "abstract": "Visual localization in complex environments remains a critical challenge for robotics and AR applications. Sequential localization, where pose estimates are refined over time, is important for autonomous agents. However, traditional methods often require storing extensive image databases or point clouds, leading to significant storage overhead. This paper introduces a novel, lightweight approach to sequential visual localization using 3D scene graphs. Our method represents the environment with a compact scene graph, where nodes represent objects (with coarse meshes) and edges encode spatial relationships. For each image in the localization phase, we extract per-patch semantic features, predicting object identities. Localization is performed within a particle filter framework. Each particle, representing a camera pose, projects the coarse object meshes from the scene graph into the image, assigning object identities to patches based on visibility. The similarity of the per-patch features, in the input image, and object features from the scene graph determines the weight of a particle. Subsequent images are incorporated sequentially, refining the pose estimate. By leveraging a compact scene graph and efficient semantic matching, our method significantly reduces storage while maintaining performance on real-world datasets. The code will be made public.", "tldr": "", "keywords": ["Visual localization", "sequential localization", "3D scene graphs"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d370a1e9c76fed4e34377b6310a603fb4a85523.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method for sequential visual localization based on compact 3D scene graphs. The approach models environments as graphs, where nodes represent coarse object meshes and edges encode spatial relations. During localization, semantic features extracted from image patches are compared with projected object identities within a particle filter framework to estimate camera poses. The method is evaluated on real-world datasets and reports comparable localization accuracy while reducing storage requirements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly structured and generally easy to follow. The idea of leveraging a compact scene graph for sequential localization is interesting, and the paper demonstrates careful engineering and integration of existing components."}, "weaknesses": {"value": "1. Motivation and scope.\n\nThe motivation of the paper is not clearly aligned with the proposed approach. The authors argue that existing scene coordinate regression (SCR) and absolute pose regression (APR) methods struggle in complex, large-scale environments (line 91, page 2), yet the experiments are limited to small, static indoor datasets such as ScanNet and 3RScan, which do not convincingly demonstrate the claimed advantages. In contrast, prior works—including SCR and APR methods—have been evaluated on more challenging benchmarks such as Cambridge Landmarks and Aachen-Day-Night, which involve illumination changes, dynamic scenes, and weather variations. In addition, according to Table 3, the proposed approach does not show a clear advantage over SCR methods in terms of storage. It is also worth noting that many existing visual localization approaches operate without explicit gravity alignment or auxiliary sensors, so the benefits of the proposed method are not fully clear.\n\n2. Missing Related Work and Limited Novelty.\n\nThe technical novelty of this paper over SceneGraphLoc appears limited, as the proposed framework mainly extends it to handle image sequences. Furthermore, the paper omits discussion and comparison with several relevant recent works. The statement (line 92, page 2) that the method avoids storing image databases or point clouds is not a significant advantage, since many modern methods—such as SCR, APR approaches—already share this property. Moreover, these existing methods typically support full 6-DoF pose estimation, whereas the proposed method appears to handle only 4-DoF localization.\n\nIn addition, comparisons with more recent and representative baselines are missing. For instance, GLACE [1] and R-SCoRe [2] have demonstrated strong results on large-scale benchmarks (e.g., Cambridge, Aachen, Hyundai Department Store), and differentiable representation-based GS-CPR [3] achieves efficient pose refinement without image databases and point clouds. Including such baselines would make the experimental evaluation more convincing and better clarify the contribution of this work.\n\n[1] GLACE: Global Local Accelerated Coordinate Encoding, CVPR 2024\n\n[2] R-SCoRe: Revisiting Scene Coordinate Regression for Robust Large-Scale Visual Localization, CVPR 2025\n\n[3] GS-CPR: Efficient Camera Pose Refinement via 3D Gaussian Splatting, ICLR 2025"}, "questions": {"value": "If the authors believe that existing SCR or APR methods perform poorly on large and complex scenes, I strongly encourage including at least two larger and more challenging benchmarks—such as Cambridge Landmarks, Hyundai Department Store, or Aachen-Day-Night—and comparing against GS-CPR, GLACE, and R-SCoRe.\n\nIf the key contribution is improved storage efficiency or the avoidance of image/point cloud databases, please provide more thorough comparisons with recent differentiable-representation-based methods and discuss these aspects more explicitly in the related work and experiments.\n\nWhy does this paper not provide the recommended Reproducibility statement and the Use of Large Language Models (LLMs) statement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7fpls3jpy7", "forum": "R2j8OsXHIf", "replyto": "R2j8OsXHIf", "signatures": ["ICLR.cc/2026/Conference/Submission9093/Reviewer_2rVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9093/Reviewer_2rVs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760867406206, "cdate": 1760867406206, "tmdate": 1762920796606, "mdate": 1762920796606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a lightweight visual localization method for sequential image inputs, utilizing compact 3D scene graphs as the scene representation.\nIn a 3D scene graph, each node corresponds to an object instance, with each instance characterized by a set of multi-modal attributes including RGB frames, a point cloud, textual annotation and a coarse 3D mesh.\n\nThe authors formulate the camera localization as a particle filtering problem, where particle weights are determined by combining semantic similarity, the depth and color scores between the query image and the map projection. \nThe proposed approach allows for pose searching and optimization over multiple iterations, with further refinement achieved through *RANSAC-PnP*. \n\nExperimental results on two public datasets demonstrate that the method achieves competitive localization performance while significantly reducing storage requirements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The concept of leveraging 3D semantic scene graphs for camera relocalization is innovative and well-justified. By compactly integrating high-level semantic relationships with geometric representations, they provide a sufficiently informative foundation for accurate localization.\n- The task is formulated as an iterative particle filtering problem, where particle weights are computed through multi-modal comparisons between the query and the rendered representations (semantic, depth, and color maps)."}, "weaknesses": {"value": "1. The core system is conceptually simple: a standard particle filter in $SE(3)$ with a similarity-based observation model. Particle filtering for robot localization has been extensively studied, including with visual or learned models.  Although simplicity is not a drawback in itself, I'm questioning the suitability of particle filtering for this specific task: \n\t1. The framework relies on a considerable number of hyper-parameters, especially the initial particle sampling strategy, which uses four predefined heights, appears quite contrived to me.\n\t2. The multi-round particle optimization and ray tracing operations are computationally expensive, potentially limiting the method's adoption in real-time or resource-constrained applications\n2. The evaluation may be not entirely fair and convincing. While the method is designed for sequential input, it is only compared against baselines targeting per-frame localization. Besides, the direct use of ground-truth poses provided by the datasets as ego-motion raises another major concern on its practical performance.\n3. It is suggested to provide more details on how these baselines are adapted and implemented, as the statistics reported are questionable - particularly in `Tab.1`, for *HLoc* and *MeshLoc* that rely on image retrieval, the average position errors seem unexpectedly large.\n4. The claimed low-memory benefit and performance gain are not sufficiently compelling. Established scene coordinate regression methods, like *ACE*, *GLACE* and *R-SCoRe*, demonstrate similar or even better savings while achieving  top performance within minutes—and crucially, *without relying on additional priors such as depth, meshes, or annotations*."}, "questions": {"value": "1. I remain concerned about the unexpectedly large average pose errors reported for *HLoc* and *MeshLoc*. The authors should verify their implementation. It is also recommended to constrain the final pose estimates within the scene's bounding box as a straightforward sanity check.\n2. The selection criteria for a fix-length of sequential inputs remains unclear to me. For instance, it is not specified whether frame downsampling is applied to form the query sequence, or how frames in a sequence are chosen. This pre-processing step should be clearly detailed.\n3. The results in `Tab.8` indicate that semantic signals are the most contributive factor, which is not fully straightforward to me, since intuitively depth cues are more important."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AuH4CTEnx9", "forum": "R2j8OsXHIf", "replyto": "R2j8OsXHIf", "signatures": ["ICLR.cc/2026/Conference/Submission9093/Reviewer_sH4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9093/Reviewer_sH4f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794612400, "cdate": 1761794612400, "tmdate": 1762920796161, "mdate": 1762920796161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SG2Loc, a method for sequential visual localization that relies on 3D scene graphs instead of traditional dense 3D maps or large image databases.\n\nThe key idea is to represent an environment as a compact graph of object nodes and spatial relationships, where each node includes a coarse mesh and a semantic embedding.\n\nLocalization is formulated as a particle filtering problem that refines the camera pose over time.\nThe method achieves comparable accuracy to classical approaches such as HLoc and MeshLoc while requiring 10× less storage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall, the paper is well-written and clearly structured.\n\n2. The work extends the use of scene graphs in visual localization to the sequential setting, which is a meaningful and natural progression for this line of research.\n\n3. While the use of a particle filter is not new, the paper integrates it effectively with semantic and geometric cues from scene graphs, resulting in an effective approach.\n\n4. The method is compared against well-known baselines and achieves comparable performance while requiring less storage (with the exception of ACE).\n\n5. The authors include thorough ablation studies that help clarify the impact of key design choices."}, "weaknesses": {"value": "1. The contribution is primarily an integration of existing components, scene graphs, particle filtering, and standard similarity measures, rather than a fundamentally novel idea.\n\n2. The method relies on pre-built, labeled 3D scene graphs with coarse object meshes, but the paper does not discuss how such graphs are generated.\n\n3. The system estimates only 4 degrees of freedom (assuming known gravity direction), making it unsuitable for general 6-DoF localization and resulting in an unfair comparison with fully 6-DoF baselines such as HLoc and ACE.\n\n4. The approach is considerably slower than retrieval-based methods.\n\n5. Experiments are conducted only on indoor datasets, leaving their performance in outdoor scenes unexplored.\n\n6. While the paper emphasizes low storage requirements, it does not discuss or compare to other memory-efficient visual localization methods such as SceneSqueezer [A] and [B].\n\n-[A] Scenesqueezer: Learning to compress scene for camera relocalization. CVPR 2022. \n\n-[B] Differentiable product quantization for memory efficient camera relocalization. ECCV 2024."}, "questions": {"value": "1. Regarding W3, can you please show an experiment where you estimate the gravity direction using GeoCalib (Veicht et al., 2024)?\n\n2. Please clarify how you construct the scene graph for each scene.\n\n3. Please discuss/compare the memory-efficient methods mentioned in W6.\n\n4. Is it possible to show a small experiment on an outdoor scene?\n\nminor: In line 199, there is a small mistake: a 14×14 grid results in 196 patches, not 144 as mentioned."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UKVdZc3Rxs", "forum": "R2j8OsXHIf", "replyto": "R2j8OsXHIf", "signatures": ["ICLR.cc/2026/Conference/Submission9093/Reviewer_dsUg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9093/Reviewer_dsUg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961868961, "cdate": 1761961868961, "tmdate": 1762920795609, "mdate": 1762920795609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SG2Loc, a sequential visual localization framework that utilizes 3D scene graphs as the underlying map representation. Unlike prior image- or point-cloud-based approaches, SG2Loc employs a particle filter operating on a 4-DoF state space, where each particle’s observation likelihood is computed by comparing ray-casted object predictions against semantic patch embeddings derived from a SceneGraphLoc-style encoder. The method further integrates SSIM and depth consistency terms, adopts coarse-to-fine search and KLD-based adaptive resampling, and refines the final pose using RoMa + PnP. The main claim is that SG2Loc achieves competitive localization accuracy with a dramatically smaller storage footprint, demonstrating the viability of semantic scene-graph maps for sequential localization tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper makes a clear and logical extension of SceneGraphLoc from single-frame reasoning to sequential probabilistic localization. The integration of semantic, photometric, and geometric cues in a unified particle-filter framework is well motivated and technically sound.\n\nS2. The overall system is well-structured, including motion prediction, adaptive resampling, and pose refinement. The coarse-to-fine search is a sensible addition that improves robustness in practice.\n\nS3. Experimental results indicate that the method achieves accuracy comparable to strong baselines while using significantly less map storage, supporting its motivation for efficient localization."}, "weaknesses": {"value": "W1: The system mainly combines existing components, such as scene-graph-based embeddings,  particle filtering, SSIM/depth fusion, and RoMa+PnP refinement, without introducing a fundamentally new algorithmic contribution. The conceptual leap beyond SceneGraphLoc is relatively small.\n\nW2: The semantic likelihood assigns a high score only when the predicted object ID from ray-casting matches the predicted object ID from the image patch. This hard matching assumption ignores soft uncertainty in detection and segmentation, which could make the system brittle to misclassification, occlusion, or open-set objects. A more principled probabilistic treatment would have been preferable.\n\nW3: The paper combines semantic, photometric, and depth likelihoods, but does not specify how these are normalized or weighted. Without clear scale calibration or hyperparameter justification, the combined likelihood may behave unpredictably across scenes.\n\nW4. The transition model seems to rely on ego-motion estimation between frames, but details on how this motion is obtained are unclear."}, "questions": {"value": "Q1. How robust is SG2Loc to semantic segmentation errors or to dynamic scenes where object layouts change?\n\nQ2. How is ego-motion estimated between frames for the particle filter’s prediction step?\n\nQ3. How are the weights between semantic, photometric, and depth likelihoods tuned? Are they fixed, or learned?\n\nQ4. How sensitive is the performance to sequence length and particle count?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RxlrTfnOAQ", "forum": "R2j8OsXHIf", "replyto": "R2j8OsXHIf", "signatures": ["ICLR.cc/2026/Conference/Submission9093/Reviewer_M57c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9093/Reviewer_M57c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762321304330, "cdate": 1762321304330, "tmdate": 1762920794926, "mdate": 1762920794926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}