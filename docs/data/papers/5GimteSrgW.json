{"id": "5GimteSrgW", "number": 14881, "cdate": 1758245016576, "mdate": 1759897343738, "content": {"title": "CAR-LoRA: Training Compression-Aware and Robust LoRA Adapters for Evolving LLMs", "abstract": "The deployment of large language models (LLMs) for specialized tasks on resource-constrained edge devices like smartphones and sensors presents a significant scalability problem. To run on such hardware, these massive models must be compressed using techniques like \\emph{quantization or pruning} to reduce their memory and computational footprint. Concurrently, foundational LLMs are periodically updated by their developers with new data, making their $\\textit{internal parameters shift over time}$. While parameter-efficient methods like Low-Rank Adaptation (LoRA) streamline personalization by fine-tuning only a small fraction of parameters, the resulting adapters are $\\textbf{brittle}$; a LoRA trained for one specific compression scheme is incompatible with another, and an adapter trained on an older base model performs poorly on an updated one. This forces a costly cycle of retraining for each unique device and every new model release. To address this, we introduce a novel framework that creates a single, universally portable adapter that is both $\\textbf{\\textit{(i)} compression-aware and \\textit{(ii)} temporally robust}$. We achieve this by augmenting the training process with a variety of simulated compression techniques during a single run, utilizing a quantized forward pass to build resilience while maintaining a full-precision backward pass for stable gradient optimization. $\\textit{This method yields a unified adapter robust to diverse compression artifacts and the subtle parameter shifts from model evolution}$. Extensive experiments on models such as $\\texttt{Llama-2, Llama-3.1, Gemma-2}$, and $\\texttt{Mistral}$ across reasoning benchmarks like $\\textit{SQA, MATH, and GSM8K}$ demonstrate that our single adapter achieves performance comparable to specialized adapters ($\\textit{e.g.}$, QLoRA) that are individually retrained for each compression scheme. Furthermore, we show this single adapter maintains its high performance when applied to future, evolved versions of the base model, eliminating the need for periodic retraining. Our work pioneers an efficient paradigm for edge AI, creating portable model patches that bridge the gap between cloud-based personalization, the diverse hardware ecosystem, and the lifecycle of evolving LLMs.", "tldr": "", "keywords": ["Low Rank Adaptation", "Edge Devices", "Quantization", "Compression", "Efficient Fine-tuning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b38da80084118660f4b6936848152ae4214b351a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "During the deployment of LLMs, compression is often a necessity. However, customizations through LoRA face a problem in that they are trained to fit one specific compression scenario and must be retrained for other scenarios. In this work, CAR-LoRA is proposed as a unified training framework that produces a single adapter that is both compression-aware and temporally robust. It achieves robustness through a two-loop structure. In the outer loop, it simulates different compression scenarios to make the LoRA adapter aware of potential compressions during deployment. Results show competitive evaluations on benchmarks compared with qLoRA."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is intuitive and seems easy to apply to current models.\n- CAR-LoRA shows competitive results with existing qLoRA while only needing one training for all compression.\n- It's important and beneficial to see the training cost report, and CAR-LoRA does not seem to be more costly."}, "weaknesses": {"value": "- I'm not sure if this method makes the adapter generalize to \"all scenarios.\"\n\nCurrent ablations on the generalization of CAR-LoRA stop at 4-bit quantization. Does the conclusion in Sec 4.4 still hold if CAR-LoRA is tested on lower bits (2-bit or 3-bit)? We know that model weights need to be finetuned to a very distinct distribution if they were to be quantized to extremely low bits. I wonder if CAR-LoRA remains robust under those scenarios.\n\n- Needs more discussions for previous one-for-all attempts.\n\nThe authors miss discussions on previous attempts to make models robust to multiple compression scenarios. For example, [1] and [2]. Please search relevant works in this area and add them to the related works.\n\n[1] Xu et al. MultiQuant: Training Once for Multi-bit Quantization of Neural Networks, IJCAI 2022\n\n[2] Yi et al. One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments, ACL 2025."}, "questions": {"value": "1. I'm not sure if I understand the notations around Theorem 1 correctly.  $\\|\\|\\Delta \\theta^* - \\Delta \\theta^*_{t,k}\\|\\|$ is a function of the difference between the applied adapter and the oracle adapter (line 272). \n\n (There seems to be some problems formatting LaTeX in OpenReview; the next lines belong to the same question)\n\n$\\Delta \\theta^*$ is the CAR-LoRA adapter (line 262). I think this is not a good notation definition. By notation, Theorem 1 proves that some unknown function of the difference between two LoRA weights has a bound. There are two potential problems. Are you directly subtracting the weights? How do you define the function (currently it says it's a function of the difference, but how to formally understand it)? Notation reads like it's the norm of the weight differences, but weight does not necessarily transfer to accuracy (i.e., the loss defined in Eq. 5). Intuitively, I seem to get what the authors try to say, but please either put it as an intuitive discussion or revise the notation.\n\n2. If possible, please provide the training loss curve for CAR-LoRA vs. qLoRA. \n\n3. Please provide additional experiments for 2-bit and 3-bit (INT2 and INT3) quantization when finetuned with CAR-LoRA that did not simulate those quantization during training (first point of weakness)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CWWhRQ17AD", "forum": "5GimteSrgW", "replyto": "5GimteSrgW", "signatures": ["ICLR.cc/2026/Conference/Submission14881/Reviewer_CNEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14881/Reviewer_CNEk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619541658, "cdate": 1761619541658, "tmdate": 1762925229794, "mdate": 1762925229794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CAR-LoRA, a compression-aware, temporally robust LoRA training framework that uses a single universal adapter to work across various deployment settings. By sampling compression operators during training, CAR-LoRA trains a single LoRA adapter to stay effective across diverse compression settings(INT8/FP4/NF4, pruning, layer skipping). Experiments show near-parity or small gains vs. QLoRA, including transfer to unseen compressions and stability under continued-pretraining drift. The proposed approach aims to improve efficiency by avoiding retraining while maintaining competitive task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Natural and persuasive method\n  - It cleanly combines “compressed-forward + STE-backward” to train a single adapter across heterogeneous deployments, matching real-world needs. The design is simple to implement yet principled.\n- Extensive and robust experiments\n  -  Across multiple backbones and standard reasoning benchmarks, the performance stays competitive with strong baselines, indicating the portability of this method."}, "weaknesses": {"value": "- Insufficient comparison to naive LoRA.\n  - Figures 3/4 don’t plot naive LoRA performance across evolving checkpoints, so the claimed degradation is unsubstantiated. Please include the LoRA curve or a table with checkpoint-wise metrics.\n- limited cross-operator generalization evidence.\n  - “Unseen compression” tests stay mostly within the quantization family. A stronger test would train only under quantization and evaluate on pruning or layer skipping to demonstrate true cross-operator robustness."}, "questions": {"value": "- In section 4.5 you claim CAR-LoRA is resilient to temporal parameter drift, but your test only compares checkpoints trained on the same data, where drift may be small. Suppose we train CAR-LoRA on base model using task A, then continue training base model on a different task B. If we apply the original CAR-LoRA to this B-updated model, does it still perform well on task A? What do you think about tackling this problem?\n- About layer skipping, what factors make LS particularly brittle. Do you consider other methods to close the gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J700OE66uc", "forum": "5GimteSrgW", "replyto": "5GimteSrgW", "signatures": ["ICLR.cc/2026/Conference/Submission14881/Reviewer_8M4z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14881/Reviewer_8M4z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896559827, "cdate": 1761896559827, "tmdate": 1762925229195, "mdate": 1762925229195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CAR-LoRA, a training framework for creating a universal robust LoRA adapter. The key idea is to integrate simulated compression techniques (quantization, pruning, etc.) during training. Experiments across Llama-3.1, Mistral, and Gemma-2 on benchmarks like SQA, MATH, GSM8K demonstrate that CAR-LoRA matches or slightly outperforms specialized QLoRA adapters, while requiring only one training run."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-structured and articulates the deployment challenge (heterogeneous compression and evolving LLMs) clearly.\n- The authors use a toy example to motivate their problem.\n- The idea of training a single LoRA adapter under randomized compression perturbations is simple yet conceptually appealing."}, "weaknesses": {"value": "- Incremental novelty:\n     - The core idea of injecting random compression perturbations during LoRA training is conceptually simple and largely derived from prior work in quantization-aware and robustness training.\n- Unsubstantiated claims:\n     - The paper claims that standard LoRA exhibits a “steeper decline” under temporal drift, but does not present quantitative evidence or citations to support this claim. Figure 4 appears to plot only CAR-LoRA results, with LoRA’s decline mentioned qualitatively. The absence of explicit LoRA baselines per checkpoint prevents verification of the claimed robustness gap.\n    -  In addition, while CAR-LoRA shows limited robustness to numerical quantization, there is no evidence of robustness to actual hardware diversity. There are no inference metrics, no deployment tests on constrained or heterogeneous devices, and no demonstration across hardware backends. This weakens the core claim of “hardware heterogeneity.”\n- Edge-deployment claims without device-level evidence: \n     - Despite the edge framing, the paper only uses  “Amortized” parameter/GPU-hour accounting (Table 4) to claim efficiency, but that doesn’t substitute for real deployment metrics. For edge devices, metrics such as latency, memory footprint, and energy consumption matter more than the time taken to train a LoRA for that device.\n- Failure to validate robustness for layer skipping:\n    - The method explicitly motivates robustness to diverse compression schemes, including layer skipping, but empirical results show that CAR-LoRA performs poorly when layers are removed.\n- Superficial theoretical analysis:\n    - Theorem 1 offers a generic Lipschitz-based bound decomposing errors into drift, compression, and generalization terms, but it lacks quantitative insight or predictive power."}, "questions": {"value": "- Table 4 shows that CAR-LoRA requires 350 GB of GPU memory. Is that during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zlECnGMDRq", "forum": "5GimteSrgW", "replyto": "5GimteSrgW", "signatures": ["ICLR.cc/2026/Conference/Submission14881/Reviewer_XCK8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14881/Reviewer_XCK8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899354535, "cdate": 1761899354535, "tmdate": 1762925228834, "mdate": 1762925228834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CAR-LoRA (Compression-Aware and Robust LoRA) — a framework for training a single, universal LoRA adapter that remains effective across both compressed (e.g., quantized or pruned) and evolving large language models (LLMs).\nTraditional LoRA adapters must be retrained whenever a base model changes or when deploying to devices with different hardware compression formats, which is inefficient and costly. CAR-LoRA solves this by integrating simulated compression operators (quantization, pruning, layer skipping) during training. It uses a quantized forward pass and full-precision backward pass, ensuring the adapter learns to be robust against compression-induced perturbations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong empirical validation.\nExperiments span multiple open-source LLM architectures and reasoning benchmarks, showing broad generalizability. Results show negligible degradation compared to retrained baselines.\n\n2. Solid theoretical grounding.\nThe authors provide a theoretical bound explaining why the adapter remains effective under compression and temporal drift, supported by assumptions like Lipschitz continuity and bounded perturbations."}, "weaknesses": {"value": "1. Limited exploration of layer-skipping robustness.\nResults show notable performance drops (e.g., MATH accuracy from 38.9% to 31.1%) under layer skipping. The authors mention this but do not deeply analyze why or propose mitigation strategies.\n\n2. Computational cost not negligible for initial training.\nAlthough amortized cost is lower, CAR-LoRA still requires longer single-run training (20 epochs vs. 5 for baselines). Some organizations might find this up-front cost high.\n\n3. Missing ablation studies.\nThe paper lacks a detailed breakdown of which components (e.g., quantized forward pass, structured pruning simulation, STE approximation) contribute most to robustness.\n\n4. Unclear figures and writing.\nThe paper writing needs improvement, with many details remaining unclear. The model architecture in figure 2 is unclear."}, "questions": {"value": "How exactly is the distribution p(C) of compression operators chosen during training? Is it uniform across quantization, pruning, and layer skipping, or weighted to reflect real deployment likelihoods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2RbB3RUg2V", "forum": "5GimteSrgW", "replyto": "5GimteSrgW", "signatures": ["ICLR.cc/2026/Conference/Submission14881/Reviewer_M81r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14881/Reviewer_M81r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007266947, "cdate": 1762007266947, "tmdate": 1762925228555, "mdate": 1762925228555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}