{"id": "0z0xhXRbN3", "number": 21866, "cdate": 1758322815662, "mdate": 1763416202020, "content": {"title": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents", "abstract": "Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.", "tldr": "We propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance.", "keywords": ["AI agents", "computer-use", "planning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cfbabce426bd9f347266df4cacd45c7502c3b370.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the Dyna-Think framework, which integrates the world model with the reasoning process of agents to enhance the decision-making performance of long-term computer-use task. Firstly, by reconstructing the reasoning process of DeepSeek-R1, Dyna-Think imitates the reasoning and world modeling related to the final action. Then, Dyna-Think trains the policy and world model using the world model dataset and the policy dataset, adopting critique-style training for the world model. The experimental results show that critique-style world model training is effective; world simulation is crucial; and the world model's ability is strongly correlated with the success rate of tasks. Contributions: Dyna-Think with 32B model achieves the performance of the 685B R1, with a 50% reduction in tokens, proving the synergistic effect of world model learning and policy learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is clear. Starting from the observation that existing reasoning models tend to overthink. The authors identify that world model simulation is the most influential part for the final action, leading to a framework that integrates reasoning, acting, and world modelling.\n\n2. Dyna-Think extends traditional Dyna algorithms, which separate policy π(θ) and world model W(μ), into a unified model πW(θ) that internalizes world model simulation into the agent's thinking process, which is innovative.\n\n3. The experiment is detailed, validating the effectiveness of Dyna-Think and exploring different world model training objectives, different thinking behaviors, the world model data scale and the correlation between world model accuracy and agent performance.\n\n4. The paper is well-written with clear motivation, method description, and experimental results. The figures effectively illustrate key concepts (e.g., Figure 1 comparing R1 and Dyna-Think, Figure 2 showing the framework), and the experimental results are presented systematically with appropriate ablations and analysis."}, "weaknesses": {"value": "Refer to questions section."}, "questions": {"value": "1. Dyna-Think adopts Qwen2.5-32B-Instruct as the base model and DeepSeek-R1 as the teacher model. Is the current model's performance ceiling primarily constrained by R1's performance limit, or is it limited by the scale of the base model? Could training a larger base model further improve performance and potentially surpass R1?\n\n2. Dyna-Think uses GPT-4o to reconstruct the thinking process (DIT) and generate critiques for world model training (DDT). Could the use of a single model lead to biased datasets and training processes? Would adopting different LLMs of varying scales or with different reasoning styles affect Dyna-Think's results?\n\n3. Critique-based world model training may lead to overfitting to the training environment. As shown in Lines 338 and 340, critique-based world model training improves in-domain (ID) performance but shows a drop in out-of-domain (OOD) performance compared to next-state prediction. Additionally, as shown in Lines 462 and 465, Dyna-Think achieves higher world model accuracy but lower task success rate in certain cases. What causes this discrepancy? Does this suggest that critique-based world model training leads to overfitting to the training environment? Further analysis on the generalization properties of different world model training objectives would be valuable.\n\n4. The experimental results shown in Section 5.1 (Figure 4b) appear to report performance on the full test set (All: 174 tasks). Could the authors provide a breakdown showing the impact of synthetic data scaling separately on ID and OOD performance? This would help understand whether scaling world model training with synthetic tasks primarily benefits ID generalization, OOD generalization, or both, and provide insights into the data efficiency of world model learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xSLO4s3OLM", "forum": "0z0xhXRbN3", "replyto": "0z0xhXRbN3", "signatures": ["ICLR.cc/2026/Conference/Submission21866/Reviewer_yrjn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21866/Reviewer_yrjn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706811803, "cdate": 1761706811803, "tmdate": 1762941962633, "mdate": 1762941962633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Dyna-Think, a framework that aims to integrate world model simulation into an LLM agent's reasoning process. The method consists of two main parts: Dyna-Think Imitation Learning (DIT), where the reasoning process of a large expert model (DeepSeek-R1) is \"reconstructed\" and abbreviated using GPT-4o to distill a more efficient thinking style into a smaller model; and Dyna-Think Dyna Training (DDT), an online fine-tuning stage that uses environment rollouts to train the agent on both policy and world model objectives."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work tackles the critical trade-off in AI agent design by proposing a framework to distill the capabilities of large expert models into smaller, task-optimised agents. This approach positions itself as a middle ground, mediating between the high computational cost of test-time search (e.g., MCTS) and the high token cost of the elaborate reasoning found in leading thinking models.\n\nThe authors have employed a solid evaluation setup by using an held-out in-domain test sets, completely out-of-domain application suites (GIMP, Thunderbird), and a different out-of-domain operating system (WindowsAgentArena). The empirical results are indeed strong and provide a compelling case for the proposed methodology.\n\nThe empirical support for the proposed framework is robust, featuring a comprehensive evaluation that is not limited to aggregate performance metrics but also includes valuable ablations and a quantitative analysis of the agent's underlying reasoning."}, "weaknesses": {"value": "The claim of achieving performance similar to the R1 expert warrants a more nuanced analysis, as the aggregate metrics conceal significant weaknesses in generalization and reliability. For instance, the model's reliability drops precipitously in unfamiliar contexts: its Average Success Rate falls from 30.3% on in-domain tasks to just 17.6% out-of-domain. This suggests that the agent's learned 'world model' is highly specialized to the training applications and does not transfer effectively. This issue of brittleness is also reflected in the large gap between the agent's high Best-of-N scores and its much lower Average Success rates across all domains. A comprehensive analysis would need to address these points directly, as they are central to understanding the practical limitations and the true generalization power of the proposed framework.\n\nWhile the \"Dyna\" framing is evocative, it is also confusing. Classic Dyna involves learning a separate world model to generate simulated experiences for planning, which then augments model-free learning. The proposed method does neither of these. A more accurate framing would be \"Distilling Planning into Reasoning.\" The DIT stage distils R1's verbose reasoning into a more structured, plan-like format, and the DDT stage uses online data to further refine this internalised planning capability. The current framing obscures the core contribution, which is about the structure of reasoning, not a classic learning-planning loop.\n\nThe results show that DDT critic performs best on the in-domain OSWorld benchmark, while next-state prediction DDT T hat performs best on the OOD WindowsAgentArena benchmark. The authors briefly hypothesize that next-state prediction might enhance the model's fundamental understanding of the environment, which is useful for novel states. This is a plausible but crucial point that deserves a much deeper discussion. It complicates the paper's narrative and suggests that the optimal form of world model training may not be universal, which is a significant practical limitation.\n\nA key methodological concern is the framework's deep dependency on the proprietary GPT-4o model, which serves multiple roles as an editor, critic, and evaluator. This makes it difficult to attribute performance gains solely to the proposed Dyna-Think architecture versus the distilled intelligence of the powerful GPT-4o oracle. The paper would be significantly strengthened by an ablation study that investigates this dependency. For instance, demonstrating the framework's effectiveness when using a different model to generate critiques would help isolate the contribution of the architectural choices and assess the method's broader applicability."}, "questions": {"value": "The results show a significant gap between the proposed model's average success rate and that of the R1 expert, particularly the collapse in reliability from 30.3% in-domain to 17.6% out-of-domain. Could you provide a more detailed analysis of what contributes to this performance gap and brittleness? Does this suggest the framework is more effective at domain-specific optimization than at learning generalizable skills?\n\nRegarding the methodology, could you discuss the rationale for using GPT-4o as the central oracle over the in-study expert, R1? To what extent are the framework's successes tied to the specific SOTA capabilities of GPT-4o, and would the results be replicable with a different critic model?\n\nThe results indicate that different DDT objectives are optimal for in-domain versus out-of-domain tasks. Could you expand on your hypothesis for why this occurs, and does this imply that a robust generalist agent might require a curriculum of world model objectives rather than a single approach?\n\nTo better understand the model's behavior, could you please provide the scatter plot that visualizes the correlation between per-task World Model Accuracy and Task Success from which the Pearson correlation in Table 4 was calculated? A more granular breakdown of performance by task category would also be helpful to identify systematic strengths and weaknesses.\n\nThe \"Dyna\" framing is potentially confusing given the departure from the classic architecture. Have you considered framing the contribution more directly as a method for \"Distilling Planning into Reasoning,\" which seems to more accurately capture the DIT and DDT processes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1J9yxnTgCY", "forum": "0z0xhXRbN3", "replyto": "0z0xhXRbN3", "signatures": ["ICLR.cc/2026/Conference/Submission21866/Reviewer_dbiP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21866/Reviewer_dbiP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843273122, "cdate": 1761843273122, "tmdate": 1762941962315, "mdate": 1762941962315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dyna-Think, a way to get language agents to perform world modeling (1) through imitation learning on reconstructed trajectories (Dyna-Think Imitation Learning) and (2) policy learning with auxiliary world modeling objectives (Dyna-Think Dyna Training). The authors evaluate on OSWorld and WindowsAgentArena and generally find their proposed approach to outperform the training-based baselines. In addition, the authors provide insights on how to synthetically scale up world model learning and evaluate world modeling accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper aims to tackle a timely topic as both language agents and world models have shown promise in their own respective domains, but they have not been super successfully combined so far.\n2. I like that the paper measures both in-domain and out-of-domain performance.\n3. The results tend to suggest some of the proposed modifications indeed help performance over the trained baselines.\n4. Figure 4 seems like a nice result since it implies you can continue learning mostly (or purely?) through world model training."}, "weaknesses": {"value": "1. Section 3.2: Based on Table A4 in the appendix, I’m not convinced that the benefits seen from doing imitation learning on R1 thinking reconstructions have anything to do with world modeling simulation. Specifically, did the authors try simpler prompts than the one listed in Table A4, maybe just keeping “remove unnecessary thinking parts without touching anything else”. In other words, how much of the benefit is just coming from generally letting GPT-4o clean up the thinking trace?\n2. Figure 2 could be improved in terms of clarity: what’s the difference between Dyna Training (b) and Dyna-Think Training (c). Also, it seems (a) is just showing a general RL diagram - it’s unclear to me how this helps explain the proposed method?\n3. Page 5: “then constructing a policy learning dataset that trains π_W(θ) to predict each action a_i given the previous context context (o0,a<i,o<i) based on a reward function (e.g., task success).” → Could the authors be more specific here? Predicting each action given the previous context sounds like behavioral cloning, but the authors also mention a reward so I’m not sure exactly what objective is being optimized?\n4. The paper could also use some improvements in writing. There are several sentences that contain spelling or grammar errors. \n5. Section 3.3: why does providing a critique generated by GPT-4o have anything to do with world modeling?\n6. Section 3.3: a lot of the algorithm is described in words, and it’d greatly help the clarity of exposition if some of this was written out more formally.\n7. Page 7: “… and that focusing on/improving simulation ability is effective at improving agent performance.” → Could the authors point to the exact evidence to support this claim?"}, "questions": {"value": "1. Page 7: “... these critique data provide a more direct signal for π_W(θ) to improve its world modeling and planning ability during inference (see Section 5.3 for more quantitative study).” → Could the authors expand on why this is?\n2. Section 5.1: “We suggest future work should scale both world model and policy training data together” → Isn’t that already happening in Figure 4? Since it says “After world model training, we perform one-round of policy training using the same set of policy learning data.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jSxLZX9pYk", "forum": "0z0xhXRbN3", "replyto": "0z0xhXRbN3", "signatures": ["ICLR.cc/2026/Conference/Submission21866/Reviewer_G1k1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21866/Reviewer_G1k1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935991485, "cdate": 1761935991485, "tmdate": 1762941961850, "mdate": 1762941961850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Dyna-Think, a framework that integrates action-centric world-model simulation into an agent’s thinking process and then jointly improves the policy and its internal world model: (1) DIT reconstructs expert R1 thinking to keep only reasoning plus the world simulation directly relevant to the chosen action, and trains a policy on this curated data; (2) DDT is a two-stage, Dyna-style procedure that first trains internal world-model capabilities (via next-state prediction, state-difference, or critique prediction) and then improves policy via rollouts and rejection sampling. On OSWorld and WindowsAgentArena, DDT outperforms RFT and a vanilla Dyna baseline, and achieves similar Best-of-N success to R1 with ~2× fewer tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- this work is generally well-written and presented cleanly\n- the idea of integrating world model simulation into the cot process is interesting, and various variants of such an integration have been explored and compared\n- the experiments are extensive, with useful ablations and analyses"}, "weaknesses": {"value": "- Why DDT > vanilla Dyna is under-explained. The authors show DDT (especially next-state and critique prediction) > vanilla Dyna under the same rollout budget (Table 1), but do not disentangle why world modelling helps in DDT and not in vanilla Dyna. Can the authors specify how the \"separate $W(\\mu)$\" is trained/used in vanilla Dyna (e.g., model size, training signals, fidelity checks, rollout usage)?\n- The authors say added evaluation hints are removed during training and testing in Sec 5.2, so what exactly are the hints used for? From Fig. 5a/b, it appears hints are appended only to rejection-sampling prompts to collect better policy data (w/ eval hint), not learned directly. Can authors clarify whether this is an ablation on critique-style training or a separate data-collection trick?\n\n- for critique prediction, why does masking to critique tokens constitute “world-model training”? Is the critique treated as a predictive explanation of state difference? If so, any evidence that models trained with critiques actually improve next-state prediction accuracy (not only policy success)?\n- also it is not clear what the DIRECT DISTILL is used for in Sec. 4.4?\n\n- Given non-trivial data curation (rejection sampling, synthetic task generation, critique extraction), can the author add (or open source its code) a clear table listing, per method: number of tasks, real vs simulated rollouts used for policy vs world model training for both proposed method and baselines. This will help others replicate the presented results.\n\ntiny presentation issues\n- for easier scanning, it is recommended to bold the best proprietary model results in Table 1 and Table 3 too (you currently bold your methods only)\n- The green/orange highlighting is hard to verify. Provide an explicit annotation protocol (what marks “simulation”), and an example with line-by-line tags so readers can see where simulation starts."}, "questions": {"value": "- in Sec. 5.2, are evaluation hints used only to re-collect better trajectories via rejection sampling (and then removed), or do they leak into training? What is the precise purpose of this section—data bootstrapping ablation or critique-objective ablation?\n- DIRECT DISTILL vs DIT (in Sec. 4.4): can the authors detail the pipelines? How is “R1 no-think” built, and how does DIT’s filtering to action-centric simulation differ from naive distill on all R1 tokens?\n\n- why choose rejection sampling over GRPO/PPO-style RL for policy improvement? Did you try GRPO with the same budgets?\n\n- sec. 5.1 scaling world-model training: which world modeling objective(s) were used? Do gains persist if the synthetic tasks come from a different generator or domain?\n\n- what happens if you skip DIT and run DDT from the base Qwen2.5-32B-Instruct? This ablation would clarify DIT’s necessity vs DDT’s sufficiency.\n\n- consider DIT reconstruction and critique injection both rely on GPT-4o for filtering/extraction and generating critiques, how do authors handle failure cases, and how sensitive is the judge? If we can switch to a stronger judge for a better performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f7MCanmGbo", "forum": "0z0xhXRbN3", "replyto": "0z0xhXRbN3", "signatures": ["ICLR.cc/2026/Conference/Submission21866/Reviewer_f5TZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21866/Reviewer_f5TZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995598368, "cdate": 1761995598368, "tmdate": 1762941960749, "mdate": 1762941960749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}