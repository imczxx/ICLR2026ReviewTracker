{"id": "fGGcovg6oW", "number": 8347, "cdate": 1758079379164, "mdate": 1763719588615, "content": {"title": "Variational Reasoning for Language Models", "abstract": "We introduce a **variational reasoning** framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where *an implicit weighting by model accuracy* naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models.", "tldr": "We propose a variational reasoning framework that treats thinking traces as latent variables optimized via variational inference, yielding a principled and stable training objective that improves LLM reasoning across diverse benchmarks.", "keywords": ["Language Models", "Variational Reasoning", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99cce6183ed6913fa0d36b63703eb2fac1c2d135.pdf", "supplementary_material": "/attachment/cfd358bbc801621d588c7d44652d610690f68439.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for fine-tuning a reasoning LLM to generate better “thinking traces”. Specifically, this paper first trains an auxiliary LLM to use the answer as a hint to generate better thinking traces, then trains the original LLM to generate similar high-quality thinking traces without seeing any hint. The idea is put in a variational framework: the thinking traces are formulated as latent variables and the auxiliary LLM and original LLM are alternately optimized for ELBO. The auxiliary LLM is viewed as a variational posterior, and a forward-KL objective is used to optimize it to avoid collapsing to shortcut reasoning. A tighter IWAE‑style multi‑trace bound is used, and practical tricks are proposed for it to work. Besides, The authors show that RFT and binary‑reward RL including GRPO are local forward‑KL updates implicitly weighted by model accuracy, explaining a bias towards easier questions. On math/code/general‑knowledge benchmarks with Qwen 2.5/3 models, the method outperforms strong baselines and trains more stably."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed method is intuitive at a high level: to train an auxiliary LLM which can make use of a hint to generate better thinking trace, and use this model to improve the original one.\n* Many techniques are proposed to make the method work in practice, including the tighter IWAE-style lower bound, accuracy-based estimator for lower worst-case variances, forward-KL to avoid the variational posterior collapsing to trivial solution, etc.\n* The connection between RFT/binary reward RL and forward-KL is revealed, which also reveals their bias toward easier questions."}, "weaknesses": {"value": "* The paper states that \"our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods\". However, tricks like using geometric mean as a surrogate for the likelihood ratio and using forward-KL to optimize the variational posterior are not compatible with VI as far as I can see. I appreciate the method for its practical impact and clarity of intent, but I think connecting it to VI to make it \"principled\" may be overstating without additional justification or ablations.\n* The presentation is difficult to follow on a first read. Algorithm 1 greatly clarifies the pipeline, but it appears after substantial technical detail. Reordering the exposition (high‑level pipeline, objective, implementation details) could make the paper more accessible to newcomers."}, "questions": {"value": "* Have you tried to use the variational posterior to generate high-quality thinking traces and use them to do simple SFT on the original LLM? How crucial are the VI/ELBO components in that case?\n* Is it possible to extend this method to scenarios where a rule-based verifier doesn't exist?\n* As a lightweight qualitative check, it would be interesting to compare the auxiliary model’s traces when conditioned on a correct vs. incorrect answer hint."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EG9ArH18Co", "forum": "fGGcovg6oW", "replyto": "fGGcovg6oW", "signatures": ["ICLR.cc/2026/Conference/Submission8347/Reviewer_xe5w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8347/Reviewer_xe5w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939416175, "cdate": 1761939416175, "tmdate": 1762920264509, "mdate": 1762920264509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies variational inference to enhance language models' chain-of-thought reasoning by treating reasoning traces as latent variables and optimizing an ELBO-based objective. The authors further extend the objective to an IWAE-style multi-trace formulation for a tighter bound and introduce a stabilized training procedure using a forward-KL term. Experiments on math, coding, and general reasoning benchmarks show that this variational reasoning framework consistently outperforms SFT, RFT, and RL baselines, while also generalizing well to out-of-distribution tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Leverages a principled variational inference framework to improve reasoning by treating thought traces as latent variables, providing a clean probabilistic formulation.\n- Uses a forward-KL based objective to address instability and collapse issues in training the variational posterior, leading to more stable optimization.\n- Algorithmic pipeline is clearly presented and appears reproducible, with explicit sampling, weighting, and update procedures.\n- Strong empirical evaluation across diverse reasoning tasks (math, coding, and general benchmarks), including out-of-distribution settings.\n- Comprehensive ablations that assess key design choices, such as the effect of the answer hint, different estimators for correctness probability, and varying the number of sampled traces."}, "weaknesses": {"value": "- The forward-KL update is largely heuristic and does not come with formal guarantees on bias or convergence. While it stabilizes training in practice, the theoretical trade-offs and conditions under which the approximation holds are not fully analyzed.\n\n- The geometric-mean modification of the importance ratio reduces variance for long reasoning traces but may introduce systematic bias. The paper does not characterize the direction or magnitude of this bias, nor discuss potential failure modes.\n\n- The method relies on external verifiers to estimate correctness probabilities. The overall stability and performance depend on verifier quality; imperfect or incomplete verifiers may mislabel valid/invalid reasoning paths and influence learning dynamics."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BPRVsL7rAs", "forum": "fGGcovg6oW", "replyto": "fGGcovg6oW", "signatures": ["ICLR.cc/2026/Conference/Submission8347/Reviewer_nKtU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8347/Reviewer_nKtU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979683650, "cdate": 1761979683650, "tmdate": 1762920263668, "mdate": 1762920263668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes reasoning in language models as a latent variable problem, where the intermediate “thinking trace” is treated as a hidden variable optimized through variational inference. Starting from the ELBO, the authors derive a tighter multi-trace objective and introduce a forward-KL formulation that stabilizes training of the variational posterior. This yields a unified probabilistic view that connects supervised finetuning, rejection sampling finetuning, and binary-reward RL as approximate inference objectives under the same framework. Empirically, the method consistently improves reasoning performance across math, code, and general benchmarks on Qwen models, showing both higher stability and generalization. Overall, it offers a principled bridge between likelihood-based reasoning and RL-style optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper offers a clean and principled unification of reasoning-oriented training methods for LLMs under a single probabilistic framework. By treating reasoning traces as latent variables and optimizing an IWAE-style ELBO, it grounds both RL and supervised finetuning as approximate inference procedures, clarifying their implicit assumptions. The forward-KL formulation is a well-motivated and technically strong addition that improves stability without sacrificing generality. The paper is clearly written and empirically supported with competitive results across diverse reasoning domains. Overall, it provides both conceptual clarity and practical value, advancing the theoretical understanding of reasoning optimization in large models. \n\nThere is a nice connection to older works of RL for non-LLM domains: see https://arxiv.org/pdf/1805.00909. They also use latent optimization via ELBO."}, "weaknesses": {"value": "I believe the authors haven't done a full literature search on prior RL works, such as https://arxiv.org/pdf/1805.00909, https://openreview.net/pdf?id=a147pIS2Co that discuss a similar idea in the non-LLM setting. Could the authors explain the differences to these works."}, "questions": {"value": "How does the variational posterior q_{\\phi} scale with model size? Is there evidence that a separate learned posterior remains necessary for very large models, or can it be amortized within the base model?\n\nHow sensitive is the proposed method to the quality or accuracy of the “hint” used in the variational posterior? Would the framework still hold when the hint is noisy, partially correct, or unavailable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wjH5bHec4A", "forum": "fGGcovg6oW", "replyto": "fGGcovg6oW", "signatures": ["ICLR.cc/2026/Conference/Submission8347/Reviewer_b5vB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8347/Reviewer_b5vB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762282753259, "cdate": 1762282753259, "tmdate": 1762920263278, "mdate": 1762920263278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for training reasoning models by treating thinking traces as latent variables and optimizing the log-likelihood of correct answers using variational inference (VI). The authors derive an IWAE-style multi-trace objective (ELBO) to update the reasoning model $\\pi_{\\theta}$ and a forward-KL objective (Eq. 9) to stabilize the training of the variational posterior $q_{\\phi}$.\n\nA key theoretical contribution is showing that existing methods like Rejection Sampling Finetuning (RFT) and GRPO can be interpreted as local forward-KL objectives, revealing an implicit bias toward easier questions. Empirically, the method demonstrates strong performance, outperforming baselines like Bespoke-Stratos on several reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Insightful Theoretical Framework: The paper's primary strength is its principled unification of several reasoning-as-inference methods (SFT, RL) under a single probabilistic lens. Framing reasoning as a latent variable problem is elegant.\n\n2. Valuable Unification and Bias Discovery: Showing that RFT and GRPO can be derived as forward-KL minimizations (Eq. 10 & 11) is a significant insight. The discovery of the implicit accuracy-weighting, and its resulting bias toward easier questions, is a valuable, non-obvious finding for the community that could influence future method design.\n\n3. Strong Empirical Performance: The method achieves impressive and consistent gains across multiple model scales (4B to 32B). It surpasses strong baselines, for instance, by over 8.5% on average accuracy compared to Bespoke-Stratos-4B on the same training data (Table 1), demonstrating its effectiveness."}, "weaknesses": {"value": "1. Extreme Methodological and Computational Complexity: The proposed training pipeline (Algorithm 1) is substantially more complex than the baselines. As detailed in Appendix C, the full process requires (at minimum) three separate, full finetuning runs ($\\pi_{\\theta_0}$, $q_{\\phi}$, and $\\pi_{\\theta}$) plus a massive offline data generation and re-weighting pass. This re-weighting step (for the \"-Acc\" estimator) is itself a huge computational burden, requiring lots of verifications. \n\n2. The final algorithm essentially boils down to two complex, weighted SFT steps. This raises the critical question: is the VI formalism necessary? The argument for its necessity is weakened by the authors' own admission that they must modify the \"principled\" importance weight (Eq. 8, using a geometric mean) to mitigate high variance, thereby introducing bias and moving away from the pure ELBO derivation. The paper is missing the important ablation: a comparison against a simpler weighted SFT (e.g., using only verifier accuracy as a weight) to prove that the complex $\\pi_{\\theta} / q_{\\phi}$ importance-weighting is truly beneficial.\n\n3. While Appendix C provides training details, the paper presents its strong empirical results without a clear accounting of the total compute. The performance gains are achieved with what is likely an orders-of-magnitude increase in total compute over the SFT baseline. Without a compute-matched baseline (e.g., SFT on more data or for more epochs), it is impossible to assess the actual efficiency of the method."}, "questions": {"value": "The author mentions directly ELBO formulation with reverse KL will have mode collapse. How bad is it? Do you have small scale observations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ib0YOscz6S", "forum": "fGGcovg6oW", "replyto": "fGGcovg6oW", "signatures": ["ICLR.cc/2026/Conference/Submission8347/Reviewer_7MH7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8347/Reviewer_7MH7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762581183433, "cdate": 1762581183433, "tmdate": 1762920262929, "mdate": 1762920262929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Paper Revision"}, "comment": {"value": "We thank all reviewers for their constructive feedback, and we have responded to each reviewer individually. We have also uploaded a **Paper Revision** including additional results:\n\n\n- $\\\\textrm{\\\\color{blue}Appendix A.8}$: Proof of the variational posterior's optimality as a variance-minimizing behavior policy.\n\n- $\\\\textrm{\\\\color{blue}Appendix B}$: Discussion about more related works.\n\n- $\\\\textrm{\\\\color{blue}Table 13}$: Ablation study on the effects of different SFT weights for training the reasoning model.\n\n- $\\\\textrm{\\\\color{blue}Table 14}$: Ablation study on the effects of the quality of hints used for sampling from the variational posterior.\n\n- $\\\\textrm{\\\\color{blue}Table 15}$: Ablation study on the effects of the verifier's accuracy.\n\n- $\\\\textrm{\\\\color{blue}Table 16}$: Ablation study on the effects of geometric mean modification of the likelihood ratio.\n\n- $\\\\textrm{\\\\color{blue}Table 17}$: Performance comparison with the baseline under a similar computational budget."}}, "id": "Tu7HlWYN4r", "forum": "fGGcovg6oW", "replyto": "fGGcovg6oW", "signatures": ["ICLR.cc/2026/Conference/Submission8347/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8347/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8347/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763721427461, "cdate": 1763721427461, "tmdate": 1763721427461, "mdate": 1763721427461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a variational inference framework for training LLM reasoning model by treating thinking traces as latent variables and maximizing evidence lower bound. Key innovations include an IWAE-style multi-trace objective that tightens the bound with multiple samples, as well as optimization via forward KL divergence to avoid mode collapse. Authors show existing methods like rejection sampling finetuning and GRPO can be treated as local forward KL objectives with implicit reweighting. Experiments on Qwen models show consistent improvements over baselines on math, coding and general reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Authors address the important topic of injecting reasoning into LLMs by providing a probabilistic perspective. Theorems and proofs are clear to understand. There are some parts of the method that require additional discussion (see weakness), but it is overall a well-polished work.\n* The paper provides a novel perspective on RFT/GRPO and can have impact on future works.\n* Experiments are well conducted on serveral benchmarks spanning different aspects of reasoning. Results show consistent gains across multiple model scales."}, "weaknesses": {"value": "* The mode collapse issue mentioned in section 2.3 brings up an important research question for introducing latent reasoning in LLMs but may require additional elaboration. The proposed forward KL is claimed to prevent mode collapse, but weighted SFT still concentrate probability on high-accuracy traces. My understanding is that the model will still collapse to the \"correct\" modes while sacrificing diversity. I wonder if the authors can provide more insights and experiment results on this part.\n* Authors mention that the method currently is applied to only single iteration, while standard variational EMs alternate until close to convergence. Is there any reason why T>1 wasn't specifically tested? Will we observe certain collapse or divergence, as commonly observed in experiments with EM algorithms? Additional results can improve the credibility especially given that the paper's claim focuses on mode collapse/ diverse reasoning. \n* The paper is missing discussion on computational overhead from the method as compared to other baselines, especially with multiple rollouts and multiple answer samples per trace. \n* Could the authors provide some insights on the scaling properties of the method, especially with different model sizes and different K?"}, "questions": {"value": "See questions in weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5gLiulcWim", "forum": "fGGcovg6oW", "replyto": "fGGcovg6oW", "signatures": ["ICLR.cc/2026/Conference/Submission8347/Reviewer_xKMW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8347/Reviewer_xKMW"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762586386595, "cdate": 1762586386595, "tmdate": 1762920262357, "mdate": 1762920262357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}