{"id": "l8PM2RukoP", "number": 17493, "cdate": 1758276676117, "mdate": 1759897171728, "content": {"title": "Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model", "abstract": "We present Kaleido, a subject-to-video (S2V) generation framework, which aims to synthesize subject-consistent videos conditioned on multiple reference images of target subjects. Despite recent progress in S2V generation models, existing approaches remain inadequate at maintaining multi-subject consistency and at handling background disentanglement, often resulting in lower reference fidelity and semantic drift under multi-image conditioning. These shortcomings can be attributed to several factors. Primarily, the training dataset suffers from a lack of diversity and high-quality samples, as well as cross-paired data, i.e., paired samples whose components originate from different instances. In addition, the current mechanism for integrating multiple reference images is suboptimal, potentially resulting in the confusion of multiple subjects. To overcome these limitations, we propose a dedicated data construction pipeline, incorporating low-quality sample filtering and diverse data synthesis, to produce consistency-preserving training data. Moreover, we introduce Reference Rotary Positional Encoding (R-RoPE) to process reference images, enabling stable and precise multi-image integration. Extensive experiments across numerous benchmarks demonstrate that Kaleido significantly outperforms previous methods in consistency, fidelity, and generalization, marking an advance in S2V generation.", "tldr": "", "keywords": ["Video Diffusion"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/033e0febe13a785519b9fca4cafcfcd4468ac677.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Kaleido, a multi-subject reference video generation model， aiming to address the shortcomings of existing S2V models in maintaining multi-subject consistency and background disentanglement. Its core innovations include: constructing a dedicated training data pipeline with low-quality sample filtering and cross-paired data synthesis, and proposing Reference Rotary Positional Encoding (R-RoPE) to achieve stable and accurate multi-image integration. Experiments show that Kaleido significantly outperforms existing open-source models in key metrics such as subject consistency  and background disentanglement and is comparable to closed-source models"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- R-RoPE is an effective strategy:  Isolating reference images only in the temporal  dimension is indeed unreasonable. This approach is prone to confusing the model between reference image information and the original video generation sequence during video generation, as the model may misinterpret reference images as consecutive frames in the video. Therefore, it is necessary to additionally introduce distinctions in the width (W) and height (H) spatial dimensions."}, "weaknesses": {"value": "1. Lack of innovation in the data pipeline: In related methods such as Conceptmaste and  Phantom-data [1,2], in-depth explorations have been conducted on cross-paired strategies for synthetic data and real data. \n2. Lack of experimental validation for data pipeline-related methods: The paper only provides an overall comparison of experimental results, it lacks targeted validation for individual key components of the pipeline.\n\n[1] ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning\n[2] Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3CEPdsfvXE", "forum": "l8PM2RukoP", "replyto": "l8PM2RukoP", "signatures": ["ICLR.cc/2026/Conference/Submission17493/Reviewer_45Cb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17493/Reviewer_45Cb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761701198429, "cdate": 1761701198429, "tmdate": 1762927375831, "mdate": 1762927375831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose Kaleido, a fully open-sourced S2V generation model. It includes a scalable data pipeline to collect training data and a lightweighted conditioning schemes that apply R-Rope. It achieves SOTA result among open-sourced S2V models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to understand.\n- The proposed data collection pipeline takes into account the cross-paired images, that can solve the background leakage problems during training.\n- The proposed R-RoPE is simple but effective to disentangle denoised image from condition.\n- The model achieves SOTA results, and it is fully open sourced and faciliate the community."}, "weaknesses": {"value": "- This proposed framework concatenates tokens but not token-channels, which may make the inference slow. \n- The paper does not discuss why they did not use channel-wise concatenation, which is efficient and widely adopted.\n- For R-RoPE, why the t-dim of RoPE for refernces images is not shift-T?\n- The paper lacks novelty and is mostly engineer work, but it should be fine."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "human face may be used as reference images in the collected dataset, make sure it is authorized since the model will be fully open sourced"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WbN1L0vL2W", "forum": "l8PM2RukoP", "replyto": "l8PM2RukoP", "signatures": ["ICLR.cc/2026/Conference/Submission17493/Reviewer_FNr6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17493/Reviewer_FNr6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904992978, "cdate": 1761904992978, "tmdate": 1762927374641, "mdate": 1762927374641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Kaleido, an open-source framework for subject-to-video (S2V) generation that focuses on maintaining multi-subject consistency and background disentanglement. The authors propose (1) a comprehensive data construction pipeline with cross-paired data, filtering, and augmentation; and (2) a novel Reference Rotary Positional Encoding (R-RoPE) for integrating multiple reference images. Experiments show that Kaleido achieves state-of-the-art results on both general video quality metrics and S2V-specific metrics, approaching the performance of closed-source systems like Kling and Vidu."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The proposed large-scale, cross-paired data construction process is well-designed and will be valuable for the community.\n2. Comprehensive experiments: Evaluation covers humans, objects, and multi-subject settings, with both quantitative and user studies."}, "weaknesses": {"value": "1. The architectural novelty is limited. The model mainly relies on simple concatenation for conditioning; R-RoPE, while useful, is a modest modification. Besides, its design is mostly empirical without deeper analysis.\n2. The validation of proposed dataset is missing. It lacks quantitative evidence for dataset diversity and annotation accuracy, as well as the comparision with previous dataset."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pkc3NF4I2a", "forum": "l8PM2RukoP", "replyto": "l8PM2RukoP", "signatures": ["ICLR.cc/2026/Conference/Submission17493/Reviewer_LNtv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17493/Reviewer_LNtv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978097676, "cdate": 1761978097676, "tmdate": 1762927374024, "mdate": 1762927374024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an open-source subject-to-video (S2V) generation framework that creates subject-consistent videos from multiple reference images and text prompts. Built upon the Wan 2.1 T2V-14B base model and fine-tuned for multi-reference input, it achieves near–closed-source performance in subject fidelity, background disentanglement, and video quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduce a pipeline to enhance subject and scene diversity, improve overall data fidelity, and ensure clear separation of subjects from irrelevant components.\n2. A reference-based position encoding to emphasize the references, leading to better results."}, "weaknesses": {"value": "1. The paper use CLIP as evaluation metrics. However, CLIP is not finegrained enough for Subject consistency. I suggest using face recognition metrics for human faces."}, "questions": {"value": "1. How does the artifacts produced by image editing methods like Flux affects the generated video? For example, Flux redux reposes the human, which could introduce subject inconsistencies. \n2. How many subjects can be inserted to the video at the same time? What is the limiting factor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "055vR7xNBh", "forum": "l8PM2RukoP", "replyto": "l8PM2RukoP", "signatures": ["ICLR.cc/2026/Conference/Submission17493/Reviewer_xLfE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17493/Reviewer_xLfE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021605290, "cdate": 1762021605290, "tmdate": 1762927373261, "mdate": 1762927373261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}