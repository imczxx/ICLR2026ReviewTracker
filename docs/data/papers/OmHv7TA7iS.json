{"id": "OmHv7TA7iS", "number": 11680, "cdate": 1758203044520, "mdate": 1762927035174, "content": {"title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations", "abstract": "With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods and reward models are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. Furthermore, we conduct RL experiments with xVerify as the reward model. Compared with direct generation, it shows an improvement of 22.9% for Qwen2.5-7B. which is greater than when Math Verify is used as the reward function. These results validate the effectiveness and generalizability of xVerify. All resources for xVerify are available at GitHub.", "tldr": "We propose xVerify, a high-accuracy verifier for reasoning models that addresses the limitations of existing evaluation models and reward models in assessing the correctness of long CoT responses, outperforming all existing evaluation methods.", "keywords": ["llm evaluation", "reasoning model", "slow thinking", "judge model", "reward model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/52e514e665d3cca1dbe9ff45a4837e454f01c579.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to construct a unified verifier for various subjective Q&As. The authors start by creating the Verify Answer for Reasoning (VAR) dataset, followed by training a series of xVerify models using the VAR datasets based on LLMs of different series of sizes.  The authors first conduct proof-of-concept experiments with RL to demonstrate the effectiveness of the proposed models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to understand.\n- The idea is easy and straightforward.\n- I like the experiments of RL, which should be the key to training a unified verifier."}, "weaknesses": {"value": "- About problem definition:\n  - In lines 150-154, the authors claim to \"extract answers\", which, however, is actually conducting Best-of-N.\n  - I wonder why the authors spend half a page discussing different domain-specific verifiers in Eqn. 1-4, which is neither practical (i.e., 1), you cannot know the question type ahead of time; 2) you cannot guarantee subjective questions belong to one of the 4 question types. nor necessary, since it has no connection with the dataset annotation (mainly relying on GPT-4o and humans) and xVerify models.\n- About dataset construction:\n  - In Sec. 4.1.3, why do you decide to do two-rounds of GPT-4o prompting?\n  - If the two rounds are connected, I cannot see it from the prompt templates.\n  - If the two rounds are independent and only for confidence filtering, why do you choose to use different prompts?\n- About VAR datasets:\n  - Just to make sure, the training targets in VAR are simply Correct/Wrong, right? Do you collect any reasoning texts?\n  - If so, when you train xVerify, you simply fine-tune an LLM to conduct a binary classification problem using SFT, right?\n  - Can you explain more about how your generalization sets are different from the training/testing sets? At least I cannot see it in Tables 1-2.\n- About xVerify:\n  - When you train on VAR and then evaluate on VAR, the biggest concern is data leakage.\n  - Although the authors claim to do data cleaning and separation (e.g., the generalization set), the results in Tables 1-2 seem to demonstrate different conclusions.\n  - All xVerify models' results are over 95% for both F1 and accuracy for both the testing and generalization sets, spanning from the 0.5B to the 32B variants.\n  - Model sizes do not matter. Base models do not matter. And a 0.5B variant performs better than GPT-4o LLM-as-a-judge. The empirical results fully demonstrate the strong correlation between the training and testing sets.\n- Overall, a unified verifier is important, but the key is generalizability."}, "questions": {"value": "Check the Weakness part for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w0hUDHBtFj", "forum": "OmHv7TA7iS", "replyto": "OmHv7TA7iS", "signatures": ["ICLR.cc/2026/Conference/Submission11680/Reviewer_eRsX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11680/Reviewer_eRsX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799249709, "cdate": 1761799249709, "tmdate": 1762922730830, "mdate": 1762922730830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "yk6c5L0bfU", "forum": "OmHv7TA7iS", "replyto": "OmHv7TA7iS", "signatures": ["ICLR.cc/2026/Conference/Submission11680/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11680/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762927033879, "cdate": 1762927033879, "tmdate": 1762927033879, "mdate": 1762927033879, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes xVerify, a lightweight â€œjudgeâ€ model family trained to determine answer equivalence for objective reasoning tasks, addressing two pain points in evaluating reasoning models: (i) extracting the final answer from long chains of thought and (ii) judging semantic/mathematical equivalence to a reference answer. The authors also contribute VAR, a supervised dataset aggregating responses from 19 LLMs across 24 benchmarks, labeled via multiple rounds of GPT-4o plus human verification. They fine-tune models on their proposed dataset and achieve high judge accuracy on generalization sets; notably the 0.5B model beats most baselines (including some 32B judges), and larger versions can even surpass GPT-4o on their evaluation. They also show that xVerify as a reward model in RL can improve average downstream task accuracy. Their method is aimed at providing one model for unified equivalence assessment combining mathematical, natural-language, and symbolic normalization/matching."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem of evaluation is important and the paper does a good job in using diverse model family, sizes and various datasets to comprehensively evaluate their approach. A lot of past work has been done on using LLM-as-a-judge (which authors already cite) but has mostly been focused on subjective tasks as objective tasks are generally taken care via rule-based approaches. Several baselines are considered for comparison showing how xVerify models are better than many other approaches. It is also laudable that they do RL training using their proposed judge since the ultimate application of measurement (evaluation) is to provide signal for learning which the authors demonstrate. The results are strong with x-Verify models having >= 95% F1/acc scores.\n\n\nThe paper does not tackle a totally new problem but takes a data-focused approach to improve LLM-judges. The VAR dataset could also be a very useful resource for the open-source community (assuming it will be released publicly if accepted). The paper is dense and comprehensive with all the details in the Appendix but is mostly easy to follow."}, "weaknesses": {"value": "Main Concerns:\n\n1. I don't fully agree with the premise of the paper that reasoning models pose extra challenges for evaluation as all the extra thinking steps is often enclosed inside <think> tags which can be stripped away from final evaluation. For example, I find the claims in L45-L47 unjustified. The claims are not backed by any proper evidence -- the only paper cited is Chang et al. came out in 2023 well before the release of any reasoning models (o1 came out in Sept'24). \n\n2. Inter-Annotator Agreement: Almost all the technical details are deferred to appendix like dataset composition, how annotation was done, what exactly was asked to the annotator (judge/human). Neither inter-annotator agreement rate between humans nor GPT4o-annotator agreement with humans is reported thus questioning the credibility of GPT-4o.\n\n3. Response Generation: It is well-known that models are asked to \"Provide final answer in \\\\boxed{} format\" for math/MCQ evaluations and also trained via such prompts. Thus, this should be used in when generating responses (when constructing the dataset) otherwise it would lead to unfair comparison between evaluation frameworks downstream. This way of prompting is well-established and works well in practice as also used in frontier evals like Humanity Last Exam [4]. \n\n    It is not possible to assess the actual benefit of xVerify models when the response being evaluated is not properly sought at the first place (i.e., the test/generalization split of VAR dataset since it does not reflect \"exactly\" how LLMs are prompted in LM-eval-harness or LightEval frameworks). I would recommend the authors to re-run response generation in well established template as followed by LightEval (with CoT prompting). \n\n    Classification can be viewed as special case of MCQ essentially. Thus, all formats (MCQ, math, classification) except short answer can be handled via \\\\boxed format which is well established in the community. For short text answer, the prompt can be to \"Provide final answer in \\<answer\\> \\</answer\\> tags.\" This has been done in recent work like training DeepSeek-R1 [1]. \n\n4. In Section 5.2, what was the training dataset for RL? How is the evaluation actually performed? It does not make sense to \"compare\" with direct generation as direction generation as per my understanding is without any training. The fair comparison is only with math-verify and the improvement there is less than 1% so there hardly seems to be any benefit of using xVerify over math-verify. Further, xVerify would require loading a judge model in memory compared to math-verify which would also require more GPU memory/time (although loading a 0.5B model should not be an issue). \n\n5. Continuing on RL training, in the referenced Fig. 22, why is the reward between xVerify and math-verify so different at step 0 (which is before training began)? All methods should start from the same starting point assuming the underly evaluation is consistent across all. \n\n6. If math-verify can almost match x-verify (based on the results of RL-training), then the large gain in accuracy reported in Table 1 seems confusing? I would appreciate if the authors can clarify how exactly is the evaluation being performed for the final results table/figure. \n\n7. More baseline: While many LLM-as-a-judges have been compared, all of them are quite old (For example, while JudgeLLM got published in ICLR'25, it came out on Arxiv in 2023 and is based on models released > 1.5 year ago). I would also like to see whether other recent models (both reasoning/non-reasoning eg: Qwen3-4B) can act off-the-shelf as good judges as shown in a recent study [3] which also focuses on using judges for objective answer assessment. \n\n8. Important Related Work: No comparison is made with existing general-verifier model used in recent work [2] where the authors also fine-tune small 1.5B judge for their tasks (also leading to high accuracy downstream with RL) and judge model is shown to have high agreement with Gemini 2 Flash. [2] is highly similar with this paper and deserved to be compared. \n\n\nTypos:\n\nL-070: \"We fine-tune xVerify on a variety of base models\" -> \"We finetune a variety of base models on xVerify\"\n\nIn Fig.1, LLM-response second box corresponding answer should be (B) instead of (A)?\n\nL311-L312: Spacing is weird. Please increase spacing. \n\n\nReferences: \n\n[1] Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., ... & He, Y. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948.\n\n[2] Ma, X., Liu, Q., Jiang, D., Zhang, G., Ma, Z., & Chen, W. (2025). General-reasoner: Advancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652.\n\n[3] Chandak, N., Goel, S., Prabhu, A., Hardt, M., & Geiping, J. (2025). Answer Matching Outperforms Multiple Choice for Language Model Evaluation. arXiv preprint arXiv:2507.02856.\n\n[4] Phan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., ... & Wykowski, J. (2025). Humanity's last exam. arXiv preprint arXiv:2501.14249."}, "questions": {"value": "Q1: How is the accuracy in Table 1 calculated? Is it calculated by comparing the judgement to human judgement? \n\nQ2: In L152, a scoring function is defined but nothing is mentioned about how the suitability is calculated or assessed for each candidate. No details is provided in this selection. I would appreciate if authors can elaborate here.\n\nQ3: For annotation, was ethics board IRB approval taken to hire annotators? While I have not flagged the paper for ethical concerns, nothing was mentioned regarding IRB approval in the paper. \n\nQ4: What do the suffix \"-I\", \"-Ia\", \"-Ib\" for xVerify models in Table 1 and 2 mean?\n\nQ5: For different judge models, was different ways of prompting tried? Like providing in-context examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I8TqPmI3ZI", "forum": "OmHv7TA7iS", "replyto": "OmHv7TA7iS", "signatures": ["ICLR.cc/2026/Conference/Submission11680/Reviewer_sQTd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11680/Reviewer_sQTd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917516705, "cdate": 1761917516705, "tmdate": 1762922729849, "mdate": 1762922729849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "> This paper may have violated the Double-blind submission and Anonymity policy of ICLR. The huggingface link of the authors' lab has been \nlisted in their [repo](https://anonymous.4open.science/r/xVerify-5702):\n\n```html\n<a href=\"https://huggingface.co/IAAR-Shanghai\">\n    <img alt=\"Huggingface\" src=\"https://img.shields.io/badge/ðŸ¤— Huggingface-Models-orange.svg\">\n</a>\n```\n\n\nThe paper proposes xVerify, a lightweight and efficient verifier for reasoning-based tasks. \nIt introduces the VAR dataset covering 19 LLMs and 24 benchmarks, and trains models of different sizes to predict whether an answer is correct.\nEvaluation demonstrates xVerify's two key capabilities: 1) It exhibits superior accuracy and robustness against existing methods on both in-domain and out-of-distribution benchmarks; 2) It shows advantages when used as a reward model for RL training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed xVerify framework provides an efficient and scalable solution, achieving strong performance across diverse reasoning benchmarks.\n2. The introduction of the VAR dataset contributes valuable large-scale resources for objective answer verification.\n3. The experimental evaluation is extensive, covering multiple model sizes, in-domain and out-of-domain tests, and reinforcement learning scenarios."}, "weaknesses": {"value": "1. The novelty of the work is limited. The main contribution lies in building the VAR dataset and fine-tuning existing LLMs for verification. The paper does not introduce new architectures or learning mechanisms, and similar evaluator training approaches have been explored in prior works such as xFinder[1] and CompassJudger[2]. A clearer explanation of what makes xVerify distinct or innovative would strengthen the paper (Q1-Q2).\n\n2. The implementation details of the core verification components are insufficiently described, making it unclear how symbolic and semantic alignments are realized (Q3-Q4).\n\n3. While the results are strong, the paper does not explain *why* xVerify outperforms rule-based or LLM-based judge models (Q5)."}, "questions": {"value": "1. Since the VAR dataset is labeled only with *Correct* or *Incorrect* judgments, how does this binary annotation handle borderline or partially correct cases, especially in reasoning-intensive answers? Would introducing graded or confidence-based labels better reflect nuanced correctness and reduce evaluation bias?\n\n2. The paper does not include a clear analysis of the VAR datasetâ€™s quality or consistency. Could the authors report inter-annotator agreement metrics or other measures to validate annotation reliability?\n\n3. In Section 3 (*Problem Definition*), Equations (1)â€“(3) introduce the sub-functions $\\psi_{\\text{math}}$, $\\psi_{\\text{nl}}$, and $\\psi_{\\text{sym}}$, but their concrete implementations are not clearly described. Are these functions realized through symbolic or string-based normalization methods (e.g., regular expressions, symbolic computation), or are they implemented via LLM-based semantic alignment?\n\n4. In Section 3 (*Problem Definition*), Equations (2), for natural language equivalence, could the author clarify how $\\varphi_{\\text{nl}}^{\\text{align}}$ measures semantic equivalence (e.g., embedding similarity or LLM scoring) and how the threshold $\\tau$ is determined? Since $\\tau$ critically affects the correctness boundary, it would be valuable to include sensitivity analyses or ablation studies regarding its influence on xVerifyâ€™s performance.\n\n5. While xVerify shows strong results, the paper does not clearly explain *why* it outperforms rule-based methods (e.g., RegEx) or other LLM-based judges. Could the authors provide more analysis of the main factors driving this improvement? In addition, how does xVerify differ in core methodology and design from recent evaluator models such as xFinder [1] and CompassJudger [2]?\n\n   [1] Qingchen Yu et al., *xFinder: Large Language Models as Automated Evaluators for Reliable Evaluation*, ICLR 2025.\n\n   [2] Taolin Zhang et al., *CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards*, arXiv preprint arXiv:2507.09104, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N4igz7ZsDR", "forum": "OmHv7TA7iS", "replyto": "OmHv7TA7iS", "signatures": ["ICLR.cc/2026/Conference/Submission11680/Reviewer_cERW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11680/Reviewer_cERW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921546569, "cdate": 1761921546569, "tmdate": 1762922729352, "mdate": 1762922729352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed xVerify, an efficient answer verifier for objective reasoning tasks that extracts a final answer from long LLM responses and checks equivalence via math, symbol, and natural-language comparators; it also introduces the VAR dataset built from 19 LLMs across 24 benchmarks to train and evaluate such judges."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. high performance and efficiency, showing 95% f1 with 0.5B parameters\n2. the proposed dataset can benefit other research in the community"}, "weaknesses": {"value": "1. I am curious how the process-involved reward can be superior than the pure rule-based reward. And why the process involved reward is important in the multiple choice questions, which are very easy to verify via answer correctness.\n2. The final output of xverify only focous on the final answer, not the soudness of the reasoning chain. The output is a binary label, also doesn't provide more fine-grained information regarding the correcctness of the reasoning chain.\n3. The scope is limited, xVerify targets objective tasks and needs reference answers; it does not handle subjective evaluation and is not a drop-in reward model when ground truth is absent.\n4. xVerify decides equivalence by first normalizing answers, and compare the semantic alignment score. If the normalizer cannot cover some edge case, the model can be hacked."}, "questions": {"value": "please see weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "igFbEO3VAw", "forum": "OmHv7TA7iS", "replyto": "OmHv7TA7iS", "signatures": ["ICLR.cc/2026/Conference/Submission11680/Reviewer_i4jB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11680/Reviewer_i4jB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941562303, "cdate": 1761941562303, "tmdate": 1762922728827, "mdate": 1762922728827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}