{"id": "WXGb9unEHo", "number": 6369, "cdate": 1757974547903, "mdate": 1759897919265, "content": {"title": "Scalable Offline Model-Based RL with Action Chunks", "abstract": "In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion,\ncan provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. \nModel-based value expansion fits an on-policy value function using length-$n$ imaginary rollouts generated by the current policy and a learned dynamics model.\nWhile larger $n$ reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions.\nWe address this trade-off with \nan *action-chunk* model that predicts a future state from a sequence of actions (an \"action chunk\")\ninstead of a single action, which reduces compounding errors.\nIn addition, instead of directly training a policy to maximize rewards,\nwe employ rejection sampling from an expressive behavioral action-chunk policy,\nwhich prevents model exploitation from out-of-distribution actions.\nWe call this recipe **Model-Based RL with Action Chunks (MAC)**.\nThrough experiments on highly challenging tasks with large-scale datasets of up to $100$M transitions,\nwe show that MAC achieves the best performance among offline model-based RL algorithms,\nespecially on challenging long-horizon tasks.", "tldr": "Action chunking enables long-horizon rollouts, scaling offline model-based RL to complex long-horizon tasks.", "keywords": ["Offline RL", "World Models", "Model-based RL", "Action chunking", "Long-horizon tasks"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c0b9a119be31f8197d051f40775624242a554b6.pdf", "supplementary_material": "/attachment/169346665f998c80617a2dc47ffebc7e5d299587.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Model-Based Reinforcement Learning with Action Chunks (MAC), a scalable offline model-based RL framework designed to handle long-horizon tasks. The key idea is to mitigate the trade-off between long-horizon bias reduction and model error accumulation in model-based value expansion by using action-chunk models—multi-step dynamics that predict future states from sequences of actions rather than single actions. Complementarily, an action-chunk policy is trained via flow matching and rejection sampling to model complex, high-dimensional action distributions while avoiding out-of-distribution exploitation. Experiments across large-scale datasets (100M transitions) and standard offline RL benchmarks show that MAC achieves state-of-the-art results among model-based methods and competitive performance compared to model-free baselines, particularly in complex, long-horizon manipulation tasks (Tables 1–2; Sec. 5.1–5.2)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper focuses on model-based value expansion in the offline setting, aiming to balance value bias reduction and model error reduction. This is a highly valuable and meaningful problem, as it contributes to improving the performance of offline reinforcement learning on long-horizon tasks.\n- The method proposed in the paper mainly adopts a multi-step dynamics model (action-chunk model) to reduce rollout errors and uses rejection sampling to model complex, high-dimensional action distributions. Such an approach is intuitively effective, and the experiments show that MAC achieves outstanding performance on long-horizon offline RL tasks."}, "weaknesses": {"value": "- The multi-step dynamics model (action-chunk model), which takes s_t and a multi-step action sequence (a_t, a_{t+1}, …, a_{t+k-1}) as input to predict s_{t+k}, has already been proposed in previous works [1][2][3]. The multi-step dynamics model used in MAC does not differ from these earlier approaches. Similarly, the flow policy [4], model-based value expansion [5] are also not introduced for the first time in this paper. Therefore, I believe the contribution of this work is relatively limited.\n- The application of multi-step dynamics models to the offline RL domain has been explored in ADMPO [6]. Similar to MAC, ADMPO investigates how multi-step dynamics models can reduce rollout errors in the offline setting. The key difference is that ADMPO adopts a more traditional policy learning approach, representing another way of leveraging multi-step dynamics models in offline settings. I believe ADMPO is highly related to MAC, as ADMPO has already demonstrated that multi-step dynamics models can significantly improve policy optimization performance in offline RL. Therefore, MAC should be compared against ADMPO.\n- Although this paper targets long-horizon offline tasks, it is inappropriate for an offline RL algorithm to lack experiments on the standard D4RL benchmark.\n\n\n[1] Kavosh Asadi, Evan Cater, Dipendra Misra, and Michael L. Littman. Towards a simple approach to\nmulti-step model-based reinforcement learning. CoRR, abs/1811.00128, 2018.  \n[2] Kavosh Asadi, Dipendra Misra, Seungchan Kim, and Michael L. Littman. Combating the\ncompounding-error problem with a multi-step model. CoRR, abs/1905.13320, 2019.  \n[3] Tong Che, Yuchen Lu, George Tucker, Surya Bhupatiraju, Shane Gu, Sergey Levine, and Yoshua\nBengio. Combining model-based and model-free RL via multi-step control variates. https://\nopenreview.net, 2018. URL https://openreview.net/forum?id=HkPCrEZ0Z.  \n[4] Seohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. In International Conference on\nMachine Learning (ICML), 2025c.  \n[5] Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E.\nGonzalez, and Sergey Levine, ‘Model-based value estimation for ef-\nficient model-free reinforcement learning’, CoRR, abs/1803.00101,\n(2018).  \n[6] Haoxin Lin, Yu-Yan Xu, Yihao Sun, Zhilong Zhang, Yi-Chen Li, Chengxing Jia, Junyin Ye, Jiaji\nZhang, and Yang Yu. Any-step dynamics model improves future predictions for online and\noffline reinforcement learning. In ICLR 2025."}, "questions": {"value": "- Could the authors include a performance comparison with ADMPO? Since both methods apply multi-step dynamics models to offline RL, such a comparison would better highlight whether MAC offers advantages in terms of methodological design.\n- Could the authors provide experimental results on D4RL? This is essential for evaluating the performance of an offline RL algorithm.\n- The experiments in the paper show that the multi-step dynamics model effectively reduces rollout errors, and in particular, the larger the chunk size, the more significant the reduction—this aligns well with intuition. However, it remains unclear whether MAC’s performance gain truly comes from the reduction in rollout error, or merely from the flow action-chunk policy. Although the paper provides results for different chunk sizes, this does not fully address the issue. Could the authors provide additional experiments comparing MAC’s performance across different chunk sizes and rollout lengths, explicitly reporting the rollout error and policy performance for each configuration? This would help verify whether smaller rollout errors indeed correlate with better policy performance.\n- From the experiments in the paper, when the action-chunk length is set to 25 the rollout error is still effectively reduced, but the algorithm’s performance degrades. Could the authors provide a deeper explanation for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uOYYi6PlUq", "forum": "WXGb9unEHo", "replyto": "WXGb9unEHo", "signatures": ["ICLR.cc/2026/Conference/Submission6369/Reviewer_P1Zz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6369/Reviewer_P1Zz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760796912054, "cdate": 1760796912054, "tmdate": 1762918659284, "mdate": 1762918659284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Model-based offline reinforcement learning (RL) approaches often face a key dilemma: model rollouts must be sufficiently long for the value function to capture accurate long-horizon information, yet they suffer from error accumulation as the rollout horizon increases. To address this, the paper employs action chunks, a common technique in robotics that predicts a sequence of actions rather than a single step. This reduces the number of function calls needed to generate future trajectories, enabling longer horizons while simultaneously mitigating error accumulation. Once the value functions are obtained, the policy is implicitly extracted via rejection sampling from an expressive behavior policy modeled by a distilled one-step flow matching policy.  Experiments on OGBench and D4RL demonstrate the superior performance of the proposed method, MAC (Model-based Offline RL with Action Chunks), in robotic manipulation tasks. Nonetheless, the method falls short in locomotion tasks that require precise control, where approaches based on differentiable reward maximization or policy gradients may be more effective. Moreover, all experiments are conducted with low-dimensional state inputs, without evaluation on image-based tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Scale up RL methods to tackle long-horizon problems is very important.\n2. The idea of adopting action chunks to address the dilemma between long horizons and small functional calls is quite straightforward.\n3. The proposed method (MAC) is quite simple to be implemented. So, this paper offers a good starting point for future research on this direction.\n4. The empirical performance on robotics manipulation tasks in OGBench are also strong, offering potential solutions for robotics."}, "weaknesses": {"value": "I have a few concerns regarding the novelty and the complexity of the proposed method:\n\n1. `Novelty of Action Chunks` The idea of adopting action chunks to extend RL horizons is not particularly novel. In fact, a recent work [1] has already explored a model-free version of this idea. Therefore, the novelty contribution of this paper is somewhat limited. However, since action chunking appears to be a principled and simple approach to addressing the dilemma highlighted by the authors, I would not consider the lack of novelty a major concern.\n\n2. `Method Complexity` While the high-level idea of MAC is straightforward and the implementation relatively simple and robust to hyperparameter variations, the method introduces multiple components, including a flow policy, a distilled flow policy, a reward function, a state value function, a state-action value function, and a dynamics model. This design may help address the horizon-scaling issue, but it also adds significant complexity, potentially restricting MAC to smaller model sizes. In the long run, a unified model or framework that consolidates these components would be preferable. Nonetheless, given that the primary contribution of MAC lies in leveraging action chunks to scale up model-based RL horizons, I would also not consider this limitation a major concern.\n\n[1] Reinforcement Learning with Action Chunking, 2025."}, "questions": {"value": "1. I understand that rejection sampling from a behavior policy implicitly imposes a behavior constraint. However, I still wonder whether additional penalties are needed to mitigate value overestimation. Can simply applying best-of-N sampling alone account for the scale of performance improvements reported?\n\nAt this point, I have no further questions. Please see my comments under Weaknesses for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z49WvU9ZMx", "forum": "WXGb9unEHo", "replyto": "WXGb9unEHo", "signatures": ["ICLR.cc/2026/Conference/Submission6369/Reviewer_ZGYu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6369/Reviewer_ZGYu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484135854, "cdate": 1761484135854, "tmdate": 1762918658799, "mdate": 1762918658799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAC, an offline MBRL algorithm designed particularly for long-horizon tasks. They use action-chunking to reduce the number of recursive calls to the dynamics model to hopefully reduce compounding error. They also use flow-based rejection sampling to aim to prevent model exploitation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The issue they address is relevant, the results seem strong, and the paper is reasonably clear in explaining a complicated method."}, "weaknesses": {"value": "**W1.** There are so many moving parts, including training 6 models. This is expensive, and probably also more difficult to implement and debug compared to many other methods.\n\n**W2.** The main benchmark in offline RL is probably D4RL. The authors have not provided results on this.\n\n**W3.** As far as I understand the proposed method is roughly N times more expensive than prior algorithms(?) Furthermore, N is high (~32), and they potentially need larger models since action-chunk inputs to the model are n times larger. The authors should include a comparison and discussion of the runtime to help justify this significant extra cost.\n\n**W4.** Performance appears highly sensitive to the action-chunk length hyperparameter. This could make it difficult to tune, especially offline.\n\nMore weaknesses: See Questions below."}, "questions": {"value": "**Q1.** What do the authors mean by “Thanks to the expressivity of the flow BC policy, we query the model only with in-distribution\naction-chunk samples”? Relatedly, isn't it easier to be OOD since \"action\" space is exponentially larger in the number of action steps included as input to the model?\n\n**Q2.** The baselines are a mixture of results from the prior papers and from the author's reimplementations. Are the authors confident they understood implementation details that might have unfairly weakened results from prior paper, and also that they correctly implemented and fairly tuned their reimplemented baselines?\n\n**Q3.**  Why was flow matching chosen over diffusion? Did the authors try/consider diffusion as an alternative?\n\n**Q4.** The method involves training 6 models at the same time (?). What was this like in terms of convergence and stability? It would be interesting to include training curves.\n\n**Q5.** Related to ^, often you would train the dynamics model p_\\phi, r_\\phi to convergence first, and then train the policy. Why train them side by side?\n\n**Q6.** Is \\pi_\\theta really needed if you then replace it with a less expressive model. For completeness, the authors should include and ablation bypassing \\pi_\\theta to show whether it is important to their method.\n\n**Q7.** (Comment) “one-step” is confusing since \"steps\" are also used to refer to RL environment steps. Is there another term you can use?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZuQEcZEvnc", "forum": "WXGb9unEHo", "replyto": "WXGb9unEHo", "signatures": ["ICLR.cc/2026/Conference/Submission6369/Reviewer_Qjkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6369/Reviewer_Qjkz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780107395, "cdate": 1761780107395, "tmdate": 1762918658363, "mdate": 1762918658363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Model-Based RL with Action Chunks (MAC), an offline RL algorithm designed to address two critical challenges: compounding errors in model-based rollouts and action selection that leads to out-of-distribution (OOD) model exploitation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[S1] MAC's action-chunk model, combined with in-distribution rejection sampling, offers a promising solution to the problems of compounding model error (a key issue in MBRL) and OOD model exploitation (a key issue in offline RL). It has achieved excellent empirical performance in long-horizon and large-scale benchmark tests."}, "weaknesses": {"value": "[W1] The main problem lies in the scalability of sampling with increasing action chunk length $n$. As $n$ increases, the dimensionality of the action chunk space also increases, making it exponentially more difficult to find high-value, in-distribution action sequences by rejection sampling. This sampling challenge may weaken the \"return maximization\" aspect of RL and may cause the agent's final behavior to be closer to behavior cloning (BC) rather than RL."}, "questions": {"value": "[Q1] Following Weakness 1: How does the simpler reward- or goal-conditioned BC policy trained on the same flow-based architecture compare to the proposed rejection sampling plus value function approach? This comparison will help reveal the true advantage of value-based return maximization over the BC baseline policy.\n\n[Q2] Given that performance peaks at an intermediate chunk length ($n=10$) and degrades with larger sizes ($n=25$), could the method be improved on large $n$ by using a mixture of chunk sizes, similar to the idea presented in [(1)](https://arxiv.org/pdf/2412.11253)? This might allow the agent to balance the low compounding error of large chunks with the more effective sampling of smaller chunks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1Msqa4P8O1", "forum": "WXGb9unEHo", "replyto": "WXGb9unEHo", "signatures": ["ICLR.cc/2026/Conference/Submission6369/Reviewer_9Rxo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6369/Reviewer_9Rxo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079874308, "cdate": 1762079874308, "tmdate": 1762918657952, "mdate": 1762918657952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}