{"id": "25F5ot3UWg", "number": 19447, "cdate": 1758296345932, "mdate": 1763688114826, "content": {"title": "The Role of Learning and Memorization in Relabeling-based Unlearning for LLMs", "abstract": "This work studies how the nature of a response generated by a large language model (LLM) impacts the efficiency of relabeling-based unlearning, a common unlearning technique that trains the model to fit an ``unlearn'' set (i.e., a dataset that we wish the model to unlearn) with alternative responses to prevent it from generating unwanted outputs that align with the unlearn set. We distinguish between two different ways LLMs can generate undesirable outputs: learning-based generation, where the model learns an underlying rule connecting the input and the response (e.g., social stereotypes), and memorization-based generation, where the model memorizes specific information about a given input (e.g., private information like a phone number). We demonstrate that relabeling-based unlearning can be detrimental to the model performance when the undesirable outputs are generated based on learning-based generation whereas it is more effective with memorization-based generation. We provide theoretical justifications for this through the lens of hypothesis testing, showing that memorization-based hypotheses are more stable in presence of fabricated evidence that contradicts the hypothesis' prediction and more flexible to produce alternative responses. Our empirical results further support our findings by showing a clear performance gap in relabeling-based unlearning under these two types of data generation mechanisms.", "tldr": "We study how the nature of response generation (learning-based versus memorization- based) affects the unlearning efficiency for the relabeling-based method.", "keywords": ["Unlearning", "LLM", "Statistical Learning", "AI safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/15cff5c32ea4efb0aa049c42596d4c7105fbc6ae.pdf", "supplementary_material": "/attachment/0955504551c8b99af71098e1ca355af2814f4737.zip"}, "replies": [{"content": {"summary": {"value": "Authors claim that undesirable outputs can arise either from rule-learning (discovered through training) or memorization (just memorizing individual training instances). The claim is that relabeling-based unlearning works more effectively for overwriting undesirable outputs acquired through memorization, with a Bayesian-theory-based analysis. In high-level terms the proofs provided show that in light of contradictory evidence, decision boundaries shift globally when learning is rule-based, leading to degraded performance on many (potentially irrelevant) inputs. Meanwhile, in the memorization regime, the contradictory evidence only shifts things locally, so relabeling-based unlearning can affect only that fact more easily without touching others. At a high level, relabeling works well for memorization but not for learned rules.\n\nAuthors then provide some light experimental evidence of binary 2D Foo/Bar classification, under 3 different label regimes. They show that the 2/3 cases where the model learns a general rule, relabeling-based unlearning is more difficult. In the 1/3 of the cases where the model has to rely on memorization due to the randomness of the labels, unlearning is easier."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the distinction between learning-based and memorization-based generation and how it relates to unlearning is interesting and novel\n- the intuition of why learning-based unlearning is more \"challenging\" for the model than memorization-based unlearning makes sense"}, "weaknesses": {"value": "- the experimental setup feels extremely toy-ish. i understand the need to create a constrained scenario but \"known function vs. random vector\" feels very far from \"learning stereotypes vs. memorizing personal information\"\n- there are unlearning papers that do not deal with such artificial scenarios AFAIK -- why weren't more real-world scenarios used? factual forgetting?\n\n- paper is structured a bit bizarrely -- mostly proofs with the experimental setup seeming to be somewhat of an afterthought, despite feeling like a core part of the argument\n\n- the math/proofs feel very simplified and make a lot of assumptions that make them very rough approximations for modern LLMs in an open-ended generative setting. binary labels, constant-prediction baselines, flip noise, etc. it's very abstracted"}, "questions": {"value": "- do you think the learning vs. memorization dichotomy exists for other unlearning methods or just for relabeling?\n\n- learning-based and memorization-based generation in real world terms probably exist as a spectrum, no? not that something cleanly falls in one category or another. would be interesting to provide some sort of method or heuristic to discover where on the spectrum a given dataset lies"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YwqYKzQSbp", "forum": "25F5ot3UWg", "replyto": "25F5ot3UWg", "signatures": ["ICLR.cc/2026/Conference/Submission19447/Reviewer_oVtL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19447/Reviewer_oVtL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894468094, "cdate": 1761894468094, "tmdate": 1762931366554, "mdate": 1762931366554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates what kinds of knowledge can be effectively unlearned by a large language model (LLM). The authors distinguish between two mechanisms of knowledge formation: learning-based, where the model abstracts general rules to approximate data, and memorization-based, where it stores and reproduces specific information verbatim. The study finds that memorization-based knowledge can be unlearned more efficiently through relabeling-based unlearning compared to knowledge derived from generalization."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper broadens our understanding by demonstrating that generalizable patterns are significantly harder to unlearn than memorized data points.\n- It presents a clear and well-defined thesis that is both theoretically motivated and empirically validated.\n- The explanation of the loss function is particularly clear and accessible.\n- Well-chosen examples provide strong intuition about the mechanisms underlying unlearning.\n- The experimental design is logical and consistent with the theoretical claims, reinforcing the paper’s overall coherence."}, "weaknesses": {"value": "- While the paper highlights important aspects of unlearning, the broader impact of these findings remains unclear. What are the practical or theoretical implications of this additional knowledge? How might it inform future research or applications?\n- The finding that general rules are more difficult to unlearn than isolated memorized points is somewhat expected and lacks an element of surprise.\n- The motivation for unlearning generalized rules could be made clearer. Under what real-world circumstances would such unlearning be necessary or beneficial? The authors could strengthen their argument by presenting a concrete example where unlearning a general rule is desirable or essential.\n- The paper would benefit from a visual summary (e.g., a conceptual diagram) near the beginning to make the core findings more accessible to readers."}, "questions": {"value": "- What do the authors mean by “unstable”? While the general concept is clear, how is instability defined and measured in the context of their framework?\n- How does this work relate to the field of alignment? There appears to be a conceptual overlap between unlearning generalized rules and ensuring model alignment. Could the authors clarify whether their findings have implications for alignment research?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3fjr4kwX6n", "forum": "25F5ot3UWg", "replyto": "25F5ot3UWg", "signatures": ["ICLR.cc/2026/Conference/Submission19447/Reviewer_d8cB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19447/Reviewer_d8cB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919667517, "cdate": 1761919667517, "tmdate": 1762931366060, "mdate": 1762931366060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how different forms of response generation, learning-based generalization versus memorization of examples, affect the difficulty of machine unlearning. It introduces a Bayesian framework comparing how the posterior belief in these two hypotheses changes when presented with fabricated, conflicting evidence. The theoretical analysis suggests that memorization-based hypotheses are more stable under such adversarial evidence, whereas learning-based hypotheses experience belief collapse as contradictory examples accumulate. The authors then conduct experiments using a single finetuned language model on synthetic  classification tasks and visualize decision boundaries before and after unlearning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper establishes a controlled way of running experiments to quantify which types of samples are harder to unlearn. This helps articulate an emerging question on how large language models maintain or lose generalization under unlearning interventions. The theoretical setup is presented in an accessible way and highlights a meaningful direction for studying robustness of model beliefs.\n\n2. The theorems show that learning-based hypotheses suffer compounding belief degradation under concentrated noise while memorization-based hypotheses remain stable. Even if the setting is simplified, the formal results help clarify why unlearning strategies might be brittle."}, "weaknesses": {"value": "1. Limited applicability. The datasets are artificially constructed and low-dimensional. Because the tasks are artificially simple and small-scale, and because the unlearning procedure is evaluated only on these toy domains, the conclusions drawn about “learning-based” vs. “memorization-based” behavior do not reliably translate to practical LLM applications. Moreover, it is unclear what is the take-away for better unlearning methods, or if there is metric to check for difficulty when the boundary between learning and memorization is unclear.\n\n2. Gaps in the belief modeling. The hypotheses $H_0$ and $H_1$ are defined only as generative structures for labels, not as full probabilistic models over functions or latent variables with priors. However, in the posterior stability analysis, each hypothesis is assumed to have a well-specified likelihood and a fully Bayesian update mechanism. Since these are not part of the hypothesis definitions, the comparison of posterior collapse reflects assumptions introduced later, not properties implied by the hypotheses themselves. All fabricated evidence uses one input which maximally harms learning models and minimally affects memorization, affecting the conclusions as it is biased towards memorization.\n\n3. Change in assumptions. The paper changes the modeling assumptions mid–analysis: the hypotheses in Theorems 2–3 operate in an unstructured discrete input world with independent labels, but Theorem 4 suddenly invokes structured continuous inputs and linear hypothesis spaces. Because the two regimes have fundamentally different generalization properties, the results about posterior stability and memorization do not carry over. As a result, the conceptual connection claimed between the Bayesian analysis and neural unlearning behavior is unsupported."}, "questions": {"value": "1. Unclear latent: In H1, the hypothesis on memorization, what is the latent vector? How can such hypotheses be tested in practice, does it depend on the input $x$? Do we treat flip noise as part of the latent model?\n\n2. How is the decision boundary estimated for the LLM experiments? The theoretical claims about decision boundary structure do not follow from the underlying model hypotheses which are about outputs ($H_0$) and latent vectors ($H_1$).\n\n3. How is memorization of examples any different from defining exposure as a metric [1], or other works that explore out-of-distribution (random) examples [2], or works studying unlearning hardness [3]? \n\n[1] Carlini, Nicholas, et al. \"The secret sharer: Evaluating and testing unintended memorization in neural networks.\" 28th USENIX security symposium (USENIX security 19). 2019.\n\n[2] Baluta, Teodora, et al. \"Unlearning in-vs. out-of-distribution data in LLMs under gradient-based method.\" arXiv preprint arXiv:2411.04388 (2024).\n\n[3] Zhao, Kairan, et al. \"What makes unlearning hard and what to do about it.\" Advances in Neural Information Processing Systems 37 (2024): 12293-12333."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sqN2oQ1Tat", "forum": "25F5ot3UWg", "replyto": "25F5ot3UWg", "signatures": ["ICLR.cc/2026/Conference/Submission19447/Reviewer_daWV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19447/Reviewer_daWV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968817724, "cdate": 1761968817724, "tmdate": 1762931365745, "mdate": 1762931365745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the effectiveness of relabeling-based unlearning for LLMs, distinguishing between learning-based and memorization-based generation. The results show that relabeling-based unlearning is more effective for memorization-based generation, while learning-based generation proves more resistant to unlearning both in theory and practice. The paper establishes bounds on belief updates, provides a lower bound illustrating the difficulty of changing priors, and empirically confirms these findings on controlled binary tasks. Experiments demonstrate that memorization-like (random) tasks are unlearned faster and with stable retain accuracy, whereas rule-like tasks experience global disruption and slower convergence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper uniquely focuses on the distinction between learning-based and memorization-based generation in LLMs, which is an important aspect of understanding how the model behaves.\n- The paper provides formal theoretical analysis, offering bounds on belief updates and illustrating the challenges of changing priors."}, "weaknesses": {"value": "- While the research offers valuable theoretical insights, the empirical section has a limited connection to a practical LLM unlearning scenario. In the experiments, the tasks (classifying 2-dimensional points into binary labels) are quite simple and may not accurately represent how LLMs learn and memorize information in real situations. First, in practice, knowledge in LLMs is learned through sentences both explicitly and implicitly, rather than explicit question prompts like \"What is the label for this input?\\n Input: 62 87 \\n Label:\". Second, practical LLM training involves a mix of learning and memorization, not just one or the other as in the experiments. This greatly limits how well the findings apply to more complex, real-world cases. Controlled experiments are certainly acceptable (and even helpful), but the paper could improve by including experiments on more realistic language tasks (such as learning and unlearning factual knowledge) to better demonstrate the practical implications of the theoretical results, since the paper focuses on **LLM unlearning**, not traditional unlearning scenarios for non-LLMs.\n- In the empirical section, the paper assumes that if the LLM is given a dataset with regularity (i.e., LINEAR, RECTANGLE), it will learn the underlying rule, whereas if given random data (i.e., RANDOM), it will memorize. However, in practice, LLMs can exhibit a mixture of learning and memorization even on datasets with regularities, depending on factors like model architecture, training regimen, and data complexity. This assumption may oversimplify how LLMs process information, raising questions about the validity of the conclusions drawn from these experiments.\n- Minor: The term \"hypothesis testing\" is used in an uncommon way, which might confuse readers who are familiar with traditional concepts of hypothesis testing. The paper explores the model's unlearning behavior under two hypothetical data generation processes, rather than determining which of the two processes the model functions best under. Clarifying this terminology would improve readability.\n- Minor: The paper lacks conclusion and limitation sections. Including these sections would help summarize key findings and discuss the potential limitations of the study."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "glMWjfgJGQ", "forum": "25F5ot3UWg", "replyto": "25F5ot3UWg", "signatures": ["ICLR.cc/2026/Conference/Submission19447/Reviewer_X47C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19447/Reviewer_X47C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994494018, "cdate": 1761994494018, "tmdate": 1762931365375, "mdate": 1762931365375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}