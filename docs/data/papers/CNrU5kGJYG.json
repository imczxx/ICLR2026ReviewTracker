{"id": "CNrU5kGJYG", "number": 15464, "cdate": 1758251660347, "mdate": 1759897305359, "content": {"title": "TrojanTO: Action-Level Backdoor Attacks Against Trajectory Optimization Models", "abstract": "Trajectory Optimization (TO) models have achieved remarkable success in offline reinforcement learning (offline RL). However, their vulnerability to backdoor attacks remains largely unexplored. We find that existing backdoor attacks in RL, which typically rely on reward manipulation throughout training, are largely ineffective against TO models due to their inherent sequence modeling nature and large network size. Moreover, the complexities introduced by high-dimensional continuous action further compound the challenge of injecting effective backdoors. To address these gaps, we propose TrojanTO, the first action-level backdoor attack against TO models. TrojanTO is a post-training attack and employs alternating training to forge a strong connection between triggers and target actions, ensuring high attack effectiveness. To maintain attack stealthiness, it utilizes trajectory filtering to preserve the benign performance and batch poisoning for trigger consistency. Extensive evaluations demonstrate that TrojanTO effectively implants backdoors across diverse tasks and attack objectives with a low attack budget (0.3\\% of trajectories). Furthermore, TrojanTO exhibits broad applicability to DT, GDT, and DC, underscoring its scalability across diverse TO model architectures.", "tldr": "Post-training, action-Level Backdoor Attacks to Decision Transformer", "keywords": ["Backdoor attack", "Decision Transformer", "Offline RL"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ad10028b9d1cc050f22e24c9e7e90c46353cd24.pdf", "supplementary_material": "/attachment/db010155918bed3666f566485725347f1ce5fae3.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes the first method for backdoor attacks against Trajectory Optimization methods (TO). The authors first explore some challenges in attacks against TO models along with challenges in poisoning continuous action space RL in general. Using these motivations they develop their method, TrojTO, which iteratively optimizes the agent's policy along with the trigger such that the agent learns some desired backdoor attack behavior. This poisoning occurs at a post-training, fun-tuning phase over which the adversary has full control. They evaluate their method on MuJoCo tasks and compare to some related works on offline data poisoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* As far as I know, this is the first paper studying backdoor attacks against TO models.\n\n* The proposed threat model is reasonable, though it isn't unique to TO models or RL in general.\n\n* The paper properly places itself within the related work.\n\n* I greatly appreciate sections 3.1 and 3.2 since the choice of target action and trigger design are often over looked in the study of backdoor attacks in general, especially against continuous control agents. The findings are also consistent with informal observations I have made while studying backdoor attacks.\n\n* Section 3.3 does a good job of motivating the method. \n\n* The proposed method is fairly simple but seems to work well. \n\n* Evaluation is fairly comprehensive with many ablations in the appendix. The effort is appreciated."}, "weaknesses": {"value": "1.) Even though the threat model is reasonable, I do think it is still fairly strong as the adversary has full control over the fine-tuning phase of the attack. Given this it would have been nice if the authors could devise an \"optimal\" attack which future attacks, with weaker threat models, could try to replicate.\n\n2.) The paper spends a decent amount of time designing and figuring out what the optimal trigger is. In my opinion the trigger is a constraint of the test time environment and should therefore be determined based upon real world feasability. Therefore, I think this makes the authors' chosen evaluation environments somewhat mislead. In MuJoCo envionments one is unable to modify the agent's observations at test time to exploit the trigger without manipulating their internal, proprioceptive sensors. Therefore the trigger requires invasive levels of test time access to exploit. I think evaluating on MuJoCo environments like this is fine for evaluation purposes, but it would be insightful to additionally evaluate on image-based versions of MuJoCo since images are easier to manipulate at test time. \n\n### Minor Weaknesses\n\n* I understand that CP is the authors' chosen metric, but I would appreciate if they could simply underline the best performer in terms of ASR and BPT for each environment. To readers like myself CP is a difficult metric to interpret while ASR and BTP separately are easier to reason about. Therefore having underlines will make such a comparison much easier without compromising the authors' chosen metric.\n\n* Missing table reference around line 448\n\n* Typo in heading of main body section 2.4 and Appendix section C, should probably be \"Experimental Setup\""}, "questions": {"value": "* Where does the adversary obtain the fine-tuning dataset they manipulate? (Important)\n\n* Which part of the TrojTO method do you think is most important to its success?\n\n* Would Baffle perform similarly to TrojTO if it also used trigger optimization?\n\n* Why would an adversary choose to target and release a TO model rather than a standard RL model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FaphdeU71M", "forum": "CNrU5kGJYG", "replyto": "CNrU5kGJYG", "signatures": ["ICLR.cc/2026/Conference/Submission15464/Reviewer_f45h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15464/Reviewer_f45h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761572744, "cdate": 1761761572744, "tmdate": 1762925754219, "mdate": 1762925754219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TrojanTO, a post-training, action-level backdoor attack framework targeting Trajectory Optimization (TO) models used in offline reinforcement learning. While previous backdoor attacks in reinforcement learning largely focused on manipulating rewards during training, TrojanTO departs from this paradigm by conducting post-training attacks that directly modify pretrained TO models. The authors begin by analyzing the roles of actions, states, and rewards in determining the vulnerability of TO models and find that reward manipulation has negligible influence, while action–trigger coupling is the key factor governing attack success. Building upon these insights, TrojanTO incorporates three main mechanisms—trajectory filtering, batch poisoning, and alternating training—to achieve both high attack effectiveness and stealth. The framework is evaluated extensively across six D4RL benchmark tasks and three representative TO architectures (Decision Transformer, Graph Decision Transformer, and Decision ConvFormer). Results demonstrate that TrojanTO achieves superior composite performance (CP = 0.701) with only 0.3% data poisoning, significantly outperforming existing baselines such as Baffle and IMC. Overall, the paper provides the first systematic study of action-level, post-training backdoors in trajectory optimization models, revealing an underexplored and practically relevant threat vector in modern reinforcement learning systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of this paper lies in its novel problem formulation and practical significance. By shifting the focus from training-time to post-training attacks, TrojanTO highlights an emerging vulnerability relevant to supply-chain security in large pretrained RL models. The analysis of key contributing factors—action, state, and reward—is methodical and provides new insights into the structural vulnerabilities of TO models. Methodologically, TrojanTO’s design elegantly combines trajectory filtering to preserve benign performance, batch poisoning for trigger consistency, and alternating optimization for co-training the trigger and model. This composition demonstrates thoughtful engineering and strong empirical grounding. The experimental section is comprehensive and robust: evaluations span diverse environments (locomotion, navigation, manipulation) and architectures, with consistent improvements in attack success rate (ASR) and stealth (BTP). Extensive ablations further clarify the role of each module, providing transparency into design choices. The paper is clearly written, well-organized, and thoroughly supported by appendices, including detailed algorithms and hyperparameter descriptions that enhance reproducibility. In summary, TrojanTO represents a well-executed and relevant contribution that substantially advances understanding of backdoor threats in offline RL."}, "weaknesses": {"value": "Despite its strong empirical performance, the work is somewhat limited in theoretical depth and scope of generalization. The core algorithmic components—particularly alternating optimization via MI-FGSM and trajectory filtering—are based on established techniques, and the paper lacks a unifying theoretical framework to quantify stealth–efficacy trade-offs or to analyze convergence guarantees. Additionally, the experimental evaluation, while extensive, remains confined to the D4RL suite, which primarily includes simulated continuous-control environments. Demonstrating TrojanTO on more realistic or high-dimensional embodied tasks (e.g., robot manipulation in MuJoCo or real-world datasets) would strengthen the practical impact. Another limitation is the absence of defense or detectability analysis. Since the authors motivate TrojanTO as a supply-chain threat, it would be valuable to assess how easily such attacks could be detected or mitigated by standard backdoor defenses (e.g., activation clustering or fine-tuning). Finally, some assumptions—such as full access to model weights and perfect control over trigger insertion—are strong and may not hold under more restricted adversarial conditions. These issues do not undermine the main contribution but suggest avenues for extending and deepening the work."}, "questions": {"value": "Several clarifications would improve understanding of the work. First, how sensitive is TrojanTO’s success to the weighting parameter λ that balances the poisoned and clean losses, and how does it affect the trade-off between attack success and benign performance? Second, what are the computational and memory costs of alternating training relative to standard fine-tuning or model-editing attacks such as TrojanEdit (Guo et al., 2024b)? Third, can the authors discuss the attack’s robustness in black-box or transfer settings, where the attacker does not have full model access? Fourth, how does TrojanTO perform if the model undergoes partial fine-tuning or pruning after attack insertion—does the backdoor persist or degrade? Finally, are there practical countermeasures or defensive signals (e.g., gradient or activation anomalies) that defenders could exploit to identify TrojanTO-style backdoors during model validation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0atOPAkjvi", "forum": "CNrU5kGJYG", "replyto": "CNrU5kGJYG", "signatures": ["ICLR.cc/2026/Conference/Submission15464/Reviewer_d4Cw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15464/Reviewer_d4Cw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800146290, "cdate": 1761800146290, "tmdate": 1762925753055, "mdate": 1762925753055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of backdoor attacks against Trajectory optimization algorithms that try to predict the next actions based on partial trajectory and expected reward to go inputs. The authors find that the traditional method of reward manipulation backdoor attacks is not very effective for TO algorithms and instead proposes a fine-tuning method to inject backdoor using state and action manipulation. Their alternative optimization algorithm utilizes trajectory filtering with batch poisoning to install the backdoor in the policy. The authors present extensive experiments to support the effectiveness of their methods in various TO domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is among the first few to study backdoor attacks in Trajectory Optimization settings to expose their vulnerability to backdoor attacks.\n2. They propose an effective method called TrojanTO to successfully install a backdoor in pretrained policy through fine-tuning.\n3. The authors have presented good experimental results to validate the effectiveness of their algorithm using dataset from different environments."}, "weaknesses": {"value": "1. TO is nothing but a supervised learning problem where the input space is a sequence and output space is an action. As such, attacking a TO algorithm is same as attacking a supervised sequential model that has been studied a lot in the past. So, the correct comparison should be to compare it to a backdoor attack method in a supervised learning setting.\n2. It is not surprising that reward manipulation does not lead to successful backdoors in TO because TO never tries to optimize for rewards so a reward signal cannot be used to install a bad behavior. Rather it takes rewards-to-go as input and only tries to predict an action that will achieve the corresponding reward input. So, direct action manipulation in the triggered state should be the way to go and there appears almost no value of doing an experiment to refute that the reward manipulation can be effective. \n3. The paper also lacks a related works section. The authors have failed to articulate the novelty of their method compared to prior works and so it's difficult to position the contribution of this paper well."}, "questions": {"value": "1. Why is attack on TO different from a standard supervised learning attack(say on sequential models)? \n2. Have you done any comparisons with backdoor attack algorithms in supervised learning settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "p341y5n9ow", "forum": "CNrU5kGJYG", "replyto": "CNrU5kGJYG", "signatures": ["ICLR.cc/2026/Conference/Submission15464/Reviewer_ozCU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15464/Reviewer_ozCU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951719371, "cdate": 1761951719371, "tmdate": 1762925752522, "mdate": 1762925752522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the security vulnerabilities of Trajectory Optimization (TO) models in offline reinforcement learning (RL) and introduces TrojanTO, the first action-level backdoor attack targeting such models. While backdoor attacks have been well-studied in traditional RL agents (which rely on reward-driven policy updates), the paper argues that these methods are largely ineffective against TO models such as Decision Transformer (DT), Goal Decision Transformer (GDT), and Decision ConvFormer (DC), due to their sequence modeling nature (transformer-based trajectory modeling rather than reward-based learning), large network capacity, and continuous action spaces that complicate precise manipulation. TrojanTO is proposed as a post-training attack that forges strong couplings between triggers and target actions without interfering with the training pipeline.  Experiments show that TrojanTO achieves high attack success rates (ASR) across multiple TO model architectures and tasks while maintaining performance on benign data"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The TrojanTO design consists of three key components: Alternating Training - reinforces the association between the trigger pattern and target action; Trajectory Filtering - maintains benign performance by filtering non-critical trajectories; and Batch Poisoning - ensures trigger consistency and stealthiness across evaluation conditions. This novel combination of ideas makes TrojanTO effective and data-efficient for injecting post-training, action-level backdoors in offline trajectory optimization models. \n\nThe paper conducts a systematic empirical study of how action, state, and reward manipulations affect TO models, leading to the finding that reward manipulation is largely ineffective while action-state triggers dominate attack success.\n\nThe paper has strong empirical evaluation and the attack requires very small poisoning budget."}, "weaknesses": {"value": "The paper does not discuss possible defenses or mitigation strategies against TrojanTO. Even a brief evaluation of trigger detectability, model auditing, or defensive retraining would help position this research within the broader context of AI security and trustworthiness. The appendix B.1 has some discussion on defense but a more focuesed synopsis of experiments could be added to the main paper."}, "questions": {"value": "Have you tested whether methods such as fine-tuning, weight pruning, spectral analysis, or trigger anomaly detection can mitigate TrojanTO? Some of these are very briefly mentioned in B.1 appendix but it is unclear if there was a statistically significant observation made in the experiments. \n\nCould TrojanTO be executed on large foundation-scale TO models (e.g., Gato, RT-2)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DWnW9tLLPH", "forum": "CNrU5kGJYG", "replyto": "CNrU5kGJYG", "signatures": ["ICLR.cc/2026/Conference/Submission15464/Reviewer_Q3rq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15464/Reviewer_Q3rq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960937680, "cdate": 1761960937680, "tmdate": 1762925751925, "mdate": 1762925751925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}