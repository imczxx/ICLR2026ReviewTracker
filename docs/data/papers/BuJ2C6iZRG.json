{"id": "BuJ2C6iZRG", "number": 19222, "cdate": 1758294544940, "mdate": 1759897051646, "content": {"title": "Stable-SPAM: How to Stably Train Large Language Models in 4-Bit", "abstract": "This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, a recent optimizer featuring momentum reset and spike-aware gradient clipping, achieves the best performance across various bit levels, but struggles to stabilize gradient norms, requiring careful learning rate tuning. \nTo address these limitations, we propose **Stable-SPAM**, which incorporates enhanced gradient normalization and clipping techniques. In particular, **Stable-SPAM** $(1)$ adaptively updates the clipping threshold for spiked gradients by tracking their historical maxima; $(2)$ normalizes the entire gradient matrix based on its historical $l_2$-norm statistics; and $(3)$ inherits momentum reset from SPAM to periodically reset the first and second moments of Adam, mitigating the accumulation of spiked gradients. Extensive experiments show that **Stable-SPAM** effectively stabilizes gradient norms in 4-bit LLM training, consistently delivering superior performance compared to Adam and SPAM across model sizes from LLaMA-130M to LLaMA-7B. Notably, our 4-bit LLaMA-1B model trained with **Stable-SPAM** outperforms Adam by up to $3.1$ perplexity. Furthermore, when both models are trained in 4-bit, **Stable-SPAM** achieves the same loss as Adam while requiring only about half the training steps. Code is submitted.", "tldr": "We propose Stable-SPAM, a spike-aware optimizer for training stably LLMs in 4-Bit", "keywords": ["4-bit training", "training stability", "loss spike", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7de13ec29950188323c6c8488ee86aa45762c969.pdf", "supplementary_material": "/attachment/fafb7c765b471c000e46e6871a9af7958351c4ee.zip"}, "replies": [{"content": {"summary": {"value": "This paper has two main contributions.  First, it analyzes the behaviour of different optimizers when training with lower precision, testing sensitivity to different learning rates, and observing gradient norm spikes and training divergences at lower precision levels.  Second, it introduces the Stable-SPAM optimizer, an enhancement of the SPAM optimizer, but with new techniques to mitigate the gradient norm spikes that were observed with vanilla SPAM training.  Results at different model sizes show lower perplexity compared to other optimizers."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "It is well-motivated to improve the efficiency of LLM training, both through lower precision (reduced memory) and through mitigating instabilities.\n\nStudying sensitivity to LRs at different precision levels is interesting, and continues a line of work (e.g., Zhao et al 2024b's work, Wortsman et al's work) into an interesting area.\n\nI found the Abstract and Introduction relatively easy to follow, and I absorbed the key points, and these led into Section 2 nicely, which provided the fuller description."}, "weaknesses": {"value": "Overall, when we get into the details of the comparison between the models, it seems like the models are not always being trained to compute-efficient token budgets, and gradient clipping is only being applied in some cases (even though I believe it to be standard, if not always mentioned) and I'm confused about how many tokens different results were trained on, and whether different warmups were used in different models, and whether the models were given their best chance (e.g., just using AdamW's default weight decay setting, yet tuning all the Stable-SPAM hyperparameters on the 60M model scale).  What if larger weight decay is all we need for AdamW to shine at lower-bits?\n\nI would have thought that gradient clipping is standard practice with LLM training (let alone low-precision training), so anywhere in the paper a result is reported without “GradClip”, I feel like this is a straw man baseline.  And that’s a lot of the figures/tables in the paper!\n\nI’m confused about the presentation\n- The abstract said gains from 130M to 7B, but then the experiments mention testing from 60M, so do the lower models not show gains?  You might mention that specifically.  I think maybe 60M was used for hyperparameter tuning?  We can make this clear when 60M is first mentioned.\n- The intro doesn’t mention Lion, but it’s tested later, it’s just weird to not mention that specific one (although technically you mentioned Adam but not AdamW)\n- “As shown in Table 1, the perplexity gap between BF16 (Adam) and INT4/FP4 (Adam) exceeds 1.5” – I’m confused, Table 1 doesn’t show BF16, right?\n- I put some other confusions into Nitpicks below.\n\nSoundness:\n- Note it’s my job as a reviewer to make sure the evaluation is done in practically-relevant scenarios, e.g., at the point where models are trained to compute-optimal levels or higher (20 tokens-per-parameter or greater).  But I’m having a very difficult time tracing how many tokens each model was trained on.  I have further points on this below.\n- So only SPAM uses LR warmup of 150 steps?  But then I see warmup for 2000 steps in Appendix B. Can you clarify?\n- I am not sure about using the HPs from the original paper for Adafactor, like, that was a long time ago, compared to SPAM, tuned in 2025, and Stable-SPAM, tuned in this very paper.\n- Why does Figure 4 only go to 4K steps?  Why does Figure 3 go to 5K steps?  Do these use the same batch size, LR, etc., as the other experiments?\n- So we report a batch size in tokens of 512*256.  We say 350M models train for 6.6B tokens, so that would be 50K steps, right?  And 1B should be even more.  But then Figure (1) only shows results until 20K steps.  This is really undertrained, right, compared to the Chinchilla compute-optimal setting of around 20 tokens-per-parameter (TPP).  In Table 1, the 1B model trains for 7.7, but then in Table 2, they train for 11.6B.  Why?\n- The 1B models are also really undertrained, only training to 7.7 TPP, which is below the level anyone would actually train an LLM to, since a smaller model could be trained to a lower loss with the same compute by training on more tokens.  So I’m not sure how practically-relevant these results are. I.e., when you say, “Notably, Stable-SPAM performs particularly well with larger models, such as LLaMA-350M and LLaMA-1B, showcasing its strong potential for large-scale training,” could we interpret this as, “Stable-SPAM performs particularly well at compute-inefficient settings that no one would actually train to in practice”?\n- I want to be clear about this: as a practitioner, I would only ever train large models to compute-efficient TPP ratios.  If I read your paper, I might think I can train stably at 4-bit if I used Stable-SPAM.  But you do not test compute-efficient TPP ratios with your larger models.  You mention Fishman’s finding that as we train on more tokens, low-bit data formats may struggle, but your results don’t show this --- is it because you don’t evaluate in these regimes, or because your methods are robust to this?  The practitioner needs to know.\n- Figure 5, we only have 20K update steps, how many tokens-per-parameter is this?\n\nNitpicks:\n- Be nice if you referred to Figure (1) in the intro.\n- Recent studies (Zhao et al., 2024b; Wortsman et al., 2023b; Huang et al., 2025; Takase et al., 2023; Wortsman et al., 2023b) – multiple Wortsman citations here.\n- Be nice to report the token counts in Figure (1) as well so we can see if this is a practically-relevant regime\n- If we could split Figure (2) left from the other ones it would help a lot, like it seems the legend below doesn’t apply to this one, it’s just confusing, and also, what’s the point since these same lines are in the three other figures?\n- Figure (4), which optimizer is this?  I’m looking for spots Figure 4 is referred to in the text, and I get “The final validation loss [of Stable-SPAM] is presented in Figure 4,” but I assume this is a typo?\n- Your description of gradient clipping in Section 5: is it really “rescaling” the gradient or just clipping dimensions that go above the cutoff?"}, "questions": {"value": "- In all the figures, does the LR decay to 10% of the peak at the very final step as shown in the figure?  E.g., Figure 1, Figure 4, Figure 3, Figure 5, Figure 6, Figure 7.\n\n- Do all of these training runs use the same batch size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SGuWluZLjr", "forum": "BuJ2C6iZRG", "replyto": "BuJ2C6iZRG", "signatures": ["ICLR.cc/2026/Conference/Submission19222/Reviewer_zR6K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19222/Reviewer_zR6K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517119649, "cdate": 1761517119649, "tmdate": 1762931208102, "mdate": 1762931208102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new optimizer, Stable-SPAM, which builds on the previous SPAM variant by adding gradient normalization and adaptive clipping to stabilize gradient norm spikes during training. However, the reported empirical results are unreliable and questionable, likely due to implementation issues.\n\nThe training configurations are only partially disclosed, leaving key details unclear—such as whether FP4 quantization is applied to gradients, the precision of optimizer states and computations, and whether the BF16 baseline follows standard mixed-precision practice or forces all operations into BF16. These ambiguities raise concerns about reproducibility and interpretation.\n\nMoreover, the paper’s title is misleading. As noted in line 265, the experiments employ “quantization-aware training strategies” derived from LLM-FP4 (2023), which explicitly states that \"quantizing both weights and activations ..., in a post-training manner\" rather than full 4-bit training, yet the tile suggests the method are general for true 4-bit training."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The proposed methods are simple and straightforward, combining adaptive gradient normalization, adaptive spike-aware clipping (with limited novelty), and the momentum reset mechanism from SPAM.\n\nHowever, given the weaknesses (stated in next part) and questionable experimental reliability, it is difficult to draw firm conclusions. It remains possible that the method genuinely stabilizes training—reducing gradient spikes in a proper BF16 mixed-precision setup with master weights and high-precision optimizer states—but it is equally plausible that the observed stability arises from implicit effects such as an effectively larger learning rate introduced by normalization."}, "weaknesses": {"value": "**Implementation, baseline and FP4 training setup**\n\n- After examining the codebase, it appears that the authors perform a full BF16 training with\n```python\nmodel = model.to(dtype=torch.bfloat16)\noptimizer = torch.optim.Adam(trainable_params, lr=args.lr, weight_decay=args.weight_decay)\nloss = model(**batch, labels=labels).loss\n```\nas a result, optimizer states (and likely also intermediate activations e.g., softmax outputs) are in BF16. Since starting pytorch 1.13+, it uses\n```python\nstate['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n```\nwhich will be same precision as the parameters. However, the standard practice is BF16 mixed-precision training (e.g., AMP or Megatron-style) that closely matches FP32 performance. Pure BF16 training with BF16 optimizer states is known to be unstable, especially when $\\beta_2$ is close to 1.0, as stated in [Yu et al., 2024]. \n\nSeveral observations suggest that the experiments indeed use BF16 optimizer states (for all experiments):\n\ni) In Table 10, BF16 (Adam) fails to match FP32 (Adam).\n\nii) In Table 5, Stable-SPAM (FP4) even outperforms BF16 Adam.\n\niii) In Table 2, BF16 Stable-SPAM and BF16 Adam + GradClip outperform plain BF16 Adam by a large margin.\n\nThese patterns indicate that BF16 Adam serves as a weak and even bad baseline, and the proposed methods primarily stabilize training under poor BF16 optimizer precision—an issue that practitioners typically avoid using standard mixed precision or techniques with Kahan summation or stochastic rounding.\n\n**FP4-E1M2 vs INT4 equivalence**\n\n- The paper studies FP4-E1M2 and INT4, but FP4-E1M2 is effectively equivalent to INT4 when scaled by 4.0. If the implementation follows LLM-FP4 (2023), the only difference is rounding mode—there the FP4-E1M2 uses floor rounding, whereas INT4 uses round-to-nearest. This equivalence diminishes any claimed novelty of FP4 usage in the paper.\n\n**Why not use AdamW with weight decay?**\n\n- Weight decay is standard in large-scale training, and AdamW is the de-facto optimizer rather than vanilla Adam. The omission raises the question of whether the proposed method is incompatible with weight decay or whether the comparison is incomplete.\n\n**Missing Baseline optimizer configurations**\n\n- Only Stable-SPAM hyperparameters (Tables 7–8) are reported, while the baseline optimizer settings are missing. Are learning rates shared or separately tuned? What values of $\\beta_1, \\beta_2$ are used? These omissions make the comparisons unclear and hinder reproducibility.\n\n**Generality of “4-bit training”**\n\n- As stated in line 265, the experiments rely on “quantization-aware training strategies” from LLM-FP4 (2023), which explicitly perform post-training quantization of weights and activations rather than full 4-bit training. Yet, the title suggests a general 4-bit training framework. The authors should clarify exactly which components are quantized and, if it is QAT, make this explicit in the title and introduction.\n\n**Convergence and experimental reliability**\n\n- Figures 1, 4, 5, and 6 show incomplete convergence; curves may align with more training. Are all models initialized identically (weights and optimizer states)? Were multiple runs with different seeds performed for smaller models? At least one long-token, large-scale run (chinchilla optimal) should be provided to verify stability and scaling behavior.\n\nYu et al. \"Collage: Light-Weight Low-Precision Strategy for LLM Training.\" Forty-first International Conference on Machine Learning."}, "questions": {"value": "- line 300, it refers to Figure 4, and jumps to Table 4, which is actually a figure. Also, for this study, do you have any insights on why W3A3 shows the smallest gap?\n\n- can the authors plot the equivalent lr (treating the normalization as if scaling the un-normalized gradients & updates), across iterations for stable-SPAM and compare with the original lr schedule?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "See the details in the weaknesses part; the authors may not have realized that there were bugs & unreliablity in their experiments."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MpnIUbzh6s", "forum": "BuJ2C6iZRG", "replyto": "BuJ2C6iZRG", "signatures": ["ICLR.cc/2026/Conference/Submission19222/Reviewer_vRDd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19222/Reviewer_vRDd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893159931, "cdate": 1761893159931, "tmdate": 1762931207608, "mdate": 1762931207608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the stability challenges of training large language models using 4-bit precision and proposes Stable-SPAM as a solution. The authors conduct a comprehensive evaluation of recent optimizers (Adam, Adafactor, Adam-mini, and SPAM) under 4-bit training conditions, revealing that low-bit precision significantly increases sensitivity to learning rates and causes unstable gradient norms that can lead to training divergence. While SPAM demonstrates strong performance across different bit levels, it still suffers from gradient instability requiring careful hyperparameter tuning. To address these limitations, Stable-SPAM incorporates two novel techniques: Adaptive Spike-Aware Clipping (AdaClip), which dynamically adjusts clipping thresholds based on historical gradient maxima, and Adaptive Gradient Norm (AdaGN), which normalizes gradients using historical $\\ell_2$-norm statistics. The method also inherits SPAM's momentum reset mechanism. Experiments across model sizes (LLaMA-130M to LLaMA-7B) demonstrate that Stable-SPAM effectively stabilizes 4-bit training, achieving superior performance and training efficiency compared to existing approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and clearly structured, making the technical content accessible and easy to follow.\n- The comprehensive evaluation across multiple optimizers (Adam, Adafactor, Adam-mini, SPAM) and model scales (130M to 7B parameters) provides nice observations into 4-bit training dynamics.\n- The proposed techniques—Adaptive Spike-Aware Clipping and Adaptive Gradient Norm—sounds reasonable."}, "weaknesses": {"value": "-  Experiments are confined to small-scale settings (1B tokens, perplexity >10) using only LLaMA-2 architecture. This resembles continual pre-training rather than full-scale pre-training scenarios. The practical significance remains unclear, as the paper lacks evidence that addressing loss spikes meaningfully improves downstream task performance—evaluation perplexity alone is insufficient validation.\n- The proposed methods are incremental extensions of SPAM rather than fundamentally novel contributions, which diminishes the technical significance.\n- The paper provides insufficient analysis of the root causes behind 4-bit training instability. Critical questions remain unanswered: Are loss spikes primarily caused by 4-bit model parameters, activations, or their interaction? Deeper diagnostic insights would strengthen the contribution beyond empirical observations."}, "questions": {"value": "- What is the actual wall-clock time speedup achieved by 4-bit training compared to BF16/FP16? A comprehensive comparison plotting training time versus performance metrics (evaluation loss and downstream task accuracy) would clarify whether 4-bit training offers practical advantages beyond memory savings. Without demonstrated time-to-accuracy benefits, the motivation for 4-bit training remains weak.\n- Does increasing batch size mitigate the loss spike issue? The observed instability appears partially attributable to stochastic gradient noise from small batches. If larger batch sizes naturally stabilize training, this would suggest a simpler solution than algorithmic modifications. Investigating this relationship could reveal whether the proposed techniques are necessary or if standard practices suffice for stable 4-bit training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u14FwGnWu8", "forum": "BuJ2C6iZRG", "replyto": "BuJ2C6iZRG", "signatures": ["ICLR.cc/2026/Conference/Submission19222/Reviewer_YxM2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19222/Reviewer_YxM2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064492689, "cdate": 1762064492689, "tmdate": 1762931207220, "mdate": 1762931207220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Stable-SPAM, an optimizer designed to stabilize gradient norms in 4-bit LLM training through adaptive clipping, gradient normalization, and momentum reset. It consistently outperforms Adam and SPAM across model scales, achieving equal or better performance with up to half the training steps."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work develop a way to stabilize fp4 training upon SPAM, under low precision 4 bit and presented through experiment study with reasonable hyperparameter study on gamma 1 2 3 (of their algroithm)"}, "weaknesses": {"value": "* mxfp4 training missing, mxfp8 missing, stochasitc rounding missing, e.g., https://arxiv.org/abs/2502.20586 given mxfp8/4 are real fast , more stable and usable data-type supported from hardware for training.\n* lack of SOTA mixed precision missing (or please clarify) is SR applied to stablize, e.g., https://arxiv.org/abs/2502.20566 ?\n* baseline missing or distorted, what if applying technique even to bf16? not fair comparision, \n* hyperparameter tuning is not clear, clearer explanation how to adopt, like hyperparemter transfer from smaller scale, e.g., muP, was it tuned so that best to best comparison?\n* not enough token training, model size,\n* weight decay is critical, especially for large LLM, not considered? \n* what about recent Muon optimizer\n* any theoretical analysis?"}, "questions": {"value": "See weakness about questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5TliFm034d", "forum": "BuJ2C6iZRG", "replyto": "BuJ2C6iZRG", "signatures": ["ICLR.cc/2026/Conference/Submission19222/Reviewer_PV1f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19222/Reviewer_PV1f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762585348440, "cdate": 1762585348440, "tmdate": 1762931206850, "mdate": 1762931206850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}