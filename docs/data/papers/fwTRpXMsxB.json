{"id": "fwTRpXMsxB", "number": 10421, "cdate": 1758170903637, "mdate": 1759897651653, "content": {"title": "GT-Space: Enhancing Heterogeneous Collaborative Perception with Ground Truth Feature Space", "abstract": "In autonomous driving, multi-agent collaborative perception enhances sensing capabilities by enabling agents to share perceptual data. A key challenge lies in handling heterogeneous features from agents equipped with different sensing modalities or model architectures, which complicates data fusion. Existing approaches often require retraining encoders or designing interpreter modules for pairwise feature alignment, but these solutions are not scalable in practice. To address this, we propose GT-Space, a flexible and scalable collaborative perception framework for heterogeneous agents. GT-Space constructs a common feature space from ground-truth labels, providing a unified reference for feature alignment. With this shared space, agents only need a single adapter module to project their features, eliminating the need for pairwise interactions with other agents. Furthermore, we design a fusion network trained with contrastive losses across diverse modality combinations. Extensive experiments on simulation datasets (OPV2V and V2XSet) and a real-world dataset (RCooper) demonstrate that GT-Space consistently outperforms baselines in detection accuracy while delivering robust performance.", "tldr": "", "keywords": ["Collaborative perception", "multi-modality", "multi-agent", "sensor fusion"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9aca54e38c678ccbe761cb809fb5c3d1799f5c6a.pdf", "supplementary_material": "/attachment/95b7b98dad86a85715ef51d35bd19421e4c165bf.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes a heterogeneous collaborative perception framework named GT-Space. Instead of retraining or creating an \"interpreter\" for each heterogeneous agent, this framework projects the BEV (Bird's-Eye View) features from each vehicle's agent into a common feature space constructed from the ground truth, followed by fusion and detection."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper has a clear motivation, aiming to solve the problem of needing to retrain for each new agent, thereby increasing the system's scalability.\n- The proposed method is designed to be plug-and-play, is orthogonal to other existing methods, and can be used in conjunction with them.\n- The experimental results demonstrate that the framework can effectively improve perception performance in a heterogeneous environment."}, "weaknesses": {"value": "- The framework heavily relies on the \"ground-truth feature space\". The construction of GT-Space depends on ground truth. During inference, since ground truth is unavailable, the model can only \"imagine\" a space that aligns with it. This requires the training and deployment distributions to be sufficiently close; otherwise, the alignment will drift. The paper does not quantify the robustness under such distribution shifts.\n- The authors freeze the local encoders and detection heads, promoting a \"plug-and-play\" capability. However, this also limits the potential performance ceiling that could be achieved through end-to-end joint fine-tuning.\n- The authors claim that when adding a new, unseen agent, only the projector for that agent needs to be trained, thus enabling scalable collaboration with minimal deployment cost by deploying only a lightweight adapter. However, the paper does not explicitly report the comparison of training computation/time for \"adding a new agent\"; it only reports inference latency.\n- The implementation process is not clearly described, containing some errors or confusing points. The lack of open-sourced code makes the paper difficult to reproduce."}, "questions": {"value": "- The object-level contrastive learning \"aggregates grids within a bounding box and then computes similarity/cross-entropy.\" Larger objects, which cover more grids, might have a greater weight in the loss function. Is scale balancing or re-weighting necessary?\n- In Figure 1, the caption explicitly states, \"Interpreter-based alignment: local encoder and detection head (denoted as D) are frozen.\" However, diagram (b) shows that the local encoder is trainable. Furthermore, why do agents E#2 and E#3 point to the same detection head D?\n- In Figure 2, why is the similarity loss calculated between the GT feature and the BEV feature? The BEV feature is generated by a frozen encoder, so is calculating a loss on it meaningful? Shouldn't the loss be calculated on the feature after it has passed through the projector?\n- Equations (5) and (6) feel somewhat vague and lack rigor. I can understand the author's intention, but the formulation seems to have a circular definition. $\\Phi_{a}$ is presented as the function to be solved in the first equation, yet it is used as a known function within the loss in the second equation.\n- In Equation (8), is there a missing negative sign? In $L_{m,m'} = \\sum_{B} \\sum_{c} \\sum_{B'} \\log\\left( \\frac{\\exp(s_{B,c,B'})}{\\sum_{l} \\exp(s_{B,c,B'})} \\right)$, is the summation in the denominator performed over all $l$? However, the term inside the exponential function being summed is $\\exp(s_{B,c,B'})$, which uses the index $B'$.  Perhaps the correct formula is something like $L_{m,m'} = -\\sum_{B} \\sum_{c \\in \\text{cells}(B)} \\log \\frac{\\exp\\left(s_{B,c,B}\\right)}{\\sum_{l \\in \\mathcal{B}} \\exp\\left(s_{B,c,l}\\right)}$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "- The citation format in the paper is clearly inconsistent. For example, in Table 2, it is evident that some papers have year information while others do not. It is recommended that the authors re-check and unify the citation format.\n- It is recommended that the authors provide a detailed overview of the GT-Space framework in Section 3.2. This should include a comprehensive explanation of the workflows for pre-training, training, inference, and the process of integrating a new agent. The input, specific operations, output, and the meaning of each stage should be clearly articulated.\n- It is recommended that the authors provide more detailed explanations and descriptions for the formulas and symbols used in the paper, especially when new symbols are introduced. If the code is not open-sourced, the formulas in the paper must be sufficiently comprehensive to ensure that readers can understand the method.\n- It is suggested to add a comparison of system complexity in the \"Newly Added Agent\" section.\n- It is suggested to include a sensitivity analysis for parameters such as the temperature $\\tau$."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zC2JpfWhYY", "forum": "fwTRpXMsxB", "replyto": "fwTRpXMsxB", "signatures": ["ICLR.cc/2026/Conference/Submission10421/Reviewer_w8N3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10421/Reviewer_w8N3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760945446546, "cdate": 1760945446546, "tmdate": 1762921730239, "mdate": 1762921730239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GT-Space, a novel framework that constructs a common feature space from ground truth labels, offering a unified reference for feature alignment among heterogeneous agents. This approach eliminates the need for pairwise interactions between agents, enhancing scalability. Extensive experiments demonstrate that GT-Space achieves state-of-the-art performance in detection accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Common Feature Space for Heterogeneous Agents: GT-Space introduces a common feature space for aligning heterogeneous agents, which promotes the practical deployment of heterogeneous collaborative perception systems.\n\n2.Contrastive Learning for Consistent Representation: By employing contrastive learning to supervise the fusion network, GT-Space encourages different agents to learn consistent feature representations for the same instances, improving robustness.\n\n3.Superior Performance Across Modality Combinations: Experimental results clearly indicate that GT-Space achieves advanced perception performance across various heterogeneous modality pairings, showcasing its effectiveness."}, "weaknesses": {"value": "1.Generalization to Unseen Agent Types: The paper trains and tests with the same set of agent types (e.g., agents A1-A4 are used for both training and testing under various combinations). However, in real-world scenarios, new, unseen agent types may need to be integrated into the system. Without ground truth labels for these new agents, how can the projection layer be trained to adapt to the fusion model for collaborative perception? What would be the estimated amount of training data and training duration required for such adaptation?\n\n2.Handling Misaligned Feature Spaces in Real-World Scenarios: GT-Space trains the model using observations from a single agent that assumes spatially aligned data. In practical settings, the feature spaces of different agents are likely to be misaligned. While coordinate transformations can achieve spatial alignment, the resulting features might differ significantly from those used during training (e.g., containing more background features). How does the proposed method address this discrepancy and maintain performance robustness in such real-world, misaligned scenarios?"}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sVUtFPFtbi", "forum": "fwTRpXMsxB", "replyto": "fwTRpXMsxB", "signatures": ["ICLR.cc/2026/Conference/Submission10421/Reviewer_AVxH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10421/Reviewer_AVxH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635856468, "cdate": 1761635856468, "tmdate": 1762921729538, "mdate": 1762921729538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GT-Space is a heterogeneous collaborative perception framework that avoids pairwise feature alignment between agents with different sensors/encoders. It builds a ground-truthâ€“derived common BEV feature space from 3D box labels and learns a single lightweight projector per agent to map local features into this space. On OPV2V, V2XSet, and a real-world RCooper scene, GT-Space consistently outperforms end-to-end, interpreter, and retraining baselines, especially boosting weaker/camera agents. Ablations show the projector and GT-space supervision are key contributors to gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. GT-space requires only one projector per agent, without pairwise adapters or encoder retraining which is scalable.\n\n2. GT-space uses contrastive learning to handle the bottleneck effect in collaborative perception.\n\n3. The experiments show that GT-space has SOTA performance among widely used datasets."}, "weaknesses": {"value": "This work requires accurate, dense 3D labels to build the common space, which is often impractical in real-world deployments.\n\nThe reviewer suggests adding robustness experiments, such as how the pipeline handles communication latency and localization errors.\n\nThis work evaluates only 3D detection; performance on other tasks, such as lane segmentation or tracking, is unclear."}, "questions": {"value": "What is the advantage of this work over interpreter-based methods? The author claimed that they are not scalable but did not explain why."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "28M2uYa7lj", "forum": "fwTRpXMsxB", "replyto": "fwTRpXMsxB", "signatures": ["ICLR.cc/2026/Conference/Submission10421/Reviewer_BvnQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10421/Reviewer_BvnQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976462587, "cdate": 1761976462587, "tmdate": 1762921729128, "mdate": 1762921729128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GT-Space, which is a scalable collaborative perception framework for heterogeneous autonomous driving agents. GT-Space uses a common feature space constructed from ground-truth labels to serve as a unified reference for feature alignment and a lightweight projector for each agent to map its features into that space. It achieves SOTA performance on OPV2V, V2XSet, and RCooper dataset."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of aligning all heterogeneous features in the GT space to construct a common, unified feature space is well-motivated and novel. \n2. The designed system is highly scalable and flexible. Because individual agents' encoders and detection heads are kept frozen, a new or unseen agent can be integrated simply by training a single, lightweight projector module to map its features to the GT-Space.\n3. The framework's performance is not bottlenecked by the capability of the ego agent. By using the GT-Space as a strong, objective reference, the fusion network can effectively leverage complementary strengths and compensate for weaker agents (like camera-only models).\n4. The paper demonstrates SOTA performance on multiple benchmarks."}, "weaknesses": {"value": "1. I have a major concern in the scalability of fusion training. The fusion network is trained using a \"combinatorial contrastive loss\" across all pairs of modalities. The paper gives an example with 3 models, resulting in 3 pairs. This implies that for $M$ distinct modality types, the training complexity is $O(M^2)$. This is not scalable\n2. The visualization in Figure 5 is not very insightful. It simply shows that the fused feature map has stronger activations than the original. More compelling visualizations would have been:A visualization of the GT-Space $F_{GT}$ itself.A side-by-side comparison of a projected camera feature $\\Phi_{cam}(F_{cam})$ and a projected LiDAR feature $\\Phi_{lidar}(F_{lidar})$ to visually demonstrate the alignment quality in the common space."}, "questions": {"value": "I would like to raise my rating if the authors can address my concerns shown in weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "07ecvRO7th", "forum": "fwTRpXMsxB", "replyto": "fwTRpXMsxB", "signatures": ["ICLR.cc/2026/Conference/Submission10421/Reviewer_dxid"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10421/Reviewer_dxid"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762191451818, "cdate": 1762191451818, "tmdate": 1762921728363, "mdate": 1762921728363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}