{"id": "6wd38R8L0Z", "number": 23668, "cdate": 1758346945828, "mdate": 1763732089154, "content": {"title": "Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning", "abstract": "Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL).\nWhile they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed.\nIn this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL.\nFINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset.\nIn addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning.\nExperiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.", "tldr": "We propose FINO, a flow matching–based policy that injects noise and uses entropy-guided sampling to enhance exploration and consistently outperform baselines in offline-to-online RL.", "keywords": ["Reinforcement Learning", "Offline-to-Online Reinforcement Learning", "Flow Matching", "Noise Injection"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5fe27fc9c7f1afcf02621c32781f824f5616bf8d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new approach for offline-to-online RL via injecting noise into the flow matching objective for improved exploration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "To my knowledge, this is the first paper to introduce the idea of noise injection into flow matching for offline-to-online RL, addressing a key limitation of prior work in offline-to-online RL. The empirical results demonstrate strong performance over the existing baselines (Tables 1 and 4), and attempts to justify the benefit of their method through ablations, though I provide comments about the ablations below."}, "weaknesses": {"value": "The weaknesses I identify can be grouped broadly into three categories: exposition/justification of FINO’s objective, baselines and ablations, and smaller questions. \n\nI believe greater explanation and exposition of the noise injection for flow matching objective is needed. For example, why is this objective better than simply adding noise to the velocity target, $x_1 - x_0$? Additionally, is $\\eta$ fixed throughout training? \n\nDo any of the baselines considered employ a similar policy extraction scheme, specifically a Q-value weighted softmax? The authors argue that directly injecting noise into action selection is less effective than FINO (Figure 4), but the direct noise injection seemed to happen only at inference time. \n\nA relevant baseline may be adding noise to the offline dataset’s actions during training, in addition to at inference-time in the online stage. \n\nThe authors do not clearly define what type of noise is added in the Direct Noise ablation (Figure 4), only referring to it as “random perturbations.” I would suggest the authors investigate adding Gaussian noise to the actions, with varying standard deviations, similar to what they do for FINO."}, "questions": {"value": "What does the y-axis scale on the right-hand side of Figure 2 represent? Also see questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kfKbNh43fg", "forum": "6wd38R8L0Z", "replyto": "6wd38R8L0Z", "signatures": ["ICLR.cc/2026/Conference/Submission23668/Reviewer_QgFC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23668/Reviewer_QgFC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960326236, "cdate": 1761960326236, "tmdate": 1762942754989, "mdate": 1762942754989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response"}, "comment": {"value": "We sincerely thank the reviewers for their insightful comments.\nThe feedback greatly contributed to clarifying our intentions and enhancing the presentation of the manuscript.\nWe appreciate the time and effort dedicated to reviewing our submission.\n\nThe following list indicates the revisions made to our manuscript:\n* **Figure 2**: We revised the caption to improve the clarity of the experimental results.\n* **Section 4.1**: We added further explanation of how the proposed method influences the one-step policy to enhance the understanding of our approach.\n* **Section 4.3**: We supplemented the description of hyperparameters to improve the clarity of the algorithm.\n* **Section 5 (Results)**: We included clarification regarding the offline performance of the proposed method and expanded the discussion comparing it with IFQL.\n* **Section 6.1 & Figure 4**: We introduced a new baseline, Action Noise, and included corresponding experiments.\n* **Section 6.2 & Figure 5** (formerly Section 6.1 and Figure 4): We refined the explanation of this baseline to improve clarity.\n* **Section 6.5 & Figure 7**: We added additional experiments analyzing the effect of $N_\\text{sample}$.\n* **Appendix D**: We included a comparison between the proposed noise injection scheme and a simple noise-injection alternative, along with results from a simple example.\n\nWe highlight all revised portions in blue in the updated manuscript available at OpenReview."}}, "id": "NU91JAZImk", "forum": "6wd38R8L0Z", "replyto": "6wd38R8L0Z", "signatures": ["ICLR.cc/2026/Conference/Submission23668/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23668/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23668/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728583662, "cdate": 1763728583662, "tmdate": 1763728583662, "mdate": 1763728583662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response"}, "comment": {"value": "We sincerely thank the reviewers for their insightful comments.\nThe feedback greatly contributed to clarifying our intentions and enhancing the presentation of the manuscript.\nWe appreciate the time and effort dedicated to reviewing our submission.\n\nThe following list indicates the revisions made to our manuscript:\n* **Figure 2**: We revised the caption to improve the clarity of the experimental results.\n* **Section 4.1**: We added further explanation of how the proposed method influences the one-step policy to enhance the understanding of our approach.\n* **Section 4.3**: We supplemented the description of hyperparameters to improve the clarity of the algorithm.\n* **Section 5 (Results)**: We included clarification regarding the offline performance of the proposed method and expanded the discussion comparing it with IFQL.\n* **Section 6.1 & Figure 4**: We introduced a new baseline, Action Noise, and included corresponding experiments.\n* **Section 6.2 & Figure 5** (formerly Section 6.1 and Figure 4): We refined the explanation of this baseline to improve clarity.\n* **Section 6.5 & Figure 7**: We added additional experiments analyzing the effect of $N_\\text{sample}$.\n* **Appendix D**: We included a comparison between the proposed noise injection scheme and a simple noise-injection alternative, along with results from a simple example.\n\nWe highlight all revised portions in blue in the updated manuscript available at OpenReview as PDF."}}, "id": "NU91JAZImk", "forum": "6wd38R8L0Z", "replyto": "6wd38R8L0Z", "signatures": ["ICLR.cc/2026/Conference/Submission23668/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23668/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23668/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728583662, "cdate": 1763728583662, "tmdate": 1763765925080, "mdate": 1763765925080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FINO, a novel approach for offline-online RL. Centered on a Flow Matching generative model, it addresses the insufficient exploration capability and exploration-exploitation imbalance of existing methods through two key designs: in the offline pre-training phase, time-dependent Gaussian noise is injected to expand the coverage of the action space; in the online fine-tuning phase, an entropy-guided sampling mechanism is introduced, which dynamically adjusts temperature parameters based on policy entropy to balance exploration and exploitation. Across 45 complex scenarios from OGBench and D4RL, FINO achieves significantly better performance than baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper exhibits a clear logical flow and high readability. The proposed algorithm is simple yet effective, supported by solid theoretical foundations and validated by striking experimental results."}, "weaknesses": {"value": "Injecting noise helps enhance exploration capability, which is beneficial for the offline-to-online transition. However, why does this not compromise offline performance? After all, offline RL is inherently conservative and discourages exploration."}, "questions": {"value": "1. Why $N_{sample}$ relates to the dimension of action space?\n2. Do the baseline methods employ candidate action sampling? And would candidate action sampling significantly improve the performance of these baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3xX1VAWqnp", "forum": "6wd38R8L0Z", "replyto": "6wd38R8L0Z", "signatures": ["ICLR.cc/2026/Conference/Submission23668/Reviewer_HJNR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23668/Reviewer_HJNR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986848845, "cdate": 1761986848845, "tmdate": 1762942754095, "mdate": 1762942754095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Flow Matching with Injected Noise for Offline-to-Online RL (FINO) that leverages flow matching policy for offline-to-online RL. It injects noise into policy training to facilitate exploration, which encourages action diversity. It balances exploration and exploitation by choosing the softmax of value functions over the action candidates. FINO is evaluated on a wide range of environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. FINO explicitly integrates exploration into the learning process and balances between exploration and exploitation. The empirical result demonstrates superior performance over a wide range of tasks.\n2. The paper is overall well-written and clear."}, "weaknesses": {"value": "1. Does the noise injection step damage the performance in the offline learning stage?\n2. Does FINO reduce policy randomness during evaluation?\n3. The benefit of injecting noise during training (Eq. 7) rather than directly adding Gaussian noise to the output action is unclear. \n4. The role of entropy guidance in improving performance is also not well explained. Does it enhance policy learning, or does it serve as a technique to improve test-time behavior?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u4MuU2vs1A", "forum": "6wd38R8L0Z", "replyto": "6wd38R8L0Z", "signatures": ["ICLR.cc/2026/Conference/Submission23668/Reviewer_czZj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23668/Reviewer_czZj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999849859, "cdate": 1761999849859, "tmdate": 1762942753673, "mdate": 1762942753673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}