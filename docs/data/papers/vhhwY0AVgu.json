{"id": "vhhwY0AVgu", "number": 10178, "cdate": 1758163156676, "mdate": 1759897669021, "content": {"title": "Layerwise Learning Rate in the Era of Large Language Models", "abstract": "Learning rate configuration is a fundamental aspect of modern deep learning. The prevailing practice of applying a uniform learning rate across all layers overlooks the structural heterogeneity of Transformers, potentially limiting their effectiveness as the backbone of Large Language Models (LLMs). In this paper, we introduce Layerwise Learning Rate (LLR), an adaptive scheme that assigns distinct learning rates to individual Transformer layers. Our method is grounded in Heavy-Tailed Self-Regularization (HT-SR) theory, which characterizes the empirical spectral density (ESD) of weight correlation matrices to quantify heavy-tailedness. Layers with weaker heavy-tailedness are assigned larger learning rates to accelerate their training, while layers with stronger heavy-tailedness receive smaller learning rates. By tailoring learning rates in this manner, LLR promotes balanced training across layers, leading to faster convergence and improved generalization. Extensive experiments across architectures (from LLaMA to GPT-nano), optimizers (AdamW and Muon), and parameter scales (60M–1B) demonstrate that LLR achieves up to a 1.5× training speedup compared to uniform LR. Under the same training token budget, LLR further surpasses existing approaches by a clear margin. A key advantage of LLR is its low tuning overhead: it transfers nearly optimal LR settings directly from the uniform baseline, substantially lowering the barrier to practical adoption. Our code is submitted.", "tldr": "We propose a per-module learning rate method guided by heavy-tailedness, improving large language model performance.", "keywords": ["Learning Rate", "Large Language Models", "Heavy-Tailed Self-Regularization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc3d45dfd589789142ee0c27c53059c1e6d0824f.pdf", "supplementary_material": "/attachment/b6c7b5c14af6bf6af2e2ea4e56952859e70b0cf2.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Layerwise Learning Rate (LLR), an adaptive strategy that assigns distinct learning rates to Transformer layers based on Heavy-Tailed Self-Regularization (HT-SR) theory. The method measures each layer’s spectral heavy-tailedness via the `PL_Alpha_Hill` metric and scales learning rates. The authors demonstrate that this approach balances optimization across layers, accelerates convergence, and improves generalization, with comprehensive experiments."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-structured and easy to follow, demonstrating clear organization and logical flow.\n- Conceptually interesting: operationalizing HT-SR signals as a dynamic LR allocator; the gradient-→weight metric handoff is a reasonable design to handle HT-SR.\n- Broad empirical sweep: multiple model sizes (60M–1B), two optimizers (AdamW, Muon), distinct architectures (LLaMA, GPT-nano), plus downstream zero-shot and fine-tune results; helpful ablations on metric choice, fitting frequency, and $(1,s)$ sensitivity."}, "weaknesses": {"value": "My primary concerns is presented in questions, my other concerns:\n- **Limited Theoretical Depth.** While HT-SR provides motivation, the derivation of the mapping from spectral exponent to learning-rate scaling remains heuristic and empirical rather than theoretically grounded.\n- **Comparative Baseline Weakness.** Competing methods like LARS, LAMB are rarely used in LLM context, comparative baselines are generally weak.\n- **Method Naming** Layerwise Learning Rate is a widely used generic term in DL; it is not suitable as a distinctive method name. A more distinctive name would avoid confusion."}, "questions": {"value": "**Crucial Questions**\n\n- **Scalability to Larger Scales.** Most of experiments are below 350M model size. There is one experiemnt with 1B model size. Moreever, the training token in this experiment is only 10B, even below the Chinchilla law, which makes reader doubting aboout its scalability. Can the authors provide larger-scale runs or scaling-law extrapolations to demonstrate robustness beyond 1 B parameters?\n- **Sensitvity of hyperparameter $s$** Let loss $f$ be a function of $s$, as long as $0$ is not the minimizer of $f$, one can find a better $s$ with lower loss. The work chooses different $s$ such as 2.0 and 3.0. Please clarify the selection criterion for $s$, its sensitivity range, and whether optimal $s$ scales with model depth or spectral spread\n- **Relation to Other Works** The relation with work [1] and [2] needs to be clarified.\n  - I examined [1] (AlphaDecay) detailedly, AlphaDecay also leverages HT-SR theory to develop a layerwise weight decay method. How could HT-SR inspire two highly similar methods—one for weight decay, one for learning rate? First, The authors need to further clarify the relations with work [1]. Second, Which way is better, LLR or AlphaDecay? Is LLR orthogonal to AlphaDecay? Do gains add? (BTW, The paper's images are very similar with these of [1].)\n  - [2] performs grid search to get the best scaling $[1, 8, 4, 6, 10]$ on their settings to show the consistency with their principle. First, as you tune $s$, can you run experiment with the $s$-parametrized scaling $[1, 1+7s/9, 1+3s/9, 1+5s/9, 1+s]$ as comparison, other setting remains the same as LLR. Second, can you perform a grid search, to show that the best scaling in your setting is more close to your HT-SR or their principle?\n- **Suspicious Results** Can you explain these results?\n  - Figure 6 presents six images, but does not provide enough information. Moreever, Under Muon, the losses of Uniform (60k) and Uniform (90k) look similar near step 60k, yet Uniform (60k) has already finished cosine decay by then in some subplots; if so, it should be lower.\n  - On Table 2, the ppl gap between `Uniform` and `LLR` for model size 60M, 135M, 350M, 1B is 0.55, 1.31, 0.59, 1.97. As model size grows, the baseline perplexity decreases, so further reduction should be increasingly difficult [3]. The large gap at 1B seems abnormal and warrants verification of hyperparameter parity and token-budget consistency.\n\nOther questions are presented in the other concerns of Weaknesses. How these concerns are addressed will greatly affect my score.\n\n\n[1] He, D., Jaiswal, A., Tu, S., Shen, L., Yuan, G., Liu, S. and Yin, L., 2025. AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs. NeurIPS 2025. \\\n[2] Wang, J., Wang, M., Zhou, Z., Yan, J. and Wu, L., 2025. The sharpness disparity principle in transformers for accelerating language model pre-training. ICML 2025. \\\n[3] Wen, K., Hall, D., Ma, T. and Liang, P., 2025. Fantastic pretraining optimizers and where to find them. arXiv preprint arXiv:2509.02046."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hXkwoIbhX6", "forum": "vhhwY0AVgu", "replyto": "vhhwY0AVgu", "signatures": ["ICLR.cc/2026/Conference/Submission10178/Reviewer_uFmm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10178/Reviewer_uFmm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760931920626, "cdate": 1760931920626, "tmdate": 1762921545796, "mdate": 1762921545796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates layer-wise learning rates for LLM pretraining. Motivated by Heavy-Tailed Self-Regularization (HT-SR) theory, the method assigns larger learning rates to layers with weaker heavy-tailedness to accelerate training, and smaller learning rates to layers with stronger heavy-tailedness. By using different learning rates across layers, LLR promotes balanced training, leading to faster convergence. Experiments are conducted on 60M, 130M, 350M, and 1B models with up to 10B training tokens. The paper reports both perplexity and downstream performance, along with comparisons to relevant prior work."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The writing is good, with clear figures and text. In particular, the method is presented clearly.\n\n* I think using different learning rates for different layers can be important for pretraining. Motivated by the Heavy-Tailed Self-Regularization (HT-SR) theory, the paper proposes using the Hill metric as an indicator for setting different learning rates across layers. This helps balance training across layers, leading to faster convergence. Given the dynamics of training, the paper proposes updating the Hill metric and thus the learning rates at certain time steps.\n\n* Experiments are conducted not only on perplexity but also on downstream tasks. The paper also compares with relevant work experimentally and shows consistently better performance."}, "weaknesses": {"value": "* One main issue for me is that the proposed method typically assigns all layers learning rates that are higher than or equal to the standard uniform LR. This makes me wonder about the performance of simply increasing the uniform LR and comparing it with the proposed method. Because there are only three points in Figure 5 and only four points in the promising area in Figure 1, it is hard to assess the gap at each method’s best case. Also, if the uniform LR is already large, assigning even larger LRs for LLR may lead to loss spikes. In Figure 1, how do the uniform and LLR curves behave if we continue to increase the LR? Therefore, in terms of method design, I think it would be better and necessary if the mechanism not only scales up learning rates for different layers but also allows scaling them down.\n\n* While the paper is about different learning rates in different layers, Figure 4 seems to show that there are no large LR differences among layers of the same type. For example, uniformly using a larger learning rate for the feedforward networks and the embedding layer (2× or 3×) may also work. Therefore, can the authors show that setting different learning rates per layer is necessary compared to using different learning rates only by layer type?\n\n* While the paper dynamically adjusts learning rates based on the PL_Alpha_Hill value at certain time steps, it only shows the evolution of PL_Alpha_Hill values for different types of layers across training steps, but not the learning-rate value for each layer (not just each type of layer) at each update step. It would be better to include this information in the appendix to help readers better understand the learning-rate changes.\n\n* I think another important factor for the learning-rate study is the number of training tokens. With more tokens, the model is more amenable to convergence, which also reflects today’s LLM training settings. For example, in Table 2, there is a large perplexity gap between LLR and uniform at the 1.3B model scale but smaller gaps on smaller models. I think one highly possible reason is that for smaller models the number of training tokens is set according to the scaling law (20×), whereas for the 1B model it uses 10B tokens rather than the 20B suggested by the scaling law. When training with fewer tokens, it might be easier for LLR—with all layers having larger or equal learning rates than the baseline—to show better performance. Therefore, in terms of evaluating its effectiveness toward training convergence, can the authors show that the proposed method still works in the overtraining regime, which is how today’s language models are trained and put into practice? For example, show the performance of a 130M model trained on 100B tokens."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8TbqeGs7Tu", "forum": "vhhwY0AVgu", "replyto": "vhhwY0AVgu", "signatures": ["ICLR.cc/2026/Conference/Submission10178/Reviewer_DxYd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10178/Reviewer_DxYd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761173302932, "cdate": 1761173302932, "tmdate": 1762921544902, "mdate": 1762921544902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new algorithm for automatically tuning the learning rates of different layers of LLMs during their training. It relies on the heavy-tailed self-regularization theory and the estimation of heavy-tailedness of the weight matrices to scale step sizes. The step-size estimation is efficient and improves upon existing methods in a variety of pretraining and finetuning experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The layerwise learning rate algorithm proposed by the authors improves upon fixed learning rate and competing algorithms like LARS and LAMB, and perform very well on a large set of pretraining and finetuning experiments. \n\n- The proposed algorithm is simple and well-supported by the heavy-tailed self-regularization theory, which suggests weight matrices with heavy-tailed eigenvalue distributions are closer to convergence and should use smaller learning rates."}, "weaknesses": {"value": "- The simple range scaling based on estimated \\alpha used in Equation (3) for learning rate seems like a heuristic that lack justification. The use of min and max alpha in the scaling might also be prone to outliers. \n\n- The work by Martin & Mahoney 2019 on heavy-tailedness is mostly based on empirical results from CNN, not on LLMs. And from Figure 3 in this paper, we don't see a big change in heavy-tailedness across iterations. This doesn't seem to agree with the theory that more heavy-tailed weights are more well-trained. Rather the main difference is between different types of weight matrices (e.g. Q,K vs FFN)."}, "questions": {"value": "- How do the estimates of pl_alpha_hill evolve over training for different layers? Do they decrease as expected by theory? Showing these plots can strengthen the argument of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Phg1SLpnf0", "forum": "vhhwY0AVgu", "replyto": "vhhwY0AVgu", "signatures": ["ICLR.cc/2026/Conference/Submission10178/Reviewer_VFis"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10178/Reviewer_VFis"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838423684, "cdate": 1761838423684, "tmdate": 1762921544475, "mdate": 1762921544475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Layerwise Learning Rate (LLR), an adaptive layer-wise learning rate strategy for large language models (LLMs), grounded in Heavy-Tailed Self-Regularization (HT-SR) theory. By periodically estimating the heavy-tailedness (PL_Alpha_Hill) of each Transformer layer’s weight or gradient spectrum, LLR assigns larger learning rates to less heavy-tailed (undertrained) layers and smaller ones to more heavy-tailed (well-trained) layers. The method transitions from gradient-based spectral estimation at early training to weight-based estimation later, reflecting the empirical evolution of spectral properties. Across LLaMA models (60M-1B) and GPT-nano with AdamW and Muon optimizers, LLR achieves up to 1.5× faster convergence and consistently lower perplexity, while requiring minimal tuning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- First to apply HT-SR heavy-tailed spectral analysis for dynamic layerwise LR adjustment during LLM pretraining.\n- Builds upon established HT-SR theory linking heavy-tailedness to training quality; the design aligns with spectral evolution trends.\n- Strong, multi-scale results (60M-1B) across two optimizers, with consistent perplexity and convergence gains.\n- Comprehensive studies on PL-fitting methods, update intervals, scaling ratios, and weight-gradient combinations.\n- Inherits near-optimal uniform LR settings, minimizing tuning overhead."}, "weaknesses": {"value": "- The study omits comparison to Layerwise LR Decay (LLRD) or similar depth-based heuristic schedules widely used in LLM fine-tuning.\n- The paper claims low overhead, but it does not quantitatively report the computational cost of periodic heavy-tailedness estimation and PL-fitting. Since this step involves eigenvalue or singular value analysis per layer, its runtime impact should be clarified to confirm practical efficiency.\n- The grad to weight transition is motivated by spectral observations and partially ablated, but a direct 3-way comparison (grad-only vs. weight-only vs. two-phase) is missing.\n- The scalability of LLR beyond 1B parameters remains unclear; no evidence or discussion is provided on whether spectral estimation and PL-fitting remain stable and efficient for very large models."}, "questions": {"value": "- Could the authors quantify the computational overhead introduced by periodic heavy-tailedness estimation and PL-fitting (e.g., as a percentage of total training time or step cost)? This is important to assess whether LLR is practically lightweight as claimed.\n- Can the authors provide a direct ablation comparison among grad-only, weight-only, and two-phase versions of LLR, reporting both convergence speed and final perplexity?\n- Would LLR remain effective when combined with Layerwise LR Decay (LLRD) or other depth-based heuristic schedules commonly used in practice?\n- Could the authors demonstrate or discuss the scalability of LLR to larger models (e.g., 7B-70B)? If large-scale experiments are infeasible, can they provide analytical or empirical evidence that spectral estimation and PL-fitting remain computationally tractable and statistically stable as model size increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yZ8JvDeIwP", "forum": "vhhwY0AVgu", "replyto": "vhhwY0AVgu", "signatures": ["ICLR.cc/2026/Conference/Submission10178/Reviewer_WV8G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10178/Reviewer_WV8G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888103273, "cdate": 1761888103273, "tmdate": 1762921543894, "mdate": 1762921543894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Layerwise Learning Rate (LLR), an adaptive scheme that assigns per-layer learning rates using spectral statistics derived from heavy-tailed self-regularization theory. Building on observations that well-trained model weights exhibit heavy-tailed spectral distributions, the method dynamically adjusts each layer’s learning rate according to its degree of heavy-tailedness, promoting balanced optimization across the network. Unlike prior non-adaptive layerwise LR methods, which fail to outperform a well-tuned uniform LR, LLR periodically measures the empirical spectral density to guide adaptive updates. Experiments on Transformer models up to one billion parameters show faster step-wise convergence and modest generalization gains over standard baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Theoretical motivation.** Builds on well-documented heavy-tailed behavior in the spectra of trained model weights and connects it to optimization dynamics.\n* **Novel application to LLM pre-training.** Leverages heavy-tailed self-regularization for adaptive layerwise LR assignment—an area unexplored in large-scale Transformer training.\n* **Addresses an important regime.** Seeks to improve optimization even when the global uniform LR is already near-optimal, a practically challenging and relevant setting."}, "weaknesses": {"value": "* **Baseline coverage and tuning transparency.** Comparisons exclude recent blockwise-LR or adaptive-sharpness baselines, and tuning procedures for existing methods are under-specified.\n* **Limited novelty over prior work.** The approach extends existing heavy-tailed self-regularization ideas (e.g., TempBalance, AlphaDecay) to LLMs rather than introducing a fundamentally new principle.\n* **Lack of causal validation.** The link between heavy-tailedness and the “need for larger LR” is assumed but not empirically isolated through control or ablation studies (e.g., randomizing or inverting the mapping).\n* **Evaluation scope is narrow.** Experiments are limited to models ≤1 B parameters on a single dataset (C4), which weakens claims about LLM-scale generality.\n* **Design inconsistency.** The embedding layer is manually fixed to the maximum LR, contradicting the proposed fully data-driven adaptation rule."}, "questions": {"value": "I suspect that self-attention layers converge faster because their hidden dimensionality is smaller than that of FFN and embedding layers, leading to a lower effective rank. Since lower effective rank typically correlates with a more skewed (possibly heavy-tailed) singular value distribution, would increasing the head dimension slow down self-attention convergence and better align its spectral characteristics with those of FFN and embedding layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xspjdgGfXr", "forum": "vhhwY0AVgu", "replyto": "vhhwY0AVgu", "signatures": ["ICLR.cc/2026/Conference/Submission10178/Reviewer_zE9Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10178/Reviewer_zE9Z"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762518771326, "cdate": 1762518771326, "tmdate": 1762921543504, "mdate": 1762921543504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}