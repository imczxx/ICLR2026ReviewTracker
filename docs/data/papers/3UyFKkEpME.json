{"id": "3UyFKkEpME", "number": 7509, "cdate": 1758025470110, "mdate": 1759897848681, "content": {"title": "Diagnosing and Mitigating Systemic Reward Bias in Self-Rewarding RL", "abstract": "Reinforcement learning with verifiable rewards (RLVR) efficiently scales the reasoning ability of large language models (LLMs) but remains bottlenecked by limited labeled samples for continued data scaling. Reinforcement learning with intrinsic rewards (RLIR), in which the policy model assigns reward signals to its own rollouts, enables sustainable scaling in unlabeled settings. Yet its performance and stability still lag behind RLVR. We trace this gap to a system bias: the model tends to deem its own high-confidence rollouts correct, leading to biased and unstable reward estimation. It accumulates and rises rapidly as training proceeds, with the deviation from the oracle drifting toward over-reward. This causes unstable training and locks the performance ceiling. To understand how system bias yields these effects, we characterize it by the magnitude of reward bias, the degree of policy–reward coupling, and the proportional imbalance between over-reward and under-reward via three metrics: $\\rho_{\\text{noise}}$, $\\rho_{\\text{selfbias}}$, and $\\rho_{\\text{symbias}}$. We find that $\\rho_{\\text{noise}}$ and $\\rho_{\\text{symbias}}$ affect convergence performance and speed, while $\\rho_{\\text{selfbias}}$ has an amplification effect: it amplifies both correct and incorrect updates and induces unstable reward estimation. To mitigate system bias of RLIR, we propose reinforcement learning with ensembled rewards (RLER). It aggregates diverse models with adaptive reward interpolation and rollout selection strategy to build a unified reward-estimation space, jointly improving accuracy ($\\rho_{\\text{noise}}$), unbiasedness ($\\rho_{\\text{selfbias}}$, $\\rho_{\\text{symbias}}$), and robustness ($\\rho_{\\text{selfbias}}$).  Extensive experiments show that RLER improves by +13.6\\% over the best RLIR baseline, and is only 3.6\\% below the RLVR setting. Moreover, RLER achieves stable scaling on unlabeled samples, making it highly applicable.", "tldr": "", "keywords": ["Self rewarding", "Reinforcement learning", "noise learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0066787b92d47bb32c203c984ed05c77b189b2d4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Research has identified a systematic bias in RLIR: models tend to overestimate their own high-confidence outputs, leading to biased and unstable reward estimation, which in turn affects training convergence and performance limits. To mitigate this issue, the authors propose Reinforcement Learning with Ensemble Rewards (RLER), which improves the accuracy, unbiasedness, and robustness of reward estimation by aggregating multiple models and employing an adaptive reward interpolation strategy."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The analysis of systematic bias is quite comprehensive.\n2.  Although ensemble methods are commonly used, the specific implementation and innovative design still contribute meaningfully to the field."}, "weaknesses": {"value": "1. The experiments in this paper are almost entirely based on the Qwen series of models, and the evaluation datasets used (such as MATH500, AMC, AIME, etc.) are highly likely to be contaminated. The authors also acknowledge this issue in the Appendix. Therefore, the experimental results cannot be considered reliable unless validated on more models and datasets with favorable outcomes.\n2. Figure 1 needs improvement. The font size in the figure is too small, making it difficult to discern the method the authors intend to illustrate.\n3. The training dataset is overly simplistic and lacks diversity."}, "questions": {"value": "1. As mentioned in the Weaknesses section, the authors urgently need to supplement the experiments with more models, training datasets, and evaluation datasets.\n2. The computational cost of the ensemble method should be quantified and analyzed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0u3prhGF3y", "forum": "3UyFKkEpME", "replyto": "3UyFKkEpME", "signatures": ["ICLR.cc/2026/Conference/Submission7509/Reviewer_LJof"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7509/Reviewer_LJof"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463815577, "cdate": 1761463815577, "tmdate": 1762919618730, "mdate": 1762919618730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors focus on the \"systematic bias\" problem in RLIR. The challenge of this problem is that models tend to assign a high reward for their own high-confidence (even if wrong) output. The authors systematically analyze the reasons for this problem and propose an ensemble reward strategy. The experiments show that the strategy outperforms baselines and achieves results approaching those of methods that rely on labeled data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper is clear and specific. The authors focus on RLIR's important system bias challenge. This challenge is important for LLM's application.\n2. In this paper, the authors provide detailed experiments to validate the effectiveness of the method. They provide sufficient hyperparameters for the experiments. In general, the reproducibility of the method should not be a problem."}, "weaknesses": {"value": "1. The method proposed in this paper is an ensemble method. However, in the experiments, the authors only compare their method with single-model methods. They fail to compare the method with other ensemble methods. This lack makes it hard for the reader to evaluate the effect of the mixed signals. This may raise the question of whether the effectiveness of the method is based on the rationality of the reward or the ensemble strategy itself.\n\n2. The authors only test their method on limited LLM types and sizes. In this paper, the authors test the model only on Qwen, and the scales they choose are 1.5B and 7B. The single model choice and smaller scale may raise the question of whether the method and its strategy are only effective on specific models."}, "questions": {"value": "1. The training phase of the method requires running k models in parallel. This brings k times the computation and memory overhead compared to a single model. The increase in computational cost may limit the application of this method on a large scale or with a larger ensemble size. Authors may want to discuss in detail the trade-off between performance gain and training cost, especially in resource-constrained scenarios.\n\n2. The experiments of this paper mainly focus on mathematical reasoning tasks. The characteristic of these tasks is that the answer is clear and easy-to-validate. However, in broader reasoning tasks, such as code generation and creative writing, the reward signal may be hard to quantify. How the method can be generalized to broader reasoning tasks, or whether the method is focused on solving mathematical reasoning, is an open question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B61slSn5sO", "forum": "3UyFKkEpME", "replyto": "3UyFKkEpME", "signatures": ["ICLR.cc/2026/Conference/Submission7509/Reviewer_sZwE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7509/Reviewer_sZwE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755728479, "cdate": 1761755728479, "tmdate": 1762919618370, "mdate": 1762919618370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper diagnoses and mitigates system bias in RLIR. The work traces the performance gap between RLIR and methods using RLVR to a system bias where the model incorrectly rewards its own high-confidence outputs. To characterize this, the paper introduces three metrics: reward noise rate, self-feedback bias rate, and symmetry bias rate. Based on this diagnosis, the paper proposes RLER, which aggregates diverse models to create a more accurate and stable reward signal. RLER combines ensemble self-rewarding, adaptive soft-reward interpolation, and a confidence-disagreement balanced rollout selection strategy. Experiments on mathematical reasoning benchmarks show that RLER significantly outperforms RLIR baselines and substantially closes the performance gap with RLVR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This work introduces three distinct metrics, which provide a clear and quantitative framework for analyzing system bias. \n- The decoupling experiment is a well-designed study that effectively isolates the impact of each metric on training dynamics. \n- The paper presents extensive experiments with strong results that convincingly support its claims."}, "weaknesses": {"value": "Lack of precision in key definitions and insufficient details regarding experimental setups, particularly in Section 3. For example, the \"attained reward as r_i\" (line 150) is unclear whether it is a model prediction or a noisy label. \"hard-reward\" and \"soft-reward\" are mentioned without explanation in the main text. The motivation for choosing the three specific metrics, which are central to the paper's contribution, could also be explained in detail. \"Findings 2\" concludes that over-reward is more detrimental than under-reward, and a \"Further analysis\" is mentioned to support this. However, the text does not elaborate on this analysis, and the connection between the cited figures (Fig. 2(b) and Fig. 3(e)) and the claim about a \"near-orthogonal gradient bias\" is not explained The setup for experiments in Figure 3 is not described with sufficient detail.  the paper states that an ensemble of k=2 models is used but fails to describe how these models are chosen or initialized. A small ensemble size (k=2) is also not fully justified. p_selfbias^true(x) and p_selfbias^err(x), without defining what \"true\" and \"err\" mean in this context."}, "questions": {"value": "See weakness and:\n\nThe main experiments use an ensemble of 2 models. What is the rationale for this specific choice? How does the performance, computational overhead, and model diversity of RLER scale on the main benchmarks as ensemble number is increased beyond 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NPE27iKou3", "forum": "3UyFKkEpME", "replyto": "3UyFKkEpME", "signatures": ["ICLR.cc/2026/Conference/Submission7509/Reviewer_8jPv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7509/Reviewer_8jPv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159781094, "cdate": 1762159781094, "tmdate": 1762919617912, "mdate": 1762919617912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes the system bias issues in RLIR (Reinforcement Learning with Intrinsic Rewards) through reward bias magnitude, policy-reward coupling strength, and imbalance magnitude. To address the systemic bias issues, the paper proposes RLER (Reinforcement Learning with Ensembled Rewards). Specifically, RLER replaces the single-model self-rewarding with an ensemble, aggregating diverse models to construct a unified stable reward space that guides the ensemble to improve collaboratively. The paper uses Qwen2.5-Math-7B as the backbone model and trains it with DAPO-MATH-17K dataset. Experiments demonstrate that RLER outperforms the RLIR baseline across multiple metrics and achieves results very close to RLVR (Reinforcement Learning with Verifiable Rewards)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a novel approach to analyze the system bias of RLIR. The paper characterizes RLIR's noise, characterizing RLIR’s noise, coupling, and over/under-reward asymmetry with three metrics and validates their causal roles through experiments.\n\n2. The experiment section of this paper is comprehensive. It includes not only the overall improvement rate but also the analytical indicators mentioned earlier and a full range of ablation experiments."}, "weaknesses": {"value": "1. The reward design in this paper is relatively complex, increasing parameter tuning costs while potentially causing gradient instability. Additionally, the reward/scoring module exhibits high sensitivity to batch composition, sampling temperature, and normalization strategies, where even minor missteps can amplify training fluctuations.\n\n2. This paper assumes that free-text answers can be mapped to discrete categories, upon which rewards are calculated. While this approach may be feasible for tasks like mathematics, it struggles to adapt to open-ended tasks. Furthermore, the experimental data presented in the paper are exclusively from mathematics datasets, failing to demonstrate advantages across other task domains.\n\n3. RLER relies on the diversity of candidate results derived from model diversity. With too few models, the advantages of ensemble learning are not fully realized, while too many models introduce noise. The quantitative analysis in this paper is insufficient in this regard.\n\n4. The paper only includes LLM-as-a-Judge and typical RLIR as the baseline model. It does not incorporate recently designed improvements specifically addressing RLIR limitations as experimental baselines."}, "questions": {"value": "Model diversity brings ensemble benefits but may also amplify noise. How to optimally balance diversity versus noise within a fixed budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gysy9D5WSV", "forum": "3UyFKkEpME", "replyto": "3UyFKkEpME", "signatures": ["ICLR.cc/2026/Conference/Submission7509/Reviewer_fLzJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7509/Reviewer_fLzJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762432500603, "cdate": 1762432500603, "tmdate": 1762919617559, "mdate": 1762919617559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}