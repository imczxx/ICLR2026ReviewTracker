{"id": "Kw2mvnzCoc", "number": 12042, "cdate": 1758205375688, "mdate": 1759897537553, "content": {"title": "TSPulse: Tiny Pre-Trained Models with Disentangled Representations for Rapid Time-Series Analysis", "abstract": "Different time-series tasks benefit from distinct cues at various spaces and abstractions, yet existing time-series pre-trained models entangle these signals within large, monolithic embeddings, limiting transferability and zero-shot usability. Moreover, massive model sizes demand heavy compute, restricting practical deployments and real-time applications. To address this, we propose TSPulse, an ultra-light pre-trained model (1M parameters) that performs disentangled masked reconstruction across spaces and abstraction levels, explicitly learning three disentangled views: temporal embeddings for fine-grained time analysis, spectral embeddings for frequency-aware fidelity, and semantic embeddings for high-level task understanding. A hybrid masking scheme further randomizes mask style and span length to avoid pre-training bias and improve robustness. Despite its compact size, TSPulse achieves strong gains across four time-series tasks: +20\\% and rank-1 on TSB-AD leaderboard benchmark for reliable anomaly detection through multi-head triangulation, which correlates complementary cues across disentangled views; +25\\% in similarity search as the disentangled semantic embedding remain invariant to time, scale and noise shifts, making retrieval more robust; +50\\% improvement in imputation since hybrid masking exposes the model to diverse real-world corruption patterns; and +5–16\\% gains in multivariate classification with TSLens, a lightweight module that selectively attends to the most informative signals across variates. Overall, TSPulse outperform models that are 10–100× larger on 75+ datasets across tasks, while delivering state-of-the-art zero-shot results with GPU-free support and efficient fine-tuning. Models and source code will be open-sourced and currently shared in the supplementary material.", "tldr": "Ultra-lightweight time-series pre-trained models (1M parameters) with disentangled embeddings across spaces and abstraction levels, delivering state-of-the-art performance in anomaly detection, classification, imputation, and similarity search.", "keywords": ["time series foundation models", "pretrained models", "time series", "foundation models", "TSFM"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e9efb688962f3642e0ec7a5b2054eccfcebdf37b.pdf", "supplementary_material": "/attachment/200dba1412ac8646030948c9d7e811bd06158c96.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ultra-light pre-trained models for various time series analysis tasks, including classification, imputation, anomaly detection, and semantic search. Key techniques involve dsentangled masked reconstruction for fine-grained and semantic embeddings and a hybrid masking scheme to avoid pre-training bias and boost generalization. More importantly, the proposed TSPulse outperforms models 10–100× larger on 75+ datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. novel and practical problem setting for designing fast and efficient time series foundation models.\n\n2. Technical designs seem to be reasonable and solid.\n\n3. Experimental results are promising and demonstrate the efficiency of the model.\n\n4. Pretrained models, source code, and example scripts are included in the supplementary materials."}, "weaknesses": {"value": "1. The presentation for the proposed method should be improved to enhance clarity. It is suggested to use 1 or 2 sentences to describe the key idea of the proposed TSPulse model.\n\n2. According to Figure 6, the imputation performance is extremely good, which needs more discussion.\n\n3. Why do we need the tasks for Similarity Search? Are there some applications?"}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wVB4bvsXav", "forum": "Kw2mvnzCoc", "replyto": "Kw2mvnzCoc", "signatures": ["ICLR.cc/2026/Conference/Submission12042/Reviewer_viWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12042/Reviewer_viWQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12042/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761493112125, "cdate": 1761493112125, "tmdate": 1762923021909, "mdate": 1762923021909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TSPulse is a 1M-parameter pre-trained model for time-series analysis that learns disentangled temporal, spectral, and semantic representations through multi-head masked reconstruction. Using a lightweight TSMixer backbone, hybrid masking, and a small decoder, it achieves efficient and robust pre-training. During fine-tuning, identity-initialized channel mixers enable multivariate adaptation without retraining the full model. TSPulse delivers strong performance across classification, anomaly detection, imputation, and similarity tasks while maintaining high efficiency on CPUs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The model uses a PEFT-style multivariate adaptation. It is pre-trained on univariate data and activates decoder-side channel mixers only during fine-tuning, initialized to identity for stability. This allows lightweight adaptation without retraining the entire model.\n- The design is highly efficient, with about 1M parameters, low latency, and small memory usage. The pre-training setup on 1B samples in one day on 8×A100 makes it practical and scalable.\n- The representation is disentangled into three segments—time, FFT, and register embeddings—and each task selectively uses the relevant parts. This improves transfer flexibility and prevents unnecessary coupling between features."}, "weaknesses": {"value": "- The register-token design introduces inductive bias. Predicting spectral signatures forces phase invariance, which may hurt tasks that rely on absolute timing. The paper does not compare this approach to CLS-style aggregators or include related ablations.\n- The hybrid masking strategy lacks novelty. Mixing block and point masks is common, and the argument that it better captures “real randomness” is weak. Patching is a modeling choice, while masking should reflect a data property (i.e., real-world missingness patterns). The paper conflates these levels and provides no statistical evidence that its hybrid masking aligns with realistic data gaps.\n- The paper omits forecasting evaluation despite clear architectural similarity to imputation. In most time-series models, forecasting can be tested simply by changing the final output layer, so excluding it removes a standard and low-effort comparison point. Given that nearly all major time-series foundation models are benchmarked on forecasting, this omission significantly limits comparability and completeness."}, "questions": {"value": "Please address the identified weaknesses and limitations noted above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sDGZ9cwvfv", "forum": "Kw2mvnzCoc", "replyto": "Kw2mvnzCoc", "signatures": ["ICLR.cc/2026/Conference/Submission12042/Reviewer_9eiE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12042/Reviewer_9eiE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12042/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710302355, "cdate": 1761710302355, "tmdate": 1762923021606, "mdate": 1762923021606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TSPulse proposes a tiny pre-trained backbone for time-series diagnostics, including classification, anomaly detection, imputation, and similarity search, that is designed for low-latency CPU/GPU inference. The approach learns disentangled representations by reconstructing signals in both time and frequency domains and by separating fine-grained from semantic information, supported by a hybrid masking scheme that varies mask type and span to reflect realistic missingness. Built on a TSMixer backbone with a lightweight mini-decoder, the work integrates task-oriented adapters (e.g., TSLens for multivariate classification and multi-head triangulation for anomaly detection) to translate the embeddings into downstream gains. Empirical results across standard benchmarks indicate consistent improvements over larger baselines, while maintaining a compact footprint that facilitates deployment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The approach provides a ~1M-parameter pre-trained model with low-latency CPU/GPU inference, facilitating practical deployment.\n- The work employs a dual time–frequency objective with hybrid masking that mirrors realistic missingness and yields robust representations.\n- Task-oriented modules (e.g., TSLens, triangulation) translate the embeddings into measurable gains across multiple diagnostic tasks."}, "weaknesses": {"value": "- Reliance on task-specific loss re-weighting may limit out-of-the-box generality and increase tuning burden.\n- The representation is not assessed on long-horizon causal forecasting, which constrains claims of transferability beyond diagnostics.\n- The lack of parameter-matched plug-in baselines (e.g., gated residuals, lightweight cross-attention) weakens isolation of design effects from model capacity.\n- The empirical evidence would be stronger with uncertainty estimates and per-dataset breakdowns to substantiate the reported gains."}, "questions": {"value": "- What default loss weights work reliably across domains, and how sensitive are downstream results to these weights during task-specialised pre-training and fine-tuning?\n- Do the disentangled embeddings support causal long-horizon forecasting if a forecasting head is added, and how do they compare with forecasting-oriented pre-trained models under strict parameter matching?\n- How does TSPulse perform against equally small plug-ins—such as a gated residual or lightweight cross-attention when total parameters and training budget are held constant?\n- Can empirical missingness models from real datasets (e.g., MAR/MNAR) be provided, and can robustness be tested under these distributions to validate the hybrid masking design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ca1ismfgp6", "forum": "Kw2mvnzCoc", "replyto": "Kw2mvnzCoc", "signatures": ["ICLR.cc/2026/Conference/Submission12042/Reviewer_6n4K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12042/Reviewer_6n4K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12042/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823153071, "cdate": 1761823153071, "tmdate": 1762923021208, "mdate": 1762923021208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TSPulse, a 1M parameter pre-trained model for time-series analysis, which is built upon the TSMixer architecture. The authors claim state-of-the-art (SOTA) performance across four distinct downstream tasks: classification, anomaly detection, imputation, and similarity retrieval. The model's purported advantages stem from a collection of \"innovations,\" including:\n\n1.  **Architectural:** A dual-space (time and frequency) masked reconstruction strategy and a dual-embedding disentanglement (detailed vs. semantic) mechanism.\n2.  **Task-Specific:** A `TSLens` fine-tuning module for classification, a `multi-head triangulation` (MHT) method for anomaly detection, and a `hybrid masking` strategy for imputation.\n\nDespite the impressive reported results, the paper suffers from fundamental methodological flaws, a severe lack of focus, and claims that are not substantiated by the experimental design. The work reads less like a scientific contribution and more like an engineering report for a model that has been over-tuned on a specific set of benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "There are several strengths in the paper worth highlighting:\n\n1.  The goal of creating a single, compact model (even if the pre-training is specialized) that excels at four major time-series tasks is highly ambitious and addresses a clear need in the community.\n2.  The paper's strongest contribution is its relentless focus on efficiency. Achieving the reported results with a 1M parameter model is a significant engineering feat, especially the \"GPU-Free\" (CPU-capable) inference. This is a crucial and practical research direction, providing a counter-narrative to the \"bigger is better\" trend in foundation models.\n3.   While the methodology is questionable, the sheer scale of the reported performance gains (e.g., +20% on TSB-AD, +50% on imputation) is eye-catching. It demonstrates that small, well-engineered models can be *tuned* to outperform large, general-purpose ones, which is an interesting finding in itself."}, "weaknesses": {"value": "### 1. Fundamentally Flawed Experimental Premise\n\nThe paper's central narrative, that **one** tiny 1M model can be pre-trained for general-purpose use and subsequently outperform massive foundation models (like MOMENT, Chronos, etc.)—is directly contradicted by the authors' own methodology, which is buried in Appendix A.6.\n\nThe authors state:\n> \"...we specialize the pre-training for every task through reweighting loss objectives...\" And more damningly:\n> \"TSPulse adopts hybrid masking for all reconstruction-oriented tasks (imputation, anomaly detection, retrieval) and block masking specifically for classification-oriented pre-training.\"\nThis is a **fatal flaw**. The authors are not evaluating a single, general-purpose pre-trained model. They are training *multiple, distinct models* where the pre-training objective and masking strategy are *already specialized for the downstream task*.\n\nThis completely invalidates the comparisons against general-purpose foundation models like MOMENT or UniTS. Of course, a model pre-trained *specifically* for imputation using hybrid masking will outperform a general-purpose model on an imputation task that uses hybrid masking. The paper is comparing apples to oranges: a bespoke, task-specific small model against a general-purpose large model. The extraordinary claims of +50% in imputation or +20% in anomaly detection are therefore not surprising, but rather an expected outcome of this flawed, self-serving experimental design.\n\n### 2. Lacking a Coherent Contribution\n\nThe primary weakness of this paper is its overwhelming complexity and lack of a central, verifiable contribution. The authors introduce no fewer than five new, named components: `Dual-space reconstruction`, `dual-embedding disentanglement`, `TSLens`, `multi-head triangulation`, and `hybrid masking`. This obscures any real scientific insight. It is impossible to determine if a single, principled idea is responsible for the alleged gains, or if the model is simply a fragile collection of ad-hoc engineering tricks, each providing a marginal boost on a specific benchmark. The paper fails to present a clear, unified thesis, instead opting for a laundry list of features that make the work difficult to interpret \n\n### 3. Insufficient and Unconvincing Ablation Studies\n\nGiven the model's complexity, the ablation study (Section 5) is the most critical piece of evidence needed to justify the design. The provided study is wholly inadequate.\n\n* The ablations only remove one component at a time from the *final, complex model*. This fails to prove that all components are necessary. A proper study would start from the TSMixer baseline and *add* each component one-by-one to show its marginal contribution.\n* The study fails to disentangle the contributions. For example, in Table 1(b), removing `TSLens` causes a 16% drop, and removing `Dual-space` causes a 7% drop. What happens if *both* are removed? Does the performance revert to the TSMixer baseline, or is there a complex, non-additive interaction?\n* What is the performance of *just* the TSMixer baseline + `TSLens`? What about TSMixer + `Dual-space`? Without this, it's impossible to know if the \"dual-embedding\" or other components are providing any real value or are simply needless complexity. The ablations feel selected to justify the components rather than to genuinely investigate them.\n\n### 4.  Many of the paper's \"novel\" contributions appear to be re-brandings of existing, standard techniques.\n\n* **Dual-Space Learning:** Using both time and frequency domains is a classic signal processing technique. The paper itself cites prior work like **FEDformer** and **BTSF** that already leverage this. Claiming this as a novel contribution for \"unifying\" them in a *lightweight* model is a weak, incremental claim.\n* **Multi-Head Triangulation (MHT):** This is just a simple ensemble. The model computes reconstruction error from three different heads (time, FFT, forecast) and picks the best one (`Headtriang.`) or combines them (`Headensemble`). This is a standard validation/ensemble technique, not a novel anomaly detection framework.\n* **TSLens:** The description (\"selectively attends to and weights features\") is functionally identical to a standard attention mechanism applied as a pooling layer for classification. Giving it a new name does not make it a new contribution.\n* **Hybrid Masking:** This is a data augmentation strategy, not a model innovation. It is an obvious heuristic to combine point and block masking."}, "questions": {"value": "The key weaknesses outlined above raise several questions that the authors need to address:\n\n1.  The core claim of a single, general-purpose model appears to be contradicted by the task-specific pre-training (Appendix A.6). Could the authors please provide results for a **single model**, pre-trained with a **single, unified objective** (e.g., the hybrid-masking, all-head-loss model), and then evaluated on all four downstream tasks? This is essential to validate the paper's central thesis.\n2.  The current ablation study is \"destructive\" (removing parts from the whole). Could the authors provide a \"constructive\" study, starting from the TSMixer baseline and *incrementally adding* each new component (e.g., 1. TSMixer, 2. TSMixer + Dual-Space, 3. TSMixer + Dual-Space + Dual-Embedding, etc.) to clearly demonstrate the marginal performance contribution of each proposed innovation?\n3.  How much of the performance gain is simply due to pre-training the TSMixer backbone on 1B samples, versus the novel architectural components? A crucial missing baseline is a standard TSMixer (with a simple reconstruction head) pre-trained on the *same* 1B sample dataset.\n4.  The `TSLens` module is described as learning to \"focus on the most informative regions.\" How does this mechanism fundamentally differ from a standard attention-pooling layer (e.g., a single-head attention followed by a weighted sum) commonly used in classification heads?\n5.  **Proof of Disentanglement:** The paper claims the dual-embedding approach generates \"detailed\" and \"semantic\" embeddings. Beyond their use in different reconstruction heads, what qualitative or quantitative evidence is there that these embeddings are truly disentangled? For instance, do the \"semantic\" embeddings (from register tokens) show quantifiably more invariance to noise and time shifts compared to the \"detailed\" embeddings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Jx9fRWkI18", "forum": "Kw2mvnzCoc", "replyto": "Kw2mvnzCoc", "signatures": ["ICLR.cc/2026/Conference/Submission12042/Reviewer_RHFo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12042/Reviewer_RHFo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12042/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994946510, "cdate": 1761994946510, "tmdate": 1762923020672, "mdate": 1762923020672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}