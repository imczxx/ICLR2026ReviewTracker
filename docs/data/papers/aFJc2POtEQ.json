{"id": "aFJc2POtEQ", "number": 20574, "cdate": 1758307665519, "mdate": 1759896970163, "content": {"title": "GPTOpt: Towards Efficient LLM-based Black-Box Optimization", "abstract": "Global optimization of expensive, derivative-free black-box functions demands extreme sample efficiency. Classical methods such as Bayesian Optimization (BO) can be effective, but they often require careful parameter tuning to each application domain. At the same time, Large Language Models (LLMs) have shown broad capabilities, yet state-of-the-art models remain limited in solving continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based optimization method that equips LLMs with continuous black-box optimization capabilities. By fine-tuning large language models on extensive synthetic datasets derived from diverse BO parameterizations, GPTOpt leverages LLM pre-training to generalize across optimization tasks. On a variety of black-box optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting the capacity of LLMs for advanced numerical reasoning and introducing a flexible framework for global optimization without parameter tuning.", "tldr": "We teach LLMs to perform black-box optimization through fine-tuning on synthetic datasets.", "keywords": ["Black-box optimization", "Bayesian optimization", "Large language models"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a6f369345eececd9626f2dd3fcf1515e4eac323.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces GPTOpt, a method for black-box optimization that involves fine-tuning a 3B parameter language model (Llama 3.2) on a large, synthetically generated dataset. The training data consists of optimization trajectories from various Bayesian Optimization (BO) configurations, where the \"best\" performing trajectory for each synthetic function is used as an expert demonstration. The goal is to create a \"plug-and-play\" optimizer that requires no hyperparameter tuning. The authors evaluate GPTOpt on holdout synthetic functions as well as standard out-of-distribution benchmarks (BBOB, VLSE), demonstrating that it outperforms individual BO methods and other learned optimizers in low-dimensional settings (up to 10D)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper tackles the well-known challenge of hyperparameter sensitivity in BO\n- The goal of creating a robust, tuning-free optimizer is valuable\n- On the selected low-dimensional benchmarks (BBOB, VLSE), GPTOpt demonstrates impressive performance"}, "weaknesses": {"value": "- The conceptual framework is not original, as the idea of \"learning to optimize\" with transformers is already well-established and several recent works have explored the LLM+BO integration. For example, OptFormer [1] already established the paradigm of treating BBO as a text-based sequence modeling problem and this work follows this template directly.\n-  The authors mentioned that they mapped a continuous space to 1000 discrete points per dimension, which in my eyes makes the method fundamentally incapable of finding precise optima. This design choice makes the optimizer impractical for any real-world application where precision matters\n- The method is only demonstrated on problems with up to 10 dimensions, which is relatively low-dimensional. Many important optimization problems in science and engineering actually involve much higher-dimensional spaces"}, "questions": {"value": "- Could the authors comment on why other prominent learned optimizers and LLM-based BBO frameworks, such as OptFormer and LLAMBO, were not included as baselines in the experiments? \n- How does GPTOpt handle problems where the true optimum lies between the discrete points of your grid?\n- What is the primary bottleneck that limits the method to <10D? Is it context length, the cost of data generation, or a fundamental issue with the model's ability to learn in higher dimensions? \n- Given that the model is trained on trajectories generated by BO, is it possible that a simple ensemble of the baseline BO methods (e.g., selecting the best proposal from all 10 variants at each step) could achieve comparable performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lb1O4gSh04", "forum": "aFJc2POtEQ", "replyto": "aFJc2POtEQ", "signatures": ["ICLR.cc/2026/Conference/Submission20574/Reviewer_p1Ys"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20574/Reviewer_p1Ys"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760860266909, "cdate": 1760860266909, "tmdate": 1762933987133, "mdate": 1762933987133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GPTOpt, aiming at leveraging pretrained LLM for continuous black-box optimization (BBO). GPTOpt is trained with multiple diverse synthetic trajectories collected from different “expert” optimizers, initiated by Bayesian optimization (BO). The LLM serves as a general policy, determining the next step (including the query point and the objective score). Experiments show that GPTOpt outperforms the training experts and a few representative BBO optimizers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The direction of adding BBO ability to LLM is worth studying, capturing the flexibility and universality of LLM for BBO problems, which are usually designed with task-specific solvers.\n- I like the part of diverse synthetic data generation, which is novel in the BBO field.\n- Compared to previous works that also train an optimization policy to outperform expert methods [1-2], GPTOpt can be trained across different dimensionalities, which is important for universal BBO.\n- GPTOpt makes the first step of enhancing LLM’s ability to solve BBO, although some details in the method remains to discuss."}, "weaknesses": {"value": "- The major weakness is, from my perspective, is the limited experimental ablation, which is discussed in detail in the *Questions* part.\n- Detailed prompts for testing are not well illustrated in the experiments. For example, do the prompts used in the experiments contain problem information (since the training example prompt just include *synthetic problem with #dimensions*)?\n- No clear discussion of why we should use pretrained LLM to solve BBO. A strong motivation of utilizing LLMs for BBO is their general intelligence, including the web-scale prior knowledge and reasoning ability to discover better solutions. However, I could not find any persuasive discussion or evidences in the manuscripts.\n- Typos:\n    - line 019: “leverages LLM pre-training to” → “leverages pre-trained LLM to”;\n    - line 051: “basic optimization problems, Huang et al. (2024) but” → “basic optimization problems (Huang et al., 2024), but”;\n    - line 138: “PFNs4BO Muller et al. (2023) provides” → “PFNs4BO (Muller et al., 2023) provides”;\n    - line 159: “the training data provided Chen et al. (2021)” → “the training data provided (Chen et al., 2021)”;\n    - line 191: “the search space $\\subset \\mathcal{X}$” → “the search space $\\mathcal{X}$”;\n    - line 259: “with Unsloth Hu et al. (2022); Daniel Han & team (2023)” → “with Unsloth (Hu et al., 2022; Daniel Han & team, 2023)”;\n    - line 765: “low-rank adaption (LORA) with Unsloth Hu et al. (2022); Daniel Han & team (2023).” → “low-rank adaption (LoRA) with Unsloth (Hu et al., 2022; Daniel Han & team, 2023)”;\n    - line 961: “PFN4BO Muller et al. (2023)” → “PFNs4BO (Muller et al, 2023)”.\n- One more weakness, though not so necessary, is that the paper is not presented well. For example, I suggest the authors adding more formal notation details of the acquisition function used in the expert methods, which could help the readers that are not familiar with this field better understand some terminologies in BBO."}, "questions": {"value": "- Compared baselines: GPTOpt is trained on expert optimization trajectories and can outperform training experts, while [1-3] show similar findings via training a transformer with RL from scratch. How does GPTOpt perform compared to such methods under the same training trajectory data?\n- The necessity of pretrained LLM: In this paper, the authors repeatedly claim the importance of starting with a pretrained LLM, but why do we need pretrained LLM for BBO? In recent practices in text-to-text regression, a field similar to BBO in linguistic space, it shows that a small language model trained from scratch can work well on regression cases [4-5], and pretrained checkpoints seem not so important for performance (Fig. 5 from [6]). I recommend to add some discussions upon this view.\n- The example prompt showed in Section 4.2.1 does not state whether the problem is a maximization problem or a minimization one. What if we invert the problem landscape in the experiments?\n- During inference, you mentioned that you use the model’s predicted value distribution. How does the distribution initiated? Just capturing the histogram distribution of the number tokens at the targeting position? Can the LLM model the ground-truth distribution? What’s its benefit compared to directly applying the LLM as an end-to-end generator for the next query point? If you just use the output distribution of the LLM, why should we finetune the model with all tokens in the step (a comparison of the model before finetuning is recommended)?\n\nOverall, I like the topic this paper discussed and some experiment designs in the manuscript. However, there are still space to improve the paper’s quality. I would reconsider my ratings if my concern are addressed. \n\n## References\n\n[1] Towards Learning Universal Hyperparameter Optimizers with Transformers. NeurIPS 2022.\n\n[2] Reinforced In-Context Black-Box Optimization. IJCAI 2025.\n\n[3] ZeroShotOpt: Towards Zero-Shot Pretrained\nModels for Efficient Black-Box Optimization. arXiv, 2025.\n\n[4] OmniPred: Language Models as Universal Regressors. TMLR, 2024. \n\n[5] Performance Prediction for Large Systems via Text-to-Text Regression. arXiv, 2025. \n\n[6] Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings. ICML 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZXyl1O71pG", "forum": "aFJc2POtEQ", "replyto": "aFJc2POtEQ", "signatures": ["ICLR.cc/2026/Conference/Submission20574/Reviewer_QoBF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20574/Reviewer_QoBF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812135969, "cdate": 1761812135969, "tmdate": 1762933986706, "mdate": 1762933986706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes prompting an LLM to perform zeroth order, black box optimization. A history of function evaluations is given to the model in a prompt, and the LLM (Lamma 3B, specifically) returns the next evaluation point. The authors build a large fine-tuning set comprised of “expert” trajectories of zeroth order optimization problems from a variety of synthetic tasks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear presentation; good writing, clearly presented results. The problem studied, zeroth-order optimization, has endless applications."}, "weaknesses": {"value": "I think the existing literature of LLM-based optimization needs more citations.\n\nWhile the results look promising, the comparisons are weak. Please compare with LLM-based baselines, as well as BO methods that are SOTA on the evaluation tasks (BBOB and VLSE). An ideal comparison would be against a method that was somehow optimized on your expert data  and tested on the same evaluation tasks.\n\nThe ablations, which do more to justify the hyperparameter choices than ablate the model, are really lacking. Ablations should show how each of your design choices was important in achieving your results. For example, why did you choose the tasks to fine tune on, and how did you decide on the number of tasks to choose? Why was mapping to 1 to 999 correct and, say, not 1 to 500? Why did you choose the specific number of synthetic functions to train on? Why LoRa, and how would other fine-tuning methods work? Why the 3B model class?"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t0POeroixN", "forum": "aFJc2POtEQ", "replyto": "aFJc2POtEQ", "signatures": ["ICLR.cc/2026/Conference/Submission20574/Reviewer_tswa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20574/Reviewer_tswa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944794998, "cdate": 1761944794998, "tmdate": 1762933986288, "mdate": 1762933986288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This proposes a method to train large language models to perform continuous black-box optimization by fine-tuning them on millions of synthetic optimization trajectories generated from various Bayesian optimization algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s main contribution is the formulation of black-box optimization as a reasoning and sequence prediction problem for large language models, enabling optimization to be approached through text-based decision-making rather than analytical computation. It introduces GPTOpt, a fine-tuned LLM trained on millions of synthetic optimization trajectories generated by diverse Bayesian optimization methods, allowing it to learn generalizable optimization strategies."}, "weaknesses": {"value": "1. The approach of teaching an LLM to perform numerical optimization is conceptually questionable, as language models are not designed for precise arithmetic or quantitative reasoning.\n\n2. The experiments are limited to low-dimensional problems (up to 10D), raising concerns about the method’s scalability and effectiveness in higher-dimensional or more complex optimization tasks.\n\n3. Consequently, the general applicability of GPTOpt to real-world, high-dimensional optimization scenarios remains uncertain."}, "questions": {"value": "If computation allowed, I'd be curious to see how larger models with reasoning ability can improve the optimization performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8WehnNzNxz", "forum": "aFJc2POtEQ", "replyto": "aFJc2POtEQ", "signatures": ["ICLR.cc/2026/Conference/Submission20574/Reviewer_ZSq7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20574/Reviewer_ZSq7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990194161, "cdate": 1761990194161, "tmdate": 1762933985820, "mdate": 1762933985820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}