{"id": "L7rVSAv1b4", "number": 23825, "cdate": 1758348949456, "mdate": 1759896795128, "content": {"title": "CONCUR: Benchmarking LLMs for Concurrent Code Generation", "abstract": "Leveraging Large Language Models (LLMs) for code generation has increasingly emerged as a common practice in the domain of software engineering. Relevant benchmarks have been established to evaluate the code generation capabilities of LLMs. However, existing benchmarks focus primarily on sequential code, lacking the ability to effectively evaluate LLMs on concurrent code generation. Compared to sequential code, concurrent code exhibits greater complexity and possesses unique types of bugs, such as deadlocks and race conditions, that do not occur in sequential code. Therefore, a benchmark for evaluating sequential code generation cannot be useful for evaluating concurrent code generation with LLMs. To address this gap, we designed a benchmark CONCUR specifically aimed at evaluating the capability of LLMs to generate concurrent code. CONCUR consists of 43 curated concurrency problems and leverages formal methods techniques, namely model checking, to assess the correctness of the generated code. We conducted an evaluation of a range of LLMs on CONCUR, highlighting limitations of current models. Overall, our work provides a novel direction for evaluating the capability of LLMs to generate code with focus on concurrency.", "tldr": "Benchmark for concurrent code generation by LLMs.", "keywords": ["benchmark", "concurrent code", "large language model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef70ef595ab4e6dbbe2dfae0e550ce311492ac55.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CONCUR, the first benchmark specifically designed to evaluate large language models on concurrent program generation — a domain much more complex than sequential code due to non-deterministic thread scheduling, synchronization, and concurrency-specific bugs (e.g., deadlocks, race conditions, starvation).\nThe benchmark includes 43 curated Java concurrency problems derived from Java Concurrency in Practice (Goetz, 2006), each with structured prompts and verified ground-truth solutions.\n\nCONCUR employs formal verification via Java Pathfinder for exhaustive state-space exploration, detecting concurrency issues beyond what static metrics or unit tests can capture. The authors evaluate 22 state-of-the-art LLMs (including GPT-5, Claude-Opus-4.1, GPT-4o, Qwen-3, DeepSeek-R1, etc.) and demonstrate that while LLMs can often produce compilable code, they frequently fail under full concurrency verification.\nThe study further shows that static similarity metrics like CodeBLEU fail to correlate with true correctness, emphasizing the need for formal, dynamic evaluation frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Benchmark Domain\n\nComprehensive benchmark targeting concurrent code generation, addressing a crucial but overlooked area in code intelligence research.\n\n- Formal Verification Integration\n\nThe use of model checking introduces rigor, enabling the detection of deep concurrency bugs (deadlocks, race conditions) that conventional test-based evaluation misses.\n\n- Comprehensive Empirical Evaluation\n\nEvaluates 22 diverse LLMs under uniform conditions, includes manual validation, and provides a public leaderboard and dataset."}, "weaknesses": {"value": "- Limited Dataset Scale and Diversity\n\nOnly 43 problems, all from a single Java text book, limit coverage and generalization to broader concurrency paradigms (e.g., message passing, lock-free, or distributed models).\n\n- Single-Language Restriction (Java 8)\n\nThe benchmark excludes other major concurrency ecosystems like C++, Go, or Rust, reducing cross-language insight.\n\n- Partial Coverage of Concurrency Semantics\n\nJPF bounds and model-checking limitations (e.g., no livelock detection, time depth cutoffs) may miss certain concurrency issues, limiting completeness of the evaluation.\n\n- Evaluation granularity\n\nThe analysis could benefit from qualitative insights into why models fail (e.g., incorrect synchronization pattern, wrong locking scope).\nNo ablation on prompt variants or temperature settings."}, "questions": {"value": "– Do the authors plan to extend CONCUR to multiple programming languages (e.g., Go, Rust) that have different concurrency models? This would broaden its applicability and reveal model generalization across paradigms.\n\n– Since JPF focuses on low-level interleavings, have the authors considered incorporating semantic invariants or assertion checking to detect higher-level correctness violations (e.g., protocol violations, data consistency)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AKmmHkmpPB", "forum": "L7rVSAv1b4", "replyto": "L7rVSAv1b4", "signatures": ["ICLR.cc/2026/Conference/Submission23825/Reviewer_fAkt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23825/Reviewer_fAkt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760882663586, "cdate": 1760882663586, "tmdate": 1762942822395, "mdate": 1762942822395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors develop a new benchmark for concurrent programming, which consists of 43 curated concurrency problems drawn from a textbook, with structured prompts and ground-truth implementations in Java.  The authors evaluate 22 LLMs on the benchmark.\n\nUnlike most coding benchmarks, success is not determined by the ability to pass unit tests, because concurrent programming bugs like race conditions and deadlocks are notoriously difficult to catch with unit tests.  Instead, the authors perform model-checking with Java Pathfinder (JPF), to find concurrency-related bugs in the output code.  \n\nSomewhat surprisingly, the vast majority of errors are still compilation errors, with the majority being syntax errors.  LLMs still struggle to write syntactically valid code.   A second surprise is that LLMs often implement conceptually correct code, but then fail to actually spawn multiple threads to execute it.  \n\nThe authors find that CodeBLEU is a poor metric for code quality."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The benchmark seems to be well-designed, and the evaluation of the 22 LLMs is thorough, covering all of the important models.  \n\nBy far the greatest strength of this benchmark is its use of model-checking to catch concurrency bugs.  As the use of coding LLMs continues to proliferate, so will LLM-introduced bugs, and concurrency-related bugs like race conditions are notoriously difficult to catch. Formal static or dynamic analysis is currently severely underused as a way to evaluate code quality, so I will champion this paper as an important milestone in teaching LLMs to write correct code, by using formal measures of code correctness.  Hopefully future work will follow the same path."}, "weaknesses": {"value": "The benchmark only contains 43 problems.  \n\nPerhaps most importantly the problems are all drawn from a textbook, which was published almost 20 years ago.  This means that the textbook, or similar problems, are likely in the training data of SOTA LLMs.  \n\nThe benchmark would be strengthened by having more problems, and including not-previously published problems.  It would be interesting to include problems in a language other than Java -- e.g. the Rust type system also protects against concurrency bugs."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K7DKtFIywt", "forum": "L7rVSAv1b4", "replyto": "L7rVSAv1b4", "signatures": ["ICLR.cc/2026/Conference/Submission23825/Reviewer_DzG4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23825/Reviewer_DzG4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868883708, "cdate": 1761868883708, "tmdate": 1762942822031, "mdate": 1762942822031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark that measures how well code generation models are able to generate concurrent Java code. The authors construct a small problem set sourced from a Java concurrency textbook and measure success based on compilation success and Java PathFinder (JPF) verification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is well-written and motivations are sound (most code benchmarks focus on single proc. code).\n- Thoughtful methodology. Steps are taken to ensure solutions can be verified without excessive computation resources (e.g. by limiting num threads, etc)\n- Paper shows benefit of going beyond CodeBLEU and provides error analysis of top models."}, "weaknesses": {"value": "- Benchmark is simple and problems are sourced from a textbook published in 2006. The proposed test set is only 43 problems, curated by the authors, which is extremely small. Additionally there is a risk of contamination as models may have trained on this textbook. The authors do not provide any insight into the contamination risk. \n- Evaluation metric is based on compilation and verification success, not test case passing. Due to the nature of the problems (which are presented without completed solutions in the original data source), the authors resort to compilation and JPF based checks to measure success. However, the best verification for code is running against a test suite as code that can compile and pass JPF checks could still be wrong. \n- Benchmark initial performance is quite high-- GPT-5 is at 72% pass@1."}, "questions": {"value": "1) Can the authors explain any experiments or measures taken to avoid contamination given that the textbook may be in the training set of these LLMs?\n2) Is the proposed test set (just 43 problems) a statistically significant sample size for estimating concurrent code generation capabilities?\n3) Why is compilation success + JPF verification a sufficient measure of performance (instead of say compilation + JPF + running code against test suite)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tIUKL4fRi2", "forum": "L7rVSAv1b4", "replyto": "L7rVSAv1b4", "signatures": ["ICLR.cc/2026/Conference/Submission23825/Reviewer_oHT5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23825/Reviewer_oHT5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927831720, "cdate": 1761927831720, "tmdate": 1762942821786, "mdate": 1762942821786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces CONCUR, a benchmark specifically designed to evaluate large language models on concurrent code generation. It contains 43 carefully curated multi-threading problems with structured prompts and validated Java implementations. Unlike existing benchmarks focusing on sequential code, CONCUR leverages Java PathFinder (JPF) for dynamic, exhaustive verification of generated programs, detecting concurrency errors such as deadlocks, race conditions, and single-thread violations. By integrating structured prompts, ground-truth solutions, and dynamic model checking, CONCUR provides a rigorous and reproducible framework to systematically assess LLMs’ ability to produce correct multi-threaded programs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty in Benchmark Design: The paper introduces CONCUR, the first benchmark specifically targeting multi-threaded code generation, filling a gap left by prior benchmarks that focus only on sequential programs.\n\n2. Concurrency-Aware Problem Construction: The benchmark is carefully designed to enforce multi-threading features and concurrency-specific requirements, ensuring that generated programs must exhibit correct thread behavior and handle potential concurrency issues.\n\n3. Clear Presentation and Comprehensive Validation: The paper is well-structured and clearly written, and it demonstrates the benchmark’s effectiveness by evaluating LLMs of various sizes and architectures, providing strong evidence for its reliability in systematically assessing concurrent code generation."}, "weaknesses": {"value": "1. The benchmark includes only 43 Java programs, which is a relatively small number and may limit its coverage of diverse concurrent programming scenarios.\n\n2. Although prompts for each program are provided in the public repository, the programs themselves are simple in functionality and description, which may not effectively evaluate LLMs’ ability to generate complex multi-threaded code."}, "questions": {"value": "1. Could the benchmark be expanded with additional test programs to enhance coverage and diversity of concurrent scenarios?\n\n2. Could the authors discuss the complexity of program functionality and explain the code selection process in more detail? Additionally, could they consider including more complex, real-world concurrent programs that better reflect practical programming scenarios?\n\n3. Could the benchmark extend the oracle to include dynamic testing oracles, if feasible, for more comprehensive correctness evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FuQlxCXqER", "forum": "L7rVSAv1b4", "replyto": "L7rVSAv1b4", "signatures": ["ICLR.cc/2026/Conference/Submission23825/Reviewer_bG35"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23825/Reviewer_bG35"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982000965, "cdate": 1761982000965, "tmdate": 1762942821397, "mdate": 1762942821397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}