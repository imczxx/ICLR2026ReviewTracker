{"id": "5neiC4vYxV", "number": 15423, "cdate": 1758251218486, "mdate": 1759897307860, "content": {"title": "Self-Guided Explanation for Graph Neural Networks with Semi-Supervision", "abstract": "Post-hoc explanation for graph neural networks (GNNs) is the task of explaining their decisions by identifying important subgraphs.\nSince discretization is non-differentiable, most prior work models explainers that output continuous edge-importance scores, often yielding blurry, mixed score distributions.\nThis stems from optimizing only to preserve the original prediction without effective regularization and in the absence of edge-level ground-truth labels.\nWe present 3SG-Explainer (Semi-Supervised and Self- Guided Explainer), which converts weak prediction-preserving signals into explicit edge supervision and, in turn, markedly improves explanation accuracy while sharply polarizing edges into important versus unimportant.\nConcretely, we introduce confidence-based thresholds to convert noisy soft scores into semi-supervised pseudo-labels, then train a lightweight message-passing explainer on these labels.\nWe also prove that the improved shapes of the score distributions produced by 3SG-Explainer hold against unsupervised baselines.\nExperiments on four benchmarks and multiple metrics show that 3SG-Explainer improves the accuracy for edge-level explanation over state-of-the-art baselines.", "tldr": "We turn weak, noisy soft scores from GNN explainers into sharp, binarized edge supervision through self-guided pseudo-labeling, achieving both higher accuracy and human-level interpretability.", "keywords": ["Graph neural networks", "GNN explainability", "Semi-supervised learning", "Pseudo-labeling"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4d1d9ab704cdbfef1259b8c91bae0a66da69401.pdf", "supplementary_material": "/attachment/8103586fa73e0fd93ce1e3613d6da9b7e491b842.zip"}, "replies": [{"content": {"summary": {"value": "This paper essentially introduces a post-hoc binarization framework for GNN explainers. It extracts high-confidence pseudo-labels from the continuous edge scores of existing GIB-based explainers (e.g., PGExplainer) and trains a GCN to propagate these binary signals across the graph structure. Through iterative self-guided refinement, the edge importance scores become increasingly polarized toward 0 or 1. The theoretical analysis shows that conventional GIB optimization objectives tend to converge to intermediate, non-binary values, while their semi-supervised BCE training drives the scores away from 0.5 and spreads the binary trend to unlabeled edges. As a result, the explanations become more distinct and human-interpretable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation for self-guided explanation is well-grounded. The paper identifies a real limitation in current GNN explainers, where continuous, non-binarized edge importance scores hinder interpretability, and they aim to address it in a principled way.\n2. The proposed semi-supervised framework that propagates high-confidence binary signals through message passing is conceptually simple yet effective. It aligns well with the structure of GNNs and provides a natural mechanism for refining explanations.\n3. Theoretical analysis helps explain why traditional GIB-based objectives tend to produce ambiguous scores, and why the proposed semi-supervised BCE loss yields more polarized distributions. This strengthens the conceptual soundness of the approach."}, "weaknesses": {"value": "### 1. Limited scope of base explainers.\nThe paper only focuses on optimization-based explainers within the GIB family. However, search-based methods such as SubgraphX and attribution-based methods such as GOAt or GNN-LRP also output confidence scores. It should be possible to use these methods to obtain the initial pseudo-binary edge labels for the first round. Moreover, since GIB-based explainers inherently produce confidence scores clustered around the middle range, while other types of explainers may already yield more polarized edge scores, the need for additional binarization might not arise. Therefore, comparing this method against non-GIB explainers becomes even more necessary. The paper does not discuss this issue or include such comparisons, which makes the evaluation less convincing.\n### 2. Pseudo-label noise accumulation.\nThe authors only prove that their GCN-based guided explainer can converge to binary predictions when the pseudo-label accuracy is greater than 0.5. However, they do not verify whether these binarized labels are truly reliable. There is a risk of pseudo-label drift: if the base explainer’s initial scores are inaccurate, the resulting pseudo-labels may be wrong and such noise could accumulate and be reinforced through self-guided iterations. Although the authors introduce a confidence interval to exclude uncertain edges, this does not guarantee the correctness of high-confidence edges.\n### 3. Threshold sensitivity.\nThreshold sensitivity. The thresholds are determined by the skewness-based quantile rule, but they still depend on manually set parameters $\\alpha$ and c. Since the score distributions vary significantly across datasets, the chosen thresholds may not generalize well. Without sensitivity or robustness analysis, the stability of the pseudo-labeling process remains uncertain.\n### 4. Potential propagation bias.\nPropagation bias. The guided explainer propagates confidence signals through message passing. If the high-confidence regions themselves are biased or incorrect (for example, due to structural imbalance in molecular graphs), this bias could be amplified and spread across the graph. The paper does not analyze this potential issue.\n### 5. Interpretability paradox.\nThere is a conceptual concern: the method introduces another GCN to explain a GCN-based model. This design may appear recursive, where they use a neural network to interpret another neural network, and somewhat undermines the goal of interpretability.\n### 6. Experimental limitations.\nThe paper does not evaluate fidelity, lacks qualitative visualization of the generated explanations, and uses a limited number of datasets. The real-world interpretability, particularly on molecular datasets, remains unclear."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uqH7YJpU7E", "forum": "5neiC4vYxV", "replyto": "5neiC4vYxV", "signatures": ["ICLR.cc/2026/Conference/Submission15423/Reviewer_KmWJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15423/Reviewer_KmWJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858875397, "cdate": 1761858875397, "tmdate": 1762925701474, "mdate": 1762925701474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes 3SG-Explainer, a novel method for enhancing the interpretability of graph neural networks (GNNs). The approach leverages a self-guided semi-supervised learning framework that integrates pseudo-label generation and score distribution modeling to produce discretized explanation masks that are more interpretable to humans. Experimental results demonstrate that 3SG-Explainer consistently outperforms existing explainers across multiple datasets, achieving superior performance in both F1 and AUC metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel self-guided semi-supervised framework that effectively combines pseudo-labeling and distribution-based thresholding to enhance interpretability in GNNs.\n\n2. By generating discretized explanation masks, the method produces clearer and more human-understandable interpretations compared to continuous-score explainers.\n\n3. Extensive experiments on multiple datasets show consistent performance gains in both F1 and AUC metrics, while analyses on binarization and bimodality further validate the improved discreteness of the generated masks."}, "weaknesses": {"value": "1. Please consider including more synthetic and real-world datasets, along with additional baselines, to further validate the effectiveness and generalizability of the proposed method.\n2. The introduction of the Guided Explainer module may introduce additional computational overhead. It would be helpful to provide a complexity analysis or runtime comparison to clarify its efficiency.\n3. The method uses PGExplainer as the base explainer; however, PGExplainer is known to suffer from out-of-distribution (OOD) issues, which may lead to inaccurate masks and, consequently, noisy pseudo-labels. Please provide further clarification or experiments to address this concern."}, "questions": {"value": "1. It is known that GIB-based methods such as PGExplainer are quite sensitive to hyperparameter settings. It would be helpful to include additional analysis or discussion based on their hyperparameter configurations to further explain the limitations of GIB-based methods in mask discretization capability. Moreover, approaches such as adopting a top-k selection strategy or increasing the weight of regularization losses may also lead to more discrete mask outputs. Therefore, additional clarification is needed to justify the necessity and distinct advantage of the proposed method in explicitly addressing the mask discretization problem.\n2. As shown in Figure 3, except for the BA3 dataset, the mask optimization appears suboptimal in other datasets. For instance, in the Fluorid-Carbonyl dataset, some important edges are misclassified as unimportant, while in Mutagenicity, the non-important mask distribution seems scattered. Please consider adding further explanations or supplementary experiments.\n3. It is unclear how the thresholds are determined and how sensitive the explainer is to these threshold values. Please include a hyperparameter sensitivity analysis to strengthen the experimental evidence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XdOV7P0oti", "forum": "5neiC4vYxV", "replyto": "5neiC4vYxV", "signatures": ["ICLR.cc/2026/Conference/Submission15423/Reviewer_VDcY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15423/Reviewer_VDcY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991689152, "cdate": 1761991689152, "tmdate": 1762925700428, "mdate": 1762925700428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces 3SG-Explainer, a self-guided multi-stage framework for explaining GNNs. It transforms soft edge-importance scores from a base explainer into pseudo-labels and trains a guided GNN explainer under direct supervision. A label-conditioned graph generator augments pseudo-labeled graphs to enhance robustness. Theoretical analysis suggests a tighter VC-dimension–based generalization bound than single-stage explainers. Experiments on four benchmarks show consistent AUC gains and stable performance across various base explainers and generators."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation and simple formulation. The paper clearly identifies the limitation of non-binarized explanations and proposes a clean, self-guided remedy without architectural complexity.\n\n- Sound theoretical grounding. The analysis of GIB’s non-binarization, BCE-driven polarization, and tail-risk generalization offers valuable intuition and rigor.\n\n- Empirical validation. Improvements in both performance (AUC/F1) and interpretability metrics (binarization, bimodality) are consistent across datasets and base explainers.\n\n- Readable and modular. The framework can be readily integrated with existing explainers and GNN architectures."}, "weaknesses": {"value": "- Dependence on pseudo-label quality. The method assumes reliable confidence estimation from the base explainer; if those scores are noisy, pseudo-label errors may propagate. While the theory includes a tail-noise term, empirical robustness tests are limited.\n\n- Hyperparameter sensitivity. The approach introduces new hyper-parameters: confidence-quantile threshold alpha_0,  skew-adjustment factor c, and a class weight for balancing positive edges. However, the paper does not provide a detailed analysis of how these parameters affect performance or stability. This lack of sensitivity study partially offsets the simplicity of the design and makes it unclear how robust the method is to different hyperparameter settings.\n\n- Efficiency not reported. Multi-round retraining increases computational cost, yet runtime or scaling comparisons with single-round baselines are not provided.\n\n- Limited qualitative insight. Although the distributional metrics show better polarization, there is little qualitative or domain-specific evidence that the resulting subgraphs are semantically meaningful."}, "questions": {"value": "- How robust is the method to noisy or poorly calibrated base explainers? For instance, if the confidence scores are less separable, does the self-guided training still converge to meaningful polarization?\n\n- Can you analyze how the newly introduced hyper-parameter affect the performance?  A empirical sensitivity analysis is needed here. Is there a practical guideline or heuristic for tuning them across datasets?\n\n- What is the additional training-time cost introduced by multi-round retraining, and how does it scale with graph size or number of rounds compared to single-stage explainers?\n\n- Can you provide more qualitative examples or case studies (e.g., molecular motifs) demonstrating that the polarized explanations correspond to meaningful structural patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BiOD8UB9Nm", "forum": "5neiC4vYxV", "replyto": "5neiC4vYxV", "signatures": ["ICLR.cc/2026/Conference/Submission15423/Reviewer_xhoi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15423/Reviewer_xhoi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993739091, "cdate": 1761993739091, "tmdate": 1762925699029, "mdate": 1762925699029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes 3SG-Explainer, which generates clear explanatory subgraphs by polarizing edge-importance scores. 3SG-Explainer first employs a pretrained base explainer to produce soft importance scores, then uses skewness-adaptive analysis to identify high-confidence regions and assign pseudo-labels to them. Finally, a guided explainer is trained in a semi-supervised manner, where the GNN architecture propagates supervision signals to unlabeled edges. Extensive experiments on four benchmark datasets demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a rigorous theoretical foundation for deriving the binarization for GIB.\n2. The paper is easy to follow, and the figures are well-designed and visually appealing.\n3. Extensive experiments of this paper support the method proposed in this paper."}, "weaknesses": {"value": "1. The construction of the proposed method lacks a clear motivation. For example, in Equation 1, why was Fisher’s moment coefficient of skewness chosen to measure skewness?\n2. Could the authors provide more visualization experiments to intuitively demonstrate the model’s effectiveness?\n3. The paper could include some more recent baselines to strengthen the persuasiveness of the results, such as RegExplainer.\n4. In Figure 4, why does the F1 score for some dataset remain almost unchanged, even though other metrics (bimodality and binarization score) continue to improve in later rounds?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NQoPyGeqV3", "forum": "5neiC4vYxV", "replyto": "5neiC4vYxV", "signatures": ["ICLR.cc/2026/Conference/Submission15423/Reviewer_yC86"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15423/Reviewer_yC86"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994777893, "cdate": 1761994777893, "tmdate": 1762925698611, "mdate": 1762925698611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their constructive and detailed feedback.\nIn response, we have revised both the main paper and the appendix to strengthen empirical validation and expand qualitative and quantitative analysis.\nBelow, we summarize the major changes made in the revised manuscript.\n\n## **1. Additional Baselines**\nWe incorporated two recent non-GIB explainers, **Eig-Search** and **GOAt**, into the main quantitative evaluation.  \nThese additions appear in **Tables 1 and 2**, broadening coverage beyond the originally included GIB-based methods.\nAcross datasets, our method continues to outperform these baselines by a clear margin.\n\n\n## **2. Additional Real-World Dataset**\nWe added the **Benzene** molecular dataset to improve real-world evaluation and generalizability.\nExperimental results for this dataset were integrated into **Tables 1, 2, and 3**, with additional discussion in **Appendix G-I**.\nOn this dataset as well, the our method achieves strong performance and maintains consistent gains over baselines.\n\n## **3. Robustness Analysis**\nWe introduced two complementary robustness studies:  \n- Hyperparameter sensitivity analysis (**Appendix G, Figure 5**)  \n- Noise-injection robustness (**Appendix I, Figure 6**)  \n\nThese experiments show that performance remains stable under a wide range of settings and imperfect pseudo-labels.\n\n## **4. Time-Cost and Efficiency Analysis**\nWe added a detailed analysis of computational cost, including both training-time overhead and inference efficiency.  \nThe full results are presented in **Appendix H** and **Table 7**, clarifying that the multi-round refinement introduces only a small overhead relative to the base explainer.\n\n## **5. Expanded Visualization Results**\nWe added extensive qualitative visualizations for BA-3motifs, Mutagenicity, and Fluoride-Carbonyl with comparisons to various baselines.  \nThese results, included in **Appendix J**, illustrate that the refinement step recovers more complete and coherent rationale subgraphs."}}, "id": "cjF0f2Surt", "forum": "5neiC4vYxV", "replyto": "5neiC4vYxV", "signatures": ["ICLR.cc/2026/Conference/Submission15423/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15423/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission15423/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763741719670, "cdate": 1763741719670, "tmdate": 1763741938420, "mdate": 1763741938420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}