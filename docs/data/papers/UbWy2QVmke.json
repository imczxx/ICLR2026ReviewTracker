{"id": "UbWy2QVmke", "number": 25338, "cdate": 1758366919695, "mdate": 1759896724200, "content": {"title": "GAA-PtrNet: Graph attention aggregation-based pointer network for one-shot DAG scheduling", "abstract": "Optimizing Directed Acyclic Graph (DAG) workflow makespan by scheduling techniques is a critical issue in the high performance computing area. Many studies in recent years combined Pointer Network (PtrNet) with reinforcement learning (RL) to schedule DAGs by generating DAG task priorities in a sequence-to-sequence manner. However, these PtrNet-based scheduling methods need to repeatedly compute the decoder's hidden state or context embeddings according to the recent local decisions, which leads to limited capability of exploiting the DAG global topological structure, high computation complexity and inability to achieve one-shot scheduling. To address these issues, we propose GAA-PtrNet, a novel PtrNet based on graph attention aggregation (GAA) for one-shot DAG workflow scheduling. In GAA-PtrNet, we compute the pair-wise graph attention scores among nodes in one-shot, then directly aggregate these scores to obtain the probability of selecting candidate nodes. Consequently, the explicit decoder or context embedding structure in PtrNet is omitted in our GAA-PtrNet, and the network takes only one shot forward propagation to infer a solution for a whole DAG scheduling problem, significantly reducing the computation complexity. Additionally, to train GAA-PtrNet, we design a training strategy based on policy gradient RL with dense reward signal and demonstration learning. To our knowledge, GAA-PtrNet is the first network model to achieve PtrNet-based one-shot DAG scheduling. GAA-PtrNet can better handle with DAG workflow structures, providing high quality DAG scheduling solutions. The experimental results show that the proposed method is superior in terms of objective and runs about 10 times faster when compared to previous PtrNet-based methods, and also performs better than other learning-based DAG scheduling methods.", "tldr": "", "keywords": ["DAG  Scheduling", "Graph Attention", "Pointer Network", "Reinforcement Learning", "Combinatorial Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92e9128d1c3a8308f5df44f2882a3fb263fd4eda.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors proposed a Graph Attention Aggregation-based Pointer Network (GAA-PtrNet) for the Directed Acyclic Graph (DAG) scheduling problem to address the issues of traditional PtrNet in DAG scheduling, such as reliance on local decisions, high computational complexity, and inability to generate a scheduling plan in one shot. The model employs graph attention to compute the attention scores between nodes in one-shot, followed by an aggregation operation to obtain the probability of selecting candidate nodes. In addition, the authors designed a training strategy based on policy gradient reinforcement learning (RL). Through ablation and comparative experiments, the paper demonstrates the effectiveness of the proposed model architecture and its superiority in solving the DAG scheduling problem. In addition, this method is nearly 10 times faster than the traditional PtrNet."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors innovatively use graph attention to compute attention scores between graph nodes in a one-shot manner, thereby addressing the high computational complexity caused by repeated calculations in traditional PtrNet-based DAG scheduling methods and improving computational efficiency.\n\n2. Through extensive comparative experiments, the authors compare their approach with existing heuristic methods and state-of-the-art reinforcement learning methods for DAG scheduling, verifying the superiority of the proposed method in both solution quality and computational efficiency.\n\n3. Well-structured with clear explanations of key concepts via formulas and examples."}, "weaknesses": {"value": "1. Some parts of the method description are still unclear and require further clarification:  \n(1) The role of the virtual entry node needs further explanation. The paper states that a virtual entry node is created for computational convenience but does not specify details—for instance, whether all tasks can start dependency searches from this node without separately identifying tasks without predecessors. Readers may wonder what specific problem this node solves.  \n(2) The distinction between one-shot scheduling and step-by-step scheduling could be further compared. The paper mentions that the new method is one-shot, while traditional methods are step-by-step, but it does not clarify whether \"one-shot\" means all orders are arranged instantaneously, or whether dependencies are computed first before selecting tasks sequentially. Readers might misinterpret it as all tasks starting simultaneously. It should be clearly explained that \"the ordering process computes all dependencies at once, while task selection remains sequential, without the need for repeated computations.\"  \n(3) Since Solution(G) represents a topological order of the DAG, different Solution(G) sequences may correspond to the same DAG. Will this many-to-one relationship have any impact on the model's performance or learning stability?  \n(4) The authors appear to use the EFT-greedy rule to decode Solution(G) and assign tasks to different processors. Why not integrate this process into the model itself?  \n(5) In Section 3.2, it is mentioned that \"the attention module is applied to the reverse DAG.\" This point is crucial for understanding the flow of attention and should be highlighted more prominently in the main text, along with an explanation of its necessity.\n\n2. Some aspects of the experiments are not sufficiently explored:  \n(1) The comparison algorithms are relatively old. If there are new algorithms from the past two years, they should be included in the comparison. If not, please explain the basis for selecting the comparison algorithms.  \n(2) What are the genetic algorithm parameters in demonstration learning, and how do they impact the model?  \n(3) Could you supplement ablation experiments to clarify how GAA-PtrNet-SA and GAA-PtrNet-GAT perform under different DAG structures?\n\n3. The outlook could be further focused and enhanced:  \nDirections such as multi-objective optimization and dynamic resource adaptation lack specific technical pathways."}, "questions": {"value": "1. The experimental section could be supplemented with descriptions of the selected datasets. The experiments used datasets such as TPC-H and Pegasus, but there is no explanation of the practical scenarios these datasets correspond to. For example, TPC-H is similar to an e-commerce data processing scenario, while Pegasus is for scientific computing. Readers who are unaware of the significance of these datasets may not understand the practicality of the experimental results.  \n\n2. Although it is mentioned that \"the advantage becomes more obvious as the number of nodes increases,\" adding a time curve graph showing scalability as the number of nodes grows would provide a more intuitive demonstration.  \n\n3. It is recommended to add a paragraph after Figure 3 explaining why the attention scores need to be summed column-wise to serve as probabilities and justifying the rationality of this aggregation method.  \n\n4. Regarding the comparison with Jeon et al. (2023), in addition to mentioning its training instability, could further analysis be provided on the fundamental differences in model structure or objective function compared to the method proposed in this paper?  \n\n5. Include a brief outlook on how this research could be applied in the future. The paper concludes by mentioning expansion to more high-performance computing applications, but readers may not have a clear idea of specific use cases. This would help readers understand that the research is not merely theoretical but has broader practical value.  \n\n6. Figure presentation: Figure 1 illustrates the structure of the existing PtrNet and the proposed GAA-PtrNet, including their differences. My suggestion is to highlight the faster computation speed of GAA-PtrNet to emphasize the novelty and contribution of this paper. One feasible approach is to visually indicate within the dashed box that the existing PtrNet exhibits slower computation, while GAA-PtrNet performs the pair-wise node attention and aggregation operations much faster. In addition, Figure 1 could be further refined for better visual presentation.  \n\n7. Formatting and typographical issues:  \n(1) On the end of Page 6 and the beginning of Page 7, \"Instead of using the a single makespan value\" should be changed to \"Instead of using the single makespan value.\"  \n(2) Multiple references appear within a single pair of parentheses in Line 48; similar issues occur elsewhere in the paper.  \n(3) When referring to equations, the first letter should be capitalized, e.g., Equation 1 instead of equation 1; similar issues occur elsewhere in the paper.  \n(4) Replace \"Table 5-8\" with \"Figure 5-8\" in Line 921."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "78pPWVlgjc", "forum": "UbWy2QVmke", "replyto": "UbWy2QVmke", "signatures": ["ICLR.cc/2026/Conference/Submission25338/Reviewer_vDpe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25338/Reviewer_vDpe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798903191, "cdate": 1761798903191, "tmdate": 1762943407439, "mdate": 1762943407439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GAA-PtrNet, a method within the Pointer Network (PtrNet) family, for optimizing Directed Acyclic Graph (DAG) workflow scheduling. The technique is specifically designed to overcome the high computational complexity and reliance on local information found in traditional PtrNet schedulers, which must repeatedly compute decoder states. The authors demonstrate that by using a graph attention aggregation (GAA) mechanism, the network can compute all pair-wise node attention scores in a single forward pass. These pre-computed scores are then sequentially aggregated during the step-by-step scheduling process to determine task probabilities, enabling solutions that are significantly faster (about 10 times) and of higher quality than previous PtrNet-based approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Scheduling Method: The paper's core novelty is its method for bypassing the sequential bottlenecks of traditional PtrNets, computing all pair-wise graph attention scores in a single forward pass (\"one-shot\"). This pre-computation of scores, followed by a lightweight sequential aggregation, avoids the repeated decoder or context embedding calculations required by traditional PtrNet models.\n\n2. Strong Empirical Performance: The proposed method demonstrates superior performance in schedule quality across multiple benchmarks (TPC-H, Pegasus, and random DAGs). In experimental results, GAA-PtrNet consistently achieves a lower makespan and better \"gap/%\" than both previous PtrNet-based methods (PtrNet-LSTM, PtrNet-CE) and other modern learning-based schedulers.\n\n3. Superior Scalability and Runtime: The model is empirically shown to be significantly faster, running about 10 times faster than previous PtrNet methods. This is supported by a time complexity analysis, which shows that unlike competitors, the iterative part of the scheduling algorithm is independent of the network's embedding dimension $dim$, making it scale much better as the number of nodes ($|V|$) increases."}, "weaknesses": {"value": "1. Misleading Title and \"One-Shot\" Claim: While the attention score computation is indeed one-shot, the overall scheduling process remains sequential, as shown in Algorithm 1. The title and abstract could better distinguish between one-shot attention computation and stepwise decision sampling to avoid confusion.\n\n2. Ambiguous and Inconsistent Notation ($X$ vs. $h_i$): The paper uses notation inconsistently, particularly for node features. * In Section 2.1, $X=\\{x_{1},x_{2},...,x_{|V|}\\}$ is defined as the raw \"attribution of each task node\". * In Section 2.3, $h_{i}$ is introduced without relation to $x_i$ as the \"feature vectors of each task node\" that are *inputs* to an LSTM encoder in prior work. * However, in Section 3.2, $H=[h_{1},h_{2},...,h_{|V|}]$ is defined as the *output* embeddings from the paper's own GNN encoder. * This reuse of $h_i$ to mean both \"input features\" and \"output embeddings\" in different sections is confusing. \n\n3. Undefined Critical Notation ($S_F^{(t)}$): The paper uses the critical notation $S_F^{(t)}$ extensively in its core method description and key equations without providing a clear, formal definition. The reader must infer that $S_F^{(t)}$ represents the set of already scheduled nodes, primarily from a label in Figure 3 (\"Mask of scheduled task nodes mask $S_{F}^{(t)}$\"), which is an oversight for such a central variable. \n\n4. Clarity of Reward Function Derivation: The paper's explanation of its training strategy in Section 3.4 could be clearer. It presents a \"return-like\" signal $R_{(t)}$ in Equation 9 and then presents the final \"advantage\" function $A_{(t)}$ in Equation 10 without explicitly detailing the derivation. The paper does not formally define the baseline equivalent of $R_{(t)}$ (e.g., $R_{(t)}^{baseline} = C(Solution(G)) - C^{heur}(v_{\\pi(t)})$). By not showing how the standard advantage formula $A_{(t)} = R_{(t)} - R_{(t)}^{baseline}$ simplifies Equation 9 into Equation 10, the text requires the reader to infer these intermediate steps. \n\n5. Deferred Citations for Core Methodology: The paper consistently defers critical citations for its core components to the appendices, harming clarity and traceability. * In Section 3.2, a \"bi-directional GNN\" is introduced with the generic equation $H=GNN(G)$ without an in-line citation. The specific model (Graphormer) is only identified in Appendix E.2. * Similarly, in Section 3.4, a \"genetic algorithm\" is named as a key part of the training strategy, but its citation is deferred to Appendix G. Providing these references in-line, rather than only in the appendix, is standard practice and would significantly improve the logical flow. \n\n6. Missing Evaluation Sampling Details: The paper's scheduling algorithm is stochastic, relying on sampling to select nodes at each step. While the results tables report aggregated metrics like \"average makespan\" and \"speedup\", the authors never specify the number of sampling runs performed per DAG instance to compute these averages. This omission of a key experimental parameter means the statistical stability of the results is unknown, and consequently, the reported \"speedup\" values cannot be meaningfully interpreted."}, "questions": {"value": "1. Please see the \"Weaknesses\" section for a detailed list of concerns regarding the paper's presentation, reproducibility, and theoretical clarity.\n\n2. The recommendation in Section 3.2 to normalize attention scores across queries (before Eq. 7) is sensible but only qualitatively discussed. Could the authors clarify whether this normalization was actually implemented and whether ablations were conducted to assess its impact?\n\n3. Have the authors tested how performance degrades when the DAG size increases substantially (e.g., > 1000 nodes)? Given the pairwise attention matrix scales quadratically, it would be helpful to quantify when this becomes computationally limiting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OG2mNftfmR", "forum": "UbWy2QVmke", "replyto": "UbWy2QVmke", "signatures": ["ICLR.cc/2026/Conference/Submission25338/Reviewer_ppbH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25338/Reviewer_ppbH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829168793, "cdate": 1761829168793, "tmdate": 1762943407211, "mdate": 1762943407211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the *one-shot DAG scheduling* problem, where a neural network generates a complete task execution order in a single forward pass by assigning priority scores to all nodes. The proposed method, **GAA-PtrNet**, replaces the LSTM-based encoder in standard Pointer Networks with a GNN-based encoder and adopts a learning framework for priority-based DAG scheduling. The authors claim two contributions: (1) the novel network architecture enabling one-shot PtrNet-based scheduling, and (2) a training strategy using policy gradient RL with dense rewards and demonstration learning. \n\nHowever, these contributions are incremental. The GNN encoder has already been explored in prior work, such as [1] Jeon et al. (ICLR 2023) and [2] Qi et al. (NeurIPS 2025). Furthermore, the training framework closely follows [2]. The experimental improvements reported in Tables 5–6 are marginal, and the overall setup lacks comprehensiveness.\n\n----\n[1] Jeon et al., *Neural DAG Scheduling via One-Shot Priority Sampling*, ICLR 2023.\n\n[2] Qi et al., *Reinforcement learning for one-shot DAG scheduling with comparability identification and dense reward,* NeurIPS2025."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper applies the Pointer Network framework to DAG scheduling with a graph-aware encoder, providing a unified neural approach to one-shot scheduling.\n* The methodology is clearly presented.\n* The use of GNNs enables the model to capture structural task dependencies more effectively than sequence-based LSTMs."}, "weaknesses": {"value": "* **Limited novelty**: The core innovation of using GNNs for one-shot DAG scheduling has been previously explored in recent works, particularly [1] Jeon et al. (ICLR 2023) and [2] Qi et al. (NeurIPS 2025). \n* **Similar training**: Several technical sections (e.g., Section 3.4) appear conceptually similar to [2] but are not properly cited, raising concerns about originality.\n* **Overclaimed contributions**: The claim of being “the first PtrNet-based one-shot DAG scheduler” is overstated and contradicts the authors’ own acknowledgement of prior PtrNet-based DAG scheduling studies. One-shot scheduling with graph attention-enhanced has been previously demonstrated in [1] and [2].\n* **Insufficient experimental validation**: \n  - Performance improvements on larger-scale problems (e.g., LIGO-400 in Table 5) are minimal (<1%), suggesting limited scalability advantage.\n  - The comparison with [2] is notably absent from the results tables, despite using similar benchmarks and baselines. Independent comparison shows that GAA-PtrNet does not consistently outperform [2] across all metrics (e.g., TPC-H 50, 100; SIPHT-200, 300).\n  - Limited benchmark diversity: Evaluation is missing on several important Pegasus workflows (Montage, CyberShake, Inspiral) and larger-scale instances (e.g., SIPHT-1000).\n  - Discrepancies in reported results (e.g., different gap calculations for the same SIPHT-400 makespan values between this paper and [2]) raise concerns about experimental rigor.\n* **Inappropriate baseline selection**: The use of POMO-DAG, originally designed for routing problems, rather than comparing against more recent and relevant learning-based scheduling solvers from top-tier conferences/journals, limits the persuasiveness of the performance claims."}, "questions": {"value": "1. Section 3.4 appears highly similar to Section 4.3 of Qi et al. (NeurIPS 2025), yet [2] is not cited. Could the authors explain the methodological differences and justify how this section constitutes a contribution?\n1. Why are the baselines chosen POMO (which was designed for routing problems)? Please compare against recent neural DAG or workflow schedulers in top-tier conferences or journals, reporting both performance and inference time.\n1. The reported improvements in LIGO (Table 5) are below 2%, and gains diminish as node size increases. Could the authors analyze why the method scales poorly and whether the network saturates for larger DAGs?\n1. Could the method be evaluated on additional Pegasus datasets (e.g., Montage, CyberShake, Inspiral) and job shop scheduling benchmarks to test robustness and generality?\n1. Since the datasets and settings are nearly identical to [2], please include [2]’s reported results in your tables for a direct comparison.  Also, why were the results on SIPHT-1000 omitted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JgQJuCaDdJ", "forum": "UbWy2QVmke", "replyto": "UbWy2QVmke", "signatures": ["ICLR.cc/2026/Conference/Submission25338/Reviewer_zot3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25338/Reviewer_zot3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987231334, "cdate": 1761987231334, "tmdate": 1762943407018, "mdate": 1762943407018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses limitations in Pointer Network (PtrNet)-based methods for Directed Acyclic Graph (DAG) workflow scheduling, which suffer from an inability to exploit global DAG structure and high computational complexity due to their sequential decoding. The key technical contribution is GAA-PtrNet, a novel model that replaces the sequential decoder with a graph attention aggregation (GAA) mechanism to compute node priorities in a single, one-shot forward pass. This innovation omits the explicit decoder structure, enabling the model to capture global topological information and drastically reduce complexity. For training, a policy gradient RL strategy with dense rewards and demonstration learning is designed. Experimentally, the proposed method not only produces superior scheduling solutions but also runs approximately 10 times faster than prior PtrNet-based approaches and outperforms other learning-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-organised and presented in a way that makes it quite easy for the audience (even those with limited knowledge of workflow scheduling) to follow, with a clear blueprint of the proposed method and key components, along with their functionality, in mind.\n\n2. Detailed analysis of the computational complexity is provided."}, "weaknesses": {"value": "1. The idea of using the graph neural network and reinforcement learning to solve scheduling problems has been widely investigated, making the contribution of this paper very limited.\n\n2. The paper will have limited inspiration for the learning-for-scheduling domain."}, "questions": {"value": "1. Will the method be capable of handling dynamic workflow scheduling? Or other scheduling problem variants? Like the one proposed in the paper: https://openreview.net/pdf?id=4PlbIfmX9o"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1jmeWRrkzz", "forum": "UbWy2QVmke", "replyto": "UbWy2QVmke", "signatures": ["ICLR.cc/2026/Conference/Submission25338/Reviewer_c4UY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25338/Reviewer_c4UY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088736087, "cdate": 1762088736087, "tmdate": 1762943406575, "mdate": 1762943406575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}