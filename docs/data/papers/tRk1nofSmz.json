{"id": "tRk1nofSmz", "number": 17045, "cdate": 1758271529553, "mdate": 1759897202317, "content": {"title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs", "abstract": "While reasoning models trained with reinforcement learning (RL) excel in reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving—areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic code-augmented long-form reasoning data for cold-start training. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in performance and efficiency.  Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals generalization to broader tool-use scenarios and emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems.", "tldr": "", "keywords": ["Large Language Models", "Agent"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68c6a73554aa3eccd415cb7927aaf9adf1a48aee.pdf", "supplementary_material": "/attachment/36f001d38b6035dcb31a15c7b7dd328fc22ea986.zip"}, "replies": [{"content": {"summary": {"value": "The paper explores RL-based strategies for tool use in LLMs. I'll admit that this is quite outside of my expertise so my assessment is mostly an educated guess. Tool use is obviously well-explored though the specific implementation of incorporating RL-based reward signals into tool use pipelines is the main contribution here. There is concurrent work (which the authors cite) though the authors claim to \"scale this up\" and that the previous approach doesn't work. This comparison to perhaps the most relevant related work is a bit vague, making it harder to assess the technical contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-- Results shown are fairly promising\n\n-- Contribution seems to have some novel elements (even if the related work section is quite limited, and comparison against concurrent work is confusing)"}, "weaknesses": {"value": "-- The evaluation is mostly \"internal\", i.e., comparing the model to different versions of itself, etc. There's some evaluation against some standard baselines, though it's hard for me to assess these, i.e., do they just show that the use of tools outperforms general purpose models (which seems known?). It's hard to know where the strong and really comparable baselines are.\n\n-- Experiments overall a bit thin by ICLR standards.\n\n-- Very thin related work section. The most relevant related papers seem to be swept under the rug a bit and described only vaguely.\n\n-- Parts of the evaluation seem cherry-picked or rely on anecdotal examples.\n\n-- Method itself is very simple, not exactly a criticism itself, but as a reader who doesn't know the topic it's hard to follow exactly what the technical contribution is."}, "questions": {"value": "Can you clarify the main differences compared to the concurrent work you cite? See additional points among weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AFcSeFKnit", "forum": "tRk1nofSmz", "replyto": "tRk1nofSmz", "signatures": ["ICLR.cc/2026/Conference/Submission17045/Reviewer_yanT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17045/Reviewer_yanT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760566565909, "cdate": 1760566565909, "tmdate": 1762927058982, "mdate": 1762927058982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for integrating executable tools, specifically a code interpreter, into the reinforcement learning (RL) reasoning loop. The authors introduce a mixed trajectory approach, where textual inputs are converted into executable code that interacts with an interpreter. The key steps in the process are as follows:\n1. The authors first construct training data by combining text, code, and code execution results, using a template-based approach. These trajectories are used for supervised fine-tuning (SFT), enabling the model to start with a reasonable understanding of both generating code and its subsequent execution. This cold start phase is crucial for overcoming the initial knowledge gap that would otherwise hinder RL fine-tuning.\n2. After the SFT phase, the model is further refined using Proximal Policy Optimization (PPO) with a sparse binary reward signal, based on whether the final generated code is correct. This feedback loop integrates code execution into the generation process, where the model learns when and how to invoke the interpreter (i.e., when to call external tools). The execution step is embedded within the model's reasoning process, allowing it to improve its performance over time by adjusting its tool usage.\n\nThe authors evaluate their method on several benchmarks, including AIME2024/2025, GSM8K, MATH500, and GPQA, where the method demonstrates significant improvements compared to existing models, including those that do not use RL. Notably, the proposed method, with a Qwen2.5-32B backbone, achieves AIME2024=67.0% and AIME2025=49.3% in just 400 steps of training, outperforming larger models. Further improvements are seen when using a distilled model variant, DeepSeek-R1-Distill-Qwen-32B, which reaches 72.5%/54.3%"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It provides a practical, reproducible pipeline that many groups could adopt.\n2. Competitive results on challenging benchmarks\n3. The paper spells out the execution protocol, loss masking, async sandboxing, and caching—practical details that substantially reduce adoption friction."}, "weaknesses": {"value": "1. It has limited algorithmic novelty. The optimization relies on standard PPO.\n2. Impact of RL on reasoning upper-bound (pass@k) is missing. I strongly encourage the authors to provide pass@k (k=32,64,128,256,1024) results in the rebuttal phase."}, "questions": {"value": "1. Please report pass@k\n2. It is well known that RL results are often sensitive to seeds and minor config changes. Please provide >= 5 independent runs per key setting and report mean ± std\n3. The reported accuracy on AIME-2025 is noticeably lower than on AIME-2024. Could you clarify the causes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z4P1lh4Kjq", "forum": "tRk1nofSmz", "replyto": "tRk1nofSmz", "signatures": ["ICLR.cc/2026/Conference/Submission17045/Reviewer_Bh1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17045/Reviewer_Bh1w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630068946, "cdate": 1761630068946, "tmdate": 1762927058559, "mdate": 1762927058559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReTool, a reinforcement learning (RL) framework designed to teach large language models (LLMs) how to strategically use a code interpreter (CI) to improve their reasoning capabilities. The method involves a two-stage process: a \"cold-start\" supervised fine-tuning on curated code-augmented reasoning data, followed by RL training with PPO to optimize tool-use strategies based on task outcome rewards. The authors demonstrate that ReTool significantly improves performance on challenging mathematical reasoning benchmarks like AIME, outperforming strong baselines and showing impressive training efficiency, while also fostering emergent behaviors like code self-correction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed two-stage approach, combining supervised learning for foundational skills with reinforcement learning for strategic optimization, is logical, well-motivated, and shown to be highly effective.\n2. ReTool achieves state-of-the-art performance on the challenging AIME benchmarks, substantially outperforming both its own 32B backbone and other strong, often larger, models. The reported efficiency (e.g., achieving high scores with only 400 training steps) is particularly impressive and highlights the effectiveness of the tool-integrated RL paradigm.\n3. The \"cognitive analysis\" in Section 3.6, which tracks metrics like response length, code ratio, code complexity, and invocation timing, offers valuable insights into *how* the model learns to use tools more effectively. The identification of emergent behaviors like code self-correction (the \"aha moment\") is a compelling finding that deepens our understanding of RL's impact on LLM reasoning processes.\n4. The motivation is well-established, the methodology is described in sufficient detail for understanding, and the figures (especially Figure 2 and Figure 4) are effective at illustrating the core concepts and findings. The ablation studies convincingly demonstrate the importance of both the CI integration and the RL training stage."}, "weaknesses": {"value": "1. The performance gain from the RL stage is substantial (e.g., from 40.9% to 67.0% on AIME2024 in the ablation). Based on your cognitive analysis, could you provide more intuition on what you believe is the most critical strategic capability the model learns during RL that SFT on curated data fails to instill? Is it primarily about *when* to invoke the tool, or does it also learn more complex policies like using the tool for iterative verification or hypothesis testing?"}, "questions": {"value": "1. Could you provide more details about the cold-start dataset, $D_{CI}$? Specifically, what was its final size after the two-stage verification protocol, and what were the statistics of tool usage (e.g., average number of tool calls per sample)?\n2. The binary reward function ($+1/-1$) is remarkably simple yet effective. Did you experiment with more shaped reward functions, such as providing intermediate rewards for successful code execution or penalties for syntax errors? If so, how did they compare? If not, could you elaborate on your hypothesis for why the simple outcome-based reward is sufficient for learning complex behaviors like self-correction?\n3. In the PPO training details (Section 2.3.2), you mention setting the KL coefficient to 0.0. This effectively removes the trust region constraint that differentiates PPO from vanilla policy gradient methods. Could you explain the rationale for this choice and whether you observed any training instability as a result?\n4.  I am curious about the learning dynamics from an exploration-exploitation perspective. Could you plot and discuss the evolution of the policy's entropy during the RL training phase? An analysis of this trend would be insightful; for example, does the entropy decrease monotonically as the model becomes more confident in its tool-use strategy, or are there distinct phases where it might change as the model discovers new, more complex patterns?\n5.  The cold-start SFT phase is used to ensure the model adheres to the specified tool-use protocol (e.g., using `<code>` tags). With the advent of more powerful base models that exhibit strong in-context learning abilities (e.g., Qwen3-8B), do you think this SFT step is still essential? Could a sufficiently capable model learn the required format and syntax directly from the prompt during the RL phase, potentially simplifying the overall training pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ne4wgiVLnO", "forum": "tRk1nofSmz", "replyto": "tRk1nofSmz", "signatures": ["ICLR.cc/2026/Conference/Submission17045/Reviewer_MF9u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17045/Reviewer_MF9u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891732998, "cdate": 1761891732998, "tmdate": 1762927058074, "mdate": 1762927058074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to enable a Python code execution tool during reasoning in LLMs by RL training with the final solution correctness posing as the reward. Tool calls are delimited via special markers and the rollout function hands over to a Python interpreter when it encounters them and the execution result is inserted into the context again with special delimiters, after which it continues with generating tokens."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper gets decent results compared to the baselines and the ablations seem to show that the LLM can use tools in a way that raises its success chance\n- The statistics on how often responses use the Python code execution tool and how long or correct the code is, is interesting, albeit calling it cognitive analysis seems a bit much"}, "weaknesses": {"value": "The primary contribution is an RL framework that integrates reasoning and tool use, by adding delimiter tokens to the tool use part and having a parser check whether it should hand over to a Python interpreter before continuing with the token generation. The reward is the final correctness, which is the same is in the standard RL reasoning paradigm. This is a straightforward approach that has been considered by many people and to distinguish this paper from a \"flag-planting\" paper, I'd suggest adding more analysis to improve our understanding on how far this method can go and where the limitations lie. For example:\n- the experiments are currently limited to Qwen2.5 32B models and it's unclear from the paper results whether it will work for other models that might have seen less reasoning like data during pretraining or differ in other aspects\n- the analysis in 3.6 shows the behavior of the ReTool model but doesn't show how general that behavior is, by comparing to how the reasoning RL model without tool use behaves or whether the behavior is consistent across different models"}, "questions": {"value": "Why use PPO over GRPO? How was the value model parameterized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "14NILeDVl6", "forum": "tRk1nofSmz", "replyto": "tRk1nofSmz", "signatures": ["ICLR.cc/2026/Conference/Submission17045/Reviewer_xeUG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17045/Reviewer_xeUG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762742762375, "cdate": 1762742762375, "tmdate": 1762927057796, "mdate": 1762927057796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}