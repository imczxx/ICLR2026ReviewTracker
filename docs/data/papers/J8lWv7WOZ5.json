{"id": "J8lWv7WOZ5", "number": 24975, "cdate": 1758362689742, "mdate": 1763121493247, "content": {"title": "Dyna-ViT: Parameter-Free Dynamic Token Pruning for Efficient Vision Transformers", "abstract": "Vision Transformers (ViTs) achieve state-of-the-art results, yet their quadratic self-attention is inefficient, largely due to redundant processing of low-information background patches. We introduce Dyna-ViT, a simple, parameter-free framework for dynamic token pruning that ranks patches with an unsupervised saliency proxy and retains only the top-K before the encoder. The backbone remains an unmodified ViT; no extra modules or learnable parameters are added. Across three benchmarks, Dyna-ViT preserves accuracy while reducing compute. On PASCAL VOC, keeping 70% of patches is 25% faster per epoch and improves validation accuracy (97.1%) over the full-token baseline (96.8%). On CIFAR-100, Dyna-ViT attains 91.3% test accuracy versus 92.0% for the baseline with a 28% speed-up. On Tiny-ImageNet, it reaches 81.4% validation accuracy with 20–25% faster training. A simple analytic FLOPs model that scales with sequence length closely matches external estimates (e.g., K=60%, S=119: 10.48 vs. 10.23 GFLOPs), aligning with measured throughput gains. Ablations over K and alternative scoring functions (Sobel, Entropy) confirm robustness, and LIME visualizations show retained tokens align with semantically relevant regions. Under matched token budgets and backbones, Dyna-ViT is competitive with, and sometimes exceeds, learned sparsification (DynamicViT) and in-encoder token merging (ToMe), while introducing no additional parameters. These results indicate that parameter-free patch selection can substantially improve ViT efficiency, often acting as a beneficial regularizer with minimal or positive impact on accuracy.", "tldr": "Dyna-ViT prunes tokens before the encoder using a parameter-free saliency score (top-K patches), keeping a standard ViT backbone while delivering ~20–28% faster training with matched or better accuracy on VOC, CIFAR-100, and Tiny-ImageNet.", "keywords": ["Vision Transformers (ViT)", "dynamic token pruning", "parameter-free saliency", "sparse token selection", "efficient attention", "analytic FLOPs", "PASCAL VOC", "CIFAR-100", "Tiny-ImageNet", "LIME explainability", "DynamicViT", "ToMe"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/5c464d97cbb834bdc44d406dd0bb0393277ef424.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a parameter-free, pre-encoder token pruning method for VIsion transformers. Instead of modifying the ViT architecture or inserting learnable modules, the authors compute an unsupervised saliency score (e.g., L2 norm, Sobel edge magnitude, or entropy) for each patch, retain only the top-K% most salient ones, and feed this sparse token sequence into an unchanged ViT backbone."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The method is easy to understand, implement, and reproduce. The two-stage pipeline—saliency scoring + top-K selection—is transparent and avoids complex training or architectural changes. This method is also compitiable for most vision architectures with its pre-encoder prunning pipeline."}, "weaknesses": {"value": "The idea (pruning low-saliency patches) has strong precedents in prior work such as EviT. The novelty and contributions are limited.\n\nThe keep ratio K is static per dataset, not adaptive per image. In heterogeneous datasets, this may over-prune simple images or under-prune complex ones.\n\nExperiments are restricted to ViT-B/16 on relatively small datasets. It remains unclear how Dyna-ViT scales to larger models (ViT-L/H), higher resolutions (384+, 512+), or dense prediction tasks (detection, segmentation)."}, "questions": {"value": "The paper treats L2 norm, Sobel, and entropy as interchangeable heuristics—but doesn’t explain why these metrics correlate with semantic informativeness in ViTs. \n\nWith such a simple pipeline, can this method be used to more tasks and more senarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SwCrJGMvIU", "forum": "J8lWv7WOZ5", "replyto": "J8lWv7WOZ5", "signatures": ["ICLR.cc/2026/Conference/Submission24975/Reviewer_hkxa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24975/Reviewer_hkxa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448115421, "cdate": 1761448115421, "tmdate": 1762943270584, "mdate": 1762943270584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "I want to extend this work and cover all things according to suggestions and submit again."}}, "id": "5hGfUHGLOT", "forum": "J8lWv7WOZ5", "replyto": "J8lWv7WOZ5", "signatures": ["ICLR.cc/2026/Conference/Submission24975/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24975/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763121391891, "cdate": 1763121391891, "tmdate": 1763121391891, "mdate": 1763121391891, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dyna-ViT, a parameter-free token pruning method for Vision Transformers. It selects the top-K% image patches using simple saliency measures (e.g., L2 norm or Sobel) and feeds them into an unmodified ViT to reduce computation. The authors report up to 25% faster training on small datasets, but comparisons with prior methods are all missing, and the writing quality needs significant improvement."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe idea is very simple and easy to implement.\n\n2.\tThe parameter-free setup can serve as a trivial baseline for related work."}, "weaknesses": {"value": "1. __Incomplete experiments:__ The baselines are only two methods and all results of them are missing.\n\n2. __Poor writing quality:__ Multiple placeholders and typos indicate an unfinished draft.\n\n3. __Limited scope:__ it only experiments with small datasets without evaluations on ImageNet or real-world tasks.\n\n4. __Weak novelty:__ the idea is a minor variation of existing dynamic ViT pruning techniques like EViT."}, "questions": {"value": "The authors should complete the unfinished experiments, fill in all missing results, and polish the writing before considering submission."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "a8WbatuAsU", "forum": "J8lWv7WOZ5", "replyto": "J8lWv7WOZ5", "signatures": ["ICLR.cc/2026/Conference/Submission24975/Reviewer_Kygw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24975/Reviewer_Kygw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543030093, "cdate": 1761543030093, "tmdate": 1762943270314, "mdate": 1762943270314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a dynamic token pruning technique that ranks patches with an unsupervised saliency proxy and use that to prune tokens before the encoder. This is an incomplete work with many placeholders in the paper. The authors should have withdrawn the paper after the deadline and instead of wasting reviewers' time."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None"}, "weaknesses": {"value": "Incomplete submission."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "V7UqrKKn9R", "forum": "J8lWv7WOZ5", "replyto": "J8lWv7WOZ5", "signatures": ["ICLR.cc/2026/Conference/Submission24975/Reviewer_YtMy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24975/Reviewer_YtMy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742718341, "cdate": 1761742718341, "tmdate": 1762943268747, "mdate": 1762943268747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dyna-ViT, a two-stage, parameter-free front-end for Vision Transformers (ViTs) that prunes tokens before the encoder using unsupervised saliency. It leverages parameter-free saliency measurements to select tokens prior to the Transformer backbone and aggregate unselected tokens into the remaining tokens."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Very structured draft"}, "weaknesses": {"value": "* The most significant problem is that this paper is NOT completed. The experimental section and related tables read like a semi-finished article with lots of \"fill\" or \"fill if measured\". And many sentences are just fragments of words.\n\n* In addition, this paper lacks experiments on standard-size dataset (such as ImageNet) and downstream tasks (such detection on MSCOCO), thereby lowering its significance.\n\n* The idea of early exit and multi-branch processing are not novel. And the motivation of combining these two methods are weak.\n\n* There is an obvious citation mistake in Table 1, where ToMe should be Bolya's work published in ICLR 2023."}, "questions": {"value": "I suggest the authors to complete this manuscript first and then submit."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KP620AsokA", "forum": "J8lWv7WOZ5", "replyto": "J8lWv7WOZ5", "signatures": ["ICLR.cc/2026/Conference/Submission24975/Reviewer_5iuY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24975/Reviewer_5iuY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802970246, "cdate": 1761802970246, "tmdate": 1762943268303, "mdate": 1762943268303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}