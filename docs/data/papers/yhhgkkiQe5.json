{"id": "yhhgkkiQe5", "number": 5859, "cdate": 1757941221907, "mdate": 1763711754802, "content": {"title": "TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification", "abstract": "The enhanced reasoning capabilities of Large Language Models (LLMs) have led to longer response sequences, yet the inference efficiency is fundamentally limited by their serial, autoregressive generation. Speculative decoding (SD) offers a powerful solution, providing significant speed-ups through its lightweight drafting and parallel verification mechanism. While existing work has made considerable progress in improving the draft model's generation efficiency and alignment, this paper boosts SD from a new angle: the verification cost. We propose TriSpec, a novel ternary SD framework with proxy verification. At its core, TriSpec introduces a lightweight proxy model that handles the initial verification. This proxy significantly reduces computational cost by approving easily verifiable draft sequences and only engages the full target model when encountering uncertain tokens, thus striking an optimal balance between efficiency and quality. TriSpec can be integrated with state-of-the-art SD methods like EAGLE-3 to further reduce verification costs, achieving greater acceleration. Extensive experiments on the Qwen3 and DeepSeek-R1-Distill-Qwen/LLaMA families show that TriSpec achieves up to 30\\% speedup over standard SD, with up to 50\\% fewer target model invocations while maintaining comparable accuracy.", "tldr": "We propose a ternary speculative decoding method that employs a proxy verifier to assist the target model in verification stage, thereby further accelerating standard speculative decoding.", "keywords": ["LLM Acceleration", "Speculative Decoding", "Proxy verifier"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77ac1fd40b01309b672fb356d859c44dc00da8b1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes TriSpec, a ternary speculative decoding (SD) framework that adds a lightweight proxy verifier between the drafter and the target model to reduce verification cost. A margin-based routing rule decides when the proxy's verification is \"trusted\". Several experiments on Qwen3 and DeepSeek-R1-Distill-Qwen demonstrate the effectiveness of the method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is overall well-written.\n\n- The idea of applying a proxy verification model offers a new angle on SD efficiency to reduce the verification time\n\n- Several experiments demonstrate the effectiveness of the TriSpec."}, "weaknesses": {"value": "- Results are limited to two families (Qwen3, DSQ). It’s unclear how well the “same-family small proxy” assumption holds for other backbones, including Llama 2, Llama 3, and the Vicuna series.\n- Accuracy is measured via pass@1 on math/code; there’s little analysis of generation fidelity for open-ended text or long-form reasoning where subtle proxy deviations could matter.  \n- TRISPEC itself cannot strictly speed up the LLM reasoning process losslessly. Some minor differences are unacceptable in some fields, such as medicine and law.\n- Trispec adds a proxy model, which also brings additional deployment and memory overhead。"}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S1fKGXg9mg", "forum": "yhhgkkiQe5", "replyto": "yhhgkkiQe5", "signatures": ["ICLR.cc/2026/Conference/Submission5859/Reviewer_EJNW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5859/Reviewer_EJNW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760928328152, "cdate": 1760928328152, "tmdate": 1762918308065, "mdate": 1762918308065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **TriSpec**, a ternary speculative decoding (SD) pipeline that inserts a **lightweight proxy verifier** between the usual drafter and the target LLM. The proxy is a smaller, same‑family model (e.g., Qwen3‑1.7B for Qwen3‑32B) that pre‑verifies drafted tokens and locally corrects the first rejection; the expensive target model is called only when the proxy’s **margin test** (top‑1 minus top‑2 probability) marks positions as untrusted. The authors extend EAGLE‑style single‑layer drafters with a small **adapter** so the drafter can be seeded by proxy or target features. Algorithm 1 and Fig. 1–3 describe the flow; two regimes are covered: proxy completes the round without target, or the target verifies the untrusted suffix with **token pruning** of proxy‑trusted tokens. Experiments on Qwen3 and DeepSeek‑R1‑Distill‑Qwen show up to ~30% higher speedup than standard SD pipelines (HASS/EAGLE‑3) at ≤1% average accuracy loss, with >50% fewer target invocations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies verification time as a first-order bottleneck in modern SD stacks and operationalizes a clear, reproducible fix: insert a same-family proxy and gate escalation with a top-1 vs. top-2 margin. The algorithm is simple to implement atop EAGLE-family drafters.\n2. The presentation and the figures are intuitive and easy to understand.\n3. The experiments show large reductions in target-invocation ratio and lower per-round verification time, while keeping acceptance length stable."}, "weaknesses": {"value": "1. Novelty is limited versus recent verification-side work. While the motivation to reduce target calls with a cheaper verifier is straightforward, the idea of introducing a mid-level LLM into the draft model and the target model is well explored. \n2. TriSpec achieves better speedup ratio at the cost of losing the theoretical lossless property of speculative decoding, which is especially important in real-world applications. The method can accept proxy-approved tokens that differ from the target. While **Appendix B** argues these are usually acceptable, there is no stress test on open-ended generation, multilingual prompts, or safety-sensitive settings where small token changes may carry large semantic shifts. Meanwhile, The paper should quantify quality shifts under temperature and across domains, not only average accuracy.\n3. Missing Related Works. Some related works [1, 2] already explored the idea of multi-level speculative decoding. The lack of these baselines weakens the novelty and evidence. \n4. Evaluation is narrow and controlled. The experiments are only conducted on 2 Qwen-32B series models. The effectiveness of TriSpec on large-scale LLMs (>=70B) and other LLM backbones (e.g. llama and GLM) remains unknown.\n\n[1] Bachmann, Gregor, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Schönfeld, Ali Thabet, and Jonas Kohler. \"Judge decoding: Faster speculative sampling requires going beyond model alignment.\" *arXiv preprint arXiv:2501.19309* (2025).\n\n[2] Narasimhan, Harikrishna, Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta, Aditya Krishna Menon, and Sanjiv Kumar. \"Faster cascades via speculative decoding.\" *arXiv preprint arXiv:2405.19261* (2024)."}, "questions": {"value": "1. Could you please specify the detailed training cost of the draft model?\n2. Could you please provide more experiments on some extremely difficult tasks? Will TriSpec significantly decrease the model performance? If the user's query is out of the domain of the training data of the proxy model, may the proxy model give low-quality judge?\n3. Does the proxy and the target model run on the same GPU? Whether the KV cache is shared?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QHatKjPQ4a", "forum": "yhhgkkiQe5", "replyto": "yhhgkkiQe5", "signatures": ["ICLR.cc/2026/Conference/Submission5859/Reviewer_iPrA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5859/Reviewer_iPrA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545577607, "cdate": 1761545577607, "tmdate": 1762918307838, "mdate": 1762918307838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TriSpec is a speculative decoding framework that uses a small model of the same family as an approximate proxy of the target model for use in verification. Unlike classic speculative decoding, not every token is verified by the target model. Drafted tokens are first verified by the proxy model. Only when the proxy is unable to make a confident verification (indicated by low margin between top-1 and top-2 token probability) is the target model used for verification.\n\nOn math and code reasoning benchmarks, the authors show that TriSpec achieves larger speedups than baseline speculative decoding methods while seeing negligible performance drop despite the target model never validating the full output."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- TriSpec is a simple and effective idea, using small models as verifiers for a fast single-layer drafter, similar to model cascades but for verification.\n- Across all domains presented in the paper, TriSpec demonstrates higher speedups compared to baselines while showing negligible performance loss compared to the target model. These results show that with the right proxy, the loss of the losslessness guarantee from classical speculative decoding will not adversely affect output quality."}, "weaknesses": {"value": "- The paper only examines two model families, both based on Qwen: Qwen3 and DeepSeek-R1-Distilled-Qwen. Experiments on model families from other providers would strengthen the paper. In the paper’s current state, it is unclear whether the effectiveness of smaller model variants as proxy verifiers is particular to Qwen as a model provider.\n- The paper only examines two settings: math and code reasoning. These settings may be much more structured than more general domains, better suiting proxy models. Evaluations on other domains like question answering (e.g., HotpotQA) or instruction following would make the paper stronger. It could also be interesting to see results in domains where there is a much larger performance gap between the proxy and target models.\n- The evaluation set sizes are small, only 100 questions per benchmark. This, along with the lack of error bars and confidence intervals in the paper, makes it difficult to fully contextualize the results."}, "questions": {"value": "- Did you investigate the impact of a mis-aligned proxy (e.g., from a different model family) on accuracy/latency?\n- How did you select the proxy model size? In particular, why not Qwen3 0.6B?\n- Have you experimented with more layers of proxies between the draft and target model, and if so, why did you decide to have only one proxy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XyGHAWfEBP", "forum": "yhhgkkiQe5", "replyto": "yhhgkkiQe5", "signatures": ["ICLR.cc/2026/Conference/Submission5859/Reviewer_N6ke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5859/Reviewer_N6ke"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771202567, "cdate": 1761771202567, "tmdate": 1762918307560, "mdate": 1762918307560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TriSpec, a novel speculative decoding (SD) framework that introduces a proxy verifier to reduce verification cost—an often-overlooked bottleneck in SD pipelines. Unlike previous work (e.g., Medusa, EAGLE, SpecDec++) that primarily optimized the drafting phase, TriSpec focuses on verification efficiency by employing a lightweight, same-family small model to pre-verify tokens before escalating uncertain ones to the full target model. A margin-based routing criterion determines when to trust the proxy versus when to defer to the target."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The writing is very clear and easy to follow. I particularly appreciate that the authors clearly illustrate the bottlenecks that current speculative decoding systems suffer from, as shown in Figure 2. The proposed approach—based on introducing a lightweight proxy verifier to reduce verification cost—is both reasonable and well motivated. In terms of experiments, the authors conduct comprehensive evaluations on five benchmarks across two metrics (accuracy and speedup), demonstrating consistent improvements and strong empirical support for the proposed framework."}, "weaknesses": {"value": "The hierarchical framework seems not entirely new, previous work such as triforce [1] also employs similar hierarchical framework. I understand there are some difference, but the authors should give some discussion between them.  In addition, I find the preliminary observation in Figure 2(b) particularly interesting. However, I wonder whether this phenomenon persists under varied temperature settings. Intuitively, when the temperature is higher, the output distribution becomes smoother, which might weaken the reliability of the margin-based routing criterion. In such cases, the proposed approach may not perform as well. I hope authors can clarify this. Moreover, the main experiments are conducted only under a fixed temperature = 0 setting. I recommend the authors evaluate their approach under more diverse temperature conditions to better assess its robustness.\n\n[1] Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7KAUIjLZcg", "forum": "yhhgkkiQe5", "replyto": "yhhgkkiQe5", "signatures": ["ICLR.cc/2026/Conference/Submission5859/Reviewer_KuyU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5859/Reviewer_KuyU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762209158167, "cdate": 1762209158167, "tmdate": 1762918307334, "mdate": 1762918307334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Our Responses & Paper Revisions"}, "comment": {"value": "We sincerely thank all the reviewers for their valuable and constructive reviews. We appreciate that the reviewers acknowledged TriSpec as a novel (Reviewer KuyU), simple and effective framework (Reviewers N6ke, iPrA), offering a new angle on speculative decoding (Reviewer EJNW), well-written and easy to follow (Reviewers KuyU, N6ke, EJNW), and recognized its strong motivation and effectiveness (Reviewers KuyU, N6ke, iPrA, EJNW).\n\n1. **Experiments under different temperature settings:** In response to requests for performance evaluation across varied temperatures, we added experiments with temperature = 1 in Table 2. The results further confirm that TriSpec maintains both efficiency and accuracy under different temperature configurations.\n2. **Cross–model family evaluation:** To examine architecture generality, we extended Tables 1–3 by adding results for the DeepSeek-R1-LLaMA family. Results show that TriSpec achieves the best accuracy–speed trade‑off under LLaMA architecture, demonstrating its architecture‑agnostic effectiveness.\n3. **General and difficult task scenarios:** To address questions on broader and challenging settings, we added experiments on SpecBench, HotpotQA, and PolyMATH in Appendix B.1. Table 6 reports overall results, and Figure 5 shows SpecBench per‑category performance, demonstrating TriSpec is also effective across general and challenging task scenarios.\n4. **Additional baseline methods:** We expanded the Related Work section to include reviewer‑mentioned approaches. In Table 1, we added two new deferral rules for SpecCascade as baselines, further demonstrating TriSpec’s superior performance.\n5. **Full‑test‑set evaluation:** Responding to concerns about small evaluation sets and missing statistical measures, we provided explanations in Appendix B.2. Table 7 and Figure 6 now include full‑test‑set results and error bars from repeated‑subset evaluations, confirming the robustness of our findings.\n6. **Proxy selection analysis:** To address questions about proxy choice, Appendix C.4 presents a detailed analysis. Figure 10 compares match rates between different‑size proxy models and the target model, validating the rationality of our chosen proxy configuration.\n\nThe major revised contents in the manuscript are highlighted in blue. Point-by-point responses to specific comments are given in the reviewer-specific sections below."}}, "id": "sXmuoLnAhB", "forum": "yhhgkkiQe5", "replyto": "yhhgkkiQe5", "signatures": ["ICLR.cc/2026/Conference/Submission5859/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5859/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission5859/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724362139, "cdate": 1763724362139, "tmdate": 1763724607254, "mdate": 1763724607254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}