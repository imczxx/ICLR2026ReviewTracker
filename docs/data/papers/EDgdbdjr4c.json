{"id": "EDgdbdjr4c", "number": 7259, "cdate": 1758013337596, "mdate": 1759897863313, "content": {"title": "ICE-Coder: Integrating White-box and Black-box Testing in Execution-guided Multi-agent Code Generation", "abstract": "LLM-based coding agents are programs that utilise LLMs to automate code generation tasks. Typically, they incorporate code execution capabilities which, together with automated test generation and/or debugging methods, enhance the reliability of the generated code. However, the effectiveness of these approaches remains limited in complex problems (such as competitive programming problems) where bugs surface only in convoluted edge cases. This work builds upon multi-agent code generation techniques which emulate software engineering environments. In particular, to address obscure edge cases, we take inspiration from code coverage tools and code reviews to generate white-box tests, on top of existing black-box test generation approaches. Test case outputs are validated through a process of deliberation using the LLM. By increasing the quantity and quality of the test cases, we obtain more reliable generated code. We evaluated ICE-Coder on LiveCodeBench-Hard. Out of the 90 problems, it solves 72, compared to the baseline of 55.", "tldr": "We layer multi-agent LLM coding with white-box test generation (inspired by coverage-guided testing and code reviews), standard black-box tests, and LLM deliberation on outputs, raising LiveCodeBench‑Hard solves from 55/90 to 72/90.", "keywords": ["code generation", "test generation", "multi-agent planning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/89544eb522a6f63cdb081ad51bf19404d2ae6c93.pdf", "supplementary_material": "/attachment/d629aee3875ff434e8db854a7c6e0fe5bd4f6242.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides a multi-agent system to enhance capability of solving complex code generation tasks. It combines the benefits of both single engineer thinking and development team strategy to further help the system. Also, the authors try to integrate the coverage-based testing and code review process to strengthen testing stage. Lastly, they use a prompting-based method to validate the test case correctness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. this proposed framework is a more engineering-realistic testing loop than previous work.\n2. the workflow is clear and easy to understand.\n3. it can beat the SOTA with very impressive improvement. \n4. its coverage metric is good to measure the quality of tests. \n5. the results seem like nearly all combination of model providers and sizes are better than the original ones."}, "weaknesses": {"value": "1. like what the authors claimed, the paper only has results on single LiveCodeBench-Hard dataset. it largely limitate the confidence and reproducibility. Hope the authors can provide more results during the rebuttal process, which can make the submission better.\n\n2. No results or analysis for demonstrating the proposed prompt-based method is better than other methods in validating test case correctness.\n\n3. problem re-writing for baselines induces bias and may change the fairness also. \n\n4. need more analysis of how to avoid the negative improvement of small model like GPT 4.1 NANO. Authors can try to test more small models and investigate whether their results are similar.  \n\n5. as the most significant improvement comes from the initial test generation, it is important to analyze the token usage more deeply. maybe adding some examples can help the reader to understand."}, "questions": {"value": "please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pnIBiMnQBf", "forum": "EDgdbdjr4c", "replyto": "EDgdbdjr4c", "signatures": ["ICLR.cc/2026/Conference/Submission7259/Reviewer_96yT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7259/Reviewer_96yT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946938098, "cdate": 1761946938098, "tmdate": 1762919392717, "mdate": 1762919392717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ICE-CODER, a multi-agent framework for automatic code generation, specifically targeting complex (e.g., competitive programming) problems. The core idea is to improve code reliability by more closely simulating a human software engineering process. The system integrates existing black-box test generation with two key additions: (1) white-box, coverage-guided test generation to find edge cases, and (2) an LLM-based \"error diagnosis\" step to deliberate on and resolve conflicts between code outputs and test case outputs. The authors evaluate ICE-CODER on the LiveCodeBench-Hard dataset, demonstrating significant improvements over a baseline and claiming state-of-the-art performance by solving 72 out of 90 problems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a weakness in current code generation agents: their inability to handle subtle edge cases in complex problems. The motivation to move beyond simple black-box testing is strong and well-founded.\n2. The core idea of integrating white-box, coverage-guided testing is a logical and intuitive next step for improving test suite quality. Simulating a \"deliberation\" or \"diagnosis\" step to handle faulty tests is also a practical solution to a known problem (LLMs generating incorrect test expectations)."}, "weaknesses": {"value": "1. The primary weakness of this paper is its lack of a core, novel contribution. The work appears to be a complex engineering pipeline that skillfully integrates several existing techniques. The paper itself cites prior work on multi-agent code generation (e.g., AgentCoder), LLMs for coverage-guided testing (e.g., Pan et al., 2025; Pizzorno & Berger, 2025), and using execution traces for debugging (e.g., Zhong et al., 2024). The main contribution is this specific combination, which feels more like an incremental \"system-building\" paper than a fundamental research advance.\n2. The experimental support is not strong enough for the claims being made.\nThe entire evaluation rests on a single, 90-problem dataset (LiveCodeBench-Hard). While this is a relevant dataset, it's a very narrow benchmark. It is unknown if this complex, multi-stage pipeline generalizes to other coding benchmarks (e.g., HumanEval, MBPP) or if it might even regress on simpler problems due to its overhead and complexity.\n3. The writing is somewhat convoluted. The paper claims to combine \"simulating a software engineering environment\" (multi-agent) with \"simulating an individual developer's mind\" (single-agent), which makes the core metaphor of the framework confusing. It is difficult for the reader to disentangle which parts are re-implementations of prior art and what the single, clean takeaway contribution is."}, "questions": {"value": "1. Could you please provide precise details on the exact modifications made to the problem descriptions to allow AgentCoder, MapCoder, etc., to run? This seems like a critical confounder. How can we be sure that these modifications did not make the task easier for ICE-Coder or harder for the other methods? A fair comparison would require all methods to run on the identical, unmodified problem set.\n2. Why was the evaluation limited only to LiveCodeBench-Hard? The authors' hypothesis is that this method is good for complex problems, but this does not excuse the omission of standard benchmarks.\n3. Could the authors more clearly articulate what the primary conceptual contribution is, beyond the successful integration and engineering of these known techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IwyWgUv4Nh", "forum": "EDgdbdjr4c", "replyto": "EDgdbdjr4c", "signatures": ["ICLR.cc/2026/Conference/Submission7259/Reviewer_rdR2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7259/Reviewer_rdR2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974149358, "cdate": 1761974149358, "tmdate": 1762919392326, "mdate": 1762919392326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ICE-coder to improve the code synthesis capabilities of LLMs/LLM-agents. In particular, ICE-coder leverages whitebox test generation techniques to generate tests that can maximum coverage on the source problems. Furthermore, ICE-coder also assess the quality of the tests generated to further improve performance. The authors evaluate ICE-coder on a set of code context problems and found that the approach can improve performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- deals with an important problem of improving code synthesis capabilities of LLMs/LLM-agents\n- the idea of utilizing better whitebox feedback for better test generation approach is interesting"}, "weaknesses": {"value": "While I think the overall idea of adding more \"white-box\" testing methods into agentic-based coding is interesting, I think there are several major weakness to the paper:\n\n- Understanding of what makes coverage useful and important:\n\t- The paper pushes for a misguided understanding of what makes coverage useful in practice\n\t- I think we should first acknowledge that \"executing code and actually fully functionally testing it are not the same thing\"\n\t- The author writes \"When executing the code, we aim for complete statement coverage. This ensures that each statement behaves as expected\"\n\t\t- I would argue this is a gross simplification of what testing is.\n\t\t- Just because you have covered a line of code, it does not mean that line of code is correct in the context of that particular problem\n\t\t- The reason why companies utilize basic coverage information is to check that certain features (i.e., new implementations) are at least covered.\n\t\t- you will find that often times each line is covered multiple times with good tests written by the developer.\n\t- Furthermore, in software engineering literature and practice, there are many different ways we can measure coverage and coverage metrics\n\t\t- in the paper the authors rely on the most basic metric of line coverage and neglect arguable more useful and aligned coverage metric like path coverage (i.e., the number of execution paths in code which has been coverage)\n\t\t- this path coverage would be more aligned with the author's goal of getting more edge testcases (as they may manifest as rare paths)\n\t- In summary: I think the authors very surface-level of what makes coverage useful in practice in order to demonstrate their technique \n\t- See [1] for a great article on this topic\n\n- Limited benchmark evaluation and baseline improvement:\n\t- As pointed out by the authors themselves, the paper only evaluates on less than 100 problems in the LiveCodeBench-hard subset of the problems\n\t- It is unclear to me why the authors did not evaluate on other code-context benchmarks (or even the medium or easy subset of LiveCodeBench) just to at least show that performance is not degrading\n\t- Furthermore, from Table 2, ICE-Coder seems to only improve by less than 2 percentage points on 100 problems\n\nminor issues:\n- The high number of footnotes are a bit difficult to read, some of the footnotes also appear on the wrong page\n\n\n[1] Programs, tests, and oracles: the foundations of testing revisited. In Richard N. Taylor, Harald C. Gall, and Nenad Medvidovic, editors, International Conference on Software Engineering, pages 391–400. ACM, 2011."}, "questions": {"value": "1. Can the authors touch on how they use the idea of coverage in their approach? For example why do they only consider the most basic of coverage information (see weakness for more detail)\n2. What are the performance of ICE-coder on additional code-context benchmarks including different difficulties of livecodebench problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DIO8LG1sZ7", "forum": "EDgdbdjr4c", "replyto": "EDgdbdjr4c", "signatures": ["ICLR.cc/2026/Conference/Submission7259/Reviewer_mD7x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7259/Reviewer_mD7x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975283550, "cdate": 1761975283550, "tmdate": 1762919391809, "mdate": 1762919391809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ICE-Coder, an LLM-based multi-agent coding framework designed to improve the reliability of automatically generated code on complex programming tasks. Building on existing multi-agent code generation methods that emulate collaborative software engineering, ICE-Coder integrates insights from code coverage analysis and code review practices to generate white-box tests that complement traditional black-box testing. The approach is evaluated on the LiveCodeBench-Hard benchmark, which solves 72 out of 90 problems, outperforming the baseline of 55."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and easy to understand. \n\nThe proposed approach is demonstrated to have very good performance in code generation."}, "weaknesses": {"value": "Unclear novelty:\n\nThe paper proposes using code coverage features to simulate white-box testing; however, the use of coverage information to facilitate test generation has been extensively explored in prior work, as also acknowledged in the related work section. While the proposed pipeline consists of several steps, it remains unclear what the overall conceptual or methodological novelty is beyond existing approaches, even though the final results are promising.\n\nLimited evaluation:\n\nThe experimental evaluation relies solely on a single benchmark, LiveCodeBench-Hard. It is uncertain whether the proposed method generalises to other benchmarks or performs effectively on simpler or more diverse coding tasks. A broader evaluation would strengthen the empirical validity and demonstrate the robustness of the approach."}, "questions": {"value": "Can you further clarify the novelty of this paper?\n\nHow does ICE-Coder perform on other benchmarks? How does the difficulty of coding problems affect the effectiveness of the approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VTKOlV5AKf", "forum": "EDgdbdjr4c", "replyto": "EDgdbdjr4c", "signatures": ["ICLR.cc/2026/Conference/Submission7259/Reviewer_2bhi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7259/Reviewer_2bhi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030259788, "cdate": 1762030259788, "tmdate": 1762919390365, "mdate": 1762919390365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}