{"id": "J7xDwZSyI4", "number": 24031, "cdate": 1758351971334, "mdate": 1759896785210, "content": {"title": "FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction", "abstract": "As large language models (LLM) become increasingly powerful, the sequential nature of autoregressive generation creates a fundamental throughput bottleneck that limits the practical deployment. While Multi-Token Prediction (MTP) has demonstrated remarkable benefits for model training efficiency and performance, its inherent potential for inference acceleration remains largely unexplored. This paper introduces FastMTP, a simple yet effective method that improves multi-step draft quality by aligning MTP training with its inference pattern, significantly enhancing speculative decoding performance. Our approach fine-tunes a single MTP head with position-shared weights on self-distilled data, enabling it to capture dependencies among consecutive future tokens and maintain high acceptance rates across multiple recursive draft steps. By integrating language-aware dynamic vocabulary compression into the MTP head, we further reduce computational overhead in the drafting process. Experimental results across seven diverse benchmarks demonstrate that FastMTP achieves an average of 2.03× speedup compared to standard next token prediction with lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires only lightweight training and seamlessly integrates with existing inference frameworks, offering a practical and rapidly deployable solution for accelerating LLM inference.", "tldr": "", "keywords": ["multi-token prediction", "LLM inference acceleration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/507efc60370dcbeafc817e7def0cad78c25d8376.pdf", "supplementary_material": "/attachment/145e59a94447aac455f2ca734b38084bf7617a5f.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces FastMTP, a method that refines MTP for inference by training a single shared MTP head on self-distilled data, so it better captures dependencies among consecutive future tokens and boosts speculative decoding performance. To reduce computational overhead, FastMTP also applies language-aware dynamic vocabulary compression in the draft stage. Evaluated across seven benchmarks, it achieves ~2.0× average speedup over standard next-token prediction without loss in output quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written with good-quality diagrams like Figure 1 and methodology is well written. Speedup is achieved over vanilla MTP."}, "weaknesses": {"value": "1. My main concern with this paper is its novelty. The proposed method does not seem to be very different from Medusa (https://arxiv.org/abs/2401.10774) or Hydra (https://arxiv.org/abs/2402.05109). Could you probably explain the major differences?\n2. There is a lack of discussion of relevant work on parallel decoding. For example, BiTA (https://arxiv.org/abs/2401.12522) and PPD (https://arxiv.org/abs/2405.18628) use prompt tokens for MTP. What are the advantages of FastMTP as compared to these existing methods?\n3. The speedup ratio is not compared with SOTA speculative decoding methods like Eagle (https://arxiv.org/abs/2503.01840). What is the motivation of FastMTP if the speedup ratio seems far away from Eagle?"}, "questions": {"value": "Please see the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wm3TJyNnwH", "forum": "J7xDwZSyI4", "replyto": "J7xDwZSyI4", "signatures": ["ICLR.cc/2026/Conference/Submission24031/Reviewer_Xj93"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24031/Reviewer_Xj93"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760464418186, "cdate": 1760464418186, "tmdate": 1762942905606, "mdate": 1762942905606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FastMTP, a simple yet effective method that improves MTP performance by aligning MTP training with its inference. It fine-tunes the original MTP head of MiMo-7B-RL on 389.4K self-distilled samples.  Experimental results across seven diverse benchmarks demonstrate that FastMTP achieves an average of 2.03x speedup compared to standard next token prediction with lossless output quality, outperforming vanilla MTP head by 82%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The exploration to improve existing MTP performence in practice should be encouraged. \n2. The improvement is promising, which shows that FastMTP achieves an average of 2.03x speedup compared to standard next token prediction with lossless output quality, outperforming vanilla MTP by 82%.\n3. The manuscript is clearly written, with a well-structured narrative."}, "weaknesses": {"value": "1. **Lack of technical contributions**: The main contribution of this work appears to be fine-tuning the original MTP head of MiMo-7B-RL on 389.4K self-distilled samples. The model architecture, training objective, and inference process are similar to the EAGLE series [1]. If that is the case, the methodology should primarily highlight the construction of the self-distilled dataset. However, this version of the manuscript devotes large parts of the main body to presenting content similar to existing speculative approaches, which lack substantial technical contributions.\n2. **Stronger baselines**: FastMTP fine-tunes the original MTP head of MiMo-7B-RL on 389.4K self-distilled samples. The training process is identical to the conventional training recipe for speculative decoding (SD). Given this, FastMTP should be compared to stronger SD baselines, such as the EAGLE series, to better demonstrate its superiority and contributions.\n3. **Lack of data construction details**: The main contribution of this manuscript is the construction of 389.4K self-distilled samples. However, this process is only briefly described in Appendix A, with many important details missing. For example:\n   - What exact data sources are used to construct the dataset? These sources should be clearly introduced and properly cited.\n   - How are the proportions of the different data sources determined?\n   - What is the total token count of the 389.4K self-distilled samples?\n\n\n\n[1] EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. Li et al. ICML 2024."}, "questions": {"value": "Please check the weakness part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zr6LWfAZ9B", "forum": "J7xDwZSyI4", "replyto": "J7xDwZSyI4", "signatures": ["ICLR.cc/2026/Conference/Submission24031/Reviewer_Xx1p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24031/Reviewer_Xx1p"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899720308, "cdate": 1761899720308, "tmdate": 1762942905435, "mdate": 1762942905435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the use of self-distillation to train a single MTP to predict several future drafts. The authors also incorporate vocabulary compression to improve the acceptance rate of the drafts. FastMTP outperforms vanilla MTP by reducing inference latency and improving acceptance rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The algorithm makes use of the MTP modules obtained during pretraining which are usually aligned with the primary model’s representations and thus increase the acceptance rate.\n* The paper reduces the memory requirements over vanilla MTP by having a single module for predicting several draft tokens.\n* The algorithm improves over vanilla MTP’s acceptance rate and reduces latency."}, "weaknesses": {"value": "* The algorithm does not scale beyond K=3 draft tokens. In real world deployment scenarios with a single primary model and multiple drafters, going beyond K=3 may be required.\n* The paper does not compare with other speculative decoding methods Sequoia, SpecDec++, Eagle-3, Medusa, QuantSpec, especially with algorithms such as QuantSpec that use self-speculation.\n\nReferences\n* Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding, Chen et al., 2024.\n* EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees, Le et al., 2024.\n* SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths, Huang et al.,  2024.\n* Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads, Cai et al., 2024.\n* QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache, Tiwari et al., 2025."}, "questions": {"value": "* Does training for K > 3 improve the acceptance rate (even for K<=3)?\n* Is there an ablation on the number of MTP modules? Do 2 MTP modules trained with self distillation offer a more prominent reduction in latency when compared to 1?\n* Is there an ablation around initializing the MTP module from scratch? For models not trained with MTP, this ablation can help assess the universality of FastMTP."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TAuZHV0BTu", "forum": "J7xDwZSyI4", "replyto": "J7xDwZSyI4", "signatures": ["ICLR.cc/2026/Conference/Submission24031/Reviewer_KsDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24031/Reviewer_KsDF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970773711, "cdate": 1761970773711, "tmdate": 1762942905020, "mdate": 1762942905020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}