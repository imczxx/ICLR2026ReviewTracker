{"id": "RExGGPD5tw", "number": 156, "cdate": 1756729793013, "mdate": 1759898273972, "content": {"title": "Less Gaussians, Texture More: 4K Feed-Forward Textured Splatting", "abstract": "Existing feed-forward 3D Gaussian Splatting methods typically rely on pixel-aligned primitives, which makes scaling to higher resolutions (e.g., 4K) prohibitive as the number of Gaussians grows quadratically with image resolution. We introduce LGTM (Less Gaussians, Texture More), a feed-forward and pose-free framework that predicts both compact geometric primitives and associated per-primitive texture maps in a single forward pass without per-scene optimization.", "tldr": "", "keywords": ["Gaussian Splatting", "Feed-Forward", "3D Reconstruction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5db8bfcca157d34b729c9c810869dd4ab69a7c79.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces LGTM (Less Gaussians, Texture More), a feed-forward framework for high-resolution (up to 4K) novel view synthesis with textured Gaussian primitives. The core idea is to decouple geometry from appearance: a primitive network consumes low-resolution inputs to predict a compact grid of 2DGS geometry (centers, scales, rotations, SH base color), while a texture network consumes high-resolution inputs to predict per-primitive color and alpha texture maps via image patchifying and learned projective texturing. They used a 2-stage training, achieving coarse-to-fine prediction. Firstly a rough geometric and then detailed textures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-writen and easy to understand. \n2. The results seem to be pretty good improvement on the base models. \n3. Pretty efficient algorithm and is friendly to small GPUs."}, "weaknesses": {"value": "1. This work uses the few Gaussians from low-resolution images to serve as geometry probes, and paint the surfaces with higher resolution images. This is some what equivalent to first obtain all Gaussian points from the full resolution images (one may achieve this through similar way as e.g. Point3R[1]) and then uniformly **drop** most of the Gaussians. This relies on an important (but not necessarily true) assumption that real-world geometries are all smooth. This may causes the model to potentially fail when observing high frequency geometry details e.g. hairs, cloths, bumps, etc. No result has been given in that regard. Instead, a more clever way of downsampling / compressing Gaussians could be leveraged to both preserving geometry details & acceptable resources needed.\n2. The baseline models are never trained on high resolution images while it can be done: e.g. augmenting the original data with different zoom-in / zoom-out scale & crop to original resolution at train time and inference with full-resolution. The current large scale evaluation doesn't make too much sense.\n3. In L232-245 the authors use literally 1/4 page talking about a very elementary technique on assigning texture colors to Gaussian points, while ignoring some of the possibly more challenging topics such as how to handle view inconsistencies with less Gaussian points on high resolution images.\n4. The authors may have used the term \"primitive\" with different meanings without firmly defining any of them. As a result, it is hard to parse some parts of the manuscript. For example, what is \"per-primitive texture maps\", and how does it differ from \"2DGS primitives\"? The term \"Primitive Resolution\" repeatedly appears but is never explained.\n5. The use of mathematical symbols are not consistent, e.g. Eq 4. $f_{texture}$ seems like a mapping, while in L226, it becomes a \"feature\". The use of superscripts and subscripts are totally a mess.\n\n[1] Wu et al. Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory.  arXiv:2507.02863."}, "questions": {"value": "1. Do you have results/failure cases on scenes with thin/fine structures (hair, fabric wrinkles, foliage)? \n2. Can you compare against a full-resolution primitive predictor followed by (a) uniform decimation and (b) geometry-aware decimation/compression (e.g., curvature/edge cues)? What are the trade-offs?\n3. Were baselines retrained with strong multi-scale zoom/crop augmentation and full-resolution supervision, and evaluated under compute-matched budgets (memory/time)? If not, can you add this setting (and optionally test-time supersampling) to isolate the benefit of your texture mechanism?\n4. Can you report a simple consistency diagnostic (e.g., reprojection error or pose-jitter stress test) at high resolution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "r7E1VxNONB", "forum": "RExGGPD5tw", "replyto": "RExGGPD5tw", "signatures": ["ICLR.cc/2026/Conference/Submission156/Reviewer_T5YU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission156/Reviewer_T5YU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761260717249, "cdate": 1761260717249, "tmdate": 1762915458696, "mdate": 1762915458696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed LGTM, a general extension for feed-forward 3D/2D Gaussian Splatting \nto enable 4K resolution novel view synthesis. Specifically, the proposed method decomposes the geometry and texture, utilizing a primitive branch to predict the geometry from low-resolution images and a texture branch to predict the high-resolution textures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated.\n2. The paper is well-written and generally easy to follow.\n3. The experiments are throughout and the proposed method significantly boosts the performance of the baselines.\n4. The results is visually good."}, "weaknesses": {"value": "1. Necessity of integrating 4K texture to 3DGS: another solution to get 4K renderings can be a general feed-forward 3DGS followed by an image super-resolution model. A comparison should be made between it and the proposed method.\n2. Evaluation: As the paper claims an immersive user experience, a non-reference perceptual metric such as Niqe [1] or Q-align [2] should be added as an evaluation metric.\n3. Line space issue in L.474.\n\n[1] Zhang, L., Zhang, L., & Bovik, A. C. (2015). A feature-enriched completely blind image quality evaluator. IEEE Transactions on Image Processing, 24(8), 2579-2591.\n\n[2] Wu, H., Zhang, Z., Zhang, W., Chen, C., Liao, L., Li, C., ... & Lin, W. (2023). Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090."}, "questions": {"value": "Please see in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2JVkUVZ2An", "forum": "RExGGPD5tw", "replyto": "RExGGPD5tw", "signatures": ["ICLR.cc/2026/Conference/Submission156/Reviewer_VCdN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission156/Reviewer_VCdN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910978747, "cdate": 1761910978747, "tmdate": 1762915458569, "mdate": 1762915458569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new feed-forward framework called LGTM (Less Gaussians, Texture More) to solve a major problem: existing feed-forward 3D Gaussian Splatting (3DGS) methods cannot scale to high resolutions like 4K.\nThe Solution (LGTM): The method decouples geometry prediction from appearance prediction using a dual-network architecture.\n- A Primitive Network takes a low-resolution image (e.g., $512 \\times 288$) to predict a compact, fixed set of geometric primitives.\n- A Texture Network takes the high-resolution image (e.g., 4K) to predict detailed, per-primitive texture maps that \"paint\" high-frequency details onto the simple geometry."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A novel topic on feed-forward 3D-GS.\n2. A fair well performance compared with baseline methods."}, "weaknesses": {"value": "1. High-resolution rendering requires accurate geometric prediction. However, the proposed method seems more like a trick—it projects a high-resolution image onto relatively coarse geometry. While this may work well when the input views have small viewpoint differences, it would be helpful if the authors could include additional experiments to evaluate the novel-view synthesis quality under different camera pose settings.\n2. The paper claims to \"decouple\" geometry and appearance 20, yet the architecture (Fig. 2) and method (Sec 4.2) show that the texture network $f_{texture}$ explicitly takes the primitive network's features $F_{prim}^v$ as input. This seems to be a one-way coupling rather than a full disentanglement.\n3. Although the paper provides a comprehensive analysis of the feed-forward 3DGS paradigm, recent per-scene optimization methods (e.g., Grendel-GS, CityGS-X) have already addressed high-resolution (e.g., 4K) rendering quite effectively. The reviewer understands that a direct comparison with these methods may be beyond the scope of this work, but it nonetheless raises the concern that the problem addressed here might not be as critical for the 3D vision community as implied.\n\nOverall, the paper tackles a relatively minor problem (i.e., 4K resolution rendering) using a fairly simple approach—mainly by adding an additional downstream head."}, "questions": {"value": "Perhaps the authors could include more visualizations, especially video demonstrations of the rendered views based on the predicted 3D-GS."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "h64qPVADQp", "forum": "RExGGPD5tw", "replyto": "RExGGPD5tw", "signatures": ["ICLR.cc/2026/Conference/Submission156/Reviewer_9Eok"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission156/Reviewer_9Eok"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919798888, "cdate": 1761919798888, "tmdate": 1762915458461, "mdate": 1762915458461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes LGTM, a feed-forward framework that predicts compact Gaussian primitives coupled with per-primitive textures for high-resolution novel view synthesis. By decoupling geometry and appearance with a dual-network design, the method enables 4K rendering with significantly fewer primitives and without per-scene optimization. LGTM is applicable across various feed-forward 3DGS baselines, including single-view, two-view, and multi-view setups, and demonstrates consistent quantitative and qualitative improvements on several benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* The idea of combining low-resolution Gaussian primitives with high-resolution texture maps to achieve high-resolution feed-forward Gaussian Splatting is well-motivated and sound. Such an insightful finding may significantly boost the feed-forward 3DGS community to explore higher-quality synthesis.\n\n* The introduced module is architecture-agnostic and is thoroughly evaluated across several state-of-the-art models and large-scale benchmarks. All experiments show consistent quantitative and qualitative improvements.\n\n* The manuscript is well structured and easy to follow."}, "weaknesses": {"value": "* Lack of multi-view results. It would be better to provide video results or multi-view images to better illustrate the impact of the texture modules. I am concerned that the texture module may potentially destroy multi-view consistency to some extent.\n\n\n* Lack of evaluation under dense multi-view settings. Most experiments are conducted with 1, 2, or 4 views. Since the multi-view model is based on VGGT, which natively supports dense input views, it would be better to include results with denser settings, such as 32 or 64 views, similar to AnySplat [ref 1].\n\n\n* Missing comparison with per-scene optimization methods. It would strengthen the work to compare with per-scene optimization approaches such as BBSplat, given that BBSplat inspired this work. Such comparisons are also common in other works  exploring high-resolution feed-forward 3DGS, such as Long-LRM [ref 2] and LVT [ref 3].\n\n### References:\n\n* [ref 1] Jiang, Lihan, et al. \"AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views.\" arXiv preprint arXiv:2505.23716 (2025).\n* [ref 2] Ziwen, Chen, et al. \"Long-lrm: Long-sequence large reconstruction model for wide-coverage gaussian splats.\" ICCV 2025.\n* [ref 3] Imtiaz, Tooba, et al. \"LVT: Large-Scale Scene Reconstruction via Local View Transformers.\" arXiv preprint arXiv:2509.25001 (2025)."}, "questions": {"value": "Kindly refer to [Weaknesses] section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "w3lb9Y93o0", "forum": "RExGGPD5tw", "replyto": "RExGGPD5tw", "signatures": ["ICLR.cc/2026/Conference/Submission156/Reviewer_wL1e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission156/Reviewer_wL1e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989702740, "cdate": 1761989702740, "tmdate": 1762915458344, "mdate": 1762915458344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}