{"id": "CNLC4ZkLmW", "number": 4529, "cdate": 1757698404990, "mdate": 1759898028130, "content": {"title": "Shoot from the HIP: Hessian Interatomic Potentials without derivatives", "abstract": "Fundamental tasks in computational chemistry, from transition state search to vibrational analysis, rely on molecular Hessians, which are the second derivatives of the potential energy. Yet, Hessians are computationally expensive to calculate and scale poorly with system size, with both quantum mechanical methods and neural networks. In this work, we demonstrate that Hessians can be predicted directly from a deep learning model, without relying on automatic differentiation or finite differences. \nWe observe that one can construct SE(3)-equivariant, symmetric Hessians from irreducible representations (irrep) features up to degree $l$=2 computed during message passing in graph neural networks. \nThis makes HIP Hessians one to two orders of magnitude faster, more accurate, more memory efficient, easier to train, and enables more favorable scaling with system size. We validate our predictions across a wide range of downstream tasks, demonstrating consistently superior performance for transition state search, accelerated geometry optimization, zero-point energy corrections, and vibrational analysis benchmarks.\nWe open-source the HIP codebase and model weights to enable further development of the direct prediction of Hessians.", "tldr": "Direct prediction of the Hessian with extended MLIPs, with symmetries, >70x faster and more accurate than autograd Hessians.", "keywords": ["Hessian", "Molecules", "Atomistic Simulation", "Transition States", "Machine Learning Force Fields", "Machine Learning Interatomic Potentials", "Molecular Dynamics"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d02276aeaaa171dd200e23a9608d87de848a5248.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors investigate the accuracy of directly predicting molecular Hessian, rather than deriving them from numerical derivatives of forces or automatic differentiation, for predicting vibrational properties of molecules. They claim that forward prediction is “one to two orders of magnitude faster, more accurate, more memory efficient, easier to train, and enables more favorable scaling with system size” compared to autodifferentiation. For evaluation, the authors use an EquiformerV2 backbone to predict Hessians of the HORM dataset of organic molecules and compare HIP-EquiformerV2 against AlphaNet, LEFTNet, LEFTNet-df, and EquiformerV2 using autodifferentiation.\n\nThis paper addresses an important bottleneck in atomistic simulation. Benchmarks of this kind are valuable for the community, as they clarify the tradeoffs between direct prediction and differentiation-based approaches and help researchers decide which method is most appropriate for a given use case. The experiments are promising, but the claims about speed and generality need expanded benchmarking and clearer implementation transparency to be a solid resource for the community and thus merit acceptance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well written, accessible, and gives a solid overview of relevant literature, including clear derivations of how Hessians can be constructed from edge features using Clebsch–Gordan coefficients.\n* The experimental evaluation is comprehensive, comparing multiple aspects of Hessian prediction: convergence steps, wall time, accuracy of geometry optimization, zero-point energy, transition-state search, frequency analysis, and raw elementwise accuracy.\n* The authors clearly state limitations, such as reliance on ground-truth DFT data and the current restriction to small molecular systems."}, "weaknesses": {"value": "* The direct prediction of Hessians is not, on its own, a sufficiently novel contribution to carry the paper. Its impact depends on demonstrating clear and reproducible efficiency–accuracy tradeoffs across architectures and datasets, which currently feels underexplored.\n* It is unclear how much of the difference in AD vs. HIP accuracy for EquiformerV2 arises from training setup and task formulation since EquiformerV2 was not originally designed or tuned for Hessian prediction vs. being a “fundamentally” better approach to learn second derivatives directly. The authors should discuss how architectural or training adaptations might influence these results, and whether the comparison reflects representational capacity or differing task suitability.\n* While numerical derivatives scale poorly, they remain heavily used due to simplicity and established workflows. A direct wall-time comparison (finite-difference forces vs. HIP vs. AD) would contextualize the claimed speedups.\n* The comparisons of memory, batching, and wall time are highly implementation-dependent. The paper would benefit from a reproducible table specifying hardware, batch size, precision, and AD implementation details (e.g., JAX vs. PyTorch) since these factors dominate observed scaling.\n* When training Hessians via AD, one would typically not predict full Hessians every iteration but use sampled Hessian-vector products or partial training, which can substantially mitigate costs. This nuance should be acknowledged to make the efficiency discussion fairer.\n* As far as I can tell, the HORM benchmark is not peer-reviewed. Including results on an established dataset (e.g., Hessian-QM9) or a small material-system test would strengthen generality claims.\n* The paper evaluates HIP vs. AD only for EquiformerV2. It would be informative to test lighter-weight architectures (e.g., NequIP, MACE, Allegro) to see whether the efficiency and accuracy gaps persist across model families."}, "questions": {"value": "1. Could the authors clarify the exact computational setup for timing and memory benchmarks—hardware type, precision, AD backend (JAX vs. PyTorch), and batch sizes? These details are critical for reproducibility.\n2. How sensitive are the reported speed and memory gains to the choice of AD implementation or backend?\n3. Given that only the Hessian head was retrained on a frozen backbone, do the authors expect joint end-to-end training of energies, forces, and Hessians to change the relative trends between HIP and AD?\n4. Would the HIP approach generalize straightforwardly to periodic or larger material systems, or are there challenges related to locality assumptions and cutoff sparsity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lhXXQvZDOo", "forum": "CNLC4ZkLmW", "replyto": "CNLC4ZkLmW", "signatures": ["ICLR.cc/2026/Conference/Submission4529/Reviewer_C1zb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4529/Reviewer_C1zb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768083665, "cdate": 1761768083665, "tmdate": 1762917426318, "mdate": 1762917426318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Hessian Interatomic Potential (HIP) framework, which leverages SE(3)-equivariant neural networks to directly predict the full (3N \\times 3N) molecular Hessian matrix, thereby accelerating the computation of second-derivative information in molecular simulations. In terms of architecture, HIP employs EquiformerV2 as the core equivariant backbone, coupled with a Clebsch–Gordan (CG) expansion–based prediction head to construct the final Hessian tensor with proper rotational symmetry. Furthermore, the method incorporates a loss function that includes eigenvector-based constraints, which guide the model to produce Hessian predictions with more accurate eigenvalue spectra, improving the quality of vibrational and stability properties derived from the predicted matrices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper aims to provide a comprehensive evaluation of the predicted Hessian matrices through multiple downstream tasks, including geometry optimization, zero-point energy (ZPE) estimation, and transition-state (TS) searches to test the accuracy and the traditional methods."}, "weaknesses": {"value": "1. The paper lacks sufficient discussion and comparison with existing SE(3)-equivariant networks designed for predicting equivariant matrices. A series of works following PhiSNet have explored predicting the Hamiltonian matrix, which is also an equivariant matrix similar in nature to the Hessian. The architectures developed in those studies could, in principle, be applied to Hessian prediction as well. Therefore, including an introduction and comparison with these related models would help demonstrate the advantages of the proposed approach in this prediction task.\n\n2. Furthermore, regarding the loss function design, paper [1] introduces a method that imposes constraints on the eigen-energies of the Hamiltonian matrix. The loss function presented in Section 3.3 of the current submission appears to be identical to that in [1], and this overlap should be clearly acknowledged and discussed.\n\n3. Overall, the writing and organization of the paper are clear. However, my main concern lies in the novelty of the machine learning contribution. Most of the techniques employed have already been introduced in prior works, and the Hessian matrix prediction task closely resembles Hamiltonian matrix prediction in its formulation—an area that has been previously studied but not sufficiently discussed or differentiated in this submission."}, "questions": {"value": "N/A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HpCHxCyqdE", "forum": "CNLC4ZkLmW", "replyto": "CNLC4ZkLmW", "signatures": ["ICLR.cc/2026/Conference/Submission4529/Reviewer_22z7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4529/Reviewer_22z7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950975119, "cdate": 1761950975119, "tmdate": 1762917425360, "mdate": 1762917425360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a derivative-free method to predict Hessian matrices for interatomic potential models, which reduces the computational cost of Hessian evaluation. The method directly constructs SE(3) equivariant symmetric Hessians from higher-order equivariant features of a graph neural network. This lowers both memory and time costs for training and inference that involve Hessians in interatomic potentials. The paper validates the trained models on multiple downstream tasks and the accuracy remains consistently strong."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.It proposes a novel derivative-free Hessian prediction method and a loss function tailored to the Hessian subspace.\n2.The Hessian prediction method guarantees equivariance and symmetry, and the approach is simple, reasonable, and self-consistent.\n3.The method accelerates Hessian prediction, reducing complexity from O(N^2) to O(N), and it also lowers memory usage.\n4.The method achieves high accuracy and performs well across various downstream tasks."}, "weaknesses": {"value": "1.Direct Hessian prediction is not applicable when DFT Hessian data are lacking.\n2.The reliability of direct Hessian prediction is uncertain.\n3. This method will be quite likely to fail when long-time simulation is required."}, "questions": {"value": "1.Obtaining Hessian data from DFT is difficult, and there are currently few DFT datasets that include Hessians. In this context, AD-based Hessian prediction can produce Hessians without training specifically on Hessians. In contrast, direct Hessian prediction necessarily requires Hessian data for training, which limits its applicability.\n2.The generated Hessian matrices satisfy rotational equivariance and symmetry. However, direct Hessian prediction does not arise from differentiating energy or forces. Could this lead to other physical inconsistencies? For example, physically there should be six zero eigenvalues. Although this is included in the loss, it is not guaranteed by the model architecture. The paper provides extensive numerical and downstream evidence for effectiveness, and direct Hessians do not suffer from the non-conservative force issue. Even so, this potential non-physical aspect remains concerning. Could the authors offer theoretical discussion and clarification?\n3.In Table 1, the Hessian accuracy of the AD baselines is similar to that reported in the HORM paper, but the eigenvalue accuracy differs considerably. Was a different error metric used?\n4.The authors claim improved Hessian prediction accuracy. With Equiformer V2 as the backbone and only training a separate Hessian head, improvements are indeed observed, even in loss ablations. However, the Equiformer V2 backbone comes from HORM and has already been trained with Hessian data. This work then trains on that backbone. The authors state this isolates the effects of energy and forces, but this seems unfair. Could the authors provide results where the backbone is trained only on energy and forces, then frozen, and the Hessian head is trained on top?\n5.Among the AD baselines, some models use direct forces. Even with AD, the resulting Hessians are not fully physical. The comparison between LEFTNet and LEFTNet-df shows that conservative force models yield more accurate Hessians. The AD baseline for Equiformer V2 uses direct forces. Would an AD Hessian with conservative forces on Equiformer V2 achieve higher accuracy?\n6.Typos: line 102 Hesians. line 465 should be Table 3. line 1001 transtion.\n7. Can this method work for long time simulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sQCPI4nb6n", "forum": "CNLC4ZkLmW", "replyto": "CNLC4ZkLmW", "signatures": ["ICLR.cc/2026/Conference/Submission4529/Reviewer_vyqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4529/Reviewer_vyqg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993106920, "cdate": 1761993106920, "tmdate": 1762917424840, "mdate": 1762917424840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to everyone"}, "comment": {"value": "We thank all the reviewers for their time and constructive comments. In response to the feedback we ran several extra experiments that greatly improved the paper and we think might be interesting to all reviewers. The full results can be found in the reuploaded PDF and are summarized here:\n\n- We trained HIP end-to-end without the pretrained backbone. This improved the Hessian accuracy significantly, and also improves force and energy accuracy across all data scales (table 1 and figure 6 in the PDF)\n- Additional validation: We evaluate on the RGD1 datasplit with up to 35 atoms, significantly larger than the max of 25 atoms in the train set; HIP is still the best performing model and by far the fastest (table 4)\n- Extra dataset: We train and evaluate on the QM9 Hessian dataset. HIP consistently beats the QM9 Hessian authors baseline (a NequIP architecture) and the AD Hessian Equiformer baselines trained by us on all splits while being faster (table 11)\n- As suggested by reviewer vyqg, we included a mathmatical discussion when direct predictions are safe\n- Extra timings comparing AD Hessians from AD forces, AD Hessians from direct forces, finite difference Hessians from AD forces, Finite difference from direct forces and HIP, demonstrating HIP is significantly faster"}}, "id": "Ly3NWwHZeG", "forum": "CNLC4ZkLmW", "replyto": "CNLC4ZkLmW", "signatures": ["ICLR.cc/2026/Conference/Submission4529/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4529/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4529/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763668000605, "cdate": 1763668000605, "tmdate": 1763668000605, "mdate": 1763668000605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}