{"id": "7SdQhdojN9", "number": 1930, "cdate": 1756968115175, "mdate": 1759898177852, "content": {"title": "MedQ-Bench: Evaluating and Exploring Medical Low-level Visual Abilities in MLLMs", "abstract": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for clinical AI, yet existing approaches remain constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation. To address this gap, we introduce MedQ-Bench, a comprehensive benchmark that establishes a perception–reasoning paradigm for language-based evaluation of medical image quality with Multi-modal Large Language Models (MLLMs).\nMedQ-Bench defines two complementary tasks: (1) MedQ-Perception, which probes low-level perceptual capability via human-curated questions on fundamental visual attributes; and (2) MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks, aligning model evaluation with human-like reasoning on image quality. The benchmark spans five imaging modalities and over forty quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, covering diverse image sources including authentic clinical acquisitions, images with simulated degradations via physics-based reconstructions, and AI-generated images.\nTo evaluate reasoning ability, we propose a multi-dimensional judging protocol that assesses model outputs along four complementary axes. We further conduct rigorous human–AI alignment validation by comparing LLM-based judgement with radiologists.\nOur evaluation of 14 state-of-the-art MLLMs demonstrates that models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. These findings highlight the need for targeted optimization of MLLMs in medical IQA. We hope that MedQ-Bench will catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation.", "tldr": "", "keywords": ["medical AI", "low-level vision", "MLLM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b875db58b532a71d7c6dc5bb2d7d920bcc36f9e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a benchmarking framework for probing the capabilities of multi-modal LLMs on medical image quality assessment. Existing evaluation metrics are scalar, score-based and cannot capture the nuances of human expert evaluations of such images. The paper defines two main tasks: 1) simple perception-based questions on image artifacts, degradations and other low-level visual characteristics, and 2) a reasoning task more akin to expert evaluation. Authors introduce a judging protocol that assesses the model outputs along different dimensions, where alignment with human judgment is demonstrated. State-of-the-art models demonstrate performance somewhere between a non-expert and expert human, suggesting the need for further improvements in the medical domain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper has strong motivation and it is presented well. I especially like Fig. 2 demonstrating how traditional image similarity metrics fail to capture medical image quality for diagnostic purposes. \n\n- The benchmarking framework is overall reasonable, it incorporates a good mix of fundamental image quality assessment problems and more advanced reasoning tasks.\n\n- Questions targeting imaging artifacts are tailored towards the specific medical imaging modalities.\n\n- The benchmark and evaluation protocol incorporated human experts in multiple stages, supporting the reliability of the benchmark."}, "weaknesses": {"value": "- One key aspect I believe this benchmark is missing is the context-dependence of medical image quality. Depending on the diagnostic task, the same image can be of satisfactory for one task and unusable for another. To my understanding, this benchmark cannot consider this aspect as the questions are directly asking about an absolute score of image quality. Adding medical context to the QA problems would address this issue and make the benchmark more complete.\n\n- It appears to me that some evaluation dimensions can be subjective (e.g. no/mild/severe degradation) and might confuse the LLM. Without some kind of reference it can be challenging for the model to determine if a degraded image is mildly or severely degraded outside of medical context. One way to address this would be few-shot evaluations, where the model is shown some demonstrations before asking each of the questions and thus can \"calibrate\" its responses. \n\n- A more in-depth analysis of concrete failure modes would be helpful with some discussion on how to address the shortcomings of current models, highlighting potential future directions in model design or training improvements."}, "questions": {"value": "- How does the current benchmark consider the medical context when assessing image quality? \n\n- How is it ensured that the evaluated LLMs understand the scoring system introduced by the paper for fair/comparable evaluation?\n\n- What are the technical implications of the paper on the design and development of medical MLLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7Gr6puyYvy", "forum": "7SdQhdojN9", "replyto": "7SdQhdojN9", "signatures": ["ICLR.cc/2026/Conference/Submission1930/Reviewer_wTsk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1930/Reviewer_wTsk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761016756173, "cdate": 1761016756173, "tmdate": 1762915958765, "mdate": 1762915958765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes MedQ-Bench, a comprehensive benchmark that establishes a perception–reasoning paradigm for language-based evaluation of medical image quality with MLLMs. This work conducts a thorough evaluation of 14 state-of-the-art MLLMs, demonstrating that models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work introduces a comprehensive benchmark with 2,600 perceptual queries and 708 reasoning assessments, establishing a perception–reasoning paradigm for language-based evaluation of medical image quality with MLLMs.\n2. This work conducts a thorough evaluation of 14 state-of-the-art MLLMs. The evaluation results indicate that the models demonstrate preliminary yet unstable perceptual and reasoning skills."}, "weaknesses": {"value": "1. As the main scope is about medical images, why are AI-generated medical images collected? In reality, only real medical images will be used. Maybe some AI-generated medical images will contain some strange artifacts, which never occur in real cases. Therefore, I do not understand why this benchmark includes these AI-generated medical images. \n\n2. Line 68, SSIM is proposed in [R1]. The authors cite the wrong paper. Please do not simply search on Google Scholar. Make sure that every paper you cited is carefully checked. \n\n[R1] Wang, Zhou, et al. \"Image quality assessment: from error visibility to structural similarity.\" IEEE transactions on image processing 13.4 (2004): 600-612.\n\n3. Now, the word \"reasoning\" has some specific meanings, i.e., reasoning-based models like OpenAI-o1 and DeepSeek-R1. It is better to rename this task to \"interpretation\". I know that some prior works use this word, but at that time, the word \"reasoning\" was not so specific. \n\n4. Dataset annotation is a very important process in benchmark construction. I would like to suggest that the authors move some parts from the Appendix to the main paper. \n\n5. I am very confused about Figure 2. In this figure, the authors use \"Reasoning-IQA (Ours)\". However, this work is actually a benchmark work, and this work does not train its own medical IQA model. What is the meaning of \"Reasoning-IQA (Ours)\"? \n\n6. From Line 062 to Line 093, this part does not have any relationship to the main scope of this work. The main scope of this work is to assess the abilities of MLLMs in the medical IQA field. Why use such a large space to discuss score-based IQA? If this work discusses the weaknesses of score-based IQA, so what new medical IQA method does this work introduce? Actually, this work does not propose any new models. Overall, the introduction section is very chaotic, like a mixture of a method paper and a benchmark paper."}, "questions": {"value": "1. In Figure 3 \"Simulate Artifact\", \" Overall, the quality of this image is reject\" should be \"Overall, the quality of this image is rejected\".\n2. Some section names end without \".\" like \"2.3.3 EVALUATION METRICS\", but some with like \"2.3.2 COMPARISON REASONING TASKS.\".\n3. In Line 765, the closing quotation mark ('' in latex) should be the opening quotation mark (`` in latex). The same in Line 812, Line 813, and Line 814."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UOtqiY9Pgk", "forum": "7SdQhdojN9", "replyto": "7SdQhdojN9", "signatures": ["ICLR.cc/2026/Conference/Submission1930/Reviewer_GiGe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1930/Reviewer_GiGe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782932376, "cdate": 1761782932376, "tmdate": 1762915958588, "mdate": 1762915958588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MedQ-Bench, a novel benchmark for evaluating the medical image quality assessment (IQA) capabilities of multimodal large language models (MLLMs). MedQ-Bench establishes a perception-reasoning paradigm with two complementary tasks: MedQ-Perception (probing low-level visual attributes) and MedQ-Reasoning (encompassing no-reference and comparison reasoning). The benchmark covers 5 imaging modalities and over 40 quality attributes, comprising 2,600 perceptual queries and 708 reasoning assessments. The authors propose a multi-dimensional judging protocol for reasoning tasks and validate it through human-AI alignment. Evaluations of 14 MLLMs reveal preliminary but unstable perceptual and reasoning skills, highlighting the need for targeted optimization in medical IQA."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) Addresses a critical need for evaluating MLLMs in the safety-critical domain of medical IQA.\n2) MedQ-Bench is a comprehensive and well-designed benchmark, covering diverse modalities, quality attributes, and image sources.\n3) The multi-dimensional judging protocol and human-AI alignment validation enhance the reliability and validity of the evaluation."}, "weaknesses": {"value": "- The reasoning tasks inherently involve subjectivity, and the paper could provide more detail on how the annotation process minimizes bias and ensures reliability.\n- What are the most common types of errors made by MLLMs on MedQ-Bench? Are there specific IQA tasks or degradation types that are particularly challenging?\n- What are some potential applications of MedQ-Bench beyond model evaluation? Could it be used to guide the development of new IQA algorithms or assist clinicians in quality control?\nComputational Cost: What is the computational cost of using MedQ-Bench for evaluation? How long does it take to evaluate a single model on the benchmark?\n- Report inter-annotator agreement metrics (e.g., Cohen's kappa) for the reasoning tasks.\nDiscuss how disagreements between annotators were resolved."}, "questions": {"value": "- Some models (e.g., Grok-4, Qwen2.5-VL) performed better on fine-grained tasks than coarse-grained ones (Figure 6). This is counter-intuitive. Do you have a hypothesis for this phenomenon?\n- How did you mitigate the risk that using GPT-4o for question generation would unfairly favor GPT-family models in the evaluation? Have you analyzed whether there is a \"style bias\" in the generated questions?\n-  While the human-AI alignment is strong on the 200 sampled cases, how can you be sure this alignment holds for the full diversity of outputs from all 14 models, especially the very poor or nonsensical responses from weaker models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vDoMIoExOW", "forum": "7SdQhdojN9", "replyto": "7SdQhdojN9", "signatures": ["ICLR.cc/2026/Conference/Submission1930/Reviewer_PPyC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1930/Reviewer_PPyC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923511152, "cdate": 1761923511152, "tmdate": 1762915958470, "mdate": 1762915958470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MedQ-Bench, which it describes as “the first comprehensive benchmark” for evaluating medical image quality assessment (IQA) abilities of multimodal large language models (MLLMs). The benchmark is organized around a “perception–reasoning paradigm” with two main tasks: (1) MedQ-Perception, which uses curated visual questions to test perceptual understanding of low-level quality attributes, and (2) MedQ-Reasoning, which asks models to generate free-form analyses of quality issues, including comparative reasoning across image pairs. The dataset spans 5 imaging modalities, covers 40+ quality attributes, and claims 2,600 perception queries and 708 reasoning assessments across 3,308 samples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The topic (MLLMs for medical IQA) is timely and important. The benchmark could become useful, and the clinical framing is compelling. The data is comprehensive, and the evaluation is extensive."}, "weaknesses": {"value": "1. Novelty is overstated. The benchmark looks like a domain-specific adaptation of prior “quality reasoning” benchmarks. In Related Work, the paper itself cites several highly similar efforts in non-medical domains, e.g., Q-Bench. These explicitly aim to (i) probe perception of low-level degradations, (ii) elicit natural-language quality rationales, and (iii) score reasoning-like explanations. This work is more of an adaptation of an existing template\n\n2. The benchmark tasks are (a) Yes/No / What / How multiple-choice perceptual questions, and (b) free-form textual reasoning that ends in a 3-level “good/usable/reject” judgment, which is said to approximate clinical acceptability. However, there is no evidence that performance on these tasks correlates with actual downstream diagnostic safety, e.g. sensitivity/specificity of pathology detection by radiologists when given “usable” vs “reject” images.\n\n3. There is no reader study showing that if an MLLM calls an image “usable,” a clinician would skip reacquisition and would not miss a lesion. This claim and setting are just too intuitive. The paired comparison task is mostly about visual preference (“which image is higher diagnostic quality”), not about safety-critical failure modes.\n\n4. To score the free-form reasoning outputs, the authors introduce a GPT-4o-based measure suite. And they claim that recent studies have demonstrated GPT-4o to be a reliable evaluation tool and perform human–AI alignment validation on 200 sampled cases. You use GPT-4o as both (i) the evaluation judge for all models and (ii) a system under evaluation in Table 1. LLMs-as-judges prefer to vote the one with a similar corpus distribution to itself.  This raises fairness concerns: is GPT-4o systematically more lenient to GPT-4o-like phrasing styles?\n\n5. For perception, results are reported as aggregate accuracies for Yes-or-No, What, and How, plus an “Overall” number per model. This is overly simplistic and does not regard the uniqueness of the medical scenario. No significance testing or CIs are reported. For example, is GPT-5’s 68.97% actually significantly better than GPT-4o’s 64.79%?\n\n6. The pipeline relied on GPT-4o for question expansion. That means the benchmark may encode GPT-4o’s linguistic priors, which in turn advantages GPT-4o (or the entire GPT-family if stylistically similar) during evaluation.\n\n7. The paper does not give a quantitative definition of “fine-grained difference.” How was this threshold established? By MOS? By PSNR/SSIM gap? By the qualitative agreement of three experts?\n\n8. Our primary contributions are as follows: repeats twice.\n\n9. LVLMs and MLLMs are used alternately. Please be consistent on terminology.\n\n10. Section 2.3.1 says reasoning tasks avoid high-level diagnostic interpretation and focus on low-level technical quality factors. But examples in Figure 3 include modality and anatomical region identification. Why? Please clarify the line between “quality reasoning” and “diagnosis.”"}, "questions": {"value": "Please see the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fiol1iTp0L", "forum": "7SdQhdojN9", "replyto": "7SdQhdojN9", "signatures": ["ICLR.cc/2026/Conference/Submission1930/Reviewer_q3Dd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1930/Reviewer_q3Dd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930622692, "cdate": 1761930622692, "tmdate": 1762915958361, "mdate": 1762915958361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}