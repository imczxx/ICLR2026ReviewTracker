{"id": "wsnse46kRO", "number": 11928, "cdate": 1758204685138, "mdate": 1759897545584, "content": {"title": "Visual Planning: Let's Think Only with Images", "abstract": "Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations for these \"vision-first'' tasks, as a supplementary channel to language-based reasoning. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising supplement to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.", "tldr": "", "keywords": ["visual planning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4a58491681e1ae084e25007ac8f8207310d6a98.pdf", "supplementary_material": "/attachment/3264dd16043f2a32df837166e6a693ce36b194e3.zip"}, "replies": [{"content": {"summary": {"value": "This study proposes a visual planning approach that performs task planning entirely through visual representations. In this paradigm, planning is carried out via a sequence of images, which encode the step-by-step reasoning process in the visual domain, similar to how humans make sketches or visualize future actions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "It performs planning purely within the visual modality as a holistic process, where the actions are not explicitly predicted but instead implicitly represented by transitions between visual states."}, "weaknesses": {"value": "The experimental validation is restricted to discrete, low-dimensional grid-world navigation tasks, where the visual states are comparatively simple and straightforward to encode.\nVisual planning implicitly expresses actions by generating a sequence of visual states. Although this avoids modality switching, when planning fails, the absence of an explicit action sequence (such as textual CoT) makes the model’s decision process difficult to debug and understand."}, "questions": {"value": "1. Visual planning involves generating high-dimensional image sequences, which can be computationally more expensive than searching in low-dimensional text/action spaces as in language models. Please elaborate on how  it's computational efficiency and search space complexity compare to textual CoT methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KSj99bTRe4", "forum": "wsnse46kRO", "replyto": "wsnse46kRO", "signatures": ["ICLR.cc/2026/Conference/Submission11928/Reviewer_Rhdx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11928/Reviewer_Rhdx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880533801, "cdate": 1761880533801, "tmdate": 1762922933632, "mdate": 1762922933632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper takes visuals as a medium for expressing and structuring reasoning, which potentially learn spatial and geometrical information. To realize this, the author proposes Visual Planning, planning through purely visual representations for “vision-first” tasks. The proposed GRPO demonstrates improvement in planning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The author investigates the potential of visual representation as a medium, which expands the research of LLMs to a broader area.\n\nThe presentation of the paper is great, with a clear statement and an appropriate graph.\n\nThe paper is the first attempt to investigate whether models can achieve planning purely through visual representations."}, "weaknesses": {"value": "Can you provide any figures to clearly show the difference between language as a medium and visual as a medium in certain cases?\n\nIt will be better if we can discuss any advantages of visual as a medium in real CV tasks, such as visual grounding. And the proposed methods, whether they can be easily transferred to 3D?\n\nIf we finally want to get an MLLM, how do we add GRPO to the regular training receipt? When to align visuals with other modalities?"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uKqxAF3ePi", "forum": "wsnse46kRO", "replyto": "wsnse46kRO", "signatures": ["ICLR.cc/2026/Conference/Submission11928/Reviewer_uu3P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11928/Reviewer_uu3P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884217660, "cdate": 1761884217660, "tmdate": 1762922933048, "mdate": 1762922933048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Visual Planning, a new paradigm that performs reasoning through visual representations rather than language. Using VPRL (Visual Planning via Reinforcement Learning), a GRPO-based post-training framework for large vision models, our approach generates image sequences that visualize step-by-step inference. Experiments on visual navigation tasks (FrozenLake, Maze, MiniBehavior) show that VPRL surpasses text-based reasoning, highlighting visual reasoning as a powerful complement to language reasoning for spatial and geometry-driven problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. New Paradigm: The paper introduces \"Visual Planning\" as a genuinely new paradigm for reasoning.\n\n2. Good Empirical Results: The proposed method, Visual Planning via Reinforcement Learning (VPRL), significantly outperforms a wide range of baselines.\n\n3. Methodological Robustness: The two-stage VPRL framework is well-designed and justified."}, "weaknesses": {"value": "1. Reliance on an External Oracle for Rewards: A significant weakness in the method's detail is its reliance on non-learned, external modules to provide the reward signal. The VPRL framework depends on a \"dynamics interpreter\" and a \"progress estimator\". The appendix reveals this estimator is a Breadth First Search (BFS) algorithm —an oracle that has already solved the task and knows the optimal path from every state. The interpreter also uses rule-based pixel and IoU comparisons. This means the model isn't learning the environment's dynamics or the concept of progress; it's learning to generate images that satisfy an external oracle that already has the answers.\n\n2. Insufficient Justification for a \"Purely Visual\" Paradigm: This paper needs to justify why exploring a purely non-verbal, vision-only paradigm is a necessary or superior research direction. The authors' decision to \"eliminate language as a confounding factor\" is a research-scoping choice, but it is not a strong argument for the paradigm's utility.\n\n3. Limited Task Complexity and Scalability: The paradigm is validated only on simple, 2D, discrete grid-world environments (FROZENLAKE, MAZE, MINIBEHAVIOR). It is highly questionable if this approach can scale to complex, 3D, photorealistic, or continuous-state environments (e.g., robotics). In such settings, autoregressively generating a perfect, step-by-step sequence of future images is computationally expensive (a point the authors concede ) and the rule-based reward function (which relies on pixel comparison ) would be far too brittle to work."}, "questions": {"value": "All of my qeustions are listed in the weakness section. If my concerns are well addressed, I will raise my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FitTHFLxib", "forum": "wsnse46kRO", "replyto": "wsnse46kRO", "signatures": ["ICLR.cc/2026/Conference/Submission11928/Reviewer_2U49"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11928/Reviewer_2U49"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884965755, "cdate": 1761884965755, "tmdate": 1762922932535, "mdate": 1762922932535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on visual planning tasks in MLLMs. It proposes a novel way to perform visual planning by generating images during the planning process. Specifically, given an input image, the model reasons about the next state after performing certain actions by generating the corresponding images. To achieve this, the VLM is first trained to generate next-state images, then trained through reinforcement learning to encourage actions toward the target state. Experiments show that the proposed methods outperform strong open-source and private models in various environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Generating images makes the reasoning occur in the visual space rather than the textual space. Thus, the proposed method has the potential for more direct and better reasoning performance.\n2. Through qualitative analysis of intermediate outputs, the paper shows that the proposed method can generate reasonable intermediate images, which is key for correct visual reasoning.\n3. The experiment result demonstrates the proposed method outperforms strong baselines including private MLLMs."}, "weaknesses": {"value": "1. It can be observed that the intermediate images are not perfect (e.g., in Fig. 3, first row, the player and goal tokens have artifacts). Thus, it would be interesting if the paper could show performance when the model reasons over high-quality images. For example, each time the model generates a new image, the corresponding high-quality image (rendered by the engine rather than generated by the model itself) is fed into the model. Would this lead to better performance? If so, the performance gap could quantify the importance of generating high-quality (precise) intermediate images.\n2. If I understand correctly, $v$ stands for one of the intermediate images, as stated in L132–133. As such, how is it determined whether two images are an exact match in the EM metric in L300-L302? Or is it actually comparing in the action space (e.g., left/right) rather than the image space?\n3. Similar to this work, some recent studies also explore generating intermediate images during reasoning, such as [1]. It is good that the paper discusses these related works, but it should consider directly comparing with these baselines.\n\n[1] Imagine while Reasoning in Space: Multimodal Visualization-of-Thought"}, "questions": {"value": "Please check the weaknesses section:\n1. (for weakness 1) How important it is to generate high-quality images? Would current intermediate images quality good enough?\n2. (for weakness 2) If I understand correctly about EM measure, How to compute the exact match in the image space?\n3. (for weakness 3) Comparison with similar methods such as MVoT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2vZ6MpvGAG", "forum": "wsnse46kRO", "replyto": "wsnse46kRO", "signatures": ["ICLR.cc/2026/Conference/Submission11928/Reviewer_dg8a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11928/Reviewer_dg8a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981589905, "cdate": 1761981589905, "tmdate": 1762922932056, "mdate": 1762922932056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}