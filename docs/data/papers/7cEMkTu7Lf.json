{"id": "7cEMkTu7Lf", "number": 10009, "cdate": 1758155718651, "mdate": 1759897681063, "content": {"title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs", "abstract": "Unlearning in large language models (LLMs) aims to remove specified data, but its efficacy is typically assessed with task-level metrics like accuracy and perplexity. \nWe demonstrate that these metrics are often misleading, as models can appear to forget while their original behavior is easily restored through minimal fine-tuning. \nThis phenomenon of \\emph{reversibility} suggests that information is merely suppressed, not genuinely erased. To address this critical evaluation gap, we introduce a \\emph{representation-level analysis framework}. \nOur toolkit comprises PCA-based similarity and shift, centered kernel alignment (CKA), and Fisher information, complemented by a summary metric, the mean PCA distance, to measure representational drift. \nApplying this framework across six unlearning methods, three data domains, and two LLMs, we identify four distinct forgetting regimes based on their \\emph{reversibility} and \\emph{catastrophicity}. \nOur analysis reveals that achieving the ideal state--irreversible, non-catastrophic forgetting--is exceptionally challenging. \nBy probing the limits of unlearning, we identify a case of seemingly irreversible, targeted forgetting, offering new insights for designing more robust erasure algorithms. \nOur findings expose a fundamental gap in current evaluation practices and establish a representation-level foundation for trustworthy unlearning.", "tldr": "We introduce a representation-level diagnostic toolkit to analyze the reversibility of LLM unlearning.", "keywords": ["Unlearning", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6fd5ecb163a7e9c5d00f6578072dc7223e208ba2.pdf", "supplementary_material": "/attachment/6d16a809f762b882629e382e663a910892047fc8.zip"}, "replies": [{"content": {"summary": {"value": "* This paper evaluates unlearning in two settings: single (where all unlearning requests are available simultaneously) and continual (where unlearning requests arrive sequentially).\n* The paper defines four unlearning regimes that vary along two axes: reversible vs. irreversible (i.e., whether the unlearned knowledge can be recovered by lightweight retraining) and catastrophic vs. non-catastrophic (i.e., whether the unlearning process significantly affects unrelated knowledge).\n* Evaluating 3 unlearning methods and 2 LLMs, the paper first shows that unlearning methods cannot remove knowledge irreversibly. \n* Then, the paper describes and applies several methods for measuring representational similarity between the original LLM and its unlearned and retrained variants, finding that representational similarities correlate with reversibility of unlearning. \n* This is complemented by a theoretical analysis and a short case study of achieving irreversible, non-catastrophic unlearning. \n* The paper concludes that achieving irreversible unlearning remains hard, and representation analysis offers a new perspective on unlearning beyond accuracy on retain and forget sets"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**(S1)** The paper successfully demonstrates that current methods do not achieve irreversible and non-catastrophic unlearning\n\n**(S2)** The introduced taxonomy may be helpful in future work to better systematize and discuss the achievements of new unlearning methods\n\n**(S3)** The paper makes a convincing argument that accuracy metrics alone give an insufficient impression of unlearning success, and analyzing the model's internal representations can give important insights beyond accuracy\n\n**(S4)** The paper contains one example of successful irreversible and non-catastrophic unlearning, demonstrating that this goal may be achievable."}, "weaknesses": {"value": "**(W1)** One major concern is originality: The evaluation of unlearning methods successfully confirms that current methods do not achieve irreversible unlearning, but this is a known fact, as the paper also mentions (e.g., [24]). Likewise, the observation that models break down when applying multiple edits in continual learning has been reported before, e.g. [a]. Finally, none of the proposed metrics for representation analysis is novel.\n\n**(W2)** The paper does not contain any actionable insights. The representation analysis confirms that larger representational dissimilarity correlates with irreversible unlearning, but this is expected as the original model becomes harder to reconstruct the further the parameters move away from it. More importantly, the paper does not give practical tools, for example, how representation similarity can reliably predict successful and irreversible unlearning, which would be very helpful in practice. Overall, the takeaways from this analysis remain unclear: What is the reader to conclude beyond the observation that a larger representation shift correlates with irreversible unlearning?\n\n**(W3)** The theoretical analysis mirrors this problem: It mainly shows how a larger distortion of weights leads to greater dissimilarity (which is intuitive). The connection to irreversible unlearning is not formalized but only claimed in the paragraph starting with line 412. Sec. 5.2 additionally discusses that model outputs are not a reliable indicator of unlearning. However, this is also not a significant finding, as keeping all LLM parameters except those of the last layer frozen while randomizing parameters in the last layer will yield a practically random model (from the black-box perspective), while it is very likely that most information and knowledge learned by the model will continue to be accessible from earlier layers.\n\n**(W4)** The paper overall focuses on a narrow setting where unlearning and catastrophic forgetting are measured through fixed forget and retain sets. However, it does not consider recovering unlearned knowledge through prompt attacks (e.g., [b]) or mechanistic interpretability (e.g., [c], only intended as an example, not available before submission deadline).\n\n**(W5)** LLM unlearning is a popular research area with many methods. Claims that are meant to be generalizable to the entire field, such as the one in this paper, need to be either evaluated on a large set of methods or require a motivation for why the chosen set of unlearning methods is representative and will give such generalizable insights. This aspect can be expanded upon in the current paper.\n\n**(W6)** The experiment in 5.3. appears very interesting, because it directly targets the case of irreversible, non-catastrophic unlearning. This experiment could be one starting to inform more successful unlearning methods. Therefore, I think it would be very interesting to expand this perspective. One concern I have is to what extent this observation is due to the \"more constrained relearning conditions\" vs. actually successful unlearning.\n\n**(W7)** The supplementary material contains a large number of plots showing the representation similarity measures for different LLM layers. These plots are not individually interpreted or put in context. Their role in the paper is therefore doubtful. If they do not add any tangible value to the paper, consider removing them.\n\n### References\n[a] Thede et al.: Understanding the limits of lifelong knowledge editing in llms. In arXiv, 2025\\\n[b] Patil et al.: Can sensitive information be deleted from llms? objectives for defending against extraction attacks. In ICLR, 2024\\\n[c] Cywinski et al.: Eliciting Secret Knowledge from Language Models. In arXiv, 2025"}, "questions": {"value": "* Which novel perspectives on unlearning does this paper give the community beyond confirming known problems with current methods?\n* Which actionable improvements in LLM unlearning are informed by the representation analysis? What are the main insights beyond the expected observation that higher representation dissimilarity correlates with irreversible unlearning?\n* How can we motivate the findings in this paper to generalize to most methods for LLM unlearning, even those not evaluated in the paper? How about extraction attacks beyond retraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TIOptiNHG4", "forum": "7cEMkTu7Lf", "replyto": "7cEMkTu7Lf", "signatures": ["ICLR.cc/2026/Conference/Submission10009/Reviewer_n9vs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10009/Reviewer_n9vs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761422442464, "cdate": 1761422442464, "tmdate": 1762921427437, "mdate": 1762921427437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies two aspects of machine unlearning: reversibility and catastrophic forgetting.\nReversibility is probed via relearning, i.e., a one-epoch finetune on the forget set.\nThe authors show that, in many cases, this simple adjustment of model weights by relearning restores much of the original model’s performance.\n\nFrom this, they infer that the knowledge was not effectively removed and can be readily recovered. They further support this by analyzing the intermediate representation space of LLMs and measuring how the unlearned-then-relearned model deviates from the original.\n\nOverall, the paper finds that relearning often recovers performance, presenting this as a failure mode of unlearning and evidence that the underlying knowledge persists."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of studying how easily unlearned knowledge can be recovered after unlearning is quite interesting. In particular, applying relearning and then evaluating the model’s recovery is a valuable direction that deserves further exploration."}, "weaknesses": {"value": "I don’t find the results of this paper particularly surprising. A single step of finetuning on the forget set can naturally bring back the forgotten knowledge. I don’t quite see why the authors expected this not to work.\nAfter all, with more aggressive settings (e.g., two or three additional epochs), one could almost certainly recover the utility on the forget set. Restoring performance through one epoch of finetuning is not unexpected.\n\nIn general, unlearning methods that are truly “irreversible” often achieve this by severely degrading the model, seen as a drop in accuracy on the retain set and overall utility.\nA more interesting direction, in my view, would be to study the sample efficiency of relearning: can we recover performance using only a few samples or perhaps by providing them as in-context examples instead of full retraining?\n\nAlso, I recenlty found a paper on knowledge recovey of machine unlearning [1], I guess this also worth being discussed in this paper.\n[1] Rezaei, Keivan, et al. \"RESTOR: Knowledge Recovery in Machine Unlearning.\" arXiv preprint arXiv:2411.00204 (2024)."}, "questions": {"value": "They are discussed in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FGgD4sEYuV", "forum": "7cEMkTu7Lf", "replyto": "7cEMkTu7Lf", "signatures": ["ICLR.cc/2026/Conference/Submission10009/Reviewer_Ese5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10009/Reviewer_Ese5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763883198, "cdate": 1761763883198, "tmdate": 1762921427187, "mdate": 1762921427187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper highlights the drawback of relying solely on task-level metrics (e.g., accuracy, perplexity) for evaluating unlearning in LLMs, since these metrics cannot distinguish genuine erasure from superficial forgetting. To bridge this evaluation gap, the paper introduces a **representation-level analysis framework** to measure representational drift and categorize unlearning behavior into four regimes. The study concludes that achieving the ideal state—**irreversible and non-catastrophic forgetting**—is extremely challenging, and further provides a method combination that achieves a *seemingly* irreversible, non-catastrophic form of forgetting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- It clearly identifies the limitations of current task-level evaluations and proposes a **representation-level toolkit** that goes beyond surface metrics.\n- Provides clear definitions and a systematic taxonomy of forgetting regimes."}, "weaknesses": {"value": "- Table 2 demonstrates the weakness of task-level metrics, but it would be stronger to include results on the **Qwen2.5-7B** model to further consolidate this finding.\n- It remains unclear whether the same observations hold for **smaller (3B) or other model families (Llama)**.\n- The framework measures representational drift but does not formally assess **privacy leakage**; the notion of “irreversible forgetting” is still heuristic.\n- The proposed solution is interesting, but **cross-model validation** would strengthen its generality."}, "questions": {"value": "- Does the proposed framework also generalize to **LLaMA** or **Qwen3** models?\n- Could the **mean PCA distance** be correlated with formal privacy metrics such as **MIA AUC** in a consistent way?\n- In Tables 2 and 3, how relearning is conducted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PXpCTA3ggy", "forum": "7cEMkTu7Lf", "replyto": "7cEMkTu7Lf", "signatures": ["ICLR.cc/2026/Conference/Submission10009/Reviewer_HtjF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10009/Reviewer_HtjF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789405293, "cdate": 1761789405293, "tmdate": 1762921426501, "mdate": 1762921426501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}