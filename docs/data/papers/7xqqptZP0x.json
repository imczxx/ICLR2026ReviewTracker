{"id": "7xqqptZP0x", "number": 8884, "cdate": 1758101110601, "mdate": 1759897757368, "content": {"title": "Leveraging Self-Supervised and Supervised Embeddings for Memory-Efficient Experience-Replay Continual Learning", "abstract": "Catastrophic forgetting remains a key challenge in Continual Learning (CL). In replay-based CL with severe memory constraints, performance critically depends on the sample selection strategy - that is, which examples are stored for replay. Most existing approaches construct memory buffers using embeddings learned under supervised objectives. However, class-agnostic, self-supervised representations often encode rich, class-relevant semantics that are overlooked. We propose a new method, MERS - Multiple Embedding Replay Selection, which replaces the buffer selection module with a graph-based approach that integrates both supervised and self-supervised embeddings. Empirical results show consistent improvements over state-of-the-art selection strategies across a range of continual learning algorithms, with particularly strong gains in low-memory regimes. On CIFAR-100 and TinyImageNet, MERS outperforms single-embedding baselines without adding model parameters or increasing replay volume, making it a practical, drop-in enhancement for replay-based continual learning.", "tldr": "This work presents a novel sample selection method for continual learning that integrates supervised and self-supervised embeddings via a graph-based approach, and which yields SOTA results on CIFAR-100 and Tiny-ImageNet in low-memory settings.", "keywords": ["Life-Long and Continual Learning", "Representation Learning", "Self-Supervised Learning", "Deep Learning", "Representation Learning for Vision"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9eacd0b41ff9f8c5209ed3701ecbb41eb3afb062.pdf", "supplementary_material": "/attachment/8ed543dac51ad0bf37ea646be23c07a0b9c3dded.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces MERS (Multi-Embedding Replay Selection), a framework for exemplar selection in class-incremental continual learning that leverages both supervised and self-supervised embedding spaces to enhance sample diversity and representativeness of the examples in the memory buffer. MERS constructs k-NN coverage graphs using supervised and unsupervised embeddings (e.g., classifier features and SimCLR/VICReg representations) and greedily selects exemplars that maximize the coverage across these heterogeneous spaces. Two concrete techniques are presented: MERS-ProbCover, based on $\\delta$-ball coverage, and MERS-MaxHerding, computing coverage using a kernel. The authors propose data-driven hyperparameter alignment (median k-NN distance for $\\delta$, median heuristic for RBF $\\sigma$) and a density-based weighting scheme $\\alpha$ to balance the contributions of different embeddings. Experiments on Split CIFAR-100 and Split TinyImageNet with ER and ER-ACE baselines show consistent improvements in accuracy, particularly under tight memory budgets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "MERS combines supervised and self-supervised feature spaces, a concept that is both intuitive and proves empirically beneficial, particularly under tight memory constraints.\n\nData-driven parameter alignment: the use of median k-NN distances and the median heuristic for RBF bandwidth allows avoiding costly manual hyperparameter tuning.\n\nThe paper includes a comprehensive set of experiments across a wide range of buffer sizes, demonstrating that the proposed multi-embedding approach consistently improves performance, especially when using a small replay buffer."}, "weaknesses": {"value": "1. **Clarity**: the notation throughout the methodology section is dense and, at times, inconsistent. For example, in lines 145–147, symbols such as [M] and $\\mathcal{Z}^{\\(m\\)}$ are not clearly defined, and several equations are overloaded without adequate explanation. As a result, it becomes difficult to distinguish the actual contribution from prior work. The “Notation” subsections in Sections 3.1 and 3.2 are particularly opaque and unnecessarily formal, using several mathematical operators to describe concepts that could be conveyed far more simply and intuitively. Figure 1 is poorly designed, as it fails to visually explain the pipeline or clarify what the proposed method adds beyond existing replay selection strategies.\n\n2. Large portions of the paper are devoted to detailed explanations of well-established techniques such as SimCLR, DINO, VICReg, ProbCover, and MaxHerding. These sections occupy considerable space but add little to the reader’s understanding of the proposed contribution. The paper would benefit from condensing these descriptions to a brief summary or moving them to the appendix, allowing the main text to focus on what is actually novel in MERS (how it integrates these components and why this integration matters).\n\n3. Missing computational analysis: as this work enhances the buffer selection strategy with both supervised and unsupervised embeddings, it would be important to discuss and empirical evaluate its computational complexity, runtime overhead, and memory footprint. Currently, the authors only mention that MERS incurs double selection-time overhead and self-supervised training at the conclusion of the manuscript, which is not satisfactory.\n\n4. The approach is only applied to ER and ER-ACE. Testing with more advanced replay methods (e.g., DER++ [Buzzega et al., 2020], Co$^2$L [Cha et al., 2021], CILA [Wen et al., 2024]) would showcase the generality of MERS.\n\n### Minor points:\n5. Quantitative tables are hidden in the appendix, replaced by figures in the main paper that are difficult to interpret precisely.\n\n6. Overall writing and presentation need polishing: sentences are often long and convoluted, and key ideas are buried among excessive background information."}, "questions": {"value": "No questions.\n\n### Overall summary\nThe paper contains a potentially useful and empirically validated idea but falls short in clarity, presentation, and demonstration of novelty with respect to other works. Writing and figures need major revision to make the paper understandable and publishable. With a thorough rewrite, stronger baselines, runtime ablations, and more focused explanation on the contributions, this could become a solid work. However, in its current form, it is below the acceptance threshold."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pZa3c8oTbY", "forum": "7xqqptZP0x", "replyto": "7xqqptZP0x", "signatures": ["ICLR.cc/2026/Conference/Submission8884/Reviewer_dV8N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8884/Reviewer_dV8N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760879825709, "cdate": 1760879825709, "tmdate": 1762920643540, "mdate": 1762920643540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new memory buffer selection method, named MERS- Multiple Embedding Replay Selection, which uses a graph-based strategy to sample buffer data by integrating both supervised and self-supervised embeddings. Empirical results on three datasets (CIFAR-100, Tiny-ImageNet, MERS) show consistent improvements over some selection strategies across various continual learning algorithms, with particularly strong gains in low-memory regimes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The integration of complementary supervised and self-supervised embeddings into a graph coverage framework, formalized as a weighted multi-embedding coverage problem, offers a fresh perspective for exemplar selection.\n+ Extensive testing across multiple datasets and memory configurations demonstrates that the method consistently and significantly enhances the performance of various base algorithms, particularly under challenging low-memory conditions."}, "weaknesses": {"value": "+ The paper lacks a detailed description of how the memory buffer is specifically managed and updated. Please explicitly clarify: (a) whether the buffer capacity is allocated uniformly per class or dynamically; (b) when introducing a new task, whether sample replacement is performed globally or on a per-class basis; (c) why a very small number of exemplars (e.g., when |M|=100) can effectively retain old knowledge. It is recommended to supplement the analysis by exploring whether these selected exemplars truly capture the core features or decision boundaries of the classes. For instance, visualizations of the supervised and self-supervised embeddings could be provided.\n+ Experimental results (e.g., Figures 2, 3) indicate that the primary performance gain likely stems from the \"graph coverage selection\" mechanism itself, as the single-embedding versions already substantially outperform the baselines, while the relative marginal gain from the dual-embedding integrated version is limited. This appears somewhat decoupled from the thesis that \"multi-embedding fusion\" is the core innovation. It is recommended to add ablation studies that disentangle the contributions of \"graph coverage\" versus \"multi-embedding fusion\" to precisely identify the source of performance improvement.\n+ Please clarify whether, when comparing against ER/ER-ACE, their original methods were fully replicated (including their random sampling strategy) or only their loss functions were borrowed. Is the comparison between their \"vanilla\" versions and MERS a fair and like-for-like comparison? This detail is crucial for assessing the fairness of the comparison.\n+ Related work and performance comparisons  could be further expanded: (a) Baseline Timeliness: The selected baselines (ER, ER-ACE, MIR) are relatively early works. It is suggested to compare against newer and stronger replay-based methods to strengthen the conclusions. (b) Related Work: The literature review and experimental comparisons could be broadened to include a more diverse range of exemplar selection strategies (e.g., GGS [1], iCaRL [2], DER++ [3], GCR [4], OCS [5], PCR [6]) to more comprehensively position this work.\n- - -\n**Reference:**   \n[1] Gradient based sample selection for online continual learning.   \n[2] iCaRL: Incremental Classifier and Representation Learning.   \n[3] Dark Experience for General Continual Learning: a Strong, Simple Baseline.   \n[4] GCR：Gradient Coreset Based Replay Buffer Selection for Continual_Learning.   \n[5] Online Coreset Selection for Rehearsal-based Continual Learning.   \n[6] PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning."}, "questions": {"value": "Please see the weaknesses. Besides, I wonder if the proposed buffer selection method can be used in online continual learning setting, since current experiments focus solely on the offline task-incremental setting. It is suggested to supplement experiments in the online continual learning scenario, where data arrives in a stream and is processed only once, to demonstrate the method's effectiveness in more challenging and practical settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2SfTVsBsc8", "forum": "7xqqptZP0x", "replyto": "7xqqptZP0x", "signatures": ["ICLR.cc/2026/Conference/Submission8884/Reviewer_Un7N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8884/Reviewer_Un7N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877563823, "cdate": 1761877563823, "tmdate": 1762920642878, "mdate": 1762920642878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper deals with catastrophic forgetting in CL. In replay-based CL, where memory constraints are severe, sample selection strategy crucially affects the performance. Memory buffers have been proposed by existing works, using embeddings learned\nunder supervised objectives. Yet this school of approaches overlooks class-agnostic, self-supervised representations that often encode rich, class-relevant semantics. The paper proposes MERS, or Multiple Embedding Replay Selection, that replaces\nthe buffer selection module with a graph-based approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The evaluation section is very clearly presented and convincing.\n+ The idea is straight-forward and not hard to follow - in fact, it is quite intuitive and it would make sense intuitively that the approach would work.\n+ The writing is clear and well organized. \n+ The mathematical notations, as far as I've checked, are correct."}, "weaknesses": {"value": "- Could the authors clarify if they are the first to propose to use class-agnostic, self-supervised representations in such tasks? The idea is not that surprising, so it is very likely that someone else had thought about it before.\n- Just a minor critique - the drawing of Figure 1 doesn't look as professional as the later figures, particularly in the evaluation section.\n- I'm confused about the Section 3 and 4. Section 3 is about MERS, and Section 4 is about methodology? Shouldn't the methodology be the proposed method itself?\n- The evaluation datasets are on the easier ends when it comes to CL datasets. I wonder how the approach fares when the evaluation datasets are more complex."}, "questions": {"value": "Please see Weaknesses. Mainly, I want to know if this is the first work to propose to use class-agnostic, self-supervised representations in such tasks? The idea seems straightforward enough that others should have thought about it. If it's the case, what's the technical contribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dWrePraQKo", "forum": "7xqqptZP0x", "replyto": "7xqqptZP0x", "signatures": ["ICLR.cc/2026/Conference/Submission8884/Reviewer_vhXK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8884/Reviewer_vhXK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917749283, "cdate": 1761917749283, "tmdate": 1762920642518, "mdate": 1762920642518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel strategy for decide what samples to select for the buffer in experience replay-based methods for continual learning. In particular, the paper proposes a graph-based approach and argues that combining embeddings learned both supervised and self-supervised lead to better results, especially in a low-memory regime."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of leveraging also self-supervised embeddings for sample selection makes sense, and has not been used often in this context as far as I know. \n2. The method is evaluated in combination with three different replay-based methods and two different selection strategies for continual learning."}, "weaknesses": {"value": "1. A lot of details of the paper are unclear or confusing. Just a couple of examples:\n- eq. 1: I can't figure out how to parse the expression below the sum.  \n- l. 142, unclear what is meant with 'v_i <-> x_i'\n- l. 210, unclear what the 'memory-aware ratio' is and how k is then set\n- l. 220 & l. 227, the symbol M_C seems to be used in two different meanings. \n- l. 229: I would associate a finer subdivision with a smaller k instead of a larger one ?\n- l. 258-260: what is the difference between Epsilon_Model and Epsilon_Sup ? Idem for Epsilon_self-supervised vs. Epsilon_self ?\n- l. 282: what is B_delta^{(m)} ? Do you mean B_delta^{(m)}(x) ? \n- figure 6 is unclear: legend mentions 3 settings but figure only shows 2; x-axis mentions buffer size but captions mentions varying radius delta. \n\n2. The choice for the CIL setting is debatable. While it is often sold as \"the most challenging setting\", it's not the most realistic or relevant in practice. Experiments using other settings and on larger datasets would make the paper more convincing. \n\n3. The reported improvements are marginal. For MIR, the proposed method doesn't work much better than herding or TEAL. There's no discussion why the differences are smaller with MIR.\n\n4. Experiments are limited to small scale datasets (CIFAR100 and TinyImageNet). \n\n5. Overall, the proposed method seems to perform ok on the small-scale tested benchmarks, but the general relevance of these findings, taking into account the extra computational overhead, seems limited in practice.\n\n\nMinor comments:\n- l. 82: reference to GSS in this context is incorrect. GSS does not decide what samples to put in the buffer, but rather what samples to select from the buffer for a given iteration."}, "questions": {"value": "1. Can you discuss the computational overhead of having to train from scratch a self-supervised embedding on each task. Is it worth it ?  \n\n2. Can you motivate your choice for the class-incremental setup, the practical relevance of which is more and more questioned ? Would your method also be applicable in other settings, including domain-incremental settings or settings beyond classification (e.g. LLM or VLM) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ye0NoKytWa", "forum": "7xqqptZP0x", "replyto": "7xqqptZP0x", "signatures": ["ICLR.cc/2026/Conference/Submission8884/Reviewer_uZzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8884/Reviewer_uZzY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113194242, "cdate": 1762113194242, "tmdate": 1762920642144, "mdate": 1762920642144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}