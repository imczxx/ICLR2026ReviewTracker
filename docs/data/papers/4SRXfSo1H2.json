{"id": "4SRXfSo1H2", "number": 17076, "cdate": 1758271880004, "mdate": 1759897199809, "content": {"title": "Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals", "abstract": "While virtual try-on (VTON) systems aim to render a garment onto a target person, this paper tackles the novel task of virtual try-off (VTOFF), which addresses the inverse problem: generating standardized product images from real-world photos of clothed individuals. Unlike VTON, which must resolve diverse pose and style variations, VTOFF benefits from a consistent output format, typically a flat, lay-down style, making it a promising tool for data generation and dataset enhancement. However, existing VTOFF approaches face two major limitations: (i) they are fundamentally constrained by their exclusive reliance on ambiguous visual information from the source image, and (ii) they frequently produce images with severely degraded details, preventing their use in practical applications. To overcome these challenges, we present Text-Enhanced MUlti-category Virtual Try-Off (TEMU-VTOFF), a novel architecture featuring a dual DiT-based backbone. To resolve visual ambiguity, our model leverages a modified multimodal attention mechanism that incorporates information from images, text, and masks, enabling robust feature extraction in a multi-category setting. To explicitly mitigate detail degradation, we propose an additional alignment module that refines the generated visual details to achieve high fidelity. Experiments on the VITON-HD and Dress Code datasets show that TEMU-VTOFF sets a new state-of-the-art on the VTOFF task, significantly improving both visual quality and fidelity to the target garments. Our code and models will be made publicly available.", "tldr": "We propose a new virtual try-off method for multi-category garments", "keywords": ["Virtual Try-Off", "Fashion", "Generation", "Diffusion Transformer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eef206e82d5aeff83bcc19c758fb2c4e5bbb8391.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the virtual try-off task, which generates product images from clothed human images. The authors propose to use both visual and text cues for a richer feature representation from the clothing. In addition, an alignment loss is introduced to refine garment textures by matching DiT features with DINOv2 features. Results on two benmarks show the proposed method ourperforms existing approches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The motivatio of introducing the alignment loss is clear.\n\n2) The paper is structured well and easy to follow.\n\n3) The authors provide many visualizations of the results."}, "weaknesses": {"value": "1) The first contribution states that the proposed method does not require category-specific pipelines. However, many similar work including MGT and Any2AnyTryon have achieved this, which makes it less of a valid contribution.\n\n2) In the model design,it is unclear why choosing the eighth Transformer block to match the DINO features. What are the justifications for this design choice? If it's empirical,  can the authors provide ablations since this is a major contribution of the paper? For example, will the generated details benefit from choosing more blocks to match DINO features? Also, since the network predicts noise rather than clean image, will the alignment loss degrades the image quality if the last few transformer blocks are used in the loss function?\n\n3) Evaluation is not sufficient. Common objective metrics like the ones reported in the tables are not very consistent with human perception, especially on evaluating texture details. The authors should provide human study results on the benchmark for a more comprehensive evaluation."}, "questions": {"value": "1) How important is the first DiT model that is trained on reconstructing human images? The authors mentioned that the features are better aligned because the two DiTs have similar architecture. However, adding noise to the input human image at early stages in the VTOFF inference seems unnecessary and could be hurtful. Will using coarse-to-fine clean feature layers in DINO with simple adaptors achieve similar effects?\n\n2) In Table 1, lower-body clothing shows significantly worse scores than the other two categories. I would like to see more analysis on the reasons behind it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QJmL0pWv7m", "forum": "4SRXfSo1H2", "replyto": "4SRXfSo1H2", "signatures": ["ICLR.cc/2026/Conference/Submission17076/Reviewer_nzHz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17076/Reviewer_nzHz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761000719319, "cdate": 1761000719319, "tmdate": 1762927084406, "mdate": 1762927084406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the inverse of virtual try-on, i.e., try-off. Given a photo of a person wearing clothes, the goal is to generate a standardized product-style image of the garment. This paper proposes TEMU-VTOFF, a dual DiT architecture with a feature extractor for the dressed person and a generator that attends to image, text, and mask tokens through a multimodal hybrid attention. A garment alignment module encourages high-frequency detail fidelity by aligning internal tokens to a frozen vision backbone. Experiments on Dress Code and VITON-HD report state-of-the-art results with ablations on text, mask, extractor features, and the alignment module."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The task definition is with practical value. Inverse try-on is useful for catalog data creation and dataset enhancement. Multi-category handling in a single pipeline is appealing.\n\n- The dual DiT setup and multimodal hybrid attention integrate signals from image, text, and mask in a straightforward and scalable way.\n\n- Solid results on Dress Code. The method shows consistent improvements on distributional and perceptual metrics, with ablations that isolate design choices."}, "weaknesses": {"value": "(1) Mixed gains on VITON-HD. On VITON-HD the improvements are minor or mixed. For example, LPIPS is **22.50** for One Model for All vs **28.44** for TEMU-VTOFF (LPIPS lower is better, so this favors the baseline), while DISTS is **19.20** vs **18.04** (lower is better, so this favors TEMU-VTOFF). This suggests the gains are not uniform across metrics or categories. A deeper per-category analysis is needed.\n\n(2) Metric suitability and tradeoffs. The ablations imply the garment alignment module can trade off visual quality and alignment. Since paired ground truth exists, full-reference metrics like SSIM and LPIPS are directly meaningful. FID measures distributional similarity and can be less diagnostic in a paired setting. Please justify metric choices, add SSIM and PSNR consistently, and explain which metrics correlate with human preference for this task.\n\n(3) Generalization and robustness. No cross-dataset experiments are reported. A simple but informative test is to train on VITON-HD and test on Dress Code, and vice versa, to evaluate robustness and domain shift.\n\n(4) Qualitative artifacts. Several provided examples contain noticeable errors, which should be acknowledged and analyzed:\n- Row 1, example 1: sleeves unexpectedly longer; skirt shows an unnatural shadow at the waist.\n- Row 1, example 4: garment material looks inconsistent with the source.\n- Row 2, example 2: button count and style differ from the source.\n- Row 2, example 3: top body length should exceed sleeve length, but the result shows a shorter top.\n- Row 2, example 4: trouser color shifts; a bow appears at the waistband that may reflect dataset bias.\n\nA failure-mode analysis with per-category statistics would help.\n\n(5) Broader utility not demonstrated. The paper motivates VTOFF as a tool for data generation, but does not show that the generated product images improve downstream tasks. A small study showing that try-off generated pairs improve try-on training or retrieval would make the case stronger.\n\n(6) Notation and clarity.\nLine 189: ùëì=8 is used but f is never defined. I guess z_t  should be denoted as  (H/f)√ó(W/f)√ó3 if the latent has spatial downsampling by \nùëì. Please fix the symbol table and all affected equations."}, "questions": {"value": "- Cross-dataset generalization. Can you report train-on-VITON-HD test-on-Dress-Code and the reverse, for both global metrics and per-category breakdowns?\n\n- Metric justification. Given paired ground truth, why prioritize FID or KID? Please add SSIM and PSNR consistently and analyze correlations with human preference. If possible, add a small human study.\n\n- Tradeoff in garment alignment. Can you quantify the quality‚Äìalignment tradeoff across a sweep of alignment strengths and report the setting used in the main results?\n\n- Downstream utility. Try using TEMU-VTOFF to synthesize product tiles from arbitrary web or in-the-wild images to create pseudo-pairs for try-on training. Does this improve a standard VTON model‚Äôs accuracy or user preference?\n\n- Caption provenance. How are text captions produced and sanitized to avoid leaking color or pattern attributes beyond the intended structure-only template?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "There is potential for IP concerns when reconstructing designer items. Please document dataset licenses, prompt and caption sources, SD3 license compliance, and any safeguards against misuse."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uXOGfw2bIW", "forum": "4SRXfSo1H2", "replyto": "4SRXfSo1H2", "signatures": ["ICLR.cc/2026/Conference/Submission17076/Reviewer_Dtrc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17076/Reviewer_Dtrc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754963601, "cdate": 1761754963601, "tmdate": 1762927083981, "mdate": 1762927083981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper introduces TEMU-VTOFF, a diffusion-based virtual try-off model that reconstructs standardized product-style garment images from photos of clothed individuals, addressing a task that is largely unexplored compared to traditional virtual try-on.\n\n- This paper uses a dual DiT architecture where one Transformer extracts garment features from the person image and the other generates the clean in-shop garment image, enhanced with multimodal attention using text and masks.\n\n- This paper employs a garment alignment module and novel supervision loss to preserve structure and fine-grained textures, achieving state-of-the-art results on VITON-HD and Dress Code."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Purpose-built architecture for try-off instead of reversing VTON pipelines, enabling clean reconstruction across multiple garment categories (upper / lower / full-body).\n\n- Multimodal hybrid attention improves disambiguation and detail preservation by combining visual features with textual descriptions.\n\n- High image fidelity and alignment thanks to the garment aligner module, resulting in superior quality and consistency compared to existing methods."}, "weaknesses": {"value": "- Your attempt to explore a new direction within the VITON domain is impressive. However, while VITON-HD uses full-body datasets, this paper uses datasets without faces. Is this because including faces would cause errors?\n\n- Would VTOFF also work on more limited imagery such as VITON-CROP [1]? Since this work deals with real-world scenarios, I recommend including [1] in the references.\n\n- It would also be helpful if the ablation study section were organized in a more intuitive manner.\n\n[1] Kang, Taewon, et al. \"Data augmentation using random image cropping for high-resolution virtual try-on (VITON-CROP).\" arXiv preprint arXiv:2111.08270 (2021)."}, "questions": {"value": "Mentioned in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m04WiAIjuy", "forum": "4SRXfSo1H2", "replyto": "4SRXfSo1H2", "signatures": ["ICLR.cc/2026/Conference/Submission17076/Reviewer_izfa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17076/Reviewer_izfa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963770214, "cdate": 1761963770214, "tmdate": 1762927083725, "mdate": 1762927083725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers multi-garment Virtual Try Off. The proposed approach leverages a dual DiT architecture based on Stable Diffusion 3, where the first network serves as a feature extractor, and the second diffuses the garment itself.\n\nThe feature extractor is trained to diffuse the latents of the model image, and takes as its input the latent $z^t$, the encoded latents of the masked model image and the binary mask of the garment. The diffusion is conditioned on the CLIP embeddings of the original model image scaled and shifted with AdaLN. \n\nThe diffusion network is conditioned on intermediate outputs of the feature extractor and CLIP and T2 textual embeddings of garment captions obtained by Qwen2.5-VL. These are combined in the proposed  Multimodal Hybrid Attention module.\n\nTo further promote detail preservation, the intermediate features of the 8th internal block of the diffusion network is aligned with DINOv2 features.\n\nTraining is done in two stages: First the feature extractor is trained. In the second stage, only the diffusion network is trained, with the values of the feature extractor for timestep 0 serving as the conditioning for all of the timesteps of the diffusion process."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "[S1] Good quantitative and qualitative results\n\n[S2] A good ablation study justifying most of the design choices.\n\n[S3] Well written and easy to follow."}, "weaknesses": {"value": "[W1] A3 section of the appendix suggests that the garment captions are based on textual descriptions of the e-commerce garment image. This seems like a fundamental flaw as the original garment caption is not going to exist for samples in the wild where the ground truth is not going to be known. This presents information about test directly seeping into the inference process.\n\n[W2] Some unclear implementation details. See questions.\n\n[W3] A couple of additional ablations would be useful. e.g. Velioglu et al. (BMVC, 2025) report better results with SigLip encoding of the conditioning image. Furthermore, why is it necessary to have two different textual embedders applied to the caption and concatenated?"}, "questions": {"value": "[Q1] Which encoder is used to encode the the masked person image in the feature extractor?\n\n[Q2] What is $z_g$ in equation 7? Is the diffusion model in the latent space or the image space?\n\n[Q3] Is garment aligner used during inference?\n \n[Q4] How does this model perform on the samples not from the training dataset? How sensitive is it to the errors in the masking of the model picture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kKuWgUPmcJ", "forum": "4SRXfSo1H2", "replyto": "4SRXfSo1H2", "signatures": ["ICLR.cc/2026/Conference/Submission17076/Reviewer_mTyd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17076/Reviewer_mTyd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998992871, "cdate": 1761998992871, "tmdate": 1762927083470, "mdate": 1762927083470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}