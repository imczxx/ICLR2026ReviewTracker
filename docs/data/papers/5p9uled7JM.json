{"id": "5p9uled7JM", "number": 16078, "cdate": 1758259538403, "mdate": 1759897263127, "content": {"title": "Flow Autoencoders are Effective Protein Tokenizers", "abstract": "Protein structure tokenizers enable the creation of multimodal models of protein structure, sequence, and function. Current approaches to protein structure tokenization rely on bespoke components that are invariant to spatial symmetries, but that are challenging to optimize and scale. We present Kanzi, a flow-based tokenizer for tokenization and generation of protein structures. Kanzi consists of a diffusion autoencoder trained with a flow matching loss. We show that this approach simplifies several aspects of protein structure tokenizers: frame-based representations can be replaced with global coordinates, complex losses are replaced with a single flow matching loss, and SE(3)-invariant attention operations can be replaced with standard attention. We find that these changes stabilize the training of parameter-efficient models that outperform existing tokenizers on reconstruction metrics at a fraction of the model size and training cost. An autoregressive model trained with Kanzi outperforms similar generative models that operate over tokens, although it does not yet match the performance of state-of-the-art continuous diffusion models.", "tldr": "Flow matching tokenizers for better protein structure tokenization and generation", "keywords": ["flow tokenizers", "proteins", "generation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0781f528c0af64deb0d0223d9b2fcbcee7a86ab9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "They build a discrete VQVAE based tokenizer for protein structures, where the encoder maps 3D coordinates to discrete codes and the decoder is a DiT-based flow matching model. They additionally train an autoregressive prior over the latent codes for generation. The approach, Kanzi, simplifies aspects of prior approaches, including the use of a diffusion loss (as opposed to prior SVD-based losses), replacing frame representations with global coordinates, and using standard attention instead of SE(3) invariant architectures. Kanzi outperforms prior token-based structure generative models and is more parameter efficient."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The authors perform extensive benchmarking against prior approaches on both reconstruction and generative tasks. The results demonstrate that the model is competitive against the prior SOTAs on both tasks.\n- They perform a series of ablations on encoder variants (invariant vs not invariant), attention window sizes, and model size.\n- Simplifying the auto-encoder loss is pretty significant, as it significantly reduces the cost of training from O(L^3) or O(L^2) to O(L).\n- Using a diffusion decoder also allows you to use classifier-free guidance and the diffusion noise scales to balance the tradeoff between diversity and sample quality."}, "weaknesses": {"value": "- The idea isn't incredibly novel. The paper is a combination of many design choices (diffusion decoder, DiT rather than SE(3) invariant attention, FSQ discretization) rather than a single great idea. \n- You mentioned the cost of loss functions used in prior works. Maybe you can do an experiment measuring the iteration speeds and memory scaling of each approach? Note also that diffusion model autoencoders have a multiplicity (number of diffusion timesteps per step), which can increase the memory use / runtime of this approach. Also, despite these supposed gains, you only trained on proteins of length <256. An experiment scaling to larger systems may be worthwhile."}, "questions": {"value": "- Do you think the sliding window attention would still suffice for longer sequences?\n- While the model achieves superior reconstruction and competitive generation for token‐based methods, it still lags continuous diffusion models. What are the main causes of this gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WLU4tYg6Na", "forum": "5p9uled7JM", "replyto": "5p9uled7JM", "signatures": ["ICLR.cc/2026/Conference/Submission16078/Reviewer_BvV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16078/Reviewer_BvV7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761414191797, "cdate": 1761414191797, "tmdate": 1762926264428, "mdate": 1762926264428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Kanzi, a non–SE(3)–equivariant diffusion autoencoder–based protein structure tokenizer, trained using a single flow-matching objective on global 3D coordinates. The authors argue this architecture simplifies training relative to SE(3)-invariant tokenizers and demonstrate strong reconstruction and unconditional structure generation performance compared to recent tokenizers such as ESM3, DPLM2, FoldToken, and IST."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation to simplify protein structure tokenization by removing architectural and loss engineering complexity.\n\n- Technically solid implementation using flow-matching and FSQ-based codebooks.\n\n- Strong reconstruction quality across multiple protein benchmarks despite relatively small model size and training data.\n\n- Interesting empirical findings: non-equivariant encoders outperform equivariant ones under flow objectives, codebook utilization emerges late in training.\n\n- Introduction of rFPSD as a distributional reconstruction metric is potentially valuable."}, "weaknesses": {"value": "1. No evaluation of SE(3) stability / invariance — critically, the tokenizer may output different tokens for rotated versions of the same protein, which invalidates many downstream use cases (retrieval, clustering, homology, interpretability). This is not even measured.\n\n2. No retrieval / similarity / StructTokenBench[1]-style evaluation, even though retrieval is a core purpose of structured tokenization (cf. FoldSeek[2], ).\n\n3. No conditional generation experiments, despite stating generative capability as a core contribution; all reported results are unconditional only.\n\n4. Claims of “smaller model rivaling ESM3/DPLM2” are not apples-to-apples — those are protein sequence / multimodal models, not structure-only.\n\n5. Scope of “tokenizer quality” is too narrow — heavily focused on reconstruction, insufficient multi-dimensional evaluation (e.g. sensitivity, explainability, stability, controllability).\n\n[1] Protein Structure Tokenization: Benchmarking and New Recipe\n\n[2] Fast and accurate protein structure search with Foldseek"}, "questions": {"value": "1. Does rotating a protein structure change the tokenization output? Have you evaluated rotational consistency quantitatively?\n\n2. Why is retrieval / homology search omitted? Do Kanzi tokens perform poorly under similarity search?\n\n3. Can Kanzi support conditional generation (e.g., topology, motif, scaffold constraints)? If yes, why is it not reported?\n\n4. You do not perform residue-level local centering or relative-frame coordinate normalization (e.g., per-residue local frame / backbone-centric coordinates) — without such normalization, how are the learned tokens supposed to be interpretable or physically meaningful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i3soMTtMna", "forum": "5p9uled7JM", "replyto": "5p9uled7JM", "signatures": ["ICLR.cc/2026/Conference/Submission16078/Reviewer_zKJZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16078/Reviewer_zKJZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756785369, "cdate": 1761756785369, "tmdate": 1762926263995, "mdate": 1762926263995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Kanzi, a flow-based protein structure tokenizer that replaces SE(3)-invariant components with a simpler diffusion autoencoder trained using a flow matching loss. The approach removes the need for complex invariant losses and geometric attention while maintaining or even improving reconstruction performance compared to prior tokenizers like ESM3, DPLM2, and FoldToken. \n\nKanzi is then used to train an autoregressive model (Kanzi-AR) for structure generation, demonstrating competitive designability and diversity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **The experimental setup is solid and shows careful design choices** — e.g., diverse test datasets, detailed RMSD/TM metrics, and fair consideration of computational trade-offs. \n\n- **The writing is clear, and figures are well-designed.** The methodology section carefully explains the training and inference setup. \n\n- **Simplified yet effective formulation.** The use of a single flow matching loss instead of multiple SE(3)-invariant losses reduces training complexity and improves stability. This work is more like an altogether engineering refinement than a conceptual breakthrough. The proposed simplification aligns with recent diffusion autoencoder trends in vision. Though the novelty in biological context is limited, the \"making it simple and scalable\" is indeed a very important next step for this field."}, "weaknesses": {"value": "- **Performance is not that out-standing**: The reconstruction performance and generation performance do not look that out-standing. ESM3 seems to still be the best of all. Inference-time sampling tricks from Proteina seems to be able to boost Kanzi-AR to a next level, though unfortunately neither ESM3 or DPLM2 used this trick. This might be an unfair comparison.\n\n- **Only evaluating on reconstruction and generation, missing representation quality**: From image domain, there are some papers discussed about one point: not necessarily the best reconstruction quality leads to a better tokenizer. The representation quality also matters. e.g., see the table 1 in RAE paper (https://arxiv.org/pdf/2510.11690v1) for MAE-B and DINOv2-B. And there is a benchmark designed for structure tokenizer representation quality: from AminoAseed [1] paper. Highly suggest to also benchmark representation quality rather than reconstruction quality alone.\n\n- **Missing comparisons to other structure tokenizer benchmarks**: see questions.\n\n- **Minor typos**: There are also minor typographical issues (e.g., lines 794–797 contain “¿” and “¡” characters).\n\n\n[1] Protein Structure Tokenization: Benchmarking and New Recipe"}, "questions": {"value": "1. Compare with two more baselines: \n- Cheap [1]\n- AminoAseed [2]\n\n2. Add representation quality evaluation. For example, the benchmark from AminiAseed [2].\n\n3. Questionable use of diffusion sampling in AR models. Table 3 claims Kanzi-AR uses inference-time sampling tricks (Eqn. 5), which conceptually apply to diffusion-based samplers, not discrete autoregressive decoders. As I can understand, AR simply uses the discrete structure tokens for autoregressive training and sampling. There should not be any diffusion sampling process involved? Clarification is needed.\n\n[1] Tokenized and continuous embedding compressions of protein sequence and structure \n\n[2] Protein Structure Tokenization: Benchmarking and New Recipe"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2aNaUqGzOo", "forum": "5p9uled7JM", "replyto": "5p9uled7JM", "signatures": ["ICLR.cc/2026/Conference/Submission16078/Reviewer_xSwq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16078/Reviewer_xSwq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188716176, "cdate": 1762188716176, "tmdate": 1762926263475, "mdate": 1762926263475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a flow-based protein structure auto-encoder (\"Kanzi\") useful as a protein tokenizer. The primary contribution is that, as far as I am aware, it is the only example of flow-based structure auto-encoder. Secondary to that, but still very important, is that the encoder and decoder are not invariant or equivariant, continuing the recent trend to show that such complex models are not required to adequately model proteins. The authors additionally use the model to generate novel structures."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The authors presentation is mostly very clear, and it does a good job of discussing the previous literature. They present a novel approach to the problem of structure tokenization. I particularly encouraged to see that the output is simply the backbone atomic coordinates, which should greatly simplify the tokenizer's use. It is intriguing that the model uses real space coordinates _and_ a sequence id embedding; see questions for more on this.\n\nI like that the authors show that the resulting tokenizer can be used to generate designable structures. But see weaknesses for additional comments on this.\n\nClearly Kanzi is successful: the 30M parameter version trained with \"under optimized\" hyperparameters is best or second best across most metrics, covering the core set of structure prediction data, while being _much_ smaller. \n\nThe discussion of ablations is especially useful, although I do wish it were more detailed."}, "weaknesses": {"value": "This is a small weakness but the authors seem to flip between using \"diffusion\" and \"flow matching\" to describe their approach. It seems that flow matching better matches what they're doing--either that or there is something that needs to be clarified substantially in their writing. In any case diffusion and flow matching are not exactly the same thing, so the authors should be precise.\n\nThere are two significant weaknesses in the paper that I'd like to see addressed:\n1. The authors should examine how their tokenizer performs at other downstream tasks that are relevant for protein language models. Many are detailed in a recent paper from ICML 2025: Xinyu Yuan et al., Protein structure tokenization: Benchmarking and new recipe.\n\n2. While the authors show that their auto-regressive structure generator is capable of generating designable structures using the conventional definition of generating a sequence which folds back to the same structure, this is highly dependent on the particular folding model used. The authors chose ESMFold, which I found curious given they train on a sample of AFDB. The authors should comment on this choice given that better models are available."}, "questions": {"value": "The authors might want to cite Ellmen, ... Deane \"Transformers trained on proteins can learn to attend to Euclidean distance\", which includes an interesting discussion of how attention mechanisms process real space coordinates."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T0mJK5HcHb", "forum": "5p9uled7JM", "replyto": "5p9uled7JM", "signatures": ["ICLR.cc/2026/Conference/Submission16078/Reviewer_6hfk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16078/Reviewer_6hfk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762193587220, "cdate": 1762193587220, "tmdate": 1762926262867, "mdate": 1762926262867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}