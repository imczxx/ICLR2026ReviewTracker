{"id": "O6n4dgTBBL", "number": 23045, "cdate": 1758338762437, "mdate": 1759896834648, "content": {"title": "Stabilizing Gradient Descent via Second-Order Control-Theoretic Dynamics", "abstract": "In this paper, we establish a fundamental connection between the stability of gradient descent dynamics and the curvature of the underlying loss landscape from a continuous-time perspective. We show that the sign of the real parts of the Hessian’s eigenvalues directly governs the convergence behavior of gradient-based optimization. Through analytically tractable, low-dimensional toy examples, we\ndemonstrate that gradient descent can diverge even in simple convex settings. To address this issue, we formulate gradient descent as a second-order dynamical system and introduce a controller that guarantees locally asymptotic stability by regulating the system’s eigen-structure. Notably, we show that the proposed controller admits a variational interpretation and can be realized as a gradient guidance term\naugmenting the original gradient. Empirical results on numerical examples with various curvatures and learning rate validate our theoretical findings and demonstrate the proposed method improves both stability and convergence behaviors.", "tldr": "", "keywords": ["Gradient Descent", "Control Theory", "Stability Analysis"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f673b089817f1769608b51e1d48f39c7f02625d5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "See my concerns below."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I believe using second-order ODEs to model GD is novel, but I am not sure."}, "weaknesses": {"value": "### **Major flaw: the ODE model contradicts established discrete-time theory**\n\nThe paper explicitly claims that **gradient descent (GD) can be unstable even when** $\\eta < \\frac{2}{L}$, **unless the loss is strongly convex** (see Theorem 2 and Table 1), classifying the *convex-but-not-strongly-convex* case as “unstable.”  **This is in full contrast with the very well-established convergence analysis of GD**.\n \nThis conclusion is obtained by linearizing the **second-order augmentation** of gradient flow, $\\ddot{\\theta} = -\\nabla^2 L(\\theta)\\dot{\\theta}$, which is recast as a first-order system in $(\\theta,\\dot{\\theta})$. The reported “instability” arises from **spurious insights/artifacts** introduced by treating $\\dot{\\theta}$ as an independent state variable, not from the optimizer itself.\n\nIf a continuous-time *model* yields conclusions that **contradict well-established discrete-time theory**, then the model, or the way it is treated, is at fault, not gradient descent.\n\nTherefore, the paper should **retract** the “unstable for convex (not strongly convex)” entry in Table 1 and restate Theorem 2.\n\n---\n\n### **Incorrect discrete-time correspondence**\n\nThe bridge back to a practical update (Eq. (5) and Algorithm 1) is also **mathematically incorrect**.  \nIntegrating the controlled ODE\n\n\\begin{equation}\n\\ddot{\\theta} = -(H + K_2)\\dot{\\theta} - K_1\\theta\n\\end{equation}\nto obtain\n\\begin{equation}\n\\dot{\\theta}' = \\dot{\\theta} - \\tfrac{1}{2} K_1\\theta^{\\odot 2} - K_2\\theta\n\\end{equation}\nreplaces $\\int \\theta dt$ with $\\tfrac{1}{2}\\theta^{\\odot 2}$, which has **no mathematical basis** in this context (that identity would correspond to integrating $\\theta\\dot{\\theta}$, not $\\theta $).  \n\nConsequently, the discrete update proposed in Algorithm 1 is **not** a consistent discretization of the controlled dynamics.\n\n### Conclusion\n\n**This paper derives theoretical results in full contradiction to well-established convergence theorems from the literature. It cannot be accepted under any circumstances.**"}, "questions": {"value": "1. Have you nonetheless tried to validate the performance of your approach on modern DL experiments?\n\n2. Have you noticed that your Algorithm 1 closely resembles what happens if you apply GD to a loss $L$ which has been regularized? It looks to me like it is equivalent to applying GD to\n \\begin{equation}\n\\tilde{L}(\\theta) = L(\\theta) + \\frac{K_2}{2} \\|\\|\\theta\\|\\|_2^2 + \\frac{K_1}{3} \\|\\|\\theta\\|\\|_3^3,\n\\end{equation}\nbut maybe I am wrong.\n\n3. Have you considered including a proper literature review of the papers using ODEs and SDEs to model optimizers? The paper overlooks a substantial body of prior work analyzing optimization algorithms through continuous-time formulations (ODEs and SDEs). This literature provides the theoretical groundwork for interpreting optimizers as dynamical systems and should be discussed to position the present contribution more clearly.  I added some references below, also encompassing ODEs: I suggest the authors carry out a thorough review of these papers and look for more recent ones as well. For an accessible entry point, I recommend the Related Works and Appendix A of [1], which offer a representative overview of (tens and tens of) works using continuous-time models for optimizations. Although that reference focuses mainly on SDEs, many of the works it cites also include ODE analyses directly relevant to this discussion. The absence of this contextualization makes the paper appear somewhat disconnected from its theoretical lineage.\n\n\n**[1]** *Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise.*  \nEnea Monzio Compagnoni, Tianlin Liu, Rustem Islamov, Frank Norbert Proske, Antonio Orvieto, Aurélien Lucchi.  \n*International Conference on Learning Representations (ICLR), 2025.*\n\n---\n**Relevant prior work on ODE/SDE analyses of optimization algorithms**\n\n1. **Helmke, U. & Moore, J. B. (1994).**  \n   *Optimization and Dynamical Systems.* Springer London.  \n   — Classical textbook connecting continuous-time dynamical systems and optimization via gradient flows.\n\n2. **Su, W., Boyd, S., & Candès, E. (2014).**  \n   *A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights.*  \n   *Advances in Neural Information Processing Systems.*  \n   — Foundational ODE model for Nesterov acceleration; initiated the modern line of continuous-time analyses.\n\n3. **Li, Q., Tai, C., & Weinan E. (2017).**  \n   *Stochastic modified equations and adaptive stochastic gradient algorithms.*  \n   *International Conference on Machine Learning (ICML).*  \n   — Derives stochastic modified equations for stochastic gradient algorithms, laying the foundation for weak ODE/SDE approximations.\n\n4. **Li, Q., Tai, C., & Weinan E. (2019).**  \n   *Stochastic modified equations and dynamics of stochastic gradient algorithms I: Mathematical foundations.*  \n   *Journal of Machine Learning Research, 20(1): 1474–1520.*  \n   — Provides a rigorous mathematical foundation for the weak SDE approximations of SGD and related methods.\n\n5. **Orvieto, A. & Lucchi, A. (2019).**  \n   *Continuous-time models for stochastic optimization algorithms.*  \n   *Advances in Neural Information Processing Systems 32.*  \n   — Introduces a general ODE/SDE formalism for analyzing SGD, momentum, and adaptive algorithms, establishing the link between discrete-time optimizers and their continuous-time limits."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VD4HNm9moa", "forum": "O6n4dgTBBL", "replyto": "O6n4dgTBBL", "signatures": ["ICLR.cc/2026/Conference/Submission23045/Reviewer_eByn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23045/Reviewer_eByn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760540257354, "cdate": 1760540257354, "tmdate": 1762942489476, "mdate": 1762942489476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work claims to provide new stability analysis of gradient descent, in particular conditions for stability / unstability. It also suggest algorithmic improvements based on the analysis."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The analysis of GD through the second-order ODE seems not very common and possibly interesting."}, "weaknesses": {"value": "### Main issue\nI do not understand the general analysis. Lyapunov stability analysis of the gradient flow has been extremely well studied and I do not see what is the point of applying this analysis to the second order system. It does not seem that this yields more information than the classical analysis.\n\n\n### \"Convex but not strongly convex\" and \"concave\" cases are wrong\n\nThm. 1 only provides sufficient condition for stability but they are used as necessary conditions in 4.2.2 and 4.2.3.\n\n\n\n### On the proposed method\n\n- The proposed method merely constists in adding quadratic and cubic regularization. This should be made clear and adequate references (cubic regularization, weakly convex optimization, etc) should be provided.\n- It is far from being clear how to choose K_1, K_2 in practice for neural networks: Remark 2 is not helpful in that case as it requires a global lower bound on the Hessian. If they are not set according to the controller design (l417), then this is really just regularization.\n\n### Minor\n- Lemma 1-3 do not really require proof nor that much empahasis, they are standard.\n- l407: the function is said to be \"convex but not strongly convex\" but it is actually strongly convex."}, "questions": {"value": "Address \"Main issues\", \"\"Convex but not strongly convex\" and \"concave\" cases are wrong\", and \"On the proposed method\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6R7Fff4t7j", "forum": "O6n4dgTBBL", "replyto": "O6n4dgTBBL", "signatures": ["ICLR.cc/2026/Conference/Submission23045/Reviewer_7cLT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23045/Reviewer_7cLT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662370172, "cdate": 1761662370172, "tmdate": 1762942489017, "mdate": 1762942489017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper formulates gradient descent as a second-order dynamical system and introduces a control-theoretic modification intended to improve stability under various curvature conditions. The authors provide continuous-time stability analysis and propose a “controlled gradient descent” update, supported by toy-level numerical experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The idea of viewing GD stability through a control-theoretic lens is conceptually interesting.  \n2. The theoretical derivations are clearly written and technically correct under the continuous-time setting."}, "weaknesses": {"value": "1. The theoretical results largely rest on direct applications of standard stability analysis tools, such as local linearization, Hessian eigenvalue conditions, and known quadratic eigenvalue problem results. The contribution appears incremental and does not provide new theoretical insights for optimization or ML practice. \n\n2. All empirical studies are conducted on very simple synthetic low-dimensional functions (e.g., 2D convex/quartic). These examples are insufficient to justify that the proposed method is useful or relevant for general optimization problem. \n\n3. The paper claims to provide a “Controlled Gradient Descent for Neural Network Training,” yet does not include any experiments on neural networks."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mk8by8nPeZ", "forum": "O6n4dgTBBL", "replyto": "O6n4dgTBBL", "signatures": ["ICLR.cc/2026/Conference/Submission23045/Reviewer_V2Hd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23045/Reviewer_V2Hd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676854403, "cdate": 1761676854403, "tmdate": 1762942488797, "mdate": 1762942488797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}