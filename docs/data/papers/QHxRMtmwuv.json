{"id": "QHxRMtmwuv", "number": 1171, "cdate": 1756857478095, "mdate": 1762944559743, "content": {"title": "MAPO: MIXED ADVANTAGE POLICY OPTIMIZATION", "abstract": "Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation  across different query samples. In this work, we propose an easy but effective GRPO strategy, **M**ixed **A**dvantage **P**olicy **O**ptimization (**MAPO**). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.", "tldr": "We propose the mixed advantage strategy for GRPO to deal with advantage reversion and advantage mirror problems.", "keywords": ["Group Relative Policy Optimization", "Advantage Reversion", "Advantage Mirror"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9747f2b3785bd7b1ff57b94e18d5d98ec24bd0df.pdf", "supplementary_material": "/attachment/1cc937c532bf50c2550fec0a82a72c65c0cb7584.zip"}, "replies": [{"content": {"summary": {"value": "The present work argues that the GRPO advantage estimator can suffer from two problems: (1) advantage reversion, where small standard deviation in the group rewards can inflate the advantages, and (2) advantage mirror, where the advantages are invariant to shifting. The authors then proposed Mixed Advantage Policy Optimization (MAPO) that combines a different advantage estimator that normalizes the advantages based on the mean, and a reweighting scheme that dynamically chooses the estimator based on the certainty of the group rewards. Experiments show marginal improvements over GRPO, suggesting its usefulness in RL reasoning tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea of incorporating uncertainty into the policy optimization objective is an interesting idea, and the empirical results seem to suggest that such interventions may help GRPO-style training."}, "weaknesses": {"value": "One of the major weaknesses of the paper is that the main claims (i.e., advantage reversion/mirror are problematic in GRPO training) are neither supported by theoretical justification nor empirical evidence beyond the intuitions given in the introduction. This makes it difficult to assess the effectiveness of the proposed fixes aside from the marginal gains demonstrated in the ablation studies. Furthermore, recent studies [1] have highlighted the importance of reporting uncertainties for assessing statistical significance, especially in the case of RL with large models. I believe this is particularly relevant for this work considering the marginal gains reported.\n\nIn addition, the paper is poorly organized (e.g., why is the discussion (sec 3.3) placed before the experiments rather than after?) The figures are also unclear (e.g., in Figure 4 (right) the axes labels are missing). It is also not clear to me why the experiments in sec 3.3 and sec 4 are using different baselines.\n\n[1] Hochlehnert et al., \"A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility\", COLM 2025"}, "questions": {"value": "1. Can the authors provide theoretical arguments on why the proposed modification is effective (e.g., convergence, bias-variance tradeoff)? \n2. Why are the methods presented in Figure 4 not used in the experiments in Section 4?\n3. In Table 2, what do bold and underline mean? In Table 3, why not use bold and underline for every task?\n4. Can the authors explain why DAPO failed for EmoSet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QybGxRlq3k", "forum": "QHxRMtmwuv", "replyto": "QHxRMtmwuv", "signatures": ["ICLR.cc/2026/Conference/Submission1171/Reviewer_GUpS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1171/Reviewer_GUpS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761297911952, "cdate": 1761297911952, "tmdate": 1762915696817, "mdate": 1762915696817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "kIgLMCyR3H", "forum": "QHxRMtmwuv", "replyto": "QHxRMtmwuv", "signatures": ["ICLR.cc/2026/Conference/Submission1171/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1171/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762944558740, "cdate": 1762944558740, "tmdate": 1762944558740, "mdate": 1762944558740, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a real stability issue with a neat two-term advantage and a smooth certainty gate, leading to reliable (but small) gains at negligible cost. However, the exclusive use of Qwen2.5-VL as the backbone (amid contamination concerns in math) and the fixed, non-learned adaptivity limit the conclusiveness and novelty. Adding leakage-resistant evals and diversified backbones, plus exploring learned/alternative gates, would substantially strengthen the paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper cleanly isolates a real issue in GRPO-style training: z-score advantages behave poorly across groups with different trajectory “certainty.” The proposed two-term scheme (APD + certainty-weighted mixing) is easy to implement and numerically stable.\n- Across math (MathVista/MathVision/MathVerse) and emotion (WEBEmo/Emotion6) OOD sets, the method delivers steady improvements over Vanilla/GRPO/DAPO, with ablations that justify using both components (APD and the certainty-based mixing).\n- The method reuses the same rollout recipe; it only changes group-wise statistics and mixes two advantage measures. Wall-clock/compute should be essentially unchanged relative to GRPO."}, "weaknesses": {"value": "- All results are on **Qwen2.5-VL-7B-Instruct**. This family has been audited for contamination on math benchmarks (https://arxiv.org/pdf/2507.10532); even though the paper evaluates on multimodal math (not exactly the same test suites), relying on a single possibly contaminated backbone weakens the claim of general gains. The paper would be stronger with additional backbones that are not implicated in leakage.\n- The adaptivity is a fixed analytic gate $\\lambda(p)$ rather than a learned/conditional schedule. Reported improvements are modest (often ~0.5–1.4 points). The paper does not test alternative certainty proxies or learned mixers, leaving open whether the fixed curve is near-optimal.\n- No experiments on other strong open backbones (e.g., Llama-3.*-Vision, Gemma-3-Vision, InternVL-3.5). The emotion scenario also uses a single training set before OOD testing; broader coverage would improve external validity."}, "questions": {"value": "1. Can you reproduce the main tables on at least two additional backbones (e.g., Llama-3.1-Vision, Gemma-3-Vision, InternVL-3.5) to decouple MAPO’s gains from Qwen-specific artifacts?\n2. Will you include a math suite explicitly curated to avoid contamination (or a de-duplicated variant) to demonstrate robustness of the claimed improvements?\n3. What is the rationale for fixing $\\lambda(p)=1-4\\,p(1-p)$ instead of learning $\\lambda$ from $\\{p,\\mu,\\sigma\\}$ or using other certainty proxies (e.g., bootstrap CIs, reward-histogram entropy)? Any preliminary results?\n4. Please provide sensitivity curves for the number of rollouts per prompt ($(G\\in\\{4,8,12,16\\}$) and for each reward channel (accuracy vs. format vs. mixed).\n5. Please confirm training steps, batch sizes, rollouts, and context lengths are matched across MAPO/GRPO/DAPO, and report any wall-clock overhead (expected to be negligible).\n6. What exact rules are applied when the group mean in APD is near zero, or when $p$ is extreme? Do you clip APD or $\\lambda$, or back off to a single advantage? Please include the code-level conditions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lJBE0JDX4n", "forum": "QHxRMtmwuv", "replyto": "QHxRMtmwuv", "signatures": ["ICLR.cc/2026/Conference/Submission1171/Reviewer_DCJ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1171/Reviewer_DCJ6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985950625, "cdate": 1761985950625, "tmdate": 1762915696703, "mdate": 1762915696703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the context of RL for foundation models, the GRPO advantage function encounters both advantage reversion and advantage mirror problems which hinder allocation across different query samples (i.e., because the advantage is fixed throughout training). These issues motivate the authors to ask:\n\n1. how to design the advantage function for high-certainty samples?\n    \n2. how to adaptively combine advantage functions for samples with varying trajectory certainty?\n    \nTo answer these questions the authors correspondingly propose (1.) the Advantage Percent Deviation (APD) which essentially replaces the standard GRPO advantage $\\hat{A}_i=\\frac{r_i - \\mu}{\\sigma}$ with a mean-relative normalisation s.t. $\\hat{A}^{\\text{APD}}_i=\\frac{r_i - \\mu}{\\mu}$, and (2.) Trajectory Certainty Reweight (TCR) which determines the sample advantage function based on Bernoilli modelling, dynamically weighting the advantage between $\\hat{A}_i$ and $\\hat{A}^{\\text{APD}}_i$. The Mixed Advantage Policy Optimization (MAPO) method applies APD and TCR to adaptively adjust the advantage function during training based on trajectory certainty.\n\nMAPO is evaluated using both mathematics (Geo3K, MathVista, MathVision, MathVerse) and emotion(EmoSet, WEBEmo, Emotion6) datasets, the Qwen2.5-VL-7B-Instruct base model, and GRPO and DAPO baselines for comparison. Ablation is used to evaluate $\\hat{A}^{\\text{APD}}_i$ in contrast to  $\\hat{A}_i$, and an “out-of-domain validation” is also performed. Within domain, in Geo3K and EmoSet, MAPO provides enhanced scores over the baselines. Out-of-domain (Math-Vista,Vision,Verse and Emotion6) the results are more ambiguous."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I find the motivation and design of MAPO to be of interest, and potentially quite significant, if the analysis and evaluation can be substantially improved to support the claims made in the paper. The authors identify and address advantage reversion and advantage mirror problems that may arise given the standard advantage formation of GRPO. They address these challenges with an adaptive weighted advantage structure based on trajectory certainty and show some results that support improved scores in mathematics and emotional tasks."}, "weaknesses": {"value": "Broadly speaking the quality of writing, and particularly grammar, in this paper is below the standard I would expect for ICLR. The Tables and Figures also have room for improvement e.g., Figure 4 and 5 lack axis labels, poorly legible legend in Figure 5, underline used in EMotion6 column of Table 3\n\nIn terms of technical content, whilst I find the motivation and the design of MAPO to be of-interest and potentially significant; and I think the ablation result is a step in the right direction, I find both the evaluation and discussion of the findings to fall short of the ICLR standard. The following important aspects are seemingly unaddressed and prevent me from understanding whether MAPO is functioning according to its design principles –\n\n- In Figure 5, MAPO shows relatively convincing training accuracy improvements but this doesn’t seem to translate to testing accuracy.\n    \n- MAPO is outperformed by GRPO and DAPO in certain “out-of-domain” tasks e.g., MathVista and MathVerse.\n    \n- The above seems correlated on the rollout size G which MAPO depends upon for uncertainty estimation. It is unclear how performance and stability are impacted by G.\n    \n- It remains unclear how efficient and numerically stable MAPO is.\n    \n- No empirical consideration is given to the claim TCR “ensures that sample-specific characteristics are preserved, leading to a more faithful and stable evaluation of trajectory quality.” e.g., does certainty-based weighting moderate advantage in the way we hope?\n    \n- To the best of my knowledge there is no variance reporting in the evaluation process, nor any indication that more than one run was done. Tt’s not clear whether the reported improvements are statistically significant. It would be great if the authors could shed some light on this.\n    \n\nWriting lacking clarity or questionable (not exhaustive):\n\n- “RL serves as the key mechanism for unlocking the reasoning ability in various domains.“ – I think this is overstating the current evidence.\n    \n- I think the introduction dives into advantage reversion and mirroring without sufficiently contextualising the role of the advantage function in GRPO. A few extra sentences would significantly improve the accessibility of the paper introduction. e.g. approximately covering: GRPO samples a group of trajectories, each trajectory receives a reward, the advantage function determines how much reward each trajectory should be assigned.\n    \n- Line 137, “reasoning models are now widely deployed locally, drawing attention from the research community to the efficiency of long chain-of-thought generation for foundation models.” ([pdf](zotero://open-pdf/library/items/YASLZRUI?page=3)) This is non-sequitur/should be clarified.\n    \n- Line 138, “. And utilizing the reinforcement technique to empower foundation models”. Sentences shouldn’t begin with ‘And’.\n    \n- Make sure acronyms are used once they have been defined e.g., line 150-151 “Multimodal Large Language Model” ([pdf](zotero://open-pdf/library/items/YASLZRUI?page=3))\n    \n- The legends in Figure 5 are poorly legible - it took my longer than usual to understand which line was MAPO etc.\n    \n- It’s unclear where the data for Figure 1 comes from? How can it be reproduced?\n    \n\nMinor/typos/grammar:\n\n- Line 116, LLM should be plural, as should MLLM line 123"}, "questions": {"value": "- What is your intuition or principled understanding of why MAPO shows relatively convincing training accuracy improvements but this doesn’t seem to translate as significantly to testing accuracy?\n    \n- Why do you think MAPO is outperformed by GRPO and DAPO in certain “out-of-domain” tasks e.g., MathVista and MathVerse?\n    \n- The above seems correlated on the rollout size G which MAPO depends upon for uncertainty estimation. How might performance and stability be impacted by G?\n    \n- How efficient and numerically stable is MAPO?\n    \n- You claim that TCR “ensures that sample-specific characteristics are preserved, leading to a more faithful and stable evaluation of trajectory quality.” Can you determine whether certainty-based weighting moderates advantage in the way you hope?\n    \n- Can you provide details regarding the statistical significance of your results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LgUqAoXlHu", "forum": "QHxRMtmwuv", "replyto": "QHxRMtmwuv", "signatures": ["ICLR.cc/2026/Conference/Submission1171/Reviewer_C8s4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1171/Reviewer_C8s4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008025558, "cdate": 1762008025558, "tmdate": 1762915696510, "mdate": 1762915696510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the advantage function used in GRPO for reasoning LLMs and identifies two failure modes—“advantage reversion” and “advantage mirror”—that occur on high-certainty samples. To address them, it proposes Mixed Advantage Policy Optimization (MAPO). The mixed advantage improves stability across certainty regimes and yields consistent, albeit modest, gains over GRPO and DAPO on math and emotion benchmarks with Qwen2.5-VL-7B under different rollout counts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed definition of trajectory certainty is reasonable (though there is prior art), and the resulting mixed advantage is simple, easy to implement, and practical to apply.\n\nThe paper is clearly written, with readable figures and tables."}, "weaknesses": {"value": "1. The paper partitions sample certainty at the trajectory level and reweights accordingly. The authors should discuss related work on trajectory-level certainty/entropy-driven reweighting and sampling:\n- [1] Vanlioglu (2025) uses entropy-guided sequence weighting to schedule exploration/exploitation in RL-based LLM fine-tuning;\n- [2] Liu et al. (AAMAS 2024) present a trajectory-level perspective that systematically analyzes how data sampling techniques affect policy learning in RL.\n\nThese are highly aligned in motivation with the paper’s ‘Trajectory Certainty Reweight’. Besides, [2] also discusses using trajectory quality as a metric; in comparison, does Trajectory Certainty Reweight offer stronger advantages?”\n\n2. The set of baselines appears limited. Compared with GRPO variants such as Dr.GRPO/GPG/TreeRPO, the performance advantage seems not particularly pronounced.\n\n3. The experiments are further limited by the lack of statistical significance analysis.\n\n4. There are quite a few typos and minor presentation issues; the authors should carefully proofread. For example, the subfigure title in Figure 4 reads “Performance durning Training Step,” which should be “during,” etc. Also, the manuscript use the wrong citation format, maybe caused by misusing \\citet and \\citep.\n\n\n【1】Vanlioglu, Abdullah. \"Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning.\" arXiv preprint arXiv:2503.22456 (2025). \n【2】Liu, J., ... , 2024, May. A trajectory perspective on the role of data sampling techniques in offline reinforcement learning. In Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems (pp. 1229-1237)."}, "questions": {"value": "Does λ(p) jitter, and is smoothing necessary?\n\nIs the method broadly applicable across models of different parameter scales?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nPLVAtSjh8", "forum": "QHxRMtmwuv", "replyto": "QHxRMtmwuv", "signatures": ["ICLR.cc/2026/Conference/Submission1171/Reviewer_dE5m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1171/Reviewer_dE5m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009846046, "cdate": 1762009846046, "tmdate": 1762915696389, "mdate": 1762915696389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}