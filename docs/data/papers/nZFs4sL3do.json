{"id": "nZFs4sL3do", "number": 2251, "cdate": 1757041535262, "mdate": 1762924488587, "content": {"title": "HalCap-Bench: Benchmarking Hallucination Detector in Image Captioning", "abstract": "Recent progress in large vision-language models (VLMs) has been driven by advances in image-text alignment, i.e., learning the relationship between image and text. Hallucination detection in captions, \\textbf{HalDec}, can assess VLM's image-text alignment ability, and aims to identify errors in VLM-generated captions that misrepresent image content. Detecting these errors is crucial not only for evaluating alignment ability but also for curating high-quality image-caption pairs used to train VLMs. \nWhile VLMs have been explored as hallucination detectors, their generalizability across different captioning models, image domains, and hallucination types remains unclear due to a lack of a benchmark. In this work, we present HalDec-Bench, the first benchmark for principled and interpretable evaluation of HalDec models. It covers diverse VLMs used as captioning models, image domains, and provides high-quality hallucination-existence annotations enriched with hallucination-type labels. HalDec-Bench thus serves as a comprehensive testbed to advance HalDec and probe the image-text alignment ability of VLMs. Our analysis shows that HalDec-Bench offers tasks of varying difficulty, making it well-suited\nas a HalDec benchmark. Evaluating diverse VLMs reveals key limitations: (i) CLIP-like models are nearly blind to hallucinations in recent VLMs, (ii) detectors tend to over-score early sentences, and (iii) they display strong self-preference—favoring their own captions—which undermines detection performance. We will release our evaluation code and dataset upon acceptance.", "tldr": "New benchmark to evaluate hallucination detector in image captioning", "keywords": ["vision-language model", "hallucination", "image captioning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/71af79f794a0761e06e6c998ae2d8fcb033560d3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new benchmark dataset for assessing image caption hallucination detectors. The authors construct a large set of image-caption pairs by using diverse models, including image-to-text models and text-to-image models. Then, human annotators review the sentence-wise alignment between images and captions. Through experiments, the authors provide the following three observations as their main contributions: 1) CLIP-based models are ineffective at detecting hallucinations within image captions; 2) VLMs tend to regard sentences near the start of an image caption as factual; 3) VLMs strongly prefer self-generated image captions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is clear and well-organized overall.\n2. I'd like to thank the authors for their labor-intensive, human-involved benchmark construction."}, "weaknesses": {"value": "1. Existing works have already demonstrated that CLIP shows limitations in testing alignment between images and captions [1,2]\n2. According to several recent studies [3,4], image captions generated by VLMs are likely to contain factual information in their former part, and the possibility of hallucination goes up as models generate longer. Therefore, Figure 5 can be regarded as a result in support of the previous finding, not the one caused by the bias of VLMs.\n\n[1] Yuksekgonul et al., \"When and why vision-language models behave like bags-of-words, and what to do about it?\" ICLR 2023  \n[2] Lin et al., \"Evaluating Text-to-Visual Generation with Image-to-Text Generation\" ECCV 2024  \n[3] Zhou et al., \"Analyzing and mitigating object hallucination in large vision-language models\" ICLR 2024  \n[4] Lee et al., \"Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage\" ICML 2025"}, "questions": {"value": "Could you provide a summary of this paper’s findings that are not yet known to the community?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tEvmAPG7c9", "forum": "nZFs4sL3do", "replyto": "nZFs4sL3do", "signatures": ["ICLR.cc/2026/Conference/Submission2251/Reviewer_JAZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2251/Reviewer_JAZP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761373131690, "cdate": 1761373131690, "tmdate": 1762916162717, "mdate": 1762916162717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "r9c0t34BFL", "forum": "nZFs4sL3do", "replyto": "nZFs4sL3do", "signatures": ["ICLR.cc/2026/Conference/Submission2251/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2251/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762924487611, "cdate": 1762924487611, "tmdate": 1762924487611, "mdate": 1762924487611, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a benchmark for detecting hallucinations in image captioning. The benchmark is built from captions generated by different vision–language models and includes human annotation efforts to label the correctness of each sentence in the captions with respect to the corresponding images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A comprehensive study that includes both **natural image–text pairs** and **AI-generated image–text pairs**.  \n- Incorporation of **crowdsourced human annotations**, adding reliability to the benchmark."}, "weaknesses": {"value": "- Limited analysis on text-to-image samples:  Although the benchmark includes samples from text-to-image models, the analysis is limited. It would strengthen the work to categorize hallucination types within these generations and discuss how they differ from natural image–text cases.\n\n- Undefined task difficulty:  The paper claims to design tasks of varying difficulties but does not define or quantify difficulty levels, which weakens the interpretability of results.\n\n- Human agreement analysis missing:  Since the crowdsourcing annotation relies on majority voting from five annotators, it would be useful to analyze model performance under varying levels of human agreement (e.g., strong vs. weak consensus).\n\n\n- Inconsistent results:  Some reported results are questionable. For instance, in Table 2 (line 276), the average AUROC for SigLIP is reported below 50, while all individual AUROC values across models exceed 50. This inconsistency should be clarified.\n\n\n- Lack of visualization for correctness score distributions:  The paper could visualize the correctness score distributions (for a single detector across different caption sources) to better understand potential distribution shifts. Besides, the scores are evaluated only for sentences from one captioner. Reporting AUROC results across correct/incorrect samples from different models would clarify this effect. \n\n\n- Limited model diversity:  The closed source image-to-caption models currently include only GPT-4o. It would be beneficial to include more closed-source models such as Gemini 2.0 Flash, which has been used as a detector but not as a generator."}, "questions": {"value": "Please see **Weaknesses**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tp6bZ7SW1i", "forum": "nZFs4sL3do", "replyto": "nZFs4sL3do", "signatures": ["ICLR.cc/2026/Conference/Submission2251/Reviewer_7UvM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2251/Reviewer_7UvM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916400019, "cdate": 1761916400019, "tmdate": 1762916162491, "mdate": 1762916162491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a major data set for hallucination detection for (present) state of the vision language models. The set up is that 2k images are collected from CC12M and COCO val and captioned by in total six captioning models of diverse sises and strength. The resulting 12k captions are augmented with 2k generated images that stem from image generators operating on pre-produced captions. The images are grouped in several topical clusters. The obtained data sets of image - caption pairs are then annotated for containing hallucinations. Labeling is done by five fold crowd sourced human annotation plus review, guaranteeing high quality labels. Also 9 different annotation types are assigned. Hallucination is measured on sentence level.\nThe performance 13 open source and 6 closed source models is measured on the HalCap benchmark and subsequently analyzed. The models are prompted to score the image-text alignment. The AUROC from this score serves as metric. Among other analyses, the detection performance depends on  position of the sentence,  whether it is an detection effort from the same model as the one providing the captions. \nThe benchmark is compared to previous data sets for hallucination detection and stands out in size and difficulty."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This is a really thoroughly composed data set that exceeds prior benchmarks in size and level of difficulty\n- The human, crowd sourced labeling of correct vs incorrect captions is carried through with considerable quality checks \n- The various annotations enable numerous detail studies\n- The paper is very well prepared and graphics are instuctive\n- An extensive appendix provides numerous more details"}, "weaknesses": {"value": "- The design of HalDec works with the present state of the art captioners, but as time will move on, detecting these hallucinations will be less relevant - so I expect the results to age quickly.\n- The analysis of the position of the sentence is instructive, but here one would expect an investigation of the effect of reasoning. The effect might be due to short contexts at the start of the decoding from the detector.\n- I might have overlooked this, but is there an analysis of the performance on the synthetically generated images vs the natural images? If not, I find his to be missing.\n- The HalDec benchmark does not provide reproducible internal states of the captioning models and therefore does not enable self-monitoring methods for hallucination detection during decoding"}, "questions": {"value": "- is it planned to provide a project page with leaderboard to support the community?\n- Could you detail a little more the clustering procedure\n- What is the meaning of the last sentence in the caption of figure 5?\n- What is the conceptual difference of the right panel in figure 6 to figure 5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JIl3WpiLtT", "forum": "nZFs4sL3do", "replyto": "nZFs4sL3do", "signatures": ["ICLR.cc/2026/Conference/Submission2251/Reviewer_sgiP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2251/Reviewer_sgiP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988583227, "cdate": 1761988583227, "tmdate": 1762916162305, "mdate": 1762916162305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HalDec-Bench, a benchmark to evaluate hallucination detection (HalDec) for image captions across diverse captioners, image domains, and hallucination types. Using a uniform scoring protocol where VLMs rate image–sentence alignment from 0–100, the authors report varied difficulty across splits and several consistent phenomena: (i) CLIP-style alignment models hover near chance, (ii) detectors over-trust early sentences in a caption (positional bias), and (iii) detectors show self-preference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper focuses specifically on hallucination detection in captions (not general QA), filling a gap and enabling apples-to-apples comparisons.\n- This work includes multiple captioners, image domains, and sentence-level labels with hallucination types—useful for fine-grained analysis.\n- The result identifies positional bias and self-preference across models. It also shows that CLIP-like models are near-random, which is actionable for practitioners."}, "weaknesses": {"value": "- Limited context modeling. Evaluation scores sentences independently, discarding inter-sentence context that may help detect subtle inconsistencies.\n- Despite multi-stage quality control, deciding hallucinations, especially span/type, can be subjective and is acknowledged as a limitation."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ciieTdaoJD", "forum": "nZFs4sL3do", "replyto": "nZFs4sL3do", "signatures": ["ICLR.cc/2026/Conference/Submission2251/Reviewer_QSAu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2251/Reviewer_QSAu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762292815858, "cdate": 1762292815858, "tmdate": 1762916161421, "mdate": 1762916161421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}