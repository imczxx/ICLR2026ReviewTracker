{"id": "hQElhp8NWW", "number": 20816, "cdate": 1758310522422, "mdate": 1759896957235, "content": {"title": "Discontinuities in Sparse Mixture-of-Experts: A Measure-Stochastic Analysis", "abstract": "Sparse Mixture-of-Experts (SMoE) architectures are now widely deployed in state-of-the-art language and vision models, where conditional routing allows scaling to very large networks. However, this very Top-$k$ expert selection that enables conditional routing also renders the SMoE map inherently discontinuous. In the vicinity of these discontinuity surfaces, even inputs that are arbitrarily close may activate substantially different sets of experts resulting in significantly different outputs. In this work we give a rigorous geometric and stochastic analysis of these discontinuities. We first classify them by order, determined by the number of tied experts at a switching event. Using measure-theoretic slicing arguments, we establish asymptotic volume estimates for the thickened discontinuity surfaces, showing that lower-order discontinuity sets dominate, whereas higher-order ones occupy a vanishingly small relative volume. Next, modeling random perturbations in the input space via a diffusion process, we prove that the path eventually encounter a discontinuity, and moreover that the first hit almost surely occurs on an order-1 discontinuity with explicit finite-time probability bounds. We further derive occupation-time bounds that quantify the duration the random path spend in the neighborhoods of each discontinuity order. These theoretical results imply that inputs are more likely to lie near lower order discontinuities. Motivated by this insight, we propose a simple smoothing mechanism that can be directly applied to existing SMoEs, softly incorporating experts near discontinuities; our analysis guarantees that the added computational overhead remains small while providing localized smoothing near discontinuities, and experiments across language and vision tasks show that smoothing not only enforces continuity of the SMoE map but also enhances empirical performance.", "tldr": "We analyze discontinuities in Sparse Mixture-of-Experts via a stochastic–measure framework, showing volume concentrates near lower-order boundaries, and propose a lightweight smoothing method that enforces continuity and improves performance.", "keywords": ["Discontinuities in Sparse Mixture-of-Experts", "Theoretical Machine Learning", "Stochastic and Measure-Theoretic Analysis"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b458207fb6f33ca52717e8fc43cbe41553dc3546.pdf", "supplementary_material": "/attachment/eb13fe1ec451ec2ea81a26747e2e90df49cd05ad.zip"}, "replies": [{"content": {"summary": {"value": "This paper conducts a rigorous geometric and stochastic analysis of the inherent discontinuities in MoE architectures caused by Top-k routing, classifying these discontinuities by order (based on the number of tied experts) and proving that lower-order (especially order-1) discontinuities dominate in terms of volume and probability under random perturbations. Motivated by this theoretical insight, the authors propose SmoothSMoE, a lightweight smoothing mechanism that softly incorporates experts near discontinuity regions, ensuring the SMoE input-output map becomes continuous while maintaining low computational overhead. Extensive experiments across language modeling (WikiText-103, EnWiki-8), image classification (DomainBed), and natural language understanding (GLUE) tasks demonstrate that SmoothSMoE outperforms vanilla SMoE."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical verification is sufficient.\n\n2. Inherent discontinuities in Top-k routing is an interesting topic, which may be related to discussion of hard samples in classification."}, "weaknesses": {"value": "1. Does similar scoring imply that either choice is equally acceptable? If similar scoring imply that either choice is equally acceptable, why is this smoothing mechanism necessary?\n\n2. What is the value of k in the Top-k reported in Table 1 and Table 2?\n\nI am not particularly familiar with the theoretical details, but intuitively, if a token receives similar scores from expert 1 and expert 3, it suggests that assigning it to either expert 1 or expert 3 might make little difference. Based on this assumption, many prior works have proposed dynamic routing strategies. Given that, why is this smoothing mechanism necessary?"}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ly26QPo5Bo", "forum": "hQElhp8NWW", "replyto": "hQElhp8NWW", "signatures": ["ICLR.cc/2026/Conference/Submission20816/Reviewer_aFnx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20816/Reviewer_aFnx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761058368359, "cdate": 1761058368359, "tmdate": 1762935933353, "mdate": 1762935933353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of discontinuities in expert weighting in sparse MoEs, which occur where gating logits are tied. A series of theorems bounds the neighborhood volumes of discontinuities of different orders (defined by the number of tied experts). Then a smooth MoE method is proposed that adds nonzero weight for experts having gating logits within $\\epsilon$ of the top $k$, making the weights continuous in the inputs. This method shows strong gains over the natural baseline on several tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Rigorous and well-written. Although the results on higher-order ties are not surprising (see below), the detailed theorems are a useful contribution.\n\nThe proposed smooth MoE is intuitive and a simple modification to existing methods, achieving strong experimental results."}, "weaknesses": {"value": "The result that first-order discontinuities dominate is fairly obvious. (In fact on reading the 5th sentence of the abstract (“We first classify”) my reaction was why bother with the classification since all higher-order ones have negligible probability.) The result about finite hitting time is also unsurprising since Brownian motion hits any hyperplane in finite time with probability 1. Moreover since diffusion per se is not of interest and the real question is how close an input is to a discontinuity, the result is just that any input is a finite distance from a discontinuity.\n\nLemma A.20 holds only because $T_\\epsilon^{(\\infty)}$ is defined wrt $z$ space (compared to $T_\\epsilon$ which is defined wrt $x$ space). So the fact that distance only needs to be checked in the logits (e.g., line 358) is just a trick of the definition. Moreover what matters in practice, e.g. in an adversarial setting or for gradients under the proposed smoothing method, is distance in $x$ space."}, "questions": {"value": "Do you renormalize the gating weights after adding boundary experts?\n\nIs the smoothing useful at test or does it primarily help by giving more informative gradients during training? What happens to performance if you turn off the smoothing at test (or late in training to let the model fine-tune to the no-smoothing setup)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2eG9ELPHJP", "forum": "hQElhp8NWW", "replyto": "hQElhp8NWW", "signatures": ["ICLR.cc/2026/Conference/Submission20816/Reviewer_rUVp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20816/Reviewer_rUVp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584501948, "cdate": 1761584501948, "tmdate": 1762935881664, "mdate": 1762935881664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a rigorous theoretical analysis of the discontinuities inherent in Sparse Mixture-of-Experts (SMoE) models that utilize Top-k gating. The authors' contributions can be categorized into three main areas. First, from a geometric perspective, they classify discontinuities by an \"order\" corresponding to the number of simultaneously tied expert scores at the selection boundary. Using measure-theoretic arguments, they demonstrate that the $\\epsilon$-thickened volume of these discontinuity surfaces decays as $(\\epsilon/R)^n$ for an order-n discontinuity, implying that lower-order (simpler) discontinuities are geometrically dominant. Second, from a stochastic perspective, they model random input perturbations as an Itô diffusion process. They prove that such a process, starting within a region of a fixed expert set, will almost surely hit a boundary, and that this first hit will occur on an order-1 discontinuity. They further provide bounds on the occupation time near these surfaces, again showing that the process spends less time near higher-order discontinuities. Finally, motivated by these theoretical insights, they propose a practical and efficient smoothing mechanism called SmoothSMoE. This method applies a localized smoothing to non-top-k expert logits that fall within an $l_{\\infty,\\epsilon}$ distance of the k-th logit, effectively enforcing continuity while incurring minimal overhead. The authors validate their method with experiments on language modeling, image classification, and natural language understanding tasks, showing modest but consistent improvements over the vanilla SMoE baseline"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The core strength of the paper is its deep and formal analysis of SMoE discontinuities. The use of measure theory to quantify the size of different orders of discontinuities and the stochastic analysis of hitting times are highly original and provide insights into the behavior of these models.\n- The paper clearly identifies a fundamental problem (discontinuities from Top-k gating) and proposes a solution (local smoothing) directly motivated by a rigorous analysis of that problem.\n- The proposed SmoothSMoE is simple, requires no re-training from scratch, and is computationally efficient. The adaptive loss for tuning the hyperparameter $\\epsilon$ makes the method more practical."}, "weaknesses": {"value": "- The authors mention several related works on differentiable or smooth MoE routing in the introduction (e.g., SMEAR, Soft MoE, ReMoE) but dismiss them on conceptual grounds (e.g., breaking causality for generation, requiring costly retraining). However, they fail to provide any direct empirical comparisons, even on tasks where those methods are perfectly applicable, such as the GLUE benchmark or DomainBed classification.\n- The reported improvements, while consistent, are quite small across the board (e.g., ~1-1.5 PPL on WikiText-103, ~0.9% average accuracy on DomainBed, <0.5% on GLUE). While theoretical justification can make small gains more compelling, the authors should be more circumspect in their claims. The lack of error bars or statistical significance testing further weakens the empirical claims. Are these small gains reproducible or just noise from a single run?"}, "questions": {"value": "- Can the authors justify the omission of experimental comparisons to other relevant smoothing methods like Soft MoE on the non-autoregressive tasks (DomainBed, GLUE)?\n\n- The log-smoothstep function seems somewhat arbitrary. Did you experiment with other smoothing functions (e.g., based on splines, sigmoids)? Is there any part of your theoretical analysis that suggests this specific functional form is optimal or preferable in some way?\n\n- Could you please provide standard deviations across multiple runs for your key results in Tables 1, 2, and 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BbGgHAp77N", "forum": "hQElhp8NWW", "replyto": "hQElhp8NWW", "signatures": ["ICLR.cc/2026/Conference/Submission20816/Reviewer_VNBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20816/Reviewer_VNBy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838550033, "cdate": 1761838550033, "tmdate": 1762935860002, "mdate": 1762935860002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a rigorous geometric and stochastic characterization of discontinuities in Sparse Mixture-of-Experts (SMoE) models caused by Top-$k$ gating.\nThe authors:\n- Classify discontinuities by order (number of tied experts) and derive asymptotic volume bounds for their ϵ-thickened neighborhoods using measure-theoretic slicing arguments.\n- Analyze stochastic dynamics of random perturbations near these boundaries, proving that diffusion paths almost surely hit order-1 discontinuities first, with explicit hitting-time and occupation-time bounds.\n- Propose a smoothing mechanism—$\\ell_{\\infty, \\epsilon}$-local smoothing—that restores continuity in SMoEs by softly activating near-boundary experts.\nEmpirical results across language (WikiText-103, EnWiki-8, GLUE) and vision (DomainBed) tasks demonstrate measurable robustness and accuracy gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Foundational theory: Provides the first quantitative geometry of MoE discontinuities.\n- Stochastic insight: Connects random perturbation dynamics to boundary order and frequency.\n- Elegant theorems: Clean scaling laws (Thm. 4.4, 4.7) and finite-time hitting bounds.\n- Actionable smoothing: $\\ell_{\\infty, \\epsilon}$-smoothing has provable continuity and empirical benefit.\n- Cross-domain validation: Demonstrated on both NLP and vision benchmarks."}, "weaknesses": {"value": "- High mathematical density: May deter non-theory readers; geometric intuition could be expanded.\n- Scope of stochastic model: Assumes isotropic Brownian motion—structured or adversarial noise remains open.\n- Empirical analysis limited: Smoothing ablation on very large LLMs (e.g., >7B) not shown.\n- Adaptive $\\epsilon$ mechanism: Boundary loss behavior could be theoretically analyzed beyond heuristics.\n- No direct link to differentiable routing methods: A comparative theoretical discussion (SoftMoE, ReMoE) would strengthen positioning."}, "questions": {"value": "- How sensitive are asymptotic ratios (Theorem 4.4) to non-affine gating networks or nonlinear activations?\n- Does the stochastic hitting analysis extend to anisotropic σ or multiplicative noise processes?\n- Is the occupation-time bound tight in the small-$\\epsilon$ limit, or could there be sharper constants?\n- In adaptive ϵ optimization, is $\\mathcal{L}_{\\text{boundary}}$ stable under mini-batch estimation noise?\n- Can the smoothing be viewed as a local convex relaxation of Top-$k$ gating?\n- How does smoothing interact with expert load balancing in large-scale training?\n- Could discontinuity order distribution be empirically estimated in trained models?\n- Are there guarantees of gradient continuity for backprop through $\\ell_{\\infty, \\epsilon}$ smoothstep?\n- Would adversarially aligned perturbations (not Brownian) still almost surely hit order-1 boundaries?\n- Could these geometric insights inform new regularizers for MoE fairness or specialization?\n\n⸻"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kpgp8feW79", "forum": "hQElhp8NWW", "replyto": "hQElhp8NWW", "signatures": ["ICLR.cc/2026/Conference/Submission20816/Reviewer_ue9P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20816/Reviewer_ue9P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926230218, "cdate": 1761926230218, "tmdate": 1762935852624, "mdate": 1762935852624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}