{"id": "GD55nbjRaD", "number": 8184, "cdate": 1758072771031, "mdate": 1763120904255, "content": {"title": "Terra: Explorable Native 3D World Model with Point Latents", "abstract": "World models have garnered increasing attention for comprehensive modeling of the real world.\nHowever, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world.\nThis could undermine the 3D consistency and diminish the modeling efficiency of world models.\nIn this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space.\nSpecifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance.\nWe then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. \nOur Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process.\nFurthermore, Terra achieves explorable world modeling through progressive generation in the point latent space.\nWe conduct extensive experiments on the challenging indoor scenes from ScanNet v2.\nTerra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.", "tldr": "", "keywords": ["World Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1fb3803fcbc77958d2cf19937773b5236a5ec6b4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Terra, a native 3D world model designed to represent, generate, and progressively explore 3D environments. The authors argue that conventional world models, which rely on 2D pixel-aligned representations, struggle with 3D consistency and modeling efficiency. Terra addresses this by operating directly in an intrinsic 3D latent space using point latents. Terra proposes a Point-to-Gaussian Variational Autoencoder (P2G-VAE) that transforms colored 3D point cloud to 3D Gaussians, and proposes a Sparse Point Flow matching network (SPFlow) to learn the latent point distribution. Authors show Terra capabilities to reconstruction the scene, do uncondition generation and image-conditioned generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Authors propose several novel techniques to facilitate the VAE and flow-matching learning, like Robust position perturbation, Adaptive upsampling and refinement, etc. \n2. The final 3D Gaussian representation naturally supports multi-view consistency\n3. The model can progressively generate a large-scale, coherent world simulation step-by-step."}, "weaknesses": {"value": "1. The accuracy and completeness of the input point cloud significantly affect the model performance, no matter in the reconstruction (point to Gaussian) task or the generation task. As shown in Figure 4 and Figure 5, even Terra can learn to complete the partial objects caused by the sensor failure in dark regions, the output Gaussians still have holes. \n2. Continuing from the previous one, in your image-conditioned generation, the accuracy of depth estimation may directly affect the quality of the generation. Once the depth estimator fails or the input image is out of domain, the model might fail as well.\n3. Another baseline can be SCube[1], which uses voxels instead of points as an intermediate representation, decoding per-voxel Gaussians for rendering. It would be interesting to see their comparison or theoretical analysis.\n\n[1] SCube: Instant Large-Scale Scene Reconstruction using VoxSplats, NeurIPS 2024"}, "questions": {"value": "1. Can the author elaborate on the scalability of this method? 3D data is not easily obtainable and may contain noise. Yet this method strongly relies on 3D input data. \n2. training time is not reported.\n3. What is the maximum range of generation supported in a single inference and step-by-step exploration?\n4. In 4.3 main results - Reconstruction, are you reporting the metrics on novel views or just input views?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L8oBUsnfTg", "forum": "GD55nbjRaD", "replyto": "GD55nbjRaD", "signatures": ["ICLR.cc/2026/Conference/Submission8184/Reviewer_FWFD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8184/Reviewer_FWFD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536713396, "cdate": 1761536713396, "tmdate": 1762920143568, "mdate": 1762920143568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "6Sm9tEtAnP", "forum": "GD55nbjRaD", "replyto": "GD55nbjRaD", "signatures": ["ICLR.cc/2026/Conference/Submission8184/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8184/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763120900556, "cdate": 1763120900556, "tmdate": 1763120900556, "mdate": 1763120900556, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Terra, a framework for generating explorable 3D environments, where each environment is represented by point latents. The overall pipeline consists of two main components:\n1. A Point-to-Gaussian VAE (P2G-VAE) based on PTv3, which encodes colored 3D points into a compact latent space.\n2. A Sparse Point Flow Matching (SPFlow) model that learns the distribution of these point latents in the latent space.\n\nThe method is trained and evaluated on the ScanNet v2 dataset.\n- For reconstruction, Terra achieves better depth accuracy than PixelSplat, MVSplat, Prometheus, and Can3Tok, though it performs worse on the LPIPS metric.\n- For unconditional/image-conditioned generation, its geometric quality surpasses Trellis and Prometheus, while its visual quality is higher than Trellis but below Prometheus.\n\nAblation studies show that:\n- Robust Position Perturbation reduces reconstruction quality in P2G-VAE but significantly enhances generative capability.\n- Adaptive Upsampling and Refinement and Explicit Color Supervision both improve reconstruction and generation performance.\n- Distance-Aware Trajectory Smoothing plays a key role in stabilizing training for generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Distance-Aware Trajectory Smoothing is novel and demonstrates clear effectiveness in the context of sparse point flow matching models."}, "weaknesses": {"value": "1. The term “world model” is conceptually broad. Using it as the paper’s main title may be misleading, as the method focuses more narrowly on 3D world generation and exploration, primarily within indoor scenes.\nMoreover, most compared methods do not explicitly position themselves as world models — e.g., Prometheus (text-to-3D generation), Can3Tok (3D scene-level generation), and PixelSplat/MVSplat (3D reconstruction)."}, "questions": {"value": "1. What is the motivation for removing all residual connections in PTv3?\n2. What exactly is the ground-truth distribution $\\mathbf{P}$ of point latents, and how is it sampled? During inference, are the positions in $\\mathbf{P}$ also randomly sampled from Gaussian noise?\n3. How do the authors position Terra relative to recent approaches such as WorldMem [Xiao et al., 2025], VMem [Li et al., 2025], and Voyager [Huang et al., 2025]? From a visual standpoint, Terra’s generated results appear somewhat blurry, incomplete, or low-resolution compared to these models. While it remains an open question what the ideal representation for world models should be (e.g., 3D Gaussian Splatting, video-based, or otherwise), it would strengthen the paper if the authors clarified why comparisons to these methods were omitted and articulated Terra’s distinct advantages or future potential.\n\nThings to improve the paper that did not impact the score:\n- Please report the GPU hours required to train P2G-VAE and SPFlow.\n- Table 1 appears far from its first citation — consider adjusting its placement for readability.\n- Consider adding a section on the use of large language models (LLMs)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qPy2kvh7Cp", "forum": "GD55nbjRaD", "replyto": "GD55nbjRaD", "signatures": ["ICLR.cc/2026/Conference/Submission8184/Reviewer_jrCy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8184/Reviewer_jrCy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156196919, "cdate": 1762156196919, "tmdate": 1762920143028, "mdate": 1762920143028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method that, given an input colored point cloud, generates shapes in a latent space and allows for progressive exploration. The method uses a Point-to-Gaussian VAE to compress 3D inputs into sparse point latents and decodes them into 3D Gaussian primitives for rendering. It then uses a sparse point flow matching model to jointly denoise point positions and features for generative modeling."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper attempt to tackle the reconstruction problem from a native 3D generation perspective. The idea makes sense."}, "weaknesses": {"value": "1) The method utilizes an input point cloud from fused multi-view depth sensors, which should provide high-quality shapes and textures. However, the generated results appear to make both the shapes and texture blurrier.\n\n\n2) The comparison with Trellis is unfair. A more appropriate baseline would be to add the point cloud condition to Trellis, for instance, by voxelizing the point cloud to serve as the sparse grid for trellis's structured latent.\n\n3) It seems that the paper is regenerating things that is already available from the input. In Fig. 1. What portion of the generated scene is not present in the input?   It is recommend to visualize the difference between input point cloud and the generated one.  \n\n\n4) The paper is missing comparisons with important RGB-D reconstruction baselines, such as classic depth map fusion methods (e.g., BundleFusion [1], ElasticFusion [2]) or methods based on neural fields [3]. The reconstruction results reported in this paper appear to be much worse than those achieved by the aforementioned baselines.\n\n[1] BundleFusion: Real-time Globally Consistent 3D Reconstruction using Online Surface Re-integration\n\n[2] ElasticFusion: Real-time dense visual SLAM system\n\n[3] Neural RGB-D Surface Reconstruction"}, "questions": {"value": "- Is this method trained on random 3D crops?\n- Does this method complete occluded geometry?\n- Line 462. Can this method explorable unseen geometry in the input point clouds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pigTNuXOyH", "forum": "GD55nbjRaD", "replyto": "GD55nbjRaD", "signatures": ["ICLR.cc/2026/Conference/Submission8184/Reviewer_mSMJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8184/Reviewer_mSMJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173373845, "cdate": 1762173373845, "tmdate": 1762920142628, "mdate": 1762920142628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the fundamental limitation of existing world models that rely on pixel-aligned representations. The authors introduce Terra, a native 3D world model, that represents and generates explorable environments using an intrinsic 3D latent space through two key technical innovations: a Point-to-Gaussian Variational Autoencoder (P2G-VAE) that encodes 3D inputs into latent point representations and decodes them as 3D Gaussian primitives to jointly model geometry and appearance, and a Sparse Point Flow Matching Network (SPFlow) that generates latent point representations by simultaneously denoising positions and features of point latents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured and easy to follow\n2. Point-to-Gaussian Variational Autoencoder (P2G-VAE) effectively reduces redundancy in 3D input data while creating a compact latent space that jointly models both geometry and appearance through 3D Gaussian primitives, making it highly efficient for generative modeling.\n3. Flexible rendering from any arbitrary viewpoint with only a single generation process\n4. Progressive training strategy with three well-designed stages (reconstruction, unconditional pretraining, masked conditional generation)"}, "weaknesses": {"value": "1. No inference / training time comparison\n2. No memory usage analysis\n3. I believe that performance relates more to the method timing and suggest to use terms \"Reconstruction Accuracy\" and \"Generation Accuracy\" in tables 1 and 2.\n4. I suggest a couple of high resolution renders in appendix or videos in supplementary materials to evaluate a visual quality of Terra\n\n\nOverall, I'd be glad to increase the score if the authors address the above issues"}, "questions": {"value": "see weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9tIgZ4eYmn", "forum": "GD55nbjRaD", "replyto": "GD55nbjRaD", "signatures": ["ICLR.cc/2026/Conference/Submission8184/Reviewer_fH8q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8184/Reviewer_fH8q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186383582, "cdate": 1762186383582, "tmdate": 1762920142028, "mdate": 1762920142028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers and ACs for their efforts and constructive comments. We have tried our best to answer the questions on a very tight schedule. We will continue to improve our work."}}, "id": "ulutXh3SWN", "forum": "GD55nbjRaD", "replyto": "GD55nbjRaD", "signatures": ["ICLR.cc/2026/Conference/Submission8184/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8184/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission8184/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763120768895, "cdate": 1763120768895, "tmdate": 1763120768895, "mdate": 1763120768895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}