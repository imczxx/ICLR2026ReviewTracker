{"id": "1qLZsyJN2t", "number": 24937, "cdate": 1758362195748, "mdate": 1763245773239, "content": {"title": "The Information Game: Active Inference as Bilevel Optimization and a Game-Theoretic Benchmark for LLM Inquiry", "abstract": "Large language models (LLMs) increasingly operate in settings where they must gather information rather than simply recall facts. We model this  task as a multi-street game of incomplete information casting each round of information gathering as a bilevel optimization: an inner variational Bayesian step that updates beliefs over a hidden target object, and an outer query-selection step that minimizes expected free energy, which is equivalent to maximizing expected information gain. This game-theoretic formulation motivates \\emph{Optimal Question Asking} (OQA), a benchmark designed as a tractable \"toy game\" to measure an agent's inquiry strategy by measuring how quickly an agent reduces uncertainty about the target. By solving this game for its Game-theory optimal (GTO) policy, we create a perfect oracle against which we measure the planning gap—the expected number of suboptimal queries.On 25-object tasks, models like GPT-4o and Claude 3.5 Haiku exhibit a planning gap of 1-2 queries. On 100-object tasks, flagship models like GPT-o3 and Gemini 2.5 Pro, while closer to optimal, still show significant strategic leaks. Our synthetic datasets, which remove linguistic priors, reveal deeper deficits. OQA exposes inefficiencies invisible to answer-centric metrics, offering a controlled testbed for forging agents that play the information game not just exploitatively, but optimally.", "tldr": "We frame question answering as bilevel optimization and use that to benchmark frontier LLMs on their efficiency at reducing uncertainty through question asking; we find these LLMs still lag an information-theoretic oracle", "keywords": ["active inference", "bilevel optimization", "question asking", "query optimality", "inference", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d906aedf92345936f636241eb400a1db1efdf48d.pdf", "supplementary_material": "/attachment/eb639218e64dadd32da00fed52d513b28b430fd5.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel task for LLM, which is optimal querying for information gathering and elimination of uncertainty. Compared to prior optimal querying tasks such as twenty questions, this task is based on game theory and has better theoretical, quantitative properties. The paper gives theoretical proof on such type of task, proposes a query efficiency benchmark as an instance of the task, and conducts empirical study on this benchmark with several state-of-the-art models, which shows state-of-the-art models are suboptimal."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a detailed description on the experiment, and the authors also provide their code, experiment logs and dataset. The reproducibility of their work seems promising.\n\n2. The general motivation and idea of this paper is clearly conveyed: to quantitatively measure LLM's ability to curate queries for gathering information.\n\n3. The related work seems well-rounded, with investigations on many related areas to this work."}, "weaknesses": {"value": "1. I am not sure about the necessity of involving game theory in this proposed task. The paper models the task as a two-player zero-sum game, with the inquirer asking question and the responder answering question. However, the responder does not seem to have any alternative strategy rather than answering \"yes\" or \"no\" honestly according to fact (as mentioned in line 215). In this case, the game degrades into a single-agent task where the responder is simply part of the static environment, and proposition B.15 can be also modeled as finding the shortest path from the full set to the node with the subset that only contains the nature's choice in definition B. 13 (ii), which can be done by dynamic programming (backward induction of Appendix B.5). This seems to be different from the AKQ game mentioned in the paper, where the two agents both have degree of freedom on their actions.\n\n2. If the selection of nature (definition B.13) and attributes are uniformly randomly sampled (line 190), then I would assume randomness plays an important role in the performance of the models as the difficulty can significantly vary with the attribute/answer chosen. However, there is no error bars, standard deviations, seeds or confidence interval reported in the main results.\n\n3. The ability that the benchmark is trying to test is intertwined. The author mentions in line 1453 that \"Without memorable names, models often drop viable candidates from their implicit belief state\". However, the purpose of synthetic dataset is only to remove linguistic priors (line 1426), not to test the model's long-context memorization ability. Thus, when there exists an optimality gap, it is hard to say which factor is the bottleneck - this obfuscates the insight that we can gain from the benchmark and how we can build a better model to address the task. The paper also does not try to address this issue by ablation (which should be in the main paper as currently it has one unused page).\n\n4. The empirical analysis does not provide much insight on \"how we can build a better model\" (with either successful or failed attempts) beyond \"state-of-the-art models are suboptimal\" (model dropping viable candidates is one, but this alone is insufficient). The message \"current state-of-the-art cannot solve\" can often be a short-lived message - cutting-edge LLMs improves very quickly on super-hard benchmarks such as Humanity's Last Exam [1] and ARC-AGI-2 [2]. The behavior analysis and attempted solutions, however, are much more valuable. Besides, the models does not seem to be too far away from optimal - is \"a planning gap of 1-2 queries\" unacceptable?\n\n**Minor Weakness**\n\nIn line 177, 190 and line 260, periods are missing.  \n\n**References**\n\n[1] L. Phan et al. Humanity's Last Exam. ArXiv: 2501.14249, 2025.\n\n[2] https://arcprize.org/arc-agi/2/"}, "questions": {"value": "I have one question: Did the authors find any behavioral difference between the synthetic dataset and the dataset with real-life semantic priors (maybe besides that the synthetic items are easier to be forgotten as suggested in line 1452 - for this part, is there any quantitative metric?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QuywZcObxS", "forum": "1qLZsyJN2t", "replyto": "1qLZsyJN2t", "signatures": ["ICLR.cc/2026/Conference/Submission24937/Reviewer_53vQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24937/Reviewer_53vQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459987802, "cdate": 1761459987802, "tmdate": 1762943251269, "mdate": 1762943251269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "PmxAtw0cJ5", "forum": "1qLZsyJN2t", "replyto": "1qLZsyJN2t", "signatures": ["ICLR.cc/2026/Conference/Submission24937/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24937/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763245772547, "cdate": 1763245772547, "tmdate": 1763245772547, "mdate": 1763245772547, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Optimal Question Asking (OQA) benchmark, which measures how efficiently LLMs reduce uncertainty through yes/no questions about hidden objects. The authors propose to view the OQA game objective as a game-theory optimal solution minimizing the number of queries. Based on this, the oracle is a dynamic programming-based solution that makes the most informative query at every step, to minimize the number of queries. They authors evaluate several LLMs and measure the gap between the number of queries used and the optimal number of queries, showing that LLMs show suboptimal query usage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper studies an important problem: Benchmarks measuring query efficiency in information gathering are an under-explored area of research, and as the authors argue, relevant to many real world scenarios such as alignment, tutoring and clarification seeking.\n- The authors define an information-theoretic lower-bound that is conceptually useful\n- Experiments cover real world data and sythetic data"}, "weaknesses": {"value": "- I'm not sure that the optimal solution as found by dynamic programming is actually feasible to be computed by humans or LLMs through reasoning, without access to computational tools. This makes it difficult to interpret what exactly the measured planning gap actually implies.\n- The evaluation is limited to object-guessing games, which is a narrow subset of information gathering tasks.\n- The connection between GTO, EIG maximization, and EFE minimization is difficult to follow, and overall I could not understand the purpose of section 3's bilevel optimization view, as it does not seem to effect any of the following sections or the paper's experiments.\n- The experiments use the chat UI rather than APIs, without any details such as temperature, prompts, or usage of reasoning capability, making the experiments difficult to reproduce.\n- The organization of the paper is difficult to follow, with many results and analysis pushed to the appendix without proper integration in the main text (e.g. experiments on synthetic data)"}, "questions": {"value": "- Did the authors consider establishing human baselines, or LLM baselines with computational tools?\n- Does the \"implicit belief state\" refer to computations internal to the LLM, or does it refer to some kind of intermediate text such as the model's thought or reasoning outputs?\n- What are some possible ways to extend the framework to more domains not restricted to binary choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XBQjssao2M", "forum": "1qLZsyJN2t", "replyto": "1qLZsyJN2t", "signatures": ["ICLR.cc/2026/Conference/Submission24937/Reviewer_JAUX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24937/Reviewer_JAUX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886601753, "cdate": 1761886601753, "tmdate": 1762943251095, "mdate": 1762943251095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper proposes a new benchmark for measuring an LLM agent’s information-seeking strategy. The benchmark consists of 25 or 100 objects that can be queried through a series of yes/no questions and answers. The simplicity of the domain and the binary question-answering setup allow for comparison with an oracle strategy that maximizes expected information gain at each step.\n- The benchmark uses the mean number of queries (fewer indicates better “active inference”) to measure the effectiveness of different existing LLMs.\n- The paper formulates active inference as a bi-level optimization problem involving belief updating and decision-making to minimize variational free energy."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "While this paper discusses an interesting problem of active information gathering in multi-turn settings( where both the agent’s information-gathering and response-generation abilities are crucial to success) there is a significant gap between Sections 2–3 and the empirical results in Section 4, which limits the paper’s contribution and originality. Please see Weaknesses section for further details."}, "weaknesses": {"value": "Disconnect between framework and theory (sections 2 & 3) and empirical demonstrations (section 4): \n- Sections 2 and 3 focus on formalizing the bi-level optimization and propose, in Proposition 3.2, a structure that can be used for gradient-based training. However, the empirical results in Section 4 only evaluate the zero-shot capabilities of existing models using the prompt in Fig 1. It is unclear how the experiments connect to the proposed bi-level optimization framework beyond the comparison with the oracle strategy, as the oracle selects according to the maximal expected information gain (which would be optimal under the bi-level framework). The training objective (from Proposition 3.2) is neither implemented nor referred to in the experiments. \n- The contributions of this paper could be significantly strengthened if the proposed bi-level approach were either (a) used to improve models via fine-tuning or (b) incorporated into the models’ inference-time action selection. This would help establish the work's originality and significance.\n\nLack of empirical evidence and statistical significance:\n- Table 2 is generated from \"the mean over five random targets per tier\" but no standard deviation across the five targets is shown. Also testing with only five targets is too small to determine whether the reported results are statistically significant or meaningful. \n- Given this small sample size, the gap of 1-2 queries between the tested models and the oracle strategy is too small to make the empirical claims (e.g., \"Lighter models close the gap to within two questions on the 25-object tier ... larger models ask more optimal questions on average and maintain a near-optimal belief state even late in the dialogue\" in Lines 282-285) convincing. \n\nClarity:\n- Terms in proposition 3.2 can be formally defined. \n- Consistent notations can be used through Section 3: for example, $H(P(x))$ in line 161, if referring to states, could use $S$ instead of $X$ for consistency. \n- Section 3 would benefit from a formal problem setup section and introduction of relevant notations prior to mentioning any propositions for readability.\n\nPotential consideration for adding real-world motivated complexities to the tasks in addition to the object guessing games:\n- While the motivation for keeping the benchmark tasks simple and tractable is clear, the problems can be made more complex by adapting existing real-world datasets or multi-turn datasets from education (e.g., Shani et al., 2024; Wan et al., 2025). The actions can still be constrained to binary options (querying for yes or no answers) if tractability is a concern. \n\nShani et al., 2024. Multi-turn Reinforcement Learning from Preference Human Feedback. https://arxiv.org/abs/2405.14655\n\nWan et al., 2025. Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward. https://arxiv.org/html/2504.03206v2"}, "questions": {"value": "- A stronger connection could be made between Sections 2 & 3 (formulation of the bi-level optimization problem for active inference) and Section 4 (experiments). Please see the first point under \"Weaknesses.\"\n- The empirical results could be strengthened by evaluating larger samples and reporting statistical significance and standard deviation in addition to the mean. This is especially important given the small gap between the tested models and the oracle (only 1-2 queries)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2pjdeSeXic", "forum": "1qLZsyJN2t", "replyto": "1qLZsyJN2t", "signatures": ["ICLR.cc/2026/Conference/Submission24937/Reviewer_Lf2E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24937/Reviewer_Lf2E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944757048, "cdate": 1761944757048, "tmdate": 1762943250913, "mdate": 1762943250913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper frames optimal question asking as a sequential information game. Beliefs update via variational inference (inner loop). Question selection minimizes Expected Free Energy (EFE) is the outer loop.  Under their assumptions, this reduces to maximizing Expected Information Gain (EIG).\n\nThe paper introduce OQA, a small controlled benchmark where an agent identifies a hidden target (25 or 100 objects) by asking yes/no attribute questions on a list of attributes. They provide an oracle that greedily maximizes EIG each turn which is “game-theory-optimal (GTO)” and measure LLMs’ “planning gap” to this oracle on real (Animals, Cars, Places) datasets along with synthetic datasets. Empirically, frontier models trail the oracle by about 1–3 questions on 25 and 100-object tier tasks across benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Clear, focused benchmark for inquiry.** OQA isolates question selection rather than answer quality. The instruction template and dataset design are simple and well motivated (Sec. 4).\n\n**Clear lower bound and metric.** The paper provides an oracle and a crisp metric (gap to the oracle in questions), making results easy to interpret. Figures 2–4 make the effect visible: models reduce entropy like binary search early, then loses efficiency later."}, "weaknesses": {"value": "**[Critical] Cost depends on structure, not only set size/entropy.** Step 1 (“Cost depends only on |S|”) does not hold when the question set is restricted to a fixed attribute table (the benchmark’s setting). Two subsets with the same size can admit different partitions and thus different optimal costs. Therefore, C(S) is not a function of ∣S∣ alone. How does the theory translate the OQA benchmark?\n\n**[Critical] “MDP with nature” formalism.** The paper presents a two-player zero-sum game with SPNE terminology (§2, App. B), yet isn't the “Responder” is a deterministic channel with no strategy? The current framing seems misleading/over-theorizing. \n\n**[Critical] Greedy EIG can be suboptimal for expected depth.** With restricted tests, locally minimizing expected posterior entropy need not minimize the expected number of questions. Classical decision-tree results and counterexamples show that greedy information gain can yield deeper trees in expectation [1], [2].\n\n- [1] Constructing Optimal Binary Decision Trees is NP-Complete\n- [2] Performance Bounds on the Splitting Algorithm for Binary Testing\n \n**[Critical] Conflicting complexity claims.** Section 6 and Algorithm 1 state the oracle DP runs in O(d|S|^2) and is tractable up to ~100 items. Appendix D (Thm. D.1) gives O(d\\cdot N \\cdot 2^N) time and O(N\\cdot 2^N) space -- which seems to be standard for memoizing all subsets. These statements plainly conflict. Could the authors reconcile these two aspects?\n\n**[Major] Unclear experimental procedure.** Algorithm 1 does not make clear how you select the maximal-information-gain question in the benchmark’s restricted attribute set. Could you describe how the oracle was computed and how the attributes were queried? Generally, experimental details are lacking -- It is unclear how scores in the table are produced. \neg. \n- The algorithm seem to require each model to output a probability distribution over 100 objects after every turn; please detail the prompting, decoding, calibration strategies. (2) \n- The paper states “We can stop when there are no more distinguishing attributes” but ANIMALS contains duplicates (p. 4). How does the paper count queries when equivalence classes remain and confirm consistent handling across models?\n\n**[Major] Resolution for proposed procedures.** The paper proposes procedures to improve information seeking. Do the experiments have enough resolution to detect the claimed improvements? What are the confidence intervals across the models tested?"}, "questions": {"value": "Please address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fxyDRGWGr4", "forum": "1qLZsyJN2t", "replyto": "1qLZsyJN2t", "signatures": ["ICLR.cc/2026/Conference/Submission24937/Reviewer_nWPQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24937/Reviewer_nWPQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762369793217, "cdate": 1762369793217, "tmdate": 1762943250686, "mdate": 1762943250686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}