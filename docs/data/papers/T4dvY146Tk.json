{"id": "T4dvY146Tk", "number": 5190, "cdate": 1757863223057, "mdate": 1762954547918, "content": {"title": "FLEx: Personalized Federated Learning for Mixture-of-Experts LLMs via Expert Grafting", "abstract": "Federated instruction tuning of large language models (LLMs) is challenged by significant data heterogeneity across clients, demanding robust personalization. The Mixture of Experts (MoE) architecture, where experts can specialize in distinct data patterns, presents a natural architectural solution to this challenge. The inherent sparsity of the MoE architecture, achieved by selectively activating experts, poses a significant challenge to its integration with federated learning (FL). Conventional FL frameworks, designed for dense models, naively aggregate all expert parameters irrespective of their local activation patterns. This naive approach not only undermines MoE's dynamic sparsity but also risks corrupting the world knowledge within pretrained experts. To address this, we propose FLEx (Federated LLMs with Personalized Experts), a novel framework that leverages pretrained MoE-based LLMs for efficient personalization. By aggregating only the shared non-expert parameters, FLEx significantly reduces communication overhead and preserves the world knowledge stored within the frozen pretrained experts. For personalization, we introduce a novel expert grafting mechanism that leverages dynamic sparsity to construct a client-specific expert from selected components of pretrained experts, tailored to local data. This grafted expert is then fine-tuned locally alongside the gating mechanism. This joint training enables the model to learn when to leverage the shared knowledge from frozen experts and when to employ the personalized one. Evaluations on diverse, non-IID instruction tuning datasets show that FLEx consistently outperforms federated baselines on average, while demonstrating strong knowledge preservation on the knowledge-driven benchmark MMLU. Our code is available at \\href{https://anonymous.4open.science/r/FLEx-8F12}{\\texttt{https://anonymous.4open.science/r/FLEx-8F12}}.", "tldr": "We introduce FLEx, a federated learning framework for MoE-based LLMs that freezes pretrained experts to preserve general knowledge and locally trains a new, personalized expert \"grafted\" from existing ones to efficiently handle data heterogeneity.", "keywords": ["Large Language Models; Federated Learning; Mixture of Experts; Personalization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b98215acb25cf1d1ef8f0dd4de154091b219f239.pdf", "supplementary_material": "/attachment/a32347e383c959fb33792409423af11411e1a014.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces FLEx (Federated LLMs with Personalized Experts), a novel personalized Federated Learning (FL) framework for fine-tuning pre-trained Mixture-of-Experts (MoE) LLMs. FLEx addresses two major MoE-FL challenges—high communication cost and catastrophic forgetting—via a decoupled strategy: \n1) Selective Aggregation: Only dense, shared non-expert parameters are aggregated, while all pre-trained expert parameters are kept frozen locally. \n2) Expert Grafting: Each client performs a greedy selection to \"graft\" a lightweight personalized expert from the frozen pool, which is then jointly fine-tuned with a novel, trainable Adaptive Gating Mechanism. Experiments on non-IID instruction tuning show FLEx achieves superior personalization performance (ROUGE-L) and robust knowledge retention (MMLU) compared to FL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) Effective Solution to MoE-FL: Successfully utilizes MoE sparsity to reduce communication overhead and leverage the frozen experts to prevent catastrophic forgetting. \n2) Strong Performance Gains: Demonstrates significant improvements over standard FL baselines, particularly under challenging pathological non-IID settings. \n3) Knowledge Preservation: MMLU results strongly support the efficacy of freezing experts for retaining general world knowledge."}, "weaknesses": {"value": "1) Greedy Selection Limitation: The expert grafting strategy relies on a greedy selection process that picks only a single expert for personalization. While effective, this approach does not explore the potential gains—or added complexity—of using multiple experts or more sophisticated selection mechanisms.\n2) Lack of Communication Cost Measurement: Although reduced communication is presented as a key advantage, the paper does not provide concrete empirical evidence—such as tables or numerical comparisons—to quantify how many fewer parameters or bytes are transmitted compared to a full MoE model under FedAvg.\n3) Limited Generalizability: Experiments are conducted solely on one MoE architecture, Qwen1.5, and focus on NLP instruction tuning tasks. Results may not generalize to other MoE-based models or different modalities, and no cross-architecture or multi-domain validation is provided."}, "questions": {"value": "1) The expert grafting uses single greedy selection. Did you investigate using a weighted combination or mixture of multiple experts for local adaptation? How would this compare in terms of performance vs. additional complexity or communication/computation cost?\n2) Given that communication efficiency is a core contribution, could you provide a quantified analysis detailing the reduction ratio in transmitted parameters or total bytes per round compared to a MoE + FedAvg baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7yEoyLlnHN", "forum": "T4dvY146Tk", "replyto": "T4dvY146Tk", "signatures": ["ICLR.cc/2026/Conference/Submission5190/Reviewer_KzTs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5190/Reviewer_KzTs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802467181, "cdate": 1761802467181, "tmdate": 1762917937240, "mdate": 1762917937240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "eCIXJvFK5Y", "forum": "T4dvY146Tk", "replyto": "T4dvY146Tk", "signatures": ["ICLR.cc/2026/Conference/Submission5190/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5190/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762954547157, "cdate": 1762954547157, "tmdate": 1762954547157, "mdate": 1762954547157, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FLEx, a novel framework for federated learning with MoE-based LLMs. The central idea is to disentangle globally shared parameters from personalized components: only the dense non-expert parameters (e.g., attention layers) are aggregated across clients, while all pretrained experts remain locally frozen to retain general knowledge. To personalize the model, each client grafts a new lightweight expert by selecting the most effective frozen expert based on local data, and integrates it using a jointly trained adaptive gating mechanism. The proposed design aims to address both data heterogeneity and communication efficiency in FL scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Clarity and Structure:** The paper is clearly written and well-organized. Figures and algorithmic descriptions are intuitive, enhancing the accessibility of the methodological exposition.\n- **Reproducibility:** The authors have provided code and implementation details, which supports reproducibility and validation of the reported results."}, "weaknesses": {"value": "- **Potential Representation–Routing Misalignment:** The design updates dense non-expert layers through global aggregation while keeping all pretrained experts and their routers frozen. This raises concerns about potential misalignment between the evolving feature representations and the static routing mechanism, which could lead to suboptimal or unstable expert activation, especially under non-IID data distributions. The paper lacks analysis—such as expert utilization statistics or routing entropy—to evaluate this potential drift.\n- **Unsubstantiated Efficiency Claims:** Although the method is described as efficient, inference actually involves routing each token to $K+1$ experts instead of $K$, introducing additional computational overhead. While the appendix suggests this overhead is negligible, no quantitative evidence (e.g., FLOPs, latency measurements) is provided to substantiate this claim.\n- **Experimental Rigor:**\n    - **Limited Baselines:** Experimental comparisons are confined to classical FL methods (e.g., FedAvg, FedProx, SCAFFOLD), omitting several recent personalized or MoE-based federated LLM approaches [1–4].\n    - **Lack of Statistical Reporting:** Results appear to be based on single runs, with no reported variance or statistical significance tests, which limits the robustness of the empirical evaluation.\n\n[1] Dual-personalizing adapter for federated foundation models, 2024.  \n\n[2] FDLoRA: personalized federated learning of large language model via dual LoRA tuning, 2024.  \n\n[3] FedMoE: Personalized Federated Learning via Heterogeneous Mixture of Experts, 2024.  \n\n[4] Personalized Federated Fine-Tuning for LLMs via Data-Driven Heterogeneous Model Architectures, 2024."}, "questions": {"value": "Why does FLEx preserve all pretrained experts while adding one personalized expert per client? Would it not be more parameter-efficient to retain $N-1$ frozen experts and adapt one via a parameter-efficient method like LoRA, thereby maintaining the overall parameter budget while still enabling effective personalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U1B7yW1sIC", "forum": "T4dvY146Tk", "replyto": "T4dvY146Tk", "signatures": ["ICLR.cc/2026/Conference/Submission5190/Reviewer_679S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5190/Reviewer_679S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886786513, "cdate": 1761886786513, "tmdate": 1762917936925, "mdate": 1762917936925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the significant challenges of applying federated learning (FL) to Mixture-of-Experts (MoE) large language models (LLMs), namely prohibitive communication costs, catastrophic forgetting of pretrained knowledge, and poor personalization under data heterogeneity. The authors propose FLEx (Federated LLMs with Personalized Experts), a novel framework that decouples shared knowledge from personalization. FLEx's core mechanism involves freezing all pretrained experts (preserving world knowledge and eliminating their communication cost) and aggregating only the shared, dense non-expert parameters (e.g., attention layers). To achieve personalization, FLEx introduces an \"expert grafting\" mechanism, where each client identifies the single, most suitable pretrained expert based on a local reconstruction loss. A copy of this expert is then added to the model as a new, trainable, client-specific expert. An adaptive gating mechanism is jointly trained to learn when to route to this new personalized expert versus the frozen pretrained ones. Experiments on non-IID instruction-following tasks show that FLEx outperforms standard federated baselines (like FedAvg) in average performance and demonstrates superior general knowledge retention as measured by MMLU."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper identifies a critical and highly practical problem. The naive application of standard FL to massive MoE models is untenable due to communication overhead and knowledge corruption. The paper's core strategy--decoupling the aggregation of shared non-expert parameters from the personalization of frozen experts--is an elegant and effective solution to this problem.\n\nS2. The \"expert grafting\" mechanism for personalization is a clever and efficient approach. Instead of training a new, randomly initialized expert from scratch (which would be data-hungry and slow to converge), the framework leverages the rich, pretrained knowledge by copying the most relevant existing expert. This provides a high-quality initialization for the personalized expert, allowing for rapid and effective adaptation to local data."}, "weaknesses": {"value": "W1. The framework's core design--aggregating only non-expert parameters while freezing all pretrained experts--prevents the model from collaboratively learning new, shared knowledge within its most critical components. In Transformer architectures, the expert layers (FFNs) are the primary location for knowledge storage, whereas the aggregated attention layers mainly handle information routing. By freezing all experts, the FL process is blocked from updating the model's core \"knowledge\" stores with insights from multi-client data. This is a major trade-off. A more robust solution might involve a compromise, such as including one or more shared experts that are aggregated by the server like DeepSeekMoE, allowing the model to collaboratively learn new shared concepts without the full cost of aggregating all experts. The paper should investigate this possibility.\n\nW2. The core \"expert grafting\" mechanism is limited to selecting only a single expert for personalization (per layer). The paper's own ablation study (Appendix C.2, Table 7) shows that performance improves (albeit slightly) when grafting 2 or 3 experts, though this is dismissed due to the computational cost of the selection process. This suggests the \"one expert\" design is a heuristic for efficiency, not necessarily the optimal choice for personalization. For clients with genuinely multi-modal data (e.g., a mix of finance and medical texts), a single personalized expert may be forced to compromise, whereas a small set of personalized experts would be a more natural fit.\n\nW3. The paper's baseline comparisons are not as strong as they could be. The baselines (FedAvg, FedProx, etc.) are all applied in a \"naive\" full-aggregation setting, which the paper itself argues is flawed and non-viable. A more meaningful comparison would be against other parameter-efficient federated learning (PEFT-FL) strategies. For example, how does FLEx (which trains a full expert copy + gates) compare to a standard Federated LoRA (FLoRA) applied to all parameters (including experts), or FLoRA applied only to the non-expert layers? Without these comparisons, it's difficult to ascertain if FLEx's specific \"grafting\" design is superior to other, more established PEFT-FL techniques.\n\nW4. The performance improvements in some key experiments are marginal and lack statistical validation. For instance, in Table 2 ($\\alpha=1.0$), the improvement of FLEx (37.54) over the FedAvg baseline (37.51) is less than 0.1 ROUGE-L points. Similarly, the gains in Helpfulness and Harmlessness (Figure 3) over the next-best methods are small. The paper does not report variance or standard deviation for these results, making it impossible to assess whether these minor differences are statistically significant or simply noise from a single experimental run.\n\n**Comments**\n\nC1. As a point of clarification on Table 1: In the \"Summ\" (Summarization) column, the paper's markings are consistent with the caption. The best score is MoE + FedAvgM (42.54) and the second-best is MoE + FedAdagrad (42.24), which are bolded and underlined respectively. The FLEx score (42.09) is third in this specific category, which should not be underlined."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4xarqjMyqD", "forum": "T4dvY146Tk", "replyto": "T4dvY146Tk", "signatures": ["ICLR.cc/2026/Conference/Submission5190/Reviewer_Zkor"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5190/Reviewer_Zkor"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912151039, "cdate": 1761912151039, "tmdate": 1762917936695, "mdate": 1762917936695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FLEx, a federated learning framework for Mixture-of-Experts (MoE) models. To reduce communication cost and avoid catastrophic forgetting, FLEx aggregates only the dense non-expert parameters while keeping pretrained experts frozen. To handle data heterogeneity, it introduces an expert grafting mechanism that builds lightweight personalized experts via pruning and integrates them through an adaptive gating module. Experiments on non-IID instruction-tuning datasets reportedly show improved personalization and preserved general knowledge compared with existing federated baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important and underexplored problem of enabling personalization for large MoE-based LLMs in federated settings.\n\n2. The motivation (preserving pretrained knowledge and avoiding high communication cost) is sound and practically relevant.\n\n3. The overall framework is well-written and supported with some empirical evaluation."}, "weaknesses": {"value": "1. The term “expert grafting” is semantically misleading. In biology, grafting refers to physically attaching a part of one organism onto another to form an integrated system, such as the work FedGraft [R1]. In this work, however, the method only selects or prunes pretrained experts and attaches a small adapter for local personalization. This is conceptually closer to adapter-based personalization or lightweight expert extension, rather than genuine “grafting”.\nUsing an inaccurate metaphor may cause conceptual confusion and oversell the contribution. The authors should revise the terminology to reflect the actual mechanism.\n\n2. Despite the new terminology, the proposed mechanism closely resembles prior personalized adapter or LoRA-style methods, such as [R2-R4], where clients retain frozen pretrained parameters and inject small local modules for adaptation. The authors need to compare and discuss with relevant baselines.\n\n3. Although the paper claims to address personalized federated learning (PFL), it does not compare the proposed method with any state-of-the-art personalized FL baselines or recent MoE-based FL methods, such as [R5-R8]. This omission severely limits the credibility of the claimed effectiveness. Without such comparisons, it is difficult to assess whether the proposed approach truly advances the SOTA.\n\n4. The proposed method shows limited performance improvement and even underperforms some baselines under non-IID settings.\n\n5. The evaluation focuses on one dataset (Databricks-Dolly-15K) and one model (Qwen1.5-MoE-A2.7B). There is no analysis of scalability (datasets, model, and different numbers of clients) and ablation on pruning thresholds.\n\n6. Lacks of convergence analysis.\n\n[R1] FedGraft: Memory-Aware Heterogeneous Federated Learning via Model Grafting\n\n[R2] FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations\n\n[R3] Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources\n\n[R4] Selective Aggregation for Low-Rank Adaptation in Federated Learning\n\n[R5] GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning\n\n[R6] Exploiting Shared Representations for Personalized Federated Learning\n\n[R7] FedMoE: Personalized Federated Learning via Heterogeneous Mixture of Experts\n\n[R8] FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained Aggregation"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W6GYGAWDsK", "forum": "T4dvY146Tk", "replyto": "T4dvY146Tk", "signatures": ["ICLR.cc/2026/Conference/Submission5190/Reviewer_kmrK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5190/Reviewer_kmrK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971650264, "cdate": 1761971650264, "tmdate": 1762917936309, "mdate": 1762917936309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}