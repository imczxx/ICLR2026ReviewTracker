{"id": "97IeQrvSqb", "number": 15781, "cdate": 1758255139631, "mdate": 1759897282506, "content": {"title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models", "abstract": "Vision-Language Models (VLMs) extend large language models with visual reasoning capabilities but remain vulnerable to jailbreak attacks. Existing multimodal red-teaming methods largely rely on brittle templates, operate in single-attack settings, and expose only narrow modes of vulnerability. To address these limitations, we introduce VERA-V, a variational inference framework that recasts multimodal jailbreak discovery as learning a joint posterior distribution over paired text-image prompts. This probabilistic view enables the generation of stealthy, coupled adversarial inputs that bypass model guardrails. We train a lightweight attacker to approximate the posterior, allowing efficient sampling of diverse jailbreaks and providing distributional insights into vulnerabilities. VERA-V further integrates three complementary strategies: (i) typography-based text prompts that embed harmful cues, (ii) diffusion-based image synthesis that introduces adversarial signals, and (iii) structured distractors to fragment VLM attention. Experiments on HarmBench and HADES benchmarks show that VERA-V consistently outperforms state-of-the-art baselines on both open-source and frontier VLMs, improving up to 53.75\\% ASR over the best baseline on GPT-4o.", "tldr": "", "keywords": ["Vision-language models", "Jailbreaking", "Red-teaming"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2bae50f0609fb03189ff5440b8eaebef559af883.pdf", "supplementary_material": "/attachment/400addb93516e46ee50affb3e2d1eaf13595472b.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces VERA-V, a variational inference framework for jailbreaking VLMs. It learns a joint distribution of adversarial text–image prompts to generate diverse and stealthy jailbreaks. The method combines typography-based text rendering, diffusion-generated images, and structured distractors to bypass safety filters, optimized through feedback-driven variational learning. Experiments on HarmBench and HADES show up to 53.75% higher attack success rates and lower toxicity detection compared to prior methods like CS-DJ and HADES, demonstrating scalable and transferable multimodal attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear, and the paper is easy to follow\n2. Demonstrate superior empirical performance over the baselines\n3. Comprehensive evaluation on 4 SOTA VLMs,"}, "weaknesses": {"value": "Main Concerns\n1. The contribution is relatively incremental. The design of VERA, typographic images, diffusion-generated images, and distractors have all has been explored in prior works. This paper mainly combines these existing ideas, making the novelty insufficient for an ICLR paper.\n2. Similar approaches exist that train attacker models based on feedback from the target model [1, 2, 3]. Although some of these works focus on LLMs, their methods can be easily extended to VLMs. The paper should clarify and demonstrate the advantage of the proposed framework over these related methods.\n\nMinor Concern\n1. The baseline comparison is limited. Since VERA-V jointly optimizes text and image inputs for attacks, more baselines that optimize both modalities are needed to better demonstrate the effectiveness of the proposed approach.\n\n[1] RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs \n\n[2] Reinforcement Learning-Driven LLM Agent for Automated Attacks on LLMs\n\n[3] AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs"}, "questions": {"value": "The authors should clearly state their contribution and distinguish it from prior works with similar ideas."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5vgfZcdWzj", "forum": "97IeQrvSqb", "replyto": "97IeQrvSqb", "signatures": ["ICLR.cc/2026/Conference/Submission15781/Reviewer_iv43"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15781/Reviewer_iv43"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754532474, "cdate": 1761754532474, "tmdate": 1762926014166, "mdate": 1762926014166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Multimodal large models are confronted with jailbreak security issues. This paper proposes Vera-V, which models the jailbreak attack generation problem as a variational inference problem of the joint posterior distribution of text-image prompt pairs, expanding the previously effective VERA framework for multimodal scenarios on plain text LLMS. Images that integrate layout rendering and diffusion guidance, combined with structured interference, distract the model's attention. The effectiveness of VERA-V was verified on multiple datasets and VLMS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The motivation of this article is clear. It focuses on the jailbreak vulnerability of multimodal large models, expands the pure text method to multimodal scenarios, compares the limitations of one-time attack generation methods, and proposes iterative optimization that provides feedback.\n\n2.VERA-V learns to generate paired adversarial prompts through interactive feedback with the target VLM. After dual-path processing of text (typesetting and rendering) and image (adversarial signal generation), structured interference terms are added to the image, and then the prompt distribution is iteratively optimized based on the results, enabling the attacker model to continuously learn and generate effective attack strategies.\n\n3.This paper is well-experimental. Four mainstream VLMS such as Qwen were tested on the HarmBench and HADES benchmarks, demonstrating the effectiveness of VERA-V. The attack migration capability test across VLM models demonstrated the generalization of VERA-V.\n\n4.This article presents the prompt templates used in the VERA-V framework to guide the attacker model, including the relevant requirements for role setting, task-driven, and format constraints. Combined with the typical attack prompt pairs generated in practice, it clearly shows how to guide the language model to generate adversarial prompt pairs. This article presents the complete process and effects in real attack cases."}, "weaknesses": {"value": "1.This paper conducted thorough ablation experiments, including the influence of image composition, attack models, and evaluation models. It is possible to add the contrast effects of different approaches such as Typography transformation, Visual distraction strategy and Diffusion-based image generation. And the analysis of the ablation experiment requires more in-depth insights.\n\n2.Table3 presents the cross-model attack effect of prompts, which is a proof of the effectiveness of the VERA-V method. However, I have some doubts about the experimental results. For instance, the prompts generated by two GPT models show relatively good effects on other models. And the performance of other models on GPT-4o is better than that on GPT-4O-MINI? It is hoped that the author can provide more detailed analysis and explanations. In addition, the dataset and the comparison model can be appropriately increased."}, "questions": {"value": "1.Different forms of Typography transformation, Visual distraction strategy and Diffusion-based image generation ablation experimental results can be added. Conduct a more in-depth analysis of the ablation experiment.\n\n2.Provide a more thorough explanation and analysis of the results in Table 3. Appropriately add comparative datasets and models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dVf40KqQaL", "forum": "97IeQrvSqb", "replyto": "97IeQrvSqb", "signatures": ["ICLR.cc/2026/Conference/Submission15781/Reviewer_DirK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15781/Reviewer_DirK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814022421, "cdate": 1761814022421, "tmdate": 1762926013812, "mdate": 1762926013812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VERA-V, a black-box multimodal red-teaming framework that frames jailbreak discovery as variational inference over paired text–image prompts. An attacker LLM with a LoRA adapter learns a joint distribution $q_{\\theta}(x_t, x_v)$ of latent text and image prompts, which are converted into (i) typographic renderings, (ii) diffusion-generated images, and (iii) structured visual distractors. The method optimizes an ELBO where the intractable likelihood of “harmful output” is replaced by a judge model’s continuous harmfulness score, and is trained with REINFORCE."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1: The combination of typography (explicit cues), diffusion-generated images (implicit cues), and distractors (attention fragmentation) forms a coherent and novel attack strategy\n\n2: The proposed attacker is flexible to be continuously optimized by leveraging the feedback from the judge model."}, "weaknesses": {"value": "1: This work appears to offer limited technical novelty, as it can largely be regarded as an incremental extension of VERA. The overall framework of VERA-V inherits most of its structure and methodology from VERA, raising concerns about the depth of innovation..\n\n2: The intuitive explanation — combining explicit and implicit adversarial cues with distractors to fragment attention — is reasonable and conceptually appealing. However, the paper provides little mechanistic evidence to substantiate this claim. Analyses such as attention visualization, saliency mapping over visual/text tokens, or safety-layer activation tracking would greatly strengthen the argument and provide empirical grounding for the proposed mechanism."}, "questions": {"value": "See Weakness 1 and 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0rZscI3dyY", "forum": "97IeQrvSqb", "replyto": "97IeQrvSqb", "signatures": ["ICLR.cc/2026/Conference/Submission15781/Reviewer_BurA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15781/Reviewer_BurA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982617820, "cdate": 1761982617820, "tmdate": 1762926013266, "mdate": 1762926013266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VERA-V, a variational-inference-based multimodal jailbreak framework that jointly optimizes typographic text, diffusion-generated images, and distractors to craft composite adversarial inputs, achieving a 67.75 % attack-success rate against GPT-4o."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Solid engineering: the framework integrates typography, diffusion and distractors into an end-to-end pipeline (with a LoRA attacker and judge feedback loop) and is systematically implemented.\n\n2. Broad experimental coverage: evaluated on two datasets and four VLMs (including GPT-4o), demonstrating transferability and scalability; the results offer useful reference points.\n\n3. Introduces a “distributed red-team” perspective: emphasizes the paradigm shift from single attacks to distributional exploration. While not original (VERA already proposed it), the paper makes a first attempt in the multimodal setting."}, "weaknesses": {"value": "1. Limited methodological novelty: The core framework is a direct port of VERA.\nVariational inference, REINFORCE optimization, and the LoRA attacker are all lifted unchanged; the paper merely moves from single-modal to multimodal inputs. It neither argues why this extension is non-trivial nor provides any theoretical justification for the necessity or benefit of a cross-modal joint model.\n\n2. Misleading “stealth” evaluation:\nTable 4 uses an “image-toxicity detection rate” as the stealth metric, but the detectors (Appendix G) are exclusively image-based (e.g., NSFW-I). Typographic cues in the text channel are ignored. A real safety pipeline would run OCR + text filtering; the authors test no such stack, thereby over-estimating stealthiness.\n\n3. Incomplete ablation and faulty attribution:\nTable 5 shows that “two typography images” alone reach 70 % ASR, only 10 % below the full VERA-V (80 %). The authors credit “diffusion + typography synergy,” yet never ablate a single-typography-image + distractors condition (closer to CS-DJ). The true driver may simply be “distractors + multiple typography,” not the diffusion component.\n\n4. Defense perspective completely absent:\nDespite a large body of published VLM jailbreak defenses, the paper tests none. It only evaluates the stealth of its adversarial prompts; the reported toxicity-detection rates have no direct bearing on whether the attack would succeed against an actual defensive system.\n\n5. Joint-posterior modeling lacks theoretical or empirical necessity:\nThe authors claim “learning a joint posterior over (xₜ, xᵥ)” is their key innovation, but Appendix E Table 6 shows Best-of-N (no joint learning) at 8 % ASR versus VERA-V at 66 %. However, Best-of-N uses a frozen Vicuna-7B attacker (no LoRA fine-tuning), while VERA-V is fully fine-tuned. The performance gap likely stems from fine-tuning alone, not from the variational framework."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mFN0z1CaaI", "forum": "97IeQrvSqb", "replyto": "97IeQrvSqb", "signatures": ["ICLR.cc/2026/Conference/Submission15781/Reviewer_fkg4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15781/Reviewer_fkg4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762924597506, "cdate": 1762924597506, "tmdate": 1762926012817, "mdate": 1762926012817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}