{"id": "4J3kkHI6m5", "number": 23224, "cdate": 1758340982399, "mdate": 1759896825429, "content": {"title": "ReIn: Conversational Error Recovery with Reasoning Inception", "abstract": "Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose **Reasoning Inception (ReIn)**, a test-time intervention method that *plants* an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.", "tldr": "We propose test-time reasoning intervention method under constraints where parameter and prompt are not encouraged.", "keywords": ["Conversational AI", "Agent Reasoning", "Task-oriented Dialogue"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63efd898f841f6dddd463adc47357440d093b7f0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method to address the error recovery problem in multi-round user-agent interactions, particularly when the agent must handle ambiguous and unsupported user requests. The authors frame this problem under the constraints of real-world proprietary agent deployment, where traditional training-based and prompting-based methods are often prohibitive due to resource or performance considerations. The proposed solution involves querying an external LLM to determine if a user's request matches an error from a pre-defined list. If an error is detected, the external LLM generates a corresponding recovery plan to assist the agent. Applying this module, the authors conduct experiments on an adapted $\\tau$-bench (which includes simulated ambiguous and unsupported user requests) and verify the method's effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Detecting and trying to resolve errors is a critical capability for ensuring that agents can work reliably in the wild with general users. This paper provides a good initial exploration in this largely uncharted area. The experiments also provide strong support for the effectiveness of the proposed method. Although the assumption that the agent model cannot be trained or even have its system prompt tuned seems overly restrictive, it is impressive that the proposed method remains competitive against prompting-based baselines when these constraints are relaxed to allow prompt modification. This provides additional evidence of its utility and suggests greater flexibility in application."}, "weaknesses": {"value": "There are three main problems that remain to be addressed:\n\n**Method Scalability and Efficiency**: Introducing an external LLM module to monitor every turn is inefficient and likely unscalable for massive parallel scenarios, which are common in real-world, large-scale agent deployments. This inefficiency is exacerbated when an error is detected, triggering an internal loop (with no specified upper bound) to resolve the issue. This approach could waste significant resources and cause substantial user-facing delays. While automated resolution is valuable, users may abandon the interaction if forced to wait too long.\n\n**Simulation Fidelity and Task Generalization**: The fidelity of the user simulations is doubtful, limiting generalization to real-world use. The paper uses curated initial contexts with injected errors to prompt an LLM simulator; however, in real-world interactions, errors do not necessarily occur in the initial rounds. The reliance on a pre-defined error list also seems highly restrictive. As the authors note, users often struggle to express their intents, making it unclear if their errors can be neatly categorized into a fixed list. Furthermore, it is unclear how this list generalizes to tasks beyond database operations (e.g., code-assisting, web browsing). \n\n**Insufficient Evaluation**: The evaluation needs further improvement. First, the authors only demonstrate performance in curated error contexts. It is unclear how the module performs in a normal workflow and whether it might incorrectly flag non-errors (i.e., false positives), thereby interrupting normal interactions. It is possible the module could cause the agent to \"overthink\" or defer more actions to a human, both of which would hinder deployment. Relatedly, regarding the definition of \"successfully handling errors\", in the \"unsupported request\" scenario, success is defined as escalating to a human agent. This could incentivize the agent to become \"lazy\" and defer too many requests, defeating the purpose of the agent. There should also be metrics on \"over-deferral\". Second, the role and utility of the \"think\" step are not clearly explained, and there are no ablation studies for this component."}, "questions": {"value": "1. The authors should consider narrowing the paper's framing to focus specifically on database-operation agentic use cases, as the generalization to other domains is not yet demonstrated.\n\n2. It would be helpful to include a minimal illustrative sample (a \"toy example\") of the entire workflow to aid audience comprehension.\n\nMinor: The notation in Section 3.1 is confusing. For example, on L175-176, $z_t^{(i)}$ is defined as both a normal \"control action\" and a \"termination action\". I suggest using a different notation for \"termination action,\" since $z_t^{(i)}$ is more prevalently used to denote the normal \"control token\" elsewhere in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rg267RYBz3", "forum": "4J3kkHI6m5", "replyto": "4J3kkHI6m5", "signatures": ["ICLR.cc/2026/Conference/Submission23224/Reviewer_4bnp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23224/Reviewer_4bnp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761318481344, "cdate": 1761318481344, "tmdate": 1762942566978, "mdate": 1762942566978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Authors' General Response [1/N]"}, "comment": {"value": "Dear reviewers and chairs,\n\nThank you for providing valuable feedback to our work. As we observe a few overlapping comments among reviewers, we here write a general response for them. Our additional experiments and justifications will be reflected to the camera ready version of our manuscript.\n\n---\n\n## 0. Positive remarks from reviewers\n\nWe clustered notable strengths of our work and corresponding quotes from reviewers.\n\n- **Important and realistic problem**\n    - ```Reviewer BUSn```: *The paper identifies an practically important problem*\n    - ```Reviewer QXtB```: *REIN addresses a highly practical and underexplored challenge*\n    - ```Reviewer 4bnp```: *This paper provides a good initial exploration in this largely uncharted area.*\n- **Clear writing**\n    - ```Reviewer BUSn```: *The writing is clear and concise.*\n    - ```Reviewer QXtB```: *Clarity and polish are consistently high throughout the manuscript. Figures and tables are well-integrated and meaningfully support the narrative.*\n\n- **Methodological contribution**:\n    - ```Reviewer BUSn```: *REIN's mechanism is clearly formalized with Algorithm 1 providing precise implementation details.*\n    - ```Reviewer QXtB```: \n        - *The core innovation is conceptually elegant and technically compatible with instruction hierarchies*\n        - *The paper presents a clear, rigorous, and well-structured algorithm (Algorithm 1) to formalize REIN's operational flow.*\n    - ```Reviewer 4bnp```: *it is impressive that the proposed method remains competitive against prompting-based baselines (...) This provides additional evidence of its utility and suggests greater flexibility in application.*\n- **Experimental contribution**\n    - ```Reviewer JYAX```: \n        - *Goes beyond empirical results to explore theoretical grounding through instruction hierarchy analysis*\n        - *Tests the approach across diverse models (...) Includes explicit seen/unseen error splits to verify the method generalizes beyond training examples to novel error types.*\n    - ```Reviewer QXtB```:\n        - *The empirical evaluation is particularly compelling.*\n        - *Another strength lies in the careful analysis of instruction hierarchy*\n        - *It also highlights a subtle but powerful mechanism of control within the existing capabilities of LLM agents.*\n    - ```Reviewer 4bnp```: *The experiments also provide strong support for the effectiveness of the proposed method.*"}}, "id": "s5imnnHcvj", "forum": "4J3kkHI6m5", "replyto": "4J3kkHI6m5", "signatures": ["ICLR.cc/2026/Conference/Submission23224/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23224/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23224/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763631390151, "cdate": 1763631390151, "tmdate": 1763631409532, "mdate": 1763631409532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces REIN, a lightweight test-time intervention framework designed to enhance the robustness of LLM-based conversational agents by addressing user-induced dialogue errors. Unlike conventional methods that rely on fine-tuning or prompt engineering, REIN operates without altering the agent's parameters or system prompts. It employs an external inception module to detect predefined error types in the dialogue context (e.g., ambiguous or unsupported user requests) and injects a recovery plan as an initial think[...] step into the agent’s internal reasoning process. This mechanism guides the agent to make corrective decisions while respecting the instruction hierarchy.\nREIN is evaluated on a curated benchmark adapted from τ-Bench across airline and retail domains, featuring 98 dialogue sessions and 588 dialogue contexts. It demonstrates substantial improvements in task success (Pass@1) across both seen and unseen error types and consistently outperforms baselines including prompt-modifying strategies like Naive Prompt Injection and Self-Refinement. REIN is also shown to generalize to novel error scenarios and operate effectively when applied dynamically at every dialogue turn.\nKey contributions include: (1) a novel test-time recovery mechanism requiring no model changes, (2) a formal algorithmic framework (Algorithm 1) for reasoning injection, (3) a curated error recovery benchmark, and (4) extensive analysis of instruction hierarchy and dynamic triggering scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "REIN addresses a highly practical and underexplored challenge in LLM-based conversational systems: recovery from user-induced errors during multi-turn interactions. It introduces a novel and lightweight test-time intervention that avoids prompt or parameter modification, a major benefit for commercial or constrained deployment settings. The core innovation—injecting reasoning plans as think[...] steps through a tool interface—is conceptually elegant and technically compatible with instruction hierarchies used in many modern agent frameworks.\nThe paper presents a clear, rigorous, and well-structured algorithm (Algorithm 1) to formalize REIN's operational flow. This formalism not only preserves the integrity of the agent's decision process but also provides transparency and modularity. It helps explain why REIN succeeds where prompt edits or instruction overrides fail, particularly in settings where the agent's system prompt and alignment must remain untouched.\nThe empirical evaluation is particularly compelling. The authors evaluate REIN across multiple agent models, inception module sizes, and interaction scenarios. They design a diverse and realistic testbed by curating 98 dialogues with 588 total contexts derived from τ-Bench, encompassing both ambiguous and unsupported user queries. The coverage of both seen and unseen error types further demonstrates the generalizability of the approach. Notably, REIN consistently boosts task success (Pass@1) and outperforms prompt-editing baselines like Naive Prompt Injection and Self-Refinement.\nAnother strength lies in the careful analysis of instruction hierarchy. The paper shows that REIN’s strategy of inserting plans as tool outputs respects the system-user-model-tool order while still guiding the agent’s behavior. This sidesteps potential safety and reliability issues that arise when manipulating system prompts. It also highlights a subtle but powerful mechanism of control within the existing capabilities of LLM agents.\nThe authors conduct additional robustness checks, including dynamic turn-by-turn ReIn activation and performance under unseen error categories, adding confidence in REIN's practical utility. The paper also commits to releasing its benchmark and code, which will facilitate reproducibility and community adoption.\nClarity and polish are consistently high throughout the manuscript. Figures and tables are well-integrated and meaningfully support the narrative. Taken together, these qualities make REIN a timely and important contribution to research on robust, deployable conversational agents."}, "weaknesses": {"value": "While REIN is a valuable and thoughtfully executed contribution, there are a few areas where the current study could be expanded or improved. First, REIN depends on a predefined taxonomy of error types and their corresponding recovery strategies. This reliance introduces a knowledge engineering bottleneck: new domains or emerging failure modes may require manual updates to the error library and inception prompt. Although the method generalizes to some unseen errors, its ability to adapt autonomously to novel or compound issues remains limited.\nSecond, the evaluation focuses on synthetic and curated failure scenarios using simulated user interactions. While these are carefully designed and realistic, they do not fully capture the complexity and unpredictability of real human users. A user study or online deployment would strengthen the evidence that REIN can handle spontaneous and noisy dialogue situations effectively.\nThird, the computational cost and latency introduced by REIN are not fully addressed. Since the inception module must be queried on each turn, using large models (e.g., Claude or GPT) could double response time and significantly increase inference costs. The authors partially address this with ablations on smaller inception models, but further analysis of performance–cost tradeoffs would be valuable.\nLastly, the scope of comparative baselines could be broadened. While prompt-injection and self-refinement are reasonable choices, additional baselines like simple system prompt modifications or rule-based fallbacks could contextualize the gains more thoroughly. Testing REIN with smaller or open-source base agents would also help assess its generality across deployment settings."}, "questions": {"value": "1. How does REIN handle compound errors that may involve multiple overlapping issues (e.g., ambiguity and unsupported action in the same user turn)?\n2. Have you evaluated the precision and recall of the inception module's error detection, particularly in mixed or error-free dialogues?\n3. Can REIN scale effectively with smaller inception models (e.g., 7B or below) while maintaining acceptable success rates?\n4. Do you envision a semi-automated approach to extending the error taxonomy and associated recovery plans for new domains?\n5. How does the REIN mechanism interact with existing fallback strategies or dialogue safety protocols present in commercial agents?\n6. Could you clarify how your approach performs in longer dialogue chains with cascading or unresolved errors?\n7. Did you explore injecting recovery guidance as user-side clarification requests rather than tool-based thoughts, and if so, how did agents respond?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3yEpiBv2TA", "forum": "4J3kkHI6m5", "replyto": "4J3kkHI6m5", "signatures": ["ICLR.cc/2026/Conference/Submission23224/Reviewer_QXtB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23224/Reviewer_QXtB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711602648, "cdate": 1761711602648, "tmdate": 1762942566731, "mdate": 1762942566731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReIn (Reasoning Inception), a test-time intervention method for conversational AI error recovery. The core idea is to \"inject\" recovery reasoning into an agent's internal thought process without modifying model parameters or system prompts."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Structured Framework with Theory**: Provides systematic error classification across 6 types under 2 categories (ambiguous vs unsupported requests). Goes beyond empirical results to explore theoretical grounding through instruction hierarchy analysis, showing how proper tool definitions enable safe bypass for error recovery purposes.\n- **Verified Cross-Model Generalization**: Tests the approach across diverse models (Claude, Mistral, Llama) in both agent and inception roles. Includes explicit seen/unseen error splits to verify the method generalizes beyond training examples to novel error types."}, "weaknesses": {"value": "- **Synthetic Data Reliability and Limited Coverage**:\n    - **Oversimplified scenarios**: Error contexts are LLM-generated rather than from real users. Real humans produce messier, more varied, and less structured errors. The synthetic generation process likely filters out the complex edge cases that actually break production systems.\n    - **Short conversation bias**: Only 3-turn initial contexts tested despite user simulator instability in longer dialogues. Real customer service routinely handles 10+ turn conversations where errors compound and context becomes more complex.\n    - **Selection bias through filtering**: Heavy reduction from τ-Bench (50→27 airline, 115→71 retail) suggests difficult scenarios were removed, artificially inflating performance on remaining cases.\n    - **Insufficient scale and diversity**: 98 conversations across 2 domains is tiny. Production systems handle thousands of daily conversations spanning dozens of domains, each with unique error patterns and recovery needs.\n- **Evaluation Methodology Lacks Depth**:\n    - **Oversimplified success metric**: Pass@1 only checks final correctness, ignoring recovery quality, efficiency, and user experience.\n    - **Low annotation reliability**: Cohen's κ of 0.36-0.42 indicates only moderate agreement between evaluators. This suggests either unclear task definitions or inherent evaluation ambiguity, casting doubt on the validity of reported results.\n    - **Missing critical metrics**: No measurement of recovery time, dialogue length, user frustration indicators, escalation rates, or cost per resolution. These matter more than binary success in real deployments.\n- **Limited Error Coverage and Simplistic Recovery Strategies**:\n    - **Narrow taxonomy**: Only 6 error types when production systems encounter hundreds of patterns, many domain-specific or compositional. The framework's scalability to realistic error diversity remains undemonstrated and likely faces fundamental challenges.\n    - **Binary recovery actions**: Only 2 strategies (report vs transfer to human). Real systems require clarifying questions, partial solutions, alternatives, and varied escalation paths. Transfer-to-human as a strategy introduces new variables like human labor costs and makes system necessity harder to justify through cost-benefit analysis.\n    - **Single-turn limitation**: Cannot handle multi-step recovery patterns like detect → clarify → confirm → execute that are common in real error scenarios.\n    - **High variance**: Acknowledged performance variability suggests unreliability even within the limited tested scope.\n    - **Manual domain adaptation**: Each new domain requires custom error definitions and recovery plans, limiting practical scalability.\n- **Detection Reliability Concerns**:\n    - **Missed detections**: Small models achieve only 81-93% activation rates, missing 7-19% of errors. The paper provides no breakdown of false positives vs false negatives, which have very different user experience and cost implications.\n    - **Extreme brittleness**: Section 4.6 shows 0% performance without proper tool definitions. This reveals the method is highly fragile and strongly dependent on careful tool design.\n    - **Unexplained failures**: Appendix M documents cases where agents ignore injected reasoning, but provides no systematic analysis of when or why this happens, making it impossible to predict or prevent these failures.\n    - **Contradictory claims**: Tool dependency undermines the \"no system modification\" motivation since adding specialized tools requires system changes, validation, and ongoing maintenance equivalent to prompt modifications.\n- **Insufficient Baseline Comparisons**:\n    - **Weak alternatives**: NPI and single-iteration Self-Refine don't represent realistic deployment options. Stronger baselines like multi-iteration refinement, RAG-based error retrieval, constrained generation, or tree-of-thought reasoning are missing.\n    - **Limited model diversity**: Primarily Anthropic models tested. No GPT-4/4o evaluation. Sparse open-source coverage prevents drawing strong cross-platform conclusions.\n    - **No component ablations**: Unclear whether the full inception architecture is necessary or if simpler variants achieve comparable performance at lower cost.\n    - **Missing comparable methods**: No comparison with other test-time interventions like RAG-based error handling that could achieve similar goals with different trade-offs. Comprehensive comparison would clarify where ReIn provides unique value versus adding unnecessary complexity.\n- **Production Deployment Viability Unclear**:\n    - **Cost overhead**: Running both inception module and base agent roughly doubles inference cost per turn, but no cost-benefit analysis demonstrates when this overhead is justified versus simpler alternatives.\n    - **Latency concerns**: Sequential detection → plan generation → injection adds processing steps, but no measurements quantify the latency impact on time-sensitive applications like real-time chat.\n    - **Monitoring challenges**: Injected reasoning is invisible to standard logging and debugging tools. When recovery fails, diagnosing whether the issue is in detection, planning, or execution becomes difficult.\n    - **Maintenance burden**: Requires ongoing updates to error taxonomies, recovery plan mappings, and tool definitions as systems evolve. This engineering overhead may approach or exceed the cost of direct system modifications."}, "questions": {"value": "1. What happens when you test on actual user logs instead of synthetic conversations? How much does performance drop?\n2. As error types grow from 6 to 50+, does detection accuracy collapse? Is there a complexity limit?\n3. Can ReIn be extended to inject reasoning multiple times across turns? \n4. At what error frequency does doubling inference cost become worthwhile versus simpler fixes? Need break-even analysis.\n5. What are the actual false positive/negative rates? How do they affect user experience differently?\n6. How much work goes into designing tools that make Section 4.6's 0% result not happen? Isn't this just moving the modification burden?\n7. How does ReIn compare to RAG retrieval of similar error cases or multi-turn self-correction? These seem less complex.\n8. Why do agents ignore injected reasoning in Appendix M? Can you predict when this happens?\n9. What happens in realistic 15-20 turn interactions where the user simulator breaks down?\n10. How much manual work is needed per new domain? Does the method actually solve generalization if every domain needs custom setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AvXPCSNfrf", "forum": "4J3kkHI6m5", "replyto": "4J3kkHI6m5", "signatures": ["ICLR.cc/2026/Conference/Submission23224/Reviewer_JYAX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23224/Reviewer_JYAX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898325747, "cdate": 1761898325747, "tmdate": 1762942566512, "mdate": 1762942566512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focus on dialogue error recovery for tool-using agents based on large language models under realistic conditions where model parameters and system prompts cannot be modified. The authors propose a test-time intervention method called Reasoning Injection (REIN), which injects an external reasoning module into the agent's internal decision-making process to guide error recovery without modifying parameters or prompts. \nAn external inception module detects predefined error types—including ambiguous requests (e.g., unclear pronoun references, multiple interpretations, contradictions) and unsupported requests (e.g., unavailable operations, parameters, or domains), and generates recovery plans implemented as JSON-schema tools. These plans are injected as a \"thinking\" step before the agent performs action sampling.\nEvaluated on an improved version of the τ-Bench benchmark dataset consisting of 98 manually constructed conversations with 588 dialogue contexts, REIN improves task completion rate (Pass@1 metric) across various agent and injection module combinations, generalizes to unseen but related errors, and outperforms prompt modification baseline methods while avoiding costly re-validation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies an practically important problem: error recovery under realistic operational constraints that prohibit model fine-tuning and prompt modification. \n\n2. The writing is clear and concise. REIN's mechanism is clearly formalized with Algorithm 1 providing precise implementation details."}, "weaknesses": {"value": "1. This paper only evaluates six predefined error types across two domains (airline, retail), representing only a small fraction of real-world conversational failures. \n\n2. The evaluation heavily relies on LLM as judge. It lacks human study/evaluation of recovery quality, conversational naturalness, or user satisfaction."}, "questions": {"value": "How would REIN scale to production systems with different scenarios, error types and tools? \n\nSince conversational systems typically serve human users, can you provide human evaluations of REIN's recovery quality? eg, Do human users think REIN-recovered conversations as more natural, helpful, or trustworthy? How do humans rate recovery strategies for different error types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mwLMHjzCi7", "forum": "4J3kkHI6m5", "replyto": "4J3kkHI6m5", "signatures": ["ICLR.cc/2026/Conference/Submission23224/Reviewer_BUSn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23224/Reviewer_BUSn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979690427, "cdate": 1761979690427, "tmdate": 1762942566313, "mdate": 1762942566313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}