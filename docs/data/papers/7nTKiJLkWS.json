{"id": "7nTKiJLkWS", "number": 19162, "cdate": 1758294004064, "mdate": 1763406046744, "content": {"title": "Efficient and Sharp Off-Policy Learning under Unobserved Confounding", "abstract": "We develop a novel method for personalized off-policy learning in scenarios with unobserved confounding. Thereby, we address a key limitation of standard policy learning: standard policy learning assumes unconfoundedness, meaning that no unobserved factors influence both treatment assignment and outcomes. However, this assumption is often violated, because of which standard policy learning produces biased estimates and thus leads to policies that can be harmful. To address this limitation, we employ causal sensitivity analysis and derive a semi-parametrically efficient estimator for a sharp bound on the value function under unobserved confounding. Our estimator has three advantages: (1) Unlike existing works, our estimator avoids unstable minimax optimization based on inverse propensity weighted outcomes. (2) Our estimator is semi-parametrically efficient. (3) We prove that our estimator leads to the optimal confounding-robust policy. Finally, we extend our theory to the related task of policy improvement under unobserved confounding, i.e., when a baseline policy such as the standard of care is available. We show in experiments with synthetic and real-world data that our method outperforms simple plug-in approaches and existing baselines. Our method is highly relevant for decision-making where unobserved confounding can be problematic, such as in healthcare and public policy.", "tldr": "We develop novel bounds on the policy value under unobserved confounding using the marginal sensitivity model, and derive a semiparametrically efficient estimator.", "keywords": ["policy learning", "causal inference", "unobserved confounding", "partial identification"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2265169c4e025c48eff4ebafeab278e24f8a3586.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper derives closed-form sharp bounds for policy value under MSM, plus a semi-parametrically efficient estimator. It avoids unstable minimax/IPW and proves optimal confounding-robust policy learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Closed-form sharp bound for value under MSM\n- One-step bias-corrected estimator hits the efficiency bound\n- Learning guarantees to the optimal confounding-robust policy"}, "weaknesses": {"value": "- The EIF and one-step estimator rely on quantiles $F_{x,a}^{-1}(\\alpha_{\\pm})$. You do not state standard conditions ensuring pathwise differentiability.\n- You claims the estimator “is semi-parametrically efficient” and points to D.2, which provides an influence function expression and cites a chain-rule lemma. But you never identify the canonical gradient in the nonparametric model nor verify your influence function equals it.\n- Theorem 4.4 needs a uniform bound, but the current version is pointwise in $\\pi$.\n- The nuisance $\\eta$ in (14) includes the quantiles, but your EIF contains terms like $(\\Delta-\\alpha)F^{-1}$ that rely on differentiability of these nuisances. Please clarify.\n- Before Theorem 4.4, you write “parametric policy classes (e.g., neural networks) have vanishing $R_n(\\Pi)\\in O(n^{-1/2})$\". For neural networks, this needs norm constraints, otherwise $R_n$ need not decay at root-n rate."}, "questions": {"value": "- In Algorithm 1, Step 6 says Estimate $V^{+,*}$ as in (2), but (2) just defines the propensity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VfdRdvPycZ", "forum": "7nTKiJLkWS", "replyto": "7nTKiJLkWS", "signatures": ["ICLR.cc/2026/Conference/Submission19162/Reviewer_P2Zy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19162/Reviewer_P2Zy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801587376, "cdate": 1761801587376, "tmdate": 1762931171823, "mdate": 1762931171823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work offers a method for off-policy learning under unmeasured confounding, whereby unmeasured covariates can jointly affect treatment decisions and the outcome. Concretely, the authors propose a one-step bias-corrected estimator that estimates “sharp” (i.e., tightest possible) bounds around the true policy value under the marginal sensitivity model. These estimated bounds are then used for downstream policy learning. The authors show that their bias-corrected estimation approach is efficient — i.e., obtains the lowest variance among unbiased estimates — and relies upon a simplified minimization objective as compared to the mini-max style objective studied in prior work. The authors validate the approach theoretically and via experiments on synthetic and real-world data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Policy learning under unmeasured confounding is an important problem with broad applications. The authors identify a key gap in the literature and address it via appropriate methods. Presentation of the work is effective: I especially appreciate Figure 2 illustrating how the concept of sharpness connects to regret. The theoretical results - including identification bounds (4.1), bias-corrected estimator (4.2), and learning guarantees (4.3) are also well suited to this problem setting. The synthetic + real-world empirical validation is also well-suited to the goals of the work."}, "weaknesses": {"value": "## Connection to prior work & significance\n\nIn general, the authors provide solid coverage of prior work and appropriately situate the contribution in the literature. However, this work can be viewed as a targeted improvement on top of the basic framework established in Kallus & Zhou (2018a; 2021). While I still believe such work is valuable and worthy of publication, this somewhat limits the significance of the results.\n\nMore specifically, it would be helpful if the authors could provide more detailed technical discussion of the differences with (Kallus & Zhou, 2018a; 2021). How does instability in IPW weights propagate up to the estimated policy value/regret, and how is this solved by the proposed approach? Similarly, Kallus & Zhou (2018a; 2021) also show that the regret interval obtained under their proposed approach is sharp. While lines 211-226 offer helpful initial discussion, adding additional technical clarity would strengthen the discussion for the reader. \n\nFurther, Rambachan, Coston, and Kennedy (2022) derive sharp bounds for the policy value under a related Mean Outcome Sensitivity Model (MOSM), which they then estimate via a doubly-robust method. The authors further show that bounds on the MSOM imply MSM bounds. It would be helpful to outline similarities and differences with this approach. To start, I think the method proposed in this work generalizes to non-binary actions, and the bounds in  Rambachan, Coston, and Kennedy (2022) may not remain sharp w.r.t. the MSM after converting from the MOSM framework used in this work. \n\n[1] Robust Design and Evaluation of Predictive Algorithms under Unobserved Confounding, https://arxiv.org/abs/2212.09844, Ashesh Rambachan, Amanda Coston, Edward Kennedy\n\n\n## Empirical validation\n\n\nThe general empirical validation of the work is sound, and the authors demonstrate that the proposed approach yields a benefit over relevant baselines. However, I do have several questions about the empirical validation. \n\nRelated to my point above, could the authors report experiments which illustrate the mechanism by which the proposed approach obtains improved bounds over Kallus & Zhou (2018a; 2021)? For instance, can the authors illustrate how the efficiency of the estimator yields tighter finite-sample bounds, and in turn, improves downstream policy learning? Or, similarly, that error in IPW weights propagates down to learned policies? Evidence along these dimensions would help the reader understand why the proposed method is necessary over Kallus & Zhou (2018a; 2021).\n\nFurther, it appears in several of the empirical results that the proposed method obtains high variance across runs. I find this surprising given that the estimation approach should in principle reduce variance in estimated policy value bounds. For example, confidence intervals are quite wide with Efficient + sharp as compared to baselines. We also see similar behavior in Figure 3. Could the authors explain why this is the case, and also include Kallus & Zhou (2018a; 2021) in Figure 3 for a clear comparison of variance across runs? \n\nAdditionally, while comparing regret against a fully randomized policy is a reasonable starting point, this seems overly simplified for a real-world experiment, especially because non-randomized baseline policies may yield more challenging distribution shift. Can the authors also report comparisons against other baseline policies? \n\nFinally, given that the proposed estimator depends on learned nuisance functions, can the authors report details surrounding the procedure used to fit and select nuisance functions used to construct the doubly-robust estimates? \n\nOverall, I see this work as providing a valuable contribution but do have significant concerns. I am open to re-considering my score if these concerns regarding significance and empirical validation are appropriately addressed."}, "questions": {"value": "## Questions:\nSee questions raised above. Additionally:\n- Kallus and Zhou also require \"Strong overlap\" to hold w.r.t. the true propensity - i.e., exists $ν > 0$ such that $e_a(x, u) ≥ ν, \\; \\forall a \\in A$. Is such an assumption also needed here given the use of the MSM? \n- Algorithm 2: Can cross-fitting be performed to improve sample efficiency? \n- Figure 5: Why are other approaches not sensitive to the choice of sensitivity parameter? Can you show results for the full range, starting at $\\Gamma=1$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YerWnvzyxi", "forum": "7nTKiJLkWS", "replyto": "7nTKiJLkWS", "signatures": ["ICLR.cc/2026/Conference/Submission19162/Reviewer_d6rY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19162/Reviewer_d6rY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943598329, "cdate": 1761943598329, "tmdate": 1762931171338, "mdate": 1762931171338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of unobserved confounding in offline policy learning. They assume that the unobserved confounding satisfies the marginal sensitivity model (Tan, 2006), which is often used in the sensitivity analysis literature (Aronow and Lee 2013, Miratrix et al. 2018, Zhao et al. 2019, Yadlowsky et al. 2018, Kallus et al. 2018, Kallus and Zhou 2020)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and easy to understand.\n- The main contribution of this paper is a semiparametrically efficient estimator for offline robust policy learning problem, arguing that the approach of Kallus and Zhou 2020 may be unstable due to the dependence on inverse propensity weights. Instability of inverse propensity weights is a known problem that can lead to instability of estimators.\n- They propose a naive plug-in estimator for the optimal robust policy but note that it will suffer from first-order bias. Then, they derive the semiparametrically efficient estimator that does not suffer from the first-order bias from the estimation of nuisance components\n- The theoretical contributions are sound."}, "weaknesses": {"value": "- The problem of robust offline policy learning under the marginal sensitivity model and Rosenbaum selection model is quite well-studied, e.g. Aronow and Lee 2013, Miratrix et al. 2018, Zhao et al. 2019, Yadlowsky et al. 2018, Kallus et al. 2018, Kallus and Zhou 2020. Furthermore, other works such as Bruns-Smith and Zhou, 2023 consider dynamic policy learning. So, the problem that the authors aim to solve has limited novelty. Nevertheless, this paper does cite and reference many of the relevant works in this area and I believe they do make a technical contribution (in terms of semi-parametric efficiency of their estimator), relative to the Kallus and Zhou 2020.\n- The new estimator appears to only provide modest improvements over the naive plug-in estimator."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "00rDa4Mwg8", "forum": "7nTKiJLkWS", "replyto": "7nTKiJLkWS", "signatures": ["ICLR.cc/2026/Conference/Submission19162/Reviewer_emv2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19162/Reviewer_emv2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762278798147, "cdate": 1762278798147, "tmdate": 1762931170973, "mdate": 1762931170973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all reviewers"}, "comment": {"value": "We thank the reviewers for the comprehensive and helpful feedback on our work. \n\nTo improve our work, we added the following improvements to our revised paper (highlighted in $\\color{blue}{\\text{blue}}$):\n\n\n\n* **More discussions:** We added a **new Supplement B**, where we provide a more technical discussion on the key differences of our method from the baseline by Kallus & Zhou.\n* **New sensitivity experiment:** We added a **new experiment** in our **new Supplement C**, where we study the sensitivity of our method and the baseline with respect to **more complex treatment propensities**. For this, we use synthetic data and increase the action space. As expected, our method performs well, whereas the baseline deteriorates.\n* **New experiment on IHDP:** We added a **new experiment** in our **new Supplement D** on another dataset, the **IHDP data**. Therein, we introduce unobserved confounding and study the performance of our method against all baselines. Our results **confirm** our previous findings: our method finds the best-performing, confounding-robust policy.\n* **More references:** We **added more references** as suggested by the reviewers to our related work in our **revised Section 2,** and to the extended related work in **Supplement A**.\n* **Clarifications:** We **revised our technical assumptions** (strong overlap, standard assumptions for pathwise differentiability), and now clearly state that our improvement guarantees in Theorem 4.4 **simultaneously** hold for all policies $\\pi \\in \\Pi$.\n* We highlight the connection between the EIF, the canonical gradient, and semi-parametric efficient estimators in our proofs.\n* We made some **smaller additions** to the paper (e.g., we explained the poor baseline performance in the real-world dataset, we highlighted cross-fitting to improve sample-efficiency in Algorithm 1, and we clarified  the asymptotic behaviour of neural networks as a parametric policy class).\n\nWe are confident that, with the help of the reviews and our improvements, our work will be an important contribution to the community. Finally, we wanted to say “thank you” – we truly appreciate the help, which allowed us to revise our work and make important improvements."}}, "id": "yS63BTcOLH", "forum": "7nTKiJLkWS", "replyto": "7nTKiJLkWS", "signatures": ["ICLR.cc/2026/Conference/Submission19162/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19162/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission19162/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763407111727, "cdate": 1763407111727, "tmdate": 1763407111727, "mdate": 1763407111727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}