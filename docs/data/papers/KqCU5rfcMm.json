{"id": "KqCU5rfcMm", "number": 6564, "cdate": 1757989104716, "mdate": 1763654120120, "content": {"title": "Integrating Selective State-Space Models and Bayesian Graph Attention for Uncertainty-aware Time-Series Analysis", "abstract": "This paper presents \\textbf{BIMAMBA \\& Bayesian-MAGAC}, a unified framework that integrates bidirectional Selective State-Space Models with Bayesian Multi-head Adaptive Graph Attention Convolution for uncertainty-aware financial forecasting. The framework addresses two fundamental challenges: capturing long-range temporal dependencies across volatile market regimes while maintaining linear complexity, and learning adaptive cross-sectional structure with calibrated predictive uncertainty. BIMAMBA processes sequences bidirectionally via reversible state-space filters, extracting complementary temporal features while preserving strict causality. MAGAC constructs dynamic adjacencies through Gaussian kernel and attention blending, followed by Chebyshev spectral filtering for multi-scale aggregation. The Bayesian extension treats adjacencies and spectral filters as stochastic variables via Monte Carlo Dropout and DropEdge, yielding posterior predictive distributions with closed-form variance propagation at $\\mathcal{O}(N)$ complexity. Comprehensive evaluations on U.S. equity indices demonstrate that the architecture achieves substantial improvements in both point prediction accuracy and uncertainty calibration compared to established baselines, with statistically significant correlation between predicted uncertainty and prediction difficulty, suggesting practical utility for risk-aware decision making in financial applications.\n\\textit{The code is available on GitHub but has been hidden to preserve anonymity during the review process.}", "tldr": "Bi-Mamba & Bayesian MAGAC combines bidirectional selective state-space models with Bayesian graph attention for scalable, uncertainty-aware financial forecasting in linear time.", "keywords": ["mamba", "transformer", "bayesian networks"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2df5bc4bd83d8fe264929f03c5c341ba93bd102f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a model that uses a bidirectional Mamba backbone combined with Bayesian graph attention for financial forecasting. The authors claim their model achieves state-of-the-art performance on three financial datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper proposes a lightweight architecture that demonstrates strong performance on the evaluated financial datasets.\nSome design choices, such as the use of bidirectional encoding, are well-justified and intuitive."}, "weaknesses": {"value": "_Insufficient Citations and Details_: Several core components of the proposed model appear to be derived from prior work without proper attribution. For instance, the bidirectional Mamba architecture has been explored in numerous previous studies (e.g., [1], [2], [3]). A similar issue is present in Sections 3.2.2 and 4. Since the paper does not claim to have invented Bayesian Graph Neural Networks, it should cite the original proposers of this idea (likely [4]). Furthermore, the authors do not provide citations for the datasets used, which hinders reproducibility and disregards the contribution from dataset makers.\n\n_Lack of Ablation Studies_: The authors provide explanations for their design choices in Section 3.2.3, which is appreciated. However, these explanations remain as claims without empirical support. The paper shows that the full model show performance improvement, but it does not demonstrate the individual contribution of each component (e.g., the use of $\\beta=\\text{softmax}(\\gamma)$ versus a directly learned $\\beta$). Further, if the authors are claiming the MAGAC backbone as a novel contribution (see 3), it is critical to compare its performance against existing backbone architectures.\n\n_Confusing Terminology_: The terminology in Section 3.2 is confusing. \"Graph Attention\" is a well-established technique, first introduced in [5]. However, the description in Section 3.2.1 appears to describe standard attention mechanisms, not specifically graph attention. Similarly, while \"graph convolution\" is mentioned, no corresponding graph convolutional structure is detailed. This ambiguity makes it difficult to determine whether the authors are proposing a novel network architecture or have misconceptions about existing ones (GAT, GCN, etc.).\n\n_Other Issues_: Figure 1 need additional polish--the texts are not clear. I also believe for $A_{mn}^{g}$ and $\\tilde{A}_{mn}^{(g,s)}$, the use of $\\text{softmax}(\\exp(\\cdot))$ might be a mistake since otherwise it tends to cause instabilities (so you have $\\exp(\\exp(x))$).\nOverall, the exact contributions of the paper are unclear. For the parts that are relatively clear, there is a lack of theoretical or empirical evidence to support the authors' claims. Therefore, I recommend that this manuscript undergo significant revision.\n\n[1] Zhu, Lianghui, et al. ‘Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model’. arXiv [Cs.CV], 2024, arxiv.org/abs/2401.09417. arXiv.\n\n[2] Liang, Aobo, et al. ‘Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting’. arXiv [Cs.LG], 2024, arxiv.org/abs/2404.15772. arXiv.\n\n[3] Erol, Mehmet Hamza, et al. ‘Audio Mamba: Bidirectional State Space Model for Audio Representation Learning’. IEEE Signal Processing Letters, vol. 31, 2024, pp. 2975–2979,\n\n[4] Hasanzadeh, Arman, et al. ‘Bayesian Graph Neural Networks with Adaptive Connection Sampling’. CoRR, vol. abs/2006.04064, 2020, arxiv.org/abs/2006.04064.\n\n[5] Veličković, Petar, et al. ‘Graph Attention Networks’. arXiv [Stat.ML], 2018, arxiv.org/abs/1710.10903. arXiv."}, "questions": {"value": "The authors should directly address the specific concerns raised in the \"Weaknesses\" section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wkJdsb5eyh", "forum": "KqCU5rfcMm", "replyto": "KqCU5rfcMm", "signatures": ["ICLR.cc/2026/Conference/Submission6564/Reviewer_z7D8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6564/Reviewer_z7D8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761628470480, "cdate": 1761628470480, "tmdate": 1762918903176, "mdate": 1762918903176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors have used bidirectional MAMBA for time and Bayesian multi-head graph attention for space to perform forecasting. The work claims to preserve the causal and anti-causal dependencies across the temporal dimension."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novel architecture design \n- Good experimental results"}, "weaknesses": {"value": "- No ablation studies\n- Equations are not numbered\n- Limited number of datasets\n- MAE metric should also be reported, as RMSE can dramatically shrink components less than 1\n- The training is slow compared to other baselines\n- No paragraph to clarify notations\n- sparse citations\n- Experimental results cannot be verified"}, "questions": {"value": "1. Is there any reference on the benefits of processing the temporal sequence in both forward and reverse orders?\n2. Were any ablation studies performed that show that bidirectional MAMBA is better than regular MAMBA? (both in terms of performance and training time)\n3. How does the performance of the proposed model change with the size of the training data available? \n4. What are the confidence intervals of the reported metrics?\n5. Is the matrix P an anti-diagonal matrix of ones?\n6. How are the sequences being merged? Does '+' represent element-wise addition?\n7. Could the authors please cite some prior work on BGNN, esp. for time series forecasting?\n8. Is there a way through which the results can be verified? I do not see any link to the code.\n9. In Table 1, for which dataset is the training time reported?\n10. Could the authors please comment on the causality in the data, and if any tests were performed? See: _Pearl, J. (2009). Causality. Cambridge University Press_"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JsM9zRqRas", "forum": "KqCU5rfcMm", "replyto": "KqCU5rfcMm", "signatures": ["ICLR.cc/2026/Conference/Submission6564/Reviewer_1o2G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6564/Reviewer_1o2G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745937962, "cdate": 1761745937962, "tmdate": 1762918902822, "mdate": 1762918902822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a hybrid sequence–graph framework for equity forecasting that couples a bidirectional Mamba (Bi-Mamba) state-space encoder with a Bayesian Multi-head Adaptive Graph Attention Convolution (MAGAC) layer. The Bi-Mamba encoder processes price windows in both forward and reverse directions and feeds sequence features to MAGAC; MAGAC builds an adaptive adjacency by blending a Gaussian kernel on learnable node embeddings with attention-based scores, then applies Chebyshev spectral filtering and multi-head aggregation. A lightweight Bayesian treatment is introduced by applying MC-Dropout to node embeddings and DropEdge at inference to obtain posterior predictive means/variances optimized via heteroscedastic Gaussian NLL. Experiments on NASDAQ/NYSE/DJIA report very large gains in RMSE/IC/RIC over LSTM/Transformer/GNN baselines, with modest MACs and parameter counts claimed for the proposed model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.   A linear-time temporal encoder (Mamba) with an adaptive spectral/attention GNN is a coherent and potentially scalable combination for long horizons. The bidirectional Mamba block and residual/normalization design are clearly described.\n2.   The MAGAC construction (Gaussian + attention blend, Chebyshev supports, head mixing) is laid out step-by-step with equations that are easy to implement.\n3.    The paper spells out how MC-Dropout on node embeddings and inference-time DropEdge are used to derive mean/variance and the closed-form variance propagation through a linear head, optimizing a heteroscedastic Gaussian NLL. This is clearly written and easy to reproduce technically.\n4.   The paper reports MACs/params/epoch time and positions the model against Transformers with respect to efficiency (albeit for S=1). Consistent, large improvements on all benchmarks, with excellent calibration.\n5.   Consistent, large improvements was reported on all benchmarks, with excellent calibration."}, "weaknesses": {"value": "W1. The reported IC values are extraordinarily high for daily single-day returns (e.g., IC=0.9413 on NASDAQ). The paper defines IC/RIC as correlations with realized single-day returns, but offers no safeguards against common leakage vectors (e.g., improper chronological splits, cross-sectional standardization using future info, or target leakage through feature engineering). Please justify these numbers or audit the pipeline.\nAdditionally, the evaluation uses a fixed L=5 with bidirectional processing; while the text claims “causality is preserved,” there is no precise description of how labels are formed and how windows are cut to guarantee no look-ahead within the window (e.g., predicting $r_{t+1}$ from $[t−4,t]$). Provide a rigorous data construction diagram and masking rules.\nW2. The dataset description states N=82 features per day; however, the graph notion (nodes = assets) is not specified (how many tickers? which universe? how are nodes aligned over time? dynamic graph cadence?). The text also equates $E = d_{model}$ with “number of graph nodes (model width),” which is dimensionally inconsistent—E is typically feature width, not the number of assets. Later equations sum over E as if it were the node count. This confusion must be resolved with explicit tensor shapes (batch, nodes, time, channels) and a clear asset universe size.\nW3. The paper claims “well-calibrated uncertainty” and “without inflating computational cost,” yet no calibration metrics (NLL comparisons, CRPS, PIT/QQ plots, ECE, PICP/ACE) are reported. Moreover, Table-reported MACs are for S=1 while the method elsewhere enables S=10 at inference; the “no overhead” claim is therefore misleading. Also, DropEdge is applied only at inference, effectively evaluating a different model than trained. Provide proper calibration evaluation and fair compute accounting for the posterior estimates.\nW4. There is no ablation disentangling the gains from bidirectionality, Gaussian+attention blend, Chebyshev order, number of heads, Bayesian sampling, or DropEdge. Without such analysis, the source of improvements is unclear.\nW5. The authors acknowledge restrictions to daily closing prices and the need for theory on Bayesian MAGAC stability, but neither robustness checks (e.g., regime shifts, crisis periods) nor sensitivity analyses are presented."}, "questions": {"value": "Please see above W1-W5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b6Xo4evNmf", "forum": "KqCU5rfcMm", "replyto": "KqCU5rfcMm", "signatures": ["ICLR.cc/2026/Conference/Submission6564/Reviewer_Pwni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6564/Reviewer_Pwni"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799570907, "cdate": 1761799570907, "tmdate": 1762918902534, "mdate": 1762918902534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Bi-Mamba Bayesian-MAGAC, combining bidirectional selective state-space models (Bi-Mamba) with Bayesian multi-head adaptive graph attention convolution (MAGAC) for financial time-series forecasting. Bi-Mamba processes sequences bidirectionally with linear complexity, while MAGAC constructs adaptive graph topologies and performs spectral filtering. Experiments on NASDAQ, NYSE, and DJIA show improvements over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The integration of linear-time Mamba for temporal dynamics with graph convolution for cross-asset relationships effectively addresses both long-range dependencies and inter-asset correlations in financial forecasting.\n\n2. The Bayesian treatment captures both epistemic and graph structural uncertainty through MC-Dropout and DropEdge, with closed-form variance propagation maintaining computational efficiency."}, "weaknesses": {"value": "1. Evaluation uses only three datasets with implausibly high IC values (~0.94) compared to baselines (~0.2-0.3), raising concerns about potential data leakage or experimental setup issues.\n\n2. The code is not available for reproducibility."}, "questions": {"value": "See the Weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tzfZGYei73", "forum": "KqCU5rfcMm", "replyto": "KqCU5rfcMm", "signatures": ["ICLR.cc/2026/Conference/Submission6564/Reviewer_7kbC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6564/Reviewer_7kbC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056679948, "cdate": 1762056679948, "tmdate": 1762918902284, "mdate": 1762918902284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}