{"id": "iu9dbz2lB9", "number": 11329, "cdate": 1758196419660, "mdate": 1759897588821, "content": {"title": "Difference Predictive Coding for Training Spiking Neural Networks", "abstract": "Predictive coding networks (PCNs) offer a local-learning alternative to backpropagation in which layers communicate residual errors, aligning well with biological computation and neuromorphic hardware. In this work we introduce \\emph{Difference Predictive Coding} (DiffPC), a spike-native PC formulation for spiking neural networks. DiffPC replaces dense floating-point messages with sparse ternary spikes, provides spike-compatible target and error updates, and employs adaptive threshold schedules for event-driven operation. On MNIST and Fashion-MNIST, DiffPC attains 99.3\\% and 89.6\\% test accuracy, respectively, while maintaining low spike counts during training. As such, DiffPC supports energy-efficient training and inference, offering a hardware-aligned alternative to backpropagation for neuromorphic platforms.", "tldr": "We create a spiking neural network compatible learning algorithm based on predictive coding theory", "keywords": ["Spiking neural networks", "predictive coding", "biologically plausible learning", "neuromorphic computing", "difference predictive coding", "local learning rules", "energy efficiency", "communication efficiency", "spike-based learning", "surrogate gradient alternatives"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5fb6639a71cb4cb9b657a1c9a428b69948cbc78c.pdf", "supplementary_material": "/attachment/0a915f482cca58904c66a89eb81de491fe1ba2e2.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes DiffPC, a spike-native local learning algorithm for SNNs. DiffPC fits the Predictive Coding algorithm into SNNs, replacing floating-point messages with sparse ternary spikes, providing spike-compatible target and error updates, and employing adaptive threshold schedules for event-driven operation. Experimental results have shown the effectiveness and energy efficiency of DiffPC."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed algorithm only needs local information propagation, which is much more compatible with SNNs than BP-based algorithms.\n2. The proposed algorithm propagates information through spikes during training and inference, which suits the characteristic of SNNs."}, "weaknesses": {"value": "1. There are many symbols in this paper, and some are not clearly explained, making it difficult to follow.\n2. The writing of this paper is not clear enough. See Questions for details."}, "questions": {"value": "1. Could you please introduce Predictive Coding more clearly? How to initialize the parameters such as $x_T$? From Algorithm 1, it seems there are $x_T, x_F, x_A$ for the neuron state. However, only $x_T$ and $x_F$ is introduced in Section 3.2. Does $x_A$ appear in PC? Also in Algorithm 1, the step of updating weights seems not included.\n2. I recommend the authors to place a table illustrating the meaning of each symbol. If the room is not enough in main body, you can put it in appendix.\n3. What is the intuitive behind the PC method? How the network is trained to predict classification result through PC? Please give more detailed intuition.\n4. What is the number of 'the number of approximation steps *n*' in line 426? Is it the number of optimization steps?\n5. There are two 'Energy efficiency' in 5.2 RESULTS. Please change one of them. In line 355, a full stop is missing.\n6. In Figure 2, why there are two peaks of spike activity after input change? Could the authors give some intuition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pKBOZDWqFV", "forum": "iu9dbz2lB9", "replyto": "iu9dbz2lB9", "signatures": ["ICLR.cc/2026/Conference/Submission11329/Reviewer_htVv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11329/Reviewer_htVv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761188819098, "cdate": 1761188819098, "tmdate": 1762922467672, "mdate": 1762922467672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The primary contribution of DiffPC lies in being the first predictive coding framework trained fully through spike-based event-driven mechanisms, effectively bridging the gap from theoretical feasibility to hardware-ready neuromorphic deployment. Nevertheless, the current experimental scope remains limited, and the theoretical analysis could be further strengthened. More extensive evaluations on diverse tasks and real neuromorphic hardware measurements are needed to substantiate the method‚Äôs generality and energy-efficiency advantages."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work is the first to fully discretize both target updates and error transmission in predictive coding into sparse ternary spike events, eliminating the need for floating-point communication and external ANN-based training pipelines.\n\n2. On the MNIST benchmark, DiffPC achieves a test accuracy of 99.3%, comparable to standard predictive coding and backpropagation, while delivering more than a 5,000-fold improvement in communication efficiency."}, "weaknesses": {"value": "1. The Introduction provides an insufficient and somewhat disorganized treatment of the importance of local learning. I recommend a thorough rewrite of this section to clarify the narrative and strengthen the conceptual motivation.\n\n2. The Introduction lacks a high-level articulation of the motivation for proposing DiffPC. Based solely on this section, readers cannot clearly infer what the authors aim to do, what has been accomplished, or the scholarly significance of the contributions.\n\n3. The experimental evaluation is severely limited, and results on MNIST and Fashion-MNIST are inadequate to substantiate the method‚Äôs effectiveness. At minimum, an evaluation on CIFAR-10 is recommended.\n\n4. In the Related Works section, citations to similar training frameworks should be mapped one-to-one with the corresponding methods and claims.\n\n5. There are multiple presentation issues: a symbol error at the end of line 195; missing punctuation after Equation (11); inconsistent decimal precision in the ‚ÄúAcc.‚Äù column of Table 2; and numerous other minor formatting and symbol inconsistencies, likely due to a rushed preparation. Please correct these errors throughout."}, "questions": {"value": "1. Lines 156‚Äì164 state that four innovations are introduced. However, the first contribution‚Äîternary spiking‚Äîhas already been widely adopted in SNN research. The third contribution‚Äîdynamic thresholding‚Äîhas also been extensively explored in prior SNN literature. Regarding the fourth contribution, the near-exact PCN approximation resembles a conversion or distillation process; therefore, clarification is needed on whether it should be viewed as a form of teacher‚Äìstudent distillation rather than an independent methodological innovation.\n\n2. The manuscript argues that surrogate-gradient-based backpropagation is biologically implausible due to reliance on global gradients, whereas DiffPC uses local plasticity and is thus more brain-like. However, validating DiffPC only on MLP architectures does not substantiate this claim. The experiments do not reveal whether a fully local learning mechanism can scale to larger networks. More importantly, when every layer is trained exclusively through DiffPC, the paper does not analyze how network depth or size influences final model performance.\n\n3. Although the paper emphasizes its relevance to neuromorphic tasks, no temporal benchmarks are included. Evaluation on event-driven datasets would be necessary to support this claim.\n\n4. The authors are encouraged to provide real power measurements on Loihi 2 hardware, and further discuss the quantitative advantages over PC-SNN and surrogate-gradient-based SNN training, along with a clearer articulation of the fundamental differences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7Otxipxe2C", "forum": "iu9dbz2lB9", "replyto": "iu9dbz2lB9", "signatures": ["ICLR.cc/2026/Conference/Submission11329/Reviewer_8Q9N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11329/Reviewer_8Q9N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761285422484, "cdate": 1761285422484, "tmdate": 1762922467228, "mdate": 1762922467228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a spike-based version of the Predictive Coding (PC) for training, equivalent to standard PC and called DiffPC. The method consists of tracking the changes of the continuous signal by sending \"corrective\" ternary spikes, by iteratively adjusting the \"weight\" of the spikes using an adaptative threshold. Equivalence with PC is empirically demonstrated on MNIST and Fashion-MNIST benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and easy to read;\n- The DiffPC implements a relatively classic spike coding method (based on tracking a continuous signal with spike corrections) that has already been successfully applied to backpropagation as well;\n- The method is mathematically equivalent to standard PC, modulus the final threshold value approximation."}, "weaknesses": {"value": "- The cyclic scheduler equations are not very well explained. Parameter m is not defined (equation 8).\n- The DiffPC relies on scheduling hyperparameters that may be hard to tune to get the best efficiency vs accuracy compromise. A discussion about this is missing from the main paper (although mentionned in the supplementary material);\n- Although some data on energy efficiency are provided, it is not clear of they translates to real world implementation efficiency."}, "questions": {"value": "The paper lacks a conclusion/perspective section, is it on purpose?\nRegarding implementation efficiency, could the author provide some insight or benchmark that compares standard PC and DiffPC?\n\nOverall, the quality of the paper could still be improved: more explanation/insight on the method scheduling could be provided. The paper also lacks a conclusion. Table 4 and related content could be improved, perhaps with a graph better illustrating the convergence between PC and DiffPC as the threshold becomes smaller and an absolute scale or a relative one. But the mean absolute difference is not very meaningful..."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VbXSsipdYV", "forum": "iu9dbz2lB9", "replyto": "iu9dbz2lB9", "signatures": ["ICLR.cc/2026/Conference/Submission11329/Reviewer_VdSN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11329/Reviewer_VdSN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761500153529, "cdate": 1761500153529, "tmdate": 1762922466351, "mdate": 1762922466351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes Difference Predictive Coding (DiffPC)‚Äîa spike-native reformulation of predictive coding (PC) for spiking neural networks (SNNs). DiffPC replaces dense floating-point message passing with sparse ternary spikes for both state and error communication, introduces adaptive threshold schedules to drive event-based updates, and targets on-chip feasibility by aligning operations with Intel Loihi 2‚Äôs instruction model (validated in simulator). On MNIST and Fashion-MNIST with MLP backbones, DiffPC reports up to 99.3% and 89.6% test accuracy, respectively, while dramatically cutting communicated bits per neuron during training (e.g., 0.18 bits across 120 timesteps for DiffPC-L versus 32‚Äì960 bits for backprop/PC baselines). The method is positioned as a hardware-aligned, energy-efficient alternative to backpropagation and prior PC-SNN hybrids."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Spike-native PC formulation. Clear conceptual shift from continuous residuals to ternary spike streams for both targets and errors, enabling fully spiking training and inference under local rules.\n\n2. Hardware awareness. The algorithm and simulator validation target Loihi 2 semantics, improving credibility for on-chip training pathways."}, "weaknesses": {"value": "1. Evaluation scope (datasets & models). Experiments are restricted to MNIST/Fashion-MNIST and MLP backbones. The claims of hardware-aligned efficiency would be more compelling on spatiotemporal or event-camera benchmarks, and with convolutional or recurrent SNNs typical of neuromorphic use.\n\n2. Learning rule completeness (weights). The PC weight update (Eq. 6) is reviewed, but Algorithm 1 does not explicitly show the weight-update pathway in the spike-only regime (e.g., how ternary error/state spikes translate into synaptic updates compatible with Loihi). This leaves ambiguity about exact stochasticity/quantization and memory consistency.\n\n3. Inference protocol and latency. Inference appears iterative/event-driven; it would help to quantify latency‚Äìaccuracy trade-offs (how many timesteps to reach stable predictions) and to compare against rate-coded SNNs with fixed horizons.\n\n4. Energy model granularity. Bits-per-neuron is informative, but ignores NoC hops, memory hierarchies, spike routing costs, and neuron state updates on specific hardware; without this, absolute energy claims remain provisional."}, "questions": {"value": "1. Exact synaptic update mechanics. How are weight updates computed and applied in the purely spiking formulation at run time? Is Eq. (6) implemented via accumulated spike counts/low-precision accumulators, and how are precision/overflow handled on Loihi 2? Please make this explicit in Algorithm 1 (or add an Algorithm 2 for weight updates).\n\n2. Scheduler sensitivity & autotuning. Can you provide ablations on ùëõ, ùëé, ùëá, ùëê showing accuracy, spikes/neuron, and timesteps across datasets, and discuss any rule-of-thumb for choosing them at larger scales? Table 4 addresses approximation error only.\n\n3. Hardware-level metrics. Do you have on-chip measurements (energy per inference/training iteration, latency, utilization) or realistic cycle-accurate estimates on Loihi 2 for DiffPC vs. PC-SE and surrogate-gradient SNNs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hfQTqdHwUd", "forum": "iu9dbz2lB9", "replyto": "iu9dbz2lB9", "signatures": ["ICLR.cc/2026/Conference/Submission11329/Reviewer_8biP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11329/Reviewer_8biP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762679644939, "cdate": 1762679644939, "tmdate": 1762922466002, "mdate": 1762922466002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}