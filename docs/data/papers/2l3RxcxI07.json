{"id": "2l3RxcxI07", "number": 7348, "cdate": 1758017207533, "mdate": 1763373559776, "content": {"title": "Characterising Overprecision in Black-Box LLMs: A Cognitive Science Inspired Framework", "abstract": "Overconfidence in large language models (LLMs) has attracted growing attention due to its implications for the reliability of model outputs. Most existing approaches study verbalized confidence, where LLMs are asked to state their certainty, but such methods are prone to biases and hallucinations. Inspired by the cognitive science notion of overprecision—excessive certainty in narrow interval judgments—we propose a framework for evaluating overprecision in black-box LLMs. Our protocol comprises three phases: (1) generation, where models produce numerical confidence intervals under imposed confidence levels; (2) refinement, where intervals are adjusted via aggregation or self-refinement strategies; and (3) evaluation, where outcomes are assessed using calibration and correlation metrics adapted from cognitive science. Using datasets spanning general knowledge, medical, and financial domains, we find that: (i) LLMs are systematically miscalibrated, with large gaps between imposed confidence and actual coverage; (ii) interval lengths do not scale with requested confidence, showing limited responsiveness to explicit confidence instructions; (iii) calibration quality varies by task, domain, and answer scale, with finance and medicine posing greater challenges than general knowledge; and (iv) Refinement helps only when it trivially widens (union); reflective self-refinement tends to narrow and can worsen coverage. Taken together, these findings show that miscalibration persists across settings. This work is an exploratory, descriptive study: our goal is to characterize black-box LLM behavior under a fixed protocol, not to benchmark or optimize models for maximal performance.", "tldr": "Characterising Overprecision in Black-Box LLMs", "keywords": ["LLMs; Uncertainty; Overconfidence"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5040c20bd41ad356b672bec2146a5f9f6af71a93.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper adapts the cognitive-science notion of overprecision—excessive certainty in interval estimates—to study LLMs. The authors propose a three-phase generation–refinement–evaluation framework: Generation – models generate numeric confidence intervals at imposed confidence levels. Refinement – intervals are aggregated or self-refined. Evaluation – empirical coverage and correlation between interval width and imposed confidence are assessed. Experiments with GPT-3.5-turbo and GPT-4o-mini across FinQA, MedQA, and MMLU show that LLMs are “overprecise”: intervals are too narrow, coverage is far below nominal confidence (e.g., only ~20% coverage for 95% intervals on FinQA), and self-refinement tends to make intervals narrower rather than better calibrated. The paper aims for descriptive analysis rather than optimization, claiming novelty in introducing a black-box, cognitive-science–inspired protocol for overprecision measurement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Conceptually interesting framing — connecting cognitive-science constructs (overprecision, interval judgments) with LLM calibration is a fresh angle, distinct from common probabilistic calibration or verbal confidence studies.\n\nTransparent methodology — the generation/refinement/evaluation pipeline is clear and easily reproducible from prompts listed in Appendix.\n\nData variety — the inclusion of financial, medical, and general-knowledge datasets provides some cross-domain coverage.\n\nNegative results are valuable — showing that self-refinement and CoT do not necessarily improve calibration challenges prevailing assumptions in the confidence-elicitation literature."}, "weaknesses": {"value": "1. Conceptual contribution is limited and largely descriptive. While the cognitive-science framing is novel, the work provides no theoretical development beyond restating the overprecision paradigm. The method is a direct adaptation of human interval-judgment tasks, not an original LLM methodology. The study yields descriptive statistics (hit @ c, correlations, DS/ILS) but no actionable insights for modeling uncertainty or improving calibration. The authors repeatedly stress that this is “not a benchmark or optimization study”, but this stance limits scientific value: the paper ends up confirming an already known fact — LLMs are miscalibrated — without explaining why or how to fix it.\n\n2. Methodological limitations undermine interpretability. Lack of internal validation: imposing a nominal 90 % confidence and measuring empirical coverage is straightforward, but the protocol mixes sources of variation (model stochasticity, numeric reasoning errors, parsing errors) that confound true overprecision effects. Small model/sample scope: only two OpenAI models, one temperature, and narrow numeric subsets of three datasets; this makes conclusions about “LLMs” overgeneralized. Data filtering biases: converting multiple-choice medical questions into single numeric answers strips semantics and may distort task difficulty. Metrics are redundant: “Deviation Score” and “Interval Length Score” are simple normalized distances and widths; they do not meaningfully add insight beyond coverage.\n\n3. Refinement experiments are weakly justified. The aggregation and self-refinement procedures are ad hoc and disconnected from cognitive-science theory. “Union” trivially improves coverage by widening intervals — hardly evidence of cognitive correction.The “self-refinement” mechanism reveals that models overwhelmingly pick the narrowest interval, but the paper does not probe why this occurs or whether prompt wording causes it. The claim that self-reflection mimics peer-judgment correction (Haran et al. 2010) is superficial and empirically unsupported.\n\n4. Statistical rigor and presentation issues. No statistical significance tests, confidence intervals, or effect sizes — only tiny decimal differences (±0.2 %) reported with three decimals in Tables 2–4 pp. 6–7, which exaggerate precision. Figures 2 and 3 (pp. 8 & 16) are visually cluttered and fail to convey new insights. Some tables (e.g., Table 2) misinterpret correlation magnitudes < 0.01 as meaningful; these are essentially zero. Sample size after filtering is unclear — some datasets drop to only ~1–3 k examples (Table 1 p. 5).\n\n5. Limited originality relative to prior work. The paper positions itself as the first to study overprecision in LLMs, but prior studies on numerical calibration, interval estimation, and uncertainty (e.g., Xiong et al. 2024; Wen et al. 2024; Shrivastava et al. 2023) already evaluated similar ideas with probability or interval formats. The difference here is largely terminological (borrowing “overprecision” from psychology) rather than methodological.\n\n6. Weak insight and discussion. Section 6 (p. 9) summarizes that miscalibration persists and CoT/self-refinement give mixed results — conclusions that add little beyond previous literature. No deeper analysis (e.g., linguistic factors, reasoning depth, or token-level uncertainty) is attempted. The discussion reads as observational rather than explanatory. Figures 5–6 (pp. 18–19) merely restate known dataset difficulty orderings."}, "questions": {"value": "How do you ensure parsing correctness of numeric intervals in model outputs? Could failures inflate apparent miscalibration?\n\nHow many prompts per sample were used, and how sensitive are results to temperature or phrasing?\n\nCould the “overprecision” pattern simply reflect model under-dispersion due to deterministic decoding, rather than a cognitive-style bias?\n\nWhat is the advantage of interval elicitation over directly sampling numeric uncertainty (e.g., via logits or surrogate ensembles)?\n\nCan the protocol scale to non-numeric tasks, or is it limited to trivial numeric Q&A?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no ethical concerns"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qBs4oTvNnr", "forum": "2l3RxcxI07", "replyto": "2l3RxcxI07", "signatures": ["ICLR.cc/2026/Conference/Submission7348/Reviewer_Czgx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7348/Reviewer_Czgx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618357855, "cdate": 1761618357855, "tmdate": 1762919482991, "mdate": 1762919482991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for evaluating overprecision in black-box large language models (LLMs), drawing inspiration from human studies in cognitive science. The authors define overprecision as excessive certainty in numerical interval judgments. Their method proposes a three-phase procedure: (1) generating numerical confidence intervals from LLMs with prompt engineering, (2) refining these intervals using aggregation and self-refinement strategies, and (3) evaluating them with calibration and correlation metrics. The experiments (conducted in general knowledge, medical, and financial domains) reveal that LLMs are systematically miscalibrated, showing large gaps between requested confidence and actual coverage. Furthermore, the paper finds that interval lengths do not scale with confidence levels. Refinement strategies(self-refinement and CoT) offer limited improvement."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's main strength lies in its well-structured framework for evaluating overprecision in black-box LLMs. The proposed three-phase method is clear, systematic, and easily reproducible. The experiments are comprehensive, covering multiple domains and providing a detailed analysis of the results. The findings that LLMs are systematically overprecise and that their confidence does not correlate with their predictions are significant and contribute to a better understanding of LLMs."}, "weaknesses": {"value": "There are a few areas that could be improved. \n- The study is limited to two old models, all from OpenAI. It would be interesting to see if the findings generalize to other model families (e.g., Llama, Claude, and Gemini), closed- and open-sourced models, and reasoning models.\n- The refinement strategies explored are relatively simple. More sophisticated methods, such as those involving more complex reasoning or external feedback, could be explored. Both refinement strategies showed limited improvements. Although the problem of overprecision is important and novel, the proposed solutions seem underdeveloped. This limits the practical impact of the work.\n- The paper focuses on numerical answers, and it would be interesting to see how the framework could be adapted to other types of data, such as text or images."}, "questions": {"value": "1. The main question I have is how the proposed framework would generalize to other LLMs beyond the two OpenAI models tested.\n2. The paper shows that CoT prompting has mixed effects on calibration. Do the authors have any hypotheses about why this might be the case? CoT is known to be a simple and robust method to induce the zero-shot reasoning ability of LLMs to improve the model's reasoning capability. It is less intuitive to see that it did not improve the uncertainty estimation since the uncertainty estimation task strongly involves a reasoning process. Could it be related to the complexity of the reasoning required for different tasks?\n3.  The self-refinement strategy did not yield improvements. The authors suggest that this is due to a narrowing bias. Could this bias be mitigated by providing the model with more diverse examples or by explicitly instructing it to consider a wider range of possibilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iRdaNhecdm", "forum": "2l3RxcxI07", "replyto": "2l3RxcxI07", "signatures": ["ICLR.cc/2026/Conference/Submission7348/Reviewer_7183"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7348/Reviewer_7183"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951303901, "cdate": 1761951303901, "tmdate": 1762919482473, "mdate": 1762919482473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether LLMs can produce numerical confidence intervals that meaningfully correspond to a given confidence level (e.g., 90%). For example, when we ask LLMs to “Provide an interval that you are 90% sure contains the answer”, does the interval actually contain the answer 90% of the time? \n\nIn order to test this, the authors propose a three-phase framework (generation → refinement → evaluation) for eliciting and assessing such intervals under imposed confidence levels. This setup is repeated across confidence levels (60%, 70%, 80%, 90%, 95%) and applied to datasets spanning financial reasoning (FinQA), medical QA, and general knowledge tasks. The study evaluates two black-box models (GPT-3.5-turbo and GPT-4o-mini) and introduces two complementary metrics—Deviation Score (DS) and Interval Length Score (ILS)—to characterize calibration behaviour beyond simple coverage rates\n\nFindings\n1. Across all models and domains, empirical coverage (hit rate) is consistently below the stated confidence, indicating strong overprecision.\n2. Interval widths show little to no correlation with confidence levels, suggesting that LLMs fail to internalize the concept of confidence intervals.\n3. Calibration quality varies with domain, numerical scale, and prompt formulation, with larger deviations observed in financial and medical tasks.\n4. Limited benefit of refinement: Simple union-based aggregation trivially improves coverage by widening intervals, whereas self-refinement tends to narrow intervals and further degrade calibration, revealing a systematic narrowing bias."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem itself is interesting, and the evaluation framework can support this motivation. \n2. Metric improvements: DS and ILS meaningfully extend the analysis beyond raw coverage.\n3. The consistent narrowing bias finding is an interesting empirical observation.\n4. The writing, tables, and figures are polished and easy to follow."}, "weaknesses": {"value": "1. Beyond the problem (i.e., evaluating LLMs regarding their ability to understand the confidence interval), the paper mostly refines and formalizes an existing evaluation setup rather than introducing new conceptual or methodological ideas.\n2. Restricted scope: Only GPT-series models are tested; no comparison to open-source or white-box methods.\n3. The cognitive framing is more decorative than explanatory; there is little theoretical connection explaining why overprecision occurs or how to mitigate it."}, "questions": {"value": "1. Do you see any connection between your framework and conformal prediction?\n2. Is the “narrowing bias” prompt-dependent or model-dependent? Have you tried to explicitly ask the model to avoid this?\n3. What would be required to make this framework predictive (i.e., useful for detecting unreliable outputs rather than just describing behaviour)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lffsWVp3qy", "forum": "2l3RxcxI07", "replyto": "2l3RxcxI07", "signatures": ["ICLR.cc/2026/Conference/Submission7348/Reviewer_Umvq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7348/Reviewer_Umvq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976099609, "cdate": 1761976099609, "tmdate": 1762919477725, "mdate": 1762919477725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary**: This work proposes to investigate model’s calibration in terms of *overprecision*, i.e., whether the model conveys excessive certainty in one's estimate.  To this end, and focusing on numerical output tasks, the paper investigates whether LLMs are able to adjust the numerical interval based on a fixed confidence interval (e.g., “Provide an interval that you are $c$% sure contains the answer”). Experiments are conducted using gpt-3.5-turbo and gpt-4o-mini and 3 datasets (MMLU, FinQA, MedQA and MedMCQA)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel perspective on examining uncertainty quantification, merging interesting concepts from cognitive science frameworks;\n2. Experiments concern different domains, including both general more knowledge (i.e., MMLU) and more domain-expertise focused datasets (i.e., MedQA, MedMCQA, FinQA)"}, "weaknesses": {"value": "- W1. **Motivation for the need of studying overprecision in LLMs is insufficient**: the paper mentions that studying “overprecision in black-box LLMs is crucial” (lines 43-44) but does not mention why or the implications to the field. \n- W2. **Some statements (and claims) in the paper do not seem to be supported or well-motivated**, raising questions about the soundness of this work (see Question section below).\n- W3. While providing novel dimension to miscalibration, this cognitive-science inspired framework is limited to the short numerical answers. It is unclear how such results would generalize in open ended generation.\n- W4. Focuses evaluation on single model family, offering limited insights about generalization in other models. Evaluated models (GPT-3.5 and GPT-4) employ multi-digit tokenization (e.g., numbers in [0, 100] are represented using 101 different tokens). However, more recent models (e.g., Gemma 2, Llama 3, OLMo) adopt single digit tokenization. These may exhibit different biases ([Singh et al 2024](https://arxiv.org/abs/2402.14903)), so it could be insightful to add experiments with different model families and across model sizes."}, "questions": {"value": "**Questions**: \n1. Lines 100-101 refer to limitations of self-reported confidence including variability with prompt wording, sampling randomness, linguistic biases and because of which may represent unreliable measures of true model uncertainty. Is there empirical evidence that this is the case? Experimental results or adequate citation should be provided to ground such claims.\n     a. Similarly, in lines 59-60, the authors mention limitations of verbalized self-reports mentioning that existing methods do not ensure that stated probabilities correspond to empirical frequencies. It would be great if experimental results or relevant citations are added to back these arguments. \n    b. Related to the previous comment, other peer-reviewed work ([Xu et al 2024](https://aclanthology.org/2024.emnlp-main.343), [Lyu et al 2025](https://ojs.aaai.org/index.php/AAAI/article/view/34120)) seems to be relevant for this discussion, as they propose to calibrate verbalized confidence scores to empirical frequencies. I wonder how this affects the paper’s argument, since these papers provide a way of generating self-reporting scores that are aligned with the empirical frequencies.\n    c. Lines 212 shed light on how focusing on numerical outputs helps reduce the influence of linguistic biases such as positivity biases. But there may be still other biases present, such as generating numbers ([Shao et al 2025](https://openreview.net/forum?id=AOe1aUhEQQ))). \n2. The paper mentions that “to mitigate these issues”, the confidence specification is shifted to the prompt (lines 101-102) by imposing explicit confidence levels and evaluating whether intervals align. However it is unclear to me how this addresses the previously mentioned limitations for verbalized confidence (sampling randomness, word sensitivity, and linguistic biases). If I understand correctly, none of the experiments in the paper (or appendix) provides support for the claim that specifying confidence in the prompt leads to more robust results. Perhaps the authors can help clarify any misunderstanding I may have.\n3. In Section 4.1 the lowest confidence value considered for the generation phase is 60%. Is there a reason why lower values (e.g., 20%, 40%, 50%) were not used for evaluation as well?\n4. There is an assumption that during Phase 1 (Generation) the LLMs always generate an interval. Was this empirically validated? How often did the LLM generate some answer that was not an interval? How do you ensure a consistent output format?\n     a. . Can you specify the generation configurations for Phase 1 (Generation)? The configurations are mentioned for Phase 2 (refinement) but I could not find them for Phase 1.\n5. Interpretation of results and metric choice: Results in Table 2 seem to be constant irrespective of the prompted confidence level. My understanding of the hit@$c$ metric is that it is a 0-1 metric considering only whether the value $c$ lies in the specified interval. However, it doesn’t provide an idea of whether the intervals are systematically to the left or to the right of the desired confidence intervals. Can the authors share some insights about this?\n     a. Such analysis can help provide additional evidence to support the claim in Section 5.2 that “LLMs [...] remain insensitive to confidence cues” (lines 360-361). Especially given that the direction of deviation is not currently being accounted for by any metric – which could potentially provide some useful signal.\n6. The claim “results show a widespread miscalibration (overprecision) across datasets and models” in the caption of Table 2 (lines 294-295) seems to not fully capture the observed patterns. If I understand correctly, models are actually miscalibrated (underprecise) for confidence values of 95% and 90% but overprecise for confidence values of 60%, 70%, and 80%.      \n\n\n**Clarity**: \n- It was not clear to me what “refinement strategies” meant when reading through the introduction. It may be worth clarifying that.\n- Can you clarify which aggregation (or refinement strategy) was used to report the values in Table 2?\n- Are standard deviations values reported in Table 2 expressed in the same unit as the mean? They appear to be very small compared to the absolute value of the hit@c metric.\n- How is performance reported in Table 4? Consider adding such information to the caption.\n\n**Formatting**:\n- Wrong citation format is being used throughout the paper.\n- Table 1 format: missing top and bottom row.\n- Figure 2 captions are difficult to read. Consider adding whitespace around the “|” character. \n- Figure 2 legend’s font size is too small and difficult to read. \n\n**Missing citations**:\n- Section 2.2.1 is missing a citation to the work from [Lin et al 2022](https://openreview.net/forum?id=8s8K2UZGTZ), which is one of the first methods exploring the use of words to express uncertainty.\n\n**Typo**:\n- Line 26: “Refinement” → “refinement”\n- Line 140: “ (q_i, a_i)_i” → “ \\{(q_i, a_i\\}_{i=1}^N ” is more commonly used as the notation of a set of questions\n- Line 140: $qi$ → $q_i$\n- Line 322: “pp” → “percentage points (pp)”\n- Line 323: The expression “are much too narrow” sounds ungrammatical in this context.\n\n**Suggestion**:\n- As a subjective preference, it would be more appealing if lines 27-30 could motivate the importance of this study for the field, as opposed to describing it as a descriptive study. I.e., how can this analysis or the findings in this paper impact the field? \n- Add citations (whenever possible) to the metrics used in the study. For instance, when mentioning how “both measures follow established practice in cognitive psychology studies of overprecision”, it could be useful to add citations to said words in cognitive science."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K0hkaoTRWK", "forum": "2l3RxcxI07", "replyto": "2l3RxcxI07", "signatures": ["ICLR.cc/2026/Conference/Submission7348/Reviewer_1E2H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7348/Reviewer_1E2H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117339735, "cdate": 1762117339735, "tmdate": 1762919476040, "mdate": 1762919476040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}