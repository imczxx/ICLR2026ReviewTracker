{"id": "tNnXu5bKoo", "number": 1966, "cdate": 1756972546949, "mdate": 1763266461894, "content": {"title": "FuseGPT: Prune-and-Fuse Knowledge Redistribution for Efficient Transformers", "abstract": "Structured pruning of Generative Pre-trained Transformers (GPTs) offers a promising path to efficient models, but often at the cost of performance degradation from discarded transformer blocks.\nIn this paper, we introduce FuseGPT, a compression paradigm that reframes structured pruning as knowledge redistribution rather than simple removal.\nInstead of discarding less salient blocks, FuseGPT recycles them by fusing their knowledge into neighboring blocks, thereby preserving the model's performance.\nOur approach has two core components.\nFirst, we propose a fusion-aware importance metric, Macro Influence (MI), that identifies blocks not by their redundancy, but by their capacity to be effectively absorbed by other blocks.\nSecond, we introduce a learnable layers fusion mechanism that uses low-rank matrices to graft the knowledge from a pruned block onto its neighbors.\nThis process is guided by a lightweight, group-level fine-tuning procedure that uses a distillation-based loss to ensure the fused knowledge is properly integrated.\nFuseGPT works for both large language and multimodal models, generally surpassing representative prior methods in perplexity and zero-shot task performance, using as few as 32 calibration and 1024 fine-tuning samples. \nThis ``prune-and-fuse'' approach opens a new avenue for model compression, focusing on repurposing rather than discarding valuable pre-trained knowledge.", "tldr": "", "keywords": ["Pruning", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f88879ab41ff7a8d6dc2ed68a8f2bc18b7911009.pdf", "supplementary_material": "/attachment/d801547ff48c5c704dadc2293440d3388435cead.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces FuseGPT, a structured pruning paradigm that reframes block removal as \"prune-and-fuse\" knowledge redistribution. The method uses a Macro Influence (MI) metric to identify absorbable blocks and a learnable low-rank fusion mechanism to inject their knowledge into neighbors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The \"prune-and-fuse\" paradigm is a creative conceptual departure from standard \"prune-and-retrain\" approaches, as it attempts to recycle, rather than discard, the knowledge within pruned blocks.\n\n2. The implementation of knowledge transfer via learnable low-rank matrices to fuse weights into neighboring layers is a technically sound and clearly described mechanism."}, "weaknesses": {"value": "1. Computational Cost of Compression: The iterative \"prune-one-by-one\" approach, which requires re-computing importance scores (MI) and performing local adaptation for each block being removed , is acknowledged as computationally intensive. This high one-off cost to compress the model is a significant drawback compared to one-shot pruning methods.\n\n2. Local Fusion Heuristic: The fusion mechanism is restricted to a local partial group of neighboring blocks (size $G=7$), based on an assumption of functional similarity between adjacent blocks. This heuristic may be sub-optimal if a block's knowledge is more relevant to a functionally similar but distant block, a possibility the paper does not explore.\n\n3. \"Fusion-Aware\" Metric: The paper claims the MI metric is \"fusion-aware\" and identifies blocks by their \"capacity to be effectively absorbed\". However, the metric itself is calculated by measuring the impact of removal (cosine similarity on final hidden states), which primarily identifies redundancy. The link between low redundancy and high \"fusibility\" is an inference, not a direct measurement provided by the metric.\n\n4. While FuseGPT marginally outperforms other pruning methods, it still incurs a catastrophic >11-point drop in zero-shot performance on LLaMA-2-7B at 25% sparsity. This severe degradation in capability makes its 1.33x speedup  an unacceptable trade-off, raising doubts about its practical utility."}, "questions": {"value": "1. Compression Cost: Could you quantify the total computational cost (e.g., in GPU-hours) required to compress LLaMA-2-7B to 25% sparsity using FuseGPT? How does this one-off cost compare to the cost of one-shot methods like SLEB or SliceGPT, combined with a standard (non-local) fine-tuning run needed to achieve their reported results?\n\n2. Iterative vs. One-Shot Fusion: The iterative process is a clear bottleneck. Have you experimented with a \"one-shot\" version of FuseGPT, where the $N$ blocks with the lowest-MI scores are identified once, and then all fusions are performed simultaneously (or sequentially without re-scoring)? How much performance is lost in this more efficient scenario?\n\n3. Local vs. Global Fusion: To test the local fusion assumption, have you considered an alternative? For example, identifying the $G$ most similar blocks (e.g., by feature or weight similarity) in the entire network as fusion targets, rather than just the immediate neighbors? This would test the hypothesis that adjacent blocks are indeed the optimal recipients.\n\n4. Group Size Sensitivity: The partial group size was fixed at $G=7$. How sensitive is the final model's performance to this hyperparameter? What is the trade-off between a larger $G$ (more computational/memory cost during adaptation) and the quality of the fused model?\n\n5. Rationale for Updating $W_{i,j}$: During the fusion fine-tuning (Eq. 4), the pruned block's weights $W_{p,j}$ are frozen, but the neighboring block's weights $W_{i,j}$ and the coefficient $C$ are updated. What is the rationale for updating $W_{i,j}$ (via LoRA )? Does this not risk degrading the neighbor block's original knowledge? What happens if you only train the fusion coefficients $C$ and keep all original weights ($W_{i,j}$ and $W_{p,j}$) frozen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TVZteebD1U", "forum": "tNnXu5bKoo", "replyto": "tNnXu5bKoo", "signatures": ["ICLR.cc/2026/Conference/Submission1966/Reviewer_AeHK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1966/Reviewer_AeHK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942647340, "cdate": 1761942647340, "tmdate": 1762915978779, "mdate": 1762915978779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Some problems"}, "comment": {"value": "This work sounds good for me. Still, there is a key problems I am interested:\n\nMI score is similar to Laco's. While Laco calculate cosim of last hidden state between dense and after removing n consecutive layers, MI use individual layer score. Can authors provide theoretical analysis about why MI is better than its consecutive layers' version score?"}}, "id": "K6adLY5JQf", "forum": "tNnXu5bKoo", "replyto": "tNnXu5bKoo", "signatures": ["~Zhiguo_Yang3"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Zhiguo_Yang3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1966/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763519311962, "cdate": 1763519311962, "tmdate": 1763519311962, "mdate": 1763519311962, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests a new block-wise compression approach for LLMs which iterates on block/layer dropping. \nSpecifically, the authors propose a new \"MI\" metric for layer dropping, and combine it with a \"fusion\" approach by which a removed block is \"fused\" into its neighbors by retraining. \nExperiments on fairly standard datasets are provided, suggesting that FuseGPT works better than prior dropping methods such as ShortGPT, and various ablations on components of the method."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a new solution to a standard efficiency problem."}, "weaknesses": {"value": "- The solution is a fairly complex heuristic. \n- The speedups are quite small for the amount of accuracy that is dropped. \n- Some of the choices made, for instance in the metric choice, appear questionable."}, "questions": {"value": "There are two major shortcomings to the work, in my view.\n\n1. The first is that its assumptions regarding the metric appear to be invalid. Specifically, one basic assumption behind the work is that compression can be done iteratively, by ranking via the MI metric, followed by removal and fusion. This assumes that there exists a monotone metric hat can be applied to blocks, with the property that minimizing the metric upon removal would minimize the accuracy loss. \nHowever, to my understanding, the EvoPress, work (https://arxiv.org/abs/2410.14649) shows that the assumption of monotonicity is _invalid_ for DNN pruning, there exist configurations where pruning more leads to _lower_ accuracy loss (possibly due to redundancy, co-dependence, or other phenomena that we don't understand). As such, the authors argue that search is the correct approach, and that no monotone metric is \"correct\" given that it's based on an invalid assumption. Moreover, the authors provide quite good results for layer dropping, which seem to be SOTA.\nCan the authors position their work relative to EvoPress, and explain why this isn't cited? \n\n2. The second significant weakness is that the accuracy drops are really major, especially for such a complex method. From a deployment perspective, the models would be unusable. (E.g. a 2-point PPL increase for 33% speedup improvement.) \nCan the authors explain why one would use their method on a recent model family (e.g. Qwen3) rather than just pick the next smallest model from the model family? Their technique does not appear to be Pareto-competitive in terms of size-vs-accuracy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OpoFqVnvF5", "forum": "tNnXu5bKoo", "replyto": "tNnXu5bKoo", "signatures": ["ICLR.cc/2026/Conference/Submission1966/Reviewer_Ycs9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1966/Reviewer_Ycs9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999077181, "cdate": 1761999077181, "tmdate": 1762915978586, "mdate": 1762915978586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In my understanding, FuseGPT proposes a structured pruning approach for large language models that merges redundant transformer blocks rather than simply discarding them. The method works in three steps: (1) identify the least important block using a \"Macro Influence\" (MI) metric that measures how much removing a block perturbs the final hidden states. (2) For each block, fuse that block's parameters into neighboring blocks using learnable low-rank coefficients. (3) Perform lightweight fine-tuning on a partial group of blocks around the removed one using KL divergence loss. This specific cycle will repeat iteratively until reaching the target compression rate. The core insight is that redundant blocks still contain valuable knowledge that can be redistributed to neighbors before removal."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Clarity in presentation and motivation: The paper is well-written and easy to follow. The motivation for knowledge redistribution over simple deletion is intuitive and well articulated. The progression from problem statement through methodology to results flows logically.\n\n\nFigures and tables: Figure 1 provides an excellent visual comparison showing how FuseGPT differs from unstructured, channel-wise, and block pruning. Figure 2 clearly illustrates the partial group update mechanism. Tables are comprehensive and self-explanatory, with consistent formatting that facilitates cross-method comparison.\n\n\nProblem significance: Post-training compression of large language models is critically important for deployment today in our resource-constrained environments. Methods that preserve performance while reducing computational requirements have substantial practical value, especially as models continue to scale. Additionally, the fact that this method is not training is super critical."}, "weaknesses": {"value": "W1: Novelty\n\nW1A: MKA\nThe paper completely omits \"Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging\" (Liu et al., arXiv:2406.16330, EMNLP 2024, June 2024). This is a major issue because: (a) MKA predates FuseGPT by and implements layer fusion/merging as its core mechanism, (b) In the paper, MKA reports better results than FuseGPT (43.75% compression on Llama3-8B with only 2.82% MMLU drop versus FuseGPT's 25% compression). The novelty claimed for this work is significantly undermined without addressing MKA. Authors must  provide head-to-head comparison, and explicitly articulate what FuseGPT contributes beyond MKA's approach(both experimentally and in theory).\n\n\nW1B: Layer Merging in Other methods\nEven with cited work like LaCo, the paper doesn't adequately explain how FuseGPT's low-rank coefficient fusion differs from or improves upon existing layer merging approaches (such as LaCo's RDSC). The technical distinctions remain unclear.\n\n\nW2: Evaluation on older/outdated architectures. \n\nExperiments focus on LLaMA-2 (2023), LLaMA-3 (early 2024), and LLaVA-1.5, while recent pruning papers evaluate on LLaMA-3.1, Mistral NeMo, Phi-3.5, and Qwen models. This is not sufficient an ICLR 2026 submission; including more recent architectures strengthen generalization claims.\n\n\nW3: Wall clock/GMacs\n\nThe paper claims \"lightweight\" fine-tuning but provides no wall-clock time comparisons or even GMacs. For a pruning/compression paper, it is super important."}, "questions": {"value": "Q1: How does FuseGPT compare to MKA on the same setup?\n\nQ2: What is the computational overhead versus MKA's progressive merging or one-shot methods?\n\nQ3: Can you provide results on LLaMA-3.1 or other recent (2024) architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g5eyviYHcw", "forum": "tNnXu5bKoo", "replyto": "tNnXu5bKoo", "signatures": ["ICLR.cc/2026/Conference/Submission1966/Reviewer_t6pW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1966/Reviewer_t6pW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762197309642, "cdate": 1762197309642, "tmdate": 1762915978354, "mdate": 1762915978354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}