{"id": "1sffPGGQyT", "number": 22487, "cdate": 1758331763801, "mdate": 1759896863618, "content": {"title": "Achieving Expert-Level Agent from Foundation Model via Complexity Curriculum Reinforcement Learning with Synthetic Data", "abstract": "Large Language Model (LLM)-based agents exhibit strong mathematical problem-solving ability and can even solve International Mathematic Olympiad (IMO)-level problems with the assistance of a formal language prover. However, hindered by the weak heuristics of auxiliary constructions, the AI for solving geometry problems remains to be specialist models such as AlphaGeometry2, which heavily relies on large-scale data synthesis and search for training and testing. \nTherefore, this paper makes the first attempt to investigate how to build a medalist-level LLM agent for solving geometry problems and eventually proposes InternGeometry. InternGeometry conquers the weak heuristics of geometry problems by continuously proposing propositions and auxiliary configurations, verifying them in the symbolic engine, and reflecting on the feedback from the symbolic engine for the next proposal, where the dynamic memory mechanism allows InternGeometry to conduct model-symbolic engine interactions more than two hundred times. To further accelerate the learning process of InternGeometry, we introduce Complexity-Boosted Reinforcement Learning (CBRL) that gradually scales the complexity of the synthesized problem at different training stages.\nBased on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000–2024), exceeding the average gold medalist score (40.9), using 13K training examples, only 0.004\\% of the data used by AlphaGeometry2, demonstrating the potential of LLM agents on expert-level tasks. InternGeometry is also capable of proposing novel auxiliary constructions on IMO problems that are unseen in human solutions.\nModel, data, and symbolic engine will be released to benefit future research.", "tldr": "We propose Complexity Curriculum Reinforcement Learning to train LLMs to solve IMO-level geometry with minimal data, surpassing gold medalists and showing emergent creativity.", "keywords": ["Large Language Model", "Reinforcement Learning", "Geometry Agent Prover"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e34d7e8365e37f2f7d617c4d26c866a7b735ebde.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "1. The paper introduces InternGeometry, an LLM-based agent that achieves medalist-level performance on International Mathematical Olympiad (IMO) geometry problems. \n2. Unlike previous systems such as AlphaGeometry2, which rely heavily on large-scale data synthesis and heuristic search, InternGeometry integrates long-horizon reasoning, symbolic feedback, and a Complexity-Boosted Reinforcement Learning (CBRL) framework. \n3. This work performs efficient training using only 13K examples (0.004% of AlphaGeometry2’s data)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work introduces a long-horizon agentic reasoning paradigm for geometry, moving beyond static and propose a model toward interactive proof reasoning.\n2. The complexity curriculum addresses sparse-reward issues and ensures stable reinforcement learning progression.\n3. This work achieves state-of-the-art performance on IMO geometry problems with significantly less training data."}, "weaknesses": {"value": "1. Does Pass@256 with 200 steps implies ~51K LLM–engine interactions per problem? Does this raise scalability and efficiency concerns. \n2. The paper does not report inference time per problem or single-shot success rate (Pass@1); it will make the paper better if we have this presented so we can assess the model’s true reasoning efficiency.\n3. Would be nice if authors can add a baseline between the pretrained InternThinker-32B and the CBRL-trained InternGeometry, this will make it clear on how much improvement gain from CBRL does the proposed method have. \n4. Although case studies are insightful, a deeper analysis of failure cases (the unsolved problems in Table 2) or reasoning trajectories would provide better understanding. Authors can add this qualitative analysis in the draft, which will make the readability better. \n5. Will be great if the paper can mention inference/training compute requirements to reproduce the results. This will help in reproducibility.\n6. To justify the InterGeometry's generalization capabilities, authors may want to include an analysis of the cross-domain performance on mathematical and scientific reasoning tasks, which can improve the usefulness of the paper."}, "questions": {"value": "1. How crucial are the additions of \"dynamic diagram adjustment\" and \"double points\" handling for solving the IMO-level problems? Will be good if authors can add this to improve the quality of the paper. \n2. As mentioned in Equation (9) add a brief explanation of how maximizing \"absolute advantage\" creates a curriculum of moderate difficulty and how the search for ‘k’ is done in each round.\n3. The paper builds upon InternThinker-32B as the foundation for InternGeometry model. However, the current version lacks sufficient details about InternThinker-32B’s architecture, training setup, and reasoning capabilities. Hope authors can add this to the draft and improve the paper quality. \n4. Also, please clarify what aspects of InternThinker-32B enable effective long-horizon reasoning and symbolic interaction in this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fca4R6SSqc", "forum": "1sffPGGQyT", "replyto": "1sffPGGQyT", "signatures": ["ICLR.cc/2026/Conference/Submission22487/Reviewer_5LSi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22487/Reviewer_5LSi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698094220, "cdate": 1761698094220, "tmdate": 1762942238274, "mdate": 1762942238274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "While LLM agents and agentic workflows have shown strong performance (gold medal level) in International Mathematical Olympiad (IMO), solving geometry problems is still by far the most challenging task, and existing IMO math agents typically have poor performance on such problems. The main reason is that a large subset of geometry problems typically requires multiple trial and error to find the right auxiliaries to add to the current problem diagram to make it solvable. Solutions like AlphaGeometry exist, these solutions typically require an enormous amount of formal data to either train a custom transformer from scratch or perform continual training on the same data but on already pre-trained models. Then, the trained model on formal data would go in a loop with a deductive symbolic engine, where the model proposes constructing a new auxiliary to the problem at hand, and then the symbolic engine would run over and over again to extensively perform deductive search for reaching the proof goal, and upon exhaustion of the engine, the loop continues.\n\nThe vast amount of search done on formal deductive engines, and the enormous amount of training data make the existing solutions very inefficient. On the other hand, it is widely known that LLM agents and workflows can reach perfect performance on non-geometry problems in either formal or non-formal settings even when fine-tuned on a very limited amount of training data compared to AlphaGeometry. Thus, this work aims to address this timely question of how LLM agents can be designed to tackle geometry problems as well. They propose introducing a novel tool call for dynamic interaction with their enhanced deductive engine, supporting multiple actions such as building the formal geometric configuration, adding auxiliary constructions, and proposing a proposition to be verified. The dynamic interaction with the engine enables leveraging the already strong high-level natural language planning and reasoning of frontier LLMs for optimized interaction with the engine.\n\nTo enable this tool call, they performed cold-start training to teach their base model (InternThinker-32B) how to work with this new tool, and then performed curriculum RL to improve the reasoning with their proposed tool call, and the final solution is InternGeometry which is claimed to perform on par with, or even sometimes outperform the two main baselines: AlphaGeometry and SeedGeometry on the IMO 50 evaluation suite (all geometry problems from 2000-2024 IMO). In contrast to these baselines, InternGeometry is trained on roughly 0.005% of the training data size of other baselines (13k samples, 7k for cold-start, and 6k for RL)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The introduction of a new tool call for leveraging strong planning and reasoning of LLMs for dynamic interaction with the formal deductive engine is novel and an exciting idea. The paper did a good job explaining this tool call and motivating it, which makes their high-level approach clear and sound.\n\n2. As mentioned in the summary, much trial and error is typically expected for figuring out the helpful auxiliary constructions for solving the given problem, and this fact would require the proposed agent to deal with the challenge of long-horizon reasoning that demands careful context management of past exploration after each tool call. The paper then proposes to maintain a dynamic memory containing key information from past tool call iterations (e.g., what actions were made in previous turns, what were the outcomes of those actions, plus the current action and current feedback from the deductive engine which contains all successful propositions). The introduction of this dynamic memory is novel and an interesting solution for solving the weak heuristic nature of auxiliary construction in IMO-level geometry problem solving. The ablation also confirms the long-horizon interaction is indeed necessary for obtaining the InternGeometry performance.\n\n3. Only requiring roughly 0.005% of the key baselines' training data (AlphaGeometry and SeedGeometry) to reach comparable performance, and even outperform them on the IMO 50 dataset is an impactful contribution to the LLM math agents community, and this work can be seen as an initial step to harness general-purpose LLMs for solving complex geometry problems.\n\n4. The paper mentions the model, data, and the deductive engine used will be open-sourced, which is of great benefit to the community, and this would make this work reproducible."}, "weaknesses": {"value": "1. While the RL reward and RL loss are clearly defined, the handling and explanation of the curriculum algorithm lacks clarity, which makes it hard to evaluate the soundness of the proposed curriculum approach. The paper attempts to touch on the theory behind the curriculum algorithm on the surface, and both Theorem 1 and 2 statements are hard to follow and vague, and could be better explained. More importantly, the paper does not explain the CBRL algorithm with sufficient detail and particularly it is not clear how the complexity $\\kappa$ is updated in each CBRL round. The paper only briefly mentions the following (line 253): \"In practice, in each CBRL round, we sample data conditioned on complexity $\\kappa$, perform RL training to the agent, and finally update κ according to learning rate $\\alpha$.\"\n\n2. Data curation for cold-start requires clarity, as the complexity of obtaining and curating such data is actually high. Regarding this, the paper only briefly addresses this around line 262, mentioning that \"First, due to the scarcity of data in formal systems, we fine-tuned InternThinker-32B as InternGeometry-Formalizer through expert iteration (Anthony et al., 2017) and then exploit large-scale natural language problem data from diverse sources. This process produced a total of 7K formal problem and solution trajectory pairs, which provide a cold start for InternGeometry.\" I believe due to the complexity of data curation for this phase, one would want to know the details of how exactly the cold-start trajectory is generated."}, "questions": {"value": "1. Could you provide a simplified high-level pseudocode for the working implementation of CBRL so the selection of the curriculum complexity $\\kappa$ becomes clear, as well as explaining the technical challenges of implementing the CBRL algorithm? This would greatly benefit the soundness and clarity of the RL training done in this paper.\n\n2. Please see Weakness 2, and elaborate more on how exactly the cold-start data is generated?\n\n3. A cost comparison between deploying InternGeometry versus AlphaGeometry or SeedGeometry would be greatly valuable for this work. I understand this might or might not be feasible based on the information available from the prior work, but it would be interesting to compare the number of interaction steps between the deductive engine and the LLM with the prior work. Especially since the LLM usage for proposing propositions might greatly improve the interaction efficiency as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ghBkOz407F", "forum": "1sffPGGQyT", "replyto": "1sffPGGQyT", "signatures": ["ICLR.cc/2026/Conference/Submission22487/Reviewer_2RDT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22487/Reviewer_2RDT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188895761, "cdate": 1762188895761, "tmdate": 1762942237778, "mdate": 1762942237778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces InternGeometry, an LLM agent system with a symbolic geometry solver (InternGeometry-DDAR) that solves math competition geometry problems. It improves the previous SOTA on 50 IMO geometry problems from 43/50 to 44/50, and achieves this with significantly less geometry problem training data than previous methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. InternGeometry is the new SOTA on IMO 50.\n2. The fact that the training set of geometry problems is much smaller than in previous approaches is impressive.\n3. The analysis about scaling max steps vs. scaling # samples is insightful."}, "weaknesses": {"value": "1. The paper would benefit from more discussion regarding inference-time costs of the compared methods. For example, listing the model sizes in Table 1 would be helpful, as well as explaining the different search parameters in AlphaGeometry2’s custom beam search. If available, information about the total # of output tokens or wallclock time etc. would also be appreciated.\n\n2. Similarly, I think it would be nice if the paper also discussed training costs and compared them with previous methods. Currently, it mentions the size of the training set, but other information would also be helpful, such as the total # of tokens in the training set and a version of Figure 4 where the x-axis is the number of training tokens.\n\n3. Writing quality is poor:\n  - Contextualization within previous work is often missing, e.g., it is not clear what the novelty of CBRL is compared to previous work (curriculum learning for RL, GRPO).\n  - There are many grammatical mistakes and unidiomatic uses of English, including in the title and abstract. I recommend using something like ChatGPT to improve the writing.\n  - There are missing citations (e.g., the last two paragraphs of the introduction section have no citations)\n  - Other miscellaneous issues, e.g., labeling imprecise claims as “theorems” (lines 247-251), potential typos in Eq. (5), and missing explanations (e.g., what’s InternThinker-32B? what’s “split” in Table 2?)"}, "questions": {"value": "1. What is InternThinker-32B? There is no citation, and I couldn’t find any information about it on the Internet.\n\n2. How are Eqs. (5) and (6) different from GRPO? (other than what appears to be typos)\n\n3. How is CBRL different from RL with a curriculum?\n\n4. Should the top-left cell of Table 1 say “AlphaGeometry2” instead of “AlphaGeometry”?\n\n5. The claim that InternGeometry’s test-time scaling budget is “far lower than that of AlphaGeometry2” (lines 311-312) is not clear to me. InternGeometry does pass@256 and uses a larger model (32B instead of 3.3B), so it’s not clear that it uses \"far less\" computational resources than AlphaGeometry2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "avskq5SetY", "forum": "1sffPGGQyT", "replyto": "1sffPGGQyT", "signatures": ["ICLR.cc/2026/Conference/Submission22487/Reviewer_YNAr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22487/Reviewer_YNAr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762675543324, "cdate": 1762675543324, "tmdate": 1762942237580, "mdate": 1762942237580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces InternGeometry, a LLM agent designed to solve geometry proof problems at the IMO level. By integrating Complexity-Boosted Reinforcement Learning (CBRL) and dynamic memory mechanisms, the model achieves gold-medalist performance on IMO geometry problems, significantly outperforming strong baselines AlphaGeometry2 and SeedGeometry. Notably, InternGeometry only uses 13K training samples, two orders of magnitude less than AlphaGeometry and SeedGeometry. The experiments are thorough, demonstrating the effectiveness in data efficiency and long-range reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Propose the first LLM agent for IMO-level geometry proving, avoiding the use of specialist models.\n2. Propose a dynamic memory mechanism and rejection sampling strategy, enabling up to 200-step interactive reasoning and guiding diverse explorations in interactions.\n3. Solid experiments well justify that InternGeometry outperforms current SOTA models, with exceptional data efficiency. Comprehensive ablation studies validate the necessity of key components like long-range interactions, CBRL, and dynamic memory. The case study justifies the model's creative construction capabilities."}, "weaknesses": {"value": "1. The title is not clear. Since the manuscript focuses on developing plane geometry prover, the title should contain such information.\n2. Equation 5 extends beyond the page margin.\n3. Experiments on more datasets (e.g., JGEX-AG-231 proposed in AlphaGeometry) and other LLMs (other than InternThinker) can further demonstrate the generalization ability of InternGeometry.\n4. Considering that the interactive reasoning requires many steps, analyzing and comparing the computational resources of InternGeometry, AlphaGeometry, and SeedGeometry during reasoning is necessary."}, "questions": {"value": "1. Line 40-42: Add references to justify LLM agents can obtain medalist-level performance on IMO-level problems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SJGg1LvoKG", "forum": "1sffPGGQyT", "replyto": "1sffPGGQyT", "signatures": ["ICLR.cc/2026/Conference/Submission22487/Reviewer_vjZE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22487/Reviewer_vjZE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762783498975, "cdate": 1762783498975, "tmdate": 1762942237319, "mdate": 1762942237319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}