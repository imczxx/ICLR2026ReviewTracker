{"id": "34WC2yucLy", "number": 10909, "cdate": 1758184570364, "mdate": 1759897621478, "content": {"title": "LyFormer: Context-aware feature fusion for industrial small-object detection", "abstract": "Accurate detection of small electronic components, such as semiconductors and printed circuit board (PCB) elements, is crucial for maintaining product quality and operational efficiency in surface mount technology (SMT) assembly lines. However, existing YOLO-based detection frameworks, while effective in general scenarios, often struggle with small, visually ambiguous objects under complex backgrounds, variable illumination, and subtle visual distinctions. To address these challenges, we propose \\textbf{LyFormer}, a YOLOv8s-based framework that integrates four specialized modules: (1) an Adaptive Multi-level Preprocessing Module (AMPM) for dynamic image preprocessing, (2) a Spatial Relation-aware Image Segmentation Patch (SRISP) for precise object localization, (3) a Fine-grained Cue Extraction Module (FCEM) for amplifying subtle texture details, and (4) a Context-aware Transformer Module (CaT) for integrating global and local contextual information. This modular design significantly improves detection accuracy while maintaining real-time performance. Experiments on real-world SMT production line X-ray images of semiconductor reels demonstrate that LyFormer achieves a mean Average Precision (mAP@0.5) of 0.672, substantially outperforming the baseline YOLOv8s (mAP@0.5: 0.399). These results confirm LyFormer’s accuracy and robustness for small, densely packed components in challenging industrial environments.", "tldr": "", "keywords": ["Industrial Vision", "PCB/SMT Inspection", "Feature Fusion", "Transformer Block", "YOLO Extension", "Attention Mechanism", "Domain Adaptation", "Few-shot Transfer", "Class Imbalance", "Ablation Study", "mAP", "Density Score", "Counting Accuracy", "Robust Preprocessing", "Noise-Resilience"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a451762dac16fdafa6763e20b63bdae4206ce23.pdf", "supplementary_material": "/attachment/84dfb92b4c8f6c3db6e099999618c2f196258fdd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes LyFormer for industrial small-object detection. The paper demonstrates both engineering innovation and empirical rigor, offering a practically deployable system with measurable performance gains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong experimental evidence and broad comparison scope.\n2. Maintains real-time throughput despite architectural complexity."}, "weaknesses": {"value": "1. The architecture builds upon existing paradigms (YOLOv8 + Transformer + attention modules); thus, I can't easily get the novelty of this paper. \n\n2. Integration of local and global reasoning is not new, it is not clear the main difference with existing methods.\n\n3. No complexity or parameter analysis is provided; efficiency justification remains empirical. \n\n4. Lack of visualization of attention maps or failure cases for interpretability.\n\n5. The interaction between modules (e.g., fusion weights α in Eq. 4) could be further analyzed. It is unclear how much each contributes dynamically per image or class.\n\n6. The text in figures are hard to see."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uCTrZFalrw", "forum": "34WC2yucLy", "replyto": "34WC2yucLy", "signatures": ["ICLR.cc/2026/Conference/Submission10909/Reviewer_nj6L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10909/Reviewer_nj6L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761450232087, "cdate": 1761450232087, "tmdate": 1762922112208, "mdate": 1762922112208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose various improvements to a YOLOv8 object detection pipeline for the task of industrial assembly line SMT x-ray object detection, which they show result in notable performance gains compared to a wide range of strong baselines on a fairly large private dataset."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The main four novel components (AMPM, SRISP, FCEM, CaT) seem to be intuitive and reasonable solutions to the various unique challenges of this domain. The authors seem to have a strong understanding of these challenges and the domain, as indicated by the extensive related works for small obj detection in SMT x-ray imagery.\n2. The experimental design is solid: broad range of appropriate metrics,  wide range of competitive baseline models. Also, the ablation studies (Table 5) clearly support that each of the four novel components are important to have, as each does indeed add a fair improvement to all metrics, so its clear that they are all useful.\n3. The results are strong: the proposed method noticeably outperforms all competing methods in various metrics by a solid margin (Table 4), including for count estimation (Table 6). Also, the computational time is strong, ensuring levels of throughput that would be realistically useful.\n4. Table 1 is helpful for contextualizing the novel components in terms of existing challenges.\n5. The paper is generally well-written."}, "weaknesses": {"value": "**Major Weaknesses**\n1. The method is bespoke for (and only evaluated on) the particular, fairly niche task of SMT x-ray object detection. Many design choices in the method seem uniquely developed for this domain, the format of the imaging data and labels, and the accompanying challenges. For example, the AMPM pre-processing module is built on hand-crafted feature extraction. This makes the impact of the work high within this very specific subfield, but unclear in the broader contexts of object detection and computer vision.\n2. Because the method is specifically designed for one fairly niche task, it is OK that it is only evaluated on one dataset, although it hurts reproducibility that it is private. But more to the point, the nicheness of this task make it so the paper and method will likely not be of broad interest to the ICLR community, especially because as mentioned, the novel components are uniquely designed for this domain/task, and it is unclear if they are useful in other settings.\n3. There are a fair amount of missing details of the method, making it likely not fully reproducible solely from the paper. For example:\n    1. Are the softmax gates/weights pi_b (Eq. 2) learned? How are their values set? Are they not a function of the input, as written (as opposed to them being akin to mixture-of-experts routing weights, for example)?\n    2. Undefined terms in Eq 3: X_tok, W_{k,v,q}, F_edge, and others. I'm 99% sure that X_tok is the tokenized input, W_{k,v,q} are key/query/value matrices, and I could maybe guess what F_edge is. But in any case, all terms need to be specifically defined throughout the paper.\n    3. How are the attention weights in Eq. 4 computed? Are they fixed a priori? Learned? Computed based on the input or fixed for all inputs? The text says \"dynamically computed (via GAP→MLP→softmax) so that they sum to one\" but there isn't sufficient detail here for reproducibility.\n    4. There are also missing details on the only dataset that the authors use, a private dataset, and more details are needed details are needed for readers to understand this dataset and associated benchmark (e.g., added to Table 3 and/or the appendix). For example: number of images per class, example images and annotations for each class, etc.\n\n**Minor Weaknesses**\n1. There is an unusually high number of acronyms in the paper, which seems unnecessary and a bit hard to keep track of. Moreover, several appear to be undefined, such as APA and FCI in Section 3.3, and VPS in the last paragraph of Section 3.4.\n2. There are various formatting and grammar issues. For example:\n\t- Please cite things parenthetically correctly via \\citep, not \\cite or\\citet.\n\t- Fig. 1 is low quality and blurry; I would suggest using a vector format, e.g. PDF. the text is also small and challenging to read.\n\t- In the ablation study section (4.3), it would be helpful to again reference the relevant table, table 5.\n\"miss detections\" in the conclusion should be \"misdetections\"."}, "questions": {"value": "**Suggestions for Revisions:** \nOverall, the method, experimental design, and results are all sound and seemingly novel, but only within the specific context of SMT x-ray detection. It is specifically designed for this context, and excels within it, but my concern is that because the method is so bespoke for (and only evaluated on) this context, it may not be of much interest to the broader ICLR community. Do the authors believe that the proposed method/novelties would be of interest in object detection contexts outside of SMT x-ray detection? Better yet, can the authors provide experimental evidence for this? This would significantly strengthen the paper's potential impact and interest from the broader ICLR community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z74qCHwYew", "forum": "34WC2yucLy", "replyto": "34WC2yucLy", "signatures": ["ICLR.cc/2026/Conference/Submission10909/Reviewer_VPyV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10909/Reviewer_VPyV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776306418, "cdate": 1761776306418, "tmdate": 1762922111679, "mdate": 1762922111679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenges of detecting tiny electronic components in industrial SMT production lines by proposing the LyFormer framework. This framework integrates four innovative modules: Adaptive Preprocessing, Spatial Relation-Aware Segmentation, Fine-Grained Feature Extraction, and a Context-Aware Transformer. Evaluated on a real-world X-ray image dataset, LyFormer achieves a mAP@0.5 of 0.672, significantly outperforming baseline methods while maintaining a real-time performance of 48.5 FPS. This work provides an effective solution for small object detection in industrial applications."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposed an ROI-guided attention bias and a Gibbs-weighted spatial relation-aware mechanism, innovatively integrating dynamic preprocessing with a lightweight Transformer.\n2. Extensively validated on a real-world industrial dataset, with ablation studies demonstrating the effectiveness of each module, complemented by comprehensive counting accuracy metrics.\n3. Addresses key challenges in industrial small object detection by achieving a balance between high accuracy and real-time performance, demonstrating clear engineering and application value.\n4.The method demonstrates consistently superior performance across various component categories, such as PA, AC, and IC, reflecting its strong adaptability to different types of small objects.\n5.While achieving a 68.4% increase in mAP (from 0.399 to 0.672), the model still maintains real-time performance at 48.5 FPS, which is crucial for industrial deployment."}, "weaknesses": {"value": "1.  Lack of Methodological Detail: Key components, such as the FCEM gated multi-branch structure and the CaT density rule s(x), are not clearly defined.\n2.  Limited Comparative Analysis: The comparison is restricted to baselines such as YOLOv8s, lacking a broader comparison with state-of-the-art (SOTA) small object detectors and other current methods.\n3.  Insufficient Generalization Validation: The method was evaluated solely on a private dataset, with no validation on public benchmark datasets.\n4.  Opaque Parameter Selection: The selection strategy for the AMPM dynamic threshold parameter, k, is not clearly specified.\n5. The analysis only reports FPS and lacks a comparative analysis of other efficiency metrics such as FLOPs and parameter count; however, the computational overhead of each module was not specifically analyzed."}, "questions": {"value": "1. How is the density rule s(x) in the CaT module specifically calculated? Is it based on local object density?\n2. What is the specific interaction mechanism between the multi-branch detail extractor and the QKV attention within the FCEM?\n3. While maintaining real-time performance, can the model be further compressed to accommodate deployment on edge devices?\n4. While maintaining real-time performance, can the model be further compressed to accommodate edge device deployment?\n5.In the Gibbs-weighted aggregation of SRISP, how is the weight balance between spatial distance and feature similarity determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fHE1vYfROZ", "forum": "34WC2yucLy", "replyto": "34WC2yucLy", "signatures": ["ICLR.cc/2026/Conference/Submission10909/Reviewer_dUpm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10909/Reviewer_dUpm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962624954, "cdate": 1761962624954, "tmdate": 1762922111298, "mdate": 1762922111298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed an object-counting framework, LyFormer, based on YOLOv8. It comprises four specialized modules — AMPM, SRISP, FCEM, and CaT — each designed for specific feature enhancement functions and to address challenges such as varying object sizes, foreground–background trade-offs, illumination variations, and occlusions, particularly in the semiconductor domain. The authors claim that their proposed framework demonstrates significant performance improvements over the baseline."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "a. State-of-the-Art Performance: outperformance across multiple dataset (AI-TOD, TinyPerson, VisDrone, Chip class), metrics (mAP@0.5:0.95, AP@0.5, AP_s, FPS, MAE, MAPE tec.) and against several baseline architectures (DETR, Deformable DETR, Swin Transformer, YOLOv8 etc.), providing satisfactory evidence for the feature-aware (AMPM, SRISP, FCEM, and CaT) approach's superiority.\n\nb. Rigorous Experimental Validation: Comprehensive ablation studies systematically validate each architectural component (AYH, ASYH, ASFYH, ASFCYH), proving their individual contributions are critical and non-redundant."}, "weaknesses": {"value": "a. Limited generalization assessment: All experiments confined to benchmark on several datasets and few SOTA baseline models, whereas the reviewer believe the main comparison should be on respective feature modules (image pre-processing in broader sense incorporated in backbone) of corresponding architectures as well. \n\nPlease compare computational time/cost, model size, latency as well. Please compare with SAHI tiling method as well, as SAHI is model agnostic inference framework and can be integrated into various SOTA object detectors, a comparative study should be demonstrated on \"latency\" and \"threshold sensitivity\" against proposed approach for better clarity. \n\nb. The related work and evaluation section provide minimal discussion or comparison minimal discussion or comparison with previous industrial or semiconductor-specific benchmarking efforts, including those related to methodology, architectural design, or public/private (if any) datasets.\n\nc. The overall clarity and flow of the writing could be improved to enhance readability.\n\nd. Please include visual demonstration of detection performance of SOTA models (like SAHI-based framework, DPNet, YOLOv8s etc.) to depict the detection diffentiation.\n\ne. Author claimed superiority of metrics on chip dataset, as in Table 4. DPNet for  {mAP@0.5:0.95} is 0.286,   {AP@0.5} is 0.601, {AP_s} is 0.276 and {FPS} is 45.8 against proposed approach 0.308, 0.672, 0.342 and 48.5. However, in Table 12 for PA DPNet [0.207 0.450 0.250 45.3] against proposed approach [0.216 0.465 0.210 50.0] --> APs is DPNet best and FPS Bottom-heavy Tiny-Backbone. In TABLE 13 for AC, DPNet outperforms proposed approach for {mAP@0.5:0.95} as 0.276 against 0.224,  {AP@0.5} 0.531 against 0.537 (its comparable as per), for { APS} 0.256 against 0.215 and {FPS} 45.8 against 49.5 and similar balanced trade-off in Table 14 for IC. Again, a comparative analysis of computational cost, latency and model size should be demonstrated to highlight on significant contribution. What if we apply SAHI with DPNet during inference ? Table 6 depicts no comparison for DPNet for MAE, RMSE and MAPE (%)?"}, "questions": {"value": "1. Computational Analysis: What are the training time, memory requirements, and computational costs of the full pipeline compared to prior baseline methods? Additionally, what would be the impact if more effective image preprocessing modules were integrated into existing architectures, instead of proposing a new framework such as LyFormer, given that the model still utilizes the YOLOv8 head?\n\n2. Why no demonstration on FN for prior baseline models against LyFormer detection results, like Fig. 8, 9 etc. It will help understand the significant differentiation.\n\n3. Fig. 3, is the binary mask for the localization (bounding box) or full image Need more clarity of the explanation provided regarding the figure shown in terms of binary bits of 1 and 0's, only 2 zeros surrounding of all 1s in that ROI?\n\n4. The related work section does not cite any prior studies using similar datasets or addressing the same domain (semiconductor). Could the authors clarify whether this demonstration represents the first effort of its kind?\n\n5. Generalization remains uncertain, given that prior baselines like DPNet already achieve a balanced trade-off. The absence of discussion on computational costs and training complexity further limits the practical adoption of the proposed method compared to simpler alternatives (e.g., SAHI-based tiling)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qGsENmeMvE", "forum": "34WC2yucLy", "replyto": "34WC2yucLy", "signatures": ["ICLR.cc/2026/Conference/Submission10909/Reviewer_h6RZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10909/Reviewer_h6RZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255730263, "cdate": 1762255730263, "tmdate": 1762922110769, "mdate": 1762922110769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}