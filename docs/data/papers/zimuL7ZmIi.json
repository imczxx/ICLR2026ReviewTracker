{"id": "zimuL7ZmIi", "number": 22069, "cdate": 1758325579950, "mdate": 1759896888114, "content": {"title": "TSM-Bench: Detecting LLM-Generated Text in Real-World Wikipedia Editing Practices", "abstract": "Automatically detecting machine-generated text (MGT) is critical to maintaining the knowledge integrity of user-generated content (UGC) platforms such as Wikipedia. \nExisting detection benchmarks primarily focus on \\textit{generic} text generation tasks (e.g., ``Write an article about machine learning.'').  \nHowever, editors frequently employ LLMs for specific writing tasks (e.g., summarisation).  \nThese \\textit{task-specific} MGT instances tend to resemble human-written text more closely due to their constrained task formulation and contextual conditioning. \nIn this work, we show that a range of MGT detectors struggle to identify task-specific MGT reflecting real-world editing on Wikipedia.  \nWe introduce \\textsc{TSM-Bench}, a multilingual, multi-generator, and multi-task benchmark for evaluating MGT detectors on common, real-world Wikipedia editing tasks.  \nOur findings demonstrate that (\\textit{i}) average detection accuracy drops by 10--40\\% compared to prior benchmarks, and (\\textit{ii}) a generalisation asymmetry exists: fine-tuning on task-specific data enables generalisation to generic data---even across domains---but not vice versa. \nWe demonstrate that models fine-tuned exclusively on generic MGT overfit to superficial artefacts of machine generation.  \nOur results suggest that, in contrast to prior benchmarks, most detectors remain unreliable for automated detection in real-world contexts such as UGC platforms.  \n\\textsc{TSM-Bench} therefore provides a crucial foundation for developing and evaluating future models.", "tldr": "We propose an LLM-generated text detection benchmark for realistic editing tasks and show that state-of-the-art detectors considerably underperform.", "keywords": ["llm-generated text detection", "editing tasks", "wikipedia", "benchmark", "multilingual"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c0f368b1c81c40c9f19bccb36d02dfbb91ad9ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a multilingual, multi-generator, and multi-task benchmark for machine-generated text (MGT) detection on Wikipedia, encompassing 4 tasks, 152,910 examples and six text generators in total. An extensive set of experiments is conducted, including evaluation of 12 detectors from different methods (zero-shot, off-the-shelf, supervised) and model families, feature importance analysis, out-of-domain and cross-task generalisation. Results show degradation of metrics on task-specific generation data compared to evaluations on generic data, and generalisation asymmetry in favor of task-specific generation data. Importantly, the limitations of existing benchmarks and MGT detectors in real-world conditions are emphasized."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper identifies a critical gap in existing benchmarks, where inflated performance metrics often fail to translate to real-world applications. The authors propose a novel benchmark that is multilingual, incorporates multiple generators, and covers main tasks relevant to user-generated content platforms like Wikipedia. An underlying contribution is the original formulation dividing data generation into \"task-specific\" (constrained by context) and \"generic\" (free-form) categories. The experiments demonstrate a substantial performance decrease for all detector types on task-specific data and reveal a clear generalization asymmetry: task-specific training aids in detecting generic text but not the reverse. The feature importance analysis indicates that models trained on generic data tend to overfit to superficial cues.\n\nThe paper is well-structured, competently written, and backed by thorough analysis, including a prompt impact evaluation and an examination of performance gaps between different detector methods.\n\nThe public release of the code and data represents a meaningful contribution to the community, enabling reproducibility and future research, in particular the development and evaluation of MGT detectors."}, "weaknesses": {"value": "1. The out-of-domain generalization experiment lacks transparency, as it conflates the effects of the domain and the generation conditioning (task-specific vs. generic). A more complete picture would require a fine-tuning experiment on the authors' own \"Our\" data using generic MGT to directly compare the performance impact of the training data's contextual conditioning generation.\n2. The benchmark, though covering crucial real-world tasks, omits others like translation and creative writing, and the scope of domains remains limited to Wikipedia.\n3. The paper misses an opportunity to provide examples of generic MGT cases and does not explore balancing task-specific and generic data for optimal detector training.\n4. The quality of the LLM-based translation is not checked."}, "questions": {"value": "1. Some details are unclear, such as the specific model used for BERTScore, the references for per-language fine-tuned models, and the definition of \"Wiki IO.\" in Exp. 3.\n2. The prompt evaluation relies on a single model, limiting the reliability of its conclusions.\n3. The reliability of the RAG pipeline, particularly the relevance of retrieved chunks, is not verified.\n4. The justification for the specific language selection is currently poor and needs strengthening.\n5. Below are some suggestions to fix typos and text errors:\n— L384 “accuracy of 89.7%”, in Appendix D.1 captions for figures are the same.\n— Appendix A: “DATA SET STATISTICS” –> “BENCHMARK STATISTICS”; “Corpus denotes the size of the data” –> “Corpus denotes the source of the data”.\n— Rephrase L070  “content integrity of UGC”.\n— Formulation “generic generation” is not clear.\n— The subsection \"PROMPT EVALUATION\" is missing a section number"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PNOLi5ScIY", "forum": "zimuL7ZmIi", "replyto": "zimuL7ZmIi", "signatures": ["ICLR.cc/2026/Conference/Submission22069/Reviewer_mKYw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22069/Reviewer_mKYw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756394643, "cdate": 1761756394643, "tmdate": 1762942048252, "mdate": 1762942048252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TSM-Bench, a multilingual, multi-generator, and multi-task benchmark designed to evaluate machine-generated text (MGT) detectors in real-world Wikipedia editing scenarios. Unlike prior benchmarks that rely on generic prompts (e.g., \"Write an article about X\"), TSM-Bench focuses on task-specific LLM usage—such as paragraph writing, continuation, summarization, and text style transformation across English, Portuguese, and Vietnamese. The benchmark comprises approximately 152,000 texts and evaluated using 12 detectors. The authors find that existing detectors suffer a 10–40 percentage point drop in accuracy on task-specific MGT, exhibit asymmetric generalization (models trained on task-specific data generalize to generic data, but not vice versa), and tend to overfit to superficial artifacts when trained on generic outputs. Overall, the study compellingly argues that realistic, editor-aligned benchmarks are essential for assessing detector reliability on user-generated content (UGC) platforms like Wikipedia."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a pressing concern: maintaining content integrity in Wikipedia and similar UGC ecosystems in the era of LLM-generated text. The motivation is clear and important for both the NLP and broader AI ethics communities.\n- Comprehensive evaluation across 12 detectors and 5 experimental dimensions; inclusion of SHAP analysis gives interpretable evidence of overfitting to surface artifacts.\n- The observed generalization asymmetry, where task-specific detectors generalize to generic data but not vice versa, is a notable and reproducible pattern that could inform future model training paradigms."}, "weaknesses": {"value": "- The paper convincingly grounds its task design in empirical studies of Wikipedia editors'  LLM use and policy structures such as Manual of Style and Neutral Point of View. However, the benchmark ultimately treats detection as binary classification (HWT vs. MGT), despite acknowledging blended paragraphs where human- and machine-generated text co-exist. This abstraction may be practical for dataset construction, but undermines the ecological validity of the \"real-world\" claim. Real Wikipedia editing may involve mixed authorship, such as post-editing of LLM drafts, LLM-assisted revisions of human text, and stylistic corrections. A benchmark faithful to these workflows would ideally include segment-level or mixture-aware labeling to capture such dynamics. Thus, while the task types are empirically grounded, the evaluation protocol remains synthetically simplified.\n- I kindly recommend that the authors improve the manuscript's presentation quality. For example, several key abbreviations and table entries are used without any definition at first mention, forcing readers to infer their meaning:\n  - \"TST\" first appears in Section 2.2, where it evidently denotes \"Text Style Transformation\", but the acronym is not explicitly defined in the caption of Figure 6 and again in Section 5. This redundancy and delayed definition confuse the reader and suggest a lack of editorial consistency. Readers must deduce its meaning from context or later descriptions.\n  - \"English P.\" appears in Table 1 but is never defined. From the nearby mention of \"paragraph-level English samples,\" one can infer that it stands for English Paragraph, yet this is not stated anywhere in the text or legend.\nSuch omissions may appear minor, but they significantly reduce readability for new readers and hinder reproducibility, as one cannot easily map metrics to task definitions.\n- The factuality evaluation for Portuguese and Vietnamese data relies on GPT-4 translation followed by QAFactEval (line 212–213). However, the paper does not justify or quantify how this translation step affects factual consistency. Without back-translation checks or metric comparisons, it is unclear whether observed performance differences reflect model ability or translation artifacts.\n- Prior work (such as M4GT, MAGE, and MULTITuDE) already explores multilingual or task-specific MGT; contribution is incremental without a stronger methodological leap."}, "questions": {"value": "- Could the benchmark be extended to mixture-aware or segment-level labeling to better reflect real Wikipedia editing?\n- How significant is translation bias in PT/VI QAFactEval via GPT-4?\n- In what specific aspects does TSM-BENCH advance beyond prior work (such as M4GT, MAGE, and MULTITuDE) in design or analysis?\n- Did supervised models trained on English task-specific data show any zero-shot gains on Portuguese or Vietnamese? \n- Could the authors expand on whether overfitting features are artifacts of Wikipedia formatting specifically or general stylistic cues common across corpora?\n\n\nIf the authors can substantively address the questions and weaknesses outlined above, I would be inclined to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hz2188yi9K", "forum": "zimuL7ZmIi", "replyto": "zimuL7ZmIi", "signatures": ["ICLR.cc/2026/Conference/Submission22069/Reviewer_gz51"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22069/Reviewer_gz51"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817375921, "cdate": 1761817375921, "tmdate": 1762942048046, "mdate": 1762942048046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TSM-BENCH, a multilingual, multi-generator, and multi-task benchmark for detecting machine-generated text (MGT) in realistic Wikipedia editing scenarios. The key insight is that existing benchmarks primarily evaluate detectors on generic text generation (e.g., \"Write an article about X\"), while real-world Wikipedia editors use LLMs for specific constrained tasks such as summarization, paragraph writing, and style transfer. The authors demonstrate that task-specific MGT more closely resembles human-written text (HWT) than generic MGT, making detection significantly more challenging."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes a valuable contribution by shifting focus from generic to task-specific MGT detection, which better reflects real-world LLM usage patterns. The identification of generalization asymmetry between task-specific and generic training data is an interesting finding. The multi-dimensional benchmark design (multilingual, multi-task, multi-generator) is comprehensive.\n2. The paper is well-written with clear motivation and effective use of figures (especially Figure 1 and Figure 2). The experimental setup is described in detail, and results are presented in an organized manner. The distinction between generic and task-specific MGT is clearly articulated."}, "weaknesses": {"value": "**1. Data Provenance and Contamination Concerns**\n\nA fundamental assumption of this work is that the sampled Wikipedia texts are purely human-written. However, recent studies have documented that Wikipedia already contains LLM-generated content mixed with human contributions. For instance, [Detecting LLMs in the Wild: A Field Study of Latent Misuse](https://openreview.net/pdf?id=R2bp8xLLao) and [Wikipedia in LLM era](https://arxiv.org/abs/2503.02879) provide evidence of LLM usage in Wikipedia editing. The paper does not address this contamination issue or provide any verification methodology to ensure the \"human-written\" samples are actually purely human-written. This is a critical concern that potentially undermines the benchmark's validity. The authors should at minimum: (i) discuss this limitation explicitly, (ii) provide some analysis or filtering approach to mitigate contamination risk, or (iii) acknowledge the uncertainty in their ground truth labels.\n\n**2. Missing Comparison with Highly Related Work**\n\nThe paper fails to discuss or compare with WETBench (cited as Quaremba et al., 2025 in the references), which appears to be a highly related concurrent work also focusing on detecting task-specific MGT on Wikipedia. This is a significant omission. The authors should provide a detailed comparison explaining: (i) how TSM-BENCH differs from WETBench, (ii) whether there is overlap in the data sources, and (iii) how the findings complement or contradict each other. Without this comparison, it's difficult to assess the novelty and positioning of this work. Moreover, like \"Wikipedia in the Era of LLMs\" and \"Studying the Role of LLMs on Wikipedia\" also discuss the LLM usage in Wikipedia while do not get any discussion and citation.\n\n**3. Oversimplified Task Formulation for \"Real-World\" Claims**\n\nWhile the paper claims to reflect \"real-world Wikipedia editing practices,\" the task formulations are actually quite coarse-grained and unrealistic:\n\n- **Paragraph-level detection is insufficient**: Modern Wikipedia editing with LLMs rarely involves writing entire paragraphs or complete continuations purely with AI. Real-world usage is much more fine-grained, involving sentence-level or even token-level human-AI collaboration. Recent work on collaborative writing has extensively studied this: [Coauthor (Clark et al., 2018)](https://arxiv.org/pdf/2201.06796) and [LLM-as-a-Coauthor](https://arxiv.org/abs/2401.05952) all demonstrate that human-AI co-writing happens at much finer granularities.\n\n- **Binary classification may not be appropriate**: For the Paragraph Continuation task, the authors mention \"blending HWT and MGT\" but still frame it as binary classification at the paragraph level. If the continuation mixes human and machine text, shouldn't the detection task be formulated at a finer granularity? Labeling an entire human-machine hybrid paragraph as simply \"MGT\" seems oversimplified and may not align with practical needs. You can refer to LLM-as-a-Coauthor and Llm-detectaive for fine-grained HWT and MGT detection.\n\n- **Impact on results interpretation**: This coarse-grained formulation might partially explain why white-box and black-box methods show such poor performance (<60% accuracy in Table 2). These methods rely on statistical patterns that may be diluted when human and machine text are mixed at the paragraph level. The strong performance of supervised methods (which can learn task-specific artifacts) compared to zero-shot methods supports this concern. Is the low performance due to the inherent difficulty of the task, or due to the mismatch between the evaluation setup and the actual detection problem?\n\n**4. Language Selection Lacks Justification**\n\nThe choice of Portuguese and Vietnamese as the multilingual evaluation languages is not well motivated. More widely-studied and resource-rich languages like Chinese or French would seem like natural choices, given that: (i) they have substantial Wikipedia presence, (ii) there are more available detection tools and LLMs for these languages, and (iii) they would increase the benchmark's applicability to a broader research community. The paper should provide clear rationale for the specific language selection beyond data availability.\n\n**5. Limited Discussion of Related Work on Human-AI Collaborative Writing**\n\nThe related work section misses an important body of literature on human-AI collaborative writing and mixed-authorship text. Works on co-writing, human-in-the-loop generation, and collaborative authorship have direct relevance to the \"Paragraph Continuation\" task and the real-world editing scenarios the paper aims to capture. This omission weakens the positioning of the work within the broader context of human-AI collaboration research."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xsGJEgNMCw", "forum": "zimuL7ZmIi", "replyto": "zimuL7ZmIi", "signatures": ["ICLR.cc/2026/Conference/Submission22069/Reviewer_qQWg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22069/Reviewer_qQWg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965431537, "cdate": 1761965431537, "tmdate": 1762942047839, "mdate": 1762942047839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce TSM-BENCH, a new benchmark for evaluating machine-generated text (MGT) detectors in realistic Wikipedia editing scenarios. TSM-BENCH comprises 152,910 machine-generated texts across three languages, four editing tasks grounded in actual Wikipedia editing practices, and six different language models. Results show taht Off-the-shelf detectors achieving over 93% accuracy on generic MGT drop to between 47-73% on task-specific data. The authors discover a \"generalization asymmetry\": detectors trained on task-specific data can generalize to generic data and even across domains, but detectors trained on generic data fail catastrophically when applied to task-specific scenarios. Through feature analysis, the authors find that Models trained on generic data overfit to superficial artifacts like section formatting markers rather than learning genuine linguistic patterns that distinguish machine from human text."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. TSM-BENCH is large-scale and comprehensive, featuring 152,910 machine-generated texts across 3 languages, 4 tasks, and 6 different generators.\n\n2. The paper identifies a significant limitation in existing machine-generated text (MGT) detection benchmarks: previous work has primarily focused on generic text generation, while real-world users employ LLMs for specific, constrained tasks.\n\n3. The authors evaluate 12 different detectors across multiple model families and conducts five comprehensive experiments including zero-shot performance, supervised learning, out-of-domain generalization, feature analysis, and cross-task transfer. The findings are significant and help explain why existing detectors may be unreliable in real-world settings."}, "weaknesses": {"value": "1. While authors adapt prompts from the natural language generation literature shown to be most effective for each task, this might not reflect how people using LLMs for the task.\n\n2. The study relies heavily on accuracy as the primary metric, with F1 scores provided but not deeply analyzed. In AI content detection task,  metrics like false positive rates also provide valuable insight.\n\n3. The feature analysis in Experiment 4 shows that mDeBERTa trained on generic data overfits to surface-level features, while the model trained on task-specific data focuses more on semantically meaningful tokens. However, this analysis only examined mDeBERTa, Is this pattern of feature learning generalizable across other detection model families?\n\n4. The prompt evaluation uses GPT-4o Mini as the sole judge for selecting the best prompts. This introduces potential biases and lacks validation against human judgments or alternative automatic metrics, which could affect the quality assessment of generated text."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eXMCbCmRfT", "forum": "zimuL7ZmIi", "replyto": "zimuL7ZmIi", "signatures": ["ICLR.cc/2026/Conference/Submission22069/Reviewer_TeVN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22069/Reviewer_TeVN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762083051651, "cdate": 1762083051651, "tmdate": 1762942045482, "mdate": 1762942045482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}