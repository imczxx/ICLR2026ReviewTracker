{"id": "W4BwSXVxhp", "number": 6531, "cdate": 1757987992132, "mdate": 1759897909488, "content": {"title": "LinAJD: Scalable Gradient-Free Jailbreak Defense via Linearly Separable Embeddings", "abstract": "Large language models (LLMs) continue to face vulnerabilities to jailbreak attacks. Existing adversarial training (AT) approaches mitigate these attacks yet typically require expensive gradient-based perturbations and substantial auxiliary datasets. In this work, we propose \\textbf{Linear Adversarial Jailbreak Defense (LinAJD)}, a gradient-free framework that exploits the linear separability of harmful and safe prompts in embedding space. LinAJD provides a highly efficient framework for adversarial training, delivering up to $4\\times$ faster forward–backward pass speed and reducing total data usage by over $90$%. Under this framework we introduce two variants, LinAJD-D and LinAJD-S, based on different preference optimization strategies. Empirical results on multiple open-source models show that LinAJD achieves state-of-the-art robustness against a wide range of jailbreak attacks, with fine-tuned LLaMA-2-7B model even reducing the success rate of a recent white-box attacks to $0$%. At the same time, LinAJD maintains a favorable robustness–utility tradeoff, as general performance experiences only minor degrade without reliance on extra utility datasets. We further analyze the effects of data quality, safety alignment, and domain shifts, offering deeper insight into LinAJD’s robustness and generalizability. Our code is available at https://anonymous.4open.science/status/LinAJD-anon-4BBE.", "tldr": "", "keywords": ["Large Language Models", "Robustness and Safety", "Jailbreak Defense", "Adversarial Training"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6f5368dfae17939dd4860414eff41544e233f18.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors propose LinAJD, the Linear Adversarial Jailbreak Defense for large language models. In contrast to previous works, they craft perturbations based on the linearity of the safety bound. Extensive experiments on multiple open-source models indicate the efficiency and effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 The soundness of the method is good.\n\n2 The writing is easy to follow.\n\n3 The experiments are relatively solid."}, "weaknesses": {"value": "1 The categorization of related work needs to be optimized. In Section 2, the authors attribute the SmoothLLM defense as one variant of adversarial training for the jailbreak defense. However, as stated in this paper, SmoothLLM is a defense that “perturbs multiple versions of a given prompt and aggregates their outputs to detect adversarial behavior through consensus.” With this definition, it seems that it is an input preprocessing defense instead of adversarial training.\n\n2 With the proceeding of the training process, the embedding features of the target models could diverge from the original distribution that the pretrained classifiers are trained on. This could explain why in adversarial training, it usually requires generating adversarial samples on-the-fly instead of prior to the training. Thus, adopting the pretrained classifier could weaken the defense.\n\n3 The evaluations are performed against the optimization-based attacks, such as GCG and SCAV which adversarial training is good at dealing with. However, in addition to optimization-based attacks, query-based attacks [1-3] and context-based attacks [3-4] are demonstrated to be powerful in reality. Authors should perform experiments across these attacks to demonstrate the generalization of the proposed method.\n\n4 As claimed in this paper, the LinAJD achieves a good trade-off between efficiency and performance. Contrastly, PAT in [5] and RPO in [6] propose to perform adversarial training on the input prompts, making it more efficiently and transferable to the closed-source model. I recommend that authors discuss both of them in the related work section and select them as baselines.\n\n\n[1] Jailbreaking black box large language models in twenty queries\n\n[2] Tree of attacks: Jailbreaking black-box llms automatically\n\n[3] AutoDAN-Turbo: A Lifelong Agent For Strategy Self-exploration to Jailbreak LLMs\n\n[3] Jailbreak and guard aligned language models with only few in-context demonstrations\n\n[4] Improved few-shot jailbreaking can circumvent aligned language models and their defenses\n\n[5] Fight Back Against Jailbreaking via Prompt Adversarial Tuning\n\n[6] Robust prompt optimization for defending language models against jailbreaking attacks"}, "questions": {"value": "1 Noticing that experiments are performed on LLMs of smaller scales, such as 7B and 8B, will it work for models of larger scales, such as Llama2-13B or Qwen2.5-32B?\n\n2 Why adopt the DPO and SimPO as the objectives for learning? Why not directly fine-tune the target models with supervised fine-tuning (SFT)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QPUc65DtSr", "forum": "W4BwSXVxhp", "replyto": "W4BwSXVxhp", "signatures": ["ICLR.cc/2026/Conference/Submission6531/Reviewer_Cg8Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6531/Reviewer_Cg8Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892302464, "cdate": 1761892302464, "tmdate": 1762918886387, "mdate": 1762918886387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel technique for adversarial training which users activation perturbations instead of iterative gradient-base attacks. Prior work suggests that safe and unsafe final-token prompt embeddings are linearly separable. This work leverages this fact to greatly decrease the cost of activation-space perturbation attacks, by solving the closed form linear equation to create a perturbation that simply moves an unsafe prompt across the decision boundary. This makes it cheap enough to apply in parallel across all layers.\n\nThey propose two different training setups, one with and without a reference model, to prefer harmless responses over harmful responses.\n\nThey train on HarmBench, train a classifier with 20 pairs of safe and unsafe prompts, and evaluate utility on MMLU, ARC-C, MBPP, GSM8K, and Harmless. This decreases the adversarial success rate of the previously successful attacks, as well as decreasing the hamfulness of successful attacks. They then study the computational efficiency, response length, cross-domain robustness, and hyperparameter sensitivity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The paper proposes a novel gradient-free method for adversarial perturbations based on the finding that harmful and harmless prompts are generally linearly separable. As far as I am aware, this method is original.\n\nQuality: The paper supports its core claims with empirical results. The method learns to interrupt unsafe responses in an unsupervised manner.\n\nClarity: The paper is generally well-written, and has a particularly clear and helpful explanation of its underlying idea.\n\nSignificance: Adversarial robustness is an important problem, and this paper proposes an effective method for constructing latent space perturbations without gradients, drastically decreasing computational cost."}, "weaknesses": {"value": "* The A-Score is somewhat non-standard, and could use more explanation -- it seems to me to be a measure of how useful the successful attacks are, but with general measures of answer quality rather than more specific harmfulness evaluation such as in StrongREJECT.\n* It would be interesting to see data-controlled experiments -- the proposed method is claimed to be more data efficient, but if it's also more compute efficient then why not use the same amount of data as other versions? This presumably would make the results stronger.\n\nMinor:\n* It's unclear why the LinAJD-D experiments are not repeated for Zephy and Phi3\n* The results are substantially stronger for Llama than other models, which could be worth elaborating on.\n* The results are strongest for the SCAV attack, and weakest for the LAA attack, and it could be worth elaborating on why."}, "questions": {"value": "* Why does the method result in a 90% reduction in data usage? It seems like the results would be stronger using the same amount of data for every method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DQi2UAEG2m", "forum": "W4BwSXVxhp", "replyto": "W4BwSXVxhp", "signatures": ["ICLR.cc/2026/Conference/Submission6531/Reviewer_q78C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6531/Reviewer_q78C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934517477, "cdate": 1761934517477, "tmdate": 1762918885904, "mdate": 1762918885904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LinAJD, an efficient adversarial training (AT) algorithm for LLMs. The main idea behind the proposed LinAJD is to use trained layer-wise adversarial embedding classifiers to directly generate closed-form optimal adversarial perturbations for inputs from each layer of LLMs. Experiments are (mainly) conducted on three open-source LLMs and four jailbreak attacks, which show that the proposed method enjoys better efficiency and jailbreak robustness than the existing CAT baseline method (where adversarial perturbations are only performed on the input token embeddings of LLMs)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors design an efficient training-time adversarial example generalization pipeline that uses trained layer-wise harmful embedding classifiers to directly obtain closed-form optimal adversarial embedding perturbations for the inputs of each layer. Such a design is smart and novel.\n\n2. In the experiments, the authors have also considered a more realistic setting where the harmful embedding classifiers are updated during the AT training process. I appreciate that."}, "weaknesses": {"value": "1. Robustness evaluation experiments are conducted on four jailbreak attacks, but most of these attacks (except GCG) are not common baselines adopted in existing jailbreak robustness evaluations such as those in [r1, r2, r3, r4]. Therefore, I suggest that the authors evaluate their proposed defense against more representative and stronger jailbreak attacks such as [r5, r6, r7, r8].\n\n2. The main AT experiments are conducted on three models (i.e., Table 2), which, from my perspective, might not be enough. Additional experiments on more LLM families such as Qwen2.5/3, Gemma-2/3, Vicuna, and Mistral are highly suggested.\n\n3. **Missing details about the construction of the dataset for training the layer-wise classifiers.** I wonder how the harmful and benign prompts were collected. Were they directly collected from public datasets or synthesized from jailbreak attacks? If the trainset were collected from public datasets, would they not be adversarial enough? On the other hand, if the trainset was synthesized from jailbreak attacks, how would the leveraged jailbreak attacks affect the performance of the trained classifiers as well as the overall AT? Please comment.\n\n\n**References**\n\n[r1] Xhonneux et al. Efficient Adversarial Training in LLMs with Continuous Attacks. NeurIPS 2024.\n\n[r2] Yu et al. Robust LLM safeguarding via refusal feature adversarial training. ICLR 2025.\n\n[r3] Dekany et al. MixAT: Combining Continuous and Discrete Adversarial Training for LLMs. NeurIPS 2025.\n\n[r4] Fu et al. Short-length Adversarial Training Helps LLMs Defend Long-length Jailbreak Attacks: Theoretical and Empirical Evidence. NeurIPS 2025.\n\n[r5] Chao et al. Jailbreaking Black Box Large Language Models in Twenty Queries. arXiv 2023.\n\n[r6] Hayase et al. Query-Based Adversarial Prompt Generation. NeurIPS 2024.\n\n[r7] Sadasivan et al. Fast Adversarial Attacks on Language Models In One GPU Minute. ICML 2024.\n\n[r8] Andriushchenko et al. Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks. ICLR 2025."}, "questions": {"value": "See **Weaknesses**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MbiJqudDDO", "forum": "W4BwSXVxhp", "replyto": "W4BwSXVxhp", "signatures": ["ICLR.cc/2026/Conference/Submission6531/Reviewer_EPGM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6531/Reviewer_EPGM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762174346512, "cdate": 1762174346512, "tmdate": 1762918885016, "mdate": 1762918885016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LinAJD, a novel framework for defending Large Language Models (LLMs) against jailbreak attacks through a scalable, gradient-free adversarial training (AT) approach. The authors identify that current gradient-based AT methods are computationally expensive, require substantial auxiliary data, and can degrade model utility. The core insight of LinAJD is to leverage the empirically observed linear separability of safe and harmful prompts in the LLM's embedding space. Instead of using costly iterative gradient-based methods to find adversarial perturbations, LinAJD simplifies the problem into a linear one, allowing for the generation of deterministic, embedding-level perturbations via a closed-form solution. This approach aims to efficiently \"harden\" the linear safety boundary within the model.\n\nIn addition, this paper presents two variants, LinAJD-D and LinAJD-S, which are based on different preference optimization algorithms. Through comprehensive evaluations on models like LLaMA-2-7B and Phi-3, the authors claim that LinAJD achieves state-of-the-art (SOTA) robustness against a wide range of jailbreak attacks, significantly outperforming prior work like R2D2 and CAT/CAPO. Key results include a reported 4x speedup in training, a 90% reduction in data usage, and even reducing the success rate of a specific white-box attack to 0%, all while incurring only minor degradation in general model performance and without needing auxiliary utility datasets. The paper also provides systematic analyses of factors influencing defense performance, such as data quality and domain shifts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Good Novelty and Potential Impact: The main advantage of this work lies in proposing a gradient-free, closed-form solution method for generating adversarial perturbations, directly addressing a significant pain point in the security of large-scale language models (LLMs): the high cost of robust alignment. If this method proves to be as effective and broadly generalizable as described, it could represent a critical advancement in making robust security alignment more achievable and practical.\n2. Exceptional Efficiency: The reported efficiency gains are a major contribution. A 4x speedup in training and a 90% reduction in data requirements are not merely incremental improvements; they are transformative. This makes the process of adversarially training LLMs substantially more feasible in practice, thereby lowering the barrier to accessing robust defense techniques. Figure 1 provides a compelling visual summary of this efficiency-robustness trade-off, clearly showing that LinAJD achieves superior results with significantly fewer training steps.\n3.Strong Empirical Results and Comprehensive Evaluation: The paper claims state-of-the-art (SOTA) performance, supported by comparisons against multiple recent and relevant baselines (e.g., R2D2, CAT, CAPO) across various models. The claim of reducing a white-box attack's success rate (ASR) to 0% is particularly striking and demonstrates the potential to achieve a high level of robustness. Furthermore, the intention to analyze factors like data quality and domain shifts indicates that the evaluation is comprehensive and scientifically rigorous, which is commendable."}, "weaknesses": {"value": "1. Brittleness of the Core Assumption: The success of the entire framework is predicated on the assumption of linear separability. While this is supported by recent work, it is likely an oversimplification of the true, complex geometry of the embedding space. The most sophisticated attacks may not lie along a simple linear boundary. The paper needs to robustly address the limitations of this core assumption.\n2. Opacity of Key Methodological Details: The initial sections of the paper repeatedly mention a \"closed-form solution\" but fail to provide its mathematical derivation, nor has its validity been effectively demonstrated. This is a major weakness that the paper needs to address.\n3. Potential Risk of Overfitting: While the paper avoids overfitting to specific gradient-based attack algorithms, it may introduce a risk of overfitting to a specific geometry—namely, the assumed linear boundary. An adaptive adversary, aware of the LinAJD defense strategy, would no longer waste effort on attacks that cross this linear boundary. This problem requires further explanation and discussion.\n4. Uncertain Scalability to Larger Models: The experiments were conducted on models with up to 7B parameters. A critical and open question is whether the clean linear separability property still holds for much larger and more powerful models (e.g., 70B models). The geometric properties of the embedding space can change significantly with model scale and architecture. The paper's claims of scalability would be much stronger if supported by either experiments on larger models or a compelling theoretical argument as to why this property should persist."}, "questions": {"value": "1. On the Core Assumption: How did you validate the linear separability assumption on your specific models and datasets? How does your method perform if the boundary is not perfectly linear? For instance, what is its robustness against attacks specifically optimized to find non-linear decision boundaries?\n2. On Adaptive Attacks: The 0% ASR claim is very powerful. However, this is likely against a known, static attack. What is the performance of LinAJD against an adaptive white-box adversary who has full knowledge of your defense mechanism and whose objective is to actively find a non-linear path to bypass the hardened boundary? Such an evaluation is the gold standard for measuring defenses and is crucial for an ICLR paper.\n3. On Utility Evaluation: You claim that LinAJD maintains model utility with only \"minor degradation.\" To make this claim concrete, could you please provide quantitative results on a suite of standard general-capability benchmarks? A comparative table showing the performance of the base model, LinAJD-D/S, and other defense methods on these benchmarks would be essential.\n4. On Method Variants: What are the specific differences between the \"preference optimization strategies\" underlying LinAJD-D and LinAJD-S? How does this choice (e.g., DPO vs. another algorithm) specifically affect the mathematical formulation of the perturbation and the resulting trade-offs between robustness, utility, and training dynamics?\n5. On Attack Generality: How does your framework conceptualize and defend against attacks that are not easily represented as small embedding-space perturbations? For example, attacks based on complex semantic manipulation, multi-turn role-playing, or logical reasoning exploits. Does the static, geometry-based view of defense remain effective in these more abstract attack scenarios?\n6. On Figure Readability: The font in Figure 4 is a bit small, and the numbers above the bars in Figure 5 are also small. We suggest adjusting them to improve readability.\n7. In the \"Efficiency evaluation\" section on page 6, you mention a speedup \"by over 60×.\" Could you please clarify which specific metric is improved by over 60 times compared to the baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l003aylAqy", "forum": "W4BwSXVxhp", "replyto": "W4BwSXVxhp", "signatures": ["ICLR.cc/2026/Conference/Submission6531/Reviewer_M7u1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6531/Reviewer_M7u1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762239375907, "cdate": 1762239375907, "tmdate": 1762918884630, "mdate": 1762918884630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}