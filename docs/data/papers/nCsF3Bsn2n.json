{"id": "nCsF3Bsn2n", "number": 13063, "cdate": 1758213207466, "mdate": 1759897467723, "content": {"title": "Probabilistic Kernel Function for Fast Angle Testing", "abstract": "In this paper, we study the angle testing problem in high-dimensional Euclidean spaces and propose two projection-based probabilistic kernel functions, one designed for angle comparison and the other for angle thresholding. Unlike existing approaches that rely on random projection vectors drawn from Gaussian distributions, our approach leverages reference angles and employs a deterministic structure for the projection vectors. Notably, our kernel functions do not require asymptotic assumptions, such as the number of projection vectors tending to infinity, and can be both theoretically and experimentally shown to outperform Gaussian-distribution-based kernel functions. We further apply the proposed kernel function to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X-3X higher query-per-second (QPS) throughput compared to the state-of-the-art graph-based search algorithm HNSW.", "tldr": "We propose two probabilistic kernel functions for angle testing, which can be used to accelerate vector-based similarity search.", "keywords": ["Randomized algorithm", "Locality Sensitive Hashing", "Directional statistics"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef4eed8c1311741a700e4188154db480a33f8687.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the angle testing problem as part of similarity search in high-dimensional Euclidean spaces. The key idea is to depart from conventional random projection methods based on Gaussian distributions, and instead propose probabilistic kernel functions with deterministic structures that leverage reference angles. Unlike prior approaches, the proposed method does not rely on asymptotic assumptions and theoretically and experimentally outperforms Gaussian-based methods. Applied to approximate nearest neighbor search (ANNS), the proposed approach achieves 2.5–3× higher query throughput (QPS) compared to HNSW."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper's strengths are as follows.\n\n(1) The paper presents a theoretically rigorous approach to the angle testing problem without asymptotic assumptions, proposing probabilistic kernel functions with deterministic structures.\n\n(2) By designing appropriate projection vector structures, the method improves estimation accuracy and further demonstrates that Gaussian structures are suboptimal.\n\n(3) The proposed method can be easily integrated into existing algorithms such as HNSW, suggesting strong practical applicability.\n\n(4) Extensive experiments on multiple datasets show that the proposed method achieves both high speed and high accuracy."}, "weaknesses": {"value": "This paper's weaknesses are as follows.\n\n(1) Constructing the projection vector structures is computationally expensive and could become a bottleneck for extremely large-scale datasets.\n\n(2) Minor typographical issue found: Line 153: “ZS(·))” → “ZS(·)”."}, "questions": {"value": "My questions about this paper are as follows.\n\n(1) Since the proposed method is based on probabilistic kernel functions, it presumably involves stochastic errors. Is it possible to predict or estimate the error rate analytically?\n\n(2) How does the error rate depend on the projection vector structure? Can this relationship be analyzed theoretically?\n\n(3) How should the number of projection vectors m and the number of subspaces L be optimized? These parameters likely depend on the data distribution, but what practical optimization strategy do the authors envision for real-world applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D58RdTMGfJ", "forum": "nCsF3Bsn2n", "replyto": "nCsF3Bsn2n", "signatures": ["ICLR.cc/2026/Conference/Submission13063/Reviewer_mXUD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13063/Reviewer_mXUD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778983601, "cdate": 1761778983601, "tmdate": 1762923792679, "mdate": 1762923792679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The most commonly used metrics in approximate nearest neighbor, i.e. Euclidean distance, cosine similarity, and inner product, can crucially be reduced to just inner product computation with appropriate preprocessing. The paper proposes two probabilistic kernel functions that approximate (1) the comparison of two inner products $\\langle q, v_1\\rangle$ and $\\langle q, v_2\\rangle$ and (2) whether $\\langle q, v\\rangle$ exceeds a given threshold. These approximations are much faster than the corresponding computations, making it possible to significantly accelerate approximate nearest neighbor search algorithms.\n\nThe proposed kernel functions are based on reference angles. Based on these functions, the authors introduce the KS1 test for CEOs tasks such as maximum inner product search and the KS2 test as a routing test in graph-based approximate nearest neighbor search. In the authors' experiments, the KS1 test yields slight improvements, while the KS2 test applied to the HNSW algorithm yields significant improvements."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes an interesting approach, angle testing, which has a central role in approximate nearest neighbor search. The authors propose methods for both angle comparison and thresholding. The provided theory is neat and well-motivated.\n\nThe proposed KS2 test can be generally applied to many different graph-based approximate nearest neighbor methods, and is amenable to an efficient SIMD implementation that yields a significant improvement in throughput when combined with the popular HNSW algorithm. This combination is also more efficient than combining HNSW with the earlier PEOs approach.\n\nThe experiments provided by the authors are comprehensive and use standard benchmark datasets."}, "weaknesses": {"value": "In practice, the improvement provided by KS1 over CEOs is very minor. The improvement of HNSW+KS2 over the earlier HNSW+PEOs is slightly larger but still relatively small.\n\nThe state-of-the-art methods for approximate nearest neighbor search combine graphs and quantization, e.g. Glass combines graphs with scalar quantization and SymphonyQG [1] combines graphs with RaBitQ, yet these are not included in the comparisons. The authors mention that they do not compare to e.g. Glass as it was deemed less efficient than PEOs in the corresponding paper, but experimental results in the PEOs paper do not seem to align with e.g. the results of ANN-benchmarks [2]. The KS2 test is not applicable to quantized vectors in e.g. uint8 or binary precision which are increasingly popular due to the high dimensionality of modern embedding datasets.\n\nA weakness in presentation is that the numerous references to analysis and explanations in the PEOs paper make it difficult to understand the paper without a thorough reading of the PEOs paper.\n\n[1] Gou et al. SymphonyQG: Towards Symphonious Integration of Quantization and Graph for Approximate Nearest Neighbor Search. Proceedings of the ACM on Management of Data. 2025.\n\n[2] Aumüller et al. ANN-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. Information Systems, 2020."}, "questions": {"value": "- Could the authors point out where in the provided code release the SIMD implementation of the KS2 test is located? I tried briefly looking in the code but was unable to figure out where in the code it was.\n\n- The integration of KS2 into HNSW moderately increases the index size and indexing time. How do these compare to HNSW+PEOs?\n\n- Have the authors considered testing HNSW+KS2 by integrating it to a standard benchmark such as ANN-benchmarks such that it is easier to reliably compare results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MfpNZ4qZCw", "forum": "nCsF3Bsn2n", "replyto": "nCsF3Bsn2n", "signatures": ["ICLR.cc/2026/Conference/Submission13063/Reviewer_yHFc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13063/Reviewer_yHFc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995998894, "cdate": 1761995998894, "tmdate": 1762923792417, "mdate": 1762923792417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two projection-based probabilistic kernel functions for fast angle testing in the context of similarity search in high-dimensional Euclidean spaces. Its core observation is that the reference angle can determine the estimation accuracy of angle comparison and testing, which in turn can be determined by the structure of the projection vectors. A detail theoretical analysis is presented to support the effectiveness of the proposed probabilistic kernel functions. Then, algorithms are proposed to compute the projection vectors, together with an analysis of the computational complexity. The proposed probabilistic kernel functions are integrated into high-dimensional similarity search algorithms (Maximum Inner Product Search and Graph-based ANN Search) to enhance their effectiveness and efficiency, which are verified with experiments on six commonly used benchmark vector datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper studies an important problem - high-dimensional similarity search.\n\n2. Detail theoretical analysis is presented to show the effectiveness and correctness of the proposed probabilistic kernel functions.\n\n3. Experimental results are presented to show the empirical effectiveness of the proposed probabilistic kernel functions and algorithms.\n\n4. Source code has been released."}, "weaknesses": {"value": "The empirical performance of the proposed kernel functions is not as strong. It produces marginally higher recall than the CEOs technique (Pham, 2021) as shown in Table 1, while HNSW+KS2 is only 1.1 to 1.3 times faster than HNSW+PEOs. Also, why are Tiny, GIST, and SIFT omitted from Table 1?\n\nIt would be good to discuss if the proposed kernel functions are guaranteed to lead to more accurate (and/or more efficient) similarity search than Gaussian-distribution-based kernel functions given the same number of projection vectors. If this is not guaranteed, it would be good to tune down the claim: \"can be both theoretically and experimentally shown to outperform Gaussian-distribution-based kernel functions\".\n\nPresentation issues:\n\n- It would be good to add figures to help illustrate the key concepts and proposed algorithms. \n\n- The statement \"On the other hand, $v^\\top u_{max}$ can be computed beforehand during the indexing phase and can be easily accessed during the query phase\" needs further clarification. How can this value be easily accessed given that $u_{max}$ depends on $q$? \n\n- Typo: \"with i.i.d. Gaussian entries)\" => \"with i.i.d. Gaussian entries.\"; \"and Additional experimental results\" => \"and additional experimental results\""}, "questions": {"value": "There are no further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0aFnzond6u", "forum": "nCsF3Bsn2n", "replyto": "nCsF3Bsn2n", "signatures": ["ICLR.cc/2026/Conference/Submission13063/Reviewer_XabQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13063/Reviewer_XabQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058480298, "cdate": 1762058480298, "tmdate": 1762923792170, "mdate": 1762923792170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes angle estimation from stochastic Gaussian projections to deterministic reference-angle-based kernels. Authors first derive theoretical bounds and apply them to accelerating graph-based retrieval. Specifically authors design (what seems to be) a pruning algorithm that eliminates certain search paths. Although the pruning approach is not new in principle, the resulting approach HNSW-KS2 outperforms a previously proposed variant (HNSW-PEO) by 10% – 30%, along with a 5% reduction in index size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Important topic and well motivated-problem \n\n2. Theoretically grounded approach that achieves substantial practical gains. \n\n3. The paper is well-written \n\n4. Source code is provided."}, "weaknesses": {"value": "1. Gains over HNSW-PEO are relatively modest (yet non-trivial!) and the method requires extra space, which is non-trivial in some cases. For example, it is >= 40% in the case of the SIFT dataset. \n2. Evaluation is only single-threaded.\n\n\n**Detailed comments:** \n\n**Please, do not respond to these, all questions are rhetorical. If suggested correction is not valid, just ignore it** \n\nEq. (2) Shouldn’t Z_{HS} be Z_S? \n\n \nL341 This is not understandable without a basic explanation of what a routing test is. \n\nL410 there is a missing dot after and HNSW+PEOs \n\nL427-428 On the other hand, in the high-recall region for Word, ScaNN outperforms HSNW+KS2 due to the connectivity issues of HNSW. -> This requires justification."}, "questions": {"value": "**Detailed comments:** \n\n \n\n1. Eq (1) does it really come from Theorem 3.1 in “Pham, Simple yet efficient algorithms for maximum inner product search via extreme order statistics.” It looks very different. \n\n2. Due to its ease of implementation, CEOs has been employed in several similarity search tasks (Pham, 2021; Andoni et al., 2015; Xu & Pham, 2024) -> CEO came after Andoni et al. Do you mean it was used in FALCON++? \n\n3. L354-356 you claim that you skip a distance computation. However, this doesn’t seem to be correct according to Alg 6. I think you also do not add a node to the queues. Basically, this seems to be search pruning approach, not the the approach to reduce the number of distance computations. Please, clarify. \n\n4. L376 Why didn’t you test multi-threaded retrieval as well? \n\n \n\n5. Is L the number of projections? It will be great to clarify in the experimental section. \n\n6. Does Figure 1 compare PEO and KS2 using the same L? \n\n7. Gains over CEO are marginal. For example (see Table 1), Probe@100: 6.98 vs 6.9, which is 1% relative. PEO improves upon reverse CEO and you improve upon PEO by double digits percentage points (e.g., 30%). How can you explain this discrepancy? What makes new kernel functions to be more effective? Is it due to the introduction of the threshold approximation? I think it is an important clarification to add to the paper as well. \n\n8. Thank you for sharing the code: which part of the code benchmarks HNSW-PEO though? I think it benchmarks just HNSW-K1/K2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dK9Spna0T2", "forum": "nCsF3Bsn2n", "replyto": "nCsF3Bsn2n", "signatures": ["ICLR.cc/2026/Conference/Submission13063/Reviewer_EhUf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13063/Reviewer_EhUf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117282627, "cdate": 1762117282627, "tmdate": 1762923791749, "mdate": 1762923791749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}