{"id": "bxbs91b5o2", "number": 8289, "cdate": 1758077574720, "mdate": 1763000309385, "content": {"title": "MCM: Multi-layer Concept Map for Efficient Concept Learning from Masked Images", "abstract": "Masking strategies commonly employed in natural language processing are still underexplored in vision tasks such as concept learning, where conventional methods typically rely on full images. However, using masked images diversifies perceptual inputs, potentially offering significant advantages in concept learning with large-scale Transformer models. To this end, we propose Multi-layer Concept Map (MCM), the first work to devise an efficient concept learning method based on masked images. In particular, we introduce an asymmetric concept learning architecture by establishing correlations between different encoder and decoder layers, updating concept tokens using backward gradients from reconstruction tasks. The learned concept tokens at various levels of granularity help either reconstruct the masked image patches by filling in gaps or guide the reconstruction results in a direction that reflects specific concepts. Moreover, we present both quantitative and qualitative results across a wide range of metrics, demonstrating that MCM significantly reduces computational costs by training on fewer than 75\\% of the total image patches while enhancing concept prediction performance. Additionally, editing specific concept tokens in the latent space enables targeted image generation from masked images, aligning both the visible contextual patches and the provided concepts. By further adjusting the testing time mask ratio, we could produce a range of reconstructions that blend the visible patches with the provided concepts, proportional to the chosen ratios.", "tldr": "", "keywords": ["masking strategies", "concept learning", "disentanglement", "Transformers", "generative model"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/de271d229a65211693a45f8ca684ac91ff863fae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes Multi-layer Concept Map (MCM), a vision Transformer framework that combines masked image reconstruction with concept learning. The core idea is an asymmetric encoder–decoder architecture where learnable concept tokens are processed alongside unmasked tokens through multiple encoder layers, and these concept tokens are then used via cross-attention in several decoder layers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "MCM is demonstrated on the CelebA dataset for predicting face attribute concepts and reconstructing masked images, which enables image editing by manipulating concept tokens. The authors highlight architectural novelty and improved efficiency as the main contributions."}, "weaknesses": {"value": "The paper suffers from several critical weaknesses in novelty, experimental validation, and clarity of motivation, as detailed below.\n\n1. The architectural contributions are not truly novel. The asymmetric encoder–decoder with a lightweight decoder for masked image modeling is directly inspired by MAE; thus, MCM’s use of a mask-based asymmetric architecture is an application of known methods rather than a new invention. Further, Introducing learnable concept tokens is positioned as a key novelty, but this closely parallels ideas from concept bottleneck models and visual concept tokenization. Similarly, the proposed loss functions, the disentanglement loss and weighted concept loss, provide only incremental improvement and are standard techniques in concept learning, rather than innovative algorithms.\n\n2. All experiments are performed only on the CelebA dataset, and further, only 11 concepts (out of 40 attributes) are used.\nThis raises serious concerns regarding generalisation to other datasets, scalability with respect to the number of concepts, and overall practicality.\n\nThe decision to focus on 11 “easy” concepts is understandable for stability, but it likely inflates apparent performance. Harder or subtler concepts might reveal that MCM’s tokens do not generalise well. Moreover, without a principled selection criterion, this subset appears hand-tuned.\n\nIt would strengthen the paper to report results on all 40 CelebA attributes, even if weaker, to demonstrate generalisation. Also, to evaluate on additional datasets (e.g. LFWA) with non-overlapping concept sets.\n\n3. The authors claim MCM is “efficient”; however, the model uses only 25% masking, which modestly reduces computation at best.\nThe introduction of concept tokens likely offsets these savings, and there is no analysis of training/inference time or memory usage to support the efficiency claim. Without such evidence, the claim of efficiency is unconvincing.\n\n4. The sharp performance drop of ViT-Large at a 0.1 masking ratio is unjustified. If this is due to random initialisation sensitivity, that should be demonstrated with multiple seeds. Moreover, since the 0% (no mask) case achieves almost the same performance as the 0.25 mask, the masking strategy appears to provide only marginal benefit, calling into question whether it is essential to the proposed method at all.\n\n5. Table 2, which provides the only comparison to SOTA, represents an unfair setup. The baseline MAE + MLP uses a simple classifier on top of unmasked tokens representations, where the loss is overwhelmed by pixel reconstruction; the concept head receives tiny gradients that do not shape encoder features. In contrast, MCM injects concept supervision deeply throughout the model and benefits from stronger signals. Therefore, the comparison does not isolate architectural improvements, it conflates different supervision regimes.\n\n6. The visualisation of 0% masking in Figure 4 is counterintuitive. One would expect that more visible context leads to better reconstruction especially that the model is trained with only 25% masking. However, the model fails in this case, producing averaged outputs.\n\n7. The authors list “novel image editing capabilities for masked image reconstruction” as a main strength, stating that this is a functionality MAE cannot provide. However, there is no evidence supporting this claim. Even in Figure 6, the “edited” images look almost identical, same smiles, cheeks, glasses, and overall appearance. The model does not demonstrate any genuine or personalised semantic edits, calling into question this claimed advantage.\n\n-----------\n\nBeyond technical implementation, the purpose of the paper remains unclear. While the method combines MAE and concept learning, the authors never articulate a convincing reason why such a hybrid is necessary or beneficial. If the goal is concept-controllable image editing, there are stronger and more established models."}, "questions": {"value": "Addressing the mentioned limitations would significantly improve the clarity, credibility, and impact of the paper. However, the main concerns with this work relate to its overall novelty, motivation, and experimental scope, rather than clarifications that could be addressed in a short rebuttal. These issues appear more structural than technical and would likely require substantial rethinking rather than minor revisions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JuTMN78wWZ", "forum": "bxbs91b5o2", "replyto": "bxbs91b5o2", "signatures": ["ICLR.cc/2026/Conference/Submission8289/Reviewer_zFEV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8289/Reviewer_zFEV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670600515, "cdate": 1761670600515, "tmdate": 1762920220646, "mdate": 1762920220646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "pHAtrv2Qzf", "forum": "bxbs91b5o2", "replyto": "bxbs91b5o2", "signatures": ["ICLR.cc/2026/Conference/Submission8289/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8289/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763000308024, "cdate": 1763000308024, "tmdate": 1763000308024, "mdate": 1763000308024, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Multi-layer Concept Map (MCM), an architecture for learning human-interpretable visual concepts directly from masked images, and using those learned concepts to guide reconstruction of the missing regions. MCM first encodes only the visible patches of an input image along with a small set of learnable “concept tokens”; at each encoder layer, cross-attention updates these concept tokens so that different layers capture concepts at different levels of granularity, without using self-attention between concept tokens to keep them disentangled. The decoder then reconstructs the masked patches by attending to these layer-specific concept tokens (rather than just visible context), using an asymmetric design where every two encoder layers supervise one decoder layer to reduce compute. Training of MCM composes three loss terms: a masked reconstruction loss, a disentanglement loss that enforces that each concept token controls a distinct semantic factor, and a weighted concept loss that upweights rare concepts using frequency-based weights. The method is evaluated on CelebA for both concept prediction (accuracy, precision/recall/F1) and reconstruction quality (FID)。"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper has a solid motivation. The author tackles concept learning from masked images, which is underexplored, and proposes the MCM as an explicit solution. \n2. MCM benefits from asymmetric encoder-decoder design, similar to MAE, where only the visible patches are passed through the encoder. This design promotes efficiency. \n3. The work evaluates across multiple MCM sizes and performs comprehensive ablations that isolate each proposed component.\n4. Empirical results show good tradeoff in training time and performances."}, "weaknesses": {"value": "1. All experiments are on single dataset CelebA with only 11 attributes. Without empirical results on dataset with other attributes, this limits the generalization beyond faces or to richer concept taxonomies.\n2. Disentanglement loss and weighted concept loss are strictly tied up to the predefined list of concepts. This limits the continual learning or expanding the concept set. \n3. The author included the disentanglement loss to forcibly disable self-attention among concept tokens, which can hinder modeling dependencies (co-occurrence, mutual exclusivity) between concepts and limit compositional reasoning. The author should provide more analysis on the cons and pros for including this disentanglement loss. \n4. The direct application of this MCM is unclear. While the qualitative examples present in Figure 3 and 4 show the potential of image editing, it is unclear to me how MCM goes beyond prediction of finite facial attribute. The author should consider include broader implication of MCM and provide more insights into how this work will be beneficial."}, "questions": {"value": "1. I'm wondering to what extent the concept disentanglement is useful and beneficial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0ioZTiTJES", "forum": "bxbs91b5o2", "replyto": "bxbs91b5o2", "signatures": ["ICLR.cc/2026/Conference/Submission8289/Reviewer_eBrH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8289/Reviewer_eBrH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675156202, "cdate": 1761675156202, "tmdate": 1762920220001, "mdate": 1762920220001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Multi-layer Concept Map (MCM) for Concept learning. The goal is to learn concept representations that can be used both for classification and for concept-driven counterfactual image reconstruction. The motivations of the method are to make concept learning more efficient via masking image parts in the encoder. MCM is a encoder-decoder architecture, where a ViT encoder transforms image and learnable concept tokens. Image tokens are transformed via regular Self-attention blocks, and then used for keys and values in cross-attention which transforms the concept tokens. \nThe decoder ingests the encoded image tokens, as well as an appropriate number of mask tokens, and cross-attends to concept tokens. The model is trained via a mixture of three objectives:\n* decoder reconstruction loss\n* concept loss – forces the encoder outputs of the concept tokens to mimic CLIP text encoder embeddings of the concept names - potentially reweighted to account for the sparsity of certain concepts.\n* disentanglement loss – we replace random concept embeddings in the encoder output with their antonyms (i.e. embeddings of the antonyms of concept names), feed them through the decoder, feed the output through the encoder, and force the similarity of the encoder outputs to the antonyms.\n\nThe model exhibits the ability to accurately recognize the learned concepts, as well as generate counterfactual images with the reversed concepts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. MCM is an interesting way to generate counterfactual predictions.\n2. The proposed method learns strong concept representations.\n3. MCM enables flexible test-time control over edit strength via mask ratio, which is an appealing property."}, "weaknesses": {"value": "1. The architecture largely resembles prior cross-attention masked autoencoders; the main semantic capability is imported from CLIP embeddings rather than emerging from reconstruction. As such, the methodological novelty appears incremental.\n2. While MCM “does not require binary concept labels for training” (L358), it uses CLIP embeddings as targets derived from those exact binary labels - an almost equivalent form of supervision.\n3. The experimental section is limited to only the CELEB-A dataset. It remains unclear whether MCM scales to domains with richer compositional structure. The experiments would be more compelling if they included other datasets and concepts, for example the CUB dataset and prototypes learned by Prototypical Part Networks [1].\n4. Despite being one of the core contributions of the method, the counterfactual generation ability is not evaluated quantitatively (e.g. by reporting the efficacy of fooling the encoder with switched concepts). Leaning into this property of the model could potentially strengthen the contributions of the paper.\n5. Evaluation details (e.g., mask sampling, reconstruction metrics, classifier design) are insufficiently described for reproducibility, and no code is provided.\n6. CLIP is described as self-supervised, which is not accurate; CLIP uses natural-language supervision.\n\n[1] This Looks Like That: Deep Learning for Interpretable Image Recognition \nChaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, Cynthia Rudin https://arxiv.org/abs/1806.10574"}, "questions": {"value": "1. Per Table 1, the increased mask ratio leads to only marginal reduction in training time (e.g. from 9.8 to 8.1 hours). Previous literature (e.g. MAE [2]) reports three-fold speedup thanks to masking. Why is that the case?\n2. What would be the result of using binary concepts as targets and decoder inputs, instead of CLIP embeddings?\n3. Can we see a comparison of images generated by MCM and other approaches which report low FID, especially the MAE?\n\n\n[2] Masked Autoencoders Are Scalable Vision Learners \nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick\nhttps://arxiv.org/abs/2111.06377"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kuqYTUgYi0", "forum": "bxbs91b5o2", "replyto": "bxbs91b5o2", "signatures": ["ICLR.cc/2026/Conference/Submission8289/Reviewer_8PL8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8289/Reviewer_8PL8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826355374, "cdate": 1761826355374, "tmdate": 1762920219383, "mdate": 1762920219383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a novel concept learning framework that leverages masked images to learn a set of concept tokens. These tokens are randomly initialized and then optimized through a multi-layer concept decoder, which performs image reconstruction guided by the learned concepts. To enhance interpretability, a disentanglement loss is introduced to ensure that each concept token controls a distinct semantic aspect, while a weighted concept loss is employed to address the challenge of unbalanced concept distributions. Experiments on the CelebA dataset demonstrate that the learned concept tokens enable both accurate concept classification and controlled image reconstruction, outperforming existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and easy to follow. \n- The model trained on CelebA show both quantitative and qualitative improvements over baselines.\n- The asymmetric architecture make the training to be efficient."}, "weaknesses": {"value": "- The novelty is somewhat incremental, as it mainly integrates known components  into a single framework.\n- The method is only evaluated on CelebA, which is a relatively simple and small dataset; it is  unclear whether the approach generalizes to more complex or non-face domains.\n- The model will need the pretrained CLIP to get the concept embeddings, so it is somehow like distililling the knowledge but not acctually the proposed method's effect.\n- According to the ablation the proposed looses, the proposed losses improve the performance quite marginal."}, "questions": {"value": "- The model appears to be quite sensitive to the mask ratio, particularly for larger models. It would be helpful to discuss whether there exists a more principled way to determine the optimal mask ratio beyond grid search, especially when applying the method to large-scale datasets where exhaustive tuning becomes impractical."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KyOdql2iXV", "forum": "bxbs91b5o2", "replyto": "bxbs91b5o2", "signatures": ["ICLR.cc/2026/Conference/Submission8289/Reviewer_uiqj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8289/Reviewer_uiqj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849422278, "cdate": 1761849422278, "tmdate": 1762920218728, "mdate": 1762920218728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}