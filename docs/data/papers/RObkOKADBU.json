{"id": "RObkOKADBU", "number": 25519, "cdate": 1758368821608, "mdate": 1759896717630, "content": {"title": "CORDS - Continuous Representations of Discrete Structures", "abstract": "Many learning problems require predicting sets of objects without knowing their\nnumber in advance. Examples include object detection, molecular modeling, and\na variety of inference problems for scientific data, such as astrophysical source\ndetection. Existing methods often rely on padded representations, or must explicitly\ninfer the cardinality directly from data, which often poses challenges. We present\na novel strategy for addressing this challenge by casting prediction of variable\ncardinality as a continuous inference problem, where the number of objects is\nrecovered directly from field mass. Our approach, CORDS (Continuous Representations\nof Discrete Structures), provides a bijective representation that maps sets\nof spatial objects with features to continuous density and feature fields. Because\nthe mapping is invertible, models can operate entirely in field space and still be\ndecoded back to discrete sets. We evaluate CORDS across molecular generation\nand regression, object detection, simulation-based inference in astronomy, and a\nmathematical task that recovers local maxima, demonstrating robust handling of\nvariable cardinality with competitive accuracy.", "tldr": "We turn discrete objects into continuous fields that implicitly encode their count, offering a simple way to handle variable cardinality across tasks and domains.", "keywords": ["Continuous set representations", "Neural fields", "Variable-cardinality prediction", "Invertible encoding/decoding", "Diffusion and flow matching", "Object detection", "Molecular generation", "Simulation-based inference"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa1ccc49d1a4be2b4ed8db90c5cdb47f094341ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper claims to develop CORDS (continuous representation of discrete structures), which provides a \"bijective\" representation that maps sets of spatial objects with features to continuous \"density\" and \"feature fields\"."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None."}, "weaknesses": {"value": "The major problem with this paper is that it uses some jargon and terms that are not mathematically defined, nor are they standard in machine learning research. This hinders the ability of a reviewer to fully understand and judge this paper. For example, starting from the first sentence of the Abstract, what does \"number\" mean when the paper says \"their number in advance\"? The Abstract talks about existing methods in the Abstract, but no problem has been laid out. The Abstract then talks about a challenge that is not substantiated.  What does \"variable cadinality\" mean? What does \"bijective representing\" mean? What is \"Feature field\"? These are all non-standard terminologies that I have not seen before."}, "questions": {"value": "What is the mathematical/ML problem that the paper is solving?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "I have concerns about this paper's excessive reliance on AI, as I explained in the Summary section."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oGPQpTkps8", "forum": "RObkOKADBU", "replyto": "RObkOKADBU", "signatures": ["ICLR.cc/2026/Conference/Submission25519/Reviewer_7wBB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25519/Reviewer_7wBB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753345129, "cdate": 1761753345129, "tmdate": 1762943459965, "mdate": 1762943459965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of predicting the cardinality of object sets and proposes a method that utilize kernel based continuous field that can map to the finite set of spatial objects. In this case, the total density mass is the number of objects. The proposed CORDs provides an invertible mapping that enable the exact decoding from the field space to the discrete space. The authors demonstrate the effectiveness of proposed representation in various applications, including molecular generation, object detection, simulation-based inference and synthetic functions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method demonstrates a clean and clear idea that enable the encoding to continuous fields and exact decoding to discrete sets, which also makes the prediction of object cardinality feasible\n- This problem it tackles itself is a rather fundamental issue that can be further applied to many downstream tasks as shown in the paper without being restricted to specific use case\n- The bijective construction make the transition between discrete and continuous space smooth without the need of additional module"}, "weaknesses": {"value": "- The computational cost including the encoding and decoding process might increase when dealing with larger samples\n- The experimental performance of CORDS may fall behind some baselines, for instance, the performance in the molecular tasks, so what could be the benefit of this method compared to those baselines in this case."}, "questions": {"value": "- Now, the current representation is rather unified for all downstream tasks, do you think it might be valuable to calibrate current method to become some task-specific representation?\n- How the kernel choice and kernel parameter affect the construction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HTQQHgjX4n", "forum": "RObkOKADBU", "replyto": "RObkOKADBU", "signatures": ["ICLR.cc/2026/Conference/Submission25519/Reviewer_gPMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25519/Reviewer_gPMT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973140847, "cdate": 1761973140847, "tmdate": 1762943459794, "mdate": 1762943459794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to represent discrete sets of arbitrary cardinality as continuous density and feature fields. This enables models to predict/generate sets of objects without fixing cardinality in advance. The authors formally show this bijection. The density field is simply a sum of Gaussians centered at each object, whereas the feature field is the same except each Gaussian is weighted by the corresponding feature vector. Recovering a discrete set requires drawing samples to integrate the density field to find the cardinality $N$. Then, positions are recovered by fitting a Gaussian mixture model to the samples and densities and refining object positions with LBFGS. Feature vectors are recovered by solving a linear system with coefficients and constraints defined by the recovered Gaussians and feature field. The framework of CORDS generalizes Gaussians to generic kernels. CORDS is validated with experiments across 3D molecules, images, astrophysical signals, and local maxima of functions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Motivation for modelling cardinality is clear in the introduction.\n\nContextualization with related works is extensive without being wordy.\n\nThe paper is beautifully written with both intuitive descriptions as well as formalism. Figures are clear and visually appealing.\n\nThe proposed method is validated with experiments across multiple domains, showcasing the generality of CORDS. In 3D molecules, CORDS achieves state-of-the art generation quality out of field/voxel-based methods."}, "weaknesses": {"value": "The paper does not discuss its limitations enough. Representing 3D molecules as continuous fields still requires handling discrete samples as inputs to the neural network, so this blows up $N\\approx 20$ atoms into $M\\approx 1000$ points, and incurs extra cost in integrating densities, gradient-based recovery of positions, and a $O(N^3)$ linear solve to recover feature vectors. It is unclear how much overhead is added by each of these steps, as wall-clock timing, number of training steps, and compute resources are not reported for molecule generation."}, "questions": {"value": "1. In 3D molecule generation, how much is the computational cost of integration-based decoding, both in absolute wall-clock timing and relative to the neural network part? How long does training take, in wall-clock time and number of training steps, and what compute resources are used?\n2. Is it possible to make the kernel learnable?\n3. Why is it that (line 1129) \"feature solve typically dominates cost only for very small $\\hat{N}$\"?\n4. The bijection between discrete sets and continuous fields seems more promising in the other direction: mapping high-dimensional signals down to sparse discrete sets, like tokenization. Alternatively, one could coarse-grain large, discrete sets into continuous fields with fewer points. Does the CORDS framework enable/motivate any approaches in this line of thinking? In physics and chemistry, there are many continuous fields that if represented as discrete but sparse objects could reap benefits: electron densities, wavefunctions, scalar and vector fields in quantum field theory and statistical field theory (e.g. $\\phi^4$ theory or solutions to a Landau-Ginzburg Hamiltonian).\n5. Future work in decoding spectra (NMR, IR) with overlapping signals could be interesting.\n\nSome potentially related works are Gaussian splatting for rendering images [1] and representing electron density as sums of Gaussians [2].\n\n[1] Kerbl, B., Kopanas, G., Leimkühler, T., & Drettakis, G. (2023). 3D Gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4), 139-1.\n\n[2] Elsborg, J., Thiede, L., Aspuru-Guzik, A., Vegge, T., & Bhowmik, A. (2025). ELECTRA: A Cartesian Network for 3D Charge Density Prediction with Floating Orbitals. arXiv preprint arXiv:2503.08305.\n\n\nnit-picking:\n1. line 269: \"using Eq. equation 1\" has an extra word\n2. Several citations do not have parentheses around them.\n3. line 886: \"we will define Field of Graph is a quadruple\"\n4. line 1062: BIC is not defined\n5. line 1102: there is no mention of Laplacian/Epanechnikov kernels elsewhere in the paper\n6. line 1253: \"diffusion machinery\"\n7. When introducing kernels, it may be clearer to suggest that the reader imagine Gaussians as a typical example."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "enPXeyA5l7", "forum": "RObkOKADBU", "replyto": "RObkOKADBU", "signatures": ["ICLR.cc/2026/Conference/Submission25519/Reviewer_6aGq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25519/Reviewer_6aGq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150123040, "cdate": 1762150123040, "tmdate": 1762943459627, "mdate": 1762943459627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CORDS, a framework that maps variable-sized discrete sets (e.g., points with features) into continuous representations through density and feature fields. Each element contributes a kernel function centered at its position, whose integrated density encodes cardinality while the feature field carries attribute information. The author conducts experiments on diverse domains—molecular generation, object detection (MultiMNIST), scientific simulation inference, and detection of local maxima. Experiments show competitive performance and robustness to variable object counts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "CORDS provides an elegant formulation that continuously encodes cardinality, positions, and attributes in a single differentiable field, with provable invertibility. This offers a strong conceptual link between discrete structures and continuous implicit fields.\n\nThe framework is tested across four distinct domains, showing broad potential beyond a single application type."}, "weaknesses": {"value": "The approach relies on dense sampling (∼10³ points per structure) for high-fidelity reconstruction, which can become prohibitively expensive for large-scale data (e.g., complex molecules or real images). The paper lacks quantitative analysis of runtime, memory footprint, or scaling trends with respect to the number of objects or spatial resolution. Without this, the practical feasibility on larger datasets remains unclear.\n\nWhile CORDS achieves performance comparable to existing models on molecular generation task, it does not clearly surpass them in generation quality. Moreover, as the number of atoms increases, the computational overhead of the continuous-field representation (especially the sampling and kernel-matching stages) grows rapidly, potentially making the approach less scalable than discrete or equivariant diffusion models. Given these results, it remains unclear what practical advantage CORDS offers for molecular generation compared to current SOTA methods—beyond the theoretical elegance of its continuous formulation.\n\nHow are weights for LMSE and count penalty (λ) tuned in detection, and how do they affect OOD-count robustness?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dGq7jBbUWn", "forum": "RObkOKADBU", "replyto": "RObkOKADBU", "signatures": ["ICLR.cc/2026/Conference/Submission25519/Reviewer_BLiG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25519/Reviewer_BLiG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762168616873, "cdate": 1762168616873, "tmdate": 1762943459417, "mdate": 1762943459417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a representation for discrete data based on neural fields, named CORDS. The central idea is to define an invertible mapping between features and continuous fields. The authors show experiments with CORDS on different tasks/domains (molecular generation, object detection, simulation-based inference in astronomy)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The unification of continuous fields and discrete encodings is interesting.\n* The presentation is generally clear, and the theoretical derivations are also clear.\n* The idea is applied to different tasks/domains, which means that the method shows some generalization."}, "weaknesses": {"value": "* In molecular generation (Table 1) the performance is considerably lower than other models. In molecular regression (Table 4) the performance is also significantly worse than other methods (e.g. EGNN, PaiNN, SE-GNN). In particular, the dipole estimation has the worst error (0.086) of all methods, several times higher than other methods (e.g. PaiNN: 0.012, SE-GNN: 0.023). Results for object detection are limited and have a similar issue. This does not match the \"competitive performance\" described in the abstract. Results for OOD are relatively better, but generally worse than other methods."}, "questions": {"value": "* The idea seems interesting, but the method is not showing particularly good results for the chosen tasks. Moreover, the results do not support the initial claims.\n\n* For molecular benchmarks, please clarify if baseline numbers were re-run or taken from earlier papers (if any)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lzUxofaZNl", "forum": "RObkOKADBU", "replyto": "RObkOKADBU", "signatures": ["ICLR.cc/2026/Conference/Submission25519/Reviewer_YEsV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25519/Reviewer_YEsV"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission25519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762201548322, "cdate": 1762201548322, "tmdate": 1762943459210, "mdate": 1762943459210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank reviewers `YEsV`, `BLiG`, `6aGq`, `gPMT`, and `7wBB` for their time and detailed feedback. We are encouraged that several reviewers are receptive to the core idea: `6aGq` highlights the clear motivation, concise yet extensive related-work discussion, and the balance between intuitive descriptions and formalism, and notes that CORDS achieves state-of-the-art generation quality among field/voxel-based methods for 3D molecules, while `BLiG` and `gPMT` emphasize the elegance and generality of the bijection between sets and fields across multiple domains.\n\nOur read of the main points in each review is:\n\n- `YEsV`: Values the unification of continuous fields and discrete encodings, clear presentation, and multi-domain experiments. Main concern: molecular generation and regression lag behind strong baselines, so the abstract’s “competitive performance” sounds too strong. Also asks how baselines were obtained and whether results fully support the claims.\n\n- `BLiG`: Highlights CORDS as an elegant invertible field representation that jointly encodes cardinality, positions, and attributes, with promising results on four domains. Main issues: reliance on dense sampling, lack of runtime/memory/scaling analysis, and no clear advantage over existing molecular models in current experiments.\n\n- `6aGq`: Very positive about the motivation for modeling cardinality, the balance of intuition and formalism, and the wide range of applications, and notes that CORDS achieves state-of-the-art generation quality among field/voxel-based methods on 3D molecules. Main concern: limitations and overheads (sampling, integration, position refinement, feature solves) are not discussed or quantified in enough detail.\n\n- `gPMT`: Stresses that CORDS gives a clean, invertible construction for cardinality prediction with broad potential. Concerns: encoding/decoding cost may become large for bigger samples, and molecular performance can lag behind baselines; asks about task-specific variants and sensitivity to kernel choices.\n\n- `7wBB`: Criticizes the abstract/introduction for using non-standard terms (“variable cardinality”, “bijective representation”, “feature field”, etc.) before defining them, and asks what precise ML problem is being solved. Also raises an ethics concern about AI reliance; we note that the paper already includes an explicit LLM usage statement clarifying that LLMs were used only for polishing text and helping reconstruct baselines, not for ideas, theory, or results.\n\nWe respond to individual points below. Here we briefly highlight two recurring themes:\n\n- **Empirical performance and “competitive” wording (`YEsV`, `BLiG`, `gPMT`)**  \n  These reviewers note that our molecular results do not match the strongest specialized equivariant GNNs and that “competitive performance” can be read too strongly. We agree that CORDS does not claim SOTA on these benchmarks. Our goal is to show that a single, unified field-based representation stays in the general performance range of strong graph methods, improves over prior continuous/voxel baselines, and naturally models both object features and cardinality. We will revise the abstract and main text to make this representation-first, cross-domain focus explicit and soften the wording around “competitive”.\n\n- **Scalability and computational cost (`BLiG`, `6aGq`, `gPMT`)**  \n  These reviewers highlight that CORDS uses more samples than graphs and adds decoding overhead. We agree that scalability is an important limitation that we should quantify more clearly. Conceptually, encoding runs inside the gradient loop, is highly GPU-parallelized, and in our current setups adds only a small fraction of per-step cost relative to the Erwin+EDM backbone, while decoding is used only at inference and is where most extra cost arises (mainly from position refinement and feature reconstruction). Graph-based methods themselves often have quadratic cost in the number of nodes, and in CORDS some of the extra sampling cost can be offset by pairing the representation with sparse-attention transformers like Erwin, which scale close to linearly in the number of points. We will add a concise complexity and runtime summary with wall-clock timings and simple scaling trends, and clarify where CORDS is currently practical and where further algorithmic work is needed.\n\nWe hope this overall comment helps contextualize our more detailed responses, and we again thank the reviewers and the area chair for their careful reading and constructive suggestions."}}, "id": "nLHV1xF64e", "forum": "RObkOKADBU", "replyto": "RObkOKADBU", "signatures": ["ICLR.cc/2026/Conference/Submission25519/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25519/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission25519/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763555702932, "cdate": 1763555702932, "tmdate": 1763565235064, "mdate": 1763565235064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}