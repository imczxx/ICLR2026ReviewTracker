{"id": "GSLoB1myS8", "number": 9275, "cdate": 1758117108516, "mdate": 1759897733895, "content": {"title": "EAT: Expert Account Tracker for Efficient MoE Inference", "abstract": "Mixture-of-Experts (MoE) models have emerged as a revolutionary method to scale Transformer models. However, traditional MoE architecture still suffers from inefficiency since a large number of experts are unnecessarily activated. Existing approaches for reducing the number of activated experts often overlook the historical performance of each expert. In this paper, we propose EAT, a novel method called $\\textbf{Expert Account Tracker (EAT)}$, which utilizes history-awareness metrics and adaptive thresholding to dynamically select the most important experts, thereby reducing the activated expert number while effectively maintaining the model performance. Experiments show that EAT outperforms the existing baseline Top-P method across multiple models and datasets, achieving over 25% an average reduction compared to the vanilla method in the number of activated experts and performing better token generation speed compared to the baseline. Additionally, through ablation studies, we find that excessively reducing the number of activated experts can significantly harm model performance, and the importance of experts varies across layers, with higher-level experts being generally more critical.", "tldr": "", "keywords": ["EAT; Efficiency MoE"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54fd275240bd431a8fc3af6433fcaf402841658f.pdf", "supplementary_material": "/attachment/b2dbca17be375cfb8b20548781e2473562d03470.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes EAT (Expert Account Tracker), a novel dynamic expert activation method for Mixture-of-Experts (MoE) models. Unlike traditional routing strategies that rely solely on current gating probabilities, EAT incorporates history-aware metrics to evaluate experts’ long-term reliability. It computes a comprehensive importance score combining each expert’s historical activation frequency, cumulative weight, and contribution magnitude, and integrates this with an adaptive thresholding mechanism to dynamically select experts during inference. The method aims to reduce redundant activations while maintaining model quality. Extensive experiments on Mixtral-8x7B and Phi-3.5-MoE-instruct across diverse benchmarks demonstrate that EAT achieves around 25% fewer activated experts and faster token generation speed compared to the baseline."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a history-aware expert selection mechanism that tracks experts’ past performance using a composite score (activation frequency, cumulative gate weight, and contribution score). I think this consideration is quite comprehensive.\n\n\nThis effectively addresses a limitation in prior dynamic routing approaches (e.g., Top-P) that rely solely on current routing probabilities"}, "weaknesses": {"value": "The method combines historical information and performs adaptive thresholding. Why is this necessary? Is it solely for improving performance? The paper lacks detailed analysis and motivation for these design choices. \n\nComparisons are limited to Top-P; more modern baselines (e.g., Ada-K, Expert Pruning and Skipping, CMoE) are mentioned but not empirically compared.\n\nThe paper’s exposition is occasionally dense, especially in the method section; a clearer pseudocode or algorithm box would help reproducibility."}, "questions": {"value": "I find the proposed method rather complicated. Could this complexity affect the inference efficiency in practice?\n\nHow does the paper achieve “better token generation speed”? Theoretically, which step or operation ensures this improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2NeAvGpHnL", "forum": "GSLoB1myS8", "replyto": "GSLoB1myS8", "signatures": ["ICLR.cc/2026/Conference/Submission9275/Reviewer_Y9Xf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9275/Reviewer_Y9Xf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562516498, "cdate": 1761562516498, "tmdate": 1762920921821, "mdate": 1762920921821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Expert Account Tracker (EAT), a history-aware expert routing strategy for Mixture-of-Experts (MoE) models.\nEAT maintains a long-term importance score for each expert, computed as a weighted combination of historical activation statistics and the current router-assigned score.\nThis history-based routing stabilizes expert utilization and accelerates inference by reducing redundant expert activations.\nExperiments on Mixtral-8×7B and Phi-3.5-MoE reportedly show slightly improved accuracy and higher token throughput compared to Top-P routing."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed method is simple and compatible with existing architectures.\n2. The concept of combining short-term gating with long-term statistics is intuitive and interpretable."}, "weaknesses": {"value": "1. Insufficient validation of inference acceleration:  Despite claiming acceleration, only tokens/sec are measured. Latency (TTFT, TPOT), throughput, and memory consumption are not reported.\n2. Unclear experimental setup: Details about the inference environment, implementation, KV-cache, and batching are missing.\n3. Lack of references: Key claims in the Introduction, such as performance degradation from high sparsity, are unsupported by citations.\n4. Limited experimental scope: Only outdated MoE models are used; no evaluation on recent architectures or reasoning/coding benchmarks.\n5. Results against vanilla baseline: EAT often loses to vanilla routing in generation speed and accuracy."}, "questions": {"value": "1. What exact inference framework, batch size, and KV-cache configuration were used for measuring tokens/sec?\n2. Why were only Mixtral and Phi-3.5 chosen? How does EAT perform on more recent MoE models (e.g., Qwen3, DeepSeek)?\n3. Why are well-known math and code reasoning benchmarks such as MATH-500, GSM8K, HumanEval+, and LiveCodeBench missing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h8Dws1ySKH", "forum": "GSLoB1myS8", "replyto": "GSLoB1myS8", "signatures": ["ICLR.cc/2026/Conference/Submission9275/Reviewer_Xtf2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9275/Reviewer_Xtf2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743908621, "cdate": 1761743908621, "tmdate": 1762920921462, "mdate": 1762920921462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EAT (Expert Account Tracker) to utilizes history-awareness metrics and adaptive thresholding to dynamically select the most important experts, aiming to reduce the activated expert number."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Easy to follow. The figures and tables are clear.\n\n- Useful Ablation Study: The analysis in Section 4.3 provides a key insight that experts are not equally important across layers."}, "weaknesses": {"value": "- Heavy Hyper-parameter Issue. This method introduces a large number of new hyper-parameters.\n\n- Lack of performance on larger MoE LLMs, such as Qwen3-30B-A3B.\n\n- Poor performance. As shown in Table 1, proposed EAT underperform Vanilla strategy. Such as 69.00 vs. 75.17 for Phi model on HellaSwag."}, "questions": {"value": "- ZERO reference in the Introduction section. More references would help reader to understand better.\n\n- The caption of Table should above the tabular, following the official guidelines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "98PpOYqqyP", "forum": "GSLoB1myS8", "replyto": "GSLoB1myS8", "signatures": ["ICLR.cc/2026/Conference/Submission9275/Reviewer_THST"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9275/Reviewer_THST"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930053750, "cdate": 1761930053750, "tmdate": 1762920921112, "mdate": 1762920921112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem that the history of expert selection is not considered in the expert routing of MoE models. The authors propose an additional importance metric based on a linear combination of three indicators (activation count, total probability, contribution score). By mixing this value with the expert probability calculated from the current token, they aim to achieve routing based on both the current situation and historical data. Furthermore, to adaptively change the number of activated experts, they introduce an adjustment mechanism based on the shape of the expert probability distribution and the recent perplexity.\n\nAccording to experiments where the routing strategy of existing MoE models was replaced with the proposed method, it was found that while the performance does not match the original (vanilla) performance, it reduces the number of activated experts while maintaining better performance than the top-P method, which is a similar adaptive expert selection technique. The analysis investigates the relationship between the number of activated experts and performance, revealing that excessively reducing the number of experts significantly degrades performance in terms of perplexity, and that layers closer to the input are more amenable to expert reduction than layers closer to the output."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method contains no components that require training and can be easily applied to any MoE model with a general architecture.\n\nIt is an almost rule-based adaptation method, and its computational cost is smaller than the main computation of the MoE parts, making it practically negligible."}, "weaknesses": {"value": "For most tasks, there is a non-negligible performance gap compared to the vanilla MoE. Although the overall computational cost of the model is reduced, a trade-off between cost and performance must be considered.\n\nThe parameters (indicators) added by the proposed method need to be cached within the inference engine, similar to a KV-cache, which may require method-specific implementation support."}, "questions": {"value": "Why did you choose these three indicators: activation count, total probability, and contribution score? Were any other indicators considered? Is there any important information that these indicators fail to capture?\n\nAround L.269: There appear to be typos in the $\\tau_{\\mathrm{PPL}}$ definitions—$\\lambda_+$ and $\\lambda_-$ are possibly inverted.\n\nAre the indicators reset for each test example? If not, dependencies between test examples could arise, making it impossible to interpret the results as independent. Conversely, what would happen if a \"burn-in\" process for the indicators was inserted before processing the test example?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B8l1hOHLTc", "forum": "GSLoB1myS8", "replyto": "GSLoB1myS8", "signatures": ["ICLR.cc/2026/Conference/Submission9275/Reviewer_iAzD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9275/Reviewer_iAzD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024285032, "cdate": 1762024285032, "tmdate": 1762920920688, "mdate": 1762920920688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}