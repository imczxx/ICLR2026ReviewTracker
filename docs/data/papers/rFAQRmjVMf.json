{"id": "rFAQRmjVMf", "number": 22969, "cdate": 1758337659508, "mdate": 1759896838154, "content": {"title": "Transfer Entropy as a Measure of Information Flow in LLMs and VLMs", "abstract": "Understanding how information propagates within large language models (LLMs) and vision-language models (VLMs) is central to both interpretability and efficient deployment. While attention weights reveal where probability mass is allocated, they do not capture causal influence or redundancy between layers, tokens, and heads. In this work, we introduce transfer entropy (TE) as a principled, information-theoretic measure of directional information flow in Transformer architectures. We develop tractable estimators of TE tailored to high-dimensional hidden states and apply them to unimodal (RoBERTa, T5) and multimodal (CLIP ViT-B/16) models. Our analysis shows that layerwise TE profiles expose depth-dependent redundancy, enabling targeted pruning with minimal accuracy loss. At the token level, TE distinguishes sink, broadcaster, and benign roles, revealing distinct patterns in text and vision encoders. At the head level, combining TE with CLS/EOT attention mass yields role-aware head classification, offering interpretable maps of responsibility across depth. Experiments on GLUE and MSCOCO-2014 demonstrate that TE-guided pruning achieves competitive retrieval and classification performance while improving efficiency. Overall, TE provides a unified lens for diagnosing, interpreting, and compressing large Transformer-based models, bridging theoretical information flow with practical model optimization.", "tldr": "Information Flow in LLMs and VLMs", "keywords": ["Large Language Models", "Vision Language Models", "Transfer Entropy", "Information Flow", "Layer Pruning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/edc537fabefc3942e739711edc18b5ec979be59f.pdf", "supplementary_material": "/attachment/603454a877404f05cfa66b495aeb9ada57877bd0.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Transfer Entropy as a tool to understand and prune LLMs and VLMs. Contributions include justifying the use of Transfer Entropy, providing an approximation for Transfer Entropy and then empirical analysis of TE on models such as RoBERTa and CLIP. In doing so, they show that TE corroborates certain intuitions/hypotheses about the mechanisms in the transformer. Lastly, the paper also lays out one practical use of Transfer Entropy - to use as a heuristic for pruning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's usage of Transfer Entropy to analyze LLMs (and VLMs) is original to the best of my knowledge. Further their approximation of TE, laid out by Theorem 1 is also novel to the best of my knowledge. Their methodology and math is discussed clearly and in sufficient detail - it is largely sound. Experimental results are also promising: Not only are findings on RoBERTa are interesting - an inverted U shape, the application to pruning shows good results too."}, "weaknesses": {"value": "My main critique is that significance of the approach is not very strong, and is sometimes contradictory:\n1. Claims around role-aware head classification are not backed by experimental results in the main body of the paper? I see a few plots in the Appendix, it would help to discuss them in more detail in Section 5. \n2. The experiments on RoBERTa are quite limited - using only GLUE benchmark. It is unclear to me if the same pattern holds on other benchmarks? I worry that it will not because of the results on CLIP model where the text encoder doesn't show an inverted U but the vision encoder shows something similar to what is seen on RoBERTa and GLUE. Further, in the appendix, it seems T5 doesn't follow the same trend either. I encourage the authors to focus on few claims and use the main body of the paper to substantiate their central claims.\n3. I worry that pruning results on the CLIP and RoBERTa model aren't very valuable since the models are quite small already. We often struggle to prune larger models, such as Llama, Qwen, Gemma, - finetuning these on an A100 is still possible for their smaller variants (< 7B parameters)."}, "questions": {"value": "N/A - mentioned in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uwWMF0lGyH", "forum": "rFAQRmjVMf", "replyto": "rFAQRmjVMf", "signatures": ["ICLR.cc/2026/Conference/Submission22969/Reviewer_kCWr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22969/Reviewer_kCWr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761086030810, "cdate": 1761086030810, "tmdate": 1762942458071, "mdate": 1762942458071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes applying \"transfer entropy\" (TE) to study the flow of information across layers over time in language and vision-language models.  They derive an approximation to the TE (as computing it directly is infeasible) and use it to make some observations/suggest the following applications: (1) layers that are less \"informative\", as suggested by TE, can be removed, leading to a smaller model with similar performance; (2) vision and text models display differing characteristic trends in the information flow"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed TE + approximation method (conditioned on the mathematical correctness, see Weaknesses) is interesting and promises to be useful in understanding the inner workings of such models in the future.\n2. The experiments use TE to provide interesting observations, such as vision models displaying hierarchical organization of information in layers.\n3. As an application of TE, the paper also proposes a method for model pruning, which is a relevant practically important problem."}, "weaknesses": {"value": "1. The presentation of mathematical details in Sections 3 and 4 is rather poor (see Questions).\n\n2. This might be an artifact of the poor presentation, but I cannot verify that the statements claimed in Section 4 are correct (again, see Questions).  Thus, though the experiments suggest that the method \"works\", I cannot be sure that it is working for the intended reasons.  As such, I cannot recommend acceptance, but I will be happy to improve my score if my questions are addressed satisfactorily."}, "questions": {"value": "1. How does TE compare with Granger causality (GC)?  Though GC is mentioned in related works, it is not clear what exactly differentiates GC and TE (their features are simply listed without direct comparison, either quantitative or qualitative).  Comments on what TE gives that GC does not?\n\n2. What exactly is the perturbation in Section 4.2? Are you assuming that (X^{t+1}_{l}, X^{t+1}_{l+1}) approx (X^{t}_{l}, X^{t}_{l+1}) + delta x?  (i.e., in your model, is this \"perturbation\" affecting the value of X_{l} and X_{l+1} at the next instant?  If yes, then what is the need for the Jacobian matrices in (13) and (14)?  If not, i.e., if you mean delta x to be the perturbation at the same instant, then shouldn't y = X^{t}_{l+1} also change (i.e. not be deterministic, as is claimed)? \n\n4. Other examples of poor presentation/notation (most of this is minor but adds to the general difficulty in reading the paper): \n      1. Section 4.1:  There is no need to introduce u,y,v to denote different X's at t/t+1 and l/l+1, eqns (3), (4), (5) are also completely unnecessary --- TE_l(t) as given in (6) would have been completely fine, along with a possible comment that it is TE as in (2) evaluated between two processes: the activations at layer l shifted backwards in time by 1, and the activations at layer l+1;  Same section, h_l and h_{l+1} are introduced without description)\n     2. Section 4.3:  \\mathcal{T}, S^{prev} and S^{null} are new, no description."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0g6Hp0Hhgn", "forum": "rFAQRmjVMf", "replyto": "rFAQRmjVMf", "signatures": ["ICLR.cc/2026/Conference/Submission22969/Reviewer_4ntR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22969/Reviewer_4ntR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761593630834, "cdate": 1761593630834, "tmdate": 1762942457351, "mdate": 1762942457351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces transfer entropy (TE) as a principled, information-theoretic measure to quantify directional information flow in Transformer-based models. The authors propose tractable estimators of TE for high-dimensional hidden states and apply them to both unimodal (RoBERTa, T5) and multimodal (CLIP ViT-B/16) architectures. The work positions TE as a complementary diagnostic to attention weights, providing insights into which components actually drive representational changes during training and fine-tuning.\nAt the layer level, the authors demonstrate that TE profiles reveal depth-dependent redundancy that can guide structured pruning with minimal performance degradation. They claim further analyses at the token and head levels, identifying “sink,” “broadcaster,” and “benign” roles and combining TE with CLS/EOT attention mass for role-aware head classification, but these are only presented in the appendix.\nOverall, the paper aims to connect theoretical information flow measures with practical interpretability and compression of LLMs and VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel formulation: Introducing transfer entropy as a directed information flow metric for Transformers is conceptually interesting and theoretically grounded. It adds a principled alternative to similarity- or attention-based interpretability measures.\n\n2. Clear writing and presentation: Despite the mathematical content, the paper is readable and well structured. The main ideas are intuitive once introduced.\n\n3. Strong empirical demonstration (for the layer-level part): The TE-based layer pruning experiments are thorough and convincing. The results show clear improvements over heuristic baselines in both interpretability and efficiency.\n\n4. Broader vision: The discussion and conclusion sections thoughtfully position TE as a unifying tool for interpretability, model compression, and even safety-related analyses.\n\n5. Methodological soundness: The proposed estimators are carefully derived, with complexity analysis and empirical validation (albeit mostly in the appendix)."}, "weaknesses": {"value": "1. Incomplete coverage of claimed contributions:\nThe main text only discusses the layer-level experiments and pruning. The token-level and head-level analyses, explicitly listed as key contributions, are entirely relegated to the appendix and never referenced in the main paper. As a result, the main body does not substantiate all claims made in the abstract and conclusion.\n\n2. Weak referencing to the appendix:\nThe main text does not guide the reader to where the omitted results can be found, making it non–self-contained. For a conference paper, the core contributions and evidence should be visible in the main submission, not buried without pointers.\n\n3. Unexplained model choices:\nThe rationale for selecting specific models (RoBERTa, T5, CLIP) is only briefly justified by “unimodal vs. multimodal” variety, without deeper discussion on why these are representative or what their differences reveal.\n\n4. Limited discussion of limitations or theoretical caveats:\nWhile TE is powerful, its estimation under high-dimensional, correlated activations is nontrivial. A short discussion of possible biases or instability of the approximations would strengthen credibility.\n\n5. Potentially overclaimed generality:\nThe conclusion presents TE as a “unified lens” for interpretability and compression, but given that large parts of the evidence are missing from the main text, this feels overstated."}, "questions": {"value": "1. Why were the token-level and head-level analyses (which are listed as major contributions) excluded from the main text? Could you briefly summarize their main findings or justify why they were omitted?\n2. What are the main differences between the two TE approximations you propose in terms of bias, computational efficiency, and stability?\n3. How generalizable are your pruning results beyond the chosen models and datasets (e.g., to decoder-only LLMs like GPT-style architectures)?\n4. Could TE estimation feasibly be integrated into online training as suggested in future work, or is it still computationally too heavy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kwbSqY9weD", "forum": "rFAQRmjVMf", "replyto": "rFAQRmjVMf", "signatures": ["ICLR.cc/2026/Conference/Submission22969/Reviewer_GMWd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22969/Reviewer_GMWd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679638265, "cdate": 1761679638265, "tmdate": 1762942456854, "mdate": 1762942456854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Transfer Entropy (TE) as a principled information-theoretic measure to quantify directional information flow and causal influence between layers, tokens, and attention heads in Transformer architectures. By developing tractable estimators (e.g., Jacobian approximations) for high-dimensional hidden states, the authors analyze information propagation patterns in LLMs and VLMs, proposing a novel pruning strategy based on low TE signals to enhance efficiency while preserving performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## Strengths\n\n1. **S1: Introduction of a Principled Causal Measure (Transfer Entropy).**\n    The paper addresses a fundamental limitation of existing interpretability methods (like attention weights) by introducing **Transfer Entropy (TE)** to measure directional information flow and causal dependency within Transformers. Attention weights only reflect probability mass allocation, failing to capture true causal influence or information redundancy. The TE framework provides a **more principled and theoretically grounded** tool for understanding how information is processed and transferred across the Transformer hierarchy.\n\n2.  **S2: Application of Information Flow Analysis to Practical Efficiency Improvement.**\n    This work extends information flow analysis beyond purely explanatory research into practical model efficiency improvement. The authors design a **novel pruning strategy** based on identifying layers with low redundant information flow signaled by TE. The experiments demonstrate that this TE-based strategy achieves **comparable or superior performance** to existing pruning methods across various models (e.g., RoBERTa and T5) and tasks, successfully translating information-theoretic insights into an actionable model optimization technique."}, "weaknesses": {"value": "## Weaknesses\n\n1.  **W1: Quantitative Reliability Compromised by Linear Approximation.**\n    The core measure, Transfer Entropy (TE), is deemed **intractable** due to the high-dimensional, non-linear nature of Transformer hidden states. The authors resort to a \\textbf{Jacobian Linear-Gaussian Approximation} or a \\textbf{Cosine-Similarity Approximation}.\n    * Given the highly non-linear nature of deep models, this strong linearity assumption fundamentally \\textbf{violates the theoretical rigor} of the information-theoretic measure.\n    * This approximation casts serious doubt on the reliability and quantitative accuracy of all subsequent findings derived from TE, such as the layer redundancy analysis and the proposed pruning strategy.\n\n2.  **W2: Discrepancy in Interpreting Layerwise Information Flow (Training vs. Inference).**\n    The layerwise TE, $TE_l(t)$, is defined to track information flow across **two consecutive training steps** $t$ and $t+1$. This captures dependencies in **training dynamics** (i.e., how back-propagated updates propagate).\n    * This definition is conceptually distinct from the functional information flow during a single **inference** forward pass.\n    * The paper fails to clearly justify why redundancy identified in the *propagation of training updates* robustly translates to a reliable signal for pruning layers during *inference*, potentially leading to a disconnect between the measured quantity and the claimed functional importance.\n\n3.  **W3: Narrow Scope of VLM Experiments and Limited Generalizability.**\n    The VLM analysis is only demonstrated on **CLIP ViT-B/16**, a relatively older and non-generative VLM architecture.\n    * The conclusions regarding information flow in the vision and text encoders are thus of \\textbf{severely limited generalizability}.\n    * The framework's utility must be proven on more modern, mainstream **large generative VLMs** (like LLaVA or BLIP-2) which feature complex cross-attention mechanisms, which are the primary site of multimodal information exchange."}, "questions": {"value": "## Questions\n\n1.  **Q1: Justification for Linear/Cosine Approximation on Non-linear Activation.**\n    The cornerstone of this work is the use of Transfer Entropy (TE) via either a Jacobian Linear-Gaussian Approximation or a Cosine-Similarity Approximation. Transformers are inherently highly non-linear. The authors must rigorously justify why a \\textbf{linear assumption} is sufficient and theoretically sound for high-dimensional, non-linear hidden states. Is there an empirical validation showing that the approximation error for TE (relative to a computationally infeasible exact calculation) remains within an acceptable bound, especially at deeper layers where non-linearity accumulates?\n\n2.  **Q2: Disconnect between TE (Training Dynamics) and Pruning (Inference Functionality).**\n    The paper defines layerwise TE ($TE_l(t)$) based on two consecutive training steps ($t$ and $t+1$), capturing dependencies in the \\textbf{gradient propagation} (training dynamics). Pruning, however, requires identifying layers that are functionally redundant during a single \\textbf{inference forward pass}.\n    * Please provide a clear theoretical argument or empirical evidence to bridge this gap: Why does measuring the redundancy of *update signals* reliably map to the functional redundancy of *hidden layer activations*?\n    * Have the authors explored defining TE between layer $l$ and $l+1$ *during a single forward pass* (i.e., conditional on the input $x$) to more directly measure functional information flow?\n\n3.  **Q3: Generalizability and Applicability to Generative VLMs.**\n    The VLM analysis is limited to CLIP ViT-B/16, which lacks the complex generative decoder and cross-attention mechanisms central to modern VLMs (e.g., LLaVA, BLIP-2).\n    * Is the proposed TE framework computationally tractable for **large generative VLMs** (e.g., Llama/Mistral-based architectures with 7B+ parameters)? Runtime analysis suggests potential bottlenecks.\n    * How does the framework handle the TE flow across the \\textbf{cross-attention block} (Vision $\\rightarrow$ Language)? Analyzing this specific junction is critical for understanding multimodal fusion, yet it is absent from the current VLM case study.\n\n4.  **Q4: Causality and Redundancy vs. True Functionality.**\n    The pruning strategy relies on identifying layers with low TE (low causal flow) or high redundancy. Low TE implies a layer's output is highly independent of the previous layer's update, or that its information is already largely accounted for.\n    * Is it possible that a layer with \\textbf{low TE} (i.e., low informational contribution from the previous layer's update) is actually performing a \\textbf{critical non-linear transformation} on the existing information, making it essential for final output?\n    * How is the risk of \\textbf{pruning a functionally critical layer} based solely on low TE signals mitigated in the proposed strategy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OFYX2ayliG", "forum": "rFAQRmjVMf", "replyto": "rFAQRmjVMf", "signatures": ["ICLR.cc/2026/Conference/Submission22969/Reviewer_4vdZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22969/Reviewer_4vdZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912864577, "cdate": 1761912864577, "tmdate": 1762942456503, "mdate": 1762942456503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}