{"id": "5H8kxW0Efk", "number": 14252, "cdate": 1758231215394, "mdate": 1763675365415, "content": {"title": "Neural Network Ising Machines: Algorithm Unrolling for Combinatorial Optimization", "abstract": "We propose a new data-driven neural approach to combinatorial optimization in which we learn the parameters of an iterative dynamical system which efficiently solves typical instances of the NP-hard Max-Cut/Ising problem. The dynamical system is parameterized by a small neural network which is trained using a zeroth-order optimization method. We find that our method is able to learn efficient and scalable algorithms for solving these combinatorial optimization problems. We show that even with a limited parameter count, the neural network is able to learn sophisticated dynamics which allow it to efficiently navigate the non-convex landscapes that are characteristic of NP-hard problems. We compare our method against state-of-the-art neural-CO approaches as well as other classical Max-Cut/Ising solvers and show that is can achieve competitive performance.", "tldr": "We propose a new data-driven neural approach to combinatorial optimization in which we learn the parameters of an iterative dynamical system which efficiently solves typical instances of the NP-hard Max-Cut/Ising problem.", "keywords": ["Ising Machines", "Combinatorial Optimization", "Algorithm Unrolling", "Zeroth Order Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92448ec3e56251c1c109b80ae3a689d5b44422d4.pdf", "supplementary_material": "/attachment/2c210e7db9d85b6b99fb140fb9928bf7740e8fc3.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel neural network-based method inspired by Ising machines for solving combinatorial optimization problems. The approach is trained using a zero-order optimization method proposed by Reifenstein et al. (2024). The authors benchmark their method against recent state-of-the-art (SOTA) techniques from both the unsupervised combinatorial optimization and Ising machine communities, demonstrating strong performance across a wide range of problems. Additionally, the paper includes ablation studies examining problem difficulty, parameter count, and problem size."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novelty:** The proposed method appears to be original and distinct from existing approaches.\n- **Performance:** The method achieves competitive results on well-established benchmarks.\n- **Rigor:** The inclusion of ablation studies provides valuable insights into the method’s robustness and sensitivity to key variables."}, "weaknesses": {"value": "- **Lack of Clarity on Optimization:** The paper employs a zero-order optimizer from Reifenstein et al. (2024), but the algorithm itself is not explained. A brief introduction or intuition about the optimizer would significantly improve accessibility for readers unfamiliar with the reference.\n- **Reward Function Assumptions:** The reward functions in Appendix F relies on knowledge of the optimal energy value. This assumption raises concerns about fairness in comparisons with methods that do not use such information."}, "questions": {"value": "1. **Gradient Derivation (Q1):**\n   Could the authors provide an intuitive explanation of the principles underlying the gradient derivations in Appendix G? This would enhance the paper’s clarity and make it more self-contained.\n\n2. **Reward Function Justification (Q2):**\n   How is the use of the optimal energy value in the reward justified? Since other methods do not rely on this information, it is unclear whether the comparison is equitable. A discussion on this point—and an exploration of the method’s performance using only the raw energy as a reward—would be highly informative.\n\n3. **Architectural Alternatives (Q3):**\n   The MLP parametrization resembles recurrent architectures like LSTMs. Have the authors explored using LSTMs or similar architectures instead? If so, how does their performance compare to the proposed MLP-based approach?\n\n4. **Clarification on Table 1 (Q4):**\n   What does “top 30” refer to in Table 1? Could the authors please clarify?\n\n5. **Diversity of the solutions (Q5):**\nIf I understand correctly in each trajectory the best solution is taken and for example in Table 1 then the average of these solutions is computed. Is there some diversity in the best solution between trajectories or do all trajectories propose give the same best solution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vGSgZOtbpc", "forum": "5H8kxW0Efk", "replyto": "5H8kxW0Efk", "signatures": ["ICLR.cc/2026/Conference/Submission14252/Reviewer_GHVz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14252/Reviewer_GHVz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760696100080, "cdate": 1760696100080, "tmdate": 1762924704842, "mdate": 1762924704842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a learning-based approach for combinatorial optimization via learning the parameters of an iterative dynamical system (Ising machines).  The major contributions of this work are in the combination of algorithmic unrolling, ising machines, and zeroth-order optimization in the context of CO problems.  Computationally, once trained, their approach achieves strong results on benchmark instances for max independent set, max clique, and max cut CO problems."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novelty**:  This paper proposes a novel approach that combines algorithmic unrolling, ising machines, and zeroth-order optimization in the context of CO problems.  While this is a combination of existing frameworks, I believe this combination is sufficiently novel and quite refreshing from recent approaches that primarily utilize RL/transformer-style rollouts. \n-  **Results**: On the instances evaluated, this approach performs quite well, with the ability to compute high-quality solutions relatively quickly."}, "weaknesses": {"value": "- **Adaptability**: From my understanding, this approach is relatively limited in terms of the classes of optimization problems that it can be used on, e.g., those that can be formulated as Equation (1).  Compared to other exact/heuristic/learning-based methods, this is a relatively strong limitation in terms of applicability.  \n- **G-Set Results**: The authors compute time-to-solution (TTS) on the G-set benchmarks using reference cut values drawn from prior Ising-machine literature rather than from the globally best-known Max-Cut results reported in combinatorial-optimization studies. While this choice maintains consistency with neural Ising comparisons, it also means that the reported TTS values correspond to approximate rather than truly optimal targets. For this reason, it would be more informative to include information on the differences in solution quality and time compared to the best-known approaches.  \n- **Scalability**: The experiments are limited to G-set instances up to $N=1000$, with no results reported for the larger, more challenging graphs. The authors state that the scalability is with respect to model size, rather than CO problem size, so I am not sure why they would limit their evaluation to small instances.  The absence of results on larger instances makes it difficult to assess how well the method scales relative to state-of-the-art Ising and Max-Cut solvers.  This is a further concern given the limited applicability to other classes of problems. Additionally, there is no reporting of training time, which makes the scalability of training unclear."}, "questions": {"value": "**Questions**\n- [1] propose an approach based on learning and Ising machines for CO.  Can the authors detail the differences in these works and include this in the paper?  \n- How long do these methods take to train, especially compared to other methods, e.g., DiffUCO?  These should all be included in the appendix.  \n- How does the performance of a model generalize out-of-distribution?\n- Can the authors provide more information on the Gurobi results, i.e., optimality gaps and the time Gurobi takes to find equivalent quality solutions (when Gurobi finds better solutions)?  Furthermore, was Gurobi run with MIPFocus=1 (to prioritize primal solutions)?  If not, this should be done, given the heuristic focus of this work.  \n\n**Remarks**:\n- In the abstract and throughout the paper, the authors constantly state that they are \"solving\" instances.  This needs to be changed since their method is a heuristic, and solving should be reserved for exact methods.  \n- Figure 7 \"training training\" should be \"training\". \n\n**References**:\n- [1] Bo Lu, Yong-Pan Gao, Kai Wen, and Chuan Wang. Combinatorial optimization solving by coherent\nising machines based on spiking neural networks. Quantum, 7:1151, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wh1OKMUW9r", "forum": "5H8kxW0Efk", "replyto": "5H8kxW0Efk", "signatures": ["ICLR.cc/2026/Conference/Submission14252/Reviewer_6UyN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14252/Reviewer_6UyN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866615420, "cdate": 1761866615420, "tmdate": 1762924704295, "mdate": 1762924704295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies Ising machines as optimizers for combinatorial problems—specifically Max-Cut, Maximum Independent Set (MIS), and Max-Clique—and proposes a zeroth-order (gradient-free) optimization approach for tuning/steering the machine. The authors compare against prior Ising-based methods and aim to demonstrate improved solution quality and/or efficiency."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The Ising formulation is a natural modeling choice for quadratic objectives, and the paper targets three canonical NP-hard problems with broad interest.\n\nUsing zeroth-order optimization is well-motivated in settings with noisy or non-differentiable hardware, and the paper’s perspective could be useful to practitioners working with analog or black-box solvers.\n\nThe manuscript attempts to position the work within the growing literature on physical/Ising-style optimizers for CO, which is timely."}, "weaknesses": {"value": "Evaluation metrics (TTS vs objective quality). The paper emphasizes “time to solution” (TTS), defined as the time required to reach a solution with 99% success probability. While TTS is common in annealing/Ising communities, it is less standard in the combinatorial optimization literature, which typically leads with objective quality (cut value, clique size, independent set size), approximation ratios or normalized optimality gaps, and then reports wall-clock time. I encourage the authors to complement TTS with conventional CO metrics. This would make results easier to compare with non-Ising baselines."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "svTPR8DirD", "forum": "5H8kxW0Efk", "replyto": "5H8kxW0Efk", "signatures": ["ICLR.cc/2026/Conference/Submission14252/Reviewer_SQVt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14252/Reviewer_SQVt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950655465, "cdate": 1761950655465, "tmdate": 1762924703885, "mdate": 1762924703885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}