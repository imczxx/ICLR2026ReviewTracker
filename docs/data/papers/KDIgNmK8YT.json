{"id": "KDIgNmK8YT", "number": 3778, "cdate": 1757518824276, "mdate": 1759898070131, "content": {"title": "WorldAlignment: Benchmarking Expert-Level Human Preference Alignment across Domains and Aspects", "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet aligning their behavior with human preferences remains both challenging and essential. Human evaluation of model outputs is often costly and must account for diverse user preferences. To address this, recent methods leverage LLM-as-a-judge to assess alignment quality, which achieves higher agreement with human judgments while being more cost-effective. However, existing widely used benchmarks such as AlpacaEval 2.0 primarily rely on simplistic, instruction-following data pairs derived from general user preferences. These benchmarks are insufficient for evaluating complex, domain-specific, and nuanced scenarios, and many are outdated for current alignment evaluation. \nTo overcome these limitations, we introduce WorldAlignment, an expert-level, multi-domain human preference benchmark designed for efficient and comprehensive evaluation of alignment capabilities. WorldAlignment provides more challenging, higher-quality, and diverse preference pairs across multiple domains, enabling more robust alignment assessment. Our evaluation results show that WorldAlignment offers comprehensive insights into both SoTa and post-trained models, establishing a modern benchmark for domain-oriented alignment. Furthermore, WorldAlignment supports evaluation across various dimensions, including instruction-following, mathematical reasoning, and code-related tasks, providing a holistic view of alignment performance. \nThrough our evaluation, we find that several state-of-the-art alignment-tuned models still exhibit substantial performance gaps compared to GPT-4-level models on our benchmark, highlighting critical limitations and directions for future improvement. Our code and data will be available at https://anonymous.4open.science/r/WorldAlignment.", "tldr": "", "keywords": ["Expert-Level LLM Alignment", "RLHF", "Preference Optimization"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3859efbca7e1d1bbdd253eb6263f04e332d79af1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces WorldAlignment, a new human preference alignment benchmark for evaluating large language models (LLMs). It aims to address the shortcomings of existing benchmarks like AlpacaEval 2.0 and WildBench, which focus on simple, instruction-following tasks and general user preferences. These benchmarks fail to assess complex, domain-specific, and nuanced scenarios adequately. In response, WorldAlignment provides a more robust, comprehensive evaluation by leveraging expert-level preference judgments across multiple domains, including instruction-following, mathematical reasoning, and code generation. The paper highlights the substantial performance gaps observed when evaluating state-of-the-art models, emphasizing the need for a benchmark tailored to real-world, expertise-driven tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a clear introduction to the limitations of existing benchmarks and effectively introduces WorldAlignment as a solution to these problems. The motivations for the proposed benchmark are well-articulated."}, "weaknesses": {"value": "1. **Benchmark Comparison**: **Regarding coverage**, you mention that existing benchmarks primarily assess general instructions, which leads to the introduction of Instruction Following, Math, and Code in your work. However, Livebench also includes code and math subsets—how does your work innovate beyond that? Furthermore, why does the inclusion of code and math tasks justify calling your benchmark WorldAlignment? WildBench, which is constructed from real-world dialogues, also provides nuanced evaluation; however, you do not offer a Spearman correlation with ChatBot Arena for comparison. **Regarding difficulty**, you claim to introduce more challenging tasks, but from the prompt templates in Appendix C, it’s unclear why these prompts lead to more difficult questions. Your evaluation of \"difficulty\" using GPT-4o as a Judge is also somewhat limited—aren’t benchmarks like AIME, which are constructed manually, already difficult enough? **Regarding response length**, the analysis in Figure 2 suggests that longer instructions and responses correlate with higher diversity, yet it doesn’t clearly explain why longer prompts necessarily indicate greater difficulty. Do difficult tasks require long instructions? This reasoning is not entirely convincing. Furthermore, in terms of **bias control**, Section 3.1 and [2] show significant overlap—mainly just the addition of \"d.\" Equations (2) and (3) also appear largely unchanged from [2], with no clear explanation of how \"d\" is reflected on the right-hand side, raising concerns about the novelty.\n\n2. **Benchmark Construction Details**: There is a lack of detailed information regarding the construction of your benchmark. For example, how were the multiple domains under Instruction Following structured? What does \"persona\" refer to, and how is it defined? How do you ensure diversity and difficulty in the tasks? It seems that a single prompt is insufficient to achieve this. Appendix C doesn’t clarify these aspects, and the main text doesn’t provide further insight into how these benchmarks were constructed or how they ensure the claimed diversity and difficulty.\n\n3. **Model Evaluation Bias**: The use of GPT-4o both as a reference model and as a judge introduces potential self-judging bias, as noted in [1]. To improve the robustness of the evaluation, it would be valuable to address this bias through alternative models or by incorporating more explicit control measures.\n\n4. **Mathematical and Code Tasks**: The paper claims that post-trained models perform poorly on mathematical reasoning and code generation tasks. However, the open source models tested in Table 1 might not represent the state-of-the-art in these domains. Models like Qwen3 or DeepSeek-R1, which may have been specifically trained on mathematical and coding tasks, would provide a more appropriate baseline. Moreover, instead of relying on LLMs as judges, it would be useful to construct objective, standard answers for these tasks to ensure more consistent and accurate evaluation.\n\n5. **Overclaim in Figure 1**: The paper suggests that traditional benchmarks primarily focus on Instruction Following. However, both Livebench and WildBench also include code and math tasks, which are not adequately acknowledged in your claims. Additionally, WildBench is constructed from real-world dialogues, not just Instruction Following. Therefore, the argument that the conventional scope is limited to Instruction Following is not entirely accurate."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XeFvDHv6X0", "forum": "KDIgNmK8YT", "replyto": "KDIgNmK8YT", "signatures": ["ICLR.cc/2026/Conference/Submission3778/Reviewer_R9CP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3778/Reviewer_R9CP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760672443767, "cdate": 1760672443767, "tmdate": 1762917003092, "mdate": 1762917003092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes WorldAlignment, a novel benchmark for assessing LLM alignment. It critiques existing benchmarks like AlpacaEval 2.0 for their focus on simplistic instruction-following tasks and introduces a multi-domain dataset covering instruction-following, mathematical reasoning, and code generation. The benchmark comprises 2400 high-quality synthetic prompt-response pairs (800 per domain), generated using GPT-4o conditioned on diverse personas to ensure complexity and diversity."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The regression-based length correction and domain-aware modeling may provide a fairer, more robust metric than raw win rates.\n2. Persona-guided synthetic generation yields diverse, challenging prompts (mean difficulty 7.21 vs. AlpacaEval's 3.20), assessed via feasibility and quality scores, supporting cost-effective benchmark updates."}, "weaknesses": {"value": "1. Different from the authors' main claim that WorldAlignment is \"an expert-level, multi-domain human preference benchmark\" contradicts with its reliance on GPT-4o-generated pairs. It risks contamination and may not capture authentic human preferences, as real-world interactions (e.g., from Chatbot Arena or human annotators) could introduce.\n2. The domains in WorldAlignment, instruction following has been extensively studied in AlpacaEval and ArenaHard. Coding and math have their corresponding objective benchmark like LiveCodeBench and AIME, MATH. As there are ground truths in code generation and math reasoning, it is not necessary to propose a synthetic benchmark for coding and math reasoning.\n3. Using GPT-4o and GPT-4.1-Mini as judges introduces potential self-bias, with inconsistent ratings across domains, lacking human validation for gold-standard reliability."}, "questions": {"value": "- In FIgure 2, what model is used for response generation? Why is positive correlation between instruction and response length better than weak relationship?\n- The results that GPT-4.1 is better than GPT-5 on code generation is very wired, and conflicts with other coding benchmarks. Can you explain why?\n- Is the proposed multi-domain regression model better than the original one? Do you have any experiment results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XOoCZypI61", "forum": "KDIgNmK8YT", "replyto": "KDIgNmK8YT", "signatures": ["ICLR.cc/2026/Conference/Submission3778/Reviewer_PWx4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3778/Reviewer_PWx4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706978550, "cdate": 1761706978550, "tmdate": 1762916993086, "mdate": 1762916993086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WorldAlignment, a multi-aspect human preference benchmark. It features complex preference pairs and incorporates domain-specific knowledge, providing a more rigorous standard for evaluation. This benchmark enables a more fine-grained assessment of current open-source and closed-source models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work introduces WorldAlignment, a benchmark that offers a more comprehensive evaluation of human preference alignment across multiple aspects and domains.\n\n2. Additionally, this work proposes specialized evaluation metrics to better leverage the assessment capabilities of the WorldAlignment benchmark."}, "weaknesses": {"value": "1. The data cleaning methodology is unspecified. It is unclear whether samples from various vertical domains were manually inspected by dedicated domain experts.\n\n2. Employing GPT-4o to simultaneously generate baseline responses and act as the judge model may introduce unfairness or bias into the evaluation, as models tend to favor their own outputs.\n\n3. The analysis of post-training effects is overly limited in its scope, encompassing a narrow range of model sizes and preference alignment methods. Consequently, the study fails to yield systematic or insightful conclusions. Further exploration is required across a wider variety of models (such as the Qwen series), a more diverse set of model sizes, and a broader spectrum of preference alignment or RL-based approaches. Additionally, the choice of training data itself may have a non-negligible influence on the results, a factor that warrants careful consideration."}, "questions": {"value": "In Sections 3.1 and 3.2, the notation 'y' is used to represent both the preference label and the output from the model. This dual usage could be confusing for readers. I suggest using distinct notations to differentiate between them for improved clarity. For instance, you could use 'z' to represent the response in Section 3.2, which is the same as 3.1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AbgT29Ib5D", "forum": "KDIgNmK8YT", "replyto": "KDIgNmK8YT", "signatures": ["ICLR.cc/2026/Conference/Submission3778/Reviewer_sMQN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3778/Reviewer_sMQN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904875453, "cdate": 1761904875453, "tmdate": 1762916991933, "mdate": 1762916991933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents WorldAlignment, a benchmark just like Alpaca Eval and Arena Hard to evaluate the model's general instruction following ability. The benchmark gathers responses sampled from GPT-4o as the baseline and uses a dual judge-system of GPT-4o and GPT-4.1-mini to evaluate the win rate of a \"candidate model\". The benchmark construction and eval protocol process is very similar to AlpacaEval and ArenaHard but the authors demonstrates that the prompts are more diverse and of higher quality. The author benchmarks many proprietary models and conducts evaluation by category (general instruction following, math, coding)"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies an important problem. The current instruction following (IF) benchmarks are of low quality (e.g. Arena Hard has many weird prompts) and they have been extensively used now. We really really do need newer benchmarks for general chatbot evaluation.\n\n2. The paper made efforts in controlling for the quality and diversity, and conducted analysis to make sure that their benchmark is indeed more difficult and higher quality. \n\n3. The presentation of the paper is clear, with comprehensive results on both proprietary models and post-trained open-sourced models."}, "weaknesses": {"value": "1. The issue (which is also an issue in Alpaca Eval and Arena Hard) is that they mixed Math and Coding prompts with general instruction following tasks, and adopts a fixed way (LM-judge) for evaluation. Math and Coding questions are tasks that the model either is or is not correct. For example, if you are asking a model to solve a math problem, then the evaluation should be whether the answer matches the groundtruth. If the question is about coding, then you need to pass unit tests. Adopting an LM-judge for all of such question doesn't seem correct to me. It would be much much better if the authors can focus on chatbot general instruction following tasks instead of trying to make it \"multi-domain\".\n\n2. The prompts that the author used was sampled from GPT-4o. While it enhances the difficulty of the prompts, it results in a mismatch between how people generally use such models. There are natural prompts datasets (e.g. WildChat) out there and you can filter from it to control for quality. Manually conditioning on personas and sampling prompts from GPT-4o raises the question on how practical  / how aligned this eval procedure is to human judgement. \n\n3. The author mentions that past works \"are often susceptible to spurious correlations, including response length bias, formatting preferences, and positional effects\" in the introduction. But I don't really see how the authors modify their benchmark design choices in resolving these problems. I think such problems are systematic and no matter how you control for the data quality, you will still have positional biases, and stylistic biases in LM-judges. For example, GPT-4o might favor a certain style of response that humans dislike."}, "questions": {"value": "027 SoTa and post-trained models. I believe that the two are not complimentary? Maybe just use \"frontier models\"\n\n191 - 192: As shown in Figure 2a, WorldAlignment instructions exhibit a substantially broader length distribution, extending to longer and more complex prompts.\n\nLonger prompts/generations doesn’t necessarily mean that they are more complex. I wouldn't make this claim here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OghvgYmwMK", "forum": "KDIgNmK8YT", "replyto": "KDIgNmK8YT", "signatures": ["ICLR.cc/2026/Conference/Submission3778/Reviewer_ySE5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3778/Reviewer_ySE5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949046762, "cdate": 1761949046762, "tmdate": 1762916984685, "mdate": 1762916984685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}