{"id": "LnBEASInVr", "number": 22507, "cdate": 1758332034927, "mdate": 1759896862350, "content": {"title": "Empowering LLM Tool Invocation with Tool-call Reward Model", "abstract": "Large Language Models (LLMs) have recently alleviated limitations in outdated internal knowledge and computational inaccuracies by invoking external tools such as search engines and code generation. While reinforcement learning (RL) has substantially enhanced tool usage in LLMs, most existing agentic RL approaches rely solely on outcome-only reward signals, which assign credit at a coarse granularity and often induce gradient conflict (e.g., correct tool calls may be penalized due to incorrect final answers). To address this, we propose the *Tool-call Reward Model* (TRM), a specialized process reward model meticulously designed to evaluate and reward each tool invocation. Since previous PRM research has predominantly focused on traditional reasoning tasks such as step-wise mathematical reasoning, the introduction of TRM brings two unique challenges: (1) limited understanding of how to construct effective TRMs, including data requirements and model size; and (2) difficulties integrating TRM with classical RL algorithms such as PPO and GRPO, where naive adaptation may lead to reward hacking (minimizing tool calls to avoid penalties). To tackle these challenges, we establish a systematic TRM construction workflow and propose refined credit assignment and turn-level advantage estimation for effective integration with PPO and GRPO. Experiments show that a 3B TRM trained on 10K samples achieves robust performance. On search-based QA and Python code-based math tasks, integrating TRM consistently outperforms outcome-only reward RL methods across models of different sizes.", "tldr": "We propose a Tool-call Reward Model that provides fine-grained signals for tool invocation and adapts classical RL algorithms, significantly enhancing LLMs' tool usage compared to outcome-only reward methods.", "keywords": ["Large language model", "Tool invocation", "Tool-call reward model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f62dec76191632bf90467991bb061d3bf0afb7d.pdf", "supplementary_material": "/attachment/71c7087d85735fd0c20e2ea04bd4081aa2f0a80a.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces TRM, a Tool-call Reward Model for LLMs tool learning. TRM is a process reward model that assigns per-tool-call rewards based on necessity and quality, so that it overcomes the limitations of outcome reward signals that provide credit only for the final result. The paper describes methods for constructing effective TRM model and introduces integration strategies with RL algorithms including PPO and GRPO using refined credit assignment and turn-level advantage estimation. Experiments on both search-based QA and code-based math tasks demonstrate robust gains over RL with outcome rewards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem of designing a PRM for tool-integrated reasoning is well motivated and very timely.\n- The reward function designed to evaluate necessity and quality is reasonable, and empirical results showed it worked well.\n- Empirical results showed that the proposed reward functions worked pretty well on search and code tool related datasets."}, "weaknesses": {"value": "- The eval on search tool and code tool are relatively thorough. Although these are important, the approach is advertised as broadly applicable—yet evidence supporting cross-task generalization is mainly anecdotal, and the set of tools remains narrow. A more ambitious empirical scope (e.g., more diverse tool types, additional real-world external APIs) would strengthen the claim of general utility.\n- TRM relies on a strong model (DeepSeek-R1) to annotate and produce the actual reward values for both necessity and quality. This essentially brings extra knowledge and inductive bias into the training process, which might be a bit unfair. The compared methods (including Search-R1) only use the training dataset (the ground truth answer label).\n- The baseline missed some leading program-aided tool-integration works (e.g., PAL, StableToolBench).\n\nMinor issue:\n1. To be more precise, a reward function instead of a reward model is proposed in this paper to guide more effective/efficient tool calls. There is not really a reward model being trained to guide something that is unverifiable in the answer and needs tool call. As a result, I do not think the \"Reward Model\" in the paper title is appropriate."}, "questions": {"value": "- In principle, the reward function designed in the paper encourages the base model to act both effectively (quality) and efficiently (necessity). Most of the evals in the paper are about effectiveness regarding the answer accuracy. Is there any eval regaring the efficiency of tool use by the learned model? For example, the learned model should use less tool calls to get the correct answer compared to baseline methods.\n- How would the proposed TRM construction pipeline adapt to tool types with more complex or less binary notions of utility?\n- Have you analyzed failure cases of TRM or inspected which features/representations contribute most to per-tool-call utility predictions, possibly to increase process-level transparency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DPZNTR0F6r", "forum": "LnBEASInVr", "replyto": "LnBEASInVr", "signatures": ["ICLR.cc/2026/Conference/Submission22507/Reviewer_R2Sf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22507/Reviewer_R2Sf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907647897, "cdate": 1761907647897, "tmdate": 1762942247271, "mdate": 1762942247271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Tool Reward Model (TRM) that scores each tool call with a binary “necessity × quality” signal and combines these turn-level scores with an outcome reward for the final answer. TRM is integrated into PPO/GRPO via turn-level credit assignment and is used for RL training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear.\n2. Extensive experiments validate the effectiveness of TRM."}, "weaknesses": {"value": "1. Necessity/quality labels are auto-judged by the same model family that produced the rollouts, no multi-judge or human verification is reported, which may imprint model bias.\n2. Tool coverage is narrow (search, code), generality to other tool families is unclear.\n3. There is no direct comparison to current PRM/agent-PRM baselines."}, "questions": {"value": "1. How do necessity and quality contribute individually? A comparison among (i) necessity-only, (ii) quality-only, and (iii) the combined  reward (as in the paper) would be helpful.\n\n2. How do you ensure the distillation labels remain reliable for long rollouts, given that feeding entire traces to a judge model may introduce unfaithful judgments or evaluation bias?\n\n3. Beyond search and code, how well does TRM generalize to other tools?\n\n4. How does TRM size affect downstream results? Does a scaling trend emerge for reward-model size?\n\n5. Adding a comparison to existing PRM/agent-PRM baselines such as [1] would further strengthen the evaluation.\n\n[1] Process Reward Models for LLM Agents: Practical Framework and Directions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0ffn3cc9nZ", "forum": "LnBEASInVr", "replyto": "LnBEASInVr", "signatures": ["ICLR.cc/2026/Conference/Submission22507/Reviewer_S77r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22507/Reviewer_S77r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934749648, "cdate": 1761934749648, "tmdate": 1762942247019, "mdate": 1762942247019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Tool-call Reward Model (TRM), a specialized process reward model designed to evaluate and reward tool invocations. The authors further enhance TRM's application in PPO and GRPO training through refined credit assignment and turn-level advantage estimation. The effectiveness of the proposed method is demonstrated on search-based QA and Python code-based mathematical tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Current tool-calling agent training predominantly relies on outcome rewards. The proposed TRM addresses a crucial need for process-level evaluation in tool-calling agent training, which is highly valuable for the field.\n2. The introduction in this paper is detailed, with comprehensive explanations of both the background and methodology.\n3. The authors conduct comprehensive experiments, including both scaling assessments of TRM and practical evaluations on search-based QA and coding tasks. The experimental results demonstrate consistent and substantial improvements over baseline methods across different tasks."}, "weaknesses": {"value": "1. The paper lacks critical details regarding the construction of TRM training data. Specifically, how do the authors ensure the correctness of annotated tool call rewards? Evaluating the correctness and necessity of tool invocations at each turn requires a comprehensive understanding of the overall problem and reasoning process, which poses significant challenges for models, particularly when tools provide information beyond the model's knowledge scope. Have the authors conducted any validation of the annotation quality?\n2. For open-ended tasks such as search-based QA, it is challenging to directly assess the effectiveness of final results. The paper does not adequately address how such evaluations are conducted, which raises concerns about the reliability of the reported improvements."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MLdGAOcvCx", "forum": "LnBEASInVr", "replyto": "LnBEASInVr", "signatures": ["ICLR.cc/2026/Conference/Submission22507/Reviewer_8EFK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22507/Reviewer_8EFK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997270119, "cdate": 1761997270119, "tmdate": 1762942246784, "mdate": 1762942246784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Tool-call Reward Model (TRM), a process reward model and a training/credit-assignment recipe to integrate TRM with PPO/GRPO. The authors distill per-turn \"necessity\" and \"quality\" labels from tool-enabled rollouts, train a binary classifier head on top of a small LLM (1.5B–7B). Experiments on search-based QA and code-based math claim consistent gains over outcome-only RL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is straightforward and easy to follow. It shows consistent improvements on search-QA and code-based math.\n\n2. The paper identifies the limitation of outcome-only rewards for agentic tool use and motivates per-call supervision with concrete failure modes.\n\n3. It finds that mid-sized TRMs (1.5B–3B) trained on ~10k labeled trajectories suffice, with larger models overfitting at this data scale"}, "weaknesses": {"value": "1. Insufficient comparison with existing tool-augmented LLMs. It omits direct comparisons to process-supervised tool-use methods that score steps or calls (e.g., rule/PRM-guided selection for search or code).\n\n2. Missing positioning relative to tool-based RM in the literature. The paper does not cite or discuss the line of work referred to as tool-augmented reward modeling [1]. The motivation of proposing Tool-call Reward Model is unclear. The authors did not discuss with previous tool-augmented RM and completely ignores previous literature. \n\n3. While turn-level estimation mitigates \"fewer calls is better\", the paper does not report behavioral metrics, such as average #tool-calls, redundant calls, early-stop rates, or task-time/latency impacts under TRM vs. baselines.\n\n4. TRM inference is performed during RL and optionally at inference (best-of-n). The paper should quantify the compute/memory overheads.\n\n5. Lack sufficient ablations. Ablations to disambiguate distillation vs. TRM:\n   - Train a trajectory-level RM/PRM (no per-call labels) on the same data, and compare to TRM under equal compute.\n   - Train a tool-based RM such as [1] on the same data, to demonstrate the good claims of PRM. The paper states TRM is a specific PRM for tool invocation, yet it does not compare to a generic PRM trained on the same trajectories. At minimum, the authors should implement a standard PRM that scores intermediate steps or turns\n\n6. The training data is completely generated by LLMs, without mentioning any interventions or pipelines. The author did not report the details of distilled data.\n\n7. It is unclear how the authors evaluate Tool-call RM on downstream tasks, such as AIME.\n\n8. The authors evaluate TRM on best-of-n inference. The author should report the comparisons with previous reward models. Additionally, the author may report the RL training performance using proposed TRM.\n\n9. The usage of \"necessity\" and \"quality\" requires a detailed ablation comparison, while the paper only describes the concept/motivation in the method section. Most importantly, and how to do the quality check, how to measure the ground truth score of the intermediate steps, and how does this impact on the final results, remains still unclear.\n\n\n\nReferences:\n\n[1] tool-augmented reward modeling. ICLR 2024."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U2kwPU1mlP", "forum": "LnBEASInVr", "replyto": "LnBEASInVr", "signatures": ["ICLR.cc/2026/Conference/Submission22507/Reviewer_TgrD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22507/Reviewer_TgrD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021494489, "cdate": 1762021494489, "tmdate": 1762942246607, "mdate": 1762942246607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}