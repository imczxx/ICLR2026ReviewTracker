{"id": "9Kcfs1xNeq", "number": 1653, "cdate": 1756901382342, "mdate": 1759898196904, "content": {"title": "Activation with Intrinsic-Extrinsic Consensus", "abstract": "Artificial Neural Networks (ANNs) are powerful tools for complex pattern recognition and decision-making. While existing activation mechanisms often promote sparsity through thresholding, they lack an explicit assessment of channel relevance, making networks susceptible to interference from noisy channels. Such irrelevant activations can propagate through the network and adversely affect the final decision. Inspired by observations that channel relevance can be assessed from both intrinsic activity levels and extrinsic decision weights---and that a strong consensus exists between these two aspects---this paper proposes AIEC (Activation with Intrinsic-Extrinsic Consensus), a novel activation mechanism designed to identify and suppress irrelevant channels during training. AIEC consists of three components: an intrinsic Activation-Counting Unit that tracks channel activation statistics, an extrinsic Decision-Making Unit that learns channel decision weights, and a Consensus Gatekeeping Unit that suppresses irrelevant channels based on the agreement between the intrinsic and extrinsic assessments. Extensive experiments demonstrate that AIEC effectively suppresses irrelevant channels and facilitates sparser neural representations. Furthermore, AIEC is compatible with a wide range of mainstream ANN architectures and achieves superior performance compared to existing activation mechanisms across multiple tasks and domains.", "tldr": "", "keywords": ["artificial neural network", "activation", "intrinsic-extrinsic consensus", "channel gatekeeping", "neural response", "neural representation", "deep learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e579bed74e5ac4054f7357395c199d30895b7f80.pdf", "supplementary_material": "/attachment/c0b8d3c5b1850bcb41af576cc1baa837cdcd4611.zip"}, "replies": [{"content": {"summary": {"value": "The idea of this paper is that some features learned by a network are not relevant and should be suppressed.  The paper proposes a mechanism to identify and then suppress features by adding some parameters and a moving average of activity for a given feature detector.  The paper proposed some additional loss functions for training these auxiliary parameters, many of which are only needed during training.  The evaluations show improved results on image classification benchmarks for ResNet-18, DeiT-Tiny, etc."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides intuitive justification for the proposed technique."}, "weaknesses": {"value": "The idea of suppressing irrelevant features seems like something gradient descent would just want to do by default with any reasonable loss function.  Invoking a simple biological model from the 1950's is entertaining in this context, but given the reasoning is very informal about how this applies it's unclear to me how sound the reasoning is that leads to the AIEC technique.\n\nThe paper lack a comparison with models extended to increase parameter counts by the same amount as required to implementing the proposed AIEC technique.  \n\nWriting could be improved:  The phrase \"decision weight\" is never defined but it's used extensively including in the abstract.  Text is duplicated unnecessarily (the caption for Figure 1 is same as text on lines 149-156).  Figure 2 is referenced on Page 1 long before Figure 1 which isn't referenced until Page 3.  There is no clear separation of training versus inference in the algorithm description (indeed, there are no algorithm figures at all -- just equations).  Some equations seem incorrect."}, "questions": {"value": "What is a \"decision weight\"?   The paper is written seemingly assuming readers are familiar with this term, but I haven't come across it that I can remember.\n\nWhat if $a^k_c < 0$ in Equation 4?  Is there some reason to believe that Equation 3 should result in non-negative elements of $a$?\n\nDoes Equation 13 not suppress the channels with high relevance?  If the point of Equation 11 is to find channels of high relevance, this seems to me to be what would happen with the loss function in Equation 13.\n\nCan you outline the steps required to apply your proposal in training and then in inference?  I did't see an algorithm figure in the main text or the appendices."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o4pc6l57Rw", "forum": "9Kcfs1xNeq", "replyto": "9Kcfs1xNeq", "signatures": ["ICLR.cc/2026/Conference/Submission1653/Reviewer_g8BT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1653/Reviewer_g8BT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760500583543, "cdate": 1760500583543, "tmdate": 1762915843613, "mdate": 1762915843613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study analyzed the channel relevance in Artificial Neural Networks (ANN) and proposed an Activation with Intrisinc-Extrinsic Consensus (AIEC) mechanism that identifies irrelevant channels and performs gatekeeping for noise cleaning. Empirical results demonstrated that the AIEC mechanism outperforms previous activation mechanisms on classification and anomaly detection tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This study revisited the activation mechanisms and demonstrated that gatekeeping from a global feature perspective may outperform original/traditional activation mechanisms.\n2. The proposed AIEC methodology exhibits robust improvements over the baseline and SOTA methods in commonly used image classification benchmarks, and anomaly detection tasks."}, "weaknesses": {"value": "1. While activation based on global feature learning may demonstrate better performance, the mechanism's scalability could be its weakness. It may require higher communication overhead that could affect training efficiency in distributed training, and requries specifc caring for every new architecture. While the additional GPU Memory cost of the proposed AIEC method is neglectable in the scope of this paper, the induced cost may become significant if applied to larger architectures or data dimensions; A theoretical analysis on the computational complexity of AIEC may help to justify."}, "questions": {"value": "1. What is the params size of the ViT-tiny model utilized in this paper? (as their are multiple claimed VIT-tiny models) A more detailed experimental setup section in the appendix is appreciated.\n2. As AIEC achieved improvements on Vision Transformers, is it possible to extend the mechanism to language transformer models, in particular GPT-2 with an accessible computational costs (in future studies)?\n3. Channels are not a uniform feature in machine learning or deep learning models and tasks; can the authors discuss if the AIEC mechanism can be extended to a different level, for example attention heads, or individual neurons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ND9cP7jJsU", "forum": "9Kcfs1xNeq", "replyto": "9Kcfs1xNeq", "signatures": ["ICLR.cc/2026/Conference/Submission1653/Reviewer_MwHQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1653/Reviewer_MwHQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663375672, "cdate": 1761663375672, "tmdate": 1762915843468, "mdate": 1762915843468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AIEC (Activation with Intrinsic-Extrinsic Consensus), a novel activation mechanism that identifies and suppresses irrelevant feature channels during neural network training through a consensus-based assessment combining intrinsic activity statistics and extrinsic decision weights. The core observation is that channel relevance can be assessed from both the frequency of channel activation (intrinsic) and learned classifier weights (extrinsic), and these two assessments exhibit strong consensus. The method is evaluated across multiple architectures (ViTs, CNNs, GNNs) and datasets (CIFAR-10/100, ImageNet-100/1K, graph datasets).This paper proposes AIEC (Activation with Intrinsic-Extrinsic Consensus), a novel activation mechanism that identifies and suppresses irrelevant feature channels during neural network training through a consensus-based assessment combining intrinsic activity statistics and extrinsic decision weights. The core observation is that channel relevance can be assessed from both the frequency of channel activation (intrinsic) and learned classifier weights (extrinsic), and these two assessments exhibit strong consensus. The method is evaluated across multiple architectures (ViTs, CNNs, GNNs) and datasets (CIFAR-10/100, ImageNet-100/1K, graph datasets)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method's effectiveness is demonstrated across a diverse set of architectures, including multiple ViT variants, standard CNNs (AlexNet, VGG, ResNet, etc.), and even Graph Neural Networks (GCN, GraphSAGE).\n- The inclusion of non-vision graph datasets (T-Finance, Elliptic, Weibo) to test generalization beyond computer vision is a commendable strength. \n- The proposed intrinsic-extrinsic duality provides a clear and intuitive structure for conceptualizing the problem of channel relevance.\n- The proposed mechanism (ACU, DMU, CGU) operates only during training, meaning it adds no computational or latency overhead at inference time, as shown in Table 4."}, "weaknesses": {"value": "- This is the most significant flaw. AIEC introduces two substantial auxiliary loss terms ($\\mathcal{L}_{aux}$, $\\mathcal{L}_{gate}$) and a complex stateful counting mechanism. It is then compared against simple, stateless activation functions like ReLU and GELU. A fair evaluation would require comparing AIEC to other stateful channel selection/gating mechanisms or, at minimum, adding equivalent regularization to the baselines.\n- The core idea of using auxiliary losses (extrinsic) or activation statistics (intrinsic) to gauge importance is well-established. The novelty hinges entirely on their consensus, yet the empirical justification for this added complexity is weak. The ablation study (Table 3) shows the intrinsic-only approach ($AIEC_I$) achieves 69.7% accuracy, while the full consensus model ($AIEC_{I\\cap E}$) achieves 71.1%. The paper fails to provide a strong justification for why this modest 1.4% gain is worth the significant increase in mechanism complexity. \n- (Fig. 2, 3) The paper provides no quantitative correlation analysis between the intrinsic and extrinsic assessments to substantiate this claim.\n- The method is applied inconsistently across architectures without a clear justification. The authors apply AIEC to every block in Transformer models but only to the last block in CNNs.\n- While the paper claims negligible overhead, Table 4 shows a non-trivial 9.5% increase in training latency for ViT-Tiny. But, its too small models and datasets to support your word.\n- The Threshold Activation Unit (TAU) ultimately employs a fixed threshold of zero, rendering it functionally identical to ReLU. The experimental results presented in the Appendix merely reaffirm the known effectiveness of ReLU, rather than demonstrating any distinct contribution of TAU itself. Consequently, TAU does not offer any genuine novelty within this work and can be regarded as a rephrased formulation of ReLU."}, "questions": {"value": "- Could you please provide a comparison against other baselines with (a) comparing AIEC to other dynamic channel selection or gating mechanisms, or (b) re-running baseline (ReLU, GELU)?\n- (Table 3) Ablation shows only 1.4% gain from consensus vs. intrinsic alone. can you provide a stronger theoretical or empirical justification for the added complexity of the DMU and the consensus mechanism?\n- The ACU and DMU both scale with the number of classes, $K$. Could the authors provide a FLOPs analysis and a memory/latency scalability study for much larger $K$?\n- Why apply to all ViT blocks but only last CNN block? Could you provide systematic layer-wise application study?\n- What makes channels irrelevant? Are they truly noisy or just task-irrelevant? \n- Since AIEC identifies irrelevant channels, can you prune them post-training? What accuracy-efficiency tradeoffs result?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zkfm1Vi2zS", "forum": "9Kcfs1xNeq", "replyto": "9Kcfs1xNeq", "signatures": ["ICLR.cc/2026/Conference/Submission1653/Reviewer_hHGn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1653/Reviewer_hHGn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722040154, "cdate": 1761722040154, "tmdate": 1762915843316, "mdate": 1762915843316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AIEC (Activation with Intrinsic-Extrinsic Consensus), a novel activation mechanism designed to suppress irrelevant neural channels by leveraging a dual assessment of channel relevance. The method combines intrinsic statistics from activation frequency and extrinsic decision weights learned via a linear classifier, using their consensus to gate unimportant channels during training. Experiments across various architectures (CNNs, ViTs, GNNs) and datasets (CIFAR, ImageNet, Weibo) show that AIEC improves accuracy and sparsity with minimal overhead. While the idea is intuitive and well-implemented, it overlaps conceptually with prior work on feature attribution and sparsity, lacks theoretical grounding for its gating mechanism, and omits comparisons to closely related structured pruning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. __Clear Identification of a Practical Problem__: The paper addresses a meaningful limitation of existing activation functions, leading to unnecessary activations, which may propagate noise. This is a well-motivated problem with practical implications for sparsity, interpretability, and robustness.\n\n2. __Conceptually Intuitive and Modular Design__: The proposed AIEC framework clearly separates intrinsic (activity-based) and extrinsic (decision-based) assessments of channel relevance, combining them via a consensus mechanism. Each module (ACU, DMU, CGU) has a well-defined role, and the design is simple enough to be adapted across architectures.\n\n3. __Proper Clarity and Visualization__: The paper is well-structured and readable, with well-crafted figures (e.g., Figure 1, Figure 2, and Appendix visualizations) that help explain both the motivation and effects of AIEC. These visualizations qualitatively support the gating effect on irrelevant channels.\n\n4. __Computational Efficiency Report__: The authors provide detailed reporting of GPU memory usage and latency (Table 4), showing that AIEC introduces minimal overhead during training and inference, which indicates an important factor for practical deployment."}, "weaknesses": {"value": "__1. Lack of Theoretical Justification__\n\nOne of the primary limitations of the paper is the absence of theoretical background and justification behind its core mechanisms, particularly the consensus-based gating strategy in Eq. (11). While the intuition that irrelevant channels should be identified when both intrinsic (activation-based) and extrinsic (decision-weight-based) assessments agree is reasonable, the choice of a hard intersection as the gating criterion lacks formal justification. There is no information-theoretic, statistical, or optimization-based rationale for why the intersection is better than, for instance, union or threshold-weighted approaches. Additionally, the choice of using a fixed threshold τ = 0 in both the ACU and DMU components is empirically motivated, yet its suitability across architectures and datasets is neither theoretically analyzed nor justified through sensitivity experiments. As a result, the reliability and generalizability of the proposed mechanism remain questionable. However, this concern could be mitigated if the authors provide clear theoretical reasoning or empirical evidence supporting these design choices.\n\n\n__2. Non-Differentiable Gating and Optimization Concerns__\n\nThe gatekeeping mechanism employed in AIEC introduces hard, binary decisions for channel suppression (Eqs. 7, 10, and 12), which are non-differentiable. While the authors state that AIEC operates during training, they do not address how gradient flow is handled through the gating logic. This is particularly concerning as the consensus gate (CGU) functions as a multiplicative mask, effectively blocking gradient propagation for suppressed channels. In the absence of surrogate gradients or soft approximations, this may lead to convergence instability or ineffective training, especially in deeper networks or more complex tasks. The lack of discussion around training dynamics, stability, or gradient handling in the presence of hard gates is a significant omission, raising doubts about the robustness of the proposed method under various optimization conditions. Introducing a differentiable approximation or continuous relaxation of the gating function could address this issue by enabling gradient flow during training [1]. It would be beneficial for the authors to provide justification or analysis of this design choice, especially regarding whether such approximations were considered or why hard gating was preferred.\n\n[1] Lee, Kyungsu, et al. \"Stochastic adaptive activation function.\" Advances in Neural Information Processing Systems 35 (2022): 13787-13799.\n\n\n__3. Missing Comparisons to Key Baselines__\n\nCan the authors elaborate on the rationale for comparing AIEC only against conventional activation functions such as ReLU, GELU, and SiLU, rather than including more structurally relevant baselines such as L0 regularization, channel pruning techniques, or attention-based gating modules like CBAM[2]? Given that the primary goal of AIEC is to suppress task-irrelevant channels, these methods share a similar motivation and mechanism. Without such comparisons, it becomes unclear whether the observed improvements are due to better activation dynamics or simply the addition of a gating mechanism. Additionally, since the current evaluation focuses mainly on classification tasks, further clarification on whether task-specific constraints or computational considerations drove this choice would help clarify the intended generalizability of the method.\n\n[2] Woo, Sanghyun, et al. \"Cbam: Convolutional block attention module.\" Proceedings of the European conference on computer vision (ECCV). 2018."}, "questions": {"value": "1. __On the Use of a Linear Classifier in the DMU__: The Decision-Making Unit (DMU) employs a linear classifier without a bias term to compute decision weights for each channel. Could the authors clarify why a linear model was chosen, and whether more expressive classifiers (e.g., MLPs or non-linear heads) were considered? How sensitive is the method’s performance to this modeling choice?\n\n\n2. __On the Epoch-Level Resetting of ACU Statistics__: The Activation-Counting Unit (ACU) resets its statistics at the beginning of each epoch, as stated in Section 3.4. What is the rationale behind this design choice? Have the authors evaluated the impact of accumulating statistics across epochs, and whether that would lead to more stable or reliable intrinsic relevance estimates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rh467njTwZ", "forum": "9Kcfs1xNeq", "replyto": "9Kcfs1xNeq", "signatures": ["ICLR.cc/2026/Conference/Submission1653/Reviewer_jQCx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1653/Reviewer_jQCx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762859595698, "cdate": 1762859595698, "tmdate": 1762915841111, "mdate": 1762915841111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}