{"id": "j7Vt2lp2jX", "number": 144, "cdate": 1756729597323, "mdate": 1763313106022, "content": {"title": "TINKER: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization", "abstract": "We introduce TINKER, a novel framework for high-fidelity 3D editing without any per-scene finetuning, where only a single edited image (one-shot) or a few edited images (few-shot) are required as input. Unlike prior techniques that demand extensive per-scene optimization to ensure multi-view consistency or to produce dozens of consistent edited input views, TINKER delivers robust, multi-view consistent edits from as few as one or two images. This capability stems from repurposing pretrained diffusion models, which unlocks their latent 3D awareness. To drive research in this space, we curate the first large-scale multi-view editing dataset and data pipeline, spanning diverse scenes and styles. Building on this dataset, we develop our framework capable of generating multi-view consistent edited views without per-scene training, which consists of two novel components: (1) Multi-view consistent editor: Enables precise, reference-driven edits that remain coherent across all viewpoints. (2) Any-view-to-video scene completion model : Leverages spatial-temporal priors from video diffusion to perform high-quality scene completion and novel-view generation even from sparse inputs. Through extensive experiments, TINKER significantly reduces the barrier to generalizable 3D content creation, achieving state-of-the-art performance on editing, novel-view synthesis, and rendering enhancement tasks, while also demonstrating strong potential for 4D editing. We believe that TINKER represents a key step towards truly scalable, zero-shot 3D and 4D editing.", "tldr": "TINKER achieves generalizable 3D editing with one or few inputs without per-scene optimization and demonstrating strong potential for 4D editing.", "keywords": ["Diffusion Model", "3D Editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ecade5c44cd2ba3dacd6078e54926c3a58f2095.pdf", "supplementary_material": "/attachment/fefa62349acd0c9b55727b02d8ae572346f8f411.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a feedforward 3D scene editing pipeline and introduces a multi-view consistent image editing dataset. The method adopts a state-of-the-art large image editing model to generate sparsely edited views, and then employs a finetuned video model to reconstruct the scene based on the estimated depth maps of the original scene and the sparse edited views. The results demonstrate strong multi-view consistency, and the approach does not require per-scene optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a feedforward pipeline for 3D scene editing, which differs from previous per-scene optimization-based editing methods.\n2. By leveraging the power of 2D editing models, Tinker preserves the identity and consistency of multi-view sparse images; moreover, by exploiting the capability of a video diffusion model, it achieves precise reconstruction by concatenating depth maps and sparse views with full attention.\n3. The writing is clear and easy to follow."}, "weaknesses": {"value": "The method appears to struggle with structural editing of scenes. In particular, it is difficult to perform large geometric changes or significant deformations. This limitation arises because, during the scene completion stage, the approach relies on the depth maps of the original videos. Such dependency introduces inconsistencies between the edited views and the original depth maps when large deformations occur, which likely leads to degraded reconstruction quality."}, "questions": {"value": "Based on the hypothesis mentioned in the weakness section, could the authors provide further insights or explanations regarding this limitation? Specifically, can the proposed model support large geometric deformations? If not, what are the potential directions to improve or extend the framework to handle such cases? I would be happy to hear the authorsâ€™ thoughts on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qT0IYQUDi0", "forum": "j7Vt2lp2jX", "replyto": "j7Vt2lp2jX", "signatures": ["ICLR.cc/2026/Conference/Submission144/Reviewer_RuqL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission144/Reviewer_RuqL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546229447, "cdate": 1761546229447, "tmdate": 1762915457583, "mdate": 1762915457583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TINKER, a framework for 3D editing that eliminates the need for per-scene diffusion model optimization, making it highly efficient compared to traditional methods. TINKER achieves multi-view consistency from as few as one or two edited images, enabling precise edits across different viewpoints."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The visual results are impressive and demonstrate the effectiveness of the proposed method.\n2. The use of a video model for 3D editing is an innovative approach. With video generative priors, TINKER is the first method capable of jointly editing both 3D and 4D scenes."}, "weaknesses": {"value": "1. There is an over-claim of contributions. Many baselines, such as DGE and GaussCtrl, also do not require fine-tuning the diffusion model. Therefore, this should not be considered a unique contribution of TINKER.\n2. The majority of the baselines use InstructNerf2Nerf or ControlNet as the base 2D editors, whereas TINKER utilizes the FLUX model. It is unclear where the true improvement lies: is it in the advanced 2D editing model, or is it in the proposed pipeline? What if these baselines were equipped with the FLUX model?\n3. AIGC tasks, such as 3D editing, necessitate a comprehensive user study to assess its performance in terms of human preference and subjective quality.\n4. More visualizations of the collected training dataset should be presented to give readers a clearer understanding of the data and its characteristics."}, "questions": {"value": "As with weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WPtnEfm5R2", "forum": "j7Vt2lp2jX", "replyto": "j7Vt2lp2jX", "signatures": ["ICLR.cc/2026/Conference/Submission144/Reviewer_yNST"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission144/Reviewer_yNST"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792050958, "cdate": 1761792050958, "tmdate": 1762915457466, "mdate": 1762915457466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for 3D editing that operates without per-scene fine-tuning. The approach consists of two core components: a multi-view consistent editor, built upon a large-scale image editing model and a multi-view image editing dataset, and a scene completion model that generates dense edited views. The method is evaluated on the Mip-NeRF-360 and IN2N datasets with qualitative and quantitative results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a one-shot or few-shot approach for 3D editing.\n2. This paper proposes a generalizable pipeline for 3D editing.\n3. The paper is well-structured and easy to follow."}, "weaknesses": {"value": "1. For the multi-view image editing model, when dealing with views that have large variations, does the multi-view consistency of the edits decrease? If so, could these inconsistencies be further propagated and amplified by the subsequent scene completion model?\n2. Since the scene completion model relies on geometric information like depth maps, in few-shot or even one-shot settings with large view variations, is it prone to introducing more hallucinations or geometric distortions to fill in the missing information?\n3. Although the method eliminates per-scene finetuning, its overall editing time, as shown in Table 1, does not present a significant advantage over some baseline methods. This suggests that the inference cost of the models involved might be a bottleneck.\n4. In the supplementary video \"Edited_Novel_View_Rendering.mp4\" for the IN2N person scene, noticeable artifacts can be observed around the edges of the edited object. What is the primary cause of these edge artifacts?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1SQP2tOT7s", "forum": "j7Vt2lp2jX", "replyto": "j7Vt2lp2jX", "signatures": ["ICLR.cc/2026/Conference/Submission144/Reviewer_KqtQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission144/Reviewer_KqtQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835181217, "cdate": 1761835181217, "tmdate": 1762915457283, "mdate": 1762915457283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, Tinker, has proosed a general-purpose 3D editing framework. Given the reconstructed 3DGS, the proposed editing pipelien will perform video depth estimation, muti-view consistent editsing, and scene completion sequentially. The edited 3dgs is of high quality under comprehensive evaluation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed method is efficient, reasonable, and offers high quality.\n2. The writing is good.\n3. The proposed dataset will be very helpful to this field."}, "weaknesses": {"value": "1. The main issue is that this method looks very complicated, though it is necessary to make the 3D editing feed-forward. Still, too many components are involved in this process.\n\nOverall, I think this is a good paper and worth acceptance. I just encourage the authors to think of the next step and tackle this task in a more elegant way."}, "questions": {"value": "1. Since VDM has made great progress recently, 3D-aware VDM like Lyra, Gen3C, and VIST3A has also shown good capability. I wonder whether the proposed pipeline can be radically replaced by the VDM-based pipeline.\n2. Besides, 3D foundation models are getting better now. Rather than directly working on the 3DGS, I wonder whether the proposed pipeline can be improved to incorporate 3D VFMs like VGG-T / AnySplat to facilitate easier 3D reconstruction editing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ak0pHy0fwU", "forum": "j7Vt2lp2jX", "replyto": "j7Vt2lp2jX", "signatures": ["ICLR.cc/2026/Conference/Submission144/Reviewer_c8ha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission144/Reviewer_c8ha"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839094324, "cdate": 1761839094324, "tmdate": 1762915457137, "mdate": 1762915457137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear reviewers, ACs, SACs, PCs,\n\nWe sincerely thank all reviewers for their dedicated time and insightful feedback on our manuscript. We are grateful for the constructive comments and the positive recognition of our work.\n\nWe are highly encouraged to note that the reviewers share our focus on the same key future directions, such as Feed-Forward 3D Gaussian. We are grateful for this insightful engagement and the opportunity to discuss the future evolution and potential improvements for TINKER with **Reviewer c8ha** and **Reviewer RuqL**.\n\nWe are particularly grateful to **Reviewer KqtQ** and **Reviewer yNST**, who provided not only highly professional critiques but also invaluable suggestions for enhancing TINKER.\n**Reviewer KqtQ** raised several great points, including: (1) the impact of view variations, (2) the potential for depth map-induced hallucinations, (3) concerns over inference cost, and (4) an inquiry into a specific edge artifact. We have thoroughly addressed each of these in our individual response, providing detailed explanations and new supporting experiments. In our response, we demonstrated that large view variations, rather than being detrimental, actually provide greater informational gain that enhances model performance. We also clarified that the use of depth maps is a key factor in mitigating model-induced hallucinations. Regarding practical concerns, we explained that TINKER's inference cost can be readily optimized by leveraging existing acceleration techniques. Finally, we clarified that the noted edge artifact is not a fundamental limitation of our method and can be straightforwardly resolved.\n\n**Reviewer yNST** identified an imprecision in our \"per-scene fine-tune\" terminology. We have corrected this in the revised paper, explicitly stating that this process encompasses both model training and costly hyper-parameter tuning. Furthermore, in response to the suggestions, we have incorporated: (1) comprehensive experiments about flux-adapted baselines, (2) a new User Study to validate that TINKER's results are strongly preferred by users, and (3) more data visualizations to improve clarity.\n\nThe professionalism and insightful feedback from all the reviewers have been instrumental in significantly improving the quality and rigor of our work. Finally, we hope to express our sincere gratitude again to all reviewers, as well as to the ACs, SACs, and PCs, for their substantial time and dedicated effort throughout this review process."}}, "id": "vJ0vF7ZEAY", "forum": "j7Vt2lp2jX", "replyto": "j7Vt2lp2jX", "signatures": ["ICLR.cc/2026/Conference/Submission144/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission144/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission144/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763312916801, "cdate": 1763312916801, "tmdate": 1763312916801, "mdate": 1763312916801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}