{"id": "xsSJw6jJBL", "number": 3675, "cdate": 1757495533215, "mdate": 1759898075781, "content": {"title": "Photon: Speedup Volume Understanding with Efficient Multimodal Large Language Models", "abstract": "Multimodal large language models are promising for clinical visual question answering tasks, but scaling to 3D imaging is hindered by high computational costs. Prior methods often rely on 2D slices or fixed-length token compression, disrupting volumetric continuity and obscuring subtle findings. We present Photon, a framework that represents 3D medical volumes with token sequences of variable length.  Photon introduces instruction-conditioned token scheduling and surrogate gradient propagation to adaptively reduce tokens during both training and inference, which lowers computational cost while mitigating the attention dilution caused by redundant tokens. It incorporates a custom backpropagation rule with gradient restoration to enable differentiable optimization despite discrete token drop. To stabilize token compression and ensure reliable use of visual evidence, Photon further applies regularization objectives that mitigate language-only bias and improve reliability. Experiments on diverse medical visual question answering tasks show that Photon achieves state-of-the-art accuracy while reducing resource usage and accelerating both training and inference.", "tldr": "Photon is a variable-length 3D medical VQA framework with instruction-conditioned token scheduling and surrogate gradients, achieving adaptive acceleration and state-of-the-art performance.", "keywords": ["3D Medical Image Analysis", "Medical VQA", "Medical VLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb689aa2d343aaac66a60d0e253e546e395abf07.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Photon, an efficient multimodal large language model (MLLM) framework designed for 3D medical volume understanding—particularly in clinical visual question answering (VQA) tasks. Existing MLLMs struggle with 3D data because of high computational costs and loss of spatial continuity from slice-based or fixed-length token compression methods. Photon addresses these limitations by introducing a variable-length token representation and adaptive token reduction strategies that preserve volumetric fidelity while reducing computational load."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed Instruction-conditioned Token Scheduling (ITS) and Surrogate Gradient Propagation (SGP) mechanisms are well presented. \nThe model achieves strong empirical performance on challenging benchmarks 3D-RAD and DeepTumorVQA, including tasks that require fine-grained spatial reasoning and longitudinal understanding—domains where many prior vision-language models underperform. Photon achieves significant reductions in training and inference memory (up to 5× speedup, 2–3× memory reduction) while preserving or improving task performance. The ablation study does clearly show that each component contributes to overall performance and training efficiency."}, "weaknesses": {"value": "1. The technical novelty remains questionable. Visual token reduction (adaptive or non-adaptive via instruction tokens) has already been established in many previous papers. For instance, ATP-LLaVA [1] learns adaptive thresholds and drops tokens per instance/layer. IVTP also perform instruction-conditioned token selection with adaptive thresholds [2]. TransPrune [3] likewise uses instruction-guided attention to prune, so the core idea of instruction-aware and adaptive token pruning is not new. What Photon adds is mainly the 3D medical-volume setting. \n\n2. Limited theoretical guarantees on convergence. While the surrogate gradient propagation in Section 3.3 and Appendix A.1.3 shows local consistency, this only guarantees alignment of update direction, not convergence or optimality. There is no formal guarantee that the retained set of tokens optimizes any global objective or converges to an optimal pruning schedule. A proof that shows the surrogate gradient descent converges to a fixed point would be interesting. \n\n3. The experiments are limited to CT data. While the paper suggests extensibility to other modalities (MRI), no results or even small-scale demonstrations support these claims.\n\n4. While the paper shows failure analysis, no standard deviation or confidence interval is shown. Can the authors quantify using statistical tests to show whether gains are significant?\n\nReferences:\n\n1. https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_ATP-LLaVA_Adaptive_Token_Pruning_for_Large_Vision_Language_Models_CVPR_2025_paper.pdf\n\n2. https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02577.pdf\n\n3. https://arxiv.org/abs/2507.20630\n\n4. https://arxiv.org/abs/2403.17834"}, "questions": {"value": "Although the paper promises code release, there is no current code repository or full details sufficient for reproduction (e.g., dataset loading, data processing, hyperparameter details). Will the authors release the code?\n\nHow robust is the model to misestimated token importance or noisy retention thresholds? The current surrogate gradient strategy assumes reasonably accurate saliency estimates and threshold predictions. In practice, misestimation may lead to dropping critical visual tokens, especially early in training. Did the authors evaluate robustness under saliency noise or threshold perturbations (e.g., through ablation or noisy injection)? What prevents collapse into degenerate token usage?\n\nHow do retained tokens spatially distribute in 3D volumes, and are critical structures preserved? The paper provides visualization of retained regions, but does not evaluate whether anatomically or clinically critical regions (e.g., organs, tumors) are consistently retained across examples. Have the authors considered overlap analysis with reference segmentations or bounding boxes to quantify spatial alignment between retained tokens and clinically important areas?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z7TiOYUlgb", "forum": "xsSJw6jJBL", "replyto": "xsSJw6jJBL", "signatures": ["ICLR.cc/2026/Conference/Submission3675/Reviewer_TVyY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3675/Reviewer_TVyY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760566584508, "cdate": 1760566584508, "tmdate": 1762916913127, "mdate": 1762916913127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Photon, a 3D-native multimodal large language model (MLLM) for medical visual question answering (Med-VQA). Photon aims to overcome the computational burden of 3D medical imaging through two novel modules:\n\n- Instruction-conditioned Token Scheduling (ITS): dynamically prunes visual tokens using instruction-aware saliency and an instance-specific threshold predictor.  \n- Surrogate Gradient Propagation (SGP): enables differentiable optimization despite discrete token dropping via surrogate gradient restoration.\n\nPhoton achieves substantial acceleration (up to 5× faster inference) and reduced GPU memory usage while maintaining or surpassing state-of-the-art accuracy on 3D-RAD and DeepTumorVQA benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The combination of *instruction-conditioned pruning* and *surrogate gradient backpropagation* for efficient 3D token handling is innovative and mathematically well-founded.  \n\n\n- Photon outperforms all major baselines (RadFM, M3D, OmniV, Lingshu, etc.) by **3–14%** across multiple Med-VQA tasks.  \n- Achieves ~2/3 GPU memory reduction and ~5× training/inference speedup, verified through detailed benchmarks and ablations.\n\n \n- Includes both 3D-RAD and DeepTumorVQA, along with visualizations, ablation on ITS/SGP, and computational efficiency metrics."}, "weaknesses": {"value": "- If the instruction-conditioned token scheduling can transfers to domains with different spatial and noise characteristics? \n \n- How does Photon perform in zero-shot settings on out-of-distribution datasets?\n\n- While comparisons are thorough within medical MLLMs, the study omits re-implementations of VisionZip, LLaVA-PruMerge, or ATP-LLaVA under medical conditions, limiting cross-domain efficiency comparisons.  \n\n- How would Photon scale when integrated with larger base models in terms of training stability and memory efficiency?\n\n- Can the authors clarify if ITS + SGP could be extended to temporal or video-based reasoning, where token dynamics change over time?\n\n- Certain figures lack descriptive captions or context. Even though the figures are well-designed and beautiful, I can not fully understand them without reading the main text carefully.\n\n- Related works are missing some relevant medical vision language models, such as Med-R1 [1], HealthGPT [2].\n\n[1] Lai, Yuxiang, et al. \"Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models.\" arXiv preprint arXiv:2503.13939 (2025).\n\n[2] Lin, Tianwei, et al. \"Healthgpt: A medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation.\" arXiv preprint arXiv:2502.09838 (2025)."}, "questions": {"value": "Please refer to the Weaknesses section. \n\n**I am willing to raise my score according to the rebuttal.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2clpeTFRrK", "forum": "xsSJw6jJBL", "replyto": "xsSJw6jJBL", "signatures": ["ICLR.cc/2026/Conference/Submission3675/Reviewer_6q9R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3675/Reviewer_6q9R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761504813027, "cdate": 1761504813027, "tmdate": 1762916912820, "mdate": 1762916912820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a 3D-native framework for applying MLLM to 3D medical volumes. Unlike existing methods that mainly take 2D slices as input, their method instead directly works with 3D volumes using variable-length visual token sequences. Core to this method are 1) Instruction-conditioned Token Scheduling (ITS) which dynamically prunes visual tokens based on the text instruction and 2) Surrogate Gradient Propagation (SGP), a custom gradient estimation technique to enable end-to-end training of the discrete token-dropping mechanism. Experiments on several benchmarks show that this method improves performance while being efficient."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work addresses an important bottleneck in medical AI (computational challenge for medical QA with MLLM due to 3D nature of the data). The motivation is clear, as processing full 3D volumes preserves volumetric information while dynamic token selection is relatively computationally affordable. \n2. The proposed method IST is novel and seems to be effective. Instruction-conditioned, instance-adaptive token pruning is a step beyond common, instruction-agnostic pruning or fixed-ratio compression methods. \n3. Ablations and computation footprint are transparent (tab 3)"}, "weaknesses": {"value": "1. The proposed method is overly complex, making it hard to follow let alone reproduce. For example, the derivation of the final surrogate gradient involves a long chain of heuristic-based calculations like standardization z_j, monotonic mapping r_j, directional term d_j, magnitude term m_j, and several clipping and clamping operations. Then there are also several regularization terms. These combined make the proposed method inherently brittle. This high sensitivity to implementation details not only hinders reproducibility but also questions the robustness of the reported performance and efficiency gains\n2. Missing comparison to other pruning baselines. The paper mentions other pruning methods like LLaVA-PruMerge and ATP-LLaVA, but did not adapt them as a comparison. The authors mention that these methods lack capabilities such as instruction awareness, etc., but without direct comparison, there is no evidence for such claims. I believe the authors should have adapted at least one other baseline to the 3D medical domain as a more direct baseline to demonstrate the superiority of the proposed ITS and SGP mechanisms, especially since the authors sell on efficiency.\n3. Is there any coefficient for the regularization terms? How sensitive is the method to each of them? I think more explanatioon is needed here."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MonX1XioM3", "forum": "xsSJw6jJBL", "replyto": "xsSJw6jJBL", "signatures": ["ICLR.cc/2026/Conference/Submission3675/Reviewer_goC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3675/Reviewer_goC1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810876791, "cdate": 1761810876791, "tmdate": 1762916912657, "mdate": 1762916912657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed Photon, a novel MLLM for efficient 3D medical volume VQA. It introduces the Instruction-conditioned Token Scheduling (ITS) module for adaptive visual token pruning based on its correspondence with the instruction and the Surrogate Gradient Propagation (SGP) module to enable end-to-end training while doing discrete token-dropping. The proposed method can efficiently reduce the computational cost during both training and inference, and it also improves the performance on 2 public 3D medical VQA datasets with a nontrivial improvement."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed method shows a significant efficiency improvement without sacrificing its performance, which is quite impressive. According to the experiments, the proposed method can successfully reduce the token length by more than 50%, speed up the training process by over 5 times, and reduce the memory usage at the same time. It also shows a non-trivial improvement against SoTA baselines, including larger ones, like the 7B Lingshu model. \n\n2. The idea of instruction-conditioned token scheduling is quite novel to the reviewer. The ITS module makes the token-dropping process instruction conditioned by measuring the saliency against the instruction tokens, providing a dynamic dropping based on different tasks. It also uses an adaptive threshold based on a series of crafted features to avoid over- or under-pruning.\n\n3. The ablation experiment is very detailed, which is helpful to understand the contribution of different modules/losses/regularizers. The visualization in Figure 5 shows the necessity of the proposed regularizations, provides strong support to the model design."}, "weaknesses": {"value": "The reviewer has 2 major concerns about this paper.\n\n1. The paper itself is well-written, although it is not that easy to fully understand all the equations; it is at least clear and detailed. However, the figures in the paper, especially Figure 1 and Figure 2, are very difficult to follow. The reviewer understands that using a uniform color scheme can make it look nicer, but it should not harm the readability. For example, (a) The performance bar plot and radar plot at the bottom of Figure 1 are very confusing; the chosen color is so similar that it makes it difficult to tell the difference. (b) The upper right part of Figure 1 is also confusing; the idea of ITS and SGP is not shown here. (c) In the upper right part of Figure 2, the author uses 2 different colors for each module, which I guess refers to training/frozen, but it is still hard to tell, and there is no annotation for this. (d) The white mask in all qualitative results is difficult to distinguish, as well. It would be better if using some other way to highlight the retained patches, e.g., highlight the contour. (e) Figure 5, maybe better to add 2 separate lines between each figure, only using the color still makes it hard to tell which is which. Also, there are some small typos in the paper, like in equation (3), the bottom part of the summation notation should just be $t'\\in\\mathcal{Q}$, as $t$ is indicated by $c_t$ in the left-hand side.\n\n2. While the proposed method shows nice results, it also increases the complexity of the framework by a lot. There are a lot more hyperparameters introduced here, and there is no explanation or guidance on how to select them. The SGP module itself contributes 6 new hyperparameters, and there are more in the whole framework, like where to insert the ITS and SGP module, and so on. It would be much better if the authors could provide some insight or analysis on how those hyperparameters are chosen and how sensitive the proposed method is to these hyperparameters.\n\n3. While the paper already includes tons of technical details, it still lacks some high-level intuition on its design. Some of the design is not that intuitive to the reviewer. This makes it hard to fully understand the design. Please see the questions below.\n\n4. While the paper already provided some visualization and failure case analysis, it would be better to include some more in-depth analysis on the reasons for failure. Also, the reviewer would like to see an analysis of the **instruction-conditioned** part of the token-dropping. Namely, is there a statistical difference in the token-dropping given different instructions? Given a different task, a different region of interest, will the model behave differently and choose to drop different tokens? This is very critical to support the paper's major claim."}, "questions": {"value": "1. What is the high-level idea to use the text token affinity as the weight for the visual saliency score? The paper claims that tokens with higher overall affinity/attention score will have a higher weight, but what is the insight behind this? Intuitively, you may want to assign a higher weight to task-related word tokens like action/organ/location, and so on. But how does this relate to the text token affinity score?\n\n2. Equation (6) uses 3 feature vectors to construct the final feature for the probability threshold. Can you provide some high-level ideas on this?\n\n3. In the Flip regularizer, the model penalizes the model for giving high-confidence results when tokens that should be dropped are kept. This is a little bit counterintuitive, since it keeps the tokens that should be dropped is technically equivalent to adding unnecessary information. The needed tokens are still the same; the module can make a confident answer with some extra unnecessary information. A more intuitive case may be to reverse the tokens that should be kept, ie, remove necessary information. In this case, the model should give a low-confidence or even wrong answer. The reviewer is curious about this; some clarification would be great.\n\n4. Figure 1 shows the training speed using a 32B level model, but it is not presented in the result section. Considering that the paper is only evaluating over 3 B-level models, it would be great if there were more results on larger models, such as 7B or 32B. Demonstrating the scaling capability.\n\n5. The proposed method is only evaluated on Qwen2.5-VL, but it would be interesting to see its behavior on some other architectures, like LLaMA or Gemma. But the reviewer understands this will take a long time and a lot of work, so this would not influence my final decision. Just curious if authors have tried it on different MLLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oaagjar05d", "forum": "xsSJw6jJBL", "replyto": "xsSJw6jJBL", "signatures": ["ICLR.cc/2026/Conference/Submission3675/Reviewer_buqX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3675/Reviewer_buqX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858964242, "cdate": 1761858964242, "tmdate": 1762916911180, "mdate": 1762916911180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}