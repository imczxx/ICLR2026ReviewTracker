{"id": "jWiWJjHtHo", "number": 10734, "cdate": 1758180709532, "mdate": 1759897632654, "content": {"title": "RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback", "abstract": "Competitive search is a setting where document publishers modify them to improve their ranking in response to a query. Recently, publishers have increasingly leveraged LLMs to generate and modify competitive content. We introduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that trains LLMs using preference datasets derived from ranking competitions. The goal of a publisher (LLM-based) agent is to optimize content for improved ranking while accounting for the strategies of competing agents. We generate the datasets using approaches that do not rely on human-authored data. We show that our proposed agents consistently and substantially outperform previously suggested approaches for LLM-based competitive document modification.  We further show that our agents are effective with ranking functions they were not trained for (i.e., out of distribution) and they adapt to strategic opponents. These findings provide support to the significant potential of using reinforcement learning in competitive search.", "tldr": "Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback", "keywords": ["competitive search", "multi-agent systems", "information retrieval"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/501aa3241035233f6255085c949ecab8c51012de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework, RLRF, for training LLM agents to engage in competitive search—a task where agents strategically edit documents to achieve higher search rankings. The method involves simulating a multi-agent competitive environment to generate a preference dataset, which is then used to fine-tune an agent using DPO. The experiments demonstrate that this dynamically trained agent (RA) outperforms prompt-based baselines and shows an ability to generalize to unseen rankers. The authors also report that their agent, despite being trained solely on a ranking objective, exhibits better content faithfulness than the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear Problem Formulation and Comprehensive Empirical Validation: The paper addresses a timely and interesting problem by formalizing strategic content generation as a learnable, multi-agent task. The experimental setup is comprehensive, providing a solid benchmark for future work in this area. \n\n2.\tIdentifies Interesting Emergent Phenomena: The study successfully identifies two non-obvious and valuable phenomena: (1) the agent's emergent tendency to preserve content faithfulness despite a purely competitive objective, and (2) the asymmetric nature of knowledge transfer between different ranking functions. These findings could inspire follow-up research."}, "weaknesses": {"value": "1.\tInsufficient Conceptual and Methodological Novelty: The primary concern lie in the paper's lack of significant novelty. The core idea—applying a reinforcement learning-style algorithm to a game-theoretic problem—is a well-established paradigm. The methodology is a straightforward combination of existing techniques: using multi-agent simulation (akin to self-play) to generate data, followed by a standard preference alignment algorithm (DPO). While this \"recipe\" is effective, it does not introduce a novel algorithmic component, or a fundamentally new way of thinking about the problem. The central hypothesis that training on dynamic, competitive data (DG) is superior to training on static, isolated data (SG) is also obvious and confirms an existing intuition rather than revealing a new insight. \n\n2.\tSuperficial Analysis of Key Findings: While the paper reports interesting emergent behaviors (faithfulness and asymmetric transfer), it fails to provide a deep investigation into mechanism behind these phenomena. For instance, explaining the emergent faithfulness could lead to profound insights about the implicit biases of modern neural rankers or the nature of strategic alignment. Without this deeper analysis, the findings remain intriguing observations. I recommend the authors could explore more on this aspect.\n\n3.\tUnaddressed Sim-to-Real Gap: The entire evaluation is conducted in a highly idealized simulation that abstracts away the most difficult aspects of the real-world problem: non-stationary and adversarial rankers, black-box algorithms, and sparse, delayed, and noisy feedback. The paper's claims of robustness are based on transferring between fixed, known-type rankers, which does not adequately address the challenges of a constantly evolving, real-world search ecosystem. A more critical discussion of these limitations is needed to properly contextualize the work's practical applicability."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S7GCQ57hKH", "forum": "jWiWJjHtHo", "replyto": "jWiWJjHtHo", "signatures": ["ICLR.cc/2026/Conference/Submission10734/Reviewer_MhmN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10734/Reviewer_MhmN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555140983, "cdate": 1761555140983, "tmdate": 1762921961974, "mdate": 1762921961974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all the reviewers for their insightful and constructive feedback. In this rebuttal, we address all points raised. Due to limited computational resources, several additional analyses are still running, and we expect to report the results and integrate them into the revised paper by the end of the discussion period. Specifically, we are currently conducting:\n(a) a sample-complexity analysis examining performance as a function of the number of fine-tuning steps (as raised in AurJ:W2), and\n(b) a sensitivity analysis of the RA agents with respect to different DPO β values (as suggested in 6VnK:Q2)."}, "title": {"value": "Disclaimer"}}, "id": "k9BfTkanWY", "forum": "jWiWJjHtHo", "replyto": "jWiWJjHtHo", "signatures": ["ICLR.cc/2026/Conference/Submission10734/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10734/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10734/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763320205510, "cdate": 1763320205510, "tmdate": 1763321183902, "mdate": 1763321183902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of editing documents via LLMs to sequentially improve their ranking in a search engine given a query, a setting defined as \"competitive search\". They propose to use the Ranking Feedback (RF) to finetune agents for the task, based on past rankings. Concretely, at each round, the highest and lowest performing document edits are stored and used to finetune the LLMs via the DPO algorithm. Experiments are performed in multi-agent simulations (where multiple agents compete for the highest rank) and show that agents aligned with RF outperform other baseline prompt-based approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper formalizes a novel problem in information retrieval, and shows Reinforcement Learning from Ranking Feedback (RLRF) to be an effective algorithm to maximize ranking, with a clear hedge over simpler prompting baselines.\n\nThe experiments are sound and the RLRF agents seem able to generalize also to unseen ranking functions, making it more realistic for real-world scenarios where the ranking functions are unknown."}, "weaknesses": {"value": "The following are the main weaknesses of the paper: \n\n- The paper lacks novelty in terms of methods: the used approach is quite standard in the LLM literature: collect documents preference pairs --in this case using the ranking function-- and finetune the model via the DPO algorithm.\n\n- The aligned agents are evaluated after DPO finetuning was completed. I think it would be interesting to see the evolution of their performance as a function of the funetuning steps. In particular, how much data (i.e. ranked document pairs) are required for them to outperform the other baselines?\n\nUnfortunately, I cannot judge the relevance of the problem and whether it is of interest to the ICLR community."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lQePbf0FG9", "forum": "jWiWJjHtHo", "replyto": "jWiWJjHtHo", "signatures": ["ICLR.cc/2026/Conference/Submission10734/Reviewer_AurJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10734/Reviewer_AurJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897080074, "cdate": 1761897080074, "tmdate": 1762921961546, "mdate": 1762921961546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Reinforcement Learning from Ranker Feedback (RLRF), a novel framework for training large language models (LLMs) as strategic agents in competitive search environments. The agents optimize document content to improve rankings against competitors by learning from preference datasets derived from ranking competitions. The authors propose two data generation methods: Static Generation (SG) (independent document modifications) and Dynamic Generation (DG) (simulated multi-agent competitions). Experiments using the LEMSS simulator show that RL-aligned agents (RA agents) consistently outperform non-aligned prompt-based agents (NA agents) across diverse ranking functions (e.g., E5, Contriever, BM25). RA agents also generalize to unseen rankers and adapt to strategic opponents. Key contributions include formalizing competitive search as a learning problem, demonstrating RLRF’s effectiveness, and highlighting transfer learning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: Novel formulation of competitive search as an RL problem, with DG simulating strategic opponent adaptation.\n- Quality: Rigorous experiments across 4 LLMs, 4 rankers, and homogeneous/heterogeneous settings. RA agents (e.g., Mistral+DG+LSW) achieve up to 75% win-rate (Ho) and 60% (He).\n- Clarity: Clear problem definition (Section 3) and accessible methodology (e.g., DPO for alignment).\n- Significance: Demonstrates transfer learning to unseen rankers (Table 2), with asymmetric generalization (e.g., Contriever-trained agents generalize better than E5-trained)."}, "weaknesses": {"value": "- Synthetic Limitations: Experiments rely on simulated competitions (LEMSS) and MS MARCO data. Real-world deployment (e.g., dynamic user queries) is unexplored.\n- Scalability: Training RA agents requires multi-round simulations (450 games × 30 rounds), but computational costs are not quantified.\n- Strategy Stability: Convergence analysis (Appendix G) shows agents gravitate toward similar documents over time, but robustness to adversarial opponents (e.g., non-LLM agents) is untested."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oyhK7Qzgu5", "forum": "jWiWJjHtHo", "replyto": "jWiWJjHtHo", "signatures": ["ICLR.cc/2026/Conference/Submission10734/Reviewer_iycE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10734/Reviewer_iycE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987903390, "cdate": 1761987903390, "tmdate": 1762921961171, "mdate": 1762921961171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Limitations of real world deployment"}, "comment": {"value": "While we agree that there is a non-trivial gap between real-world scenarios and our experimental environment, which simulates competitive search using MS MACRO data, we would like to point out that similar simulation-based approaches have been shown to be useful in various contexts, both within and beyond the scope of information retrieval (IR).\n\nIn the context of search and recommender systems, several recent works have utilized similar simulation-based approaches to imitate real-world system behavior. For instance, Ye et al. (2025) developed an LLM-driven simulation of the recommender ecosystem to improve the evaluation of long-term creator dynamics; Nachimovsky et al. (2025) adopted a similar approach to study the effect of heterogeneity of AI agents in different roles (publisher agent, query formulator, and ranker) on various aspects of the search ecosystem. Beyond the IR context (but still within the scope of ML), recent work by Shapira et al. (2024, 2025) utilized simulation-based approaches to generate synthetic data that mimics human behavior in sequential decision-making scenarios, and used it to predict human choice behavior in persuasion games.\n\nRelying on this extensive line of research that demonstrates the relevance of simulations (and particularly, LLM-based simulations) to real-world scenarios, we view our work as a significant step towards developing effective mechanisms for competitive search agent design, which could potentially be integrated and tested in non-simulated environments, as part of a future research effort.\nIn addition, exploring settings where user queries evolve over time is an important direction for future work. Conceptually, this follows the natural progression in the IR community - early work typically focused on single-query scenarios and only later moved to multi-query and dynamic-query settings. We believe a similar stepwise development is appropriate here: first establish and validate RLRF in the controlled, single-query / simulated-competition regime (this paper), and then extend it to increasingly realistic and temporally evolving query distributions in subsequent work. This staged approach both clarifies causal mechanisms and ensures the community can build reliable methods incrementally.\n\nReferences:\n\nNachimovsky, H., Tennenholtz, M., & Kurland, O. (2025). A Multi-Agent Perspective on Modern Information Retrieval. arXiv preprint arXiv:2502.14796.\n\nShapira, E., Madmon, O., Reichart, R., & Tennenholtz, M. (2024). Can llms replace economic choice prediction labs? the case of language-based persuasion games. arXiv preprint arXiv:2401.17435.\n\nShapira, E., Madmon, O., Apel, R., Tennenholtz, M., & Reichart, R. (2025). Human choice prediction in language-based persuasion games: Simulation-based off-policy evaluation. TACL 2025.\n\nYe, X., Xu, C., Sun, Z., Xu, J., Wang, G., Dong, Z., & Wen, J. R. LLM-empowered creator simulation for long-term evaluation of recommender systems under information asymmetry. SIGIR 2025."}}, "id": "ICLfemHSWp", "forum": "jWiWJjHtHo", "replyto": "jWiWJjHtHo", "signatures": ["ICLR.cc/2026/Conference/Submission10734/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10734/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10734/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763320861387, "cdate": 1763320861387, "tmdate": 1763320861387, "mdate": 1763320861387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of \"competitive search,\" a dynamic setting where document publishers strategically modify their content to improve their ranking on search engines. Recognizing that publishers increasingly leverage Large Language Models (LLMs) for this task, the authors introduce \"Reinforcement Learning from Ranker Feedback\" (RLRF), a novel framework for training these LLM-based publisher agents. The methodology involves training an LLM—termed an \"RL-aligned agent\" or \"RA agent\" —using preference datasets. These datasets are synthetically generated from simulated ranking competitions, thereby encoding signals from the ranker's outputs (i.e., the rankings). The agent alignment is performed at training time, notably using Direct Preference Optimization (DPO) , which allows the finalized RA agent to operate at test time via simple prompting without further optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The primary novelty is the RLRF framework itself—the conceptual reframing of alignment-RL for a competitive, game-theoretic task. The agent trained on SG learns to hit a static target. The agent trained on DG learns to win a dynamic game. For any competitive, multi-agent domain, training agents on static preference data is fundamentally insufficient. One must incorporate multi-agent simulation (like DG) into the training loop to learn the \"meta-game.\""}, "weaknesses": {"value": "1. The paper dismisses Proximal Policy Optimization (PPO) as \"less stable\"  and adopts DPO. While DPO is a strong and modern choice, this justification is brief. A more detailed explanation or a preliminary experiment comparing them would have strengthened this methodological choice.\n2. The paper's claim from Figure 2 (that RLRF improves faithfulness) is true only when controlling for model size. The paper fails to address the critical question: \"How does an 8B-RA agent compare to a 70B-NA agent?\" This weakness badly muddies the conclusions about faithfulness. It is plausible that the primary benefit of RLRF is rank promotion, and faithfulness is almost entirely a function of model scale."}, "questions": {"value": "1. Could you please elaborate on your hypothesis for the \"asymmetric\" transfer learning observed in Table 2? Why does Contriever appear to be a \"better teacher\" than E5-unsupervised? Does this imply that the choice of generation ranker is a critical, first-class component of the RLRF framework that requires its own line of research?\n2. could you provide a brief sensitivity analysis for a key parameter, such as the DPO beta? This would help confirm the \"robustness\" interpretation and allay fears that the results are specific to the chosen default."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MzJ5UgFU6K", "forum": "jWiWJjHtHo", "replyto": "jWiWJjHtHo", "signatures": ["ICLR.cc/2026/Conference/Submission10734/Reviewer_6VnK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10734/Reviewer_6VnK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136736553, "cdate": 1762136736553, "tmdate": 1762921960365, "mdate": 1762921960365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "The novelty of the paper"}, "comment": {"value": "Our work is the first, to the best of our knowledge, to align (via RL) content generation (and more generally document creation/modification) with ranking preferences induced by an undisclosed ranker while accounting for content faithfulness: a fundamental challenge in competitive search from an agent perspective. While we acknowledge that our work does not introduce algorithmic novelty, we emphasize its significant conceptual and applicative novelty across several dimensions:\n(1) Competitive search as a learning problem.\nPrior work on competitive search primarily analyzes the ranker’s perspective: how the ranking mechanism influences publishers' behavior. In contrast, we study the agent’s perspective: how LLM-based publishers can learn to generate documents under competition. To our knowledge, this is the first formulation of competitive search as a learning task, enabling systematic evaluation of RL-based alignment techniques for strategic content generation, going beyond prompt engineering as in Bardas et al. (2025) [full citation in the paper]. This perspective connects competitive search to broader ML settings involving dynamic, incentive-driven evaluation.\n(2) Search competition ranking outcomes as alignment feedback.\nWe introduce the use of an unknown ranking algorithm’s output as feedback for alignment. While self-play and RL-based fine-tuning are well-known, existing “AI feedback” setups (e.g., RLAIF) typically use AI proxies to replace human preferences. Our goal is different: to align LLMs with an unknown ranker, through rankings that depend jointly on the input query and on the model’s own competing, self-generated documents, while the downstream competition remains unobserved. This constitutes alignment to an objective defined by competitive interactions, a setting not explored in prior alignment studies.\n(3) Incentive-aware synthetic data generation.\n We decompose two core components of the task: algorithmic alignment to the ranker’s preference and strategic adaptation to the competitive landscape. We operationalize this distinction via two data generation schemes, Static Generation (SG) and Dynamic Generation (DG), showing that DG, which simulates real competition, leads to significantly higher performance gains. This finding formalizes the importance of strategic synthetic data for learning in competitive environments, extending similar insights recently observed in human-choice prediction tasks (e.g., Shapira et al., 2024 [full citation in the paper]).\n(4) In this sense, our methodology can be viewed as synthetic data generation for settings in which the agent is not only uncertain about the game itself (e.g., the ranker), but also about its opponents, the mapping from strategies (corpora) to utilities, and even the structure of the strategy space - here instantiated as free-text inputs with an unknown embedding geometry. This makes the problem substantially more challenging than standard stylized game-theoretic environments, where self-play is typically applied. Moreover, whereas self-play is generally used to iteratively improve a policy within a fixed game, our approach focuses on simulating families of synthetic games in order to produce data that trains a model to perform robustly in new, unseen strategic environments.\nLastly, while these contributions stand on their own, they also complement ongoing algorithmic work in RL-based alignment. By highlighting the unique challenges of alignment in competitive search - such as inter-agent dependencies and non-stationary feedback - they open the door to developing new alignment methods that explicitly account for strategic and dynamic environments."}}, "id": "GUcg30ShFZ", "forum": "jWiWJjHtHo", "replyto": "jWiWJjHtHo", "signatures": ["ICLR.cc/2026/Conference/Submission10734/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10734/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10734/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763321150300, "cdate": 1763321150300, "tmdate": 1763321150300, "mdate": 1763321150300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Relevance to the ICLR community"}, "comment": {"value": "We would like to emphasize the strong relevance of our work to the ICLR community. Technically, our study builds directly on core ML methodologies - LLM fine-tuning, RL-based alignment, preference optimization, and self-play - and adapts them to a new and practically important feedback regime. The proposed RLRF framework extends the family of RLHF/RLAIF approaches to settings where the feedback signal is an algorithmic ranking rather than human annotations. This introduces unique challenges, particularly the need for opponent-strategy adaptation: because agents condition on competition history and previous ranked lists during training, they must learn behaviors that remain effective under different strategic responses and player interactions. In this sense, our setting provides a natural testbed for studying alignment and optimization methods in multi-agent competitive environments.\nFrom an application standpoint, our setting of competitive search lies at the intersection of information retrieval, recommender systems, and algorithmic game theory, which are long-standing and central research areas within ML, as evidenced by the high number of papers in those fields consistently being published at top-tier AI/ML conferences like ICLR/NeurIPS/AAAI (e.g., Ben-Porat et al. 2018, 2019, Yao et al. 2023, 2024, Nachimovsky et al. 2025, Madmon et al. 2025, and more). The ability to train LLM-based agents that adapt strategically to ranking algorithms has direct implications for applied ML domains such as content optimization and multi-agent learning. We thus view our contribution as both methodologically grounded and directly relevant to active areas of ML research and practice.\n\nReferences:\n\nBen-Porat, Omer, and Moshe Tennenholtz. \"A game-theoretic approach to recommendation systems with strategic content providers.\" Advances in Neural Information Processing Systems 31 (2018).\n\nBen-Porat, Omer, Itay Rosenberg, and Moshe Tennenholtz. \"Convergence of learning dynamics in information retrieval games.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.\n\nOmer Madmon, Idan Pipano, Itamar Reinman, and Moshe Tennenholtz. On the convergence of no-regret dynamics in information retrieval games with proportional ranking functions. In The Thirteenth International Conference on Learning Representations, 2025.\n\nHaya Nachimovsky and Moshe Tennenholtz. On the power of strategic corpus enrichment in content creation games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 14019–14026, 2025.\n\nYao, F., Li, C., Nekipelov, D., Wang, H., & Xu, H. (2023). How Bad is Top-$ K $ Recommendation under Competing Content Creators?. In International Conference on Machine Learning (pp. 39674-39701). PMLR.\n\nYao, F., Liao, Y., Liu, J., Nie, S., Wang, Q., Xu, H., & Wang, H. (2024). Unveiling user satisfaction and creator productivity trade-offs in recommendation platforms. Advances in Neural Information Processing Systems, 37, 86958-86984."}}, "id": "KExPdbVjjh", "forum": "jWiWJjHtHo", "replyto": "jWiWJjHtHo", "signatures": ["ICLR.cc/2026/Conference/Submission10734/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10734/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission10734/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763321393461, "cdate": 1763321393461, "tmdate": 1763321393461, "mdate": 1763321393461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}