{"id": "x9OPC7j6hh", "number": 16612, "cdate": 1758266727305, "mdate": 1759897229680, "content": {"title": "Low-Resource Finetuning for Hallucination Mitigation in Language Models", "abstract": "Hallucinations in Large Language Models (LLMs) pose a significant challenge to their reliable deployment across domains, arising inherently from their design as statistical models that maximize next-token prediction probability based on training data. While methods such as LettuceDetect, RAG-HAT, and prompting techniques have demonstrated efficacy in hallucination detection and mitigation within Retrieval-Augmented Generation (RAG) frameworks, limitations persist. To address these, we propose a novel low-resource hallucination mitigation pipeline that fine-tunes LLMs on synthetic dataset using feedback from LettuceDetect. Our approach reduces hallucination rates in open-source small language models, as validated through evaluations on RAGTruth and PILE-10K benchmarks. We further discuss the pipeline’s extensibility to domain-specific applications.", "tldr": "Novel hallucination mitigation fine-tuning pipeline on synthetic dataset using feedback from LettuceDetect.", "keywords": ["LLM", "hallucination", "mitigation", "fine-tuning", "LettuceDetect"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a86dbf69cccf07d757370a0b1bb6adcbb9ed32c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work, the authors propose a fine-tuning method aimed at reducing hallucinations in small language models. They create a synthetic fine-tuning dataset using a three-step approach: (1) generating 20K nouns with Gemma-3-4b-it, (2) generating 5–15 related attributes for each noun, and (3) producing a text for each noun–attribute pair, resulting in approximately 180K samples. Several small language models are then fine-tuned on this dataset.\n\nDuring training, a hallucination detector (lettucedetect-large-v1) is used to classify each output token as either a hallucination or non-hallucination. Tokens identified as hallucinations are penalized, while non-hallucinated tokens are rewarded. The models, both before and after fine-tuning, are evaluated on the PILE-10K and RAGTruth datasets to measure perplexity and non-hallucination rate, respectively.\n\nThe evaluation results show that while the fine-tuned models exhibit slightly worse perplexity, they achieve a higher non-hallucination rate—indicating reduced hallucination compared to the base models. The authors also find that LoRA fine-tuning outperforms both QLoRA and full fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe paper tackles an important and highly relevant problem: mitigating hallucinations in decoder-only language models.\n\n•\tThe proposed fine-tuning approach demonstrates consistent improvements in reducing hallucinations across all evaluated models.\n\n•\tThe authors include a diverse set of small language models from different families and sizes, which strengthens the generalizability of the findings.\n\n•\tThe evaluation considers two complementary metrics—perplexity and non-hallucination rate—providing a balanced view of model quality and factual reliability."}, "weaknesses": {"value": "•\tWhile the paper reports hallucination rates, it does not include any task-specific evaluations. Including such experiments could help demonstrate whether the reduced hallucination rate also translates to improved or stable downstream task performance.\n\n•\tAlthough Section 1.3 lists several fine-tuning-based mitigation methods, a direct comparison with one or two of these techniques would make the empirical contribution stronger.\n\n•\tIt seems that some hyperparameters may have been adjusted using the test set, which could lead to optimistic estimates of performance. Clarifying or separating validation and test data would improve the rigor of the evaluation.\n\n•\tBecause the same hallucination detector is used for both training and evaluation, improvements on this metric may partially reflect training bias. Using an additional, independent detector (even if less powerful) could help validate the robustness of the results.\n\n•\tThe results show that while hallucination decreases, perplexity increases somewhat (e.g., from 17 to 21 for Gemma-3-4b-it). It might be helpful to discuss this trade-off and potential strategies to balance factual accuracy and fluency.\n\n•\tThe related work section is quite detailed and informative but could be made more concise. The saved space could be used to strengthen the empirical section—for example, by including comparisons with other mitigation techniques, visualizing sample fine-tuning data, or showing token-level hallucination rates before and after fine-tuning."}, "questions": {"value": "Q1. Clarity on “Low-Resource” Terminology\nThe title mentions “low-resource,” which can be confusing since the experiments are conducted in English—a high-resource language. Could the authors clarify in what sense the setting is considered “low-resource”? For example, does it refer to limited model size, limited fine-tuning data, or computational constraints rather than language resource availability?\n\nQ2. Comparison with Existing Mitigation Techniques\nThe paper introduces an interesting fine-tuning-based approach for hallucination mitigation, but it does not include comparisons with existing methods. Could the authors elaborate on this decision? Were existing techniques difficult to reproduce or incompatible with the proposed setup?\n\nQ3. Evaluation on Downstream Tasks\nIt would be helpful to understand how the fine-tuned models perform on downstream tasks (e.g., summarization or question answering). Have the authors considered evaluating their models on such tasks to assess whether hallucination reduction affects task-specific performance?\n\nQ4. Hyperparameter Optimization Details\nThe paper does not clearly specify how hyperparameters were tuned. Could the authors clarify which dataset was used for this purpose? From the current description, it seems that hyperparameters may have been selected based on the RAGTruth test set, which could lead to overfitting. If that’s not the case, additional details on the validation procedure would be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XpVBhvceHq", "forum": "x9OPC7j6hh", "replyto": "x9OPC7j6hh", "signatures": ["ICLR.cc/2026/Conference/Submission16612/Reviewer_rLs6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16612/Reviewer_rLs6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702292045, "cdate": 1761702292045, "tmdate": 1762926683239, "mdate": 1762926683239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a low-resource fine-tuning pipeline for hallucination mitigation in large language models. The method uses LettuceDetect, a pre-trained hallucination detection model, as both a weak supervisor and evaluator. The authors generate a synthetic dataset of noun–attribute pairs using Gemma-3-4B, then fine-tune several small open-source models, including LLaMA-3-8B-Instruct, Qwen3-1.7B, and Gemma variants. The training objective penalizes tokens identified as hallucinated by LettuceDetect and rewards non-hallucinatory tokens, using a Jump ReLU-based loss. Evaluation is conducted on RAGTruth (for hallucination rate) and PILE-10K (for perplexity). The results show modest improvements in non-hallucinatory rate (around 5–8%) with minimal changes in perplexity. The authors claim the approach is architecture-agnostic, computationally efficient, and suitable for small models under resource constraints."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The topic of hallucination mitigation in language models is relevant and practically important, especially for low-resource or on-device applications.  \n2. The proposed pipeline is simple, computationally light, and architecture-independent, making it applicable to smaller open models without retraining large detectors.  \n3. The use of a weak supervision setup is a pragmatic approach to avoid heavy human annotation.  \n4. The goal of integrating hallucination control into fine-tuning rather than prompting or retrieval methods is conceptually reasonable."}, "weaknesses": {"value": "1. The novelty of the work is minimal. The method merely reuses LettuceDetect as a weak labeler, without introducing new learning objectives, data strategies, or theoretical insights.  \n2. The experimental validation is insufficient. The model is evaluated on only two small datasets, and the metrics are derived from the same LettuceDetect model used for training, introducing circularity.  \n3. The writing and structure are poor, and unclear narrative flow, which makes it difficult to follow.  \n4. The paper contains no figures, diagrams, or visual explanations of the proposed pipeline, which severely limits clarity.  \n5. The synthetic dataset is overly simplistic and unrelated to the evaluation tasks, weakening the relevance of the training setup.  \n6. There is no human evaluation or comparison to other fine-tuning strategies like F2, HIPO, or preference optimization methods.  \n7. The mathematical section offers no real theoretical contribution and introduces unnecessary notation for a simple loss function.  \n8. The use of LettuceDetect as both training signal and evaluator risks overfitting to detector-specific biases rather than improving genuine factual reliability."}, "questions": {"value": "1. Provide independent evaluation using either human annotation or alternative hallucination metrics to avoid circular reasoning.  \n2. Include visual diagrams of the training pipeline, loss computation, and evaluation flow to improve readability.  \n3. Add qualitative examples comparing pre- and post-fine-tuning responses to illustrate concrete behavioral changes.  \n4. Perform ablation studies to separate the effects of LettuceDetect supervision, loss configuration, and fine-tuning strategy.  \n5. Analyze efficiency in terms of training time, memory usage, and scalability across model sizes.  \n6. Reorganize and clean the manuscript for clarity, consistent formatting, and concise writing.  \n7. Compare quantitatively against recent fine-tuning and preference optimization approaches to position the contribution properly.  \n8. Discuss the limitations of using a noisy weak supervisor and suggest potential methods to mitigate detector bias.  \n9. Given the small technical novelty and weak empirical results, the paper would be more suitable as a workshop or system report rather than a full ICLR submission."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "adZsopBB0m", "forum": "x9OPC7j6hh", "replyto": "x9OPC7j6hh", "signatures": ["ICLR.cc/2026/Conference/Submission16612/Reviewer_y69o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16612/Reviewer_y69o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753897105, "cdate": 1761753897105, "tmdate": 1762926682708, "mdate": 1762926682708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a low-resource fine-tuning pipeline that uses a token-level hallucination detector (LettuceDetect) as both weak supervisor and evaluator. The loss penalizes tokens the detector flags and lightly rewards tokens considered grounded, with LoRA, QLoRA, and full fine-tuning variants. Training uses about 180k synthetic noun-attribute texts and evaluation reports Non-Hallucinatory Rate on RAGTruth plus perplexity on PILE-10K. Reported gains in NHR are clearest for smaller models, while perplexity generally increases after fine-tuning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Nice idea and approach - Token-level supervision for hallucinations is a sensible way to train model with dense signals and could be architecture-agnostic and cheap to run.  \n\n2. Training dataset is simple - The noun-attribute generator is straightforward and scales, which makes the recipe easy to reproduce."}, "weaknesses": {"value": "1. Flawed evaluation - The same detector is used both for supervision and evaluation. The detector’s own precision and recall on RAGTruth are modest, which raises the risk that the model learns the judge rather than factuality. Human checks are described only at a high level, without any details that would establish reliability. Consider adding additional evaluations such as FAVA-Bench [1] or FactScore [2] to distinguish overfitting from generalization.  \n\n2. Lack of baselines - Results compare only base vs fine-tuned models. This problem is widely studied and has established previously proposed alternatives. At minimum include SFT on the same data with standard cross-entropy, refusal tuning [3], “corrected data” training, and simpler token-weighting or DPO variants to test whether the detector-guided loss is essential. Effects of data size are also important to study. The paper itself mentions many prior work on finetuning models for addressing hallucination which should be considered as baselines \n\n3. Flawed perplexity findings - The paper’s narrative suggests mixed perplexity effects, yet Table 1 shows perplexity increases for all listed models after fine-tuning, which implies degradation in general language modeling. Additional evaluations for broader capability checks such as MMLU or GLUE need to be done ensure core skills are not harmed.  \n\nReferences \n[1] FAVA-Bench - Mishra, A., Zhou, Y., Wang, S., et al. Fine-grained Hallucination Detection and Editing for Large Language Models. arXiv:2401.06855, 2024. \n\n[2] FActScore - Min, S., Krishna, K., Lyu, X., et al. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long-Form Text Generation. EMNLP 2023 (Main), 2023. ACL Anthology+1 \n\n[3] Refusal tuning - Zhang, H., Diao, S., Lin, Y., et al. R-Tuning: Instructing Large Language Models to Say “I Don’t Know”. NAACL 2024 (Long), 2024."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aSVJpIBFbY", "forum": "x9OPC7j6hh", "replyto": "x9OPC7j6hh", "signatures": ["ICLR.cc/2026/Conference/Submission16612/Reviewer_iefr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16612/Reviewer_iefr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974825680, "cdate": 1761974825680, "tmdate": 1762926681861, "mdate": 1762926681861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a low-resource finetuning pipeline designed to mitigate hallucinations in Small Language Models (SLMs), particularly within Retrieval-Augmented Generation (RAG) contexts. Unlike existing methods that often rely on large, expensive \"judge\" models like GPT-4 (e.g., RAG-HAT), this work employs a lightweight, open-source hallucination detector, LettuceDetect, as a \"weak supervisor\"."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The pipeline is resource-efficient and practical. By avoiding reliance on large, closed-source models (like GPT-4) or complex optimization methods (like DPO), it provides an accessible pathway for improving SLM factuality, especially for on-device applications.\nThis is possible by introducing a loss function (JReLU) specifically tailored to the task of token-level hallucination mitigation. This allows for direct optimization against the detected undesirable behavior, rather than just mimicking a style. (unlike DPO)\n\nThe method demonstrates strong empirical results when combined with LoRA. It successfully improves the Non-Hallucinatory Rate (NHR) for several SLMs, showing its potential utility."}, "weaknesses": {"value": "The proposed training pipeline appears to be incomplete and fundamentally flawed. Its success is entirely dependent on the implicit regularization of LoRA. The experiments clearly show that when using Full-Finetuning, the method fails and even degrades model performance (Table 2). This indicates the pipeline, on its own, is unstable.\n\nThe evaluation is very narrow. It only reports the target metric (NHR on RAGTruth) and a general language metric (Perplexity on PILE-10K). There is no evaluation on standard downstream benchmarks (e.g., NQ, or standard open-ended QA tasks). This makes it impossible to assess if the finetuning has catastrophically damaged the model's general reasoning and knowledge capabilities, which is a critical concern given the observed Perplexity degradation.\n\nThe paper states that the reward hyperparameter R for non-hallucinatory tokens is \"crucial\", yet it fails to provide a principled methodology for its selection. The optimal choice of R=1e-5 appears to be the result of a simple grid search and not discussed properly."}, "questions": {"value": "* Given that the pipeline fails under full finetuning, does the paper suggest that the core contribution is not the pipeline itself but rather the finding that this JReLU loss function only works when combined with a strong implicit regularizer like LoRA?\n\n* A clear trade-off between NHR improvement and Perplexity degradation is shown. From a practical standpoint, do you believe this degradation in fundamental language capability is an acceptable price to pay for the observed NHR gains?\n\n* Why did you not report performance on standard downstream benchmarks (e.g., open-ended QA tasks)?\n\n* Why was the JReLU threshold $\\tau$ arbitrarily fixed at 0.5, rather than being treated as a critical hyperparameter to be calibrated on a validation set? Given that similar thresholds are pivotal for performance in many works (e.g., conformal/selective prediction and uncertainty quantification), this \"natural\" choice seems unprincipled and suboptimal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sLILh3wrSf", "forum": "x9OPC7j6hh", "replyto": "x9OPC7j6hh", "signatures": ["ICLR.cc/2026/Conference/Submission16612/Reviewer_bA6B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16612/Reviewer_bA6B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006772894, "cdate": 1762006772894, "tmdate": 1762926681129, "mdate": 1762926681129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}