{"id": "Fbe7OnbkAn", "number": 4555, "cdate": 1757706233380, "mdate": 1759898026876, "content": {"title": "InvThink: Towards AI Safety via Inverse Reasoning", "abstract": "We present **InvThink**, a simple yet powerful approach that gives large language models (LLMs) the capability of inverse thinking: reasoning through failure modes before generating responses. Unlike existing safety alignment methods that optimize directly for safe response, InvThink instructs models to 1) enumerate potential harms, 2) analyze their consequences, and 3) generate safe outputs that proactively avoid these risks. Our method reveals three key findings: (i) safety improvements show stronger scaling with model size compared to existing safety methods. (ii) InvThink mitigates safety tax; by training models to systematically consider failure modes, it preserves general reasoning capabilities on standard benchmarks. (iii) beyond general safety tasks, InvThink excels in high-stakes domains including external-facing (medicine, finance, law) and agentic (blackmail, murder) risk scenarios, achieving up to 15.7\\% reduction in harmful responses compared to baseline methods like SafetyPrompt. We further implement InvThink via supervised fine-tuning, and reinforcement learning across three LLM families. These results suggest that inverse reasoning provides a scalable and generalizable path toward safer, more capable language models. Project Page: https://invthink.github.io/", "tldr": "We present InvThink, a simple yet powerful approach that gives large language models (LLMs) the capability of inverse thinking.", "keywords": ["AI Safety", "Inverse Thinking", "LLM"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/215ac1a78a0fee442d97a50400be80cc3238ff51.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a safety post training method leveraging SFT and RL to distill the inverse reasoning capability about potential harmful consequences from a teacher model into smaller student models (Gemma and Qwen). This method outperforms other baseline alignment approaches on 3 safety benchmarks, while preserving (and sometimes even augmenting) the student model's reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, I think this is a strong paper with a clear motivation, valid approach, and relatively strong evaluation. Using inverse harm reasoning to augment model reasoning capability is a conceptually interesting approach, particularly when it doesn't seem to harm the model's general reasoning. The paper is very well written - particularly, the method section is clear."}, "weaknesses": {"value": "1. Missing related work: (1) SafetyAnalyst (https://arxiv.org/pdf/2410.16665) and (2) RATIONAL (https://arxiv.org/pdf/2503.05021). Please discuss in Related Work or Introduction the relatedness of the present work with these papers and highlight how it differentiates from them.\n\n2. The claim that InvThink \"leverages model capacity more effectively\" is not convincing based on Figure 3. Although InvThink has the highest safety score among all methods, its curves over model size for Qwen models do not look qualitatively different from other methods, although the improvements are more steep for Gemma. Even in Figure 5, it's hard to draw this conclusion only by observing the plots, as the trends may be driven by specific data points which could be outliers. Please provide concrete statistical evidence to support this claim or revise it to reflect the data better.\n\n3. More discussion on the cost of InvThink will help contextualize the performance benefits (SFT + RL seems costly, especially compared to SafetyPrompt)."}, "questions": {"value": "1. Line 210: why was the OpenAI moderation API chosen as the safety reward model? There are other safety models that have been shown to outperform it, such as WildGuard (Table 3 in https://proceedings.neurips.cc/paper_files/paper/2024/file/0f69b4b96a46f284b726fbd70f74fb3b-Paper-Datasets_and_Benchmarks_Track.pdf). \n\n2. According to Table 1, adversarial brainstorming is what distinguishes InvThink from RevThink (please provide a citation for RevThink). Have you done an ablation study to show how effective \"adversarial brainstorming\" is in addition to backward reasoning?\n\n3. Line 262: how did you arrive at the number 20% (instead of e.g. 30%)?\n\n4. Table 2 doesn't contain all the models listed in Lines 264-269. What are the results for the rest of the models?\n\n5. In Table 2, what is the difference between InvThink and InvThink SFT (or InvThink SFT+RL)?\n\n6. Why was RL chosen over SFT to train models to produce the output?\n\n7. For Figure 4, the results look noisy and a bit challenging to interpret. Consider providing some statistical analyses to show if there is an optimal number of reasoning routes for Qwen models regardless of size."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0ScC3DQr8o", "forum": "Fbe7OnbkAn", "replyto": "Fbe7OnbkAn", "signatures": ["ICLR.cc/2026/Conference/Submission4555/Reviewer_uDgr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4555/Reviewer_uDgr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761500584541, "cdate": 1761500584541, "tmdate": 1762917437144, "mdate": 1762917437144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces inverse reasoning to enhance the training data and address safety in LLM alignment. Both forward reasoning and inverse reasoning are combined with the original prompts. A training framework has been proposed to fine-tune the model with augmented data. The training schema can be combined with SFT and RL. Experiments have been conducted on various benchmarks and models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is novel to introduce inverse reasoning to address safety in LLM. \n2. The proposed method is simple and easy to understand. \n3. Experiment quality is convincing compared with several baselines. Code has been released."}, "weaknesses": {"value": "Novelty: Though inverse reasoning has not been used in LLM training to the author's best knowledge, it has been extensively used in the RL literature, which limited the novelty a little bit.\n\nTechnology depth: From the reviewer's view, there is not much technology depth in the paper. It is basically an augmented dataset. Most importantly, ablation studies have not been conducted as described in the limitation paragraph. It is unclear which part is working.\n\nMotivation: It is unclear why the proposed method can only be used to address safety. It looks like a method that can be used to improve general reasoning. The unique relation between safety (not others) and inverse reasoning has not been analyzed.\n\nPractice: The quality of augmented data heavily depends on the teacher model. Choosing which model to use as the teacher could be another issue. Another problem is that there is no supermodel right now that can serve as a good teacher. There are several hyperparameters in Eq. (7). Tuning these parameters could be complicated.\n\nExperiment: Comparing with DPO and Tree of Thoughts (ToT) is necessary, as CoT is outdated.\nThe trade-off between safety and utility has not been investigated. The effects of hyperparameters in Eq. (7) have not been investigated."}, "questions": {"value": "Please refer to Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UeIw0cyAhQ", "forum": "Fbe7OnbkAn", "replyto": "Fbe7OnbkAn", "signatures": ["ICLR.cc/2026/Conference/Submission4555/Reviewer_JkAD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4555/Reviewer_JkAD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860339932, "cdate": 1761860339932, "tmdate": 1762917436923, "mdate": 1762917436923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors address the safety challenges of LLMs and introduce INVTHINK, which is a simple yet powerful approach that equips models with inverse thinking, i.e., reasoning through potential failure modes before generating responses. INVTHINK guides models to: (1) enumerate potential harms, (2) analyze their consequences, and (3) produce safe outputs that proactively avoid these risks. The approach is implemented in three stages: (1) dataset curation, (2) supervised fine-tuning, and (3) reinforcement learning. The authors evaluate INVTHINK across models from different series, showing that it outperforms baselines on safety metrics while maintaining general reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Interesting topic: The studied problem of LLM safety is interesting and important for the broader real-world applications of LLMs.\n\n(2) Well-written paper: The paper is well written and easy to follow. The authors clearly present their ideas and experimental results, supported by effective visualizations and illustrations.\n\n(3) Extensive model evaluation: In the experiments, the authors evaluate their method across Gemma, Qwen3, and Qwen2.5 models with varying sizes, demonstrating good compatibility across different model families."}, "weaknesses": {"value": "(1) Missing baseline: In the experiments, the authors omit an important baseline: General SFT + RL. Without results for this baseline, the effectiveness of the proposed inverse and forward reasoning behaviors remains questionable.\n\n(2) Limited teacher model selection: The authors only use Gemini-2.5 as the teacher model, which raises concerns about the generalizability of the proposed method. Although this limitation is acknowledged in the discussion section, I recommend including experimental results with additional teacher models to further mitigate this concern.\n\n(3) Additional concerns: For further comments, please refer to the question section."}, "questions": {"value": "(1) RL data portion and selection:\nIn the experimental setup, you mention that RL uses 20% of the training dataset to avoid unintended over-alignment with safety signals. Do you have any ablation results on the proportion of data used? Additionally, how is the subset of data selected for RL training?\n\n(2) Missing important baselines:\nCan you provide results for a baseline that uses General SFT + RL under the same RL setting as your method? Including this baseline would better highlight the effectiveness of the augmented inverse and forward reasoning data.\n\n(3) Optimal Routing Complexity analysis:\nIn the Optimal Routing Complexity experiments, for smaller models, increasing the number of paths from 1 to 2 and from 1 to 4 results in performance degradation. Does this imply that inverse reasoning is ineffective for 0.5B models? In practice, how did you determine the number of paths used for different models in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mHAHhs3fPV", "forum": "Fbe7OnbkAn", "replyto": "Fbe7OnbkAn", "signatures": ["ICLR.cc/2026/Conference/Submission4555/Reviewer_QUPy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4555/Reviewer_QUPy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969473830, "cdate": 1761969473830, "tmdate": 1762917436499, "mdate": 1762917436499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces INVTHINK, a safety alignment framework for LLMs that introduces “inverse reasoning” into the generation process. Instead of directly optimizing for safe outputs, INVTHINK trains models to enumerate potential harms, analyze their consequences, then generate safe responses. The process starts with data augmentation, then uses supervised fine tuning, and finally RL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper draws its inspiration from FMEA, making it well rooted in safety engineering work. \n- The used pipeline is a standard safety pipeline with some tweaks, which makes it easily adoptable."}, "weaknesses": {"value": "- Requires multi-phase training (SFT + RL), which is resource-intensive, meanwhile a lot of work is aiming for inference-time steering approaches. \n- Inverse reasoning adds tokens, potentially increasing inference time."}, "questions": {"value": "- Since evaluation relies on LLM‑as‑judge, how did you control for evaluator bias?\n- What is the end‑to‑end training cost and inference overhead ?\n- Why is Gemini Pro chosen as a teacher model and how does that impact the performance on the Gemini model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UOFXQ5Qtcd", "forum": "Fbe7OnbkAn", "replyto": "Fbe7OnbkAn", "signatures": ["ICLR.cc/2026/Conference/Submission4555/Reviewer_6JP3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4555/Reviewer_6JP3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028144908, "cdate": 1762028144908, "tmdate": 1762917435372, "mdate": 1762917435372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}