{"id": "Ym33xJYINV", "number": 13639, "cdate": 1758220290713, "mdate": 1759897423047, "content": {"title": "Pretraining Scaling Laws for Generative Evaluations of Language Models", "abstract": "Neural scaling laws have played a central role in modern machine learning, driving the field's ever-expanding scaling of parameters, data and compute.\nWhile much research has gone into fitting scaling laws and predicting performance on pretraining losses and on _discriminative_ evaluations such as multiple-choice question-answering benchmarks, comparatively little research has been done on fitting scaling laws and predicting performance on _generative_ evaluations such as mathematical problem-solving or coding.\nIn this work, we propose and evaluate three different pretraining scaling laws for fitting pass-at-$k$ on generative evaluations and for predicting pass-at-$k$ of the most expensive model using the performance of cheaper models.\nOur three scaling laws differ in the covariates used: (1) pretraining compute, (2) model parameters and pretraining tokens, (3) log likelihoods of gold reference solutions.\nWe make four main contributions:\nFirst, we show how generative evaluations offer new hyperparameters (in our setting, $k$) that researchers can use to control the scaling laws parameters and the predictability of performance.\nSecond, in terms of scaling law parameters, we find that the compute scaling law and parameters+tokens scaling law stabilize for the last $\\mathord{\\sim}1.5{-}2.5$ orders of magnitude, whereas the gold reference likelihood scaling law stabilizes for the last $\\mathord{\\sim}5$ orders of magnitude.\nThird, in terms of predictive performance, we find all three scaling laws perform comparably, although the compute scaling law predicts slightly worse for small $k$ and the log likelihoods of gold reference solutions predicts slightly worse for large $k$.\nFourth, we establish a theoretical connection that the compute scaling law emerges as the compute-optimal envelope of the parameters+tokens scaling law.\nOur framework provides researchers and practitioners with insights and methodologies to forecast generative performance, accelerating progress toward models that can reason, solve, and create.", "tldr": "Scaling laws for generative evals of language models during pretraining", "keywords": ["language models", "large language models", "scaling laws", "evaluations", "generative evaluations", "sampling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff94ff342d8b0fd082dd75e0e4ed9e85699accc7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies scaling laws for generative evaluations, which is a relatively unexplored area compared to pretraining loss or discriminative benchmarks. The authors propose three types of scaling laws that relate model performance (measured by pass@k) to:\n1. Pretraining compute\n2. Model parameters and pretraining tokens\n3. Log-likelihoods of gold reference solutions\n\nThey empirically evaluate these laws on GSM8K using Pythia models (14M-12B parameters, 300B tokens), showing that: All three scaling laws achieve comparable predictive performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper expands scaling law studies beyond pretraining or discriminative settings to generative evaluations, which are highly relevant for coding, reasoning, and mathematical tasks.\n2. The authors clearly define three scaling formulations, use consistent benchmarks (GSM8K), and employ backtesting to rigorously evaluate predictive ability.\n3. The results show stable behavior over large dynamic ranges, and the study identifies when extrapolation becomes unreliable"}, "weaknesses": {"value": "1. The writing quality of the paper could be improved. For example, Figures 3 and 5 omit certain points, and the legend partially obscures the plotted lines.\n2. All experiments are conducted on GSM8K using Pythia checkpoints\n3. The paper focuses only on pass@k with temperature sampling = 1.0. Other decoding strategies (top-p, nucleus sampling, self-consistency) might influence scaling behavior.\n4. The results are derived solely from internal checkpoints of a single architecture. Evaluating additional model families such as LLaMA or Qwen could help assess the robustness of the proposed scaling laws. Moreover, since Pythia is a relatively dated model, it may not be well suited for evaluating mathematical reasoning capabilities.\n5. I believe that pass@k can be viewed as a variant of accuracy. Could the authors clarify in detail how this work differs from Language Models Scale Reliably with Over-Training and on Downstream Tasks https://arxiv.org/abs/2403.08540."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ALvm8Pdhpt", "forum": "Ym33xJYINV", "replyto": "Ym33xJYINV", "signatures": ["ICLR.cc/2026/Conference/Submission13639/Reviewer_XVLF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13639/Reviewer_XVLF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760931353795, "cdate": 1760931353795, "tmdate": 1762924217053, "mdate": 1762924217053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates pretraining scaling laws for generative evaluations of language models and corresponding metrics such as pass@k on verifiable tasks. The author introduces a new parameter to the scaling law and identify gold reference likelihood to be a better proxy metric for stabilizing scaling laws."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is clearly written and contains the necessary details for reproducibility.\n2. The direct comparison of different types of scaling laws answers the question of how to develop scaling laws for generative evaluations well. \n3. The analysis of including k as a parameter and its role in the scaling law is clear. \n4. The finding of the stability of the gold reference likelihood scaling law is a strong result and well founded, and explained."}, "weaknesses": {"value": "1. The scaling law is developed on a single downstream task. While Pythia is an older generation model, where other tasks might be too difficult to obtain meaningful accuracy, why not average more tasks where Pythia could obtain a meaningful accuracy on, and expand beyond math?\n\n2. What is the practical implication of the penalty factor derived in Section 6? What does it look like for the studied model checkpoints?"}, "questions": {"value": "Please see my comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jawHydDHIk", "forum": "Ym33xJYINV", "replyto": "Ym33xJYINV", "signatures": ["ICLR.cc/2026/Conference/Submission13639/Reviewer_Cuov"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13639/Reviewer_Cuov"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796319073, "cdate": 1761796319073, "tmdate": 1762924216570, "mdate": 1762924216570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the study of neural scaling laws from pretraining losses and discriminative benchmarks to generative evaluations, tasks where models are assessed by their ability to produce correct outputs such as math solutions or code completions.\nThe authors propose and compare three scaling laws for predicting pass@k performance across pretrained checkpoints: (1) A Compute-based scaling law, relating log(pass@k) to pretraining FLOPs; (2) A Parameters + Tokens law, decomposing compute into model size N and dataset size D; (3) A Gold Reference Likelihood law, regressing pass@k against the log-likelihoods of reference (“gold”) solutions. \nThrough backtesting on the GSM8K benchmark using Pythia checkpoints, they show that all three scaling laws achieve comparable predictive accuracy, but the gold-likelihood-based law is notably more stable, maintaining parameter convergence over up to five orders of magnitude in compute.\nFinally, the authors establish a theoretical connection showing that the compute scaling law emerges as the compute-optimal envelope of the parameters+tokens law, explaining how optimal allocation of compute between parameters and data induces the observed power-law behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is the first (as far as I know) to systematically analyze generative scaling laws for pass@k-based evaluations, extending the classical scaling law framework beyond pretraining loss or discriminative accuracy.\n2. Evaluating three distinct covariates (compute, parameters+tokens, and gold log-likelihoods) under a unified methodology is conceptually clean and methodologically rigorous.\n3. The derivation showing that the compute law is the compute-optimal envelope of the parameters+tokens law offers a theoretical explanation of empirical scaling behaviors and clarifies the relationship between prior empirical findings."}, "weaknesses": {"value": "1. The study is restricted to a single model family (Pythia) and one generative benchmark (GSM8K).\nThe conclusions, especially regarding the stability of the gold-likelihood law, should be validated on more diverse tasks (e.g., code generation, open-ended reasoning).\n2. While figures show relative stability, a numerical summary (e.g., mean relative error across backtests) would help quantify the magnitude of differences between the three scaling laws."}, "questions": {"value": "It would be interesting to test whether the fitted α(k) exponents are consistent across different generative benchmarks or model families."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "95jGfjEZxi", "forum": "Ym33xJYINV", "replyto": "Ym33xJYINV", "signatures": ["ICLR.cc/2026/Conference/Submission13639/Reviewer_GH31"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13639/Reviewer_GH31"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121907538, "cdate": 1762121907538, "tmdate": 1762924215781, "mdate": 1762924215781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}