{"id": "96DTvuYq4h", "number": 15787, "cdate": 1758255250843, "mdate": 1763710542377, "content": {"title": "Uncertainty-Aware 3D Reconstruction for Dynamic Underwater Scenes", "abstract": "Underwater 3D reconstruction remains challenging due to the intricate interplay between light scattering and environment dynamics. While existing methods yield plausible reconstruction with rigid scene assumptions, they struggle to capture temporal dynamics and remain sensitive to observation noise. In this work, we propose an Uncertainty-aware Dynamic Field (UDF) that jointly represents underwater structure and view-dependent medium over time. A canonical underwater representation is initialized using a set of 3D Gaussians embedded in a volumetric medium field. Then we map this representation into a 4D neural voxel space and encode spatial-temporal features by querying the voxels. Based on these features, a deformation network and a medium offset network are proposed to model transformations of Gaussians and time-conditioned updates to medium properties, respectively. To address input-dependent noise, we model per-pixel uncertainty guided by surface-view radiance ambiguity and inter-frame scene flow inconsistency. This uncertainty is incorporated into the rendering loss to suppress the noise from low-confidence observations during training. Experiments on both controlled and in-the-wild underwater datasets demonstrate our method achieves both high-quality reconstruction and novel view synthesis. Our code will be released.", "tldr": "", "keywords": ["Underwater Reconstruction", "Dynamic Reconstruction"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98e958b69dc8ff4cad60a7718698fbff627e0b8d.pdf", "supplementary_material": "/attachment/31ee34b46297882063f7f9efe8dddcae1ab6a5c3.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces UDF (Uncertainty-aware Dynamic Field), a novel framework for underwater 3D reconstruction that models dynamic geometry and medium properties while incorporating uncertainty. The framework builds on 3DGS and targets underwater scenes by \n1) Initializing a set of 3D Gaussians embedded in a volumetric medium. \n2) Encoding spatial-temporal features in a 4D neural voxel space via planar factorization.\n3) Using a deformation network to model dynamic geometry and a medium offset network to capture evolving medium properties \n4) Incorporating uncertainty into the rendering loss guided by surface-view radiance ambiguity and inter-frame flow inconsistency. \n\nThe authors evaluate the effectiveness of their method in multiple underwater datasets and showcase a significant improvement over prior methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is very well written and the authors provide deep insights in their design choices. \n1) The integration of dynamic medium modeling with uncertainty-aware rendering is a substantial advancement over prior work \n2) The radiance ambiguity and flow inconsistency are physically motivated and well-integrated into the loss function\n2) Strong results across all datasets \n3) The method achieves a good balance between rendering speed and memory usage."}, "weaknesses": {"value": "1) In the experimental evaluation, the method improves over all metrics. The only metric that watersplatting outperforms the current is in SSIM. Can you comment on why that might happen?\n\n2) While the paper is technically rich, some sections are very dense in information. Even figure 1. Is very technically dense. The paper could benefit from additional diagrams and moving details to supplementary material."}, "questions": {"value": "As noted in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JqZNue0f4i", "forum": "96DTvuYq4h", "replyto": "96DTvuYq4h", "signatures": ["ICLR.cc/2026/Conference/Submission15787/Reviewer_9Jh1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15787/Reviewer_9Jh1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761529777354, "cdate": 1761529777354, "tmdate": 1762926020528, "mdate": 1762926020528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UDF, a dynamic scene representation that jointly models underwater structure (via 3D Gaussian primitives) and the participating medium (via a neural, view-conditioned medium field). A canonical scene (Gaussians embedded in a volumetric medium) is mapped into a 4D neural voxel space (K-Planes) to extract spatio-temporal features. Two heads then evolve the scene over time: a deformation network that predicts per-Gaussian offsets, and a medium-offset network that updates attenuation/backscatter conditioned on motion cues. The paper introduces a heteroscedastic rendering loss with per-pixel variance derived from surface-view radiance ambiguity and inter-frame flow inconsistency. Experiments on NUSR, DRUVA, and SeaThru show gains in PSNR/SSIM/LPIPS with qualitative  visualizations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The pipeline overview is clear and visual results further aid understanding. These choices make the method and evidence easy to follow.\n\n2. The combination of 3DGS geometry + learnable medium + 4D K-Planes provides a clean factorization of structure vs. medium over time. The two cues (surface-view ambiguity and flow-based inconsistency) are well-grounded and integrated in the NLL loss.\n\n2. The paper reports consistent improvements on NUSR, DRUVA, and SeaThru with both quantitative and qualitative evidence (novel views, medium-free renderings, depth maps)."}, "weaknesses": {"value": "1. Eq. (6) uses a single σ_med inside T_med(s), yet later the paper separates σ_med into σ_att (structure) and σ_bs (backscatter). The derivation would benefit from writing the explicit separated transmittance and emission terms to avoid ambiguity, and clarifying wavelength-dependent parameterization (RGB-channel σ_med).\n\n2. Consider adding a teaser in the introduction, e.g., a before/after comparison on a challenging underwater scene, to foreground the key difficulties and to show how your method addresses them. Without this, your advantages are not immediately clear to readers.\n\n3. User study details. The 40-person, 1,500-image study is promising, but lacks description of the protocol (randomization, rating scale, inter-rater reliability).\n\n4. You report VGGT vs. COLMAP with comparable results. Could you add experiments with noisy to quantify robustness to calibration inaccuracies typical in underwater capture?\n\n5. Some typos:\n- \"Japanese Gradens\" -> \"Japanese Gardens\"\n- \"a uncertainty-aware rendering loss\" -> \"an uncertainty-aware rendering loss\"\n- L226 \"enotes the projection function\" -> \"denotes the projection function\"\n- \"Zoomed-in regions shows\" -> \"Zoomed-in regions show\""}, "questions": {"value": "I hope the authors can address Weaknesses 1–5 in the rebuttal, and I remain positive about this work's contribution to the community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "03GyNTIJYH", "forum": "96DTvuYq4h", "replyto": "96DTvuYq4h", "signatures": ["ICLR.cc/2026/Conference/Submission15787/Reviewer_JbgZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15787/Reviewer_JbgZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740996149, "cdate": 1761740996149, "tmdate": 1762926019718, "mdate": 1762926019718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for reconstructing dynamic underwater scenes by using an uncertainty module to discard low-quality pixels in the measurements as well as dynamics modules to model the time-varying nature of both the scene and the medium. The authors demonstrate improved results on several benchmarks compared to baselines, and extensively validate components of their methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem is well-motivated because dynamics are almost unavoidable underwater, since currents and wildlife are commonplace in captured data. The qualitative results are compelling and show clear improvement, especially for dynamic objects and medium recovery, as can be seen in Figures 2 and 3, which show that both the scene and medium reconstructions are better than prior works. The quantitative metrics are also improved by relatively significant margins for most of the scenes. Finally, the evaluation is fairly comprehensive, covering pipeline as well as module design, and illustrates clearly some of the limitations of the method, shown in Figure 4."}, "weaknesses": {"value": "About the videos in the supplement, the paper claims there are four of them but there are only three in the folder. Maybe this is a typo? Furthermore, the trajectories in IUI3RedSea and JapaneseGarden are relatively choppy. I know that the authors are just rendering at the positions provided by the original dataset, but I think it would be helpful to regenerate smooth trajectories with which to render the result."}, "questions": {"value": "Could the authors rerender the supplement videos with smooth trajectories? I won't make my score conditional on the response to this question, since I think it's against the policies to ask authors to do excessive work or provide new results, particularly graphics, but I think it would just be nice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m6vIFuOSSG", "forum": "96DTvuYq4h", "replyto": "96DTvuYq4h", "signatures": ["ICLR.cc/2026/Conference/Submission15787/Reviewer_TEeD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15787/Reviewer_TEeD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921106901, "cdate": 1761921106901, "tmdate": 1762926019168, "mdate": 1762926019168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Uncertainty-aware Dynamic Field (UDF) for 3D reconstruction of dynamic undetwater scenes. UDF jointly models scene structure and view-dependent medium properties over time. UDF initializes with 3D Gaussians in a volumetric medium field, maps them to a 4D neural voxel space, and employs deformation/medium offset networks to handle spatio-temporal dynamics. To suppress noise from low-confidence observations, UDF incorporates per-pixel uncertainty estimation based on surface-view radiance ambiguity and inter-frame flow inconsistency, and further integrates the uncertainty into the rendering loss. Experiments on controlled and in-the-wild underwater datasets demonstrate UDF achieves superior reconstruction quality and novel view synthesis for dynamic underwater scenes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "++ The shared 4D neural voxel space based on planar factorization extends the 3D canonical representation to a spatial-temporal space, which further enables the dynamics modeling of scene structure and medium in a physics-informed manner.\n\n++ UDF estimates per-pixel uncertainty using physical cues and integrates it into the rendering loss to suppress noisy data during training. The effectiveness of this design is validated in the ablation study (Table 5)."}, "weaknesses": {"value": "-- The fundamental contributions and differentiating aspects of the underwater dynamic modeling approach proposed in this paper compared to existing methods require clearer articulation. The authors may need to emphasize these points in their writing.\n\n\n\n-- Regarding the distorted colors shown in WaterSplatting and SeaThru-NeRF after medium removal: could the authors provide insight into the root cause? It would be helpful to know if UDF is immune to this artifact across the entire tested cases and, crucially, what specific aspect of its design confers this advantage.\n\n-- From the supplementary videos (especially composite.mp4),  we can see significant artifacts around dynamic objects (fishes). This indicates that the method's performance in modeling moving objects is not satisfactory."}, "questions": {"value": "-- The gradient-induced pseudo-normal, used in modeling surface-view radiance ambiguity, is particularly prone to significant errors at object edges and in rapidly moving regions. Consequently, failures in uncertainty modeling can occur, potentially adversely affecting the robustness of the entire approach. Can the authors provide some analysis on this aspect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q7srYt91gz", "forum": "96DTvuYq4h", "replyto": "96DTvuYq4h", "signatures": ["ICLR.cc/2026/Conference/Submission15787/Reviewer_mECr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15787/Reviewer_mECr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984435536, "cdate": 1761984435536, "tmdate": 1762926018520, "mdate": 1762926018520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}