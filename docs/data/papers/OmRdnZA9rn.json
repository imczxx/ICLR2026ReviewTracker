{"id": "OmRdnZA9rn", "number": 21869, "cdate": 1758322860123, "mdate": 1759896899072, "content": {"title": "SimpliHuMoN: Simplifying Human Motion Prediction", "abstract": "Human motion prediction combines the tasks of trajectory forecasting, human pose prediction, and possibly also multi-person modeling. \nFor each of the three tasks, specialized, sophisticated models have been developed due to the complexity and uncertainty of human motion. While compelling for each task, combining these models for holistic human motion prediction is non-trivial. Conversely, holistic human motion prediction methods, which have been introduced recently, have struggled to compete on established benchmarks for individual tasks. To address this dichotomy, we study a simple yet effective model for human motion prediction based on a transformer architecture. The model employs a stack of self-attention modules to effectively capture both spatial dependencies within a pose and temporal relationships across a motion sequence. This simple, streamlined, end-to-end model is sufficiently versatile to handle pose-only, trajectory-only, and combined prediction tasks without task-specific modifications. We demonstrate that our approach achieves state-of-the-art results across all tasks through extensive experiments on a wide range of benchmark datasets, including Human3.6M, AMASS, ETH-UCY, and 3DPW. Our results challenge the prevailing notion that architectural complexity is a prerequisite for achieving accuracy and generality in human motion prediction. Code will be released.", "tldr": "We show a simple Transformer model achieves SOTA results across all motion prediction tasks (pose, trajectory, combined), challenging the trend toward complex, specialized architectures.", "keywords": ["Human Motion Prediction", "Generalist Model", "Pose Prediction", "Trajectory Prediction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/355fee0d578a729c752ae7e63962f1500fe52936.pdf", "supplementary_material": "/attachment/685ff4f08192e54f31634e18792a71efe54877be.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a Transformer-based approach for human motion prediction. It combines two kinds of tasks, including pose prediction and trajectory prediction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents SimpliHuMoN, a transformer-based model for 3D human motion prediction. The paper tries to prove that a single, simple architecture can achieve state-of-the-art performance across diverse sub-tasks (pose, trajectory, joint prediction), challenging the trend of increasing model specialization and complexity."}, "weaknesses": {"value": "While the empirical results across multiple benchmarks are strong and the concept of unification is valuable, the paper has weaknesses in motivating its technical choices and providing deeper analysis, which currently limit its contribution."}, "questions": {"value": "This reviewer has several major concerns regarding the novelty, technical contribution, and depth of analysis that need to be addressed to strengthen the paper.\n\n1.The paper rightly identifies the importance of modeling spatio-temporal dependencies for human motion prediction. However, the core idea of using a neural network (in this case, a Transformer) to capture spatial and temporal relationships is a well-established paradigm in the field. Many existing methods are fundamentally designed with this goal. Concepts like end-to-end training are also standard practice. Therefore, while the model is well-executed, the underlying intuition might not be perceived as sufficiently novel on its own. The contribution would be stronger if framed more precisely around the specific implementation achieved by your particular architecture, rather than the general goal of spatio-temporal modeling.\n\n2.The paper said \"challenging the prevailing trend of architectural complexity\" as a key contribution. While this is a valuable high-level message, from a technical perspective, this is more of a philosophical stance or an empirical observation than a concrete technical innovation. It is suggested reframing this contribution to emphasize the empirical finding that a simple, unified architecture can match or exceed the performance of more complex, specialized models across multiple tasks. \n\n3.THe most significant concern is the lack of a clear, intuitive explanation for why this specific, common architecture achieves such strong performance. The paper would greatly benefit from a deeper analysis that goes beyond describing the components. For instance:\nWhy does the unified self-attention mechanism over [C; Q] work better than a more traditional encoder-decoder with cross-attention?\nWhat is the specific advantage of this architecture in learning the coupled dynamics between pose and trajectory compared to prior multi-stage or late-fusion approaches?\nIt is strongly recommend that in your rebuttal, you provide a clearer mechanistic intuition or hypothesis for the model's effectiveness, explaining the \"why\" behind its success rather than just the \"what.\"\n\n4. The role and motivation for the learnable query token are currently under-specified. The description \"learnable prompts guide the decoder\" is quite high-level. A more detailed explanation is necessary.\n\n5. The discussion of the multi-person results is somewhat superficial. The paper notes that strong performance is achieved \"without any explicit interaction modules,\" but this point requires a much deeper analysis to be meaningful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "poKhLAtI65", "forum": "OmRdnZA9rn", "replyto": "OmRdnZA9rn", "signatures": ["ICLR.cc/2026/Conference/Submission21869/Reviewer_Nwar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21869/Reviewer_Nwar"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761181024698, "cdate": 1761181024698, "tmdate": 1762941963615, "mdate": 1762941963615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SimpliHuMoN, a transformer architecture that takes both the historical trajectories and pose joints as input and predicts the future trajectories and pose joints. Since the trajectories can be extracted from the pelvis joint, the tasks can be seen as global pose forecasting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Breadth of evaluation across three settings (traj-only, pose-only, traj+pose) with consistent K-mode reporting and per-task K values.\n\n2. Ablations exploring depth/width trade-offs and effect of multimodality (K>1 vs K=1).\n\n3. The text is clearly written and easy to follow."}, "weaknesses": {"value": "1. The biggest concern with this work is its novelty. The motivation of predicting global trajectory and full-body pose jointly (or condition one on the other) is not new [1],[2],[3],[4]. \n\n2. The proposed model architecture is a standard decoder with learnable queries, limiting the complex social interaction among pedestrians. Following this limitation, the dataset used for pose prediction only contained up to three pedestrians, which cannot reflect complex social interactions in real life. I would suggest trying more interactive scenarios like JRDB-GMP used in [3].\n\n3. There is no ablation showing the benefit from the feature fusion of trajectory and pose. For example, when reporting the numbers on datasets like MOCAP-UMPM/3DPW, what are the performances of trajectory prediction given (1) Trajectoy-only; (2) Trajectory+Pose. By doing this, we can know if the fusion works to bring extra pose knowledge into the trajectory task. Similarly, what are the performances of pose prediction given (1) Pose-only; (2) Trajectory+Pose?\n\n4. Missing qualitative results about the trajectory prediction task. \n\n[1] Adeli, Vida, et al. \"Tripod: Human trajectory and pose dynamics forecasting in the wild.\" ICCV 21\n\n[2] Zaier, Mayssa, et al. \"A dual perspective of human motion analysis-3d pose estimation and 2d trajectory prediction.\" ICCV 23\n\n[3] Jeong, Jaewoo, Daehee Park, and Kuk-Jin Yoon. \"Multi-agent long-term 3d human pose forecasting via interaction-aware trajectory conditioning.\" CVPR 24\n\n[4] Gao, Yang, Po-Chien Luan, and Alexandre Alahi. \"Multi-transmotion: Pre-trained model for human motion prediction.\" CoRL 24"}, "questions": {"value": "About the throughput comparison (computing speed) in Table 2, how did you set up the experiment? I.e., were numbers reported as the average of multiple runs? How many samples did you use? What was the batch size and implemented hardware?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QEi4R9N7XN", "forum": "OmRdnZA9rn", "replyto": "OmRdnZA9rn", "signatures": ["ICLR.cc/2026/Conference/Submission21869/Reviewer_7bcE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21869/Reviewer_7bcE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761244606740, "cdate": 1761244606740, "tmdate": 1762941963214, "mdate": 1762941963214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified transformer network for human motion prediction, capable of handling trajectory forecasting, pose prediction, and their combined execution. The model uses distinct embedding modules to process different inputs for each task. These inputs are then tokenized, concatenated, and fed into a shared self-attention module, which effectively mixes information and learns the complex dynamics between trajectory and pose. Task-specific prediction heads then generate the final outputs.\n\nThe authors demonstrate through extensive benchmarking that their model is a strong competitor for individual tasks and achieves new state-of-the-art results for the joint prediction of both pose and trajectory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **(S1) Unified and Versatile Architecture:** The model's key strength is its generality. A single, unified transformer architecture successfully handles pose, trajectory, and combined prediction without any task-specific modifications. This directly addresses the prevalent issue of fragmentation, where competing models are often hyper-specialized.\n- **(S2) Rigorous State-of-the-Art Evaluation:** The authors conduct a comprehensive and robust evaluation across a wide range of standard benchmarks for all three tasks. The model is shown to be highly competitive against specialized methods and achieves new state-of-the-art results on the challenging joint prediction task.\n- **(S3) A Compelling Case for Simplicity:** The paper makes a powerful argument against escalating architectural complexity. It convincingly demonstrates that a simple, end-to-end framework can outperform more complex, multi-stage pipelines, while also being more computationally efficient.\n- **(S4) Strong Qualitative Evidence:** The inclusion of visualizations and supplementary videos is highly effective. They provide clear, intuitive proof that the model generates fluid and physically plausible motions, visually demonstrating its superiority over baseline methods that produce unnatural or static predictions."}, "weaknesses": {"value": "### Major\n\n- **(W1) Ambiguous Multi-Modal Prediction Mechanism:** The method for generating K distinct future hypotheses is unclear. Section 2.3 mentions a linear projection creates K parallel branches, but the exact mechanism is not detailed. If this is a single, large linear layer, it is not obvious how this architecture efficiently scales to the K=20 proposals required for trajectory forecasting benchmarks. The paper needs to clarify if the output head's size is fixed or dynamic, and how it handles different values of K without becoming computationally prohibitive.\n- **(W2) Potential for Mode Collapse with \"Winner-Takes-All\" Loss:** The \"winner-takes-all\" loss function, which only backpropagates through the most accurate of the K proposals, is susceptible to mode collapse. The model could learn to rely on a single \"favorite\" prediction head, defeating the purpose of generating a diverse set of futures. The paper provides no analysis to ensure that the prediction modes are balanced and utilized effectively. A quantitative result, such as a histogram of the winning mode index over the test set, is needed to validate this design choice.\n- **(W3) Unclear Role and Nature of Learnable Query Tokens:** The function of the query vectors $\\mathcal{Q}_{in}$ is poorly explained. The paper should clarify their role. Are they essentially \"output slots\" that are progressively refined by the transformer's self-attention layers, starting from a learnable initial state? The term \"learnable\" itself is ambiguous, does it refer to a learned initial value for each token? A more precise explanation of this core component is necessary to fully understand the model's generative process.\n- **(W4) Pacing and Focus of Explanations:** The paper dedicates significant space to describing standard, well-known concepts (e.g., the basic mechanics of a transformer decoder in Section 2.2) while glossing over the novel aspects of its own architecture. This space would be better utilized to provide the missing details on the query mechanism, the multi-modal head, and the justification for the loss function.\n- **(W5) Absence of Key Multimodal Metrics:** For pose prediction, simply reporting the minimum error (ADE/FDE) is insufficient to evaluate the quality of the generated distribution of motions. The evaluation is missing standard multimodal metrics like MMADE. Without these, it's impossible to know if the model is generating genuinely distinct futures or just minor variations of a single prediction.\n\n### Minor Weaknesses\n- **(m1) Reproducibility and Robustness of Throughput Metrics:** Table 2 presents throughput in samples/second, a metric highly dependent on the hardware used. While an NVIDIA A6000 is mentioned earlier in the implementation details (L203), this should be explicitly stated in the caption of Table 2 for clarity. Furthermore, reporting a single number without confidence intervals or standard deviation over multiple runs makes it difficult to assess the stability and robustness of these efficiency claims.\n- **(m2) Table Formatting:** The main results table (Table 1) uses vertical lines, which can make it appear cluttered and less professional. Adopting a cleaner format, such as the one provided by the booktabs package in LaTeX, would significantly improve readability.\n- **(m3) Incomplete Supplementary Materials:** The supplementary material appears to be missing most of the qualitative video examples. While samples 8 and 10 are present, the absence of the others limits the ability to fully verify the qualitative claims of generating physically plausible and diverse motions across a range of scenarios."}, "questions": {"value": "Based on these weaknesses:\n\n**W1**\n\n1. What is the exact architectural mechanism for generating K distinct future hypotheses?\n2. Is the linear projection a single large layer or multiple separate layers?\n3. How does the output head architecture scale when K=20 (as required for trajectory forecasting benchmarks)?\n4. Is the output head's size fixed or dynamic with respect to K?\n5. What are the computational costs as K increases, and how does the method remain computationally tractable?\n\n**W2**\n\n6. How do you ensure that the winner-takes-all loss doesn't lead to mode collapse?\n7. Are all K prediction heads utilized effectively during training, or does the model favor certain heads?\n8. What is the distribution of winning mode indices across the test set?\n9. Can you provide quantitative evidence (e.g., histogram or usage statistics) showing balanced utilization of prediction modes?\n\n**W3**\n\n10. What exactly is the role of the learnable query tokens in the architecture?\n11. Are query tokens \"output slots\" that are refined through transformer self-attention layers?\n12. What does \"learnable\" mean in this context—learned initial values, learned embeddings, or something else?\n13. How are the query tokens initialized and updated during the forward pass?\n\n**W4**\n\n14. Can you provide more detailed explanations of the novel components (query mechanism, multi-modal head) rather than standard transformer concepts?\n\n**W5**\n\n15. Why are multimodal metrics like MMADE not reported for pose prediction?\n16. How diverse are the K generated futures—are they genuinely distinct or just minor variations?\n17. Can you provide quantitative metrics that evaluate the quality of the generated distribution of motions?\n\n**m1**\n\n18. What hardware was used for the throughput measurements in Table 2?\n19. What are the confidence intervals or standard deviations for the throughput metrics across multiple runs?\n20. How stable and robust are the efficiency claims?\n\n**m3**\n\n21. Why are most qualitative video examples missing from the supplementary materials?\n22. Can the complete set of qualitative examples be provided to verify the claims about diverse and physically plausible motions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g6kQBT3xgs", "forum": "OmRdnZA9rn", "replyto": "OmRdnZA9rn", "signatures": ["ICLR.cc/2026/Conference/Submission21869/Reviewer_oRqS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21869/Reviewer_oRqS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735089648, "cdate": 1761735089648, "tmdate": 1762941962086, "mdate": 1762941962086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified and general framework that simultaneously addresses trajectory forecasting and pose prediction by leveraging a shared self-attention mechanism to model both spatial (inter-joint) and temporal (inter-frame) dependencies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The experimental evaluation is thorough. The method is validated across multiple tasks and datasets, compared against numerous state-of-the-art (SOTA) approaches, and achieves competitive results.\n\nThis work successfully demonstrates that a simple network architecture can effectively tackle this complex problem, offering a fresh and inspiring perspective for future research."}, "weaknesses": {"value": "Multimodal modeling mechanism: The current approach uses only a simple type embedding to distinguish between trajectory and pose modalities, without explicitly modeling their underlying physical coupling (e.g., how gait influences arm swing).\n\nPrediction horizon: How much past observation is required, and how far into the future can the model reliably predict? How does performance degrade as the prediction horizon increases?\n\nTemporal jitter: Does the model suffer from jitter or unnatural motion artifacts in its predictions? If so, how is this issue addressed?"}, "questions": {"value": "Enhance ablation studies: Investigate the necessity of type embeddings and compare RMSNorm against LayerNorm to justify architectural choices.\n\nQualitative analysis: Provide more visual comparisons across datasets, visualize the diversity of multi-modal predictions (e.g., K hypotheses), and include failure case analyses with visual examples to better understand model limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kFZyspFg6V", "forum": "OmRdnZA9rn", "replyto": "OmRdnZA9rn", "signatures": ["ICLR.cc/2026/Conference/Submission21869/Reviewer_XHAF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21869/Reviewer_XHAF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820707572, "cdate": 1761820707572, "tmdate": 1762941961612, "mdate": 1762941961612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}