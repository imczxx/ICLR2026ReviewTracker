{"id": "MyKspTvxBl", "number": 6732, "cdate": 1757993849565, "mdate": 1763726812133, "content": {"title": "GaussianTrim3R: Controllable 3D Gaussians Pruning for Feedforward models", "abstract": "Feed-forward methods offer a promising paradigm for novel-view synthesis, eliminating computationally expensive per-scene optimization. However, current feed-forward approaches typically predict a fixed number of pixel-aligned Gaussian primitives, leading to significant redundancy. Naively pruning these Gaussians creates severe visual artifacts, necessitating fine-tuning that compromises the feed-forward nature. We introduce GaussianTrim3R, a novel framework for controllable and feed-forward 3D Gaussian representation method which gradually prunes 3D Gaussians and simultaneously adjusts the attributes of remaining Gaussians maintaining rendering quality, thus eliminating the need for finetuning 3D Gaussians post pruning. To achieve this, we construct SuperClusters by partitioning the 3D scene based on spatial and color attributes. By leveraging Discrete Wavelet Transform, we assign and rank texture complexity to these SuperClusters, enabling selective, texture-aware pruning. Doing so enables our method to directly predict attribute-adjusted Gaussians, thereby preserving scene integrity. Unlike existing methods, GaussianTrim3R offers an efficient, real-time solution with extensive experiments demonstrating superior trade-offs between quality and efficiency across diverse real world RealEstate10K, ACID and DTU datasets.", "tldr": "Feed forward pruning of 3D Gaussians in sparse view", "keywords": ["Gaussians", "feed-forward"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6ab4a59c842976aedeba16aa2ca6212b8bdb7d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GaussianTrim3R, a feed-forward framework for pruning 3D Gaussian Splatting (3DGS) representations under a limited Gaussian budget. The authors argue that current feed-forward models predict a fixed number of Gaussians, leading to redundancy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addresses a timely and relevant problem: efficient and controllable pruning of 3D Gaussian Splatting for real-time rendering and feed-forward models.\n\n2. Clearly recognizes the redundancy issue in current feed-forward Gaussian prediction frameworks such as MASt3R and DUSt3R.\n\n3. Shows a clear problem motivation and structured pipeline, even though the method itself is heuristic."}, "weaknesses": {"value": "1. 2D texture ‚Üí 3D pruning mismatch. Textureness is computed on image views via DWT and then averaged per SuperCluster. There is no principled treatment of visibility, occlusions, or multi-view consistency. A 2D texture proxy can be high where geometry is flat but textured (e.g., wallpaper) or low where geometry is complex but uniformly colored; both cases can cause harmful pruning. The paper acknowledges this failure mode but does not quantify it or provide mitigation.\n\n2. Efficiency and ‚Äúreal-time‚Äù are not demonstrated. The paper claims real-time, controllable inference but provides no FPS, latency, GPU memory, or throughput numbers under different budgets.\n\n3. Ablations are narrow; component necessity is unclear. The ablation table focuses on a single dataset and a couple of pruning points."}, "questions": {"value": "1. How exactly is the ‚Äúadaptive Gaussian expansion‚Äù implemented? Is it learned or deterministic?\n\n2. What is the exact runtime and memory cost reduction versus MASt3R or DUSt3R backbones?\n\n3. How does your method behave on scenes with high-frequency textures but simple geometry"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DiYmqeLQeu", "forum": "MyKspTvxBl", "replyto": "MyKspTvxBl", "signatures": ["ICLR.cc/2026/Conference/Submission6732/Reviewer_RPAH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6732/Reviewer_RPAH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6732/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761216002880, "cdate": 1761216002880, "tmdate": 1762919020378, "mdate": 1762919020378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose GaussianTrim3R, a feedforward 3DGS method that can adaptively change the output Gaussian numbers based input masks and targeted budget. GaussianTrim3R first obtains the point cloud from input pair images and perform clustering + frequency analysis. These information come together to rank the texture complexity of point cloud clusters; then GaussianTrim3R select from the clusters with lowest texture to start masking away features, such that the GS head can learn to produce fewer but larger Gaussians at these regions. Such a process is done progressively until certain threshold of Gaussian budget are satisfied. Overall results indicate that this approach is better than random pruning at very high compression rate (10%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The topic of feed forward Gaussian compression is timely, especially when there are a lot of input images. The current scheme of generating Gaussian per-pixel will not be sustainable.\n\nThe authors propose to train a mask-aware GS head such that the generated GS can be of larger shapes, which seems reasonable to me.\n\nThe overall performance at high compression rate seems to show that pruning over low texture regions lead to differential performance, which makes sense intuitively.\n\nThe writing is easy to understand for the most part.  \n\nDespite my issues with the baseline comparisons, I am willing to give borderline accept given the simple but straightforward innovation in training feedforward GS models, which hasn't been explored before. I would love if the authors can confirm if e.g., this method can be made to work under e.g., 6-12 views."}, "weaknesses": {"value": "While feed forward Gaussian compression is an important topic, my sense is that Gaussian count only becomes a real problem with a lot of input images. E.g., given two views, representing the scene with 200k Gaussians can be readily supported by normal computers. As such, I think it's worthwhile to see if this work can be applied in scenarios beyond two views. \n\nThis also has ramification to comparisons. The baselines that GaussianTrim3R compare with are all designed for multi-view reconstruction, not two-view reconstruction. Particularly, if the original gaussians are not maintained and are pruned instead, it is very hard for these methods to recover the 3D scene, which is conventionally known. As such, these baselines are not very useful - more useful baselines may be e.g., InstantSplat or SPARS3R, which work on sparse view reconstruction. E.g., for InstantSplat, the setting is that existing initialization will not be pruned, and it maybe useful to see the effectiveness of these methods if the initialization can be pruned. \n\nThe ablation shows that, random pruning/no contextual mask lead to similar performance at 40% pruning, indicating that there is a lot of redundancy in feedforward GS methods, though the 0% pruning GS metric is not posted. Differentiable results begin to emerge at 10%, which begs the question of whether this method only works when the compression is very severe, which is relatively niche."}, "questions": {"value": "Are all experiments with post-inference finetuning/optimization done with only two view constraint? \n\nIt would be very helpful if authors can list the original metrics at 0% pruning so that reviewers understand the full context of pruning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LgsfhDT0jF", "forum": "MyKspTvxBl", "replyto": "MyKspTvxBl", "signatures": ["ICLR.cc/2026/Conference/Submission6732/Reviewer_KvJ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6732/Reviewer_KvJ4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6732/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746487599, "cdate": 1761746487599, "tmdate": 1762919019870, "mdate": 1762919019870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a feed-forward 3D Gaussian Splatting (3DGS) framework with a primitive-number control algorithm. The method dynamically regulates the number of Gaussian primitives during training and achieves real-time optimization. Specifically, the approach introduces SuperClusters to group the 3D Gaussian primitives and employs a Discrete Wavelet Transform to assess texture complexity, generating masks that guide where and how aggressively to prune the Gaussians. Experimental results demonstrate that the method achieves superior performance compared with baseline approaches."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Pruning is integrated into the 3D Gaussian generation process rather than applied post-training, enabling joint optimization and leading to improved performance.\n- The method can accurately identify over-fitted regions and prune them precisely.\n- The proposed approach achieves superior quantization results and visual quality compared to baseline methods under the same pruning ratio. Notably, at high pruning ratios, baseline methods tend to collapse, whereas the proposed method maintains strong reconstruction quality."}, "weaknesses": {"value": "- The hyperparameters ùêæ (number of clusters) and ùëÅ (pruning ratio per patch) play an important role in the proposed method. However, their values appear to be chosen empirically. It would be beneficial to explore adaptive or data-driven strategies for determining these parameters, which could improve robustness and reduce manual tuning efforts."}, "questions": {"value": "I would like clarification regarding the training and inference pipeline. Are the center heads and Gaussian-splat heads trained on a large dataset during the training stage, and then kept frozen during inference? During inference, do we simply follow the framework shown in Figure 3 to produce the 3D Gaussians? Additionally, how many iterations are required to generate the 3D Gaussians at inference time‚Äîonly a single forward pass, or multiple iterations? Out of curiosity, how much time needed for the inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b2aMUYUtnK", "forum": "MyKspTvxBl", "replyto": "MyKspTvxBl", "signatures": ["ICLR.cc/2026/Conference/Submission6732/Reviewer_qQcZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6732/Reviewer_qQcZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6732/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872864896, "cdate": 1761872864896, "tmdate": 1762919019512, "mdate": 1762919019512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel method for generating lightweight, high-fidelity Gaussian scenes from sparse views. Prior zero-shot methods generate pixel-aligned 3D Gaussians, which often leads to duplicated Gaussians in simple-textured areas. To address this issue, given initial points, the authors identify point clusters (K=300) and, starting from the low-textured clusters ranked by Equation 2, iteratively prune 100‚ÄìN% of the Gaussians in each cluster until reaching the target Gaussian budget (Z=13k, 52k, 78k). For the experiments, since this problem is newly defined by the authors, they constructed baselines by combining existing zero-shot generation and pruning methods. Compared with these baselines, their method demonstrates significant improvements across various pruning ratios. The ablation study further justifies the necessity of the proposed components."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "This paper defines a new problem: how to generate lightweight Gaussian scenes from sparse images. It neatly addresses the problem by identifying texture-less regions, which require fewer Gaussians for representation, generating a mask, and then generating Gaussians with consideration of the mask. Although some representations still need refinement, the paper is generally clear and easy to follow."}, "weaknesses": {"value": "Please see the Questions section for the major concerns.\n\nPresentation issues:\n* In line 089, doesnt -> doesn't\n* In Figure 4, the authors need to annotate which columns correspond to 40% and 80% of the Gaussians.\n* In Figure 4, it would be a good idea to juxtapose a non-finetuned image corresponding to the same scene as the finetuned image to illustrate the \"blobby Gaussian\" artifact of the baseline.\n* In Figure 4, I suggest representing the distributions using blue and yellow line curves without filling for NoPoSplat and your method. Currently, the authors fill the overlapping area with brown, which is understandable only when I check color composition palette.\n* In Tables 1 and 2, I suggest reporting the performance of the baselines and GaussianTrim3R without pruning as a reference to show how much the performance of pruned version degrades from their full capacity."}, "questions": {"value": "* My biggest question is whether the baselines are truly designed fairly. The gap of 8 dB seems phenomenal, but upon some reflection, it makes sense because the baselines apply pruning methods after generating Gaussians, whereas GaussianTrim3R‚Äôs GS Head directly generates hole-free scenes by leveraging a mask. I suggest a baseline that first generates pixel-wise initial Gaussians using the Mast3r backbone with some trainable Initial GS Head, then applies pruning methods to these initial Gaussians. Based on the pruning scores, a mask map can be generated and concatenated with the Mast3r features before passing into the another GS Head, similar to what GaussianTrim3R does after the Mask Generation Module. This ensures that pruning methods are applied before final Gaussian generation, making it a fairer baseline.\n* The pruning method explored in this paper is limited to scene-level pruning. I suggest considering pixel-level pruning (Liu et al., EfficientGS: Streamlining Gaussian Splatting for Large-Scale High-Resolution Scene Representation, IEEE MM 2025), which ensures at least one Gaussian per ray and avoids holes.\n* Ambiguous definition of ‚Äúadaptive allocation‚Äù (line 455): where does the ‚Äúadaptive‚Äù property come from? How is it related to disabling the training of the ‚ÄúGaussian Head‚Äù in the ‚ÄúWithout Gaussian Adaptation‚Äù ablation study?\n* From an efficiency perspective: please report inference latency (fps or milliseconds). This would help readers better understand the pros and cons of zero-shot lightweight Gaussian generation.\n* I found relatively fair baseline and recommend to compare with this: Fei et al., PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views, arXiv"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4CdLDhH7qP", "forum": "MyKspTvxBl", "replyto": "MyKspTvxBl", "signatures": ["ICLR.cc/2026/Conference/Submission6732/Reviewer_D4Jq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6732/Reviewer_D4Jq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6732/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999598848, "cdate": 1761999598848, "tmdate": 1762919019198, "mdate": 1762919019198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Brief Summary of Paper Revisions"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback. The revised manuscript incorporates the following updates to address the raised concerns:\n\n1. Line 089 updated from ‚Äúdoesnt‚Äù to ‚Äúdoesn‚Äôt‚Äù.\n\n2. **Figure 4** Now includes a clearer comparison using results from the same scene under both finetuned and non-finetuned settings.\n\n3. Ablation Studies in **Appendix Section A.4 (and Table 5)** now reports additional ablations on the RE10K dataset across multiple pruning strengths.\n\n4. Additional Baselines in **Appendix Section B (Tables 9 and 10)** introduces new baselines where pruning masks are derived from existing pruning pipelines and integrated into the GS Head.\n\n5. FPS and Inference Time Analysis in **Appendix Section C (Table 11, Table 12 and Fig. 9)** presents FPS improvements across pruning strengths, and Table 12 reports latency measurements.\n\n6. Extension to Multi-View Reconstruction using InstantSplat in **Appendix Section D (Table 13)** evaluates our pruning strategy within InstantSplat for 3-view, 4-view, and 10-view inputs.\n\n7. 0%‚Äì10% Pruning Behavior in **Appendix Section E.1 (Table 14, Table 15 and Fig. 10)** reports detailed results at 0%, 5%, and 10% pruning, highlighting our method‚Äôs stability compared to baselines.\n\nAll changes introduced in the appendix are highlighted in **blue text** for easy identification.\n\nWe hope the revised paper can effectively address reviewer's concerns. Please let us know if there are any further suggestions."}}, "id": "rzdcv8XpRI", "forum": "MyKspTvxBl", "replyto": "MyKspTvxBl", "signatures": ["ICLR.cc/2026/Conference/Submission6732/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6732/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission6732/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763727495621, "cdate": 1763727495621, "tmdate": 1763727495621, "mdate": 1763727495621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}