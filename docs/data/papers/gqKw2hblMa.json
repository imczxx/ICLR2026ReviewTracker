{"id": "gqKw2hblMa", "number": 14751, "cdate": 1758243051338, "mdate": 1759897351251, "content": {"title": "Scalable Element-wise Finite-Time Optimization for Deep Neural Networks", "abstract": "Optimization algorithms are fundamental to deep neural network training, where exponential growth from millions to hundreds of billions of parameters has made training acceleration a critical necessity. While adaptive methods like Adam achieve remarkable success through element-wise learning rates, understanding their continuous-time counterparts can provide valuable theoretical insights into convergence guarantees beyond asymptotic rates.\nRecent advances in continuous-time optimization have introduced fixed-time stable methods that promise finite-time convergence independent of initial conditions. However, existing approaches like FxTS-GF suffer from dimensional coupling, where coordinate updates depend on global gradient norms, creating suboptimal scaling in high-dimensional problems typical of deep learning.\nTo address this issue, we introduce an element-wise finite-time optimization framework that eliminates dimensional coupling through coordinate-independent dual-power dynamics. Furthermore, we extend the framework to momentum-enhanced variants for deep model training while preserving convergence properties through continuous-time analysis. Under mild assumptions, we establish rigorous finite-time and fixed-time convergence guarantees. Notably, our framework reveals that widely-used sign-based optimizers like SignSGD and Signum emerge as limiting cases, providing theoretical grounding for their empirical effectiveness. Experiments on CIFAR-10/100 and C4 language modeling demonstrate consistent improvements over existing methods.", "tldr": "", "keywords": ["finite-time optimization", "Element-wise dynamics", "Deep learning optimization", "Sign-based methods"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/baeafaafa36085570bed38e9081aabd18e225dfb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper builds upon the work of Budhraja et al. by introducing an ODE that exhibits finite-time convergence guarantees. Compared to previously studied finite-time gradient flows, the proposed methods enjoys coordinatewise adaptivity and can be seen as a generalization of signSGD. A proof of finite-time and fixed-time convergence is provided in the smooth and PL regime, and the results are shown to hold for variants incorporating exponential moving averages and momentum. In addition, the empirical effectiveness of the proposed optimizer is explored on vision and pretraining tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. The idea of extending existing finite-time gradient flows to be coordinatewise adaptable is well-motivated.\n3. The hyperparameter conditions for finite-time and fixed-time convergence are clearly stated in the theorems."}, "weaknesses": {"value": "1. The method itself, equation (2), is not intuitively motivated or explained. For instance, it is not immediately obvious what $p_1$ and $p_2$ are supposed to represent, even though they are of critical importance to the effectiveness of the method.\n2. The proposed optimizer introduces a slew of hyperparameters, for which there is little discussion of practical recommendations or tuning suggestions.\n3. The theoretical results are given for the continuous-time dynamics, and no discussion is provided on how these results transfer to the discretized versions. The paper lacks the results that are expected of a typical optimizer paper, e.g. a convergence rate on the gradient norm or objective value.\n4. No empirical evidence is provided to support the claim that the method achieves finite-time convergence. If the continuous-time results do indeed transfer to the discrete-time setting, then such an empirical result would greatly support the effectiveness of the proposed method.\n5. A central claim of the paper is that the proposed method is scalable, but there is little evidence to support this with the provided experiment results. I would suggest providing results on models with 7B+ parameters and evaluating on tasks that are significantly harder than CIFAR.\n\nI did not carefully review the proofs, but unfortunately, the mentioned issues are already significant enough for me to recommend rejection."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oEpPNOHmCq", "forum": "gqKw2hblMa", "replyto": "gqKw2hblMa", "signatures": ["ICLR.cc/2026/Conference/Submission14751/Reviewer_MagU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14751/Reviewer_MagU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761456151266, "cdate": 1761456151266, "tmdate": 1762925111295, "mdate": 1762925111295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an Element-wise Finite-Time (EFT) optimization framework and its momentum variants for deep neural network training. The core motivation is to adapt control-theoretic finite-time and fixed-time stability concepts of ODEs to large-scale deep learning. The proposed EFT framework replaces this global norm dependency with element-wise operations based on the sign and fractional powers of individual gradient components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Problem Relevance: The paper addresses a practical and important issue: how to apply optimization theories with stronger convergence guarantees (finite/fixed-time) to large-scale, high-dimensional deep learning tasks, overcoming the scalability bottlenecks of existing methods.\n\nExperimental Results: The paper presents preliminary evidence showing that the proposed methods (especially the momentum variants) outperform some baseline optimizers (including SGD, AdamW, and FxTS-GF) on benchmarks like CIFAR image classification and C4 language modeling, indicating its potential."}, "weaknesses": {"value": "Major ones: \n\n1. Borrowed Core Idea: The core theoretical framework using dual-power dynamics (combining terms with exponents less than 1 and greater than 1) to achieve finite/fixed-time convergence is not original to this paper. This concept has already been developed such as Powerball method (https://arxiv.org/pdf/1603.07421), FxTS-GF method (https://arxiv.org/pdf/1808.10474) and others.\n\n2. Discretization Gap: The paper provides continuous-time analysis but implements discrete updates via Euler discretization. The authors acknowledge a \"discretization gap\" in the appendix, noting that the fixed-time regime ($p_1 < 1$) is sensitive to discretization effects and step size choices in practice, potentially leading to instability. This is a critical issue. Properties relied upon in continuous-time finite-time stability proofs often do not hold under fixed step-size discretization $\\eta$. The practical relevance of the continuous-time guarantees is therefore doubtful without a discrete-time analysis or a more thorough empirical investigation of how step size $\\eta$ affects stability and convergence. Please refer to https://www.ijcai.org/proceedings/2020/451 for further details, could these results be helpful for analyzing (1)? \n\n(Minor ones)\n3. Contribution as Adaptation: The main claimed innovation is making the dynamics element-wise to eliminate dimensional coupling. The principle of element-wise scaling (based on local gradient info rather than global norms) is a key feature of successful adaptive optimizers in deep learning (like AdaGrad, RMSprop, Adam) and is a common technique. Therefore, the core contribution appears to be applying a standard deep learning heuristic (element-wise adaptation) to an existing theoretical framework. The work should be more accurately positioned as an element-wise adaptation of existing fixed-time gradient flows, not an entirely new framework or paradigm.\n\n4. Interpretation of Results: The results presented in the paper that frames SignSGD and Signum as limiting cases of their framework were already known in Zhou et al. IJCAI, 2020  (https://www.ijcai.org/Proceedings/2020/0451.pdf). The connection drawn to SignSGD/Signum as limiting cases ($p_1 \\to 2, p_2 \\to 0$) 13 is mathematically interesting but its practical significance is unclear. As parameters approach these limits, the theoretical convergence bounds might degrade or become infinite. Does this connection offer new insights into why SignSGD works, or is it just a boundary condition of the mathematical form? The paper claims it provides \"theoretical grounding\" but doesn't elaborate on the specific insights gained."}, "questions": {"value": "please refer to my specific comments,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "arO2SYG11Q", "forum": "gqKw2hblMa", "replyto": "gqKw2hblMa", "signatures": ["ICLR.cc/2026/Conference/Submission14751/Reviewer_JQh7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14751/Reviewer_JQh7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704333433, "cdate": 1761704333433, "tmdate": 1762925110720, "mdate": 1762925110720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper motivates from the continuous-time counterpart of optimization algorithms, and proposes novel optimizers EFTOM and PEFTOM based on this perspective. Theoretical analysis and empirical justification of the algorithms are provided in the together for the algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of deriving optimizers from a continuous perspective looks interesting, and the derived algorithms are reasonable since it matches the optimzier derived from the standard steepest descent framework (e.g., SignSGD).\n2. The theoretical convergence results seem to be valid, showing finite-time convergence of the algorithm incorporated with momentum."}, "weaknesses": {"value": "1. For the theoretical part, the convergence rates all have heavy dependence on dimensionality, which can be extremely large in practice. This drawback can be avoided by analysis of similar algorithms like SignSGD or Signum.\n2. For the experiment part, could the authors give how the hyperparameters are tuned for EFTOM and PEFTOM? It also looks strange that Table 5 indicates the best SGD learning rate is $ 0.2 $ , which is not listed in the listed SGD learning rate search grid. This unclear setting raises questions in fair comparisons with other optimizers."}, "questions": {"value": "1. Could we fix the explicit dependence on dimensionality by employing similar assumptions as the SignSGD paper, i.e., we consider smoothness in a different norm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dNF2BoxmjY", "forum": "gqKw2hblMa", "replyto": "gqKw2hblMa", "signatures": ["ICLR.cc/2026/Conference/Submission14751/Reviewer_J1XU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14751/Reviewer_J1XU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982508370, "cdate": 1761982508370, "tmdate": 1762925109455, "mdate": 1762925109455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces an element-wise optimization framework that overcomes the scalability issues of control-theoretic methods in high-dimensional deep learning. By using coordinate-independent dynamics, it ensures rigorous finite-time convergence while preserving the adaptivity essential for neural network training. This theory unifies various optimizers under one principled foundation, rigorously justifying the empirical success of SignSGD and Signum as special cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work provides valuable new insights by bridging control-theoretic stability principles and large-scale non-convex optimization.\n\n- It is beneficial for understanding optimizer convergence behavior, moving beyond traditional asymptotic rates to finite-time guarantees.\n\n- The proposed PEFTom optimizer demonstrates competitive performance compared to existing state-of-the-art methods."}, "weaknesses": {"value": "- The experimental results do not fully substantiate the central theoretical claim that PEFTom achieves finite-time convergence. More targeted experiments are needed to directly illustrate this property.\n\n- The experimental setup relies on CNN architectures (e.g., ResNet, DenseNet) and datasets (CIFAR-10, CIFAR-100) that are now considered somewhat stale. To convincingly demonstrate the optimizer's effectiveness and scalability, experiments on more modern architectures (e.g., Vision Transformers) and larger, more complex datasets (e.g., ImageNet) are recommended."}, "questions": {"value": "The PEFTom optimizer introduces several new hyperparameters (e.g., $c_1, c_2, p_1, p_2$). Could the authors provide:\n\n* A sensitivity analysis to show how the algorithm's performance is affected by variations in these parameters?\n\n* Practical guidance or heuristics for tuning these hyperparameters effectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fZA4PdTWbv", "forum": "gqKw2hblMa", "replyto": "gqKw2hblMa", "signatures": ["ICLR.cc/2026/Conference/Submission14751/Reviewer_ATA2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14751/Reviewer_ATA2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762257904511, "cdate": 1762257904511, "tmdate": 1762925108936, "mdate": 1762925108936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}