{"id": "GfT31twtCw", "number": 23523, "cdate": 1758344939159, "mdate": 1763121321764, "content": {"title": "SToRe3D: Sparse Token Relevance in ViTs for Efficient Multi-View 3D Object Detection", "abstract": "Vision Transformers (ViTs) enable strong multi-view 3D detection but are limited by high inference latency from dense token and query processing across multiple views and large 3D regions. Prior sparsity methods, designed mainly for 2D vision, prune or merge image tokens but do not extend to full-model sparsity or address 3D object queries. We introduce SToRe3D, a relevance-aligned sparsity framework that jointly selects 2D image tokens and 3D object queries while storing filtered features for selective reuse. Mutual 2D-3D relevance heads allocate compute to driving-critical content and preserve other embeddings. Evaluated on nuScenes and our new nuScenes-Relevance benchmark, SToRe3D delivers up to 3x faster inference with marginal accuracy loss, establishing real-time 3D detection with large scale ViTs while maintaining accuracy on planning-critical agents.", "tldr": "SToRe3D achieves 3x faster multi-view 3D object detection with minimal accuracy loss by jointly sparsifying image tokens and 3D queries in ViTs.", "keywords": ["3d object detection", "vision transformers", "sparse attention", "token sparsity", "query sparsity", "transformer acceleration", "computer vision", "robotics", "autonomous driving"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/bd225da82577797e3597990ec4c884935f4968d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper targets the latency bottleneck of multi-view 3D object detection with ViT backbones/DETR-style decoders, where dense processing of (i) many image tokens and (ii) many 3D queries causes quadratic attention cost. It proposes SToRe3D, a planner-aligned sparsity framework that (1) jointly scores and prunes both 2D tokens and 3D queries using lightweight mutual 2D–3D relevance heads, and (2) stores filtered embeddings into buffers for possible reactivation at deeper stages, rather than permanently discarding them."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear problem framing (latency under quadratic attention) and a pragmatic solution that impacts both backbone and decoder. \n2. Recoverability through store–reactivate is a practical design that mitigates over-pruning failure modes. \n3. Planner alignment is operationalized, not just argued—definitions, labels, and metrics (RM/RA) are all consistent. \n4. Solid speed–accuracy tradeoffs with real-time throughput at high sparsity (up to ~18 FPS for ViT-L)."}, "weaknesses": {"value": "1. Framework figure clarity. The core architecture diagram (Fig. 1 & Fig. 2) appears low-resolution in the PDF, a larger, higher-resolution schematic (with per-stage keep ratios and reactivation paths labeled) would improve readability.\n2. Limited qualitative comparisons. Qualitative evidence is sparse (e.g., Fig. 5 shows a single false-negative comparison versus StreamPETR at similar latency). More comparsion should be provided.\n3. Table 1 is hard to interpret. It mixes different backbones (R101, ViT-B, ViT-L), image widths (×800, ×1408, ×1600), and sparsity levels in one block, so SToRe3D’s gains are confounded by backbone/resolution changes."}, "questions": {"value": "Why do mAP and NDS move in different directions in Table 2?\nHow sensitive are results to $H$ and $d_{min}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l87ZU2cc8D", "forum": "GfT31twtCw", "replyto": "GfT31twtCw", "signatures": ["ICLR.cc/2026/Conference/Submission23523/Reviewer_QpEr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23523/Reviewer_QpEr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658925515, "cdate": 1761658925515, "tmdate": 1762942698599, "mdate": 1762942698599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "NunPfOIJFW", "forum": "GfT31twtCw", "replyto": "GfT31twtCw", "signatures": ["ICLR.cc/2026/Conference/Submission23523/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23523/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763121268428, "cdate": 1763121268428, "tmdate": 1763121268428, "mdate": 1763121268428, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SToRe3D, a relevance-aligned sparsity framework that jointly prunes 2D tokens and 3D queries while storing filtered features for selective reuse. To better evaluate relevance-driven efficiency, the authors also propose nuScenes-Relevance (nuScenes-R), a new benchmark focusing on planning-critical agents for real-time multi-view 3D detection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Figures 1 and 2 are clear and effectively illustrate the motivation and framework design.\n- This paper introduces the concept of object relevance, which aligns sparsity with planning through a future interaction corridor in BEV space. Based on this idea, the authors further propose the nuScenes-R benchmark with relevance-based evaluation metrics.\n- The SToRe3D achieves solid results on nuScenes mAP and nuScenes-R, validating its effectiveness."}, "weaknesses": {"value": "- Limited FPS Improvement:\nAlthough SToRe3D aims to enhance efficiency via token and query pruning, the actual FPS gain over the ToC3D baseline (Table 1) is minimal.\n- Inconsistent Table Formatting:\nTable 1 and Table 2 use inconsistent decimal precision, which reduces clarity and professionalism.\n\n- Figure 4: Missing key baselines (e.g., StreamPETR, ToC3D, PETRv2). The x-axis should not use a log scale, and the latency intervals need to be labeled more clearly to show the actual differences between methods.\n- Figure 5: The visualization is confusing. The authors should explicitly annotate false negatives (left image) and correct detections within the interaction corridor (right image). More diverse examples would help clarify the qualitative improvements.\n- In the Pruning Design section, the authors claim latency reduction, but Table 3 does not include any latency or speed metrics. Including these would make the ablation analysis more convincing.\n\n- The paper contains numerous typos and grammatical errors that hinder readability.\nLine 373: “provides am ablation study” → “provides an ablation study”. Line 323: “To the best of our knowledge the first time transformer models of this scale reach real-time on nuScenes” → Missing comma and awkward phrasing. Line 466: “Since nuScenes-R relies on corridor hyperparameters (H, dmin) future work” → Missing comma after “(H, dmin)”."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AVHEc6zg3i", "forum": "GfT31twtCw", "replyto": "GfT31twtCw", "signatures": ["ICLR.cc/2026/Conference/Submission23523/Reviewer_Wqed"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23523/Reviewer_Wqed"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797900179, "cdate": 1761797900179, "tmdate": 1762942698314, "mdate": 1762942698314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SToRe3D, a framework for efficient multi-view 3D object detection using Vision Transformers (ViTs). \n\nThe method jointly prunes 2D image tokens and 3D object queries via mutual 2D–3D relevance heads that score elements according to their planning importance. \n\nThe authors further propose nuScenes-Relevance (nuScenes-R), a benchmark emphasizing accuracy on planning-critical agents. Experiments on nuScenes demonstrate that SToRe3D achieves up to 3× faster inference than dense ViT baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Unified sparsity across tokens and queries with mutual relevance heads is a creative and technically sound contribution.\n\n2. Comprehensive experiments across multiple ViT scales and sparsity regimes; consistent improvements over competitive baselines.\n\n3. nuScenes-R offers a principled way to evaluate planner-aligned perception; its release would benefit the field."}, "weaknesses": {"value": "1. Writing and figure quality need substantial improvement.\na. Figure 1 is visually cluttered and lacks clear legends, making it difficult to interpret the roles of tokens, queries, and buffers.\nb. Key terms such as planning-critical agents and store–reactivate are introduced abruptly; they should be explained concisely and intuitively before the formal equations.\n\n2. The paper lacks a per-module latency breakdown, making it unclear where the reported speedup originates. Providing detailed runtime analysis would help substantiate the efficiency claims.\n\n3. The visual examples (Figure 5) are limited and lack clarity. It is not evident where the detection boxes are in the right image. More qualitative results and better annotations would greatly improve interpretability.\n\nOverall, the paper feels rushed and not yet ready for publication"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9ZohGnMS6D", "forum": "GfT31twtCw", "replyto": "GfT31twtCw", "signatures": ["ICLR.cc/2026/Conference/Submission23523/Reviewer_8D9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23523/Reviewer_8D9F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802934287, "cdate": 1761802934287, "tmdate": 1762942698074, "mdate": 1762942698074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents SToRe3D, a method to accelerate ViT-based multi-view 3D detection by jointly sparsifying 2D image tokens and 3D object queries. \n\nThe proposed method learns to identify and prioritize \"planning-relevant\" content, using a store-and-reactivate mechanism to avoid irreversible information loss.  Besides, a new benchmark, nuScenes-R, is proposed to measure performance on these critical objects."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The work addresses the highly relevant problem of inference latency for large-scale ViT-based 3D detectors. The proposed method is intuitive and reasonable.\n\n2. Experiments show that the empirical gains are promising\n\n3. NUSCENES–R is a valuable asset with clear motivations."}, "weaknesses": {"value": "1. The proposed method is very similar to \"Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors, CVPR 23\". Please discuss your difference with this method. Proper citation might be required. \n\n2. It is difficult to disentangle the performance gains of the novel 'store-reactivate' idea from the general, already-explored strategy of hierarchical token reduction."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5qXMEPb1Ry", "forum": "GfT31twtCw", "replyto": "GfT31twtCw", "signatures": ["ICLR.cc/2026/Conference/Submission23523/Reviewer_KjxL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23523/Reviewer_KjxL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903367442, "cdate": 1761903367442, "tmdate": 1762942697837, "mdate": 1762942697837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}