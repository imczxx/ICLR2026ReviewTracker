{"id": "7ExZkCQU5Z", "number": 11198, "cdate": 1758193114292, "mdate": 1759897601568, "content": {"title": "Selective Labeling with False Discovery Rate Control", "abstract": "Obtaining high-quality labels for large datasets is expensive, requiring massive annotations from human experts. While AI models offer a cost-effective alternative by predicting labels, their label quality is compromised by the unavoidable labeling errors. Existing methods mitigate this issue through selective labeling, where AI labels a subset and human labels the remainder. However, these methods lack theoretical guarantees on the quality of AI-assigned labels, often resulting in unacceptably high labeling error within the AI-labeled subset. To address this, we introduce $\\textbf{Conformal Labeling}$, a novel method to identify instances where AI predictions can be provably trusted. This is achieved by controlling the false discovery rate (FDR), the proportion of incorrect labels within the selected subset. In particular, we construct a conformal $p$-value for each test instance by comparing AI models' predicted confidence to those of calibration instances mislabeled by AI models. Then, we select test instances whose $p$-values are below a data-dependent threshold, certifying AI models' predictions as trustworthy. We provide theoretical guarantees that Conformal Labeling controls the FDR below the nominal level, ensuring that a predefined fraction of AI-assigned labels is correct on average. Extensive experiments demonstrate that our method achieves tight FDR control with high power across various tasks, including image and text labeling, and LLM QA.", "tldr": "", "keywords": ["uncertainty quantification", "selective labeling", "conformal inference"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b889648261d30c51804f1741c2c022b8aa471bdd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the problem of selecting a subset of reliable labeled examples from data labels produced by an AI  system. The paper proposes a variant of the FDR algorithm and provides a theoretical coverage guarantee."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is very simple and easy to implement. It just sets a threshold on the prediction confidence and selects all the samples whose confidence is above this threshold.\nThe paper proves a theoretical coverage guarantee."}, "weaknesses": {"value": "see questions"}, "questions": {"value": "Some of the datasets such as Imagenet have a large number of classes. There are uncertainty (conformity)  score functions tailored to large class sets such as Regularized Adaptive Prediction Sets (RAPS). It makes more sense to use them instead of MSP, which was used in the paper.  \n\nThe proposed method seems to me as a variant of  FDR.  There is an ablation study in the paper that compares the proposed method to other variants of FDR. I think that these should be the baseline for comparison in Table 1. The difference between these FDR variants and the current method should be made clearer.\n\nI think that an end-to-end experiment, which comprises label selection and using the selected samples to train a better system, can help to validate the effectiveness of the proposed system."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1VF1yfBWcE", "forum": "7ExZkCQU5Z", "replyto": "7ExZkCQU5Z", "signatures": ["ICLR.cc/2026/Conference/Submission11198/Reviewer_UxdM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11198/Reviewer_UxdM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760556976959, "cdate": 1760556976959, "tmdate": 1762922348412, "mdate": 1762922348412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a procedure for safe automatic labeling: for each example, it either trusts the model’s prediction or defers to a human, while guaranteeing that the fraction of incorrect auto-labels stays below a chosen target level (FDR control)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Statistical guarantee: the method provides a finite-sample FDR guarantee (with proof provided in the appendix) on the fraction of incorrect labels among the accepted subset.\n\nApplicability: the procedure treats the base model as a black box and only uses its confidence scores plus a small labeled calibration set. This makes it directly applicable to frozen large models without needing to train an abstention head, add a reject class, or modify the original architecture.\n\nExperiments: the FDR guarantee is illustrated across diverse tasks and sensitivity experiments are provided."}, "weaknesses": {"value": "Guarantee interpretation: the core guarantee controls false discovery rate in expectation over the randomness of the procedure, not per run. This makes it possible that a single run still contains an error rate above the target, which could be acknowledged more clearly in the paper (but this is not specific to this method in particular, but to CP based methods in general).\n\nAdditional baselines:  the experimental comparison could include additional baselines, such as abstention / selective prediction baselines (e.g. [1]).\n\n[1] Geifman, Y., and El-Yaniv, R. “Selective Classification for Deep Neural Networks.” NeurIPS 2017."}, "questions": {"value": "Could the authors provide additional baselines?\n\nHow to handle the case where the iid assumption or exchengability between the calibration and the test set does not hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OgviLHYPRq", "forum": "7ExZkCQU5Z", "replyto": "7ExZkCQU5Z", "signatures": ["ICLR.cc/2026/Conference/Submission11198/Reviewer_CG4c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11198/Reviewer_CG4c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604064518, "cdate": 1761604064518, "tmdate": 1762922347940, "mdate": 1762922347940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a way to use an AI model to label part of a dataset while keeping the fraction of wrong AI labels under control. The method is called Conformal Labeling. It treats each test example as a hypothesis test. It compares the model’s confidence on that example with confidences observed on a small calibration set where the model is known to be wrong. This produces a p value per example. The method then selects the largest set of examples whose p values pass a data driven threshold, giving a guarantee that the false discovery rate stays below a user chosen level. The authors prove the guarantee and show experiments on image classification, text labeling, and multiple choice question answering with language models. They report tight false discovery rate control and strong coverage. For example, on ImageNet with ResNet 34 they can let the model label about fifty nine percent of the data while keeping the false discovery rate below ten percent."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Framing selective labeling as multiple testing with conformal p values is a clean and useful idea. The paper is not just another confidence thresholding scheme. It borrows mature tools from conformal inference and false discovery rate control, and adapts them to this labeling setting.\n\nThere is a clear theorem that states the procedure controls false discovery rate at the chosen level, under standard iid sampling of calibration and test points. The selection rule resembles Benjamini–Hochberg but is modified to account for the number of misclassified calibration examples. The paper also studies the choice of uncertainty score and shows that maximum softmax probability tends to separate correct and incorrect predictions better than the energy score in their setup.\n\nThe narrative is easy to follow. The three steps are named clearly: compute uncertainty scores, compute conformal p values, and apply thresholding. The method section states the null and alternative in plain terms\n\nSelective labeling is widely needed in practice when annotation budgets are tight. Prior work either used heuristics or guaranteed only the overall dataset error by relying on humans to cancel model mistakes. By guaranteeing the quality of the AI labeled subset itself, this work addresses a real pain point."}, "weaknesses": {"value": "The key guarantee assumes that calibration and test come from the same distribution and that the calibration set identifies a sufficient number of model mistakes. In real pipelines, shifts are common and the model may be updated after calibration. The paper would benefit from stress tests with shift between calibration and test, and from guidance on how often to refresh calibration in a streaming setting.\n\nThe power analyses show that maximum softmax probability and the alpha score work well, while the energy score does not separate as nicely. This means the method inherits weaknesses of the uncertainty score. The paper adopts maximum softmax probability for the main results because it is simple.\n\nThe baselines are a naive score threshold and labeling everything with the model. These are useful, but the paper would feel stronger with comparisons to more advanced selective prediction or abstention systems that use calibrated confidence, temperature scaling, or selective risk minimization, even if those do not provide the same guarantee.\n\nFor multiple choice QA, the method treats answer options as classes and takes logits for option tokens. This is standard, but many modern evaluations use chain of thought or candidates longer than a single token. The method should clarify how to handle multi token options and prompt sensitivity."}, "questions": {"value": "How sensitive is the guarantee and the realized power to moderate shift between calibration and test?\n\nHave you tried training a small auxiliary model to predict correctness on the calibration set, and then using that learned score in place of maximum softmax probability?\n\nIf I already run a selective prediction model that abstains based on calibrated confidence, what concrete benefit do I get by wrapping your conformal procedure around it? Does your threshold reliably increase the selected set size at the same target false discovery rate, and can you show this on at least one dataset in the main results instead of only in the ablation figure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FmtouSoQ8F", "forum": "7ExZkCQU5Z", "replyto": "7ExZkCQU5Z", "signatures": ["ICLR.cc/2026/Conference/Submission11198/Reviewer_ofxs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11198/Reviewer_ofxs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980873991, "cdate": 1761980873991, "tmdate": 1762922347624, "mdate": 1762922347624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new method for assuring the quality of AI model-based labeling of datasets. The method aims to find the maximum-size subset of the dataset where AI's errors (false discovery rate; FDR) can be below a given rate in expectation, while maximizing ratio of correct labels. The paper proposes to compute a p-score based on a score function (i.e. AI model's prediction confidence), which is then thresholded to decide what samples to allocate to the model for annotation: the authors provide marginal guarantee for the FDR to be under a certain level. The authors test their method with different datasets/tasks and models, as well as under various ablations.\n\nOverall I have to recommend the rejection of the paper as in my understanding 1- the utility of what they are trying to achieve is not very clear (see weaknesses, pt. 1), and 2- they do not deliver on these claims fully, at least in comparison to their presentation in the abstract and the introduction (see weaknesses, pt. 2). I cannot recommend a borderline rejection since these issues are core problems rather than superficial ones. However, if the review scale allowed I would have assigned this paper a 3, since I believe the overall direction of the paper is important, and their approach has some merit."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Given the increasing need for AI-based annotation of datasets, the topic is very timely and important.\n- The authors overall motivation is valid: being able to provide guarantees for subsets of a dataset is an important target, which could potentially offer utility that is qualitatively different than the dataset-wise guarantees (but see below)\n- The experiments contain a good variety in datasets and models for testing the method.\n- The authors conduct ablation experiments with score functions, calibration set sizes, and threshold selection method which is very helpful for a conformal inference-based method."}, "weaknesses": {"value": "- I think the motivation of the paper is not very clear. The paper states that \"This limitation underscores a critical gap: existing selective labeling methods cannot ensure the quality of AI-assigned labels, hindering their reliable deployment in real-world applications.\" Why would be separately interested in AI-labeled instances' reliability - isn't the overall dataset label reliability the fundamental objective? I can understand this reasoning if the current method was improving overall performance by utilizing a different source for the score function/heuristic than model certainty to determine annotator allocation (human vs. AI). However this is not the case as both papers are using AI model confidence, as stated in L151 for the current paper.\n- Moreover, it seems like the paper underdelivers relative to its claims. Although their procedure controls the FDR of the subset selected, it controls it *in expectation*. So, the authors' criticism of Candes et al. 2025 that \"the labeling error of AI models can be unacceptably high, even reaching 100%\" (L048) could apply to their own method as well. In fact, the high-probability, dataset-wise guarantee provided by the prior work can be considered preferable as it is a sample-specific guarantee. Again, although the explicit control over FDR vs. power provided by the current work is not unimportant, this can be emulated by Candes et al. 2025's method by increasing the human-annotated instance set size in the test set. If I am correct about the nature of the guarantee provided by the paper, then claims like \"In this work, we study the problem of identifying instances where AI predictions can be provably trusted.\" (L090) are not substantiated. Although some experimental findings are helpful (L364), I think it is not appropriate to relegate the discussion and addressing of this core issue to empirical experiments."}, "questions": {"value": "- Please fix the page-long references in the bibliography\n- L047: I do not exactly understand this criticism. In selective labeling the reasonable assumption is that a considerable portion of the dataset will be annotated by AI. Is there a realistic scenario under PAC labeling where AI error is 100%?\n- L054: I think this part is hard to understand without the technical prelude. So the authors can go for a more verbalistic summary or provide a little more technical background\n- L077: FDR control has already been discussed, it reads redundant for it to be repeated in every bullet point in the contributions.\n- L098: What does \"Since AI models are typically prone to labeling errors\" mean? I.e. do they make errors when used for labeling or are they affected by labeling errors?\n- L102: What is the expectation over? Please explicitly state. Also goes for L111.\n- L108: \"labeling as many test instances as possible *correctly*\"\n- L189: $\\mathcal{S}$ is treated both as a function (L155) and a random variable without any acknowledgment\n- L191: Why call the LHS variable $\\hat{p}\\_j$ instead of $\\hat{p}\\_{n+j}$?\n- L216: I agree with the authors' claim that energy score is worse; but y-axes sharing among the figures can make this more obvious\n- L300: I feel by this point conformal labeling can be abbreviated (CL)\n- L459: Redundant period before citations.\n- L465: Reference to the selective prediction methods is a nice touch"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jkZ0MN4GE7", "forum": "7ExZkCQU5Z", "replyto": "7ExZkCQU5Z", "signatures": ["ICLR.cc/2026/Conference/Submission11198/Reviewer_WZEv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11198/Reviewer_WZEv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033635815, "cdate": 1762033635815, "tmdate": 1762922347199, "mdate": 1762922347199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}