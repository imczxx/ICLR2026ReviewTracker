{"id": "ztGHhyicWs", "number": 12848, "cdate": 1758210924697, "mdate": 1759897481572, "content": {"title": "Adaptive Test-Time Compute Allocation via Training-Free Difficulty Proxies", "abstract": "Large language models (LLMs) excel at complex tasks but incur prohibitive computational costs, particularly when using techniques like self-consistency that require multiple generation attempts. This paper addresses the challenge of adaptive test-time compute allocation. We propose a framework that leverages **training-free difficulty proxies** derived directly from the LLM generation process to distribute a fixed compute budget across the test queries, without requiring specialized training for the allocation mechanism. Our objective is to maximize the number of solved instances by dynamically allocating more compute to difficult instances and less to simpler ones, while adhering to a total budget constraint. We first introduce several training-free proxies and empirically demonstrate their effectiveness in estimating instance difficulty. We then design an adaptive allocation strategy guided by these proxies, which is theoretically grounded in a novel bandit formulation. Experiments across math (MATH, GSM8K), coding (LiveCodeBench), and Q\\&A (e.g., GPQA-Diamond) benchmarks demonstrate that our method significantly outperforms both uniform budget allocation and training-based allocation baselines, solving substantially more problems under identical budget constraints. This work presents a practical and readily deployable approach to enhance the resource efficiency of LLM inference for demanding reasoning tasks.", "tldr": "We propose a framework for adaptive test-time compute allocation that leverages training-free difficulty proxies and requires no model fine-tuning.", "keywords": ["Test-Time Compute; LLMs; Adaptive Reasoning; Inference-Time Compute; Adaptive Allocation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dca669b587ef97c9d01d61fba1638bf00975ce10.pdf", "supplementary_material": "/attachment/85e0e934b644b462677040c2c8c11783247ffa4c.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of adaptive test-time compute allocation for large language models (LLMs). Traditional inference strategies, such as self-consistency and Best-of-N sampling, apply uniform compute budgets to all inputs, regardless of their difficulty. The authors propose DIPA, a training-free framework that dynamically allocates compute based on difficulty proxies derived directly from the LLM’s generation process (e.g., entropy, variance of gradient norms, generation length, and consistency). The method formulates adaptive allocation as a multi-armed bandit (MAB) problem, introducing probabilistic sampling to balance exploration and exploitation. Theoretically, they provide a regret bound showing that performance depends on the correlation between the proxy and true difficulty. Experiments on reasoning and coding benchmarks demonstrate that DIPA significantly outperforms uniform and training-based baselines under fixed compute budgets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed solution is interesting. The reformulation of compute allocation as a multi-armed bandit with arm elimination is well-justified.\n- The paper systematically investigates multiple training-free proxies (entropy, gradient norms, generation length, etc.) and analyzes their correlation with oracle difficult\n- The paper is easy to follow."}, "weaknesses": {"value": "- The individual proxies (e.g., generation length, entropy) are adapted from prior uncertainty estimation works. The main contribution lies in combining and evaluating them.\n- The method's performance heavily depends on the proxy's correlation with true difficulty.\n- Some works on test-time scaling are missing, such as [1,2,3].\n\n[1] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving.\n\n[2] Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect Verifiers.\n\n[3] Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling."}, "questions": {"value": "- Can the authors provide some guidelines for how to use these proxies in practical scenarios? For example, how to achieve good performance when encountering a brand new dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kKNjGmqXIO", "forum": "ztGHhyicWs", "replyto": "ztGHhyicWs", "signatures": ["ICLR.cc/2026/Conference/Submission12848/Reviewer_MUor"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12848/Reviewer_MUor"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746269589, "cdate": 1761746269589, "tmdate": 1762923644289, "mdate": 1762923644289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on adaptive test-time compute allocation for LLM reasoning under a fixed budget, arguing that uniform “N-samples per instance” and training-based difficulty predictors are either inefficient or costly to train/deploy. The authors instead propose training-free difficulty proxies and formulates allocation as a stochastic MAB with arm-elimination, introducing DIPA, which samples instances with probability inversely proportional to their current difficulty, initializes from cheap input-based priors, and updates online using generation-based proxies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses the issue that training-based difficulty predictors are inefficient or costly, provide training-free difficulty proxies insights, and present corresponding experimental results."}, "weaknesses": {"value": "- I believe that the Easy2Hard strategy aligns with DIPA’s fundamental principle of “probabilistically prioritizing arms (instances) estimated to be easier.” The reason Easy2Hard underperforms in the experiments is likely due to inaccurate initial difficulty estimation. Did the authors compare the performance of Easy2Hard when using existing training-based difficulty predictors to estimate difficulty before allocation?\n- The proposed method requires sequential rollouts, which sacrifices parallelism. Although the overall compute budget is fixed, this design increases wall-clock time. While the authors mention that DIPA could, in theory, be extended to a batched version, I think that sampling multiple instances simultaneously under the same probability distribution would likely result in over-selecting easy instances, leading to budget waste on already simple problems."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hSLKyimh0u", "forum": "ztGHhyicWs", "replyto": "ztGHhyicWs", "signatures": ["ICLR.cc/2026/Conference/Submission12848/Reviewer_cYP5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12848/Reviewer_cYP5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806634713, "cdate": 1761806634713, "tmdate": 1762923644003, "mdate": 1762923644003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the inefficiency of uniform test-time compute allocation for LLMs and proposes DIPA, a training-free framework for adaptive compute distribution. It leverages training-free difficulty proxies derived from LLM inputs or generation processes to estimate instance difficulty. It reformulates the allocation problem as a multi-armed bandit task with arm elimination upon success, using a probabilistic policy to balance exploration and exploitation. Experiments on varies benchmarks show DIPA outperforms uniform allocation, deterministic strategies, and training-based baselines, solving more problems under fixed budgets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The proposed method achieves effective inference-budget allocation without any additional training, outperforming baselines such as BoN and SC under the same compute budget.  \n2) The formalization as a multi-armed bandit and the accompanying regret-bound analysis link proxy quality to performance, providing a theoretical grounding for the approach."}, "weaknesses": {"value": "1) The experimental baselines are limited to relatively easy problems, the authors should include AIME24, a benchmark commonly used in recent reasoning-model studies, which demands greater reasoning capability and larger budgets and would better highlight the proposed method’s incremental value.  \n2) The selected training-free proxies are rather trivial.  \n3) Although casting budget allocation as a multi-armed bandit is interesting, it is not “a novel bandit formulation” but a very classical algorithmic template; the authors should acknowledge this.  \n4) The computational overhead of obtaining the difficulty proxies must also be accounted for—comparing only inference budgets is insufficient, explicit end-to-end run-time comparisons with each baseline are necessary."}, "questions": {"value": "Please refer to the \"Weakness\" part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZUfdCwHHiv", "forum": "ztGHhyicWs", "replyto": "ztGHhyicWs", "signatures": ["ICLR.cc/2026/Conference/Submission12848/Reviewer_uSZC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12848/Reviewer_uSZC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912058575, "cdate": 1761912058575, "tmdate": 1762923643706, "mdate": 1762923643706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of adaptive resource allocation during test-time scaling, proposing a training-free difficulty proxy metric and providing analysis and proofs regarding the modeling and associated regret bounds via multi-armed bandits. The effectiveness of the approach is demonstrated across mathematical, programming, and document-related problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem studied in this paper is highly important and practical.\n- The proposed method has a certain mathematical foundation, though its relevance to practice remains limited."}, "weaknesses": {"value": "- The problem is oversimplified, particularly in terms of measuring computational budget solely based on the number of steps. More specific measurements, such as FLOPs and time, are needed.\n- While the paper emphasizes the theoretical contributions of the method, further demonstration of its practical application value and significance is required.\n- Evaluations of computational overhead and FLOPs for inference deployment should be provided with specific calculations."}, "questions": {"value": "- The paper also mentions the application of reward models and suggests incorporating more procedural information for difficulty analysis. It is recommended to consider the application analysis of generative reward models, such as GenGRM:\n\n[1] GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rdZ0qBttvG", "forum": "ztGHhyicWs", "replyto": "ztGHhyicWs", "signatures": ["ICLR.cc/2026/Conference/Submission12848/Reviewer_t29B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12848/Reviewer_t29B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762647758091, "cdate": 1762647758091, "tmdate": 1762923643444, "mdate": 1762923643444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}