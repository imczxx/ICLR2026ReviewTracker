{"id": "oOf83pSxUP", "number": 9089, "cdate": 1758110331001, "mdate": 1759897744412, "content": {"title": "Old N-Grams Never Die:Towards Identifying LLMs-Generated Text using Antique N-Grams", "abstract": "The proliferation of large language models (LLMs) has triggered an influx of AI-generated content, making robust detection of such content paramount for maintaining academic, journalistic, and regulatory integrity. However, the community has largely overlooked a time-tested resource that classical n-gram models, trained exclusively on human-authored corpora, may serve as a de facto gold standard for identifying machine-generated writing. In this paper, we build upon well-trained pre-AI N-Gram models to form the backbone of a lightweight AI-text detection system called \\textbf{GramGuard}. Specifically, by generating paraphrased variants via temperature-controlled decoding from LLMs, we measure the shifts in log-likelihood, entropy, and token frequency variance between original texts and perturbed versions. These \\emph{delta} features then feed into an ensemble classifier to yield interpretable decisions about authorship. Extensive experiments on PubMed, WritingPrompts, and XSum demonstrate that \\textbf{GramGuard} matches or exceeds state-of-the-art detectors in performance and robustness. Our findings reaffirm the enduring value of pre-AI n-gram models and introduce a scalable, transparent solution for AI-text detection.", "tldr": "", "keywords": ["Antique N-Grams", "LLM Paraphrasing", "Machine-Text-Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14ae05281ea468a284d570a13e22585e64c93e56.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces GramGuard, a supervised AI text detector that uses pre-LLM n-gram models as a gold standard for human writing. The method works by paraphrasing a given text and measuring the shift in its log-likelihood, entropy, and token variance - which is then fed into a classifier."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- This work is very well motivated and highlights a curious question in this domain."}, "weaknesses": {"value": "- The submission violates the double-blind review policy. The authors should anonymize their code and model releases (AnonymousGitHub) and avoid providing GitHub and HuggingFace repository links that can potentially reveal the authors' identities.\n- The mathematical notation is often incorrect -- eqn. 3 and 4 is mathematically inaccurate and difficult to interpret. Moreover, I believe the authors must clarify the meaning of $p_{\\mathbb{N}}(g_{\\mathbb{N}}^i)$ and ensure that all symbols are clearly defined and used consistently across the paper.\n- Is there any formal / empirical proof for Corollary 1.1? I don't believe Figure 1 is sufficient for proving this claim.\n- The framework is described as efficient, yet the implementation contradicts this claim. The authors generate 60 paraphrased samples per instance per LLM for feature generation, which is computationally expensive, particularly during inference.\n- Based on the described training flow of GramGuard, it appears that the model may overfit to the specific datasets and LLMs it was trained on. As a result, I suspect that the performance on unseen domains or models is likely to degrade substantially. The authors should include OOD evaluation results, testing the detector on the dataset from these papers [1, 2].\n- The paper repeatedly claims robustness to paraphrasing; however, since paraphrasing is explicitly used as part of the training process, this robustness is not particularly surprising or novel.\n- The citation formatting throughout the paper is incorrect, significantly compromising readability. Please use the correct citation command.\n\n---\n\n[1] Li et al. MAGE: Machine-generated Text Detection in the Wild. arXiv: 2305.13242.\n\n[2] Dugan et al. RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors. arXiv: 2405.07940."}, "questions": {"value": "Several questions have already been raised in the Weaknesses. I am willing to increase my score if the authors address the concerns above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uMGgWVwqRe", "forum": "oOf83pSxUP", "replyto": "oOf83pSxUP", "signatures": ["ICLR.cc/2026/Conference/Submission9089/Reviewer_prgt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9089/Reviewer_prgt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775460523, "cdate": 1761775460523, "tmdate": 1762920793827, "mdate": 1762920793827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author’s propose GramGuard, an approach which leverages n-gram LMs on pre-LLM human corpora to detect machine-generated text. Given the ngram-LLM, the authors extract the log-likelihood, the entropy, and the variance which get fed into XGBoost (which is trained) for classification. They show that GramGuard is robust under their testing scenarios."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* S1 - The author’s show the power of simple N-Gram LM models trained on pre-LM human corpora. \n* S2 - The author’s not only consider the likelihood of the model as usual with many detectors, but also look at the entropy and variance. One could imagine using these features in other settings as well.\n* S3 - The method doesn’t require training on machine-text."}, "weaknesses": {"value": "* W1 - It’s unclear whether the experiments in 5.2 were performed on a separate validation set. If not, it’s difficult to interpret the results as the best model would’ve been picked on test data. \n* W2 - The finding that a detector (in this case n-gram-based) trained on human corpora is effective at identifying LLMs seems related to this detector that makes a similar point: https://arxiv.org/pdf/2401.06712 The authors should compare GramGuard to it given its similarity.\n* W3 -  Most of the models compared against seem to be from the ChatGPT family, except for Gemini. The authors could evaluate across the various test-beds of the MAGE dataset (https://arxiv.org/pdf/2305.13242) for a more complete evaluation. This dataset contains the OpenAI family, the Llama family, GLM, Flan-T5, and others. Moreover, they have test beds that control for various interesting factors.\n* W4 - While the N-Gram model was trained only on pre-LLM data, the XGBoost model was trained on LLM and Human data, correct? If so, were the models only evaluated in in-domain settings where the testing data matches the training data? Or was it set up so that XGBoost was trained on PubMed and evaluated on XSum, for example. There seem to be many details regarding the training of XGBoost missing. \n* W5 - There really only is one experimental result and an ablation. More things could’ve been evaluated, as for example the performance when XGBoost is trained on a different domain, the performance as the number of tokens grows, etc."}, "questions": {"value": "* Q1 - My first and most pressing concern is that it's not clear whether the hyper-parameters were chosen on a separate validation set or not (W1)\n* Q2 - Supposing my concern above (Q1) is addressed, then my main concern is W4, followed by W5 and W2. The cross-domain and cross-LLM robustness should've been evaluated, otherwise the results aren't very significant. If these concerns are addressed, I am willing to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sfrx6sqLOI", "forum": "oOf83pSxUP", "replyto": "oOf83pSxUP", "signatures": ["ICLR.cc/2026/Conference/Submission9089/Reviewer_iGoa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9089/Reviewer_iGoa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941983443, "cdate": 1761941983443, "tmdate": 1762920793488, "mdate": 1762920793488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for the supervised detection of LLM-generated text. The proposed method, GramGuard, involves (1) paraphrasing the target text many times with an LLM, (2) scoring the perplexity of the original text and paraphrased texts under an LLM, (3) computing deltas between these values, and (4) feeding the deltas into an XGBoost classifier."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. To the best of my knowledge, the idea of using n-gram models in a DetectGPT-style detector is novel\n2. While the paper would benefit from some additional proofreading for typos and consistency, it is overall pretty clear and easy to follow"}, "weaknesses": {"value": "1. However, the idea of using n-gram models in LLM-generated text detection is not novel. See, for example, Ghostbuster (Verma, et al. 2024) which uses probabilities from a range of unigram and trigram models as features in supervised classification\n\n2. My primary concern is that the baselines tested in this paper are relatively weak. In particular, many of the methods in Table 1 are unsupervised detectors, which are known to struggle when the scoring and target model differ. While the paper has some supervised methods (e.g., RoBERTa), these are relatively weak supervised baselines, cf. Verma, et al. 2024 for a comparison. I would recommend adding some stronger baselines like Ghostbuster, Binoculars, or some of the best models from the RAID benchmark. Additionally, if you are going to compare with a closed-source commercial detector like GPTZero, I would consider replacing that with the Pangram Labs detection model, which is substantially stronger.\n \n3. The paper seems to be missing some important experimental details about the baseline models. For example: how were the RoBERTa-based models trained? What scoring model was used on DetectGPT? Were classification thresholds tuned independently on each of your three domains? \n\n4. The paper would benefit from an additional round of proofreading. For example, there is a typo in the title (“LLMs-generated” -> “LLM-generated”), n-gram models are referred to as “N-Gram”, “n-gram”, and “Ngram” (lack of consistency), and \\citet is used in place of \\citep throughout the paper."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wlT2DvNUc7", "forum": "oOf83pSxUP", "replyto": "oOf83pSxUP", "signatures": ["ICLR.cc/2026/Conference/Submission9089/Reviewer_ArSt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9089/Reviewer_ArSt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993319033, "cdate": 1761993319033, "tmdate": 1762920793118, "mdate": 1762920793118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to leverage pre-AI N-Gram models exclusively trained on human corpora as the “gold standard” for AI text detection. Building on this foundation, this paper introduces GramGuard, a lightweight, interpretable framework that identifies machine-generated text through shifted statistics of paraphrastic variants. Specifically, by generating paraphrased variants via temperature-controlled decoding from LLMs, this paper measures the shifts in log-likelihood, entropy, and token frequency variance between original texts and paraphrased versions. These “shifts” features are then fed into an XGBoost model to yield interpretable decisions about authorship. Extensive experiments on PubMed, WritingPrompts, and XSum demonstrate that GramGuard matches or exceeds state-of-the-art detectors in performance and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe idea of using pre-AI N-gram models trained solely on human-written corpora as a scoring model for AI text detection is interesting.\n2.\tThe adopted XGBoost model helps improve the interpretability of the overall approach.\n3.\tThe method achieves state-of-the-art detection accuracy across three datasets and maintains robustness under paraphrastic attacks."}, "weaknesses": {"value": "1.\tThe proposed framework is not as lightweight as claimed in the paper since it involves large language models like GPT4 or Genimi for paraphrasing input samples.\n2.\tThe paper only uses responses from GPT-4_1-mini as AI responses and paraphrases with GPT3.4, GPT 4 model, and Gemini model. Note that GPT-4_1-mini, GPT3.4 and GPT 4 are from the same model family. Experiments with synthesized AI responses from one more model will help strengthen the paper to have a more robust and diverse evaluation"}, "questions": {"value": "1.\tThe citation format is not correct. Using \\citep{} and \\citet{} appropriately.\n2.\tIn Corollary 1.1, it says “Under perturbation, the process of rephrasing machine text tends to sample the tokens with lower probabilities compared with their original sample”. Is it true? And why?\n3.\tWhat is L in equation (6)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RB5KSQrCK2", "forum": "oOf83pSxUP", "replyto": "oOf83pSxUP", "signatures": ["ICLR.cc/2026/Conference/Submission9089/Reviewer_A6yE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9089/Reviewer_A6yE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999261303, "cdate": 1761999261303, "tmdate": 1762920792842, "mdate": 1762920792842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}