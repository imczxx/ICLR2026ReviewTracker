{"id": "8VyYHrm2xC", "number": 10743, "cdate": 1758180867718, "mdate": 1759897632154, "content": {"title": "Towards Stable and Effective Reinforcement learning for Mixture-of-Experts", "abstract": "Recent advances in reinforcement learning (RL) have substantially improved the training of large-scale language models, leading to significant gains in generation quality and reasoning ability. However, most existing research focuses on dense models, while RL training for Mixture-of-Experts (MoE) architectures remains underexplored. To address the instability commonly observed in MoE training, we propose a novel router-aware approach to optimize importance sampling (IS) weights in off-policy RL. Specifically, we design a rescaling strategy guided by router logits, which effectively reduces gradient variance and mitigates training divergence. Experimental results demonstrate that our method significantly improves both the convergence stability and the final performance of MoE models, highlighting the potential of RL algorithmic innovations tailored to MoE architectures and providing a promising direction for efficient training of large-scale expert models.", "tldr": "", "keywords": ["reinforcement learning", "Mixture-of-Experts"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf9e1be22c1a22fa1d73b1a4f116c827b866ba23.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "the paper presents a novel approach to optimize importance sampling of weights for off-polich RL specifically for RL training for Mixture-of-Experts (MoE) architectures that still suffers from instability. the authors present RSPO as an algorithm for this problem.  The core novelty of RSPO is the router shift ratio. Unlike prior attempts to stabilize MoE training by rigidly freezing the router or replaying old routing decisions, RSPO uses this ratio as a soft adjustment mechanism. It dynamically adapts the magnitude of policy updates based on the degree of routing drift for each token, maintaining router flexibility while preventing instability. The authors present experimental results on mathematical reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method specifically addresses unique MoE failure modes. RSPO demonstrates significantly more stable training than GRPO (reward collaps) and does outperform GSPO and GMPO on the evaluation tasks but only marginally. They provide some analysis on why the RSPO prevents collaps."}, "weaknesses": {"value": "I think the results are not super strong, especially comparing with GSPO and GMPO does not really show any significant benefit at least on the tasks that have been presented. Another weakness is that the evaluation tasks are all from within a very similar domain, raising questions about how well this method would work on a broader set of domains."}, "questions": {"value": "Have you done experiments on different tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bjQZU7YDyy", "forum": "8VyYHrm2xC", "replyto": "8VyYHrm2xC", "signatures": ["ICLR.cc/2026/Conference/Submission10743/Reviewer_mvEe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10743/Reviewer_mvEe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662879879, "cdate": 1761662879879, "tmdate": 1762921966752, "mdate": 1762921966752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies instability in reinforcement learning for Mixture-of-Experts (MoE) language models. The authors identify routing fluctuation, the changes in which experts are selected across policy updates, as a major source of variance in importance-sampling ratios.\nTo address this, they propose Router-Shift Policy Optimization (RSPO), which adds a router-shift coefficient gamma(i,t) to down-weight tokens that experience large changes in routing logits between the old and current policies. They also switch from sequence-level to token-level clipping to further stabilize training. Empirical results on Qwen2.5 and Qwen3-30B-A3B show that RSPO improves training stability and final accuracy across math-reasoning benchmarks compared to GRPO, GSPO, and GMPO."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper pinpoints a practical instability in MoE RL training.\n- The proposed fix is simple but sensible. The router-shift weighting makes intuitive sense and is easy to integrate.\n- The topic is timely and practically significant for large-scale RLHF/RLVR pipelines.\n- Experiments demonstrate performance boosts and stability improvements on competitive models. The ablations on router freezing and replay variants add useful context."}, "weaknesses": {"value": "- The novelty over GSPO/GMPO is limited. The router-shift term is the main difference, and there’s no theoretical analysis to back its variance-reduction claim.\n- Important details (values for K, gamma_min, and clipping thresholds) are missing, making it hard to judge reproducibility or sensitivity.\n- Lack of quantitative understanding of the stability improvement. The paper does not measure or visualize the actual variance reduction, distribution of importance ratios, or clipping rates. Including such diagnostics would clarify the mechanism behind the reported stability gains.\n- The experiments focus only on math reasoning; it’s unclear whether the method generalizes to other domains.\n- There’s no deeper analysis of why RSPO stabilizes training, for example, variance or clipping-rate plots would help.\n- Minor typos and formatting issues remain.\n- Some figure captions are not self-contained and require reading the main text for interpretation."}, "questions": {"value": "- Can you provide empirical or theoretical evidence that gamma(i,t) reduces gradient variance compared to GSPO/GMPO?\n\n- What exact hyperparameters were used (K, L, gamma_min, epsilon1, epsilon2), and how sensitive are results to them?\n\n- What’s the compute/memory overhead of storing router states?\n\n- Why mix geometric aggregation with token-level clipping? Would sequence-level clipping behave differently?\n \n- Were all baselines tuned under the same compute budget? GRPO seems under-optimized.\n\n- Do the gains hold in other RLVR setups, e.g., code synthesis or denser reward tasks?\n\n- Could you add diagnostics (IS-ratio distributions, clipping frequency, gradient-norm variance) to back the stability claim?\n\n- How were gradients handled in the router-replay variants?\n\n- How many random seeds were used in each experiment, and how consistent are results across seeds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are no ethics concerns to report."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Es4Jcgv1pu", "forum": "8VyYHrm2xC", "replyto": "8VyYHrm2xC", "signatures": ["ICLR.cc/2026/Conference/Submission10743/Reviewer_NVzX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10743/Reviewer_NVzX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868540351, "cdate": 1761868540351, "tmdate": 1762921966194, "mdate": 1762921966194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses  the instability of Reinforcement Learning (RL) training when applied to Mixture-of-Experts (MoE) architectures in scaling large language models., particularly the so-called \"router fluctuation\"—where the set of activated experts for a given token changes between policy updates—as a primary source of high variance and training divergence in off-policy RL like GRPO.  The authors proposes a  new algorithm, Router-Shift Policy Optimization (RSPO), which introduces a \"router shift ratio\" to quantify and penalize excessive routing deviations. Extensive experiments on various mathematical reasoning benchmarks show some promising applications."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper is well-motivated. It deals with a challenging problem and proposes a well-explored solution. \n(2)The paper performss a Comprehensive Empirical Validation: The evaluation is rigorous, using both small-scale ablation studies and large-scale models across five diverse mathematical reasoning benchmarks. \n(3) The algorithm design is well-elaborated."}, "weaknesses": {"value": "(1) Limited Task and Model Scope:  The paper's results are limited to mathematical reasoning tasks and Qwen family of models. \n(2) Ablation Study Depth: While the main components are justified, a more detailed ablation study within RSPO itself—for instance, isolating the individual contribution of the router shift ratio from the geometric mean aggregation—would provide deeper insight into which aspect is most critical for the observed gains."}, "questions": {"value": "(1) Generalization: How do you anticipate RSPO would perform on non-reasoning tasks or MoE architectures with fundamentally different routing mechanisms (e.g., Expert Choice)? Do you think the router shift ratio is universally applicable?\n(2) Reward Function Sensitivity: The experiments use a rule-based reward for mathematical correctness. How sensitive is RSPO's stability to the nature of the reward signal, particularly with denser or sparser rewards?\n(3)  Cost: Could you elaborate on the observed computational overhead of RSPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eNf1LnUBzA", "forum": "8VyYHrm2xC", "replyto": "8VyYHrm2xC", "signatures": ["ICLR.cc/2026/Conference/Submission10743/Reviewer_pQzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10743/Reviewer_pQzx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877364550, "cdate": 1761877364550, "tmdate": 1762921965737, "mdate": 1762921965737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the training instability of applying reinforcement learning to Mixture-of-Experts (MoE) models, a problem caused by \"router fluctuation\"—inconsistent expert selection for the same token across policy updates. This fluctuation creates high variance in importance sampling weights, often leading to training instabilities. The authors propose an algorithm that introduces a \"router shift ratio\" to quantify this routing deviation on a per-token basis. This ratio is then used to softly down-weight the importance sampling weights of tokens with significant drift. Experimental results on a suite of benchmarks, especially mathematical reasoning, show that this \"soft adjustment\" mechanism leads to more stable training and improved final performance over standard RL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method addresses a contemporary stability problem at the heart of the LLM post training pipeline.\nThe paper presents convincing empirical evidence that the proposed methods work on modern open weight MoE models such as Qwen3-30B-A3B and on a range of contemporary RL benchmark tasks. \nLast but not least, while maybe a bit ad-hoc, the proposed solution is relatively simple and boils down to a soft-regularization term in contrast to some recent alternative methods that instead propose hard constraints."}, "weaknesses": {"value": "While the paper presents end-to-end results for the proposed methods, there are only few detailed analysis and ablation studies, even though the design includes several choices such as using the absolute log-difference or aggregating them multiplicatively. While these choices seem intuitively reasonable, they are often neither theoretically or experimentally confirmed."}, "questions": {"value": "Pretraining often involves auxiliary load-balancing losses. How do these interact with the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nNdsSXphhg", "forum": "8VyYHrm2xC", "replyto": "8VyYHrm2xC", "signatures": ["ICLR.cc/2026/Conference/Submission10743/Reviewer_P5EK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10743/Reviewer_P5EK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954398681, "cdate": 1761954398681, "tmdate": 1762921965197, "mdate": 1762921965197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}