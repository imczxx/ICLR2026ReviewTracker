{"id": "qxDTe8fIyA", "number": 24355, "cdate": 1758356029947, "mdate": 1759896769819, "content": {"title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning", "abstract": "Long-context reasoning requires accurately identifying relevant information in extensive, noisy input contexts. In this work, we propose PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for learning to encode long contexts using gradient updates at test time. Specifically, PERK employs two nested optimization loops in a meta-training phase. The inner loop rapidly encodes contexts into a low-rank adapter (LoRA) that serves as a parameter-efficient memory module for the base model. Concurrently, the outer loop learns to use the updated adapter to accurately recall and reason over relevant information from the encoded long context. Our evaluations on several long-context reasoning tasks show that PERK significantly outperforms the standard long-context finetuning, achieving average absolute performance gains of up to 20% for Qwen-2.5 (0.5B & 7B) on synthetic and real-world long-context reasoning. PERK also maintains its advantages across model scales and families. Compared to specialized long-context LLMs, PERK matches or surpasses their performance. Finally, our analyses show PERK is more robust to reasoning complexity, length extrapolation, and the positions of relevant information in contexts.", "tldr": "A scalable meta-learning method that learns to encode long contexts through gradient updates to a model adapter at test time, achieving long-context reasoning robust to complexity and length extrapolation.", "keywords": ["test-time learning", "long-context reasoning", "meta-learning", "reasoning algorithm", "length extrapolation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/948e47a9fc74a6de6ac31772c98f16f0953c7a7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PERK (Parameter Efficient Reasoning over Knowledge), a novel method that reframes long-context reasoning as a test-time learning problem. Instead of processing a long context in a single forward pass, PERK first encodes the context into the parameters of a Low-Rank Adapter (LoRA) via several gradient-based update steps at inference time. The model then answers a given question using this newly created parameter-efficient memory module, without needing the original long context in its attention window. To enable this, PERK is trained using a bi-level meta-learning optimization scheme: an inner loop learns to rapidly encode context into the LoRA adapter, while an outer loop learns to use the adapted model for reasoning. To make this process scalable, the authors employ truncated gradient unrolling. The paper demonstrates through extensive experiments that PERK significantly outperforms standard long-context fine-tuning, shows superior length extrapolation and robustness to positional biases, and is competitive with or exceeds the performance of specialized proprietary and open-source long-context models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Novel Conceptual Framing:** The paper's most significant contribution is reframing long-context reasoning as a test-time, gradient-based adaptation problem. This is a fundamental and clever departure from the standard in-context reasoning (ICR) paradigm, moving from \"reasoning over context\" to \"reasoning from context stored in parameters.\"\n\n**Exceptional Empirical Performance and Robustness:** PERK demonstrates substantial performance gains (up to +20% absolute) over standard fine-tuning. More importantly, the method shows remarkable robustness where standard models fail:\n- Length Extrapolation: PERK models trained on 8K contexts maintain strong performance on contexts up to 128K, significantly outperforming baselines and even specialized long-context models.\n\n- Positional Robustness: By processing context as a permutation-invariant batch of segments, PERK largely overcomes the \"lost in the middle\" problem, showing minimal performance degradation when the position of relevant information changes, unlike FT-ICR models which fail catastrophically.\n\n**Scalability and Model-Agnosticism:** The combination of LoRA and truncated gradient unrolling makes the complex meta-learning approach tractable . The paper convincingly shows this advantage holds across multiple model families (GPT-2, Qwen, LLaMA) and scales (0.5B to 8B), highlighting the general applicability of the technique.\n\n**Strong Evaluation Methodology:** The introduction of the Drops-in-the-Ocean (DIO) benchmark is a valuable contribution, creating a more challenging and realistic test than standard Needle-in-a-Haystack by using distributionally similar distractors."}, "weaknesses": {"value": "**Inference Latency Trade-Off:** The paper emphasizes efficiency in terms of training memory, but the trade-off with inference latency is a major practical concern. The method requires performing multiple gradient updates at test time to encode the context before answering a question. While Figure 7b shows PERK can be faster at extreme context lengths (e.g., 64K), it appears slower than FT-ICR for more common lengths. This makes the \"efficient\" framing potentially misleading for many real-world applications where inference speed is critical.\n\n**Complexity of the Training Framework:** The PERK training pipeline is highly complex, involving bi-level optimization, truncated gradient unrolling, learned per-layer-per-step learning rates, and a weighted NLL loss in the inner loop. This high degree of complexity could pose a significant barrier to reproducibility and adoption compared to the much simpler standard fine-tuning approach.\n\n**Sub-Optimal Encoding Objective:** The inner loop's objective is simply next-token prediction over the context segments. This seems to incentivize the model to learn the surface-level statistics of the text, which may not be the most effective way to distill salient, reasoning-critical information into the LoRA parameters. The paper does not explore or discuss alternative, potentially more targeted, encoding objectives."}, "questions": {"value": "1. Inference Latency Trade-Off: Could you provide a more direct comparison of the end-to-end inference latency (context encoding + question answering) of PERK vs. FT-ICR, particularly in the common 8K-32K context range? How does this latency scale with the number of inner-loop adaptation steps, and where is the practical break-even point where PERK becomes faster than a standard forward pass?\n\n2. Optimality of NLL for Encoding: The inner loop uses a standard NLL objective to \"compress\" the context into the LoRA. Have you considered alternative objectives that might more explicitly encourage the adapter to store salient facts? For instance, would a contrastive loss or an information-theoretic objective that prioritizes reasoning-relevant information lead to a more effective parametric memory?\n\n3. Comparison to RAG: Your method encodes the entire context into parameters, which is a key difference from RAG systems that first retrieve a small subset of the context. Could you discuss the conceptual trade-offs between PERK and a standard RAG pipeline in terms of performance, latency, and robustness to different types of reasoning tasks?\n\n4. Ablation on Training Components: The PERK training process involves several advanced components (TGU, learned per-layer LR, weighted NLL). How critical is each of these to the final performance? For example, what is the performance drop if you use a fixed inner-loop learning rate or a standard (unweighted) NLL loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FZKCCiRA1f", "forum": "qxDTe8fIyA", "replyto": "qxDTe8fIyA", "signatures": ["ICLR.cc/2026/Conference/Submission24355/Reviewer_MTjy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24355/Reviewer_MTjy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760554476532, "cdate": 1760554476532, "tmdate": 1762943056385, "mdate": 1762943056385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a meta-learning method that learns to encode long contexts through gradient updates to a model adapter at test time. The inner encodes the context to Lora, and the outer loop uses the adapter for the relevant information from the encoded long-context. The experiment proves that the proposed PERK is effective for long-context reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The test-time learning problem is relatively useful to further improve the model performance.\n* Figure 1 presents the pipeline of this method, and the inner loop is clear.\n* The method compares the performance with FT-ICR, proving that the PERK is better.\n* The experiments have various models, supporting that the PERK is general."}, "weaknesses": {"value": "* Is the method test-time training? According to the definition of the outer loop, Equation 3 needs the label of the question. However, during test time, there is no such label.\n* Figure 4 is not related to length extrapolation. The Qwen-2.5-0.5B is trained with 32K by the Qwen Team. However, Figure 4 presents the performance with a maximum 32K, which is NOT longer than the Qwen-2.5-0.5B training length. This is a misclaim.\n* It should provide the training cost, such as time cost, for the comparison of PERK and FT-ICR."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ij6Df6mpfp", "forum": "qxDTe8fIyA", "replyto": "qxDTe8fIyA", "signatures": ["ICLR.cc/2026/Conference/Submission24355/Reviewer_sFKq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24355/Reviewer_sFKq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495051740, "cdate": 1761495051740, "tmdate": 1762943055626, "mdate": 1762943055626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PERK (Parameter-Efficient Reasoning over Knowledge), a novel meta-learning framework that enhances large language models’ (LLMs) ability to perform long-context reasoning. Instead of relying solely on in-context learning or expensive fine-tuning, PERK allows models to encode long contexts through gradient updates at test time, using lightweight Low-Rank Adapters (LoRA) as memory modules. The approach employs two nested optimization loops: the inner loop encodes context into LoRA parameters, while the outer loop learns how to use these adapted parameters for reasoning. Across multiple benchmarks, including Needle-in-a-Haystack, HotpotQA, and a new Drops-in-the-Ocean dataset, PERK consistently outperforms long-context fine-tuning baselines and specialized long-context models, showing strong robustness to reasoning complexity, length extrapolation, and positional variation in relevant information."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper offers a wide-ranging experimental study across multiple long-context reasoning benchmarks, including Needle-in-a-Haystack, HotpotQA, TriviaQA, and the newly introduced Drops-in-the-Ocean (DIO) dataset. The introduction of DIO, which features distributionally similar distractors to better test reasoning precision, strengthens the evaluation by addressing limitations of prior benchmarks. Comparisons against both open-source and commercial LLMs convincingly demonstrate PERK’s superiority.\n\n- PERK exhibits remarkable robustness to reasoning complexity, unseen context lengths (up to 128K tokens), and positional shifts of relevant information. Its consistent performance across model families (GPT, Qwen, LLaMA) and scales indicates that the method learns transferable reasoning strategies rather than overfitting to a particular model or data distribution.\n\n- The approach is built on solid meta-learning foundations, combining bi-level optimization with truncated gradient unrolling to balance effectiveness and efficiency. Its LoRA-based parameter-efficient adaptation enables scalable test-time learning without full model updates, making PERK both methodologically principled and computationally practical."}, "weaknesses": {"value": "- The authors don't mention any public release of the code\n\n- In section 2 of the appendix I see you only ran you experiments setting temperature to 0. Have you tried other values and what results have you found? I'd be curious to see the robustness of PERK at different decoding parameters\n\n- While I didn't find any major weakness, what prevented me from giving a strong accept to this paper is a clear metric of inference cost for PERK. For this method to be useful to researchers and ML practitioners, data on inference cost is fundamental. While a direct comparison with fine-tuning methods is not fair, at least some stats on time needed by PERK would make the paper contribution much more comprehensive\n\nOverall, I think PERK makes a solid contribution and I vote for its inclusion at the conference as is. With additional inference cost analysis, I'd be happy to further increase my score"}, "questions": {"value": "- Will the code be publicly released?\n\n- Reading PERK made me thing about PATHGOOSE [1], where at each LLM token the authors propose to route to a LoRA module that has been finetuned for a specific task. I haven't seen a comprehensive description of any future work you envision for PERK and was wondering if some variant of PATHGOOSE is something you thought about to improve efficiency. \n\n\n[1] Mohammed Muqeeth, Haokun Liu, Yufan Liu, Colin Raffel, Learning to Route Among Specialized Experts for Zero-Shot Generalization,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OkUwZcjPW9", "forum": "qxDTe8fIyA", "replyto": "qxDTe8fIyA", "signatures": ["ICLR.cc/2026/Conference/Submission24355/Reviewer_d52o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24355/Reviewer_d52o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933271393, "cdate": 1761933271393, "tmdate": 1762943055109, "mdate": 1762943055109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PERK, a method to improve reasoning over long contexts.\n\nPerk learns from the context at test time by updating a small, efficient memory module (using LoRA) while keeping the main model frozen. It uses a two-step training process: first, rapidly encodes the context, and then optimizes how the model uses that info to answer questions.\n\nExperiments on long context benchmarks show PERK significantly outperforms standard long-context finetuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Paper is well written, and the results are nice\n- PERK demonstrates strong generalization to long context extrapolation (e.g., training on 8K, testing on 128K) and superior robustness to positional biases in the relevant information.\n- The idea of using a LoRA adapter as a differentiable memory module for context is a good alternative to ICL\n- The use of LoRA and TGU is nice, and seems well executed."}, "weaknesses": {"value": "- The biggest concern is the added complexity. The gains are nice, but I'm not sure they justify using this method over FT-ICR.\n- Related to the added complexity, in general the inference time is significantly longer than ICR (at least up to 34k according to fig 7)\n- PERK is designed for length generalization, as noted in the experiments (train on 8k, eval on 32k). However, FT-ICR is not suited for this kind of evaluation, as is well known. So I'm not sure this is the best baseline to compare against. Especially since we can fine-tune on 32K lengths, I think it would be nice to see an FT-ICR training with 32k length.\n- Permutation-invariance can be detrimental for certain tasks (though maybe rare)."}, "questions": {"value": "- One of the mentioned strengths is that it encodes context as permutation-invariant representations, but that might not always be a strength. What if there is a query about positional information? The benchmarks do not consider that.\n- In figure 6, you show that you encode context + question before the LoRA updates. This means that for each new question, you need to do the update procedure. Is there a way to encode only context so that you can ask multiple questions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "boVpOHBo7J", "forum": "qxDTe8fIyA", "replyto": "qxDTe8fIyA", "signatures": ["ICLR.cc/2026/Conference/Submission24355/Reviewer_bCFY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24355/Reviewer_bCFY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946805113, "cdate": 1761946805113, "tmdate": 1762943054104, "mdate": 1762943054104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}