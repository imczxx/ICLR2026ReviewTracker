{"id": "TkHjRwbMNl", "number": 22626, "cdate": 1758333700490, "mdate": 1759896855969, "content": {"title": "FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization of Large Language Models", "abstract": "Low-rank optimization has emerged as a promising direction in training large language models (LLMs) to improve running time and reduce the memory usage of adaptive optimizers by constraining learning to a lower-dimensional space. Prior work typically projects gradients of linear layers using approaches based on Singular Value Decomposition (SVD) or QR-decomposition. Applying these techniques individually to each layer in large models is computationally expensive and incurs additional memory costs due to storing the projection matrices. In this work, we propose a computationally efficient and conceptually simple, two-step procedure to approximate SVD/QR-based gradient projections into lower-dimensional spaces by using a predefined orthogonal matrix of the Discrete Cosine Transform (DCT). We dynamically select columns from the DCT matrix based on their alignment with the gradient of each layer. The effective projection matrices are obtained via a simple matmul with the DCT matrix in $O(n^3)$ time, followed by a lightweight sorting step to identify the most relevant basis vectors. For large layers, DCT can be computed via Makhoul's $N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \\log(n))$ time. Due to the predefined nature of the orthogonal bases, they are computed once at the start of training. Our numerical experiments on both pre-training and fine-tuning tasks demonstrate the effectiveness of our dual strategy in approximating optimal low-rank projections, obtaining an approach with rank-independent running time that matches the performance of costly SVD/QR-based methods while achieving faster runtime and reduced memory usage by up to $25\\\\%$ across different model sizes.", "tldr": "We use the Discrete Cosine Transform (DCT) via a FFT-based procedure called Makhoul's N-point algorithm to dynamically select columns from the DCT matrix to perform low-rank adaptive gradient optimization for LLMs to replace the SVD/QR methods.", "keywords": ["low-rank optimization", "fast fourier transform", "computational efficiency", "memory efficiency", "efficient optimization", "large language models"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d31ed870ffca8ef6b766caf9afc779aa9ed2280.pdf", "supplementary_material": "/attachment/3785f0224bbb0164cd96f66f4461e4ff1bf07d69.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors have attempted to address the computational and memory overhead in optimizers such as AdamW, Muon, Dion, and GaLore in LLM training. Prior low rank method relies on using SVD or QR decomposition to project gradients into a lower dimensional subspace. However, they are computationally expensive. Therefore, in this paper, the authors attempt to replace these with an alternative low-rank projection approach, which is cheaper to compute."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this paper are summarized as follows:\n\n1. SVD is time-consuming in LLM training, and it is widely used in various methods, like GaLore, FIRA, and FRUGAL. Replacing SVD with the Fast Fourier Transform based algorithm can reduce the time complexity.\n\n2. Empirically, it has shown improvements on multiple optimizers and model sizes, like LLaMA 350M, 800M, and 1.3B. Also, on all of these model sizes, Trion has shown better performance and better running time compared with Dion. \n\n3. It has a strong theoretical guarantee for justifying the column selection approach may give the most significant column."}, "weaknesses": {"value": "The weaknesses of this paper are summarized as follows:\n\n1. The largest model size in the experiment is 1.3B. It would be better if the authors may consider running an experiment on larger models since modern transformer architectures are getting much larger than this size. Also, only the C4 dataset and LLaMA are tested and there is no fine-tuning or downstream benchmarks.\n\n2. Galore has numerous follow-up works, such as Galore 2 [1], Golore [2], and Sara [3]. Could the authors give a comparison with these works?\n\n[1] DiJia Su, Andrew Gu, Jane Xu, Yuandong Tian, and Jiawei Zhao. \"Galore 2: Large-scale llm pre-training by gradient low-rank projection.\" arXiv preprint arXiv:2504.20437 (2025).\n\n[2] Yutong He, Pengrui Li, Yipeng Hu, Chuyan Chen, and Kun Yuan. \"Subspace optimization for large language models with convergence guarantees.\" ICML'25.\n\n[3] Haochen Zhang, Junze Yin, Guanchu Wang, Zirui Liu, Tianyi Zhang, Anshumali Shrivastava, Lin Yang, and Vladimir Braverman. \"Breaking the Frozen Subspace: Importance Sampling for Low-Rank Optimization in LLM Pretraining\". NeurIPS'25."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dNq0Z2uamU", "forum": "TkHjRwbMNl", "replyto": "TkHjRwbMNl", "signatures": ["ICLR.cc/2026/Conference/Submission22626/Reviewer_kABp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22626/Reviewer_kABp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763650798, "cdate": 1761763650798, "tmdate": 1762942311008, "mdate": 1762942311008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the high compute and memory cost of SVD/QR decompositions in low-rank optimizers  The authors propose replacing these expensive, per layer projections with a \"dynamic column selection\" from a single, predefined orthogonal basis (the Discrete Cosine Transform, or DCT, matrix) computed once at the start . The method efficiently selects the top $r$ most aligned DCT basis vectors for each layer's gradient, replacing SVD/QR with a fast matrix multiplication (or FFT) and a sorting step. This saves memory, as each layer only stores $r$ indices instead of a full projection matrix. The authors integrate this technique into two new optimizers: Trion (improving Dion) and DCT AdamW (improving AdamW variants)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method's runtime is rank independent, whereas SVD/QR based methods like Dion get slower as the rank $r$ increases. This is a significant practical advantage for using larger and more expressive ranks.\n\n2. The core idea is simple yet elegant. The \"dynamic column selection\" is not just heuristic; the authors prove it is the optimal strategy for minimizing reconstruction error, given a fixed orthogonal basis $Q$. The motivation for using the DCT as that basis is also well justified.\n\n3. Strong Empirical Results: \n\ni. Trion vs. Dion (Table 1): Trion consistently achieves better validation perplexity, lower memory usage (~7-10%), and faster runtimes (up to 18% faster) than its direct baseline, Dion, across all tested model sizes and ranks.\n\nii. DCT-AdamW vs. LDAdamW: DCT-AdamW achieves better validation perplexity, drastically lower memory, and is significantly faster (~25%)."}, "weaknesses": {"value": "1. Theoretical speedup not realized. The paper heavily motivates using DCT by citing the fast $O(n^2 \\log n)$ FFT-based algorithm. However, the authors admit that for the model sizes tested (up to $d=2048$), this speedup was not significant, and a standard $O(n^3)$ matmul was used15. The primary speedup comes from replacing SVD/QR, not from the FFT.\n\n2. While DCT-AdamW beats its low-rank competitor LDAdamW, it is still significantly outperformed by full rank AdamW (Val. PPL 13.69 vs. 11.73). This shows the method is a better low rank compromise, but still a compromise.\n\n3. As the authors note, experiments are limited to 1.3B models. The method's true scalability and the benefit of the FFT based algorithm would only be evident on much larger models."}, "questions": {"value": "Please see Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T0MTrWnUtR", "forum": "TkHjRwbMNl", "replyto": "TkHjRwbMNl", "signatures": ["ICLR.cc/2026/Conference/Submission22626/Reviewer_F7Ct"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22626/Reviewer_F7Ct"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951969119, "cdate": 1761951969119, "tmdate": 1762942310704, "mdate": 1762942310704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a Discrete Cosine Transform (DCT)-based dynamic column selection technique that approximates optimal low-rank projections by selecting columns of a fixed orthogonal DCT matrix aligned with each layer’s gradients to replace expensive SVD/QR-based low-rank projections used in adaptive optimizers for large language models. This method reduces computation and memory overhead by avoiding per-layer decompositions and storing only column indices. They apply this idea in two new optimizers: Trion, which improves Dion, and DCT-AdamW, a low-rank variant of AdamW."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work targets a bottleneck in large-model training: the cost of SVD/QR-based low-rank projections used in adaptive optimizers.\n2. The proposed DCT-based projection method is straightforward and easily integrable into existing optimizers.\n3. The approach achieves good efficiency gains.\n4. The paper provides mathematical rationale for why DCT approximates the gradient eigenbasis and the effectiveness of norm-based selection. \n5. The presentation is easy to follow and the paper is well-written."}, "weaknesses": {"value": "1. All experiments stop at 1.3B-parameter models; there is no validation on pretraining larger LLMs (7B+), which undermines claims of scalability.\n2. The paper lacks certain ablations of critical design choices (e.g., norm type, rank sensitivity, DCT variant).\n3. Distributed and FSDP discussions are not empirically backed with wall-clock or communication cost benchmarks.\n4. While the paper targets efficiency, comparison with fast efficient baselines that are not using SVD/QR decomposition like APOLLO [1] and SubTrack++ [2] would help to strengthen the paper. \n---\n[1] Zhu et al., 2025. APOLLO: SGD-like Memory, AdamW-level Performance.\n[2] Rajabi et al., 2025. SubTrack++: Gradient Subspace Tracking for Scalable LLM Training"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wzyUN7e9aq", "forum": "TkHjRwbMNl", "replyto": "TkHjRwbMNl", "signatures": ["ICLR.cc/2026/Conference/Submission22626/Reviewer_tJpX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22626/Reviewer_tJpX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013751898, "cdate": 1762013751898, "tmdate": 1762942310529, "mdate": 1762942310529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes replacing per step SVD or QR low rank projections in adaptive optimizers with a fixed orthogonal basis using DCT and dynamic column selection. At each step the method scores basis columns by gradient to basis correlation and selects the top r to form projections, which avoids repeated factorizations and heavy state storage. Two instances, Trion and DCT AdamW, show lower memory use, faster training, and comparable or better perplexity on mid size LLM pretraining. The theory motivates norm based selection and provides simple error bounds. The implementation also explains how to integrate with distributed training to reduce communication."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper uses a precomputed DCT with on the fly selection to replace repeated SVD or QR, and only column indices are stored. Column norm ranking aligns with minimizing reconstruction error, and there is clear intuition for why DCT approximates dominant directions.\n\nThe method has consistent memory reduction and wall clock speedups while matching or improving perplexity across several model sizes.\n\nThe notes on DDP or FSDP usage and local reconstruction make adoption straightforward."}, "weaknesses": {"value": "Evidence for scaling to very large hidden sizes remains limited. The paper primarily reports results on mid size models where the benefit of fast transforms over plain matrix multiplication is muted. Without end to end wall clock measurements at dimensions around 8k to 16k, and without a breakdown of time in the similarity computation, basis selection, and reconstruction kernels, it is hard to assess whether the claimed speedups persist when layers become wide and deep. A thorough profiling study across model width, batch size, and rank would make the efficiency claims more convincing.\n\nIt is unclear whether all systems level optimizations such as ZeRO style redundancy removal, identical communication and precision settings, and error feedback or quantization choices are enabled symmetrically for both the proposed method and the baselines. Differences in these controls can easily dominate the observed speed or memory gains. The paper should report results under strictly matched configurations and, if desired, separately include best tuned variants for each method.\n\nThe paper mixes square and rectangular gradient matrices without clearly specifying when to apply left versus right projections, and how this choice is made across different layer types such as attention projections and output layers. The exact dimensional assumptions of the DCT matrices and the consistency of symbols drift across sections, which complicates reproduction and theoretical interpretation. A precise, layer wise rule set and a short ablation on these choices would improve clarity.\n\nThe set of comparative baselines is too narrow to establish superiority. Strong structured projections such as Hadamard or FWHT, CountSketch style mappings, and recent online or streaming low rank methods are not evaluated side by side under matched rank and refresh frequency. Because many of these alternatives also offer O(n log n) or near linear time with tiny memory footprints, omitting them leaves open whether DCT based selection is uniquely effective. Head to head comparisons on identical tasks and budgets are needed to justify the design choice."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KwXkmqNfXU", "forum": "TkHjRwbMNl", "replyto": "TkHjRwbMNl", "signatures": ["ICLR.cc/2026/Conference/Submission22626/Reviewer_akDN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22626/Reviewer_akDN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762285121712, "cdate": 1762285121712, "tmdate": 1762942310191, "mdate": 1762942310191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Low-rank optimization can speed LLM training and cut optimizer memory, but per-layer SVD/QR gradient projections are costly and require storing projection matrices. We propose a simple DCT-based alternative: multiply each layer’s gradient by a fixed orthogonal DCT basis, then rank-select the most aligned columns to form the projection. The DCT is efficiently computed (via FFT-based routines), precomputed once, and reused—so projections need only a matmul plus lightweight sorting. Across pre-training and fine-tuning, this yields rank-independent runtime, matches SVD/QR accuracy, and achieves faster training with lower memory use."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Provided a two-stage DCT-based projection method to avoide the computational cost of SVD\n- Developed the DCT variant Trion and DCT-AdamW\n- Demenstrated the contractivenss of the proposed compressor\n- Conduct extensive experiments"}, "weaknesses": {"value": "1. **Effectiveness in tracking gradients.** Constructing SVD-free, contractive projection matrices is straightforward (e.g., random projections). The real challenge is to remain SVD-free and contractive *while* faithfully capturing the gradient’s low-rank structure. The proposed two-stage method fixes a DCT basis $D_C$ for the entire training and, at each iteration, selects only a few of its columns. In effect, it tracks gradients using subsets of a preset basis—an approach not obviously aligned with evolving, layer-specific low-rank subspaces. The paper does not explain why this selection should match the true gradient subspaces or under what conditions it would; clearer insight or evidence (e.g., principal-angle analyses or SVD/QR approximation errors over training) is needed. \n\n2. **Necessity of compressing optimizer states.** ZeRO-style sharding already reduces optimizer-state memory by ~1/N per data-parallel replica without degrading quality. In Figure 2, DCTAdamW underperforms AdamW, suggesting that extra low-rank compression of states may be unnecessary—or even harmful—unless it delivers clear end-to-end gains. I personally think compressing optimizer states is unnecessary due to ZeRO optimizer. \n\n3.  **Necessity of saving computations in Newton–Schulz.** In Trion/Muon-style preconditioning, forward–backward passes dominate step time; Newton–Schulz iterations are typically a small fraction—especially with long context windows. The paper should provide profiler traces showing that low-rank $b_t$ meaningfully reduces *end-to-end* step time or time-to-target. Please also compare with vanilla Muon (no low-rank) to assess any convergence slowdown from low-rank preconditioning, and include both complexity estimates (per-token FLOPs) and wall-clock measurements to substantiate the claimed benefit."}, "questions": {"value": "1. **Error-feedback memory overhead**. Trion’s error-feedback buffer appears to store a full-size residual per parameter. Does this negate the memory savings from low-rank gradient projection?\n\n2. **Low-rank during forward–backward**. Beyond optimizer/state compression, can the low-rank structure be exploited to reduce the dominant forward–backward costs (FLOPs and memory)—e.g., via factored weight updates or structured bases that lower gradient-computation cost?\n\n3. I noticed that the work [R1] uses a similar idea to save SVD computations. Could the authors highlight the difference from [R1]\n\n[R1] Wavelet Meets Adam: Compressing Gradients for Memory-Efficient Training"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6z3aUMR4lI", "forum": "TkHjRwbMNl", "replyto": "TkHjRwbMNl", "signatures": ["ICLR.cc/2026/Conference/Submission22626/Reviewer_ZMSS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22626/Reviewer_ZMSS"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762378639663, "cdate": 1762378639663, "tmdate": 1762942309826, "mdate": 1762942309826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}