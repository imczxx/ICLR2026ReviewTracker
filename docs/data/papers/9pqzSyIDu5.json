{"id": "9pqzSyIDu5", "number": 7750, "cdate": 1758034626740, "mdate": 1763281829392, "content": {"title": "CP Merging: Joint LoRA Merging using Canonical Polyadic Decomposition", "abstract": "Large language models (LLMs) are often fine-tuned for specific tasks using Low-Rank Adaptation (LoRA), an efficient method that adds small, task-specific modules called LoRA adapters to a pre-trained base model. However, a major challenge arises when merging multiple LoRA adapters trained on different data sources for a specific task: it often leads to task interference, which degrades the model's performance. While recent SVD-based LoRA merging methods have shown promise by decomposing adapters into orthogonal components and keeping only the most important ones, they have an important limitation: These methods process each adapter independently,  overlooking potential interactions between different tasks. To address this, we propose a novel LoRA merging method using joint Canonical Polyadic (CP) decomposition (CP merging). We first combine the LoRA adapters into a single third-order tensor. Then, we apply CP decomposition to this tensor to disentangle factors that are unique to each task from those that are shared across tasks. This joint factorization method helps reduce cross-task interference without losing important information. Our extensive experiments on NLP tasks demonstrate that CP merging yields superior performance compared to the existing SVD-based baselines.", "tldr": "", "keywords": ["model merging", "parameter efficient finetuning", "large language models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b491a17c95861d1728cb478f2d7cd2fddc9fc849.pdf", "supplementary_material": "/attachment/74ac30195589705a1e1e90c4d2660c0ac5203f4b.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the problem of task interference when merging multiple Low-Rank Adaptation (LoRA) adapters in large language models, which often leads to performance degradation. It proposes a novel method called CP Merging, which uses Canonical Polyadic (CP) decomposition to jointly factorize LoRA adapters into a third-order tensor, disentangling task-specific and shared components. The core contribution is a unified factorization approach that reduces interference compared to existing SVD-based methods. Experimental results on held-in, held-out, and zero-shot tasks using models like Phi-3 and Mistral show that CP Merging outperforms baselines in metrics like Rouge-L and accuracy, particularly in question answering and coding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-organized and easy to read, with intuitive figures illustrating key concepts.\n\n2. The CP decomposition framework is derived rigorously as a generalization of SVD, with clear explanations of how factor matrices represent task-specific and shared components, supported by tensor operations.\n\n3. Comprehensive experimental evaluation across diverse scenarios with consistent comparisons to SOTA methods. I really appreciate that the author conduct both held-in and held-out tasks, which is a significant part that is always ignored by existing model merging methods."}, "weaknesses": {"value": "1. The baselines are somewhat out-of-date, and more recent LoRA-based[1,2] merging methods should be compared to validate the effectiveness of the proposed method.\n\n2. Experiments are only conudcted on LLM. Extending the application scenarios like MLLMs would be of great significant since there is no difference between parameter-efficient tuning of LLM and MLLM other than the structure.\n\n3. Different LoRA fine-tuning strategies like DoRA and so on are suggested to be employed to demonstrate the effectiveness.\n\n4. Although the concept of CP is well defined by previous work, the authors had better explain in detail the concrete adaptation of how to fit it for model merging, or just an application of previous work. For example, the method relies on assumptions on task interdependence that CP decomposition naturally disentangles shared/task-specific factors, but no failure cases or scenarios where factorization degrades performance are discussed. \n\n5. limited scale of model: extending experiments to larger models (e.g., >7B parameters) would better illustrate the effectiveness."}, "questions": {"value": "1.  What does oracle mean in Table 1? If it represents the results of individual training, what causes some model merging methods to surpass the theoretical upper bound in some datasets? \n\n2. How does CP Merging perform under extreme task interference scenarios (e.g., adversarial tasks or safety tasks), and are there failure modes not covered in current experiments?\n\n3. What is the computational overhead of CP tensor operations compared to SVD-based methods, and could this be quantified in terms of time/memory complexity?\n\n4. Generalization limitations: On held-out tasks, uniform merging sometimes outperforms CP Merging, suggesting unresolved generalization challenges, but this is not deeply analyzed. Could the author provide a deeper analysis?\n\n[1] RobustMerge: Parameter-Efficient Model Merging for MLLMs with Direction Robustness.\n\n[2] LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PHI2YmYNFG", "forum": "9pqzSyIDu5", "replyto": "9pqzSyIDu5", "signatures": ["ICLR.cc/2026/Conference/Submission7750/Reviewer_Zyfb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7750/Reviewer_Zyfb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760613886165, "cdate": 1760613886165, "tmdate": 1762919797077, "mdate": 1762919797077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "a5tTKvg6EZ", "forum": "9pqzSyIDu5", "replyto": "9pqzSyIDu5", "signatures": ["ICLR.cc/2026/Conference/Submission7750/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7750/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763281828561, "cdate": 1763281828561, "tmdate": 1763281828561, "mdate": 1763281828561, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to improve the performance of merging LoRA-trained models. While previous SVD-based merging methods show improvement, their limitation is that they decompose each adapter independently and do not utilize the interaction among tasks. In this paper, the authors propose to use Canonical Polyadic decomposition before merging the models. Extensive experiments show the effectiveness of their method. Ablation studies provide multiple insights based on their method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is simple and elegant, but effective and compatible with other merging methods.\n- The overall writing of this paper is good and easy to follow. Visual elements are useful to help understanding.\n- Experiments are comprehensive, including diverse ablation studies."}, "weaknesses": {"value": "- In Sec.4.1, the author discussed that in-domain multi-task learning will be evaluated on unseen tasks, which I think does not match the conventional setting of multi-task learning. For multi-task learning and model merging, the main goal is still to improve the in-domain performance. Could you provide more literature to support the claim in the paper?\n- The motivation to use Canonical Polyadic decomposition is not clear to me. The author claims that previous methods decompose the task adapters separately (lines 220-221), but actually KnOTs decomposes all the adapters together to find a shared subspace for all adapters. Moreover, why decomposing all adapters together can improve the performance is not well studied or explained.\n- Since larger models are more difficult to (re-)train, it is more necessary to achieve better performance on larger models. This leads to two challenges. First, while this paper mainly uses a 3B model in the experiments, the effectiveness of CP merging under larger models is not well studied. Second, the efficiency of CP merging when the model size and task number increase is not studied."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A49wyCi575", "forum": "9pqzSyIDu5", "replyto": "9pqzSyIDu5", "signatures": ["ICLR.cc/2026/Conference/Submission7750/Reviewer_SS4j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7750/Reviewer_SS4j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601176879, "cdate": 1761601176879, "tmdate": 1762919796660, "mdate": 1762919796660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CP-Merging, a novel method for merging LoRA adapters. Unlike existing approaches that apply Singular Value Decomposition (SVD) independently to each adapter, CP-Merging first stacks multiple adapters into a third-order tensor and then jointly factorizes them using Canonical Polyadic (CP) decomposition. The authors argue that this joint factorization more effectively disentangles shared components from task-specific ones, thereby mitigating task interference. Experimental results across multi-task learning, zero-shot generalization, and skill composition show that CP-Merging outperforms SVD-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The application of tensor decomposition, specifically CP decomposition, to LoRA merging is a novel and well-motivated approach. It provides a promising alternative to SVD-based methods for tackling task interference.\n\nThe paper includes a comprehensive evaluation across a diverse set of tasks and scenarios, convincingly demonstrating the method's effectiveness.\n\nThe ablation studies on the CP rank R and the scaling factor α are thorough and provide valuable insights into the method's behavior and sensitivity."}, "weaknesses": {"value": "1. There appears to be an error in Table 2. There are duplicate entries for the \"TSV Merging\" method, each presenting different results. It is unclear if this is a typographical error or if they represent different experimental settings, as this is not clarified in the text.\n\n2. The evaluation on the Flan dataset (Table 1) relies heavily on the ROUGE-L metric. While useful, ROUGE-L primarily measures lexical overlap and may not adequately capture semantic correctness or logical coherence. The inclusion of other metrics could provide a more comprehensive picture of performance.\n\n3. The paper demonstrates that performance is sensitive to key hyperparameters like the CP rank R and the scaling factor α. However, it lacks a discussion on a principled method or heuristic for selecting these hyperparameters in practice, which is a notable limitation for real-world applications."}, "questions": {"value": "1. The analysis of task interference in Figure 4 is limited to the first 11 layers (0-10). The paper suggests interference is higher in lower layers. Could the authors comment on or provide data for the trend of the CP-STI score in deeper layers of the model?\n\n2. The proposed CP-Merging method is presumably more computationally expensive than baselines like uniform averaging or independent SVD. The paper would be strengthened by an analysis of the time and space complexity of CP-Merging, especially as the number of tasks (N) grows. This is crucial for assessing its scalability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CB27XaAOaA", "forum": "9pqzSyIDu5", "replyto": "9pqzSyIDu5", "signatures": ["ICLR.cc/2026/Conference/Submission7750/Reviewer_aE98"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7750/Reviewer_aE98"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806040952, "cdate": 1761806040952, "tmdate": 1762919796327, "mdate": 1762919796327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of merging multiple LoRA adapters - each fine-tuned on different tasks - into a single model without large performance degradation from task interference.\nPrior SVD-based merging decomposes each adapter independently, potentially losing cross-task structure. The proposed method (“CP merging”) instead stacks LoRA adapters into a third-order tensor and performs joint CP decomposition, separating shared vs. task-specific factors.\nExperiments on NLP tasks show improved performance over SVD-based baselines, notably +3.19 Rouge-L on held-in tasks and +5.69 Rouge-L on held-out tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- CP decomposition naturally models multi-task structure via shared and task-specific factors.\n- Repeated gains over SVD methods across held-in, held-out, and skill-composition tasks.\n- Handles multi-adapter reuse in modern LoRA libraries."}, "weaknesses": {"value": "- Technical novelty is moderate. CP decomposition is standard; applying it to LoRA merging is incremental.\n- No detail in parameter overhead or computational cost of CP. Tradeoffs need discussion.\n- SVD approaches are compared, but unclear whether more recent/strong merging approaches (e.g., LoRI) were included.\n- Evaluation scope is only within NLP. Unclear generality to other modalities."}, "questions": {"value": "- CP decomposition scales poorly. What are practical computational costs vs. SVD?\n- Does performance degrade when tasks are highly dissimilar?\n- How does method behave when merging >10 LoRAs (real LoRA hubs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JdYG27aD2J", "forum": "9pqzSyIDu5", "replyto": "9pqzSyIDu5", "signatures": ["ICLR.cc/2026/Conference/Submission7750/Reviewer_ateH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7750/Reviewer_ateH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989401468, "cdate": 1761989401468, "tmdate": 1762919795060, "mdate": 1762919795060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}