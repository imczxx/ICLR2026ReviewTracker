{"id": "gWteQPhreG", "number": 21253, "cdate": 1758315435345, "mdate": 1759896932122, "content": {"title": "RASRAG: A DOMAIN-SPECIFIC RAG FRAMEWORK AND BENCHMARK FOR ROBOTIC-ASSISTED SURGERY", "abstract": "Robot-assisted surgery (RAS) has significantly improved patient outcomes by reducing blood loss, shortening hospital stays, and accelerating recovery. Despite these benefits, the widespread adoption of RAS has been slowed by a shortage of trained robotic surgeons and limited access to robotic systems. One of the major limitations is access to academic materials and expertise in this domain, which are\nmostly limited to private company programs or a few textbooks. In this regard, foundation models and large language models (LLMs) have been shown to excel in both information retrieval and knowledge synthesis. However, none have been specifically adapted to the complexities of the RAS domain. To address this gap, we introduce RASRAG, a RankLLaMA-based Tree Retrieval-Augmented Generation framework that leverages a hierarchical structure derived from the source textbook. Our contributions are: (1) a novel tree-based RAG architecture in which RankLLaMA jointly performs agentic exploration and reranking along the hierarchy (“forest of knowledge”), yielding more relevant retrieval than embedding only baselines, fine-tuned models, and alternative RAG methods; (2) a publicly available, first-of-its-kind question–answer benchmark curated by seven surgeons and two physicians, reflecting real-world RAS clinical inquiries; and (3) clinically grounded evaluation protocol, including blind grading of both model and human answers by surgeons and RAG-specific measures of retrieval and answer quality. RASRAG with significantly smaller models matches or outperforms state-of-the-art LLMs, fine-tuned LLMs, and existing RAG architectures in terms of precision and relevance for domain-specific tasks.", "tldr": "RASRAG, an agentic Tree-RAG built from a hierarchical robotic-assisted surgery textbook and using RankLLaMA to jointly perform exploration and reranking, matches or outperforms state-of-the-art LLM/RAG baselines on precision and relevance", "keywords": ["Large Language Models", "Retrieval Augmented Generation", "Robotic Assisted Surgery", "Benchmark"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/055ac7061ff9fba5769445065e114ddad9c1f2bc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents RASRAG, a novel domain-specific RAG framework designed to address the knowledge acquisition barriers in Robot-Assisted Surgery. Unlike traditional vector retrieval methods, RASRAG constructs authoritative RAS textbooks into a hierarchical knowledge tree and employs RankLLaMA for exploration and reranking. The work contributes the first publicly available RAS Q&A benchmark curated by a surgical team and establishes a rigorous clinical evaluation protocol. Experimental results demonstrate that RASRAG significantly outperforms conventional RAG approaches, state-of-the-art LLMs, and human expert responses across both automated metrics and blinded evaluations by independent surgeons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a novel tree-based RAG architecture that employs RankLLaMA for agentic exploration and reranking within a hierarchical knowledge structure.\n- The work creates the first question-answering benchmark for RAS. This benchmark is curated by a seven-member clinical expert team including five surgeons.\n- RASRAG demonstrates retrieval effectiveness largely independent of model size. Even smaller parameter models (e.g., Qwen2.5-1.5B) in RASRAG achieve retrieval performance comparable to larger models."}, "weaknesses": {"value": "- The benchmark represents a core contribution, yet the paper inadequately describes its construction process. Key methodological details are absent, including: question generation procedures ; sources of \"standard answers\" (expert-authored vs. textbook excerpts?); and quality control and validation workflows.\n- The core evaluation is conducted on a benchmark derived from the same textbook used to construct the knowledge base. While the paper includes a small-scale test on a second textbook, the framework's robustness across broader knowledge bases remains insufficiently validated.\n- The work is highly specialized in robot-assisted surgery. The core architecture relies on a hierarchical tree structure derived from a specific textbook. Despite authors' claims of methodological generalizability, the paper provides no evidence of scalability or broader applicability."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No Ethics Concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X0R9Wu47wn", "forum": "gWteQPhreG", "replyto": "gWteQPhreG", "signatures": ["ICLR.cc/2026/Conference/Submission21253/Reviewer_VAre"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21253/Reviewer_VAre"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538775697, "cdate": 1761538775697, "tmdate": 1762941656601, "mdate": 1762941656601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RASRAG, a domain-specific Retrieval-Augmented Generation (RAG) framework designed for Robotic-Assisted Surgery (RAS). It integrates a tree-structured retrieval system based on a surgical textbook and leverages RankLLaMA for semantic reranking at each node. The framework aims to enhance precision and contextual accuracy in surgical knowledge retrieval. The authors also propose the first RAS-specific QA benchmark, curated by surgeons and physicians, with over 300 questions reflecting real-world clinical scenarios. Extensive evaluations—including RAGAS, NVIDIA Answer Accuracy, and expert surgeon grading—demonstrate that RASRAG outperforms traditional RAG methods, fine-tuned models, and general-purpose LLMs in both factual accuracy and clinical relevance."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**Novel Domain-Specific Architecture**:\nThe paper effectively adapts the RAG framework to a highly specialized medical context, introducing a hierarchical “tree-of-knowledge” retrieval system that mirrors clinical reasoning structures. This design improves interpretability and retrieval precision.\n\n**High-Quality Benchmark Creation**:\nThe curated QA dataset, built by surgeons and doctors, is a major contribution. It provides a reliable foundation for evaluating medical QA systems, addressing the lack of standardized evaluation in RAS.\n\n**Comprehensive Evaluation**:\nThe authors conduct a multi-angle evaluation using both automated and human assessments. The inclusion of surgeon-based grading lends strong credibility and clinical grounding to the results.\n\n**Strong Empirical Performance**:\nAcross all metrics, RASRAG consistently outperforms both open and proprietary baselines (e.g., GPT-4o, GPT-5), demonstrating that architecture and retrieval quality can rival model scale."}, "weaknesses": {"value": "**Poor presentation**:\nThe reviewer feels uncomfortable that there is no Introduction section at the beginning and conjectures that the presentation quality seems to be quite below the expectations of the ICLR conference.\n\n**Limited Generalization Beyond Textbook Sources**:\nThe framework heavily depends on a single structured textbook as its knowledge base. While a second book test is mentioned, broader generalization to heterogeneous or unstructured data (e.g., surgical notes, videos) is not explored.\n\n**Computational Overhead**:\nThe use of RankLLaMA for multi-stage reranking introduces a latency of ~15 seconds per query. Although acceptable for research, this may hinder real-time clinical deployment.\n\n**Lack of Comparison with More Recent Agentic or Planning-Based RAGs**:\nThe study does not deeply compare against modern multi-hop or agentic retrieval approaches (e.g., Tree-of-Thought RAG, planner-verifier pipelines), which could contextualize RASRAG’s innovation more sharply.\n\n**Evaluation Bias Toward Structured QA**:\nThe benchmark and evaluations focus on structured factual questions. Open-ended, reasoning-intensive queries (e.g., decision-making or surgical risk prediction) remain underrepresented."}, "questions": {"value": "**Scalability and Adaptation**:\nHow does RASRAG handle updates or integration of new medical knowledge, such as new surgical techniques or guidelines? Would retraining or structural expansion be required?\n\n**Multimodal Extension**:\nSince RAS inherently involves visual data (e.g., endoscopic imagery), could this hierarchical retrieval method be extended to incorporate multimodal (text + image/video) sources?\n\n**Clinical Validation Path**:\nBeyond expert grading, are there plans to test RASRAG’s utility in real surgical training or decision-support settings, potentially measuring time saved or error reduction?\n\n**Model Transparency and Trust**:\nGiven that the framework emphasizes “traceability,” how effectively does RASRAG allow surgeons to verify retrieved evidence? Could future versions integrate explainable retrieval pathways?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AfY3Z8Ii8n", "forum": "gWteQPhreG", "replyto": "gWteQPhreG", "signatures": ["ICLR.cc/2026/Conference/Submission21253/Reviewer_f9dY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21253/Reviewer_f9dY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569162083, "cdate": 1761569162083, "tmdate": 1762941656375, "mdate": 1762941656375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RASRAG, a domain-specific Retrieval-Augmented Generation (RAG) framework for Robotic-Assisted Surgery (RAS). The method operates by structuring a source RAS textbook into a hierarchical knowledge. It then employs a RankLLaMA-based model to perform semantic reranking and navigation through this hierarchy, moving from high-level procedures (BTUs) down to specific text chunks (STUs) . A key contribution is the introduction of a new, 305-pair question-answer (QA) benchmark curated by a team of seven clinicians. The framework's performance is assessed using automated metrics (RAGAS, NVIDIA Answer Accuracy) and a blind evaluation conducted by three independent surgeons."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper focuses on a clear and important real-world problem domain: Robotic-Assisted Surgery (RAS). This field faces distinct challenges, including a shortage of trained surgeons , barriers to training, and limited access to specialized academic materials.\n- The creation and release of first-of-its-kind QA benchmark curated by clinical experts (surgeons and physicians) is a valuable contribution, providing a new resource for future research in this area.\n- The explanation of the current status of RAS and its critical challenges is reasonable and supports the understanding of the RAS environment and the intent of the framework. \n- The evaluation design is comprehensive, incorporating automated RAG metrics, clinical answer accuracy metrics, and a blind human expert evaluation, which represents a robust approach to validation."}, "weaknesses": {"value": "### Limited Methodological Novelty and Mismatch with Domain\nThe core method, a hierarchical search through a structured corpus is fundamentally just a structured search over a single textbook's table of contents. The validation for this (Appendix A.1) demonstrates properties of a well-organized textbook, not unique properties of the *RAS domain*. The authors themselves concede the method's generality (\"This methodology could generalize well beyond RAS\"), which undermines the central claim of domain-specific innovation.\n\n### Under-described Benchmark\nA primary contribution, the 305-pair QA benchmark, is presented with insufficient detail. Section 2 merely states *who* created it (7 clinicians) and *what* it is (305 pairs). Critical information regarding the protocol for question generation, the quality assurance process, answer curation process. This lack of transparency makes it difficult to assess the benchmark's quality or reproducibility.\n\n### Worries of overfitting to the retrieval corpus\nThe framework relies on heuristic, rule-based procedures (e.g., select definite and candidate passages). Because the approach was tuned to the specific retrieval corpus, its evaluation primarily demonstrates properties of that corpus’s organization (and the chosen search heuristics) rather than unique features of the RAS domain. The resulting complexity might introduces extra engineering burden. While the complexity of the RAS domain is understandable, the heuristic-based approach may limit the method’s extensibility; therefore, the paper should provide a stronger justification to address this concern.\n\n### Needs more baseline \nThe authors rely on a latency-heavy tree-search structure to achieve gains, but do not sufficiently evaluate stronger or refined similarity-based baselines (e.g., Qwen-based retriever, re-ranking with lightweight cross-encoders, iterative RAG loops, or search-agent framework for complicate queries). Though the authors includes a few variations in table 2 (MedGraph, PaperQA), I wonder whether it is sufficiently represent the potential of existing RAG researches (and also think it should be included in table 1.)"}, "questions": {"value": "### Chapter-level independence as a specific property of RAS\nThe \"Chapter-level conditional independence\" seems to be the main justification for the hierarchical tree structure. Can you elaborate on why this is a specific property of RAS knowledge, rather than a general property of any well-structured textbook? How would this method perform on a non-hierarchical corpus, such as 10,000 individual surgical case reports?\n\n### Regarding the benchmark\nWhat was the detailed protocol given to the 7 clinicians for generating questions and answers? What quality control measures were in place to ensure the answers were correct, consistent, and comprehensive before using them as ground truth?\n\n### Correlation with general performance of models\nIn table 1, unlike my expectation, large-scale models (including close-sourced) have relatively lower performance (context precision, context recall) tendency than smaller open-source models. It would be helpful for me to get a justification of this.\n\n### Latency-Performance Tradeoff\nThe work needs a more rigorous, quantitative comparison that measures both quality gains and latency/compute costs. Could the authors limit the number of search call (or iterations) and evaluate the performance?\n\n\n\n### paper error\nIn table 1, the performance of context precision is wrongly highlighted. the best performance is Qwen2.5-1.5B-Instruct (0.8918), not MedGemma (0.8829)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cWPeM1WR93", "forum": "gWteQPhreG", "replyto": "gWteQPhreG", "signatures": ["ICLR.cc/2026/Conference/Submission21253/Reviewer_cNBn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21253/Reviewer_cNBn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764301961, "cdate": 1761764301961, "tmdate": 1762941656030, "mdate": 1762941656030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RASRAG is introduced as a domain-specialized retrieval-augmented generation framework aimed at improving medical report generation and question-answering in a specific clinical domain. The method builds a hierarchical “forest of knowledge” from a key domain textbook, allowing an LLM agent to iteratively explore and rerank relevant sections, much like an expert searching through a textbook. \nThe authors also contribute a new expert-curated benchmark of question–answer pairs reflecting real clinical queries, along with an evaluation protocol."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a well-identified gap by focusing on a specialized medical domain where general LLMs underperform. The motivation is explained with real-world context (e.g. limited access to expert knowledge in the domain), making the case that a domain-specific model is needed and valuable.\nThe paper contributes a new expert-curated QA benchmark for the domain, which is a valuable resource for the community."}, "weaknesses": {"value": "1) The approach is tailored to a specific domain and relies on a structured hierarchy for retrieval. This dependence means that applying RASRAG to a different domain would require a similarly well-structured knowledge source. If the domain knowledge is not organized as this, the performance may degrade.  \n2) While the results are strong, the paper could benefit from deeper ablation studies or analysis of each component in the pipeline.   \n3) The custom QA benchmark, while valuable, is relatively small in scale (on the order of a few hundred expert-curated questions). This raises a concern that the evaluation, though high quality, might not cover the full diversity of real-world queries."}, "questions": {"value": "please address the concerns in weakness sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C9CPtbrxXS", "forum": "gWteQPhreG", "replyto": "gWteQPhreG", "signatures": ["ICLR.cc/2026/Conference/Submission21253/Reviewer_tVGw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21253/Reviewer_tVGw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016722422, "cdate": 1762016722422, "tmdate": 1762941655684, "mdate": 1762941655684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}