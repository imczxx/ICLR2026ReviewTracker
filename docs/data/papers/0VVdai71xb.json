{"id": "0VVdai71xb", "number": 20719, "cdate": 1758309363416, "mdate": 1759896961948, "content": {"title": "Mechanistic Independence: A Principle for Identifiable Disentangled Representations", "abstract": "*Disentangled representations* seek to recover latent factors of variation underlying observed data, yet their *identifiability* is still not fully understood. We introduce a unified framework in which disentanglement is achieved through *mechanistic independence*, which characterizes latent factors by how they act on observed variables rather than by their latent distribution. This perspective is invariant to changes of the latent density, even when such changes induce statistical dependencies among factors. Within this framework, we propose several related independence criteria -- ranging from support-based and sparsity-based to higher-order conditions -- and show that each yields identifiability of latent subspaces, even under nonlinear, non-invertible mixing. We further establish a hierarchy among these criteria and provide a graph-theoretic characterization of latent factors as connected components. Together, these results clarify the conditions under which disentangled representations can be identified without relying on statistical assumptions.", "tldr": "", "keywords": ["Identifiability", "Disentangled Representation", "Mechanistic Independence"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a71839ea9f79bbcbb8059bc7c9e2ecb2419cf82d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes mechanistic independence, which enables identifiable disentangled representations without strong statistical assumptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. As a theoretical paper, the presentation is rigorous.\n2. The theorem proofs appear solid.\n3. The authors seem familar with recent identifiability literature and compare their results with prior work across the types D, M, S, H."}, "weaknesses": {"value": "1. The abstract and main text repeatedly state that the paper’s identifiability results allow a non-invertible g, and they give the practical example of the “responsibility problem” (lines 160–161) to justify relaxing the global invertibility assumption. From my reading, permitting non-invertibility appears to be one of the paper’s contributions/novelties. However, all of the identifiability results actually rely on local diffeomorphism, which is stronger than local invertibility. Therefore, I think it is necessary to justify that this relaxation is non-trivial. By “trivial,” I mean the situation where one advertises that the framework does not require Condition A, but actually assumes Condition A holds in all the relevant region in the space (i.e. only not hold on a set of measure 0). It would be nice if you can add a discussion clarifying the gap between global invertibility and local diffeomorphism. For example, could you analyze a practical case like the “responsibility problem” that fails to be globally invertible on the space of interest but still satisfies the local-diffeomorphism condition?\n\n2. In the theoretical framwork of this paper, S is the souce factors space, Z is the representation/target factor space, X is the observational space. But before def 1 (line 120-123), you only introduce S and X. Notation Z and term 'target factor' first appear in def 1 without any explanation. Maybe you can introduce Z and target factor before def 1 like what you have done with S and X. I think this may make the paper easier to read.\n\n\n\n3. The theorem numbering in the appendix does not match the numbering in the main text."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1zFsGAS3VD", "forum": "0VVdai71xb", "replyto": "0VVdai71xb", "signatures": ["ICLR.cc/2026/Conference/Submission20719/Reviewer_Crp8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20719/Reviewer_Crp8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953529150, "cdate": 1761953529150, "tmdate": 1762934123169, "mdate": 1762934123169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses a new set of principles for the identifiability problem of disentangled representations. Distinguished from existing works that usually assume statistical independence, this paper proposes mechanistic independence instead, to achieve identifiability even when the mixing function is non-invertible. Various related mechanistic independence conditions are proposed, most of which advance some branches of existing literatures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper proposes a novel perspective for latent variable identification, i.e., consider disentanglement directly as the goal instead of traditional slot/block-wise equivalence. This is intuitive since global invertibility is not required here.\n2.\tThis paper gives a thorough discussion on the proposed mechanistic independence, providing several options for the identifiability principle. Most options have close relation to related works from different branches of identifiability research, and are unified nicely into one framework.\n\nI am particularly interested in the novel results that global invertibility of mixing function is not required in this paper. Since almost all works on identifiability rely on this assumption, this would be an important advancement if its implication is properly discussed. See my question 1 for discussion."}, "weaknesses": {"value": "1.\tAlmost no experiment is provided in this paper, which is my major concern for this paper. I understand this is mainly a theoretical paper. However, at least experiments on synthetic data should be provided (Fig. 1 is not enough), to validate each of your main theorems. Without experimental results, it is hard to understand the application scenario of different mechanistic independence principles, and reliability of the theorems are also weakened.\n2.\tLack of explanation for the applicability of proposed assumptions in real applications. It is nice to have so much different independence principles as choices, but all principles lead to one common learning method (Fig. 1). I think it is infeasible to select one principle by validating each assumption in practice, and lacking customized methods significantly blurs the boundary among different principles.\n\nI do not insist that identifiability works *must* include experimental results. But for this paper, I think it is helpful and necessary to demonstrate the superiority given that large part of results are upgrades of existing works."}, "questions": {"value": "1.\tThis work does not rely on global invertibility of mixing function, how much is this condition be relaxed? I find that local diffeomorphism is still need. Together with path-connectivity, can global invertibility be derived? If not, please provide examples, and discuss how possible is it for such cases to happen in real applications.\n2.\tHow to determine L (the number of blocks in Z) in practice? Setting $L=1$ clearly leads to trivial results.\n3.\tIn Line 86, how to understand the symbol $D_{ij}^n$? Should “id” be 0 instead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hL82XXu5KZ", "forum": "0VVdai71xb", "replyto": "0VVdai71xb", "signatures": ["ICLR.cc/2026/Conference/Submission20719/Reviewer_xBHk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20719/Reviewer_xBHk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998014536, "cdate": 1761998014536, "tmdate": 1762934122496, "mdate": 1762934122496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified framework for achieving identifiable disentangled representations through \"mechanistic independence\", a principle that characterizes latent factors by how they act on observed variables (via the generator) rather than by their statistical distribution. The authors propose several related independence criteria (Type D, M, S, and H_n) and prove that each yields identifiability of latent subspaces under nonlinear, non-invertible mixing. They establish a hierarchy among these criteria and provide a graph-theoretic characterization of latent factors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The novel theoretical perspective shifting from statistical to mechanistic independence is interesting and practically motivated (e.g. by compositionality and transfer learning). This paper is an interesting step connecting disentanglement with discovering the mechanistic structure of data-generating processes."}, "weaknesses": {"value": "The main weakness of the paper is that the theoretical ideas are not especially intuitive. For example, the paper could better discuss when the various independence assumptions are likely to hold in practice. It might help to include more empirical results, not so much because experiments are required to validate the theory, but because experiments might give intuition about settings where the theory is actually relevant and which assumptions hold in which experiment settings.\n\nAs a more minor note, the main text would benefit from a short description of the experimental setup (without reading Appendix C, I didn't understand what Figure 1 shows at all)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iqwbMkS9Hy", "forum": "0VVdai71xb", "replyto": "0VVdai71xb", "signatures": ["ICLR.cc/2026/Conference/Submission20719/Reviewer_UZdZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20719/Reviewer_UZdZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131566595, "cdate": 1762131566595, "tmdate": 1762934122051, "mdate": 1762934122051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for identifiability of disentangled representation learning based on mechanistic independence. The authors start from properties of the generator from latent variables to observations, rather than from the distribution of latent variables. A family of independence criteria is discussed: Type D (disjoint support via Hadamard orthogonality of Jacobian columns), Type M (mutual non-inclusion of supports in a fixed basis), Type S (a strict sparsity-gap condition on Jacobian representations), and Type H (vanishing cross *n*-th-order derivatives plus *n*-th-order separability). The paper proves identifiability from the local to the global level and provides a graph-theoretic view where factors correspond to connected components. But the assumptions are not tested in experiments or used to guide new methods. Overall, this paper offers a different view of identifiability of disentangled representation learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proof of identifiability is detailed and step-by-step, going further.The proof looks right and kind of makes sense.\n- The whole paper is connected by mechanistic independence, which is an interesting view for identifiability.\n- Graph-based characterization of factors as connected components aids intuition and potential diagnostics.\n- The paper considers a more general generating process than previous works."}, "weaknesses": {"value": "- Several key assumptions are strong, hard to verify, or basis-dependent: Type M depends on a fixed canonical basis; Type S requires the existence and uniqueness of a sparsest product-splitting basis.\n- The assumptions are not tested in well-controlled experiments or real-world cases, and the theory doesn't contribute to methods. There is little evidence on real datasets or complex architectures.\n- The limitations of mechanistic independence are not specifically discussed."}, "questions": {"value": "- How can these independence criteria be deleted or used in cases such as a synthetic dataset?\n- Can mechanistic independence be linked with current theories? Like in intervations: the second derivative for different components $i, j$ is zero like Type M somehow."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3Z8RS0o4Sm", "forum": "0VVdai71xb", "replyto": "0VVdai71xb", "signatures": ["ICLR.cc/2026/Conference/Submission20719/Reviewer_7EYq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20719/Reviewer_7EYq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140020559, "cdate": 1762140020559, "tmdate": 1762934121621, "mdate": 1762934121621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of identifiability in disentangled representation learning, particularly for nonlinear generative models, by introducing a unified framework based on mechanistic independence. This principle defines latent factors not by their statistical distribution but by how they act on the observation manifold through the generator function. \nThe authors propose a hierarchy of mechanistic independence criteria (and corresponding irreducibility notions) ranging from\nType D (Disjointness), Type M (Mutual Non-inclusion), Type S (Sparsity Gap), to Type H_n (Higher-Order Separability).\nFor each criterion, the paper provides identifiability theory. Furthermore, this framework successfully generalizes and unifies several existing mechanistic constraints based or sparsity based identifiability results."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Clarity: The paper is very well-written.\n\nRelevance: The paper is very relevant to the field and the contribution is significant. The framework represents a fundamental conceptual advance, providing a robust, distribution-agnostic foundation for identifying latent factors. \n\nGenerality: The framework is broad, covering multi-dimensional factors, partial disentanglement, and non-invertible generators. The Theorem 1 result, extending local to global disentanglement under mild topological assumptions, looks like a strong tool.\n\nUnification: By establishing identifiability for a range of mechanistic constraints under nonlinear, non-invertible mixing and statistically dependent latent factors, the paper generalizes and expands several very recent, disparate results."}, "weaknesses": {"value": "The experimental setting looks limited (and not clear to me).  Plus identifying robust surrogate losses for the relaxed criteria, specifically Type M (mutual non-inclusion) and Type S (sparsity gap), remains an open problem. However, given the solid theoretical contribution, these weaknesses are not major."}, "questions": {"value": "Practical Losses for Type M/S: Can the authors propose initial or speculative ideas for a robust surrogate loss that is more reliably minimized in practice?\n\nIndependence Hierarchy: Could the authors include in the figure what are the precise conditions under which one criterion is strictly weaker or stronger than another?\n\nExperimental setting: It is not very clear to me that which types of independence are considered in the experiments? By following the setup of Brady et al. (2023) are you saying that Type D is considered? Have you considered any preliminary empirical validation for the other types like $H_n$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M7VV6Wdhi1", "forum": "0VVdai71xb", "replyto": "0VVdai71xb", "signatures": ["ICLR.cc/2026/Conference/Submission20719/Reviewer_aEKR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20719/Reviewer_aEKR"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169086545, "cdate": 1762169086545, "tmdate": 1762934120370, "mdate": 1762934120370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}