{"id": "JT6hR0sNXZ", "number": 4810, "cdate": 1757770498363, "mdate": 1759898011853, "content": {"title": "MoCtrl4D: Precise and Efficient Motion-Guided 4D Content Generation", "abstract": "Promptable 4D generation is a crucial task with broad applicability across industries, thus has recently gained tremendous interest in research community. However, existing works remain predominantly limited to image and text conditioning, which neglect the nuances of motion controllability. To address this, we propose to use dynamic motion prompt defined by any number of point trajectories. \nTo translate user intention into this motion representation, we design a user-friendly interface that allows users to intuitively input motion trajectories, bringing images to life through direct interaction. Unlike prior works, in leveraging prior knowledge of a base reconstruction model, our method integrates prompts without added modules, maintaining scalability and data efficiency without overhead, achieving a full forward pass in under a second. Furthermore, instead of relying on existing appearance-focused learning frameworks, which suffers from poor motion fidelity, we design a novel physically inspired \\textit{Vector Consistency Loss (VCL)} function for explicit motion learning. \nOur quantitative and qualitative results show significant improvement in spatiotemporally-precise and expressive control.", "tldr": "", "keywords": ["4D content generation", "motion control"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9dbf97f62cdf2789fa661c09964cf9500cefafa.pdf", "supplementary_material": "/attachment/f635efe431a2698e8640c14a36652c360cb9935b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes MoCtrl4D, a pipeline for controllable 4D generation. Compared to previous works, which mainly rely on text to define motion control, this work controls motion through point trajectories. Given static images and motion trajectories, the method synthesizes 4D Gaussians that can be rendered from any viewpoint and timestep. The model is supervised with appearance and geometry losses separately."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Important task: Motion-controllable 4D generation is an important and underexplored task."}, "weaknesses": {"value": "- Weak results: The quantitative results are not very convincing, and the qualitative results lack much motion. The videos are generally super short and it is not clear why the motion is so limited, since Objaverse training data does have larger amount of motion.\n- Weak result presentation: The supplementary video material, crucial for such 4D generation papers, is not very polished. I highly recommend a supplementary website with side-by-side comparisons with previous works. The results also do not have any full 4D visualizations, i.e., space-time renderings where both time and viewpoint are changing, as well as freeze-time and freeze-camera renderings. \n- Missing comparisons: The paper does not compare with motion-controllable 4D generation methods, e.g., MagicPose4D [1] or SP4D [2]. It mainly compares with L4GM, a feed-forward 4D reconstruction method that models each time separately and does not have any motion control.\n\n[1] Zhang et al., MagicPose4D: Crafting Articulated Models with Appearance and Motion Control, TMLR 2025 \\\n[2] Zhang et al., Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation, NeurIPS 2025"}, "questions": {"value": "I am convinced by the task the paper is trying to solve but the results are not convincing.\nI would like authors to address following questions:\n- Why is the motion is so limited?\n- How does the method compare to MagicPose4D or SP4D, i.e., other motion-controllable 4D generation frameworks?\n\nI am pretty negative and it is rather unlikely that I will raise my score to an accept but I am still happy to consider a rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vrX2usyKxX", "forum": "JT6hR0sNXZ", "replyto": "JT6hR0sNXZ", "signatures": ["ICLR.cc/2026/Conference/Submission4810/Reviewer_RLzK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4810/Reviewer_RLzK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760576156430, "cdate": 1760576156430, "tmdate": 1762917588692, "mdate": 1762917588692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoCtrl4D, a feed‑forward, motion‑prompted 4D content generation framework that lifts a single image plus user‑drawn point‑trajectory prompts into dynamic 3D Gaussians with an intuitive UI and sub‑second forward pass claims. The core ideas are a trajectory‑image injection that avoids extra control modules and a Vector Consistency Loss (VCL) that complements ARAP to supervise per‑Gaussian motion more precisely than appearance‑only training."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a direct motion‑prompt mechanism using point trajectories that provides fine‑grained, expressive control beyond text/image prompts.​\n2. Proposes “trajectory images” that integrate seamlessly into a base reconstruction model, avoiding ControlNet‑style module duplication and extra parameters.\n3. Novel VCL loss for more specialised motion control.\n4. Mitigates opacity‑driven degeneracies by predicting appearance only at the first frame while updating geometry over time, improving temporal consistency."}, "weaknesses": {"value": "1. The presentation is extremely poor. There is no direction to understand the supplementary material videos. I suggest using a html page so that the reviewers understand what the videos indicate and what exactly to interpret. Authors have just poured in a bunch of videos in the suplementary material without proper direction to understand them. \n\n2. As a work on dynamics and 4D content generation, I would request atleast seeing 20-30 videos to see the fidelity of the work and not just assume that these are just cherry-picked results. Also few videos in the supplementary almost have no motion at all. It would also be good to see the same object undergoing different motion trajectories, like atleast 2-3 different motions to understand the generalisability of the model. So same object but different motion points to get different 4D gaussians and visualise their dynamics. \n\n3. The paper is also really poorly written. It is extremely difficult for the reviewer to understand what the author is trying to convey at several places like: Fig 2 caption: Figure caption should be extremely descriptive and should describe each module properly. In the figure, you have kept a bar graph of what I assume is Resblock, Multi-view attn and Temporal attn but what do the height signify? Please be more clear. Also in the input, I am really confused as to what exactly is the input? Why does the first row contain 4 images and then 2nd and 3rd row to have one point image and 3 rgb images and again 4th row contains different set of rgb images. Please properly describe the input and output space. \n\n4. The comparisons are very limited. I don’t see a single image except Fig 3, which has only comparison with one method on only 2 data samples. I would request the authors to provide more comparisons on more scenes and also comparisons with more baselines and previous works. This level of comparison is not acceptable for the standard of this conference. \n\n5. It would also be great to include a user-study done on atleast 10-15 scenes with varying object motions and view points. More photos of the UI could be shown atleast in the appendix to understand the flexibility of motion control in the UI. \n\n6. How does the model scale to real-world objects. All the results are shown on synthetic dataset which is good but what matters is results on real-world examples. It would be great if you could show results on few real-world objects. \n\n7. Also how does the model scale to longer motion trajectories. There are no discussions on failure cases which I recommend adding and explaining the limitations of the work."}, "questions": {"value": "I have highlighted the major questions in the weaknesses already, and I am summing them up below(I have summarised them shortly so that it is easier for the reviewer to quote the exact question they are answering. Please refer to the Weakness for the detailed problem and question asked.\n\n1. Could you organize the supplementary videos in a clearer way, perhaps using an HTML page, so that it’s easier for readers to understand what each video shows and what we’re supposed to interpret from them? Right now, the supplementary material feels like a random collection of videos without structure or explanation.\n\n2. Why are there so few videos presented? Since this paper focuses on 4D dynamics and content generation, it would be good to see at least 20–30 videos to properly judge fidelity. With so few examples, it’s hard to tell if the results are representative or cherry-picked.\n\n3. Some supplementary videos seem to have almost no motion at all. Could you explain why that is the case? Are those examples meant to show static scenes, or is the model struggling to generate realistic motion?\n\n4. Could you include examples where the same object undergoes different motion trajectories, say two or three variations, to test whether the model generalizes well and to better visualize its learned 4D Gaussian dynamics?\n\n5. In Fig. 2, could you clarify what the bar heights represent for the ResBlock, Multi-View Attention, and Temporal Attention modules? The caption and figure are not clear on what those heights signify.\n\n6. What exactly is the input to the model? The figure is confusing because the first row shows four images, the second and third rows have one point image and three RGB images, and the fourth row has a completely different combination. Could you describe the input and output spaces more clearly?\n\n7. Why are the comparisons so limited? Apart from Fig. 3, which only compares to one method on two samples, there are no other comparisons. Could you add more comparisons with other baselines and across a wider range of scenes?\n\n8. Would it be possible to include a small user study on around 10–15 scenes with varying object motions and camera viewpoints? That would help quantify perceptual quality and realism.\n\n9. Could you include more images of the UI (perhaps in the appendix) to show how flexible the motion control interface is?\n\n10. How does the method perform on real-world scenes? Currently, all the results seem to be on synthetic data. It would strengthen the paper to show at least a few examples on real-world objects.\n\n11. How does the method handle longer motion trajectories? There is no discussion of whether performance degrades or artifacts appear with extended motion.\n\n12. Could you include a section discussing failure cases and the main limitations of your approach? This would help readers understand where the method works well and where it struggles."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nmwBd0FjnO", "forum": "JT6hR0sNXZ", "replyto": "JT6hR0sNXZ", "signatures": ["ICLR.cc/2026/Conference/Submission4810/Reviewer_Nryf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4810/Reviewer_Nryf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760847390548, "cdate": 1760847390548, "tmdate": 1762917587378, "mdate": 1762917587378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MoCtrl4D is a motion-promptable 4D Gaussian generation framework that takes a single image + user-specified motion trajectories as input and outputs dynamic 4D Gaussians in a single forward pass. Instead of text prompts, it encodes explicit 2D point trajectories into a “trajectory image” and feeds it directly into an existing L4GM video-to-4D reconstruction model without adding any new control modules. To improve motion fidelity, it proposes a new Vector Consistency Loss (VCL) to complement ARAP and better enforce rigid yet directionally correct deformation. Experiments on Objaverse show improved motion controllability (EPE) over L4GM while maintaining similar appearance quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Using explicit 2D trajectories as motion prompt is a very reasonable and underexplored alternative.\n\n2. The UI in Fig. 4 is intuitive and gives users precise, fine-grained control over where to move and how to move.\n\n3. The trajectory prompt is natively injected via “trajectory image” encoding, without increasing model size. This is clean and lightweight design.\n\n4. VCL loss is a well-justified improvement over vanilla ARAP, addressing the translation vs. rotation ambiguity that ARAP alone cannot disambiguate."}, "weaknesses": {"value": "1. The output 4D quality is still visually limited. Motions are generally small, conservative, and low-energy (mostly local deformations, not full articulated or large trajectory motion).\n\n2. Appearance fidelity is mediocre. Textures look relatively flat/synthetic / Objaverse-style, clearly lagging behind 4Real, Imagen-3D, DynamiCrafter, or even modern feed-forward Gaussian works. It feels more like reconstructed animation than genuinely generated cinematic 4D content.\n\n3. Relies almost entirely on synthetic Objaverse rigged assets, lacking evidence of generalization to real images or real natural motion.\n\n4. Core novelty is not architectural. It is fundamentally an extension of L4GM with trajectory conditioning + new motion supervision loss. The “no extra control module” aspect is good engineering, but not necessarily a strong research novelty — many will see this as “just encoding trajectories into RGB channels” rather than a deep innovation."}, "questions": {"value": "1. How robust is your method to natural photos or messy in-the-wild images?\n\n2. Does your method still work if the motion trajectories are large or non-rigid?\nCan it handle non-linear / high-amplitude / self-occluding motion? Or does it break because the model was only trained on Objaverse’s mild rig motions?\n\n3. How do you guarantee the generated motion is physically plausible?\nThe VCL loss enforces relative vector consistency, but is there any failure case where Gaussians drift or jitter unnaturally? A failure case visualization would be important to assess reliability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d248OtYN6w", "forum": "JT6hR0sNXZ", "replyto": "JT6hR0sNXZ", "signatures": ["ICLR.cc/2026/Conference/Submission4810/Reviewer_ghJy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4810/Reviewer_ghJy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761439863894, "cdate": 1761439863894, "tmdate": 1762917587052, "mdate": 1762917587052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a promptable 4D generation framework that enables motion control through user-defined point trajectories. It provides an intuitive interface for users to create motion prompts and animates static images interactively. Unlike prior methods that add complex modules, this approach integrates prompts directly into a base reconstruction mod. Additionally, it proposes a Vector Consistency Loss (VCL) to improve motion fidelity based on physical principles, overcoming the limitations of appearance-focused methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•  The paper addresses an important problem in 4D generation.\n\n•  Providing users with an interactive experience is very interesting.\n\n•  The results demonstrate a diverse range of cases."}, "weaknesses": {"value": "1.\tHow does this work differ from SC4D [1], which also uses sparse point control?\n[1] SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer\n\n2.\tFrom the demonstrations, it seems that users need to perform multiple operations and drag several points. On average, how much time does it take to manipulate one example during inference? How many keyframes need to be adjusted — only the first and last frames, or multiple ones? Would using more frames improve accuracy while increasing interaction time?\n\n3.\tPrevious works mainly focused on video-to-4D methods. With the recent progress in controllable video generation and stronger video base models (e.g., Wan, Veo3), how should we evaluate the quality of control signals? My understanding is that video-to-4D approaches use a short front-view video as input, while your method relies on a single image and sparse points. How do you compare the advantages of these two settings?\n\n4.\tCompared with rigging-based methods, where motion sequences are either learned from data or derived from motion libraries, your point-dragging approach seems to require heavier user interaction. For complex actions such as lifting a leg or dancing, rigging methods tend to produce smoother and more natural motion. In contrast, I feel that manually controlling motion through sparse points may lead to less natural and less continuous results.\n\n5.\tAccording to Table 1, there appears to be little performance improvement. Could this be because the model initializes L4GM weights and uses the same datasets? Compared with L4GM, this work employs the same architecture with an additional input signal, which makes the contribution seem somewhat incremental.\n\n6.\tDue to the limitations of the L4GM architecture, the model can only generate very short clips — the supplementary materials show results of just about one second. The visual results are not particularly impressive, and it’s unclear what practical value a one-second 4D generation result could have.\n\n7.\tIn the supplementary materials, the ablation experiments (Exp1–5) are not clearly explained — it’s hard to tell what each experiment corresponds to."}, "questions": {"value": "Please refer to the weeknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XFkDfUneFc", "forum": "JT6hR0sNXZ", "replyto": "JT6hR0sNXZ", "signatures": ["ICLR.cc/2026/Conference/Submission4810/Reviewer_y7rG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4810/Reviewer_y7rG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801778423, "cdate": 1761801778423, "tmdate": 1762917586788, "mdate": 1762917586788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}