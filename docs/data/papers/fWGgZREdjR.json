{"id": "fWGgZREdjR", "number": 17632, "cdate": 1758278569280, "mdate": 1759897163697, "content": {"title": "Dynamic Locally Linear Graph Learning for Geometry-Aware GNNs", "abstract": "The accuracy of GNN-based node classification depends critically on the quality of the constructed graph. Common heuristics such as $k$NN or Gaussian kernels rely on Euclidean distances, which in high dimensions fail to capture manifold structure and often introduce spurious cross-class edges. During message passing, these edges propagate inconsistent signals and, with deeper layers, lead to over-smoothing where node embeddings become indistinguishable. To address these issues, we propose VLGNN, an end-to-end framework that integrates graph construction with GNN training. A variational autoencoder (VAE) encodes the full data distribution into a latent space, thereby capturing global structure, while an LLE-inspired module refines the graph by enforcing local manifold consistency. The graph structure is updated jointly with the VAE-based encoder and the GNN-based classifier, allowing neighborhoods to adapt to the learned representations and to preserve meaningful within-class relations during training. Spectral analysis further shows that our LLE-based method enlarges Laplacian eigengaps and reduces inter-class conductance, indicating weaker cross-class propagation and alleviation of over-smoothing. On standard benchmarks, VLGNN achieves higher accuracy with reduced cross-class mixing.", "tldr": "We propose a VAE-LLE framework for dynamic graph construction that preserves local geometry, reshapes spectra, and mitigates over-smoothing in GNNs.", "keywords": ["graph neural networks", "dynamic graph construction", "locally linear embedding", "variational autoencoders"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/84edb1c095a168f835fb3983df0aba1c6ca29628.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper propose a graph learning method that utilize locally linear embedding to construct the graph. The model is trained over three joint loss, VAE reconstruction loss, a LLE reconstruction loss and a classification loss. This allows the model to dynamically generate the best graph and learned weight for the problem. The method shows consistent improvement over heuristic and other learnable method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Incorporating LLE into the system is well-motivated. The paper also explained the overall framework very well.\n\n- The empirical evaluation are strong, particularly for the obtain graph properties."}, "weaknesses": {"value": "- The joint learning and LLE shed lights on how to construct the graph along with the GNN learning process. The overall contribution is limited, as the dynamic learning is also processed by previous work. Adapting LLE is a good plus, but one question arises, one can adopt many different manifold learning method to this scenario, and why not using those? What's the uniqueness of LLE?\n\n- A great portion of the paper is spend on discuss adopted feature, which can be put into appendix.\n\n- The experimental results focuses on GNN-based method, some non-gnn geometric learning method should also be compared.\n\n- While it's nice to have the graph properties results, it does not really help us understand how it improves the targe task. We can see quite significant improvement of these properties, but the improvement on prediction tasks is relatively minimal, which poses skepticism on why these properties? These properties can just be cherry-picking, and the author did not provide a higher-level insight into the advantage of LLE over other methods.\n\n- The authors mentioned theoretical insights, but there are not theoretical insights.\n\n- More ablation study should be conducted. For example, the authors should try to detach impact of LLE and GNN. One approach can be: get a graph with ground-truth edges, use the method to construct the graph and compare the constructed graph with other method's constructed graph. This can help us understand the mechanism behind the method."}, "questions": {"value": "NA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qa4O0jHojx", "forum": "fWGgZREdjR", "replyto": "fWGgZREdjR", "signatures": ["ICLR.cc/2026/Conference/Submission17632/Reviewer_mrmE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17632/Reviewer_mrmE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698686922, "cdate": 1761698686922, "tmdate": 1762927492694, "mdate": 1762927492694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VLGNN, an end-to-end framework combining a VAE, a LLE-inspired graph construction module, and a GNN classifier. The idea is to dynamically update the graph structure based on geometry-aware affine relationships in the latent space to mitigate over-smoothing and improve node classification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is clear and easy to follow.\n\n2. The effort to link geometry, Laplacian eigenspectrum, and over-smoothing mitigation is reasonable."}, "weaknesses": {"value": "1. The supposed method, i.e., embedding LLE-based adjacency updates into a dynamic GNN, is incremental. Many prior studies (e.g., IDGL, SE-GSL, OpenGSL, DAE-GSL, DG-Mamba) have already explored joint representation and graph optimization. LLE-style affine reconstruction and local manifold priors have also been integrated into deep learning (e.g., LLE-GCN, Geometry-Enhanced GNNs, adaptive neighbor methods). Here, the novelty is mostly repackaging, i.e., combining an old manifold learning technique with a standard VAE and a GNN.\n\n2. The so-called “theoretical insights” are little more than intuitive restatements. The claim that LLE suppresses cross-class edges and balances spectral energy is descriptive, not theoretically derived. There is no rigorous link between the spectral range expansion and generalization or stability. \n\n3. All three datasets (HAR, HAPT, WISDM) are wearable-sensor datasets, which are low-dimensional, smooth, and well-suited to LLE assumptions (locally linear manifolds). This setting trivially favors LLE, which is known to perform well when manifolds are nearly flat. No tests are conducted on non-Euclidean or relational graphs (e.g., citation, molecule, or social networks), which undermines the generality of the claims."}, "questions": {"value": "1. How is VLGNN distinct from previous works, such as IDGL (ICLR 2020), adaptive LLE-based GNNs (PR 2023)?\n\n2. Does your method work on graphs where linear locality assumptions fail (e.g., heterophilic or discrete feature graphs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "emVRJAPbha", "forum": "fWGgZREdjR", "replyto": "fWGgZREdjR", "signatures": ["ICLR.cc/2026/Conference/Submission17632/Reviewer_rkFS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17632/Reviewer_rkFS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922943969, "cdate": 1761922943969, "tmdate": 1762927491940, "mdate": 1762927491940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper claims that standard graph heuristics introduce spurious cross-class edges that degrade GNN performance via over-smoothing. It proposes VLGNN, an end-to-end framework that integrates a VAE, a Locally Linear Embedding, and a GNN classifier. The VAE learns a latent representation, upon which the LLE module dynamically recomputes a graph at each epoch to enforce local affine consistency. The authors claim this process reduces cross-class edges and mitigates over-smoothing, demonstrating improved accuracy on three wearable-sensor datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of using a manifold-based prior LLE to construct the graph is principled and well-motivated\n2. The paper provides a strong empirical analysis of graph quality ICER/CCWS and over-smoothing layer-wise cosine similarity on the tested datasets, successfully linking the LLE-based graph to reduced cross-class mixing and slower feature homogenization."}, "weaknesses": {"value": "1. The method is computationally prohibitive. It requires a full kNN search and LLE weight computation for every node at every single training epoch. This is non-viable for any reasonably large-scale graph, a limitation the authors admit but understate.\n2. The method is only evaluated on three small, similar HAR sensor datasets (~10k nodes). This domain is best-case scenario for LLE. There is no evidence that it generalizes to standard, large-scale citation, social, or molecular graphs where manifold assumptions may not hold.\n3. No ablation study of VAE and LLE is provided."}, "questions": {"value": "1. Can the authors provide a computational complexity analysis (per epoch) for VLGNN versus the IDGL baseline? What is the actual wall-clock training time for VLGNN on the datasets?\n2. How does the model perform on a standard, large-scale benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NS9EFEgcLn", "forum": "fWGgZREdjR", "replyto": "fWGgZREdjR", "signatures": ["ICLR.cc/2026/Conference/Submission17632/Reviewer_HcTA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17632/Reviewer_HcTA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964791822, "cdate": 1761964791822, "tmdate": 1762927491197, "mdate": 1762927491197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VLGNN, an end-to-end framework integrating variational autoencoders (VAE), locally linear embedding (LLE), and graph neural networks (GNNs) for semi-supervised node classification.\n\n The motivation is that heuristic graph construction methods (e.g., kNN, Gaussian kernels) often fail to capture manifold structure, leading to noisy cross-class edges and over-smoothing in deep GNNs.\n\nVLGNN addresses this by jointly optimizing graph structure and node representations: the VAE learns global latent representations, the LLE module enforces local affine relationships to refine adjacency, and the GNN performs classification. Spectral analysis shows that the resulting LLE-induced graphs enlarge Laplacian eigengaps and reduce cross-class conductance. \n\nExperiments on three human activity recognition (HAR) benchmarks demonstrate consistent gains over static, diffusion-based, and dynamic baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear Motivation : The paper identifies a core problem of poor geometric fidelity of heuristic graphs. The proposed coupling of VAE + LLE + GNN is novel and sense-making, combining global manifold encoding and local linearity constraints.\n\n2. Good Empirical Results: VLGNN consistently outperforms a wide range of baselines—including static heuristics (KNN, Gaussian, etc.) and other learnable/dynamic methods (GDC, IDGL)—across all three sensor-based benchmark datasets. \n\n3.  The presentation of the paper is well-structured and easy to follow."}, "weaknesses": {"value": "1.Limited Scope of Evaluation: The Experiments are restricted to small, low-dimensional sensor datasets. The method’s scalability and generalization to larger, more complex graphs (PubMed, OGBN-Arxiv) remain untested. \n\n\n2. Computational Overhead: Although claimed efficient, per-epoch kNN + closed-form LLE computation may not scale to large datasets (connecting to weakness 1). Thus, runtime analysis would strengthen the claim. \n\n3. Generalizability of the Affine Prior: The local affine assumption is central to the method's success on HAR data. How do the authors expect VLGNN to perform on datasets where this assumption may not hold, such as in heterophilic graphs or sparse, power-law graphs (e.g., citation/social networks)? Would the LLE-based term potentially harm performance by enforcing an incorrect geometric prior?\n\n4. Theoretical Depth Could Improve – While qualitative spectral reasoning is provided, a more rigorous link between the LLE Laplacian and reduced over-smoothing dynamics would be valuable."}, "questions": {"value": "1. The neighborhood size $k$ is a critical hyperparameter for LLE. The sensitivity analysis in Table 5 shows performance peaking at $k=20$ or $k=30$ and then dropping. Does this suggest the model is sensitive to $k$, and how should one approach setting this hyperparameter for a new dataset without extensive tuning?\n\n2. How frequently must the LLE graph be recomputed to maintain gains—could partial updates or stochastic sampling work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ndzLSsr0q5", "forum": "fWGgZREdjR", "replyto": "fWGgZREdjR", "signatures": ["ICLR.cc/2026/Conference/Submission17632/Reviewer_bTeS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17632/Reviewer_bTeS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971386000, "cdate": 1761971386000, "tmdate": 1762927490769, "mdate": 1762927490769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}