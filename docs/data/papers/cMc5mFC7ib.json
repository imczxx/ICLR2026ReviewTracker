{"id": "cMc5mFC7ib", "number": 14636, "cdate": 1758240621428, "mdate": 1759897358022, "content": {"title": "SafeMoE: Leveraging Unsafe Data to Train Safer, More Informative LLMs", "abstract": "The increasing ease at which large language models can be accessed has spurred debate about ensuring their responsible usage and safety. While such models can act as boundless sources of knowledge, not all information is of equal value, especially to those who can potentially exploit it as a means of inducing harm, either to themselves or on others. Ensuring user satisfaction  while avoiding exposure of problematic information therefore remains an outstanding concern regarding their application to more sensitive settings, such as public health and education. In this work, we highlight the concern of blanket _refusal_, where models actively reject producing a detailed responses that risk exposing harmful information. Thus, safe informative responses can be difficult to attain, given the various barriers that need to be overcome. Yet unsafe data is readily available, in various unique domains, while also being rich in details that render them informative. Leveraging this fact, we introduce `SafeMoE`, a Mixture-of-LoRA based routing approach that utilizes fine-tuned domain-specific adapters, trained only on unsafe data, and a router, tuned to select among these experts using minimal safe response data, to ensure that models are both safe __*and*__ informative. Comparisons with safety-aligned models on multiple domains shows that `SafeMoE` not only trains models to be more helpful than existing baselines, with over 20\\% relative improvements in safe response rate (15\\%+ raw improvement) compared to the nearest competitor, but also provides more informative responses in settings where safety and harmfulness are of utmost concern, all the while being effective using only 100 total safe responses and generalizing to even domains without such responses available for training.", "tldr": "We use unsafe data to train domain-specific adapters integrated within in a Mixture-of-LoRAs paradigm with a router trained using safe response data.", "keywords": ["mixture-of-loras", "llm safety", "expert routing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79a485df0af940cd71bc8ac5f214fcc85511cf24.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper explores the issue of blanket refusal, where an LLM unconditionally denies certain requests without considering context or providing nuanced judgment. The authors propose SafeMoE, which consists of several expert LoRA adapter models trained on unsafe and knowledge data, alongside a small router model trained on safe data. The router model is responsible for selecting the appropriate LoRA expert adapters. Experiments show that SafeMoE outperforms the baselines, including base models of similar size, in terms of both safety and informativeness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe paper is generally easy to follow, though there is room for improvement as discussed below.\n\n-\tThe performance gains are impressive. However, evaluation could be improved as discussed below\n\n-\tThe problem being addressed is both important and interesting, though challenging. The method appears to work well in some cases, but a more thorough evaluation would provide better insights."}, "weaknesses": {"value": "-\tI have some reservations about the novelty of the work, as the proposed method largely involves training MoEs, which is quite similar to how existing MoE models are trained (e.g., GPT-OSS-20B (OpenAI et al., 2025), Qwen3-30B-A3B (Yang et al., 2025), Mixtral-8x7B (Jiang et al., 2024a), DeepSeek-V2-Lite (DeepSeek-AI et al., 2024), and Phi-3.5-MoE-Instruct (Abdin et al., 2024)). The key distinction appears to be the use of LoRA on top of the feed-forward networks (FFNs) instead of training the entire FFNs from scratch, along with the specific use of unsafe, knowledge, and safe data in the defined manner. The novelty might not be strong enough to meet the thresholds for publication.\n\n-\tThe main experiments rely solely on the SafeRLHF dataset. Expanding the evaluation to include other benchmarks (e.g., BeaverTails [1], XSTEST [2], HarmfulQA [3] etc.), would add value to the paper.\n\n-\tAdditionally, it would be nice to see cross-generation performance, where SafeMoE is trained on one dataset (e.g., BeaverTails training split) and tested on another (e.g., SafeRLHF test split).\n\n-\tSafeMoE is currently trained on top of the base Mistral model only. It would be interesting to see how SafeMoE performs when applied to other base models, such as those used as baselines in Table 1.\n\n-\tLines 284-285 are missing a citation. It would be nice to include Evalassist [4] alongside the LLM-as-Judge framework (Zheng et al., 2023; Gu et al., 2024).\n\n-\tI noticed the main results don’t show error bars or standard deviations. It’d be great to include them since they help show how consistent and reliable the results are, especially with randomness in training or data sampling. Error bars help assess the statistical significance and reproducibility of findings, which are critical for drawing reliable scientific conclusions.\n\n-\tThe code is not provided. While the method seems simple and easy to implement, the absence of code raises concerns about reproducibility.\n\n\n\n(Minor):\n\n-\tline 107: typo: \"a expert\" should be \"an expert\"\n\n-\tLine 202-203: grammar issue: begins with \"Although\", a subordinating conjunction, but doesn't include a main (independent) clause to complete the thought.\n\n-\tLine 441-442: typo: \"SafeLoRA use...\" should be \"SafeLoRA uses...\"\n\n-\tThe Discussions section primarily compares the SafeMoE approach to previous related works. It might be better to merge the Related Work and Discussions sections, as they both address similar comparisons and contextualization of SafeMoE in the broader research landscape.\n\n\n[1] Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2024b. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Advances in Neural Information Processing Systems, 36.\n\n[2] Paul R¨ ottger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: A test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv:2308.01263, 2023.\n\n[3] Rishabh Bhardwaj and Soujanya Poria. 2023. Red-teaming large language models using chain of utterances for safety-alignment. arXiv preprint arXiv:2308.09662.\n\n[4] Michael Desmond, Zahra Ashktorab, Werner Geyer, Elizabeth M. Daly, Martin Santillan Cooper, Qian Pan, Rahul Nair, Nico Wagner, and Tejaswini Pedapati. Evalassist: LLM-as-a-judge simplified. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 29637–29639, 2025."}, "questions": {"value": "-\tWhy does SafeMoE train the experts on (unsafe + knowledge) data and the router on safe data? Why not reverse this approach, i.e., train the experts on safe data and the router on (unsafe + knowledge) data? Would that make a difference, and what is the motivation behind using this particular order?\n\n\n-\tIn Figure 4, why was PKU-SafeRLHF not used for this experiment? Why were different datasets chosen instead? Is there a specific reason for this choice? Additionally, why not expand the main results by incorporating AdvBench and Harmbench?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YxGoVVt0hT", "forum": "cMc5mFC7ib", "replyto": "cMc5mFC7ib", "signatures": ["ICLR.cc/2026/Conference/Submission14636/Reviewer_WtHN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14636/Reviewer_WtHN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761238363574, "cdate": 1761238363574, "tmdate": 1762925012291, "mdate": 1762925012291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SafeMoE, a Mixture-of-LoRA approach that leverages abundant unsafe but informative domain data to train expert adapters, then learns a lightweight router using a small set of safe responses to combine experts at inference. Built on Mistral-7B, SafeMoE scales and consistently boosts both safety rate and informativeness, outperforming safety-aligned baselines (e.g., RealSafe-R1, Zephyr) and targeted methods (SafeLoRA, SN-Tune) on PKU-SafeRLHF, AdvBench, and HarmBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper targets safe yet informative alignment—moving beyond blanket refusals to guidance that preserves usefulness. This is clearly framed and motivated. \n\nSafeMoE trains domain LoRA experts on abundant unsafe data, then learns a lightweight router on a small safe set to select top-K experts per layer—yielding sparse, inference-efficient routing.\n\nScaling experts (8/10/19) improves both safety and informativeness and outperforms baselines (e.g., RealSafe-R1, Zephyr), reaching >90% safety with high quality.\n\nOnly small number of safe responses per domain suffice to boost safety and informativeness, highlighting practical deployability.\n\nSafety improves even on categories without safe responses, suggesting beneficial transfer from unsafe-data experts."}, "weaknesses": {"value": "Blanket-refusal example not representative. Fig. 1’s case seems cherry-picked; many LLMs preface refusals with “Sorry…” yet still provide guidance. Please specify model, decoding (top-k/temperature), and prompts under which true blanket refusals occur; tuned decoding may mitigate the issue.\n\nExperts are trained per domain, but the router selects layer-wise top-K experts dynamically, which may misalign with domain-level training; the router’s training loss/objective is not clearly stated (only selection rule/softsign and training hyperparameters are provided). Clarifying the supervision signal and alignment rationale would be helpful.\n\nExperiments only fine-tune Mistral-7B; results on newer/larger bases would strengthen claims.\n\nSafety/quality are judged by GPT-4o with custom prompts; add standardized metrics (e.g., AlpacaEval for quality; external harmfulness classifiers/APIs like OpenAI Moderation API for safety) would be helpful.\n\nProvide diagnostics of router behavior (expert usage entropy, per-domain routing patterns, ablations) to explain why informativeness and safety rise together.\n\nThe choice of baseline in section 4.2.2 is not very appropriate, the ST-Tune is trying to locate the neurons which controls the refusal behavior and trying to prune some layers to attack LLMs or defense the harmful prompts. They are not considering imformative. The author's method is not related to ST-Tune and the target of these two methods is not very aligned. Considering the author is using base model and unsafe data to improve safety level, a more suitable baseline could be TA-SFT [1] which also only use unsafe data\n\nReport over-refusal on XSTest, OR-Bench, CoCoNot, etc., to verify the method improves helpful safety rather than just increasing refusals.\n\n[1] Lu Y, Sinha A, Varakantham P. Semantic loss guided data efficient supervised fine tuning for safe responses in LLMs[J]. arXiv preprint arXiv:2412.06843, 2024."}, "questions": {"value": "Please refer to Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U09FnngBCr", "forum": "cMc5mFC7ib", "replyto": "cMc5mFC7ib", "signatures": ["ICLR.cc/2026/Conference/Submission14636/Reviewer_TnML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14636/Reviewer_TnML"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761504981552, "cdate": 1761504981552, "tmdate": 1762925011832, "mdate": 1762925011832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for training LLMs that can output safe and informative responses - ie acting safely but not overly refusing to answer. The intention of the method is to rely on primarily unsafe responses and a small number of safe + informative responses as training data. The approach is based on a mixture-of-experts LoRA paradigm, where different LoRAs are trained for different domains. Experimentally, they demonstrate improvements in informativeness and safety on text generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- focus on safety + informativeness as a dual objective is good, this is an important direction\n- focus on large amounts of unsafe data seems unique\n- secondary study on the amount of safe response data required is a nice investigation"}, "weaknesses": {"value": "Framing: \n- the motivation is a little unclear to me. I'm not sure I would agree that often unsafe data is much easier to find than safe data - frequently unsafe data is in the long-tail and can be quite rare. Often safety classification problems are quite imbalanced, with very few unsafe examples. Some more explanation about exactly what the authors envision about how this setup is common or useful would be helpful.\n- I don't totally understand how the multi-domain piece plays in with the safety piece of this - are they related goals? complementary? disconnected? Would be good to explain more how they relate\n\nMethodology:\n- D_knowledge dataset: it's unclear to me exactly what this  - is it mostly safe or unsafe data, or neither? is it overlapping with D_safe and D_unsafe, or a distinct dataset?\n- in general, I find the methodology presented here a little hard to grasp. It doesn't seem like Sec 3.3 really presents a safety-related method at all - I don't fully understand why we need unsafe and safe experts in each domain, and how a MoE method utilizes them to improve safe outputs. I may be missing the entire point of the paper but I don't see what the unsafe experts are used for - wouldn't they be used to generate unsafe outputs, which is bad?\n- I'm not sure why in a multi-domain setting, the router should select the top-K experts - wouldn't top-1 be better? if you know your domains in advance which it seems we do\n\nExperiments:\n- \"safe data\" is collected from GPT, which seems to contradict the idea from the paper's motivation that it is hard to obtain \n- it would be good to have some sort of evaluation of the LLM-as-judge system, to ensure that results are grounded and reliable to some degree\n- I would find it useful to have a more detailed description of what exactly the evaluation task is, I don't see it in the main body"}, "questions": {"value": "- how does a MoE with unsafe/safe experts help to produce more safe and informative responses?\n- how do the multi-domain and safety aspects of this work connect?\n- why is the motivation presented (lots of unsafe + informative data, little safe + informative data) realistic? in what cases does this occur?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "saCeg9tWsT", "forum": "cMc5mFC7ib", "replyto": "cMc5mFC7ib", "signatures": ["ICLR.cc/2026/Conference/Submission14636/Reviewer_QPms"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14636/Reviewer_QPms"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933102366, "cdate": 1761933102366, "tmdate": 1762925011443, "mdate": 1762925011443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}