{"id": "lDG61tzAyQ", "number": 7230, "cdate": 1758012362315, "mdate": 1759897864945, "content": {"title": "MoVE: Synergistic Integration of Temporal and Cross-Variable Experts for Efficient Multivariate Time Series Forecasting", "abstract": "Multivariate time-series forecasting presents the dual challenge of modeling intricate temporal dynamics and complex cross-variable dependencies. Prevailing approaches often prioritize one aspect at the expense of the other, leading to suboptimal performance. To address this limitation, we introduce MoVE, a novel framework that synergistically integrates temporal and cross-variable modeling within a unified architecture. MoVE employs two specialized experts. A Temporal Expert for capturing long-range dependencies and a lightweight Cross-Variable Expert for modeling robust cross-variable interactions. By decoupling these components within a Mixture-of-Experts framework and optimizing them collaboratively, MoVE dynamically adapts to diverse forecasting scenarios. Extensive experiments demonstrate that our framework achieves superior performance, establishing a new paradigm for effective multivariate timer series forecasting.", "tldr": "", "keywords": ["Time-Series Forecasting; Periodicity; MoE; Transformer;"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9a05e6f064bb4ebcfd2635f561cbed450eb0683.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the MoVE framework for efficient multivariate time series forecasting. MoVE decouples temporal modeling from cross-variable modeling into two expert modules: a Temporal Expert based on CycleNet for periodic modeling, and a Cross-Variable Expert (RCVTE) for cross-variable modeling. These two experts are dynamically gated and fused to adapt to different types of time series. The authors also introduce Period-Aware Local Curriculum Learning (PALCL) and Robust Cross-Variable Attention (RCVA) to enhance training stability and generalization capabilities. Experiments on multiple benchmark datasets validate the performance of MoVE, achieving new state-of-the-art results on the ETTh1, Electricity, and Solar datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. By decoupling temporal modeling from cross-variable modeling through the MoE framework, the model's interpretability and adaptability are improved.\n2. The introduction of PALCL and RCVA effectively alleviates the attention collapse problem, enhancing training stability."}, "weaknesses": {"value": "1. The experimental results only achieved first place in 7 out of 16 cases, which is not very good.\n2. Figure 1 is very schematic and does not well illustrate the overall architecture of the model.\n3. MoVE has limited innovation, with an architecture that is very similar to the SST[1]. Moreover, there is a lack of relevant performance comparisons.\n4. The core modules of MoVE (such as RCVTE and PALCL) are, to some extent, combinations and improvements of existing methods (CycleNet, iTransformer), lacking entirely original modeling mechanisms.\n[1] SST: Multi-Scale Hybrid Mamba-Transformer Experts for Long-Short Range Time Series Forecasting"}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Cvzr7NCF2J", "forum": "lDG61tzAyQ", "replyto": "lDG61tzAyQ", "signatures": ["ICLR.cc/2026/Conference/Submission7230/Reviewer_goo3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7230/Reviewer_goo3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893087979, "cdate": 1761893087979, "tmdate": 1762919373160, "mdate": 1762919373160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework, MoVE, which synergistically integrates temporal and cross-variable modelling within a unified architecture. MoVE employs two specialised experts: a temporal expert to capture remote dependencies, and a lightweight cross-variable expert to model robust cross-variable interactions. By decoupling these components within a hybrid expert framework and optimising them collaboratively, MoVE dynamically adapts to diverse forecasting scenarios. The authors experimentally validate the effectiveness of the approach."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The motivation is clear. The paper contends that prevalent methods often prioritise one aspect at the expense of another, resulting in suboptimal performance. To address this limitation, the paper introduces MoVE, a novel framework that synergistically integrates temporal and cross-variable modelling within a unified architecture."}, "weaknesses": {"value": "1) The authors assert in the abstract that multivariate time series forecasting faces the dual challenge of modelling complex temporal dynamics and intricate cross-variate dependencies. Popular approaches often prioritise one aspect at the expense of the other, resulting in suboptimal performance. Nevertheless, numerous methods have already addressed the simultaneous modelling of temporal and variate dependencies, such as [1,2]. The innovation of the authors' approach is limited.\n\n[1] TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables [NeurIPS24]\n\n[2] TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable-and Time-Aware Hyper-state [ICML25]\n\n2) Poor visualization. The authors provide no visualizations in the paper, preventing readers from observing advantages over other methods. Furthermore, ablation studies also lack visualization. \n\n3）The architecture diagram (i.e. Figure 1) appears rather unappealing, and the caption lacks a narrative description of the method's workflow.\n\n4) The content of the paper is insufficient. The abstract is merely half a page long, and much of the relevant prior work has not been mentioned. Furthermore, the entire manuscript barely reaches eight pages. The details of many methods have not been described clearly."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ml996UxdRB", "forum": "lDG61tzAyQ", "replyto": "lDG61tzAyQ", "signatures": ["ICLR.cc/2026/Conference/Submission7230/Reviewer_A3WK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7230/Reviewer_A3WK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896597953, "cdate": 1761896597953, "tmdate": 1762919372756, "mdate": 1762919372756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework called MoVE (Mixture of Various Experts), designed to address the dual challenge of simultaneously capturing complex temporal dynamics and cross-variable dependencies in multivariate time series forecasting. The core innovation of MoVE lies in its mixture-of-experts architecture, which decouples temporal modeling and cross-variable modeling into two specialized expert modules: one for capturing long-term dependencies through a temporal expert (based on CycleNet), and another cross-variable expert (the newly proposed RCVTE module). Experimental results on multiple benchmark datasets demonstrate the superior performance of MoVE, accompanied by an extensive ablation study."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Significance: Effectively integrating temporal modeling with cross-variable dependency learning is a well-recognized core challenge in multivariate time series forecasting. This paper directly tackles this issue.\n\nOriginality: In contrast to existing studies that adopt static fusion methods for capturing periodic patterns and cross-variable dependencies, this paper introduces a mixture-of-experts architecture that adaptively integrates both aspects. This approach is clear in its methodology and offers a novel research perspective for addressing challenges in this field."}, "weaknesses": {"value": "The cross-variable dependencies modeling: The Recurrent Cross-Variable Transformer Encoder (RCVTE), mentioned multiple times in the paper, appears to be merely a module that uses a simple cross-attention mechanism to capture the dependencies between variables, with some modifications to the attention computation formula. However, many prior works (such as Crossformer in the baseline) also utilize basic cross-attention mechanisms to model inter-variable dependencies. Given that the paper emphasizes this module as one of its core innovations, I believe it would benefit from further clarification of the advantages of this module and its unique contributions compared to previous studies.\n\nThe temporal dimension modeling: The paper introduces two specific experts (the temporal modeling expert and the inter-variable relationships modeling expert), but seems to focus primarily on the latter, with little discussion on the former. The paper only briefly mentions \"a temporal expert inspired by CycleNet\" without providing a detailed explanation of the specific structure of this expert."}, "questions": {"value": "1. Periodic Dependency: The proposed model seems to heavily rely on the recurrent, noise-free periodic vector extracted by CycleNet. Is there a risk that the model's predictions may become overly dependent on CycleNet? Additionally, could the model potentially become too reliant on the periodicity of the data itself?\n\n2. Gated Network Weights: In the Cross-Variable Expert section, the model constructs four experts and outlines the role of each. Could the authors provide examples of the weights (g_1, g_2, g_3, g_4) of these experts in different scenarios, such as when the historical sequence exhibits periodicity or when there is no clear periodicity?\n\n3. Curriculum Learning: The paper introduces Periodicity-Aware Local Curriculum Learning (PALCL). Could the authors further elaborate on the motivation or necessity of employing a curriculum learning strategy? Additionally, would it be possible to include an experimental group that does not use the curriculum learning training strategy in the ablation study to demonstrate the actual effectiveness of this approach?\n\n4. Network Scale: Could the authors provide more details on the scale of the model’s network parameters, such as the total number of experts? For all the datasets involved in the experiments, does the Recurrent Cross-Variable Transformer Encoder consist of only one layer? Furthermore, when the number of channels is large (e.g., 862 channels in the Traffic dataset), does calculating channel attention result in substantial storage consumption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C4KaHqgjbx", "forum": "lDG61tzAyQ", "replyto": "lDG61tzAyQ", "signatures": ["ICLR.cc/2026/Conference/Submission7230/Reviewer_WPzk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7230/Reviewer_WPzk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905075605, "cdate": 1761905075605, "tmdate": 1762919371968, "mdate": 1762919371968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MoVE, a Mixture-of-Experts style architecture that attempts to combine a “temporal expert” (based on CycleNet) and a lightweight “cross-variable expert” (RCVTE) with a learned gating network and a Periodicity-Aware Local Curriculum Learning (PALCL) training protocol."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The high-level idea of combining separate modules that target temporal periodicity and cross-variable coupling is reasonable and aligns with prior modular/MoE thinking."}, "weaknesses": {"value": "1.\tThe proposed system is largely an engineering assembly of existing pieces (CycleNet temporal module, a single-layer transformer for cross-variable interaction, a simple linear gating network, RevIN). The paper provides no compelling new modeling principle beyond “put these together and gate them.”\n2.\tThe authors state broad superiority, but inspection shows the model achieves the best result on only four datasets/tasks\n3.\tTable 5 shows a limited ablation (only on six datasets) and omits a crucial ablation: the effect of PALCL itself. Given that PALCL is one of the principal methodological claims, the absence of an experiment that toggles PALCL on/off (with otherwise identical settings) is a serious oversight.\n4.\tThe detailed architecture of Recurrent Cross-Variable Transformer Encoder (RCVTE) are not clear, the architected should be plotted."}, "questions": {"value": "Provide an ablation that evaluates the model with and without PALCL\nReplace RCVTE with simpler alternatives and report accuracy/computer.\nAdd parameter counts, FLOPs, and measured inference latency and VRAM usage to allow cost-benefit comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zH93XwEeX5", "forum": "lDG61tzAyQ", "replyto": "lDG61tzAyQ", "signatures": ["ICLR.cc/2026/Conference/Submission7230/Reviewer_a321"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7230/Reviewer_a321"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961559236, "cdate": 1761961559236, "tmdate": 1762919371548, "mdate": 1762919371548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}