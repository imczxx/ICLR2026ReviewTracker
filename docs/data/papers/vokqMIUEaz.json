{"id": "vokqMIUEaz", "number": 6725, "cdate": 1757993496479, "mdate": 1759897898573, "content": {"title": "SWE-Dev: Training and Evaluating Autonomous End-to-End Feature-Driven Software Development", "abstract": "Large Language Models (LLMs) have shown strong capability in diverse software engineering tasks, e.g. code completion, bug fixing, and document generation. However, feature-driven development, a highly prevalent real-world task that involves developing new functionalities for large, existing codebases, remains underexplored. We therefore introduce SWE-Dev, the first large-scale dataset (with 14,000 training and 500 test samples) designed to evaluate and train autonomous coding systems on real-world end-to-end feature-driven software development tasks. To ensure verifiable and diverse training, SWE-Dev uniquely provides all instances with a runnable environment and its developer-authored executable unit tests. This collection not only provides high-quality data for Supervised Fine-Tuning (SFT), but also enables Reinforcement Learning (RL) by delivering accurate reward signals from executable unit tests. Our extensive evaluations on SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent Systems (MAS), reveal that feature-driven software development is a profoundly challenging frontier for current AI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test split). Crucially, we demonstrate that SWE-Dev serves as an effective platform for model improvement: fine-tuning on training set enabled a 7B model comparable to GPT-4o on hard split, underscoring the value of its high-quality training data.", "tldr": "We introduce a dataset, SWE-Dev, which is the first large-scale dataset designed to evaluate and train autonomous AI systems for feature-driven software development task.", "keywords": ["Large Language Models", "Software Development", "Coding Benchmark", "Coding Dataset", "LLM Training", "Multiagent", "RL Training"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/666ede6d7c0254dfabb60d198f9bc11f58817d5f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SWE-Dev, a large repository-level dataset for end-to-end feature-driven software development. It contains 14,000 training and 500 test tasks, each with a runnable environment and developer-authored unit tests, enabling execution-based evaluation and training. Experiments span 17 chatbot LLMs, 10 reasoning models, and 10 multi-agent systems. The authors report that strong closed models still struggle (e.g., Claude-3.7-Sonnet ~24.25% Pass@3 on Hard) and that SFT/RL yield modest gains; they also show simple MAS often outperform heavier agent frameworks.\n\nFrom this reviewer's perspective, the dataset collection approach is compelling and addresses a timely need in the community. However, the evaluation protocol, training setups, and subsequent conclusions raise concerns about the validity of the design choices and, thus, their generalizability. Lack of comparison with recent datasets and benchmarks raises a question of novelty."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Examples of high-quality, diverse SWE tasks are still limited. SWE-Dev makes a valuable contribution to this domain by providing over 14,000 tasks with a specific focus on feature development.\n2. While GitHub issues can include feature requests, most are bug reports. The proposed methodology is well-suited for generating feature-development tasks, which helps correct the existing bias towards bug-fixing in other datasets.\n3.  The call tree serves as a compelling method for differentiating task complexity. Figure 8a demonstrates that this approach is indeed effective for adjusting task difficulty."}, "weaknesses": {"value": "1. The related work section on LLMs for coding does not fully cover recent, relevant works. The authors claim that SFT and RL have \"largely focused on function-level tasks,\" but this overlooks several works [1, 2, 3] that apply RL at scale to complex SWE tasks.\n2. The paper lacks a comparison with [4], a popular large-scale dataset with SWE tasks for training, especially given its similar approach of using tests and AST for task generation.\n3. The paper claims key distinguishing characteristics (\"Realistic scale,\" \"Robust evaluation,\" \"Verifiable training set\") but fails to compare against [5], where similar claims are made.\n4. Adding [6] to the benchmark comparison in Table 1 would provide greater transparency.\n5. The paper does not appear to address common failure modes of synthetic dataset generation. For example, it is unclear if the test suites sometimes imply a specific class or function name that an agent could only guess, rather than deduce.\n6. The LLM evaluation methodology is simplistic and not representative of current best practices. SOTA evaluation typically involves running multi-step agents (e.g., SWE-agent, OpenHands). Even simpler agentic approaches like Agentless involve separate steps for localization and generation. The conclusions drawn from the paper's evaluation and subsequent training raise concerns about their generalizability.\n7. The described methodology is not language-agnostic, as it relies on Python-specific methods for code tracing. This limits its generalizability to other programming languages.\n8. Some evaluation results seem contradictory, such as Qwen2.5-72B-Instruct performing similarly to Claude-3.7-Sonnet on the easy subset.\n9. The RL training parameters are questionable. If the 2k-task dataset is used for 5 epochs, this yields 10k total trajectories. With a batch size of 256, this results in only ~40 training steps, which seems insufficient to achieve reliable improvements.\n\n[1] Michael Luo et al. DeepSWE: Training a Fully Open-sourced, State-of-the-Art Coding Agent by Scaling RL.\n\n[2] Alexander Golubev et al. Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning.\n\n[3] Shiyi Cao et al. SkyRL-v0: Train Real-World Long-Horizon Agents via Reinforcement Learning.\n\n[4] John Yang et al. SWE-smith: Scaling Data for Software Engineering Agents.\n\n[5] Ibragim Badertdinov et al. SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents.\n\n[6] Naman Jain et al. R2E-Gym: Procedural Environment Generation and Hybrid Verifiers for Scaling Open-Weights SWE Agents."}, "questions": {"value": "1. Could the authors elaborate on the process described as \"We use structural properties of the call tree (e.g., depth and node count) to identify key function nodes\"? Specifically, how do you differentiate nodes that represent core feature logic from common utilities or helper functions?\n2. Can you provide a more detailed analysis of why the evaluated reasoning models have a lower Instruction Following Rate (IFR)? What are the authors' insights on this observed behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ak4VNXSSPr", "forum": "vokqMIUEaz", "replyto": "vokqMIUEaz", "signatures": ["ICLR.cc/2026/Conference/Submission6725/Reviewer_icEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6725/Reviewer_icEc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684803418, "cdate": 1761684803418, "tmdate": 1762919013344, "mdate": 1762919013344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SWE-Dev, a large-scale repository-level dataset designed for feature-driven software development, a realistic yet underexplored setting distinct from bug fixing or function completion.  Each task in SWE-Dev is constructed from real open-source repositories by anchoring on executable unit tests. The authors dynamically trace test execution to build function-level call trees, identify and mask the core implementation regions, and generate refined Project Requirement Descriptions (PRDs) describing the intended functionality.  The dataset comprises 14k training and 500 test samples drawn from over 1,000 repositories, each with runnable environments, enabling verifiable execution-based evaluation (Pass@k).  SWE-Dev also supports supervised fine-tuning (SFT), reinforcement learning (RL), and multi-agent (MAS) training paradigms. Experiments cover 17 chatbot LLMs, 10 reasoning-oriented LLMs, and 10 multi-agent systems, analyzing how performance varies with data scale, reasoning strategy, and task complexity (measured by call-tree depth and node count). SWE-Dev provides a realistic, reproducible, and scalable benchmark for studying end-to-end autonomous software development."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work targets feature addition and integration,a dominant portion of real-world programming that existing datasets fail to capture,offering a meaningful shift from isolated code completion benchmarks.  \n\n- The construction process forms a coherent, automatable pipeline with controllable task difficulty through structural parameters.  \n\n- By grounding evaluation in developer-written unit tests, the benchmark yields clear functional correctness signals rather than proxy metrics like BLEU or CodeBLEU.  \n\n- The study spans multiple LLM families and training paradigms (SFT, RL, MAS), providing cross-model trends and demonstrating SWE-Dev’s discriminative power in evaluating realistic code-generation capability.  \n\n- The dataset aligns well with real development workflows, enabling both evaluation and fine-tuning research; the design choices are transparent, and the implementation appears reproducible."}, "weaknesses": {"value": "- The claim that “reasoning models do not always outperform standard ones” lacks supporting analysis on temperature, sampling strategy, or reflection iterations, making causality unclear.  \n\n- Although RL reportedly improves Pass@1 but reduces diversity, quantitative measures such as output entropy, candidate uniqueness, or AST variance are absent.  \n\n- The paper defines task complexity via call-tree depth and node count but does not publish explicit thresholds or masking ratios, hindering full reproducibility.  \n\n- While MAS systems are benchmarked, there is little discussion of communication structure, coordination overhead, or emergent behaviors that might explain observed differences."}, "questions": {"value": "1. How sensitive are results to inference hyperparameters such as temperature, top-p/k, or reflection rounds?  \n2. Can the authors quantify the diversity trade-off in RL models using metrics like output entropy or unique-pass ratios?  \n3. What exact thresholds distinguish easy vs hard tasks in call-tree depth and node count, and are these thresholds consistent across repositories?  \n4. What evidence supports the claim that general multi-agent systems outperform code-specific ones,is this due to communication efficiency or broader role specialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BEslSFIhJf", "forum": "vokqMIUEaz", "replyto": "vokqMIUEaz", "signatures": ["ICLR.cc/2026/Conference/Submission6725/Reviewer_pjSU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6725/Reviewer_pjSU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926122689, "cdate": 1761926122689, "tmdate": 1762919012893, "mdate": 1762919012893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SWE-Dev, a large-scale dataset aimed at evaluating and training autonomous code-generation systems specifically for feature-driven development within large, real-world repos. The authors provide extensive empirical evaluations across multiple coding systems, chatbot LLMs, reasoning models, and multi-agent systems, demonstrating the challenges of FDD and the utility of SWE-Dev for advancing research in this area."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper studies feature-driven SWE, which is a novel idea.\n2. Extensive empirical study, ranging from benchmarking to model training, from SFT to RL..\n3. SWE-Dev is a huge dataset (14k samples, all with test cases)."}, "weaknesses": {"value": "Major concerns: \n1. The experimental section lacks necessary experimental details. For example, the PPO experiments lack a detailed account of important parameters such as reward shaping, max response length, the agent framework employed, and computational resource overhead. They also omit key data from the training process—such as the convergence of training set scores and entropy values. Consequently, the experimental results exhibit limited credibility and reproducibility. It is recommended that parameters of marginal relevance, such as checkpoint saving frequency, need not be elaborated in detail.\n2. SWE-Dev is really a big dataset (14k samples), but it also raises concerns about the quality of the data. I believe this paper lacks a direct verification of data quality, such as whether task generation is reliable, whether the tasks are solvable, and whether the test cases and environments are correct.\n3. Can training on Swe-Dev generalize to performance improvements on Swe-bench-verified? I believe this is also a good perspective to evaluate the data quality of Swe-Dev.\n\nMinor concerns:\n1. The authors seem to have slightly modified the ICLR template, reducing the section spacing on the first four pages.\n2. The citation formats in the paper are almost all incorrect.\n3. Could you provide more demo cases in SWE-Dev?"}, "questions": {"value": "Please refer to \"weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LUzEfROgMj", "forum": "vokqMIUEaz", "replyto": "vokqMIUEaz", "signatures": ["ICLR.cc/2026/Conference/Submission6725/Reviewer_59nz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6725/Reviewer_59nz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102735888, "cdate": 1762102735888, "tmdate": 1762919012441, "mdate": 1762919012441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors present a new benchmark targeting the training and evaluation of Code LLMs on repo-level feature development tasks. Their dataset provides a execution environment enabling training via RL algorithms. A held-out test set is proposed to evaluate the feature development capabilities of LLMs when applied with different variations of inference algorithms (single/multi agent etc)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Strong motivation: there is a clear need to evaluate complex, repository-level feature development beyond plain code completion/generation.\n  - Task complexity is reasonable (≈190 LOC across ~3 files).\n\n- Insights are useful—e.g., MAS appears on par with, or only slightly superior to, simpler systems.\n\n- I find this to be a valuable dataset for open-source/open-weight models to fine-tune on, though the value for evaluation is practically limited.\n\n- Provides a very exhaustive analysis of the dataset and training aspects—a rich set of experiments, with nice insights on single- vs multi-agent systems (inference-based).\n\n- The processed dataset has additional research value:\n  - test-quality analysis of these repos (coverage, etc.),\n  - proposing improvements to test cases of these popular repos,\n  - evaluating LLM-driven optimizations of source code while ensuring functional correctness w.r.t. these tests."}, "weaknesses": {"value": "- Potential test-set contamination: if external models have been trained on these source repos, we can’t guarantee the absence of leakage. I don't think this testing of whether a model can build a feature if it has already seen the source code during pre-training is a reliable signal of the model's performance.\n\n- Test quality is under-analyzed: how confident are we that existing tests confirm correctness? In practice, developers often target coverage rather than correctness. Even a qualitative analysis (e.g., by frontier models) of test thoroughness would help.\n\n- PRD quality/leakage risk: do the PRDs give too many hints to the model for feature development? This warrants a qualitative analysis.\n\n- Train–test split by repository: are splits done at the repo level? Training on method_A from repo_A can help generate method_B from the same repo_A at test time. A more sanitized evaluation would keep train and test repos distinct; otherwise, the evaluation is not fair."}, "questions": {"value": "- Static analysis: why not use static analysis to identify which source functions are called by which tests (and mask the entire call stack)?\n\n- Multiple entrypoints (hypothetical): suppose method_B is called by method_A and tested by test_A, and there is also a test_B calling method_B directly. If you mask m_A and m_B and evaluate only with test_A, the LLM might implement m_A and m_B to pass test_A but still fail test_B. Do you run all implicated entrypoint tests, or just one?\n\n- Line 255: what exactly is the “relevant code context”? How is it selected?\n\n- Spec leakage: isn’t augmenting a docstring with an implementation approach a form of leakage?\n\n- RL details: how are preference pairs constructed for DPO? Why not GRPO as a simpler alternative to PPO?\n\n- Presentation:\n  - whitespace between Fig. 1–Fig. 2 and text is too little,\n  - 269 — “falls short”,\n  - 293 — “development.”\n\n- Role of tests (under-discussed):\n  - what about using execution feedback from failing tests to allow the model to re-attempt the feature development task?\n  - what about scenarios where tests are unavailable but the feature must be written—can the LLM write tests first and then implement the feature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F7Nd2ABhhy", "forum": "vokqMIUEaz", "replyto": "vokqMIUEaz", "signatures": ["ICLR.cc/2026/Conference/Submission6725/Reviewer_uYqm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6725/Reviewer_uYqm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241424671, "cdate": 1762241424671, "tmdate": 1762919012110, "mdate": 1762919012110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}