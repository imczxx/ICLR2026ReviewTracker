{"id": "NdePButGas", "number": 22386, "cdate": 1758330382808, "mdate": 1763694457124, "content": {"title": "DynaSTy: A Framework for Spatio-Temporal Node Attribute Prediction in Dynamic Graphs", "abstract": "Accurate multi‐step forecasting of node‐level attributes on dynamic graphs is critical for applications ranging from financial trust networks to traffic monitoring. Existing spatio‐temporal graph neural networks typically assume a static adjacency, and seldom deal with multidimensional timeseries prediction. In this work, we propose an end‐to‐end dynamic edge‐biased spatio‐temporal model that ingests a multidimensional timeseries of node attributes and a timeseries of adjacency matrices, to predict multiple future steps of node attributes. At each time step, our transformer-based model injects the given adjacency as an adaptable attention bias, allowing the model to focus on relevant neighbors as the graph evolves. We further deploy a masked node/time pretraining objective that primes the encoder to reconstruct missing features, and train with scheduled sampling and a horizon‐weighted loss to mitigate compounding error over long horizons.\nUnlike prior work, our model accommodates dynamic graphs that vary across input samples, enabling forecasting in multi-system settings such as brain networks across different subjects, financial systems in different contexts, or evolving social systems.\nEmpirical results demonstrate that our method outperforms strong STGCN, DCRNN, and MTGNN baselines by 10–20% on most datasets.", "tldr": "This paper proposes a novel method for multistep, multidimensional node attribute prediction in dynamic graphs with evolving edges.", "keywords": ["dynamic networks", "node attribute prediction", "graph neural networks", "time series forecasting"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/504e84c0354c3b09812695bd9f7d93ff980fd9b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "DynaSTy focuses on multi-step node-attribute forecasting on dynamic graphs where the adjacency changes over time and differs across input samples (per subject, per market, per system) while the node set stays fixed. The core idea is a transformer encoder that injects the provided per-timestep adjacency as a learnable attention bias to steer attention toward currently relevant neighbors yet remaining permutation-equivariant and portable to new graphs. A GRU-based decoder with scheduled sampling and a horizon-weighted objective stabilises long-horizon rollouts. There is also a masked node-time pretraining objective to prime the encoder to reconstruct missing features using both attributes and dynamic edges."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper targets the setting where each training sample has its own evolving adjacency matrix but shares a fixed node set. This is an underexplored area that differs from standard dynamic graph learning (which usually assumes a shared topology). This formulation expands the applicability of spatiotemporal graph neural networks to domains like trust networks, where relational structures vary across instances.\n\n- The integration of time-varying adjacency matrices as additive attention biases in the transformer encoder is both conceptually and practically strong. This design preserves permutation equivariance while allowing the model to exploit edge dynamics directly, rather than relying on learned or static graph priors. \n\n- The combination of a transformer-based spatial encoder with a GRU decoder, augmented by scheduled sampling and horizon-weighted losses, is a good choice. These design choices directly address long-horizon error accumulation, which is often a  weakness in autoregressive forecasting."}, "weaknesses": {"value": "- While DynaSTy achieves strong empirical results, the core contribution mainly combines known components of transformer attention, GRU decoding, scheduled sampling and masked pretraining, only in a dynamic-graph setting. The novelty primarily lies in applying adjacency as an attention bias rather than in introducing a new learning principle. As a result, the method reads as a well-engineered system extension rather than a conceptual contribution for dynamic graph forecasting.\n\n- The paper positions its edge-biased transformer as novel but does not include comparisons with recent adaptive or dynamic attention-based models (e.g., TGAT, PDFormer, etc.). \n\n- Although the paper provides a single ablation removing the edge bias, it does not analyse how the bias magnitude or learning dynamics affect forecasting performance.\n\n- The masked node-time pretraining is introduced as an auxiliary step, but the experiments report only a coarse quantitative gain. There is no analysis of what the model learns during pretraining, which features or temporal relationships improve or whether pretraining remains beneficial when large supervised datasets are available."}, "questions": {"value": "- The paper describes the edge-bias term as a learnable function over the adjacency entries that guides spatial attention (Section 4.4). Could the authors clarify how this bias interacts with feature-based attention during training? how does the model ensure that the bias does not dominate content-based attention in sparsely connected graphs?\n\n- In Section 4.7, the masked pretraining stage is said to improve performance on datasets with noisy or irregular structures. Could the authors elaborate on whether the pretraining is conducted jointly across all datasets or separately per domain? Also, how is the adjacency sequence incorporated during reconstruction? does the pretraining explicitly use dynamic edge information to infer masked values, or is it feature-driven?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VriAwzdzZj", "forum": "NdePButGas", "replyto": "NdePButGas", "signatures": ["ICLR.cc/2026/Conference/Submission22386/Reviewer_upry"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22386/Reviewer_upry"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506804674, "cdate": 1761506804674, "tmdate": 1762942194234, "mdate": 1762942194234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DynaSTy proposes an end-to-end spatiotemporal model for multi-step node attribute prediction in dynamic graphs. The framework ingests time series of node attributes and adjacency matrices, using transformer-based spatial encoding with dynamic edge-biased attention and a GRU-based temporal decoder. Key points include handling per-sample dynamic graphs (different graphs across training samples), masked pretraining for representation learning, and scheduled sampling with horizon-weighted loss for robust multi-step forecasting. The method targets applications where both node attributes and graph structure evolve over time, such as brain networks, trust networks, and traffic systems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed model allows each sample to have its own dynamic graph, which better reflects real-world scenarios where different systems evolve with distinct graph structures. This is a key difference from previous models that rely on a fixed, global graph shared across all samples.\n2. The model uses a Transformer-based architecture with dynamic edge-bias attention, enabling it to capture spatial patterns in temporal dependencies more effectively. This is particularly important for systems where the graph structure changes over time.\n3. The paper includes experiments on real-world datasets, evaluating the model’s performance through ablation studies, hyperparameter sensitivity analysis, and runtime comparisons. These results help assess the model’s effectiveness and practicality for real-world deployment."}, "weaknesses": {"value": "1. Although the proposed model allows a different dynamic graph for each training sample, it still builds on a fixed set of nodes across samples. As a result, there is not a significant difference from prior works that also operate under a fixed node set. Both this work and earlier methods focus on transductive node attribute learning, where all nodes are known during training. If the research had instead focused on or included inductive node attribute learning—such as predicting attributes for newly emerging nodes in the future—the motivation and contribution would be stronger and more aligned with real-world dynamic scenarios.\n2. The model incorporates several components, including dynamic edge-bias attention, a GRU-based decoder, and scheduled sampling, which together increase its complexity and may limit its accessibility and adoption. Some parts of the model, especially the edge-bias mechanism, would benefit from clearer formal definitions or pseudocode to improve understanding. Additionally, the attention mechanism in DynaSTy scales quadratically with the number of nodes (O(N^2)), which can become computationally expensive for large graphs. While the paper mentions that sparse attention could be explored in future work, the current version may not be scalable to large-scale applications without further efficiency improvements.\n3. The paper’s writing and presentation can be improved for clarity. For example, Figures 1 and 2 are included but not thoroughly explained in the main text. Terms like “small MLP” are used without specifying their structure or dimensions. The architecture diagram in Figure 2 is hard to interpret due to overlapping elements and unclear data flow. Moreover, the font sizes in Figures 1, 2, and 3 are inconsistent, making important details difficult to read. These issues affect the overall readability and reproducibility of the work."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8uobAYiiU7", "forum": "NdePButGas", "replyto": "NdePButGas", "signatures": ["ICLR.cc/2026/Conference/Submission22386/Reviewer_xivb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22386/Reviewer_xivb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571360596, "cdate": 1761571360596, "tmdate": 1762942193947, "mdate": 1762942193947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the node attribute prediction in dynamic graphs. The problem takes the historical node attribute and the dynamic graph structure as inputs and outputs the multi-step node attributes in the future. The proposed method, DynaSTy, introduces a transformer-based model that adapts to the dynamic nature of the graph by incorporating adjacency matrices (the graph structure) at each time step as an attention bias. This allows the model to focus on relevant neighbors as the graph evolves. The authors evaluate the model with there real-world and semi-synthesis datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. Predicting multi-step node attributes in a dynamic graph is a good research question.\nS2. The paper clearly presents the method, making it easy to access and reproduce."}, "weaknesses": {"value": "W1. While the authors aim to predict future node attributes in dynamic graphs for real-world applications, such as the Bitcoin network, social networks, and biological systems, the proposed method may face limitations in terms of scalability. DynaSTy computes attention between all pairs of nodes, which incurs significant memory and computational costs. This issue could hinder its applicability to large, real-world graphs that contain a high number of nodes. This may also explain why only smaller graphs are considered in the experiments.\n\nW2. The proposed approach addresses a time-series prediction problem, but with relationships among variables. Previous studies that introduce structure to assist in prediction have tackled the same problem. In essence, this paper approaches the same issue from a different perspective -- specifically, the dynamic graph perspective.\n\nW3. Although the authors focus on dynamic graphs, the datasets used in the experiments primarily consist of independent time-series variables rather than dynamic graphs. Specifically, the graph structure in METR-LA is static, and the graph in fMRI is handcrafted."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3OBazwp4nn", "forum": "NdePButGas", "replyto": "NdePButGas", "signatures": ["ICLR.cc/2026/Conference/Submission22386/Reviewer_TPoc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22386/Reviewer_TPoc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841888372, "cdate": 1761841888372, "tmdate": 1762942193490, "mdate": 1762942193490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method, DynaSTy, for spatiotemporal node attribute prediction on dynamic graphs. It is a transformer-based model that ingests both a time series of node attributes ($X$) and, crucially, a time series of adjacency matrices ($A$). In order to achieve that, it uses a spatio-temporal transformer encoder that injects the adjacency matrix at each time step as an \"adaptable attention bias\". Their main contribution is this edge-biased attention mechanism, which allows the model to handle per-sample dynamic graphs, combined with a robust training strategy using masked pretraining, scheduled sampling, and a \"Variation loss\". They perform an empirical study on traffic (METR-LA, PEMS-Bay), trust (Bitcoin-OTC, Bitcoin-Alpha), and brain fMRI datasets. Lastly, there are empirical experiments done to conclude the superior performance of DynaSTy in contrast to other strong STGNN baselines like DCRNN, Graph WaveNet, and MTGNN"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper is easy to follows.\n\nS2. The paper addresses a major limitation of most existing Spatiotemporal Graph Neural Networks (STGNNs) like DCRNN, STGCN, and MTGNN, which assume a single, static adjacency matrix for all time steps and sample."}, "weaknesses": {"value": "W1. Fixed Node Set Assumption: The most significant limitation, explicitly stated by the authors, is that the model \"assumes that the node set remains fixed over time\". This is \"unrealistic\" for many real-world dynamic graphs (e.g., social or biological networks) where nodes constantly appear and disappear.\n\nW2. Unclear Justification for Static Graph Performance: On the traffic datasets (METR-LA and PEMS-Bay), the graph is static. DynaSTy's core novelty which is handling a dynamic $A_t$ is not actually leveraged here, yet it still achieves top performance. This suggests its strong results on these datasets are due to other components (e.g., the transformer architecture, pretraining, or loss function) rather than its primary advertised contribution.\n\nW3. Confusing Runtime Results: The paper's text describes the $O(N^2)$ complexity as a \"bottleneck\", but the experimental results in Table 4 show DynaSTy has a faster average training time per epoch than most major baselines, including DCRNN, Graph WaveNet, MTGNN, and DGCRN16. This surprising and positive result seems to contradict the scalability limitations discussed in the text and is not explained.\n\nW4. Paper presentation. The figures font size is sometimes small and not clear enough. The table size and figure size is also inconsistent."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ejJozVTIYk", "forum": "NdePButGas", "replyto": "NdePButGas", "signatures": ["ICLR.cc/2026/Conference/Submission22386/Reviewer_nUFV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22386/Reviewer_nUFV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918285197, "cdate": 1761918285197, "tmdate": 1762942193275, "mdate": 1762942193275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}