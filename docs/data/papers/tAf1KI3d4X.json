{"id": "tAf1KI3d4X", "number": 8035, "cdate": 1758053589494, "mdate": 1759897812758, "content": {"title": "Physics vs Distributions: Pareto Optimal Flow Matching with Physics Constraints", "abstract": "Physics-constrained generative modeling aims to produce high-dimensional samples that are both physically consistent and distributionally accurate, a task that remains challenging due to often conflicting optimization objectives. Recent advances in flow matching and diffusion models have enabled efficient generative modeling, but integrating physical constraints often degrades generative fidelity or requires costly inference-time corrections. Our work is the first to recognize the trade-off between distributional and physical accuracy. Based on the insight of inherently conflicting objectives, we introduce a method that enforces physical constraints at training time using conflict-free gradient updates and unrolling to mitigate Jensen's gap. Our approach avoids manual loss balancing and enables simultaneous optimization of generative and physical objectives. As a consequence, physics constraints do not impede inference performance. We benchmark our method across three representative PDE benchmarks. PBFM achieves a Pareto-optimal trade-off, competitive inference speed, and generalizes to a wide range of physics-constrained generative tasks, providing a practical tool for scientific machine learning.", "tldr": "Physics-Based Flow Matching (PBFM) is a generative modeling framework that enforces physical constraints during training", "keywords": ["Flow Matching", "Physics", "PDE", "Diffusion Models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cc4fad245a1decf2da87a3880f33d2a53e3f328b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Physics-Based Flow Matching (PBFM) that balances physical consistency with distributional accuracy. The authors identify the inherent trade-off between optimizing for physical fidelity (via PDE residuals) and generative quality as a core challenge. PBFM addresses this through conflict-free gradient updates using the ConFIG method, eliminating the need for manual loss balancing. Additionally, the method employs unrolling during training to mitigate Jensen's gap—the discrepancy arising when physical constraints are imposed on intermediate predictions rather than final samples. Experiments across three benchmarks demonstrate that PBFM achieves Pareto-optimal performance, outperforming baselines in both physical residual error and distributional metrics while maintaining competitive inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Novel conflict-free optimization:** The paper identifies the conflicting gradients between generative and physical objectives and proposes an elegant solution using conflict-free gradient updates. This approach is innovative and eliminates the challenging manual tuning of loss weights, enabling simultaneous optimization of both objectives.\n    \n**No inference overhead:** Unlike existing physics-constrained methods that require costly iterative sampling procedures at inference time, PBFM maintains competitive inference speed comparable to standard flow matching, making it practical for real-world applications.\n    \n**Strong experimental results:** The method demonstrates superior performance across three diverse benchmarks, consistently achieving lower physical residuals and better distributional metrics compared to baselines. The Pareto-optimal trade-off and comprehensive ablation studies validate the effectiveness of the proposed approach."}, "weaknesses": {"value": "**Scalability Concerns.** While the paper demonstrates improvements on 2D problems with modest resolutions (64$\\times$64 and 128$\\times$128), the training overhead raises serious concerns for practical applications. The authors acknowledge that unrolling with 4 steps incurs up to 3$\\times$ memory consumption and 2.5$\\times$ training time. For higher-dimensional problems (e.g., 3D PDEs at 128$^3$ resolution), the computational cost would scale dramatically: (1) storing intermediate states during unrolling becomes prohibitively expensive, (2) computing physical residuals (gradients, divergence) in 3D is substantially more costly, and (3) the ConFIG gradient projection requires additional backward passes. Moreover, more challenging PDEs typically require a larger number of diffusion steps for accurate sample generation, which directly translates to proportionally higher memory consumption during unrolling, potentially rendering the method infeasible. The method's applicability to industrially-relevant high-resolution 3D problems remains unclear, potentially limiting its impact to low-dimensional scenarios where the training overhead is manageable.\n\n**Insufficient Analysis of ODE vs. Stochastic Sampling Discrepancy.** A fundamental principle of continuous normalizing flows is that ODE and SDE samplers should yield samples from the same distribution when properly trained. However, the empirical results in Table 3 show significant distributional differences between deterministic (t* = 0.0) and stochastic samplers (t* > 0), with Wasserstein distance varying from 1.470 to 0.138. This discrepancy suggests that the proposed training procedure may introduce optimization biases that favor one sampling paradigm over the other. The paper claims stochastic sampling as a core contribution (Contribution IV), yet provides no theoretical or empirical investigation into why the training methodology creates this distribution mismatch. Possible explanations include: (1) the conflict-free gradient updates may inadvertently bias the learned velocity field toward deterministic trajectories, (2) the unrolling procedure evaluates residuals on ODE-integrated paths, potentially misaligning with stochastic sampling dynamics, or (3) the residual weighting scheme may not be sampling-invariant. The authors should provide: (1) ablation studies isolating which training components cause the ODE-SDE discrepancy, (2) analysis of whether standard flow matching (without physical constraints) exhibits similar behavior under their training scheme, and (3) theoretical or empirical justification for why their method fundamentally requires stochastic sampling rather than fixing the training procedure to achieve sampling-invariance.\n\n**Extensive Hyperparameter Tuning Still Required.** While the paper claims to avoid manual loss balancing, the method still requires careful tuning of multiple hyperparameters including $\\sigma_{\\text{min}}$, $t^*$ for stochastic sampling, number of unrolling steps, and the power $p$ for residual weighting. Table 5 demonstrates significant sensitivity to $\\sigma_{\\text{min}}$ choice, with performance varying substantially across different values. This undermines the claim of automatic balancing and may limit practical applicability.\n\n**Minors.** Inconsistent use of $x_1$ and $x_0$: In Algorithm 1 and Figure 1, authors use $x_1$ to denote the data, while on line 069, 070, 196, they use $x_0$. Missing reference: Generating Physical Dynamics under Priors; PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models; Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems.\n    \nIf the authors can address the computational scalability to higher-dimensional problems and provide theoretical/empirical justification for the ODE-SDE sampling discrepancy, I would be willing to increase my score."}, "questions": {"value": "**Question on Stop Gradient for Unrolling.** In Algorithm 1, the unrolled prediction is computed via n integration steps and then used to evaluate the physical residual. This requires backpropagating through the entire unrolling process, leading to the reported memory and training time increase. Have you considered applying a stop gradient operation to before computing the residual? This technique is commonly used in single-step diffusion models and consistency models to reduce computational cost. \n\n**Question on Training-Inference Step Mismatch for Complex PDEs.** The paper demonstrates that the method benefits from unrolling during training and that increasing inference steps consistently reduces residual errors. However, for highly complex PDEs with non-smooth ODE flows, accurate sampling may require substantially more discretization steps (e.g., 50-100 steps). In such cases, training with only a few unrolling steps might fail to generate physically meaningful samples for computing the residual loss, as the partially-evolved states after 1-4 steps could be far from any valid solution. Could the authors discuss: (1) whether this training-inference step mismatch has been observed in practice, (2) how the method would perform when complex PDEs necessitate many inference steps, and (3) whether a curriculum strategy progressively increasing unrolling steps during training could address this issue without making the memory overhead prohibitive? Understanding this limitation would help assess the method's applicability to more challenging physical systems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NXCprFKBXU", "forum": "tAf1KI3d4X", "replyto": "tAf1KI3d4X", "signatures": ["ICLR.cc/2026/Conference/Submission8035/Reviewer_f5gi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8035/Reviewer_f5gi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789563490, "cdate": 1761789563490, "tmdate": 1762920030890, "mdate": 1762920030890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces \"Physics Based Flow Matching\" (PBFM), a method that treats the tension between physical consistency and generative fidelity as a Pareto optimization problem. By combining conflict-free gradient updates, residual unrolling, and stochastic sampling, PBFM claims to achieve a Pareto-optimal trade-off between residual accuracy and sample quality across PDE benchmarks, without a significant increase in inference cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The empirical study is comprehensive, including ablations on unrolling, stochastic sampling, and Gaussian noise effects, which clarifies the contribution of each component and isolates the respective observed gains.\n\nS2: The experimental benchmarks are well chosen and physically meaningful, spanning PDEs with increasing complexity. The results demonstrate stable improvements across tasks, and the method manages to maintain minimal inference overhead despite additional training components.\n\nS3: Writing is clear and easy to follow."}, "weaknesses": {"value": "**W1.** Many components of the proposed framework: residual-based losses from PINNs (Raissi *et al.*, 2019), **ConFIG** gradient orthogonalization [4], and stochastic sampling from ECI [1] are adapted rather than novel. The main contribution lies in integrating these existing mechanisms under a Pareto framework rather than introducing fundamentally new algorithmic principles. Moreover, since PBFM enforces *soft* constraints, it cannot match the strict physical consistency of hard-constraint approaches such as ProbConserv, ECI, PCFM, and conservation-respecting models [6], which exhibit stronger residual convergence and physical invariance guarantees.\n\n**W2.** Methods like PCFM [2] and ECI [1] already demonstrate superior trade-offs between physical residual and generative accuracy, achieving lower MMSE and FPD (aligned with the Wasserstein metric used here). These approaches explicitly enforce hard constraints throughout training and inference, leading to simultaneous gains in both physical and distributional metrics. In contrast, PBFM’s ConFIG balancing produces only a single equilibrium solution along this trade-off, without exploring or controlling the broader Pareto front. While ConFIG offers one way to mitigate gradient conflict between physics and generative objectives, I assume PBFM adopts it primarily to avoid the conflicting gradient behavior observed in PINNs. However, without broader comparisons, theoretical justification, or reference to recent developments in conflict-free training [7], it remains uncertain whether this choice is optimal within the Flow Matching framework. This uncertainty is compounded by the lack of sufficient empirical evaluation across Pareto trade-offs, leaving the “Pareto-optimal” characterization only partially substantiated.\n\n**W3.** There should be some quantitative comparison to prior physics-aware diffusion and flow methods [1, 2, 3, 5]. The explanation that ECI [1], PCFM [2], and PIDDM lack public implementations or cannot handle overlapping residuals is not convincing. Public code availability is not a sufficient reason to omit benchmarking; if previous works report standardized results, at least partial or analytical comparisons should be provided. For example, **ECI** has an open implementation and published results on the *Heat* and *Navier–Stokes* PDEs. Even if PBFM targets showcases results on different PDEs, evaluating it on these established datasets would contextualize its improvements. As written, the analysis offers only a partial view of progress relative to existing literature.\n\n**W4.** The claim of being “gradient-free” is not strictly accurate, as the method still differentiates through residuals during training. Additionally, the unrolling step introduces extra back-propagation and memory overhead, thereby reducing one of flow matching’s main advantages: its computational simplicity, and without requiring backpropagation through an ODE solve. \n\n\n**References**\n\n[1] Cheng, C., *et al.* “Gradient-Free Generation for Hard-Constrained Systems (Extrapolation–Correction–Inference).” *arXiv preprint* arXiv:2412.01786 (2024).\n\n[2] Utkarsh, U., *et al.* “Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints.” *arXiv preprint* arXiv:2506.04171 (2025).\n\n[3] Christopher, J. K., S. Baek, and N. Fioretto. “Constrained Synthesis with Projected Diffusion Models.” *NeurIPS* 37 (2024): 89307–89333.\n\n[4] Liu, Q., Chu, M., and Thuerey, N. “ConFIG: Towards Conflict-Free Training of Physics-Informed Neural Networks.” *arXiv preprint* arXiv:2408.11104 (2024).\n\n[5] Yao, J., Mammadov, A., Berner, J., Kerrigan, G., Ye, J. C., Azizzadenesheli, K., and Anandkumar, A. “Guided Diffusion Sampling on Function Spaces with Applications to PDEs.” *arXiv preprint* arXiv:2505.17004 (2025).\n\n[6] Hansen, D., *et al.* “Learning Physical Models That Can Respect Conservation Laws.” *ICML*, PMLR, 2023.\n\n[7] Wang, Sifan, et al. \"Gradient alignment in physics-informed neural networks: A second-order optimization perspective.\" arXiv preprint arXiv:2502.00604 (2025)."}, "questions": {"value": "See Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aNJuxqLzFL", "forum": "tAf1KI3d4X", "replyto": "tAf1KI3d4X", "signatures": ["ICLR.cc/2026/Conference/Submission8035/Reviewer_XbPf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8035/Reviewer_XbPf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918432463, "cdate": 1761918432463, "tmdate": 1762920030504, "mdate": 1762920030504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies physics-constrained generative modeling and introduces a method that enforces physical constraints during training using conflict-free gradient updates and unrolling. This approach avoids manual loss balancing, improves generative fidelity, and enables efficient inference. The paper also presents numerical evidence supporting the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a fundamental problem in generative modeling: how to enforce known physical constraints to improve the fidelity of generated data.\n\n- The proposed method requires more memory storage and training time, which might not scale well for larger systems.\n\n- The paper clearly identifies the challenge of conflicting gradient updates and introduces a corresponding method that enables gradient descent to simultaneously reduce both the matching loss and the physical constraint loss."}, "weaknesses": {"value": "-The proposed method appears somewhat straightforward, as it primarily leverages an existing approach (ConFIG) to compute conflict-free gradient updates.\n\n- Section 3.3 is a bit difficult to follow. The authors could add a figure to illustrate why this issue needs to be addressed and to clarify the concept of Jensen’s gap. In particular, is this a typo - should the final clean sample be $x_1$ rather than $x_0$?"}, "questions": {"value": "I think the authors could include a figure illustrating the concept of conflict-free gradient updates. This would help a general audience better grasp the idea and make the content more accessible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Z39tA9NMuB", "forum": "tAf1KI3d4X", "replyto": "tAf1KI3d4X", "signatures": ["ICLR.cc/2026/Conference/Submission8035/Reviewer_EdS2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8035/Reviewer_EdS2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984370488, "cdate": 1761984370488, "tmdate": 1762920030046, "mdate": 1762920030046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework that use flow-matching to generate admissible solution for PDEs. The key feature of the paper is that it recongize the potential gradient conflicts between the distribution fidelity and the physical consistency and propose a way to obtain the Pareto optimal solutions. Across three benchmarks—Darcy (steady), Kolmogorov (divergence-free), and a challenging dynamic stall aerodynamics case with analytic constraints—PBFM improves the physics–vs–distribution trade-off and maintains FM-like inference cost (e.g., competitive wall-clock and NFE), while acknowledging higher training memory/time due to unrolling and an extra backward pass. Figures and tables report consistent residual drops, better Wasserstein/JS metrics than inference-time constraint baselines, and near-FM inference speed; a discussion contrasts training overheads with the gain in accuracy and shows large end-to-end speedups over classical solvers once trained. The study is quite completed and the problems the authors selected for numerical experiments are chosen carefully."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "It makes sense to cast “physics vs. distribution” as a true multi-objective problem and use conflict-free gradient updates (ConFIG), so that each step descends both the flow-matching loss and the physics residual, thereby avoiding brittle manual weights and consistently improving the Pareto front over fixed-weight baselines.\n\nIt mitigates Jensen's gap through training-time unrolling, thereby lowering the residual errors without incurring additional inference costs. \n\nI appreciate that the authors use numerical examples closer to engineering applications (Darcy, Kolmogorov, and dynamic stall) beyond the standard benchmarks (Poisson's equations and Burger's equation), etc."}, "weaknesses": {"value": "Physical residuals are sensitive to the minimum noise set as a hyperparameter;​ higher noise degrades residuals and requires careful tuning. The method also introduces a time-based residual scaling, whose choice affects errors (albeit less so with unrolling). Some elaborations on how to handle the hyperparameter sensitivity would be helpful. \n\nThe paper aggregates boundary-condition penalties and the divergence-free residual into a single “physics” loss. However, these terms can induce conflicting gradients (e.g., negative cosine similarity), leading to oscillatory updates or favoring one constraint over the other."}, "questions": {"value": "Can you split the boundary and residual loss and report the gradient conflicts among these physical losses and the distribution fidelity loss?\n\nCan you gate which physics term to optimize per step (or reweight it) using online conflict indicators (e.g., negative cosine, norm ratios) or a simple KKT/dual ascent scheme, and show coverage–fidelity trade-offs?\n\nWhat extra memory/backprop cost does BC/divergence decomposition incur vs. the single physics loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a4RGXOdXl2", "forum": "tAf1KI3d4X", "replyto": "tAf1KI3d4X", "signatures": ["ICLR.cc/2026/Conference/Submission8035/Reviewer_e5nq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8035/Reviewer_e5nq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762462021433, "cdate": 1762462021433, "tmdate": 1762920029706, "mdate": 1762920029706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}