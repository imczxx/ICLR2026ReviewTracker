{"id": "PPkzchpdNK", "number": 1977, "cdate": 1756973776705, "mdate": 1763634814038, "content": {"title": "RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning", "abstract": "Large language models (LLMs) achieve high performance on mathematical reasoning, but these results can be inflated by training data leakage or superficial pattern matching rather than genuine reasoning. To this end, an adversarial perturbation–based evaluation is needed to measure true mathematical reasoning ability. Current rule-based perturbation methods often generate ill-posed questions and impede the systematic evaluation of question difficulty and the evolution of benchmarks. To bridge this gap, we propose RIDE, a novel adversarial question-rewriting framework that leverages Item Response Theory (IRT) to rigorously measure question difficulty and to generate intrinsically more challenging, well-posed variations of mathematical problems. We employ $35$ LLMs to simulate students and build a difficulty ranker from their responses. This ranker provides a reward signal during reinforcement learning and guides a question-rewriting model to reformulate existing questions across difficulty levels. Applying RIDE to competition-level mathematical benchmarks yields perturbed versions that degrade advanced LLM performance, with experiments showing an average $21.73\\%$ drop across $26$ models, thereby exposing limited robustness in mathematical reasoning and confirming the validity of our evaluation approach.", "tldr": "As LLM math performance can be inflated by data leakage and pattern matching, we introduce RIDE—an IRT-guided question-rewriting framework that perturbs competition problems to increase difficulty—leading to performance drops across many LLMs.", "keywords": ["Robust Evaluation; Item Response Theory; Reinforcement Learning; Mathematical Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb4dbf52b16f0252718b52e4d2c44cb4bf20e1e2.pdf", "supplementary_material": "/attachment/69c3351f4ee63e37e5373be09ae87f55da6a69f6.zip"}, "replies": [{"content": {"summary": {"value": "This work builds purtabated benchmarks to evaluate the robustness of mathematical reasoning LLMs using the proposed RIDE pipeline. RIDE differs from previous rule-based methods ('rule-based' in the paper means writing explicit rules in the prompt for LLM to synthesize a new question) in that:\n(1) Their question rewriting model, RIDE-8B, is trained from Qwen3-8B with an SFT stage that distills from GPT-5 and a followed RL stage using an IRT(Item Response Theory)-based difficulty-ranking model as part of the source of the reward signal during GRPO training.\n(2) Their prompt to generate purturbated questions do not explicitly contain any specific rules such as numerical variation. Instead, they only provide some general instructions such as to increase the difficulty, and not to turn the original problem into multiple sub-problems. \n\nThe authors show that RIDE is superior to one rule-based counterpart (GSM-Plus prompting strategy) under the quality evaluation by GPT-5. After curation by human annotators/advanced LLMs on the RIDE-generated questions, they build and release RIDE-AIME and RIDE-AMC benchmarks and a training dataset DeepMath-RIDE."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: (1) The question rewriting prompt is substantially different from the previous rule-based ones and allows strong LLMs to create more diverse problems than the rule-based counterparts.\n(2) In order to mitigate the scarcity of student responses for training the difficulty-ranking model, this work proposes to use VAE/Sampling augmentation strategies to obtain extra IRT rows for better training.\n\nClarity: The RIDE pipeline is clearly stated and the overall workflow shown in Figure 2 is very easy to understand. Experimental setups are introduced with enough details."}, "weaknesses": {"value": "1. The significance of using RL to train the model is not defended in the paper. It is important for the authors to show that the RL stage indeed improves the model performance to defend their contributions on the difficulty-ranking model used for providing reward signal. For example, adding extra rows in Table 2 to show the result of RIDE without IRT reward and RIDE without RL training stage would be ideal. \n\n2. The significance of building such benchmarks today seems not that clear to me. Since the ceiling of the difficulty of the rewriting question is not controlled, it is expected that the model performance will definitely decrease due to the increase of difficulty. As a result, it is not that easy to claim that the model is not robust simply because its performance degrades on RIDE-AIME/AMC. In contrast, the MATH-P-hard benchmark proposed in Huang et al. 2025 is a pure human-annotated purturbated dataset, so that they can make sure despite that the original solution paths are no longer applicable, the actual 'difficulty' of the question is not increased, which allows for focusing on the robostness of math reasoning only. \n\n3. There is no systematic analysis on how RIDE-8B tends to rewrite the original question. The paper only shows a case study in section 5.3. Adding a statistics on the types of rewriting for a certain quantity of sampled questions could help strenthen the argument."}, "questions": {"value": "My major concerns and questions have been proposed above in the weaknesses. Below are some extra questions on the details:\n1. Can you please show the results of RIDE-Qwen3-1.7B and RIDE-Qwen3-4B on AIME-25? It is acceptable that the performance maintains or even degrades, I am just curious about whether the results are consistent.  \n2. Why in Table 2, for AMC-23, the sum of average win rates of rule-based and RIDE is 99.68% rather than 100.0%?\n3. The right panel of Figure 4 shows that the pass@n performance of Qwen2.5-Math-72B hardly increases as n increases from 1 to 8. This is quite out of my expectation since the Qwen2.5-Math technical report shows that maj@64 result has a clear margin over pass@1 result. Could you help explain this?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The annotator compensation is not clearly stated in the paper. I will appreciate if the authors can provide how much the annotators are paid for an hour."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b25yoLKhEz", "forum": "PPkzchpdNK", "replyto": "PPkzchpdNK", "signatures": ["ICLR.cc/2026/Conference/Submission1977/Reviewer_GWMD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1977/Reviewer_GWMD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842544737, "cdate": 1761842544737, "tmdate": 1762915980281, "mdate": 1762915980281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RIDE, a novel adversarial benchmark framework for evaluating the mathematical reasoning robustness of large language models (LLMs). Traditional rule-based perturbation methods for benchmark augmentation often yield ill-posed or trivial question variants. To address this, RIDE integrates Item Response Theory (IRT) into a reinforcement learning–based question rewriting pipeline to obtain a rewriter as well as rewritten datasets that has considerable hardness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-detailed in its presentation about the methodology and experiments. A comprehensive pipeline is designed for data augmentation that includes reward model training and rewriter training.\n2. The paper introduced a simple model (IRT) that handsomely credits the hardness of the problem generates."}, "weaknesses": {"value": "1. Given the limited LLM resources, the paper adopted multiple augmentation methods to extend problem-response data from LLM responses, including a VAE/sampling augmentation and the training of a pairwise difficulty ranker. However, as both methods act as a bootstrapping of existing responses, it is unclear that whether this approach exacerbates the overfitting of reward modeling.\n\nIt is also unclear to the reader why it is necessary to augment the response matrix using VAE method or sampling method. The paper did not effectively explain why these augmentation methods are useful and did not incorporates other baselines as reward models. \n\n2. A major novelty of the paper is to introduce a rewritter model that automatically rewrites problems. However, it still requires human-in-the-loop to produce the final augmented datasets. This weakens the need for training a rewritter. However, the paper provides no discussion on the edges of the current methodology over previous methodologies that incorporates human rewritting the problems. (e.g. https://arxiv.org/abs/2502.06453)  \n\n3. In the experiments, many SOTA LLMs does not display significant performance drop for the more difficult dataset, and there is a lack of discussion. The definition of PDR also tends exaggerate the actual effect on weaker models."}, "questions": {"value": "1. Regarding the first point in the weakness, can you elaborate why we need to augment the response matrix, and especially why sampling can help the training of the reward model? \n\n2. Regarding the second point in the weakness, can you elaborate why the method is more efficient than human rewritting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZNpUKNikpN", "forum": "PPkzchpdNK", "replyto": "PPkzchpdNK", "signatures": ["ICLR.cc/2026/Conference/Submission1977/Reviewer_iD67"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1977/Reviewer_iD67"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868612850, "cdate": 1761868612850, "tmdate": 1762915979973, "mdate": 1762915979973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RIDE, a novel framework for generating adversarially perturbed mathematical reasoning questions to robustly evaluate LLMs. The core problem addressed is that high performance on existing benchmarks may not reflect true reasoning ability due to data leakage or superficial pattern matching. RIDE's main contribution is a principled approach to evolving question difficulty. It uses Item Response Theory (IRT) to model difficulty, leveraging a cohort of 35 LLMs as \"students\" to generate a response matrix. The IRT-derived difficulty scores are then used to train a pairwise difficulty ranker. This ranker provides a reward signal for a RL agent, which is trained to rewrite existing math problems to be more challenging. The authors demonstrate that the resulting benchmarks (RIDE-AMC, RIDE-AIME) cause a significant performance drop across 26 powerful LLMs, exposing their brittleness. The paper also shows the utility of the generated data for augmenting training sets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.  The paper's primary strength is its innovative use of Item Response Theory to formalize and quantify the concept of \"question difficulty.\" The RIDE framework provides a more systematic and data-driven way to evolve difficulty.\n\n2.  The overall technical pipeline is well-conceived and executed. Key design choices are commendable.\n\n3.  The authors test a wide range of 26 state-of-the-art proprietary and open-source models, providing strong evidence for their claims. \n\n4.  The paper is exceptionally well-written and easy to follow. The figures are particularly effective."}, "weaknesses": {"value": "1.  The entire framework hinges on the IRT difficulty estimates, which are derived from the performance of a specific cohort of 35 LLMs. This raises a question: does the framework measure intrinsic mathematical difficulty, or does it measure \"difficulty-for-LLMs\"? The generated questions might be overfitting to exploit common failure modes of the current llms rather than becoming more difficult in a way that would also challenge a human. \n\n2. The RL training process relies heavily on GPT-5-mini for the correctness reward and GPT-5 for filtering and evaluation. This creates a potential circular dependency and a performance ceiling. The rewriting model (RIDE-8B) might simply be learning to generate problems that are difficult for its peers but still solvable by its \"teacher.\" This undermines the claim of creating truly adversarial examples and instead frames it as a distillation process from a much stronger, proprietary model. \n\n3.  While the RL approach allows for more freedom than rule-based methods, the rewrites might still converge to a limited set of \"tricks\" to fool LLMs (e.g., adding distractors, increasing numerical complexity, adding specific constraints)."}, "questions": {"value": "1.  Could you elaborate on the potential bias introduced by the specific choice of 35 LLMs for the student pool? How might the IRT difficulty parameters change if the pool included more n weaker models? \n\n2.  How does the framework handle cases where the teacher model (GPT-5-mini) provides an incorrect correctness reward? Have you quantified the error rate of the teacher model on the rewritten questions, and how might this noise affect the RL training process?\n\n3.  Could you provide a more qualitative analysis of the rewriting strategies learned by RIDE-8B? Does it learn a diverse set of transformations, or does it tend to rely on a few specific patterns to increase difficulty? For example, what percentage of rewrites involve changing numerical values vs. adding new conceptual constraints vs. altering the problem structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DIgzHnJxWE", "forum": "PPkzchpdNK", "replyto": "PPkzchpdNK", "signatures": ["ICLR.cc/2026/Conference/Submission1977/Reviewer_be6K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1977/Reviewer_be6K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974197064, "cdate": 1761974197064, "tmdate": 1762915979769, "mdate": 1762915979769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RIDE, a benchmark-generation and data-augmentation framework that rewrites mathematical reasoning questions to make them more challenging and diagnostically useful. RIDE leverages Item Response Theory (IRT) to estimate intrinsic question difficulty from LLM responses, builds a pairwise difficulty ranker, and trains a reinforcement-learning–based rewriting model that increases question difficulty while maintaining solvability and answer consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The methodology is rigorous and well-motivated. Difficulty estimation is carefully implemented via variational inference on the Rasch model with data augmentation (VAE + sampling) to stabilize parameter estimation.\n2. The pairwise ranker formulation mitigates regression instability in symbolic domains and yields interpretable difficulty scores.\nExperimental coverage is extensive: 23 LLMs spanning 0.6 B–1 T parameters, multiple families (Qwen, LLaMA, DeepSeek, GPT, Gemini, etc.), and both open- and closed-source settings.\n3. Empirical results are compelling: nearly all models experience significant performance degradation on RIDE-AIME/AMC; rule-based perturbations show lower quality and consistency win rates (e.g., 55 % vs 28 %)"}, "weaknesses": {"value": "1. The “student” LLM ability parameters (θ) are estimated but not analyzed—e.g., how they correlate with model size or reasoning specialization.\n2. The difficulty ranker relies on text embeddings; semantic fidelity is measured indirectly. Cases where numerical tweaks superficially raise difficulty but not reasoning depth aren’t deeply analyzed. \n3. The paper compares only to rule-based perturbations (e.g., GSM-Plus). It omits baselines like adversarial rewriting via contrastive prompting or reasoning-guided re-sampling (e.g., Math-Perturb 2025).\n4. The pairwise ranking and RL training pipeline may be costly for larger datasets (O(N²) pair combinations). Discussion of computational efficiency or approximate ranking would be beneficial."}, "questions": {"value": "1. How do you ensure that rewritten problems truly increase reasoning difficulty rather than only numeric or lexical variation?\n2. The total reward combines difficulty, correctness, and keyword/length terms with weights (α, β, γ). How sensitive are results to these weights, and did you observe mode collapse toward trivial or overly verbose rewrites?\n3. What fraction of rewrites fail correctness verification without GPT-5-mini supervision? Could open-source verifiers (e.g., DeepSeek-Verifier) achieve comparable filtering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4N3bzXMfcp", "forum": "PPkzchpdNK", "replyto": "PPkzchpdNK", "signatures": ["ICLR.cc/2026/Conference/Submission1977/Reviewer_zuaw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1977/Reviewer_zuaw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762126799167, "cdate": 1762126799167, "tmdate": 1762915979436, "mdate": 1762915979436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}