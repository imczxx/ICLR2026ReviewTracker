{"id": "31VTD5pS2v", "number": 19727, "cdate": 1758298874521, "mdate": 1759897022758, "content": {"title": "Cognitively Inspired Reflective Evolution: Interactive Multi-Turn LLM–EA Synthesis of Heuristics for Combinatorial Optimization", "abstract": "Designing effective heuristics for NP-hard combinatorial optimization problems remains a challenging, expertise-driven task. Recent uses of large language models (LLMs) primarily rely on one-shot code synthesis, producing fragile, unvalidated heuristics and under-utilizing LLMs' capacity for iterative reasoning and structured reflection. In this paper, we introduce Cognitively Inspired Reflective Evolution - CIRE, a hybrid framework that embeds LLMs as interactive, multi-turn reasoners within an evolutionary algorithm (EA). CIRE (i) constructs performance-profile clusters of candidate heuristics to give the LLM compact, behaviorally coherent context; (ii) engages the model in multi-turn, feedback-driven reflection tasks that produce explainable performance analyses and targeted heuristic refinements to broaden the exploration--exploitation frontier; and (iii) integrates and selectively validates these proposals via an EA meta-controller that adaptively balances search. Extensive experiments on benchmark combinatorial optimization show that CIRE yields heuristics that are both more robust and more diverse, achieving consistent, statistically significant gains over one-shot LLM generation, genetic programming baselines, and population-based EAs without LLM feedback. These findings suggest that interactive, cognitively inspired multi-turn reasoning is a promising paradigm for automated heuristic design.", "tldr": "", "keywords": ["Combinatorial Optimization", "Heuristic Evolution", "Large Language Models", "Multi-turn Strategy Generation", "Reflective Mechanism"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e77f8f4f009113bfbda64b68694964cde26a7bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes CIRE, a cognitively inspired framework that embeds a large language model as a multi-turn reasoner inside an evolutionary algorithm to automate heuristic design for combinatorial optimization. CIRE clusters candidate heuristics by performance profiles to give the LLM compact, behaviorally coherent context, then elicits reflective analyses and targeted refinements. An EA meta-controller selectively validates proposals to balance exploration and exploitation. On online bin packing, CIRE yields more robust, diverse heuristics and statistically significant gains over one-shot LLM code generation, genetic programming, and EAs without LLM feedback, reducing optimality gaps and excess-bin fractions under tight capacities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "All the proposed components are well-motivated. Multi-turn critique/refinement leverages LLM reasoning beyond one-shot code. \nPerformance-profile clustering provides compact, behaviorally coherent prompts that improve generalizable updates. \nEA meta-controller balances exploration/exploitation with selective validation for stability."}, "weaknesses": {"value": "The presentation could be improved. Section 4.2 is somewhat hard to follow.\n\nThe fatal weakness is the lack of validation. The authors should evaluate their methods on more datasets and problem types, and with additional LLMs. The experiments also lack an ablation study for each algorithmic component."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rhu2ZYXLZ9", "forum": "31VTD5pS2v", "replyto": "31VTD5pS2v", "signatures": ["ICLR.cc/2026/Conference/Submission19727/Reviewer_tdXD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19727/Reviewer_tdXD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756588258, "cdate": 1761756588258, "tmdate": 1762931565741, "mdate": 1762931565741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CIRE, a hybrid framework that embeds an LLM as a multi-turn “reflective” reasoner inside an evolutionary algorithm to synthesize heuristics for combinatorial optimization (e.g., TSP, BPP). CIRE clusters candidate heuristics by performance profiles to give the LLM structured context, prompts the model to analyze strengths/weaknesses and propose targeted refinements, and uses an EA meta-controller to validate and balance exploration vs. exploitation. Experiments report consistent, statistically significant gains over one-shot LLM generation, genetic programming, and EA baselines; qualitatively, CIRE evolves novel strategies (e.g., ARP, QTBP) that surpass prior best scores."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear, well-motivated shift from one-shot LLM code synthesis to iterative, cognitively inspired reflection. \n\nThoughtful design with performance-profile clustering + multi-turn feedback + EA meta-control, which addresses key issues in current LLM+EA methods. \n\nEmpirical evidence suggests superior robustness/diversity of heuristics on BPP."}, "weaknesses": {"value": "- The paper lacks a comprehensive evaluation on more problems and datasets.\n- Quantitative details are light in the excerpted text: dataset sizes, statistical test specifics, runtime/compute budgets, etc. Ablation is lacking. Stronger reporting would aid reproducibility.\n- Potential sensitivity to clustering choices isn’t fully dissected; robustness across LLMs and hyperparameters remains to be demonstrated."}, "questions": {"value": "How are the weights α and β in the similarity metric (behavioral vs. CodeBLEU semantic) chosen, and how sensitive are results to them?\n\nWhat is the stopping criterion for multi-turn refinement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tc3wgWqqn0", "forum": "31VTD5pS2v", "replyto": "31VTD5pS2v", "signatures": ["ICLR.cc/2026/Conference/Submission19727/Reviewer_hmGT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19727/Reviewer_hmGT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759707398, "cdate": 1761759707398, "tmdate": 1762931565176, "mdate": 1762931565176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Cognitively Inspired Reflective Evolution (CIRE), a hybrid LLM–EA framework that treats the LLM as a multi-turn reasoner rather than a one-shot code generator.  It is composed of 3 phases: 1) it clusters candidate heuristics by performance profiles relative gap vectors and CodeBLEU similarity; 2) it forms both homogeneous (similar) and heterogeneous (entropy-mixed) groups for comparative reflection; and 3) runs a reflection–exploration/exploitation loop whose proposals are selectively validated by an EA meta-controller. Experiments focus on online bin packing report lower excess-bin ratios."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The main idea of multi-turn reasoning makes sense, and the calculation of similarity via CodeBLEU is a neat addition"}, "weaknesses": {"value": "1. The “multi-turn reasoning” has many connections to e.g., ReEvo and feels incremental overall\n    \n2. Experiments are insufficient:\n    \n    1. Only one setting (online bin packing) is not enough to justify the method\n        \n    2. Many methods have been mentioned but not compared against, including HSEvo, which also incorporates diversity measures\n        \n3. No ablation studies are provided, making it impossible to understand the importance of each proposed component\n    \n4. Results are shaky: ReEvo can outperform CIRE in some cases, and the first 2 columns for capacity 500 report the same results for all approaches, presumably a mistake\n    \n5. No code nor hyperparameters have been provided\n    \n6. TSP is mentioned several times but not compared against\n\nOverall paper feels rushed, with also weird formatting reminiscent of LLM writing. Given the above, I believe this paper at the current state should not be accepted."}, "questions": {"value": "1. Can you provide ablation studies?\n    \n2. Can you provide at least another setting to demonstrate your method, other than online bin packing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X8C8MUgc4V", "forum": "31VTD5pS2v", "replyto": "31VTD5pS2v", "signatures": ["ICLR.cc/2026/Conference/Submission19727/Reviewer_iq5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19727/Reviewer_iq5n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908516713, "cdate": 1761908516713, "tmdate": 1762931564618, "mdate": 1762931564618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CIRE, a framework that combines large language models with evolutionary algorithms for automatic heuristic design. Unlike one-shot generation, CIRE enables multi-turn reflection where the model critiques and refines heuristics grouped by performance similarity and diversity. Applied to the Online Bin Packing problem, it achieves better heuristic quality than classical and recent LLM-based baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The core idea is reasonable and interesting: it might be good to think about a step-by-step heuristic generation instead of a one-shot."}, "weaknesses": {"value": "1. For the claim in L52 that “While attractive, this paradigm often results in unstable or unvalidated solutions, and underutilizes the LLM’s potential for iterative reflection and improvement.”, I think this is a bit overstates the limitations of prior work because recent frameworks like ReEvo and HSEvo already employ multi-turn reflection and performance validation. And experimentally, the unvalidated solution problems are not significant. This will decrease the motivation of the proposed methods.\n    \n2. The paper does not provide any prompt templates or examples for the LLM interactions. The results are not reproducible.\n    \n3. The authors claim the proposed method is for “CO”, but actually only for the bin packing problem.\n    \n4. The computational cost of multi-turn LLM reasoning is not reported. No runtime, token usage, or reflection-turn statistics are given, leaving sample efficiency and scalability unclear.\n    \n5. The results rely solely on a single proprietary model (DeepSeek V3), with no comparison across different LLMs or open-source baselines, hindering reproducibility.\n    \n6. There is no ablation isolating the effects of clustering, entropy-based grouping, or reflective multi-turn reasoning on performance. The improvement could stem from increased prompt diversity rather than the proposed reflective mechanism.\n    \n\nOverall, I think this paper is not ready for ICLR."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YzGEwtoH4J", "forum": "31VTD5pS2v", "replyto": "31VTD5pS2v", "signatures": ["ICLR.cc/2026/Conference/Submission19727/Reviewer_o8TF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19727/Reviewer_o8TF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001291812, "cdate": 1762001291812, "tmdate": 1762931563936, "mdate": 1762931563936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}