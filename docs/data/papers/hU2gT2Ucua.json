{"id": "hU2gT2Ucua", "number": 4110, "cdate": 1757602481272, "mdate": 1759898052311, "content": {"title": "APPLE: Toward General Active Perception via Reinforcement Learning", "abstract": "Active perception is a fundamental skill that enables us humans to deal with uncertainty in our inherently partially observable environment.\nFor senses such as touch, where the information is sparse and local, active perception becomes crucial. In recent years, active perception has emerged as an important research domain in robotics. However, current methods are often bound to specific tasks or make strong assumptions, which limit their generality. To address this gap, this work introduces APPLE (Active Perception Policy Learning) - a novel framework that leverages reinforcement learning (RL) to address a range of different active perception problems. APPLE jointly trains a transformer-based perception module and decision-making policy with a unified optimization objective, learning how to actively gather information. By design, APPLE is not limited to a specific task and can, in principle, be applied to a wide range of active perception problems.\nWe evaluate two variants of APPLE across different tasks, including tactile exploration problems from the Tactile MNIST benchmark.\nExperiments demonstrate the efficacy of APPLE, achieving high accuracies on both regression and classification tasks. These findings underscore the potential of APPLE as a versatile and general framework for advancing active perception in robotics.", "tldr": "APPLE is a reinforcement learning framework for general active perception which we evaluate on a range of different tasks, focussed on tactile perception", "keywords": ["Active Perception", "Reinforcement Learning", "Tactile Sensing", "Transformers"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5a79feae8a6ab4f6f9295bc4cd01874e149ecbb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The APPLE paper introduces a unified framework for active perception that jointly trains a sensing policy and a prediction model within the learning process. Instead of treating perception and control as separate modules, the method combines them through a single objective that encourages both accurate predictions and efficient sensing. The approach is implemented using standard off-policy reinforcement learning methods and evaluated on simulated visual and tactile tasks. The results show consistent improvements over random and simple baseline policies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Strengths**\n\n**1. Clear Motivation and formulation:** Presents a simple, mathematically sound loss-augmented return that ties prediction and control under a single objective. Theoretical formulation is also explained clearly and is easy to follow.\n\n**2. Good Writing:** The paper is clear, concise, and well-organized. Its explanations of tasks and evaluation approach are logically structured, facilitating straightforward comprehension of the methodology and results.\n\n**3. Implementation clarity:** Detailed reporting of prediction rewards, loss and hyperparameters are provided, and it seems easy to reproduce."}, "weaknesses": {"value": "**Weaknesses**\n\n**1. Overstated Claims:** The paper repeatedly suggests that APPLE is sensor-agnostic and can integrate data from “diverse sensor inputs without task-specific modifications” (Line 88). However, every reported experiment uses only a single simulated sensor type: the tactile tasks (TactileMNIST, Volume, Toolbox) rely on the GelSight Mini, while the visual task (CircleSquare) uses a synthetic 5 × 5 pixel grid rather than an actual camera. For justification, the authors must provide more experiments on diverse settings.\n\n**2. Novelty:** The paper’s conceptual foundation is not particularly novel, as it builds upon long-established principles in active perception and intrinsic exploration. A broad range of prior work spanning attention-based models, curiosity-driven reinforcement learning, and active sensing  has already linked exploratory behavior to prediction-based or feedback-driven learning signals. These approaches share a common theme: agents act to maximize informational or predictive value, whether expressed through task accuracy, novelty, or uncertainty reduction. APPLE remains situated within this same conceptual framework, differing mainly in its operationalization, substituting intrinsic prediction error with a supervised task loss embedded in the reward. While the formulation is elegant and well-executed, it represents a refinement of existing paradigms rather than a substantive theoretical advancement.\n\n**3. Experimental Evaluations:** The experimental evaluation remains simulation-bound and lacks any real-world validation, making the conclusions about generality largely speculative. All tactile experiments rely on simulated tactile images rather than data from a physical sensor, so issues such as contact noise, latency, or sensor drift are never addressed. The supposedly “general” Toolbox benchmark is limited to a single rigid object (a wrench) and one fixed workspace, offering no evidence of cross-object or cross-material generalization, which is an essential aspect for active perception in robotics. Moreover, there are no multi-object or multi-tool variations to demonstrate robustness to geometry or frictional diversity, and no tests in cluttered environments. This narrow scope undercuts the paper’s broader claims of task and sensor generality."}, "questions": {"value": "1. The paper emphasizes multi-sensor and task-agnostic capabilities, yet all experiments are single-sensor and per-task. Could you clarify what evidence supports those claims?\n\n2. Can the framework handle sensor noise or real-world uncertainty (e.g., drift, latency, or stochastic contacts)?\n\n3. How does APPLE differ algorithmically from curiosity-based intrinsic reward methods such as ICM or RND, beyond replacing intrinsic prediction error with supervised task loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WHXflGpmhG", "forum": "hU2gT2Ucua", "replyto": "hU2gT2Ucua", "signatures": ["ICLR.cc/2026/Conference/Submission4110/Reviewer_yWQL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4110/Reviewer_yWQL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761103848322, "cdate": 1761103848322, "tmdate": 1762917181796, "mdate": 1762917181796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to solve active perception tasks, where the agent  interacts with the environment to collect features for prediction. The work is focused on tactile manipulation tasks where the robot must feel objects to determine their class. They use a transformer based architecture and optimize a reward function that minimizes prediction error. They show results on simulated benchmarks where the agent explores via touch sensor or limited field of view to determine an object of interest. They outperform a random policy and an LSTM policy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed objective and architecture is sensible for solving active perception tasks in the tactile sensing domain, although I have some issues with the framing of it as a general framework.\n\nActive perception for tactile sensing is a niche area with lots of potential for growth, so this paper is very timely. \n\nThe paper is well written and provides good background on tactile sensing and active perception."}, "weaknesses": {"value": "## Claim of generality of method is misleading\nThe authors claim that this active perception method is not tied to any particular task, and is \"general\". This is misleading and needs to be revised.\n\nThe method proposes an additional reward term, which is a differentiable prediction term. First, requiring the term to be differentiable reduces the amounts of tasks this method is applicable to. Next, adding an additional prediction term introduces tuning complexity and the danger of reward misspecification. This objective would not work in tasks where it is hard to specify the state (no access to pose or  state). \n\nFinally, perhaps most importantly, the authors only evaluate in tasks where the task success is defined as correctly perceiving the state. But in most realistic tasks, perceiving the state is only an intermediate step towards solving the task. And the degree to which we need to perceive the state to solve the task is often quite low.\n\nThere is an approach that bypasses all of these limitations.  Maximizing the return with RL is the most general approach. If the task demands active perception in the optimal policy, then RL will discover it to maximize reward. Of course, this can be intractable in many cases, so we need additional mechanisms, like the reward term proposed, to improve learning. But this reduces generality.\n\nI would like the authors to reframe their claims here, perhaps saying this is a good way of solving a particular class of active perception tasks where the goal is to discover a state, without overstating the generality.\n\n## Lack of baselines\nI think there could be more baselines than just a random policy and a LSTM policy. For example, what about a hardcoded raster-scanning policy? Or some policy that uses a heuristic for exploration / information gain, etc. These simple approaches would be considered first for a roboticist. \n\n\n## Framing of background work\nI have an issue with the background section. It has a few sentences that I disagree with.\n>In the context of active perception, a few works have explored RL as an option.\n> However, these methods are usually tailored to specific tasks, environments, and objectives, and often assume the agent does not influence the environment through its actions. To our knowledge, there exists no active perception method that has been shown to work on a wide range of tasks, objectives, and environments.\"\n\nThese sentences give the feeling that active perception is under-explored, and current methods are unsatisfactory in generality and performance. But this is not true.\n\nA quick search on the web for most recent papers on active perception, e.g.  \"CoRL 2025 active perception\" or \"Neurips 2025 active perception\" shows multiple papers that use RL, VLA, reasoning to do active perception on a variety of tasks on real world robots.  \n\nWhile at the time of writing the authors may not have known these works, it's clear the broader robot learning / RL / VLA community has recognized the importance of active perception, and have made progress there. Another missing line of work is the recent wave of learning active perception policies via imitation learning and teleoperation platforms with head eye cameras. Finally, the active inference literature proposes an objective that is very general and does handle both active perception and decision making jointly."}, "questions": {"value": "Aside from my concerns above:\n\nWhat happens if the prediction loss is not differentiable? Would RL still work on this objective?\n\nAs mentioned in the concerns, it would have been nice to have tasks where active perception is part of completing the task, but not the end goal. And comparing against a baseline that just gets task reward.\n\nAre there videos of the learned behaviors? Is there anything interesting there, like the agent learns some efficient exploration strategy?\n\nThe experimental section gets a bit repetitive. I wish there were more qualitative analysis and visualizations instead of just comparing final accuracy scores. For example, what behaviors are learned, visualizations of the exploration behavior of the policies, etc.\n\n---\n\nOverall, I think this paper needs some rewriting and tweaks on the experiments, and I look forward to the changes. I like the overall direction, as the intersection of tactile sensing and active perception is under-explored, and is likely to be very important as humanoids/dexterous manipulation  become popular."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "riVYV0Zc7f", "forum": "hU2gT2Ucua", "replyto": "hU2gT2Ucua", "signatures": ["ICLR.cc/2026/Conference/Submission4110/Reviewer_LSx3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4110/Reviewer_LSx3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761416277199, "cdate": 1761416277199, "tmdate": 1762917181626, "mdate": 1762917181626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a general RL framework for task-agnostic exploration using intrinsic rewards derived from a jointly optimized prediction model. The authors instantiate an active perception setting with tactile sensing for 3D-MNIST digit classification and evaluate their methods (both SAC and CrossQ variants) on a shared Transformer architecture. Experiments indicate the approach is effective and the architectural choices are reasonable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A general formulation with a clear, well-motivated architecture. Clear, principled formulation of active perception as minimizing a supervised loss inside an interactive POMDP, with a clean gradient decomposition (policy gradient minus prediction-loss gradient). This is a neat unification that explains the joint training signal succinctly.\n- The paper is well written with clear presentation.\n- Robotic scenarios are relevant and valuable for tactile-enabled active perception with a broad set of tactile tasks (classification, regression, localization)."}, "weaknesses": {"value": "- The claim of “a novel framework that leverages RL to address a range of different active perception problems” could be more specific. Which component is novel—the unified formulation (e.g., Eq. 3), the Transformer with shared components, or something else? Clarifying this and discussing generalizability e.g. with other modalities beyond tactile sensing would help.\n\n- Additional discussion/experiments comparing sparse (more realistic when labels arrive late in real-world settings) vs. dense rewards would strengthen claims about generality.\n\n- Including related baselines or discussion would be helpful, e.g., RL exploration with haptics as reward [1] and earlier related work [2].\n\n- More discussion of real-world deployment (learning time, sim-to-real if any transfer is intended) would be valuable, since real-world “exploration” can be costly or impractical.\n\n- Minor: clarifying the definition and role of $\\tilde{r}$ in Eq. 1 would aid interpretability.\n\n\n\nReferences\n\n[1] Rajeswar, Sai, Cyril Ibrahim, Nitin Surya, Florian Golemo, David Vazquez, Aaron Courville, and Pedro O. Pinheiro. \"Haptics-based curiosity for sparse-reward tasks.\" In Conference on Robot Learning, pp. 395-405. PMLR, 2022.\n\n[2] Li, Mengdi, Xufeng Zhao, Jae Hee Lee, Cornelius Weber, and Stefan Wermter. \"Internally rewarded reinforcement learning.\" In International Conference on Machine Learning, pp. 20556-20574. PMLR, 2023."}, "questions": {"value": "- Relation to IRRL: How does your RL-based active perception framework compare to the IRRL framework (Ref. [2]) in terms of objective, training signal?\n\n\n- Question on the backbone choice: the tasks in this submission are with relatively small exploration spaces, is the Transformer architecture superior to an LSTM-based alternative, and why?\n\n\n- Real-world deployment:\n(1) Is it advisable to include warm-up strategies (e.g., demonstrations, heuristic exploration priors) for the policy, or analogous prior/knowledge injection into the prediction module?\n(2) In practical deployments, access to ground-truth labels during exploration is limited or delayed. How do you handle sparse/late/partial labels, and what would the deployment workflow look like?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3IsCDzl9Zq", "forum": "hU2gT2Ucua", "replyto": "hU2gT2Ucua", "signatures": ["ICLR.cc/2026/Conference/Submission4110/Reviewer_WrTJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4110/Reviewer_WrTJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924447560, "cdate": 1761924447560, "tmdate": 1762917181371, "mdate": 1762917181371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces APPLE, a reinforcement learning framework designed for active perception tasks. The core formulation treats active perception as a POMDP and jointly optimizes a transformer-based policy for action selection and a perception module for task prediction, using a reward signal that incorporates task-specific differentiable losses. The method is evaluated on multiple simulated benchmarks covering classification, regression, and pose estimation tasks. Results indicate that APPLE outperforms both a random baseline and a prior method (HAM), demonstrating its ability to learn effective exploration strategies without relying on task-specific heuristics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The principal strength lies in the consistent performance across multiple tasks without requiring task-specific modifications, demonstrating good innovation.\nExtensive experiments across five benchmarks offer convincing evidence of the framework's capability and robustness.\nThe comparison with HAM effectively underscores the benefits of off-policy methods in improving sample efficiency."}, "weaknesses": {"value": "The comparison is largely limited to a random baseline and HAM. Evaluation against other non-RL active perception methods would strengthen the validity of the claims.\nThe paper lacks thorough ablation analysis. For instance, the importance of the transformer architecture remains unclear."}, "questions": {"value": "The \"RL reward\" is mentioned but not specified in detail for each task. How was this term defined across the different benchmarks?\nThe transformer backbone introduces considerable complexity. Was a comparison performed with a simpler recurrent model to ensure that performance gains stem from the proposed framework rather than the representational capacity of the transformer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9NYf1whK9N", "forum": "hU2gT2Ucua", "replyto": "hU2gT2Ucua", "signatures": ["ICLR.cc/2026/Conference/Submission4110/Reviewer_bEC3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4110/Reviewer_bEC3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762327777747, "cdate": 1762327777747, "tmdate": 1762917181061, "mdate": 1762917181061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}