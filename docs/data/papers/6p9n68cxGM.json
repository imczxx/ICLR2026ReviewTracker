{"id": "6p9n68cxGM", "number": 15145, "cdate": 1758248220117, "mdate": 1763703038558, "content": {"title": "EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable Text-to-Speech via Activation Steering", "abstract": "Text-to-speech (TTS) has shown great progress in recent years. However, most existing TTS systems offer only coarse and rigid emotion control, typically via discrete emotion labels or a carefully crafted and detailed emotional text prompt, making fine-grained emotion manipulation either inaccessible or unstable. These models also require extensive, high-quality datasets for training. To address these limitations, we propose **EmoSteer-TTS**, a novel **training-free** approach, to achieve **fine-grained** speech emotion control (conversion, interpolation, erasure) by **activation steering**. We first empirically observe that modifying a subset of the internal activations within a flow matching-based TTS model can effectively alter the emotional tone of synthesized speech. Building on this insight, we then develop a training-free and efficient algorithm, including activation extraction, emotional token searching, and inference-time steering, which can be seamlessly integrated into a wide range of pretrained models (e.g., F5-TTS, CosyVoice2, and E2-TTS). In addition, to derive effective steering vectors, we construct a curated emotional speech dataset with diverse speakers. Extensive experiments demonstrate that EmoSteer-TTS enables fine-grained, interpretable, and continuous control over speech emotion, outperforming the state-of-the-art (SOTA). To the best of our knowledge, this is the first method that achieves training-free and continuous fine-grained emotion control in TTS. Demo samples are available at https://emosteer-tts-demo.pages.dev/.", "tldr": "The first training-free and fine-grained approach for emotion-controllable text-to-speech.", "keywords": ["text-to-speech", "emotion control", "activation steering"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6fa170f8cb3a05a6e26f20207b93e20f335d27f.pdf", "supplementary_material": "/attachment/ec4e58066c78e0627f6b7f648a1ea5dee931b98e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes EmoSteer-TTS, a novel framework for fine-grained, continuous emotion control in Text-to-Speech (TTS) models. The key innovation is that this method is training-free; it can be applied to existing, pretrained flow matching-based TTS models (e.g., F5-TTS, CosyVoice2) without any fine-tuning.\n\nThe method works by \"activation steering.\" First, the authors compute \"steering vectors\" by finding the average difference in internal model activations between neutral and emotional speech, using a curated dataset. This vector is then refined by identifying the \"top-k\" most emotionally salient activation tokens, as validated by an external Speech Emotion Recognition (SER) model. At inference time, this steering vector is added to the model's activations with an adjustable strength parameter (α), allowing for precise control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core contribution, a training-free, \"plug-in\" framework for fine-grained emotion control, is novel for EC-TTS. It obviates the need for costly retraining or large, multi-emotion datasets for each new TTS model.\n\nThe paper validates EmoSteer-TTS on three different SOTA flow-matching models, demonstrating its general applicability within this model class. The evaluation is robust, combining objective and subjective metrics, and includes an OOD test."}, "weaknesses": {"value": "\"Training-Free\" vs. \"Data-Dependent\": The \"training-free\" claim is slightly misleading. While the TTS model is not trained, the method requires a non-trivial offline process: (1) curating a substantial (6,900-sample) high-quality emotional speech dataset, and (2) using a separate, pretrained SER model (emotion2vec) to process activations and find the top-k tokens. The success of the method is therefore highly dependent on the quality of this curated dataset and the accuracy of the chosen SER model. The sensitivity to these components is not explored.\n\nThe authors admit in the limitations (Line 511) and in the text (Line 411) that strong steering (α) can introduce artifacts and unintelligible speech. This is a critical and expected limitation. However, the paper is missing a crucial experiment that quantifies this trade-off. An ablation study plotting steering strength (α) against audio quality (N-MOS) and intelligibility (WER) would be necessary to understand the practical usable range of emotion control.\n\nThe method is exclusively demonstrated on flow matching-based TTS models using a DiT backbone. It is an open question whether the core assumption—that emotion is represented in a linearly steerable subspace of activations—generalizes to other dominant TTS architectures, such as VITS (VAE/GAN-based) or codec-based autoregressive models (e.g., VALL-E). This limits the generality of the paper's claims."}, "questions": {"value": "The top-k token selection (Sec 3.3) relies on emotion2vec. How sensitive is the quality of the resulting steering vector to this choice? For instance, what happens if SenseVoice (which was used in the evaluation) is used to generate the steering vectors instead? Does performance degrade?\n\nThe claim of composite emotion via linear addition (Eq. 11) is very interesting. Could the authors comment on the qualitative results? Does \"anger + sadness\" sound like a convincing blend, or does it sound muddled/confused?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WRDVBKODnm", "forum": "6p9n68cxGM", "replyto": "6p9n68cxGM", "signatures": ["ICLR.cc/2026/Conference/Submission15145/Reviewer_fcfH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15145/Reviewer_fcfH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621805754, "cdate": 1761621805754, "tmdate": 1762925463535, "mdate": 1762925463535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free method for achieving continuous fine-grained emotion control in TTS synthesis. While the implementation details are mostly provided, several aspects of novelty, clarity, and reproducibility remain unclear. I appreciate the potential contribution toward controllable emotional TTS but there are significant concerns regarding the method’s originality, clarity of presentation, and empirical validation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of training-free fine-grained emotional control is interesting for advancing expressive TTS systems. If it can well explained and validated, the proposed approach has the potential to reduce the reliance on large paired emotional datasets, which remains a challenge in emotional TTS."}, "weaknesses": {"value": "This paper can be further improved by addressing the limitations including insufficient literature coverage, unclear method design, limited novelties, weal results and unclear reproducibility details.\n\nThe methodology section is not clearly written. I suggest improving it by explaining the underlying motivation and the rationale behind the design choices. Additionally, please clarify what each equation represents and how it contributes to the overall approach.\n\nThe related work section would benefit from including key studies on label-based EC-TTS approaches, as their omission currently makes the authors’ claim less convincing.\n\nI am not fully convinced by the design of the proposed approach, and the paper appears to lack sufficient novelty for ICLR.\n\nThe synthesized samples do not clearly reflect effective emotion control, particularly for emotions such as disgust, happiness, sadness, and surprise."}, "questions": {"value": "Do the authors plan to release the curated emotional speech dataset along with the data processing procedures for public use? Additionally, please clarify which testing data and how many utterances were used in the experiments. Would it be possible to evaluate the proposed approach on each dataset separately, and how might this affect its performance?\n\nWhile the implementation details are mostly described, I wonder whether the authors could release the code to confirm the mathematical details and facilitate reproducibility. If possible, could the authors make it available on the demo page? If not, please provide an explanation.\n\nWhy do you think that your approach is the first method that achieves training-free and continuous fine-grained emotion control in TTS? There are several zero-shot TTS models that appear to offer similar capabilities. Please review the related works and provide a fair, detailed comparison against these approaches.\n\nDo the authors plan to release the curated emotional speech dataset along with the data processing procedures for public use? Additionally, please clarify which testing data and how many utterances were used in the experiments. Would it be possible to evaluate the proposed approach on each dataset separately, and how might this affect its performance?\nThe motivation could be further strengthened by referencing and discussing more recent works on emotional TTS.\n\nWhat are your novelties for activation steering compared with those in text-to-image models?\n\nThe authors claim that label-based approaches rely on paired emotional data, suggesting that their method eliminates this dependency. However, the proposed approach still requires calculating activation differences using paired samples. What distinguishes this method from existing label-based approaches? Moreover, several prior works already achieve comparable results using only a small amount of emotional speech data for training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vQe0DRHKkX", "forum": "6p9n68cxGM", "replyto": "6p9n68cxGM", "signatures": ["ICLR.cc/2026/Conference/Submission15145/Reviewer_dQjg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15145/Reviewer_dQjg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816936958, "cdate": 1761816936958, "tmdate": 1762925463134, "mdate": 1762925463134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EmoSteer-TTS, a training-free method for fine-grained emotion control in text-to-speech via activation steering. The key idea is to identify and manipulate a sparse subset of internal activations in flow matching, DiT-based TTS models (F5-TTS, E2-TTS, CosyVoice2) to modulate emotional tone without additional training. The method constructs per-emotion steering vectors by computing difference-in-means between activations elicited by neutral vs. emotional reference speech, then selecting top-k emotion-relevant token positions using a SER probe to form a weighted steering vector. At inference, the model steers activations with strength α for conversion/interpolation, erases target emotions with strength β via projection-based subtraction, and supports composite operations (replacement or multi-emotion blending). The approach is plug-and-play, adding lightweight hooks to the first residual stream of selected layers and CFM steps.\n\nEmpirically, EmoSteer-TTS provides continuous and interpretable control over emotion intensity, outperforming or matching strong baselines on emotion similarity while maintaining intelligibility and speaker similarity, and shows smooth interpolation and effective erasure in both in-distribution and OOD settings. The paper contributes a curated emotion dataset (6,900 utterances) for steering vector construction, detailed implementation hooks, and a thorough analysis of “emotion steering dynamics” across k, layers, and steps. Limitations include reliance on curated emotional references to build steering vectors, potential circularity from using emotion2vec for both probing and evaluation, incomplete fairness in baseline comparisons, limited prosodic analyses beyond F0, artifacts at higher α, and missing efficiency measurements. Overall, the work introduces a novel, practical, and interpretable training-free control mechanism for EC-TTS, with compelling results and clear paths for strengthening evaluation and analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Originality\n\n  - Introduces a training-free, activation-steering paradigm for emotion control in TTS, a clear departure from the prevailing label- or description-conditioned methods that require large-scale training and supervision.\n  - Creatively adapts activation steering—previously shown effective in LLMs and T2I diffusion—to flow-matching, DiT-based TTS models, demonstrating cross-domain transfer of a control technique to speech generation.\n  - Proposes a principled pipeline to discover emotion-relevant internal tokens: difference-in-means activation extraction between neutral/emotional references, top-k token selection via SER-driven probing, and weighted steering vectors. This yields interpretable, fine-grained control at inference.\n  - Expands the control space beyond discrete labels and text prompts to continuous strengths, interpolation between emotional states, erasure of target emotions, and composite manipulation (replacement and multi-emotion blending).\n  - Offers analysis of “emotion steering dynamics” across layers, steps, and top-k choices, giving novel insight into how emotion is encoded and can be modulated within flow-matching TTS architectures.\n\n* Quality\n\n  - Extensive empirical evaluation across three strong, diverse backbones (F5-TTS, E2-TTS, CosyVoice2), showing the method is plug-and-play and broadly applicable.\n  - Both in-distribution and out-of-distribution tests are reported, with robust performance and minimal degradation, strengthening claims about generalization.\n  - Uses multiple complementary metrics: intelligibility (WER), speaker preservation (S-SIM), emotion similarity via two different SER models (emotion2vec and SenseVoice) to reduce metric overfitting risks, and listener studies (N-MOS, EI-MOS, EE-MOS) for perceptual validation.\n  - Demonstrates fine-grained control via smooth interpolation curves and F0 contour visualizations; shows effective erasure and composite control with clear quantitative and qualitative evidence.\n  - Provides ablations and analyses on top-k, steered layers, and CFM steps, which clarify design choices and the method’s operating regime.\n  - Releases detailed implementation hooks and reproducibility details; constructs and describes a curated emotional speech dataset to derive robust steering vectors.\n\n* Clarity\n\n  - The paper is well-structured with clear motivation (limitations of label/prompt-based EC-TTS), methodological overview figures, and concise mathematical formulations of activation extraction, steering, interpolation, erasure, and composite control.\n  - Figure 3 and the step-wise algorithm description make the steering pipeline easy to follow; equations use intuitive normalization and projection operations explained in context.\n  - Appendices provide code snippets, metric definitions, model configurations, dataset curation, and additional visualizations—substantially improving reproducibility and reader understanding.\n  - The limitations are candidly stated (dependency on high-quality emotional samples for steering vector construction, potential artifacts at high steering strengths), which helps situate the claims.\n\n* Significance\n\n  - Addresses a long-standing pain point in EC-TTS—coarse, unstable, and training-heavy control—by enabling stable, continuous, fine-grained emotion manipulation without additional training or large labeled datasets.\n  - The training-free, inference-time approach lowers the barrier to adoption: practitioners can retrofit existing TTS models to gain controllability without re-training or data collection, which is impactful for applied scenarios.\n  - Composite control (replacement and multi-emotion blending) broadens the expressive repertoire beyond single-label styles, enabling richer user experiences and fine editorial control in applications such as storytelling, assistive agents, and audio post-production.\n  - The interpretability of steering vectors and token-level masks provides a new lens to study how emotional tone emerges in TTS models, potentially influencing future architecture designs and control strategies.\n  - As the first method (to the authors’ knowledge) achieving training-free, continuous fine-grained emotion control in TTS, EmoSteer-TTS is likely to spur follow-up work at the intersection of controllability, interpretability, and efficiency in speech generation."}, "weaknesses": {"value": "The approach, while training-free at inference, still relies on a curated pool of high-quality emotional speech to build steering vectors, which weakens the claim of being data-free and raises questions about scalability. Please quantify sample complexity (how many and what quality of references are needed), test cross-lingual transfer (build in one language, apply to another), and assess robustness to noise, reverberation, and device/domain mismatch.\n\nToken selection and several evaluations depend on emotion2vec, creating potential circularity and model bias. Beyond adding SenseVoice for evaluation, diversify both probing and evaluation: use multiple heterogeneous SERs (including VAD regressors), include human emotion identification/AB tests with confusion matrices, and explore alternative token-attribution methods (e.g., linear probes, integrated gradients, activation patching, RSA) to ensure the discovered tokens are not artifacts of a single SER.\n\nComparisons against baselines are weakened by reliance on demo samples and “unguaranteed” reproductions. Re-run competitive training-based and prompt-based EC-TTS under controlled protocols (same text/reference, recommended hyperparameters), and report perceptual results with full details: rater counts, inter-rater reliability, confidence intervals, and significance tests. This is important to substantiate the “first training-free fine-grained control” claim.\n\nThe interpretability story—that a sparse subset of tokens encodes emotion—remains preliminary. Provide stronger causal evidence by patching/swapping activations across time/layers, disentangling from phonetic content and speaker traits, and mapping top-k indices to prosodic correlates and VAD dimensions. Time-localization analyses showing whether selected tokens align with prosodic modulation regions would make the findings more convincing.\n\nAt higher steering strengths, artifacts appear, and the control lacks principled safeguards. Consider adaptive strength schedules across layers/steps, subspace-constrained steering, or calibration that sets α based on projection magnitude onto the steering vector. Report intelligibility/naturalness as continuous functions of α per backbone to give users safe operating ranges.\n\nProsodic analysis focuses mainly on F0; this underrepresents emotional variation. Include energy contours, duration/speaking rate, pause statistics, spectral tilt, jitter/shimmer, or eGeMAPS features, and show that erasure drives these toward neutral baselines while interpolation varies them smoothly with α.\n\nFinally, efficiency and composite control need deeper treatment. Quantify runtime/latency overhead (RTF, memory) for different k, layer/step settings, and streaming scenarios, and provide ablations on quality–latency trade-offs. For composite emotions, run targeted listener studies with categorical and dimensional (VAD) ratings to verify that mixtures yield recognizable blends rather than cue superposition; test cross-lingual composite control and references with multiple latent emotions."}, "questions": {"value": "* Data requirements and generalization\n\nHow dependent is the method on curated emotional references, and how well do steering vectors transfer across languages, speakers, and acoustic conditions? Clarifying sample needs and robustness to domain shifts would strengthen claims of being training-free and broadly applicable.\n\n* Evaluation methodology and fairness\n\nTo what extent do current probes and baselines provide unbiased evidence of effectiveness? A clearer, diversified evaluation (multiple SERs, human studies with rigorous protocols, controlled baseline comparisons) would reduce concerns about circularity and ensure fair positioning against prior work.\n\n* Interpretability and causal validity\n\nThe core claim is that a sparse subset of internal tokens encodes emotion and can be causally steered. Can you provide stronger causal tests and time-localization analyses to verify that these tokens specifically modulate emotion (not content or speaker), and relate them to prosodic and VAD dimensions?\n\n* Control stability and safeguards\n\nHow do you prevent artifacts and preserve intelligibility/speaker identity at higher steering strengths\nalpha (and erasure\nbeta)? Principles for calibration, adaptive schedules, and clearer safe operating ranges—alongside broader prosodic analyses—would make the control more reliable.\n \n* Practicality, efficiency, and compositionality\n\nWhat are the runtime/memory costs and scalability to streaming/real-time scenarios, and do composite emotions produce predictable, perceptually coherent blends? Demonstrating efficiency trade-offs and validating compositional control would support real-world adoption."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "Emotion steering in speech synthesis raises several ethical concerns: potential demographic bias (age, gender, accent, language) in perceived emotion intensity, intelligibility, and speaker preservation, especially if steering relies on SER models with known biases; privacy and consent issues around using and releasing voice data, as voice likeness is biometric and may implicate data protection laws and licensing; misuse risks spanning impersonation, social engineering, disinformation, and coercive manipulation, which are heightened by low-friction, real-time control and emotion erasure."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "F0lFTO0Bd0", "forum": "6p9n68cxGM", "replyto": "6p9n68cxGM", "signatures": ["ICLR.cc/2026/Conference/Submission15145/Reviewer_C8bf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15145/Reviewer_C8bf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976886276, "cdate": 1761976886276, "tmdate": 1762925462664, "mdate": 1762925462664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes EmoSteer-TTS, a training-free method to achieve fine-grained, continuous, and interpretable emotion control in pretrained flow-matching TTS models (e.g., F5-TTS, E2-TTS, CosyVoice2). The key idea is activation steering: (1) compute activation differences between neutral and target-emotion references; (2) select top-k emotion-relevant tokens to form a steering vector (and weights); and (3) at inference, add the steering vector with strength alpha to chosen layers/steps to modulate synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Works across multiple flow-matching backbones; no fine-tuning required.\n- Empirical analyses give actionable guidance: k≈200 works well; multi-layer (spaced) steering outperforms shallow-only; steering across all flow steps is strongest.\n- Maintains performance on EMNS/SeedTTS despite steering vectors built from other corpora.\n- Low WER and high speaker similarity versus strong flow-matching baselines."}, "weaknesses": {"value": "- The paper prefers a large alpha but lacks a clear tradeoff curve (alpha vs. WER/N-MOS/E-SIM) and recommended operating range.\n- Emotion scores use emotion2vec/SenseVoice; although both are reported, objective metrics can bias toward specific embeddings."}, "questions": {"value": "- How sensitive are results to the steering corpus composition? Any ablation showing performance when removing one corpus from the construction set?\n- If emotion2vec and SenseVoice disagree, which correlates better with MOS? Any human-study correlation numbers to justify metric choices?\n- Any comparison with AR-based approach regarding the emotional control? A rough comparison would be great."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vbc8Medcx7", "forum": "6p9n68cxGM", "replyto": "6p9n68cxGM", "signatures": ["ICLR.cc/2026/Conference/Submission15145/Reviewer_8RhA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15145/Reviewer_8RhA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154734864, "cdate": 1762154734864, "tmdate": 1762925461154, "mdate": 1762925461154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}