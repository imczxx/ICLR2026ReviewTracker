{"id": "XGEZ9q02Ys", "number": 12742, "cdate": 1758209947772, "mdate": 1759897490094, "content": {"title": "Decision-Theoretic Approaches for Improved Learning-Augmented Algorithms", "abstract": "We initiate the systematic study of decision-theoretic metrics in the design and analysis of algorithms with machine-learned predictions. We introduce approaches based on both deterministic measures such as distance-based evaluation, that help us quantify how close the algorithm is to an ideal solution, and stochastic measures that balance the trade-off between the algorithm's performance and the risk associated with the imperfect oracle. These approaches allow us to quantify the algorithm's performance across the full spectrum of the prediction error, and thus choose the best algorithm within an entire class of otherwise incomparable ones. We apply our framework to three well-known problems from online decision making, namely ski-rental, one-max search, and contract scheduling.", "tldr": "The paper introduces novel deterministic and stochastic decision-theoretic metrics that guide the development of better learning-augmented online algorithms", "keywords": ["Learning-augmented algorithms", "online algorithms", "competitive analysis", "performance evaluation metrics", "decision theory"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/507610aa7ea5c095227372f9dfd1911db5a0bfdc.pdf", "supplementary_material": "/attachment/650640626d7842a84df9d2e709985ff6746c7388.zip"}, "replies": [{"content": {"summary": {"value": "There has been a long misalignment among different standards that are considered in learning-augmented algorithms to model the trade-off between consistency (performance with perfect prediction) and robustness (performance with arbitrarily bad prediction), including Pareto-optimality (direct trade-off between consistency and robustness) and smoothness (performance as a measurement of prediction accuracy). \nThis paper provides two more metrics to resolve the misalignment from the decision-theoretic view, including a distance-based measurement and a risk-based measurement. \nThey are applied to the problems of ski rental, one-max search, and contract scheduling to inspire new algorithms, which seem to perform better than previous methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper is well-motivated in that existing metrics for learning-augmented algorithms could be brittle in different ways. \n+ Under the given metrics, optimal algorithms behave better than previous SOTA on some datasets."}, "weaknesses": {"value": "- To me, the major problem is that the intuitions behind the proposed metrics are vague. In fact, these metrics are somehow unnatural and may not extend well to other more complicated problems. E.g., is the \"ideal solution\" defined for distance-based metrics always solvable? Further, is the algorithm that maximizes the three metrics always solvable? \n- The relationship between distance-based and risk-based metrics is not addressed. How are they compared?\n- The optimal algorithms corresponding to each metric are not explicitly provided in the main body. \n- The experiments seem incomplete. It is not clear whether the parameter choices are set to \"exploit\" the baselines. Especially, for the one-max search, why do the authors use the inputs for evaluating only the worst-case performance instead of the average-case? \n\nOverall, I feel that the paper can benefit from justifications on the proposed measures, a better-understandable writing, and more rigorous experiments."}, "questions": {"value": "See the above \"weakness\" part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ig770BXNVb", "forum": "XGEZ9q02Ys", "replyto": "XGEZ9q02Ys", "signatures": ["ICLR.cc/2026/Conference/Submission12742/Reviewer_aGPc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12742/Reviewer_aGPc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761411199544, "cdate": 1761411199544, "tmdate": 1762923562113, "mdate": 1762923562113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* This paper provided a systematic study of decision-theoretic approaches in learning-augmented algorithms. The system defines a unifying framework, characterizing online algorithms with advice according to their distance and risk measures. An online algorithm is said to be “ideal” if it lies on the robustness-performance Pareto frontier.\n* For the continuous ski rental problem, the paper characterizes the performance ratio of ideal algorithms, and provides a CVaR-based risk analysis. For the one-max problem, the paper characterizes the ideal algorithm, gives an analytical solution for the unweighted maximum distance, and a risk-based analysis. Analysis of contract scheduling is mentioned and deferred to the appendix. \n* Empirical evaluation shows favorable performance on synthetic datasets, and evaluation on empirical data is included in the appendix."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Systematic approach helps illustrate parallels between related problems, and provides a principled way to choose the \"best\" algorithm from a set of options.\n* Risk-oriented analysis is well-motivated by the increasing application of online learning methods in high-stakes applications.\n* Algorithmic results are evaluated both theoretically and empirically."}, "weaknesses": {"value": "* Scope of novel contributions is unclear. Apart from the value of a unified perspective, it is not completely clear how algorithmic results compare to existing upper and lower bounds known in the literature.\n* Limitations and practical applicability are not discussed explicitly.\n* Empirical evaluation in the body of the paper is limited to synthetic data. Appendix D.3 seems to provide some empirical evaluation, but analysis does not seem to be conclusive (i.e., there doesn't seem to be an algorithm which performs best on all dataset, but the paper does not seem to provide any further insights regarding the root cause).\n* It seems that graphically illustrating the empirical results would make them easier to interpret."}, "questions": {"value": "* Is it possible to briefly summarize the novel contributions of the paper? (i.e. introduction of a unified framework, new settings previously not investigated, and relation between presented results and existing literature)\n* Which underlying properties of a dataset might guide a practitioner in choosing the right performance criterion for their application?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fObrxOlyKB", "forum": "XGEZ9q02Ys", "replyto": "XGEZ9q02Ys", "signatures": ["ICLR.cc/2026/Conference/Submission12742/Reviewer_dDbN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12742/Reviewer_dDbN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949311016, "cdate": 1761949311016, "tmdate": 1762923561755, "mdate": 1762923561755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies learning augmented algorithms. Algorithms are classically studied by interpolating between consistency and robustness (performance function), and in many cases cannot be compared with each other. The authors provide two measures of quantitatively comparing these algorithms, (i) distance based methods and (ii) risk measures; the latter exploits distributional properties on the quality of prediction. Existing methods include (i) pareto optimality and (ii) tolerance based methods which are special cases of the methods that the authors propose. \n\nThe paper then analyzes 3 problems: ski rental, one-max search and contract scheduling. These examples demonstrate how their measures can practically be computed and how they can be useful in characterizing performance of algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-The paper is generally easy to read and follow; it is well motivated and easy to understand even for someone outside the field. \n-The choice of CVAR is one that is easy to accept, being commonly used in decision theory.\n- For a theory paper, it is nice to see some experiments, though they seem to be slightly contrived.\n\nDisclaimer: I am not from the area and cannot confidently comment on novelty nor quality."}, "weaknesses": {"value": "- No major issues here from me.\n- Minor criticism : Based on my understanding of the experiments, it seems that the authors are showing that by directly optimizing their metrics, better \"results\" are obtained based on those very metrics. This is of course unsurprising, so claims like \"distance-based algorithms offer considerable improvements over the sota\" aren't that fair."}, "questions": {"value": "- Computing the performance ratio and optimal solutions (for the authors' metrics) does not appear to be easy, and indeed is one of the key contributions of the paper. Can the authors comment on whether there are general techniques that distance measures and risk based analysis that apply to a broader class of online problems (e.g., k-server)? This would seem to greatly strenghten the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "QqJpY81079", "forum": "XGEZ9q02Ys", "replyto": "XGEZ9q02Ys", "signatures": ["ICLR.cc/2026/Conference/Submission12742/Reviewer_oU59"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12742/Reviewer_oU59"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981400865, "cdate": 1761981400865, "tmdate": 1762923561419, "mdate": 1762923561419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A learning-augmented online algorithm is an online algorithm (in the Theoretical Computer Science sense) that takes a machine-learned prediction as extra input, uses it to guide decisions, and still guarantees a bounded worst-case loss when the prediction is wrong. The advice can be bad, so the algorithm is designed with two goals: Consistency (near-optimal when the prediction is accurate) and Robustness (provable cap on the competitive ratio even under bad predictions). This dual-goal structure induces a family of Pareto-optimal algorithms that are hard to compare.\n\nThis paper uses decision theory to score an algorithm’s full error-vs-performance curve against a principled yardstick. Given the r-robustness constraint, the decision-theoretic part of this paper concerns how to choose among all r-robust (r-competitive for every input) algorithms using principled objectives. They define \"ideal\" comparator $I_r$ as the omniscient algorithm that knows the input but is forced to be r-competitive too. \n\nFor the first objective they consider, the \"distance to the ideal\", they score any r-robust algorithm $A$ by its weighted max distance and weighted average distance from $I_r$ 's performance curve over the entire prediction-error range. A user-chosen weight function encodes preferences over error regions. Such “pick the action whose loss curve is closest to an ideal benchmark” idea is what they borrow from decision theory.\n\nFor the second objective, they consider risk with CVaR; they also view the prediction as a distribution and minimize $\\alpha$-consistency, which uses Conditional Value-at-Risk to weight the worst fraction of outcomes. $\\alpha=0$ recovers expected performance, $\\alpha \\rightarrow 1$ stresses the worst-case mass in the prediction range. Given r, the goal is to find an r-robust algorithm with minimum $\\alpha$-consistency. So this becomes a constrained risk minimization problem.\n\nThe paper empirically investigates how these ideas play out in classic problems such as ski rental, one-max search, and contract scheduling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Pre-existing work centers on consistency/robustness trade-offs and Pareto-optimality (e.g., caching, one-way trading, search) or on tolerance windows and distributional advice. Comparison of algorithms among those Pareto optimal algorithms is a new thing. The frameworks for distance measures and CVaR-based risk are both nice, actionable, and well-posed. \n\nThey benchmark on synthetic and real data and report better average ratios or profits than Pareto-optimal (PO) and $\\delta$-tolerance baselines in ski-rental/one-max/contract scheduling for both distance measures and CVaR-risk."}, "weaknesses": {"value": "I actually don't understand the benefit of going beyond Pareto, or why we wouldn't just let practitioners choose one algorithm from the Pareto set. Everyone has different preferences for trading off Consistency and Robustness; given the Pareto-optimal algorithms curve, a practitioner's preferences uniquely pinpoint one algorithm (as in economics class, where the optimal consumption position is where the indifference curve is tangent to the budget line)."}, "questions": {"value": "Just theoretical questions:\n\nCan one derive lower bounds that show the paper’s optimizers are information-theoretically tight for broad classes?\n\nCan we relax the unimodality assumption on the prediction distribution?\n\nCan you formalize when Pareto-optimal designs are provably brittle near tiny errors and show how distance-to-ideal fixes this? Can you provide sharp transition thresholds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XKAt8CLZkq", "forum": "XGEZ9q02Ys", "replyto": "XGEZ9q02Ys", "signatures": ["ICLR.cc/2026/Conference/Submission12742/Reviewer_YwzD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12742/Reviewer_YwzD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125802512, "cdate": 1762125802512, "tmdate": 1762923561092, "mdate": 1762923561092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}