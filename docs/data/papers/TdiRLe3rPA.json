{"id": "TdiRLe3rPA", "number": 7394, "cdate": 1758019547957, "mdate": 1759897855451, "content": {"title": "From Ticks to Flows: Dynamics of Neural Reinforcement Learning in Continuous Environments", "abstract": "We present a novel theoretical framework for deep reinforcement learning (RL) in continuous environments by modeling the problem as a continuous-time stochastic process, drawing on insights from stochastic control.\nBuilding on previous work, we introduce a viable model of actor–critic algorithm that incorporates both exploration and stochastic transitions.\nFor single-hidden-layer neural networks, we show that the environment’s states can be formulated as a two time scale process: the environment time and the gradient time.\nWithin this formulation, we characterize how the time-dependent random variables that represent the environment's state and the cumulative discounted return evolve over gradient steps in the infinite width limit of two-layer networks.\nUsing the theory of stochastic differential equations, we derive, for the first time in continuous RL, an equation describing the infinitesimal change in the state distribution at each gradient step, under a vanishingly small learning rate.\nOverall, our work provides a novel nonparametric formulation for studying over-parametrized neural actor-critic algorithms.\nWe empirically corroborate our theoretical result using a toy continuous control task.", "tldr": "A viable continuous time model of actor-critic algorithms for studying deep RL in continuous states and actions", "keywords": ["Reinforcement learning", "stochastic processes", "control theory"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0bc35f30cf6fb45d5f27f0bc880cb929500e277.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This article presents a continuous-time formulation of the dynamics of episodic actor-critic reinforcement learning. The goal of the paper is to provide a characterisation of the coupling between the environment and learning dynamics, going beyond the usual separation of these along their independent time scales. The authors build upon a range of existing works which bring stochastic analysis and control tools to bear in the analysis of RL (exploratory dynamics, Itô expansions, numerical schemes *à la* Kushner/Dupuis). Of course, this problem can’t be captured without further assumptions. The author’s assumptions are quite reasonable: smooth control-affine dynamics (which I would say is already above average complexity), linearised 2-layer hyperbolic tangent neural networks for the agent and critic. Combining infinite-width approximations of the neural nets with statistical theorems for the environment allows for the results, which are asymptotic normality characterisations of the changes in the value and the policy. This approach is representative of the ongoing convergence of stochastic analysis/control and Neural Network limits for RL, and is one of its first convincing and tangible applications to reinforcement learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "_Soundness and Clarity_:\n- I appreciated the down-to-earth tone of the paper: the results are presented first and foremost for what they rigourously are, without resorting to window dressing. The authors present clear assumptions and clearly discuss their results in the context thereof, which I highlight amongst the broader soundness and rigour of the article. \n\n_Presentation and Clarity_:\n- Explanations of both RL methodology and control are quite extensive, and, I think, make the topics approachable to either community.\n\n- Appendices are extensive and pedagogical. I would like to express my appreciation, in particular, for the efforts the authors put into having examples throughout the background, which bridge over to the calculations performed afterwards.\n\n_Contribution, Quality, & Originality_: The approach taken by the authors is a difficult but promising one, and the results are noteworthy both for their inherent quality and the future avenues of research they seem to make plausible."}, "weaknesses": {"value": "_Soundness_:\n- The experimental section is quite underwhelming. Section 7.1 presents a clear visual argument and is fine (except the above point about graph legibility), but section 7.2 hardly contributes to the paper in my opinion. Note that I do not consider such an experimental validation necessary.\n\n_Presentation_:\n- Figures 2 and 3 are illegible due to the legend and labels being too small. \n\n- The appendices sorely need a table of contents. While the calculations themselves are detailed and well-explained, it is hard to keep track of the bigger picture due to their sheer size. Perhaps the authors would like to consider proof diagrams to better connect the different parts of the appendices. \n\n- There are quite a few typos in the body, the references, and the appendices. Most of them are inconsequential, but they distract the reader in an already dense and, at times, disorienting work."}, "questions": {"value": "I have many *research* questions about the work, but none would affect my evaluation of it. Thus, I will refrain from bothering the authors and reviewers with them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pfWYgZERxq", "forum": "TdiRLe3rPA", "replyto": "TdiRLe3rPA", "signatures": ["ICLR.cc/2026/Conference/Submission7394/Reviewer_Rcmf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7394/Reviewer_Rcmf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558647542, "cdate": 1761558647542, "tmdate": 1762919517062, "mdate": 1762919517062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a continuous-time theoretical framework for neural reinforcement learning (RL), bridging stochastic control and deep RL theory. The authors model actor-critic learning as a two-timescale stochastic process with an environment time and a gradient time and derive stochastic differential equations that describe the infinitesimal evolution of the state distribution under infinite-width linearized neural networks with vanishing learning rates. Theoretical results are complemented by small-scale empirical validation on a linear-quadratic regulator (LQR) environment."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The combination of NTK methods from deep learning to study gradient updates and SDEs from control theory to study environment updates to provide a unified nonparametric framework across two timescales is neat\n- Clear theoretical construction by leveraging Ito-Taylor expansions and martingale CLTs to formally connect gradient-time dynamics and environment evolution\n- The LQR experiment confirms that the proposed dynamics can be simulated and produce expected theoretical behavior"}, "weaknesses": {"value": "- The entire analysis relies on single hidden layer networks, smooth dynamics, and small learning rates. These conditions, while analytically convenient, make the results difficult to generalize to realistic RL systems.\n- The key theorems depend on the infinite-width and vanishing learning-rate limits. No discussion is provided on how these approximations break down for practical networks, nor whether finite-width corrections could meaningfully affect dynamics."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "wa2ZeueuGv", "forum": "TdiRLe3rPA", "replyto": "TdiRLe3rPA", "signatures": ["ICLR.cc/2026/Conference/Submission7394/Reviewer_nz1G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7394/Reviewer_nz1G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794749523, "cdate": 1761794749523, "tmdate": 1762919516475, "mdate": 1762919516475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel theoretical framework for deep RL in continuous environments by modeling the problem as a continuous-time stochastic process, and it also provides a novel nonparametric formulation for studying overparametrized neural actor-critic algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a strong theory for continuous time actor-critic model; providing a nonparametric formulation for studying overparameterized neural actor-critic algorithms."}, "weaknesses": {"value": "The numerical experiments are very limitted, and only one-hidden-layer neural networks are considered."}, "questions": {"value": "1. Extension to deeper architectures (multiple hidden layers): this would significantly strengthen the contribution. Also, please discuss the issue and add numerical experiments. \n\n2. Evaluation across diverse environments: please evaluate the method in multiple environments that vary in dynamics and difficulty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ghK87QKfEc", "forum": "TdiRLe3rPA", "replyto": "TdiRLe3rPA", "signatures": ["ICLR.cc/2026/Conference/Submission7394/Reviewer_xDen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7394/Reviewer_xDen"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883580101, "cdate": 1761883580101, "tmdate": 1762919515868, "mdate": 1762919515868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A framework for continuous-time actor-critic RL is introduced that accounts for noisy envrionments. Prior work is adapted by including exploratory stochastic dynamics that can be simulated faithfully in discrete time. Linearized single hidden layer neural networks are used for the actor and critic functions, allowing the formulation of a system of equations that captures the evolution of state over environment time and gradient steps up to an error term. Expressions for the gradients with respect to actor and critic parameters in continuous time are derived and used to create an episodic actor-critic algorithm. The algorithm is evaluated empirically on a one-dimensional toy example and superior exploration is shown when compared to an additive Wiener process."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The preliminaries are introduced in a succinct manner. The proposed framework appears to be very thorough incorporating all aspects of stochasticity in RL. The proof is lengthy and presumably rigorous. An algorithm listing sheds light on the presented approach."}, "weaknesses": {"value": "The presented math is overwhelming and confusing at times with apparent inconsistencies of upper and lower case in symbols and of argument order.\n\nIt is stated early in the paper that \"higher dimensional results follow\" wich is later restated as \"We believe high-dimensional results should follow.\".\n\nThe empirical evaluation is very limited. Additional settings should be explored including higher-dimensional tasks.\nThe figures are very small and hard to read.\n\nMinor errors:\n- Line 82: \"the RL agent can is\"\n- Line 108: $\\Delta W$ suddenly upper case\n- Lemma 4.2: First $v(x,t)$ then $v(T,x)$, are the arguments switched?\n- Line 226: \"The deterministic policy analog of Theorem 2 by Jia & Zhou (2022) gives an expression for **dicrete** time **the** policy gradient (Sutton et al., 1999) in continuous time setting\"\n- Theorem 4.3: the gradient is first lower case $g(t,x;\\theta)$ then upper case $\\mathcal G(t,x;\\theta)$.\n- Line 235: \"... by sampling a single trajectory **to** and updating ...\"\n- Therome 6.1: extra word \"estimate\"?\n- Line 376: \"$d_{s}=d_{s}=1$\", subscript is repeated"}, "questions": {"value": "- Why does the state-value function depend on time?\n- You state that \"Although Jia & Zhou (2022) provides empirical validation for a similar algorithm, they do not do so for the neural network actor and critic.\" Can reiterate the novelty in your work compared to theirs?\n- Does the presented framework allow for an extension to multi-layer finit-width networks? Can you elaborate on how this might look like?\n- A problem with infinite-width neural networks is that the gradients become neglibly small, effectively renouncing the capability for feature learning. Is this of relevance here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "QVYk81Y5rk", "forum": "TdiRLe3rPA", "replyto": "TdiRLe3rPA", "signatures": ["ICLR.cc/2026/Conference/Submission7394/Reviewer_mQkp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7394/Reviewer_mQkp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938159642, "cdate": 1761938159642, "tmdate": 1762919514775, "mdate": 1762919514775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}