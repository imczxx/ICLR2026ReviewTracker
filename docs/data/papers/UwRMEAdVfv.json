{"id": "UwRMEAdVfv", "number": 22932, "cdate": 1758337248144, "mdate": 1759896839692, "content": {"title": "What Deep Networks Want to Learn and How to Get There", "abstract": "Are we there yet? It’s hard to say when we don't know where we’re going. The only thing that seems to stay the same in the rapidly evolving field of deep learning are long training runs until test performance saturates.  Moreover, it’s not clear when to stop training; practitioners observe extrinsic metrics like the training error, test error, and regularization terms but still might be tempted to stop early lest the deep network (DN) overfit. In this paper, we develop the first intrinsic, analytical, and interpretable characterization of where the deep learning process is headed. The key is to analyze the geometry of the tessellation of the DN input space that is induced by a continuous piecewise-affine approximation to its input-output mapping. Analogous to the Voronoi tiling that underlies K-means clustering, each tile in a DN’s power diagram tiling is parameterized by a centroid vector that equals the sum of the rows of the Jacobian of the DN input-output mapping. Our key result on learning is that a DN first reaches the point of generalization when the training data become aligned (in the sense of maximum cosine similarity) with the centroids of the tiles containing them. The DN then later reaches the point of maximum robustness when the training data become aligned with each row of the (rank-one) Jacobian. Hence, centroid and Jacobian alignment are the destination that learning algorithms aspire to reach. We leverage this new understanding in GrokAlign, a regularisation strategy for DN learning that provably and efficiently induces centroid and Jacobian alignment. Our experiments with convnets and transformers demonstrate that GrokAlign significantly accelerates delayed generalization (so-called \"grokking\") and improves robustness.", "tldr": "", "keywords": ["Generalisation", "Robustness", "Optimisation"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11c6ab605da9a8b6821f3c43e8735b941b7f3210.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies a theoretical framework to explain the generalization and robustness properties of neural networks. The authors introduce two concepts : *centroid alignment* and *Jacobian alignment*. They argue that the first is connected to generalization, while the second favors robustness.\n\nThe starting point is that typical neural networks $f$ define piecewise affine functions. Following Balestriero et al. (2019), the input space can be partitioned in regions $\\omega_\\nu$ on which $f$ is locally linear. This partition forms a power diagram (a generalized Voronoi diagram) with centroids $\\mu_\\nu$. A network is said to be **centroid-aligned** at point $x$ if the centroid $\\mu_x$ associated to the region containing $x$ is proportional to $x$. The authors provide heuristic arguments to claim that this property promotes better generalization (Theorem 4).\n\nNext, the authors define **Jacobian alignment** at $x$ by the property $J_x(f) = cx^T$, where $J_x(f)$ is the Jacobian of the network at $x$. They argue (Theorem 7) that this condition promotes robustness of the network.\n\nFinally, the authors perform empirical experiments to validate their theory. On the XOR classification task, they measure the centroid alignment and show that it is correlated with the test loss. \n\nThey also propose a new method, called GrokAlign, which aims at promoting Jacobian alignment by penalizing the Jacobian norm in the training loss, in order to accelerate the \"grokking\" phenomenon (delayed improvement in test loss). They compare this methods with other algorithms that pursue the same goal, on several classification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an central topic in machine learning (generalization and robustness of neural networks), whose importance is well motivated, with an original approach,\n- The authors perform extensive and detailed empirical experiments to validate their theory"}, "weaknesses": {"value": "While the proposed idea is interesting and original, the theoretical framework, in its current form, lacks sufficient soudndess to be convincing. The analysis would benefit from more rigour and clarity. \n\n- in Section 2.2, the authors claim that centroid alignment promotes generalization. However, this argument developed in lines 187-199 is quite heuristic and imprecise. The whole reasoning relies of the fact that \"$\\Theta$ (the neural tangent kernel) and $m$ are uncorrelated\". However, I do not see why this is the case: they both depend on the neural network parameters. I believe that there is an interesting idea to be explored there, but the analysis should be developed much more thoroughly. Therefore, stating that \"[the authors have] established that centroid-aligned DNs exhibit generalization\" (line 203) seems to be a strong overclaim.\n\n- in Section 2.3, the authors introduced Jacobian alignment as $J_x(f) = cx^T$. This seems excessively restrictive: this means that locally around $x$, the network output is invariant in *any* direction orthogonal to $x$. Are there examples of such situation, even in simple cases?\n\n- Theorem 7, which supports the idea that Jacobian alignment promotes generalization, is also quite confusing and heuristic. Again, there appears to be interesting insight, but the statement (and the proof) lack clarity in exposition. The authors could address it by illustrating the claim on a simple toy problem.\n\n- The organization of the appendix is confusing: for instance, Proposition 9 uses the $Q_x$ notation, which is only introduced in Lemma 13, at the very end. This makes the proofs hard to follow.\n\nFinally, although empirical experiments are quite extensive, they only limited support for the theory. While figure 2 indicates that centroid alignment increases during training, it stays at a relatively low value. Besides, this is only for one point in the dataset; how is alignment defined *globally*? (see Q1 below)"}, "questions": {"value": "1. What does it mean for a network to be centroid (or Jacobian) aligned? Indeed, the authors have defined those properties for a given input $x$. It is clear that this cannot hold for any $x$ (since $\\mu_x$ is locally constant, it cannot be equal to $x$, unless the power diagram is degenerate). Do the authors mean that this property holds for every *training* data point (in which case it implies that every data point must lie in a different region of the diagram)?\n\n2. In the proof of Theorem 7, the authors state that \"x will only be missclassified by the DN when $v^T(x_\\epsilon) < 0$\" (l. 1062): why is this the case? There seems to be no indication on the related classification task.\n\n3. The proof of Theorem 8 starts with the statement (l. 1071): $l(f(x_p)) = l(J_{x_p} + b_{x_p})$. Since $J_{x_p}$ is the Jacobian and $b_{x_p}$ the bias vector, those have different dimensions and cannot be added. If I understand correctly, we should have locally $f(x) = J_{x_p} x + b_{x_p}$. Is there maybe a term in $x$ missing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9P8kD8AIAh", "forum": "UwRMEAdVfv", "replyto": "UwRMEAdVfv", "signatures": ["ICLR.cc/2026/Conference/Submission22932/Reviewer_3JCQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22932/Reviewer_3JCQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908678597, "cdate": 1761908678597, "tmdate": 1762942444619, "mdate": 1762942444619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the dynamics of deep neural networks through the lens of \"functional geometry\", defined as the partition of input space into regions where the network output is linear or approximately linear. Using this tool, they claim a connection between the emergence of generalisation and the simplification of the functional geometry: during training, the linear regions containing training data expand, while those around decision boundaries contract. This behaviour is quantified via \"centroid alignment\", a measure of how the network’s representation geometry evolves. Furthermore, the authors claim that centroid alignment is followed by \"Jacobian alignment\", where the network output becomes robust to local perturbations. The theoretical framework is supported by experiments on shallow networks trained on simple tasks such as XOR and MNIST. Finally, the authors introduce *GrokAlign*, a novel regularisation scheme that promotes convergence toward Jacobian-aligned solutions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The training dynamics of deep neural networks remain an open problem, and it is valuable to approach it with new perspectives and frameworks such as functional geometry.\n* The paper is clearly motivated: the authors start from a precise conceptual goal and provide intuitive visualisations (e.g. Figure 1) that help the reader grasp the setting;\n* The approach of using the developed theoretical intuition to design a new algorithm is commendable, as it both tests the proposed framework and strengthens the connection to practical applications."}, "weaknesses": {"value": "1. The claimed connection between generalisation and simplification of the functional geometry is never theoretically supported. Is Theorem 4 intended to provide such support? If so, I disagree: showing that the NTK must change to reach alignment does not imply anything about generalisation.\n2. The empirical validation is weak. In Figure 3, there is no significant difference between the curves for weight decay and those for grokalign; in Figure 4, the test and adversarial accuracies achieved by the two methods are comparable. In addition, I believe that a paper having deep networks in the title should at least consider three-layer networks (two-layer nets are still considered shallow from many points of view), and many datasets beyond the very simple MNIST are easily accessible with modern computational resources.\n3. The presentation of some mathematical results is confusing. As mentioned above, the connection with generalisation is never clearly established. The proof of Theorem 4 is difficult to follow (Appendix H introduces a new undefined variable Q and references other unlisted propositions and lemmas), and the argument at the end of Section 2.2 is vague---what does it mean that alignment ensures that the linear regions of the partition maintain a coherent structure with the overall semantics?\n\n---"}, "questions": {"value": "Despite the interesting approach, the weaknesses above make it impossible for me to recommend acceptance. It is possible that some might stem from a misunderstanding: in any case, I would raise my score if the weaknesses (1 and 2 in particular) are addressed satisfactorily."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pPBTelsr98", "forum": "UwRMEAdVfv", "replyto": "UwRMEAdVfv", "signatures": ["ICLR.cc/2026/Conference/Submission22932/Reviewer_Kxzz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22932/Reviewer_Kxzz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917973176, "cdate": 1761917973176, "tmdate": 1762942443916, "mdate": 1762942443916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that SGD training in deep networks is headed toward a geometric 'destination' called 'Jacobian alignment'. First, networks generalize once training examples become maximally cosine‑aligned with the centroid of the piecewise‑linear region that contains them (centroid alignment). \n\nThen, with more training, networks become robust when each row of the input‑output Jacobian at a training point is aligned with that point (Jacobian alignment). \n\nThe authors also propose GrokAlign, a Jacobian‑norm regularizer that pushes networks directly toward this destination and thereby accelerates grokking and improves robustness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Well motivated with the toy setup of the centroids: They start from a concrete geometric model, each region has a centroid equal to the sum of Jacobian rows, so alignment becomes both interpretable and easy to measure.\n\nCondensing all the ideas into the simple notion of Jacobian alignment make it easy to understand: The paper collapses multiple phenomena into one geometric condition: Jacobian alignment (each Jacobian row at x is parallel to x). This single notion subsumes centroid alignment and yields a clean understanding of how robustness emerges.\n\nResults that support using GrokAlign: GrokAlign adds an average Jacobian Frobenius‑norm penalty to enforce the constraint in Theorem 8, which proves that bounded‑norm Jacobians drive the optimizer toward aligned solutions. Empirically, GrokAlign accelerates grokking and raises alignment: on MNIST it achieves 60% adversarial accuracy in 62% fewer epochs than weight decay."}, "weaknesses": {"value": "While the core idea is compelling, the exposition could be clearer. The abstract is long and dense. It could be better understood at a higher level than describing Jacobian and centroid alignment. It’s hard to use as a stand‑alone summary of what the paper claims. For instance, a potential framing could be: \"Deep nets divide the input space into locally linear regions; during training, generalization occurs when each example aligns with the centroid of its region, and robustness arrives when the network’s input‑sensitivity (i.e. Jacobian) also points in that same direction.\"\n\nA simplified version of Figure 1 with a conceptual version of the centroid alignment rather than using an actual input-space partition would help substantially.\n\nSection 2 then dives straight into heavy formalism (functional geometry, NTK, multiple definitions and theorems) before offering a plain‑language overview or running example, which makes the pitch hard to penetrate for readers without a theory background."}, "questions": {"value": "Is the l2 adversarial robustness tied to the Frobenius norm of the Jacobian? If there were other types of adversarial perturbations, would a different norm over the Jacobian be required for GrokAlign?\n\n\"One limitation of our investigation is the dependency on the assumption that gradient descent has an implicit bias to minimise the rank of the weight matrices of the DN.\"\nhttps://arxiv.org/pdf/2103.10427 This work presents empirical evidence that deep nets trained with gradient descent tend to end up with low‑rank representations, and in the linear case this directly means low‑rank weights."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rnuIwf9xjd", "forum": "UwRMEAdVfv", "replyto": "UwRMEAdVfv", "signatures": ["ICLR.cc/2026/Conference/Submission22932/Reviewer_4vUT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22932/Reviewer_4vUT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025585400, "cdate": 1762025585400, "tmdate": 1762942443502, "mdate": 1762942443502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines the problem of late generalisation (also known as grokking) and proposes a novel framework based on the input-output Jacobian to investigate its dynamics. Specifically, the authors present a theoretical result indicating that during training, two distinct phases occur: the phase of centroid alignment, followed by the phase of Jacobian alignment responsible for increased robustness of the solution. The theoretical findings are backed by the empirical results and later set the stage for designing a regularisation (GrokAlign), which is designed to induce the grokking faster."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of NNs generalisation and its mechanistic understanding is a timely and important topic for the whole community.\n\n2. The presentation of the core results is clear and relatively easy to follow, although the work is based on the previous works whose results are important to understand the whole framework in detail."}, "weaknesses": {"value": "Clarity:\n1. I'm still not sure about the link between Theorem 4 (the formation of centriod alignment) and the generalization. In Section 2.2. the authors only vaguely describe this relationship. Except for the authors' words, I don't find any experimental result supporting this relationship. Could the authors please elaborate on this more broadly?\n\nNovelty & Significance:\n\n1. The theoretical framework seems to hinge on the assumption that the input-output Jacobian converges to rank-1 matrix. Clearly, this is not the case in most of the situations (as even the authors note). Thus, it raises a question of the significance of the theoretical results when we know upfront they do not apply to real-world scenarios. Is there something I'm missing here?\n\n2. The authors propose a regularisation method based on the theoretical analysis of the input-ouptut Jacobian (GrokAlign). However, in the text, the authors have already noticed that \"similar forms of Jacobian regularisation have been demonstrated to improve the robustness of DNs in prior work\". What is the contribution of the GrokAlign method and how it differs from the already established methods?\n\n\nThe last thing that I'd like the authors to confront with is the Fit&Align phenomenon studied in several works. In particular [1] introduced the concept of \"extremal vectors\" which describe the direction of the weights during the training and observed that initially the weights align with the data and only after the initial alignment, the weights starts increasing in norm. I'm not implying here that this observation dimnish the significance of this paper's contribution, but I feel that it's important to discuss this work as a related observation.\n\n[1] https://arxiv.org/abs/1803.08367"}, "questions": {"value": "I have posted all of my questions in the previous section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1dQVaYiNTa", "forum": "UwRMEAdVfv", "replyto": "UwRMEAdVfv", "signatures": ["ICLR.cc/2026/Conference/Submission22932/Reviewer_PBfN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22932/Reviewer_PBfN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762295667083, "cdate": 1762295667083, "tmdate": 1762942443179, "mdate": 1762942443179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}