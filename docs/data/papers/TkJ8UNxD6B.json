{"id": "TkJ8UNxD6B", "number": 15428, "cdate": 1758251253364, "mdate": 1763745618810, "content": {"title": "ALT: Adaptive Low‑Rank Transformation for Learngene-based Transformer Initialization", "abstract": "Parameter initialization plays a critical role when building and training diverse models under different scenarios. The recently proposed Learngene framework firstly learns one compact parameter set, termed learngene, from a large well-trained Ancestry model (Ans-Net), which is then inherited and transformed to initialize diverse descendant models (Des-Nets). One central goal in this framework is the\npursuit of maximal inheritable efficiency, which entails learning a parameter set that is dramatically more compact than the Ans-Net. However, existing methods typically fall short in this part, thus limiting the portability of these parameters across diverse initialization scenarios. To diagnose this limitation, we rethink one state-of-the-art method LeTs, and reveal that its transformation matrices, which account for the majority of inherited parameters, are substantially overparameterized. Inspired by this insight, we introduce ALT (Adaptive Low-rank Transformation) to fundamentally improve inheritable-parameter efficiency. Specifically, we propose one novel SVD-inspired metric termed Gated Importance Score, building on which two distinct adaptation strategies, namely Flat Global Adaptation and Hierarchical Component Adaptation, dynamically refine the transformation matrices while preserving the initialization quality for Des-Nets. Comprehensive experiments across both vision and language domains demonstrate ALT’s state-of-the-art inheritable-parameter efficiency and superior downstream performance.", "tldr": "", "keywords": ["Learngene; Parameter Initialization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/640c689f7b151c89e9770b1dc05656daa6ff3d24.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ALT (Adaptive Low-rank Transformation) for the Learngene paradigm, which aims to learn a compact, reusable parameter set (“learngene”) from a large, well‑trained ancestry model and then transform it to initialize many descendant models of varying widths/depths. The key observation is that, in prior Learngene approaches (e.g., LeTs), the width transformation matrices dominate the inherited parameter budget. ALT replaces dense transformation matrices with explicit SVD parameterizations F=U Σ VF=U\\,\\Sigma\\,VF=UΣV and introduces a Gated Importance Score (GIS) to rank SVD components by combining a sensitivity-based score of the singular value with the averaged sensitivities of the corresponding singular vectors. Two pruning/adaptation strategies are proposed: Flat Global Adaptation (FGA) (global top‑H across all matrices) and Hierarchical Component Adaptation (HCA) (inter‑matrix apportionment + intra‑matrix selection). The method is trained in two stages: (1) train an Aux‑Net (with SVD-parameterized transforms) via distillation from a large teacher; (2) inherit the compact learngene + transforms to initialize many descendants. Experiments on ImageNet‑1K (and downstream classification, segmentation, detection) and on BERT/GPT‑2 pretraining + GLUE/SQuAD report 20–25× fewer inherited parameters than the teacher while maintaining or improving performance vs. baselines, and reduced steps to target loss vs. training from scratch."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear bottleneck diagnosis & targeted design. The paper carefully identifies that most inherited parameters in LeTs are in the transformation matrices (≈38M out of ≈53M in a representative setup), motivating a low‑rank reparameterization; the SVD plots show sharp spectral decay and module/layer‑dependent patterns, supporting adaptive rank selection.\n\nMethodologically coherent. The explicit SVD parameterization, orthogonality regularization, and per‑component GIS (linking singular value “energy” and vector “orientation”) form a consistent mechanism to enable dynamic pruning via FGA/HCA; the HCA capacity‑aware apportionment is spelled out with pseudo‑code. \n\nBreadth of empirical scope. Results span classification (ImageNet‑1K + CIFAR/Food‑101/Cars‑196), segmentation (ADE20K, Pascal Context, Cityscapes), detection (COCO2017), and language (BERT on Wikipedia, GPT‑2 on OpenWebText, GLUE/SQuAD). Baselines include Scratch, Pre‑Fin, Grad‑LG, TLEG, SWS, LeTs, and comparisons vs. IMwLM/MatFormer and soft distillation. \n\nPractical efficiency narrative. For vision, inheritable parameters drop to ~15M (≈20× vs. ViT‑Large 304M). For language, ~39M for BERT (8× vs. BERT‑Large; 20× excluding embeddings) and ~62M for GPT‑2 (25× vs. 1.5B; 60× excluding embeddings). The paper also argues the “learn once, initialize many” amortization, showing a break‑even example over nine models.\n\nAblations & module insights. The paper ablates ALT(Fixed) vs ALT(Add) (AdaLoRA-style) vs ALT(GIS) with FGA/HCA; it also probes selection strategies and shows that omitting MLP initialization devastates performance, underscoring where initialization matters most."}, "weaknesses": {"value": "Novelty relative to adaptive low‑rank PEFT may be overstated. While the goal (portable initialization across architectures) differs from PEFT, the mechanism—SVD‑parameterized low‑rank components with a learned importance score and progressive pruning—is close in spirit to methods like AdaLoRA/PiSSA/MiLoRA. The paper positions GIS as “multiplicative” vs. additive metrics, but lacks deeper justification (theory or controlled studies) showing that multiplicativity per se drives the gains beyond careful tuning. The ablation (ALT(Add) vs ALT(GIS)) is helpful but could be confounded by hyperparameters and schedules.\n\nGIS definition lacks clarity and could be brittle. The sensitivity term uses EMA of a per‑parameter importance and an uncertainty estimate; exact computation, stability, and scaling are not sufficiently detailed in the main paper (e.g., normalization across heterogeneous U/V shapes, invariance to re‑scalings of U/V/Σ, choice of wU,wVw_U,w_VwU​,wV​, and the impact of orthogonality losses). The notation reuses I(⋅)I(\\cdot)I(⋅) for both indicator and sensitivity in different places, which hampers clarity. \n\nData/teacher dependency undermines portability claims. Stage‑1 requires access to (a proxy of) the original pretraining distribution and a large teacher model for distillation. The authors acknowledge difficulty scaling to modern LLMs due to closed pretraining corpora. This dependence could significantly limit real‑world adoption in privacy‑ or license‑constrained settings, weakening the “universal portable initializer” narrative. \n\nCompute story is mixed. ALT’s Aux‑Net costs ≈445 GPU‑hours vs. 390 for LeTs—higher one‑time cost. The amortization study is thin: for nine students the total is only modestly better than soft distillation; for 1–2 students, ALT may be costlier overall. A formal break‑even analysis (as a function of #students, sizes, and tuning epochs) is missing. \n\nFairness of comparisons.\nSome downstream comparisons fine‑tune descendants for 300 epochs, while ALT emphasizes 5–15‑epoch efficiency in other places; the paper needs a consistent budget comparison per setting (same epochs/augs/opt).\nPre‑Fin uses specific schedules/augmentations; whether ALT’s descendants also use equally strong training recipes is not entirely transparent.\nFor language, the GPT‑2 result is shown as loss curves; consolidated final perplexity numbers and multi‑seed statistics are not reported. \n\nLimited statistical rigor. There is no discussion of variance across seeds, confidence intervals, or significance tests. Given many 0.2–0.5% improvements, error bars are essential to support claims.\nPotential selection bias in width initialization. The continuous selection policy “take the first n rows/cols” of reconstructed FFF could bias performance (e.g., if learned structure is not aligned with the first coordinates). The paper notes several selection strategies but still defaults to a fixed continuous policy during initialization; more robust, learned selection or ordering may further change the relative advantage. \n\nCounting inherited parameters excludes zeroed components post‑hoc. While reasonable for storage, it obscures the training-time memory/compute footprint (since dormant components were still carried and updated before pruning). The method’s true training efficiency vs. dense baselines is therefore less clear without detailed FLOPs/memory traces during Stage‑1. \n\nTheoretical underpinnings are light. The paper relies on empirical singular value decay to motivate low rank. There is no analysis of when GIS‑guided pruning preserves initialization quality, nor bounds on the degradation induced by component removal across modules/layers.\n\nScope of LLM validation is narrow. Results are limited to GPT‑2‑XL and BERT‑Large on public corpora; no evidence is provided on more modern architectures (e.g., decoder‑only with RMSNorm/rotary attention) or instruction‑tuned settings. The paper itself notes data access as a barrier, but this weakens the generality claim. \n\nClarity and presentation issues.\nSome figures (e.g., long SVD spectra with indices up to 1.5k) lack explicit matrix shapes, making it hard to interpret rank percentages across modules.\nNotation overload (e.g., III ) and occasional grammar/formatting issues slow down comprehension."}, "questions": {"value": "GIS specifics & stability. How exactly are the sensitivity and uncertainty terms computed (batching, normalization, temperature, EMA decay)? How sensitive are results to wU,wVw_U,w_VwU​,wV​ and these EMAs? Please provide ablations and a discussion of invariance to re‑scaling U,V,ΣU,V,\\SigmaU,V,Σ. \n\nOrthogonality regularization. What is the overhead (time/memory) of the orthogonality loss (Eq. 5) across all matrices? Does performance degrade if you stop enforcing orthogonality partway through training (or use spectral parameterization with implicit orthogonality)? \n\nBreak‑even analysis. For a given teacher, dataset, and hardware, how many descendant models (and of what sizes) are needed before ALT’s total compute is lower than (a) scratch, (b) soft distillation, and (c) LeTs? Please include a table with explicit assumptions. \n\nSeed variance. What are mean ± std across multiple seeds for key tables/figures (e.g., Fig. 2, Tab. 4–7, Tab. 1)? Several gains are small; variance is needed to judge significance. \n\nSelection policy. You default to continuous selection of rows/columns when reconstructing FFF for a given width. Have you tried re‑ordering bases (e.g., via learned permutations or sensitivity‑aware submatrix selection) and does this change the outcomes or parameter counts? \n\nTraining‑time footprint. Please report training‑time peak memory/FLOPs for Stage‑1, and the evolution of active components H(t)H(t)H(t) vs. wall clock. How do these compare to LeTs and to a dense Aux‑Net without SVD?\n\nLanguage results. Can you report final perplexities (not just curves) for GPT‑2 experiments and provide task‑level GLUE/SQuAD variances? Also, how does ALT fare on modern LLM blocks (e.g., RoPE, SwiGLU, RMSNorm) or instruction‑tuned checkpoints? \n\nRobustness. How does ALT behave under distribution shift or low‑data finetuning scenarios relative to Pre‑Fin/IMwLM? Any evidence on calibration or stability at very small budgets? \n\nAblations on GIS design. Beyond additive vs. multiplicative, have you compared (i) using only σi\\sigma_iσi​, (ii) only vector scores, (iii) using quantile‑based aggregation in HCA, (iv) different decay schedules for H(t)H(t)H(t)? A compact grid would clarify which pieces matter most.\n\nCounting conventions. You exclude zeroed components when reporting “inheritable parameters.” Please also report (i) the pre‑pruning parameter count, (ii) the average active components over training, and (iii) the runtime storage required to ship a reusable initializer (including any metadata needed to reconstruct per‑matrix selections)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PkoOLqYM6Y", "forum": "TkJ8UNxD6B", "replyto": "TkJ8UNxD6B", "signatures": ["ICLR.cc/2026/Conference/Submission15428/Reviewer_MpXW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15428/Reviewer_MpXW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761180076338, "cdate": 1761180076338, "tmdate": 1762925706538, "mdate": 1762925706538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Focusing on the issue of inefficient parameter inheritance in the Learngene framework, this paper proposes the ALT method. Through analysis of LeTs, it is observed that the transformation matrices are over-parameterized. After confirming their low-rank characteristics via SVD, ALT parameterizes the transformation matrices in the form of SVD and introduces GIS to quantify component importance. Two adaptive strategies, FGA and HCA, are designed to optimize the matrices. The method is executed in two stages: training an Aux-Net to optimize parameters, and inheriting parameters to initialize Des-Nets. Experiments demonstrate that in both vision (20× compression compared to the 304M ViT-Large) and language (25× compression compared to the 1.5B GPT2-XLarge) domains, ALT achieves state-of-the-art results in parameter efficiency, downstream performance, and training efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Precise problem identification and solid theoretical foundation: It directly addresses the core bottleneck of \"over-parameterized transformation matrices\", systematically validating the low-rank characteristics of matrices through SVD, thereby providing a rigorous basis for low-rank parameterization and dynamic adaptation.\n2. Practical technological innovation and strong adaptability: GIS accurately measures component importance by leveraging the multiplicative properties of SVD, while FGA and HCA are designed to accommodate \"extreme compression\" and \"structural adaptation\" scenarios respectively, meeting diverse initialization requirements for Des-Nets."}, "weaknesses": {"value": "Limited knowledge transfer: The approach merely employs soft distillation to transfer knowledge from the Ans-Net's output layer, without utilizing fine-grained information such as intermediate features and attention mechanisms, thereby failing to fully exploit the potential of the Ans-Net."}, "questions": {"value": "When adapting to 7B/65B LLaMA models, how is the scale of inherited parameters determined? Does the decoder structure require adjustments to ALT's SVD parameterization or adaptive strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iHi3OLFIed", "forum": "TkJ8UNxD6B", "replyto": "TkJ8UNxD6B", "signatures": ["ICLR.cc/2026/Conference/Submission15428/Reviewer_whCg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15428/Reviewer_whCg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653907279, "cdate": 1761653907279, "tmdate": 1762925705738, "mdate": 1762925705738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new approach to improve upon the learngene framework. The learngene framework focuses on learning initialization parameters for different architectures from a single pre-trained checkpoint. The paper observes that a key bottleneck of the learngene framework is the size of the transformation matrices and finds that they are low rank. The paper proposes to parameterize these matrices using dynamic low-rank ones and adjust the rank during the training process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents an extensive range of experiments on different Transformer architectures and datasets using data from different domains. \n2. The paper provides detailed analysis experiments showcasing the efficacy of their approach."}, "weaknesses": {"value": "1. Since the paper builds on the learngene framework, it should have a section explaining the detailed workflow of learngene and LeTs. For readers unfamiliar with these frameworks, it is quite difficult to understand section 3.1 and several other parts of the paper.\n\n2. It is unclear why the paper chose to go with the parameterization defined in Eq. 1. From the motivation, the paper aim to have a low-rank parameterization, which could be easily achieved using a product of low-rank matrices. This would remove the need to have regularization parameters to ensure the $U$ and $V$ are orthogonal. It would also be possible to have derive a dynamic adaptation version of this framework by writing the overall matrix as a sum of low-rank matrices (with different ranks). The dynamic adaptation could focus on selecting the constants in this setting. \n\n3. Because of the weakness 1, it is unclear what loss function is being optimized while performing dynamic component adaptation in Section 3.3."}, "questions": {"value": "Please respond to the questions in the above sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "w5iYnyRsBG", "forum": "TkJ8UNxD6B", "replyto": "TkJ8UNxD6B", "signatures": ["ICLR.cc/2026/Conference/Submission15428/Reviewer_jR65"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15428/Reviewer_jR65"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952286927, "cdate": 1761952286927, "tmdate": 1762925703939, "mdate": 1762925703939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical limitation in the Learngene framework: the low efficiency of inheritable parameters. Existing methods (e.g., LeTs) suffer from overparameterized transformation matrices, achieving only 3–6× parameter compression and poor portability. To tackle this, the authors propose ALT (Adaptive Low-Rank Transformation), a novel Learngene approach that leverages Singular Value Decomposition (SVD) for parameterization and dynamic component adaptation. Key innovations include: 1) a SVD-inspired Gated Importance Score (GIS) that integrates component magnitude and orientation to quantify importance; 2) two adaptation strategies—Flat Global Adaptation (FGA) and Hierarchical Component Adaptation (HCA)—to dynamically optimize transformation matrix structures. Comprehensive experiments across vision (with ViT-Large as the Ancestry model, achieving 20× compression) and language (with GPT2-XLarge as the Ancestry model, achieving 25× compression) domains demonstrate that ALT outperforms baselines (Scratch, LeTs, Pre-Fin) on downstream tasks (e.g., ImageNet-1K classification, ADE20K segmentation, GLUE benchmarks) while reducing training steps by approximately 2×. This work confirms that ALT significantly enhances inheritable-parameter efficiency without compromising initialization quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Precise Problem Identification: The authors accurately pinpoint the core bottleneck of existing Learngene methods—overparameterized transformation matrices (accounting for >70% of inherited parameters in LeTs). By analyzing the low-rank properties of these matrices via SVD (rapid singular value decay), they provide a solid theoretical foundation for ALT’s design, avoiding the pitfall of \"searching for a solution without a clear problem.\"\n\n2. Novel and Rigorous Methodology: The proposed GIS metric effectively captures the multiplicative interaction between singular values (energy and structural direction), offering greater accuracy than additive scores (e.g., those in AdaLoRA). The two adaptation strategies (FGA, HCA) cater to distinct needs: FGA ensures global elitism in component selection, while HCA adapts to the modular structure of Transformers through a two-stage decision process (inter-matrix allocation + intra-matrix selection), demonstrating strong algorithmic depth.\n\n3. Comprehensive and Rigorous Experiments: The work validates ALT across diverse tasks (classification, segmentation, detection in vision; pre-training, QA, reasoning in language) and model scales (from Des-H6-L10 to Des-H16-L24). Ablation studies systematically verify the necessity of dynamic adaptation, GIS, and HCA, while comparisons with competitive baselines (LeTs, IMwLM, MatFormer, knowledge distillation) highlight ALT’s dual advantages in efficiency and performance.\nHigh Practical Value: ALT achieves 20–25× parameter compression, drastically reducing storage and computational costs for model initialization. Its \"train once, initialize many times\" paradigm is particularly valuable for resource-constrained scenarios (e.g., edge devices, mobile platforms) and rapid prototyping of variable-sized models, addressing real-world pain points of existing Learngene methods."}, "weaknesses": {"value": "1. Lack of Sufficient Justification for the Priori \"Minimum Effective Skeleton\" of Learngene, Leading to Strong Subjectivity: The initial structure of Learngene (the \"minimum effective skeleton\") in the paper is set via fixed priors: \"6 heads (head dimension 64) + 8 layers\" for vision tasks, and \"6 heads + 8 layers (BERT)/12 layers (GPT-2)\" for language tasks. The authors only describe this as an \"empirically derived minimum effective scale\" but provide no clear rationale for the choices: e.g., why 6 heads instead of 5 or 7? Why 8 layers as the minimum effective number for vision tasks instead of 7 or 9? This strong priori setting poses two issues: first, there are no ablation experiments to verify how different initial skeletons (e.g., varying head counts or layer numbers) affect Learngene’s knowledge inheritance ability and subsequent parameter compression efficiency, making it impossible to confirm if the current setting is optimal or suboptimal. Second, it remains unclear whether this priori skeleton is applicable when ALT is transferred to other architectures (e.g., Swin Transformers, MoE models). Such subjectivity may limit ALT’s generalizability and reduce the method’s interpretability and reproducibility.\n\n2. Insufficient Comparison with State-of-the-Art Low-Rank and Compression Methods: Although the paper distinguishes ALT from Parameter-Efficient Fine-Tuning (PEFT) methods (e.g., LoRA, AdaLoRA) by their objectives (initialization vs. task adaptation), it lacks direct empirical comparisons with these methods within the Learngene framework. For example, it does not verify how ALT performs against \"LoRA-based transformation matrices\" in terms of parameter efficiency and downstream accuracy. Additionally, ALT is not compared with other model compression techniques (e.g., structured pruning) for inheritable parameter learning, weakening the argument for ALT’s superiority in efficiency.\n\n3. Lack of Hyperparameter Sensitivity Analysis: Key hyperparameters of ALT (e.g., initial/final component counts H (0) /H (T), orthogonality regularization weight β,) are set to fixed values (e.g., H (0) =544, β=1e−3) without testing how variations in these hyperparameters affect model performance. For example, how does adjusting H (T) balance parameter compression ratio and downstream performance? Such analysis is critical for guiding the practical deployment of ALT, and its absence undermines the method’s usability.\n\n4. Incomplete Discussion of Des-Net Initialization Strategies: The paper adopts LeTs’ initialization strategies but does not compare them with alternative strategies (e.g., random selection). It also fails to explain why continuous selection is optimal.\nInsufficient Analysis of Performance Differences Across Des-Net Scales: Experimental results show that ALT’s performance gains vary across Des-Net sizes (e.g., 0.7% improvement over LeTs for Des-H6-L10, 0.9% for Des-H12-L16). However, the authors do not analyze the reasons for this variation.\n\n5. Insufficient Analysis of Performance Differences Across Des-Net Scales: Experimental results show that ALT’s performance gains vary across Des-Net sizes (e.g., 0.7% improvement over LeTs for Des-H6-L10, 0.9% for Des-H12-L16). However, the authors do not analyze the reasons for this variation."}, "questions": {"value": "Please refer to the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i4pDD7Y55m", "forum": "TkJ8UNxD6B", "replyto": "TkJ8UNxD6B", "signatures": ["ICLR.cc/2026/Conference/Submission15428/Reviewer_7Pxt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15428/Reviewer_7Pxt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961923638, "cdate": 1761961923638, "tmdate": 1762925703164, "mdate": 1762925703164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}