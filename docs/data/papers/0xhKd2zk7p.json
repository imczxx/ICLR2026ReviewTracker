{"id": "0xhKd2zk7p", "number": 5922, "cdate": 1757946616832, "mdate": 1759897944722, "content": {"title": "APR: Asymmetric Projector Regularization Improves EEG-to-Vision Contrastive Learning", "abstract": "Contrastive learning approaches that align electroencephalography (EEG) with visual representations have made progress in recent years, yet performance remains limited under the stringent 200-way zero-shot setting. We propose APR (Asymmetric Projector Regularization), a strategy that imposes asymmetric capacity on the dual-tower alignment. Concretely, the EEG branch uses a minimal single linear head followed by $ℓ2$ normalization, while the image branch retains a small, learnable projector (linear or a two-layer MLP) to accomplish the necessary geometric alignment. Built on NICE-style contrastive training with a frozen CLIP visual encoder, APR lifts zero-shot Top-1 accuracy from 13.8% to 30.45%, with concurrent gains in Top-5/Top-10, mean rank (mRank), and mean reciprocal rank (MRR). Systematic ablations show that simplifying the EEG-side projector improves generalization, whereas over-simplifying the image-side projector causes substantial performance degradation. APR remains robust across subjects, time windows, and temperature or batch-size settings. We advocate this small EEG head plus small, learnable image head recipe as a simple, reusable practice for more reliable cross-modal alignment in high-noise, low-sample EEG-to-vision scenarios. Code available at https://anonymous.4open.science/r/only-for-test-2026.", "tldr": "", "keywords": ["asymmetric projector", "eeg", "contrastive learning", "vision"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa315d37b8e3a4910724d443e64dfa316544ed8a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Asymmetric Projector Regularization (APR), a modification to dual-tower EEG–vision contrastive learning pipelines (notably NICE) in which the EEG encoder uses a minimal linear projector while the frozen CLIP image encoder retains a small learnable head. The authors claim that this asymmetric setup improves 200-way zero-shot recognition on the THINGS-EEG dataset from 13.8% to 30.45% Top-1 accuracy, with concurrent gains in Top-5 and retrieval metrics. They argue that the EEG side benefits from reduced capacity (less noise fitting) and that the image side benefits from limited flexibility for geometric alignment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The idea of asymmetric capacity between EEG and visual branches is intuitively interesting and potentially useful for noisy cross-modal alignment tasks.\n\n2.Empirical experiments cover multiple ablations and subjects and include Grad-CAM visualizations, suggesting reasonable implementation diligence.\n\n3.The reported gains on the THINGS-EEG dataset are large (≈2× over the NICE baseline)."}, "weaknesses": {"value": "1. The proposed “asymmetry” amounts to choosing a linear layer on one side and a shallow MLP on the other, essentially a small design tweak. There is no theoretical justification, no general framework, and no principled regularization formulation. The idea is largely empirical and could be described as tuning hyperparameters (projector depth) rather than proposing a novel algorithmic contribution.\n\n2. Recent works (e.g., NICE-GA, NICE-SA, Chen et al. 2024, Mai et al. 2024) already examined projection head asymmetry and capacity effects in EEG–vision and multimodal contrastive setups. APR does not offer conceptual advancement beyond stating that “EEG should have less capacity.” This has been repeatedly discussed in the literature on noisy modality alignment and asymmetric InfoNCE setups.\n\n3. There are no cross-validation results, no training variance analysis, and no statistical testing. I think the evaluation should be separated into (1) intra-subject evaluation and (2) inter-subject evaluation, and some most recent works should be used for comparison, e.g., UBP.\n\n\n4. The NICE baseline performance reported (≈13.8%) is significantly below what most recent NICE implementations achieve (≈20–25%). It raises concerns that baselines were not properly tuned or that the training setup differs from prior work. Without stronger baselines (or even a shared-code replication), the reported gains are not convincing.\n\n\n5. The Grad-CAM topographies are anecdotal and add little scientific evidence. The claim that APR “focuses on physiologically plausible regions” is not quantified or supported by neuroscientific metrics.\n\n6. Lack of conceptual depth. While the paper is technically tidy, it reads as a minor engineering note rather than a deep scientific contribution suitable for ICLR. There is no discussion of the implications for representation geometry, no connection to contrastive learning theory, and no novel loss or regularization term."}, "questions": {"value": "1. How does the proposed asymmetry differ fundamentally from simple hyperparameter tuning (e.g., varying projector depth), and what theoretical or principled justification supports this design choice?\n\n2. In what way does APR advance beyond existing studies such as NICE-GA, NICE-SA, Chen et al. (2024), and Mai et al. (2024), which already explore projection head asymmetry and modality-specific capacity effects?\n\n3. Why are there no cross-validation, variance, or statistical significance analyses to establish the reliability of the reported results?\n\n4. How can the authors claim zero-shot or generalizable performance when all experiments are conducted on a single, subject-specific dataset (THINGS-EEG) without external validation or testing on other EEG datasets?\n\n5. Why is the NICE baseline performance substantially lower than values reported in prior literature, and were baseline models appropriately tuned and implemented following standard setups?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9dlWayK0La", "forum": "0xhKd2zk7p", "replyto": "0xhKd2zk7p", "signatures": ["ICLR.cc/2026/Conference/Submission5922/Reviewer_z6ca"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5922/Reviewer_z6ca"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761233076514, "cdate": 1761233076514, "tmdate": 1762918353422, "mdate": 1762918353422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **APR (Asymmetric Projector Regularization)**, a simple modification to contrastive EEG–vision alignment frameworks. The method introduces asymmetric projection heads—using a minimal linear head on the EEG side and a small learnable projector on the visual side—to improve robustness and generalization when aligning noisy EEG features with frozen vision encoders such as CLIP. The work aims to address the sensitivity of prior symmetric projector designs to noise and limited EEG samples."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Interesting Perspective**  \n   The paper introduces a potentially novel architectural insight by highlighting asymmetric projection capacity between modalities in EEG–vision alignment — a design consideration that appears to have been underexplored in prior work.\n\n2. **Clear Motivation**  \n   The study provides a convincing and well-structured motivation, acknowledging the intrinsic imbalance between noisy, low-resource EEG signals and a frozen large-scale visual encoder (CLIP), which may partially explain why asymmetric projection could be beneficial."}, "weaknesses": {"value": "1. **Limited Baselines and Experimental Scope**  \n   The paper mainly compares with NICE and its slight variants. Since APR is built upon NICE, these comparisons effectively serve as ablations rather than broad baselines. It would strengthen the work to include results against more recent EEG–vision or multimodal approaches to better contextualize the contribution.\n\n2. **Figure Readability and Formatting**  \n   The figures—especially Figure 3—suffer from small fonts and overlapping legends that significantly reduce readability. Please enlarge text labels and adjust layouts for clarity.\n\n3. **Writing and Structural Issues**  \n   The writing quality can be improved. The **Methods** section is too brief and only contains one subsection (“Overview”). It would be helpful to expand this part with clearer technical details and possibly additional subsections describing the architecture, training setup, and implementation specifics."}, "questions": {"value": "See the detailed comments above regarding weaknesses and improvement suggestions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9WEeOeiGkb", "forum": "0xhKd2zk7p", "replyto": "0xhKd2zk7p", "signatures": ["ICLR.cc/2026/Conference/Submission5922/Reviewer_9iw5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5922/Reviewer_9iw5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482616531, "cdate": 1761482616531, "tmdate": 1762918353221, "mdate": 1762918353221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new method for decoding the Gifford dataset. The dataset consists of pairs of images and EEG recordings from subjects viewing those images. The images are mapped to embeddings with CLIP with an added postprocessor. The EEG recordings are mapped to embeddings with TSConv with an added postprocessor. CLIP is frozen but the postprocessors are learned. While the paper does not make this clear, presumably TSConv is also trained. Training is done on a training set with contrastive loss. Test is done on a separate test set. The crucial thing that differentiates this from much other work is the metric used to evaluate performance on the test set. Since the test set does not contain any classes in the training set, it is evaluated not as a classifier but in a retrieval framework. Each test EEG is mapped to the embedding with the trained EEG to embedding mapping. All of the test images are mapped to their embeddings with the trained image to embedding mapping. For each test EEG, the image with the closest embedding is selected. The metric is what fraction of the test EEGs select the correct image.\n\nCLIP is not new. They use a pretrained CLIP. The TSConv EEG decoder is not new.  The contrastive loss is not new. The evaluation metric is not new. The dataset is not new. They use the existing Gifford dataset. What is new is the postprocessors. Various small postprocessors are evaluated, all of them small, i.e. one or two linear layers followed by normalization.\n\nThe paper conducts various ablation studies to evaluate the different postprocessors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This is essentially a \"beat the numbers\" paper. The fraction of the time the correct image is selected increases from about 13% to 30%. I am not familiar with the literature on this problem to verify that indeed this is state of the art performance. But I will trust the authors on that. I don't think it matters given my comments below."}, "weaknesses": {"value": "I have three main concerns.\n\n1. Traditional approaches to evaluating decoders employs classifiers. The stimuli are of some number of classes and classification accuracy is measured independently on each test sample. Here, each test sample is not classified independently but rather in the context of the entire test set. This accuracy measured this way depends on test set size and characteristics. EEG classification is known to be extremely difficult. State of the art performance on 40 classes from a single trial is about 7%. State of the art methods degrade to chance after about 10 classes. This paper does not evaluate or demonstrate that its methods improve performance on this problem. It gets better numbers because the problem definition has changes. I don't know how to compare results with the two different evaluation methods. For classification problems, I know what chance performance is and how to evaluated statistical significance. I don't know how to do that for this problem formulation. So I don't know how hard this problem is and I don't know how good the results are. It is somewhat misleading to claim that accuracy is 30% on 200-way zero-shot recognition because that means something very different than a classical 200-way classification problem evaluated on independent test trials. I expect peformance on the latter would be about chance (0.5%).\n\n2. The central claim is that particular postprocessors are best. But these are evaluated only on a single EEG to embedding mapping (TSconv), a single image to embedding mapping (CLIP), a single pretrained instance of CLIP (presumably pretrained on MSCOCO), a single loss function (line 148), and a single dataset (Gifford). The results might not hold if any of these are changed. Essentially all that is happening is that the space of EEG/image pairs in the test set is well separated by the embeddings This might be a particular property of the Gifford dataset and pretrained CLIP. We just don't know.\n\n3. I struggle to understand what the contribution is. This doesn't tell us anything relevant to neuroscience. It doesn't tell us what is represented in the brain or even in the EEG signal. It might be high-level information like class. It might be low-level image characteristics like contrast, edges, color, texture, shape, ... We just don't know what is in the embeddings that are constructed. It might be something interesting. It might not. It doesn't tell us anything relevant to engineering. We don't know if this system will work on a different dataset. It could just be that the Gifford dataset has some characteristic that the test set is highly separable from embeddings learned on the training set. Even if it would work, we have no idea what the \"30% zero-shot accuracy\" means. We have no way of predicting accuracy, zero-shot or otherwise, when these methods are applied to  any other dataset or task.\n\nThe central question in my mind is reconciling 30% 200-way zero-shot recognition accuracy on this retrieval-based problem formulation with 7% 40-way classification accuracy with 800 training samples per class with the classifier formulation. The former suggests that EEG decoding is easy. The latter suggests that EEG decoding is hard, nearly impossible. It must mean that the problem reformulation gives an illusion of progress in the field that isn't really there. And since both neuroscience understanding and engineering use cases would appear to be dependent on being able to decode specific characteristics of independent stimuli, this illusory progress might not translate to actual progress."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mivhWRQx70", "forum": "0xhKd2zk7p", "replyto": "0xhKd2zk7p", "signatures": ["ICLR.cc/2026/Conference/Submission5922/Reviewer_53eH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5922/Reviewer_53eH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761695337959, "cdate": 1761695337959, "tmdate": 1762918352937, "mdate": 1762918352937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how to improve the alignment between noisy electroencephalography (EEG) signals and high-level visual representations obtained from a frozen CLIP encoder under a contrastive learning framework. Existing EEG-to-image alignment methods typically adopt symmetric projection heads for both modalities. However, the authors argue that this design overlooks fundamental asymmetries between EEG and visual embeddings: EEG data are low in signal-to-noise ratio (SNR) and limited in scale, whereas CLIP’s visual space is large, stable, but geometrically anisotropic.\nTo address this imbalance, the paper introduces Asymmetric Projector Regularization (APR) — a simple yet effective architectural modification that explicitly imposes unequal representational capacity across the two branches. Specifically:\n- EEG branch: employs a single linear layer followed by L₂ normalization, acting as a strong bottleneck that limits overfitting to EEG noise and enforces regularization.\n- Vision branch: uses a small learnable projector (either linear or a shallow two-layer MLP) that adapts the frozen CLIP embeddings to the EEG feature space through geometric alignment and scale calibration.\nThrough this asymmetric design, APR significantly improves zero-shot cross-modal recognition, boosting 200-way Top‑1 accuracy from 13.8% to 30.45%, along with consistent gains in ranking-based metrics. The results demonstrate that appropriately constraining the noisy EEG branch while preserving limited flexibility on the visual side leads to more robust and generalizable EEG-to-vision alignment."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an interesting and emerging topic—EEG-to-vision contrastive learning—and proposes a conceptually simple idea of introducing asymmetric projection head regularization. \n2. The formulation is clear enough to be reproducible, and the authors conduct a number of basic ablation experiments to support their main claim. \n3. The proposed approach is lightweight and compatible with existing contrastive frameworks, which could make it easy to reuse in future research."}, "weaknesses": {"value": "1. The experimental comparison is quite limited. The authors only compare their method with the 2023 NICE baseline, ignoring several more recent and relevant EEG–vision alignment methods such as ATM, UBP, and ViEEG. As a result, the claimed improvement is not well contextualized against the current state of the art, and the retrieval performance remains substantially lower than these newer baselines.\n2. Many of the paper’s arguments rely on qualitative or inductive reasoning (e.g., claims regarding EEG noise overfitting or CLIP space anisotropy) but are not supported by quantitative analyses or visual evidence. Without empirical validation, these explanations remain speculative.\n3. The overall presentation of the paper requires considerable improvement. The architectural and results figures are not carefully prepared—Figure 3 has color bar issues, and Figure 4 appears to be a direct screenshot from wandb rather than a properly formatted scientific plot. These problems reduce the professionalism and readability of the paper."}, "questions": {"value": "1. The authors claim that the proposed asymmetric projector regularization is a simple and effective solution. However, the method appears quite minimal. Have the authors explored whether this approach generalizes to other EEG encoders such as EEGNet or alternative backbone architectures?\n2. The paper states that “a shallow EEG head resists noise,” but no experiments are presented to support this. Could the authors provide noise robustness analyses, such as controlled noise addition, data ablation, or random-label experiments?\n3. Figure 4 illustrates how performance varies with projector complexity, but no statistical significance testing is reported. Are the observed differences statistically reliable or within the margin of variance?\n4. The current experimental section lacks sufficient ablation studies. Could the authors clarify what specific ablations were performed and add experiments to verify the contribution of each proposed component?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QXKIe9IAxW", "forum": "0xhKd2zk7p", "replyto": "0xhKd2zk7p", "signatures": ["ICLR.cc/2026/Conference/Submission5922/Reviewer_3wWD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5922/Reviewer_3wWD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922716074, "cdate": 1761922716074, "tmdate": 1762918352639, "mdate": 1762918352639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}