{"id": "4ST2YyTjI7", "number": 6262, "cdate": 1757962883841, "mdate": 1759897926128, "content": {"title": "LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts", "abstract": "Recent studies have shown that combining parameter-efficient fine-tuning (PEFT) with mixture-of-experts (MoE) is an effective strategy for adapting large language models (LLMs) to the downstream tasks. However, most existing approaches rely on conventional TopK routing, which requires careful hyperparameter tuning and assigns a fixed number of experts to each token. In this work, we propose LD-MoLE, a Learnable Dynamic routing mechanism for Mixture of LoRA Experts that enables adaptive, token-dependent, and layer-wise expert allocation. Our method replaces the non-differentiable TopK selection with a differentiable routing function and a closed-form solution. Moreover, our design allows the model to adaptively determine the number of experts to activate for each token at different layers. In addition, we introduce an analytical sparsity control objective to regularize the number of activated experts. Extensive experiments on the Qwen3-1.7B and Llama-3.2-3B models show that LD-MoLE achieves the highest average scores compared to state-of-the-art baselines, across a diverse set of benchmarks. Our method not only achieves superior performance, but also demonstrates the ability to learn token-dependent and layer-wise expert allocation.", "tldr": "", "keywords": ["Mixture of Experts", "Mixture of LoRA Experts", "Dynamic routing", "Fully differentiable", "LoRA", "MoE"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2024ff1fd21b53fbe6004e7b7ed070707e747ca7.pdf", "supplementary_material": "/attachment/d3b1a2fa8e9ae454985525c789070dff48defe82.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces LD-MoLE, a Mixture-of-LoRA-Experts framework with learnable dynamic routing based on a closed-form Sparsegen function. A shared MLP predicts a token-wise sparsity factor λ , allowing the model to dynamically adjust the number of activated experts per token in a differentiable and stable manner. The method also includes an analytical sparsity loss for expert control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality - The paper proposes a Sparsegen-based dynamic routing method with a mathematically proven sparsity loss, showing novelty and clear motivation.\n\n2. Quality and Clarity – The methodology is well-designed and theoretically sound, with clear writing that makes the approach easy to follow.\n\n3. Significance – The visualization of LoRA experts selected per token clearly demonstrates the effectiveness of the proposed dynamic routing mechanism."}, "weaknesses": {"value": "1. Insufficient Ablation Studies and Experiments\nThe paper lacks an ablation study on the load balancing loss weight and its impact on the model's performance. It would be beneficial to explore how varying this parameter affects the stability and performance of the model, especially since load balancing plays a crucial role in expert distribution.  \nThe paper also does not compare against important baselines like:  \n[1] Harder Task Needs More Experts: Dynamic Routing in MoE Models  \n[2] HMoRA: Making LLMs More Effective with Hierarchical Mixture of LoRA Experts Including these baselines would provide a more comprehensive evaluation and highlight the specific advantages of the proposed approach.\n\n2. Lack of Large-Scale Model Comparison\nThe experiments are limited to relatively smaller models (Llama-3.2-3B and Qwen-3-1.7B). A comparison against larger-scale models would help validate the scalability of the proposed method and its ability to handle more complex tasks. This would provide a clearer picture of how the method performs in more resource-intensive settings.\n\n3. Unconventional Benchmark Selection\nThe choice of experimental benchmarks seems somewhat unusual. The paper compares against a small subset of tasks, whereas other multi-task MoLE baselines (e.g., [3] KASA: Knowledge-Aware Singular-Value Adaptation of Large Language Models and [4] MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning) involve a broader range of tasks. The rationale behind selecting this specific subset is unclear, and a more diverse set of benchmarks could provide a more robust evaluation. A clarification on why this subset was chosen would improve the overall experiment design.\n\n4. Unclear Necessity of Sparsegen Routing\nThe relationship between Sparsegen routing and the Mixture-of-Experts (MoE) architecture is not fully explained. It is unclear why Sparsegen routing was specifically chosen over other potential methods. Has the paper explored alternatives like Gumbel-Softmax or directly using softmax without Top-K selection? These methods could provide valuable insights and potentially simplify the routing process. A discussion on why Sparsegen is preferred, and a comparison to other discrete or continuous routing methods, would strengthen the theoretical justification for this choice.\n\n5. Computational cost\nThe paper lacks a quantitative evaluation of the additional computational cost introduced by the Sparsegen routing (such as the sorting operation and MLP prediction), and does not provide direct comparisons with baseline methods in terms of training/inference time, FLOPs, or latency.\n\nReferences:  \n[1] Harder Task Needs More Experts: Dynamic Routing in MoE Models.  \n[2] HMoRA: Making LLMs More Effective with Hierarchical Mixture of LoRA Experts.  \n[3] KASA: Knowledge-Aware Singular-Value Adaptation of Large Language Models.  \n[4] MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rpFgEQiyBu", "forum": "4ST2YyTjI7", "replyto": "4ST2YyTjI7", "signatures": ["ICLR.cc/2026/Conference/Submission6262/Reviewer_oEkd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6262/Reviewer_oEkd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823220942, "cdate": 1761823220942, "tmdate": 1762918578239, "mdate": 1762918578239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LD-MoLE, a learnable dynamic routing framework for Mixture of LoRA Experts (MoLE) that replaces the traditional non-differentiable Top-K selection with a Sparsegen-based routing mechanism. The model adaptively determines the number of activated experts per token and per layer through a shared MLP predicting a sparsity factor λ, allowing token-dependent and layer-wise routing. The authors also introduce an analytical sparsity control loss to regularize the number of activated experts. Extensive experiments on Llama-3.2-3B and Qwen-3-1.7B show consistent improvements over MoLA (Top-K routing) and ReMoLE (ReLU-based routing) across multiple NLP benchmarks. The results demonstrate improved efficiency, stability, and interpretability of dynamic routing in parameter-efficient fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Novel Differentiable Routing Mechanism:** The paper presents a clean and theoretically grounded alternative to Top-K routing using Sparsegen, providing both differentiability and guaranteed non-zero expert activation, addressing stability issues observed in prior dynamic routing work (e.g., ReMoE).\n\n**Comprehensive Analysis:** The authors provide solid theoretical derivations, ablations on sparsity and λ prediction, and empirical validation on two base LLMs across 8 diverse benchmarks, clearly showing robustness and consistency.\n\n**Strong Practical Value:** The design achieves dynamic, token-aware expert selection without increasing parameter count significantly (thanks to the shared MLP), making it practical for real-world PEFT and instruction-tuning applications."}, "weaknesses": {"value": "1. **Gains are modest and transferability isn’t established.**\nReported improvements over Top-K/RELU routers (e.g., MoLA, ReMoLE) are incremental on small–mid LLMs and a limited task suite; the paper doesn’t convincingly show zero-shot / cross-dataset transfer (e.g., to harder or different domains such as math/code/MMLU) or robustness under distribution shift. Including a brief analysis (no extra training) of cross-dataset generalization would better support the method’s claims. \n\n2. **No scale-up evidence (3B→8B→30B+) and limited backbone diversity.**\nExperiments focus on small models (e.g., 1.7B–3B), leaving open whether learnable sparsity remains stable/efficient as parameters and depth grow (routing entropy, expert utilization balance, convergence speed, memory/latency). By contrast, scalable MoE work (e.g., Switch Transformer, DeepSeek-V3) validates routing and load balancing at tens–hundreds of billions of parameters; LoRA-MoE baselines like MixLoRA also report on a broader set of bases (e.g., LLaMA/Gemma/Mistral), aiding claims of generality. Adding at least partial scaling curves and results on a more diverse set of backbones would strengthen the paper.\n\n3. **Why choose Sparsegen over other differentiable routers isn’t empirically settled.**\nTheoretical appeal aside (deterministic, truly sparse, λ-controllable), the paper lacks a side-by-side comparison under the same budget against other differentiable routing families widely used in practice:\n– Soft routing + Top-K variants from large-scale MoE (e.g., Switch/DeepSeek-V3 and modern aux-loss-free balancing);\n– Gumbel-Softmax/Concrete reparameterization for differentiable categorical choices;\n– ReLU-based differentiable routers.\nA controlled ablation (same base/rank/FLOPs) on accuracy, stability, load balance, latency, and activated-expert count would clarify whether Sparsegen offers practical advantages beyond its theory."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fB8gOYhgGb", "forum": "4ST2YyTjI7", "replyto": "4ST2YyTjI7", "signatures": ["ICLR.cc/2026/Conference/Submission6262/Reviewer_GWxU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6262/Reviewer_GWxU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850537703, "cdate": 1761850537703, "tmdate": 1762918577879, "mdate": 1762918577879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LD-MoLE, a novel **Learnable Dynamic** routing mechanism for **M**ixture **o**f **L**oRA **E**xperts. The method addresses key limitations in existing MoLE approaches by replacing the non-differentiable Top-K routing with a fully differentiable routing function based on Sparsegen projection. LD-MoLE introduces a lightweight shared MLP that predicts a token- and layer-dependent sparsity parameter (λ), enabling adaptive expert allocation. An analytical sparsity loss is derived to explicitly control the number of activated experts. Extensive experiments on Llama-3.2-3B and Qwen3-1.7B demonstrate state-of-the-art performance across diverse benchmarks, outperforming strong baselines like MoLA (Top-K) and ReMoLE (ReLU-based routing)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The idea of combining Sparsegen-based differentiable routing with a learned λ-predicting MLP is novel and impactful. It elegantly solves the non-differentiability of Top-K and the instability of ReLU routing, advancing the MoLE paradigm.\n- Theoretical grounding such as closed-form solutions, proofs is solid. Experiments are comprehensive, covering multiple models, tasks, and ablation studies including sparsity control, λ analysis and zero-activation issue.\n- The paper is well-structured, clearly written, and figures effectively illustrate the methodology and findings.\n- LD-MoLE provides a principled, efficient, and high-performing framework for dynamic expert routing, with potential influence on both PEFT and MoE research communities."}, "weaknesses": {"value": "- **Computational Overhead**: While parameter-efficient, the computational cost (e.g., latency, FLOPs) of the dynamic routing mechanism—especially the Sparsegen projection—compared to simple Top-K is not quantified. A brief analysis would strengthen the efficiency claim.\n- **Baseline Comparison**: Including a fixed-λ Sparsegen baseline would more directly isolate the contribution of the learned λ (via the MLP) from the contribution of Sparsegen itself.\n- **Hyperparameter Sensitivity**: The impact of the new hyperparameters (α, β, and target sparsity k) on performance and stability is not discussed. A sensitivity analysis would improve practical usability."}, "questions": {"value": "1. The shared MLP predicts λ for the same module type across all layers. Does this design potentially limit the model's ability to learn highly specialized, layer-specific routing strategies? What is the trade-off between parameter efficiency and layer-wise flexibility here?\n\n2. In the sparsity control analysis (Sec. 4.4), using the sparsity loss (β>0) reduces expert count at a performance cost. Do you see a pathway for the model to learn a task-optimal sparsity level automatically, rather than relying on a pre-defined k?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v8KvTZ3Djm", "forum": "4ST2YyTjI7", "replyto": "4ST2YyTjI7", "signatures": ["ICLR.cc/2026/Conference/Submission6262/Reviewer_Ckcz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6262/Reviewer_Ckcz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918420149, "cdate": 1761918420149, "tmdate": 1762918577522, "mdate": 1762918577522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LD-MoLE, a learnable dynamic routing mechanism for Mixture of LoRA Experts. Built on Sparsegen, the method introduces a differentiable and token-wise adaptive expert allocation strategy, with a learnable sparsity parameter predicted by a shared MLP. The framework also includes a sparsity control objective to regulate the number of activated experts. Experiments across several downstream tasks show improved performance compared to TopK and ReLU-based routing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a clear motivation for improving expert allocation in MoE models and proposes a differentiable routing mechanism with controllable sparsity. The method is well-integrated into the LoRA-based MoE setting, and the overall presentation is clear. Experiments cover multiple tasks and include comparisons against relevant baselines, along with some ablation studies."}, "weaknesses": {"value": "* **Applicability to Standard MoE Architectures**: While LD-MoLE is instantiated within the MoLE setting, it remains unclear whether the proposed routing mechanism can be seamlessly integrated into conventional MoE architectures with full FFN experts or applied during pretraining. Clarifying its plug-and-play compatibility with standard MoE training and inference pipelines would help better demonstrate the generality and practical advantages of the approach.\n\n* **Insufficient Efficiency Evidence**: The paper claims that LD-MoLE effectively reduces the number of activated experts, but the evaluation largely reports task accuracy and qualitative activation plots. To substantiate efficiency, more concrete system metrics (e.g., FLOPs, latency, throughput and peak memory) would better support the conclusion that reduced activation translates into real compute savings.\n\n* **Missing Discussion Regarding Sparse Routing.**: the paper lacks comparison with prior methods that target similar goals through dynamic routing or explicit sparsity control [1-3]. A clearer conceptual and empirical comparison with related works would strengthen the contribution.\n\n[1] Team M L C, Li B, Lei B, et al. Longcat-flash technical report[J]. arXiv preprint arXiv:2509.01322, 2025.\n\n[2] Zeng Z, Miao Y, Gao H, et al. Adamoe: Token-adaptive routing with null experts for mixture-of-experts language models[J]. arXiv \npreprint arXiv:2406.13233, 2024.\n\n[3] Yue T, Guo L, Cheng J, et al. Ada-k routing: Boosting the efficiency of moe-based llms[C]//The Thirteenth International Conference on Learning Representations. 2024."}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HabOsB21Zd", "forum": "4ST2YyTjI7", "replyto": "4ST2YyTjI7", "signatures": ["ICLR.cc/2026/Conference/Submission6262/Reviewer_5J1o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6262/Reviewer_5J1o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987927858, "cdate": 1761987927858, "tmdate": 1762918577077, "mdate": 1762918577077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}