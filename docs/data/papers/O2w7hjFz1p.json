{"id": "O2w7hjFz1p", "number": 8171, "cdate": 1758072331612, "mdate": 1763170086402, "content": {"title": "ArgQA: Evaluation of Reasoning Over Elementary Logical Structures in Arguments", "abstract": "As large language models advance in their reasoning capabilities, their adequate evaluation is becoming increasingly important. Existing logical reasoning benchmarks are typically constructed by automatically converting symbolic logic into natural language or curating questions from standardized exams, such as LSAT. However, both synthetic and exam-style questions are composed of unnatural language, thereby limiting their applicability to real-world contexts. Also, the systematic assessment of reasoning over diverse logical structures remains underexplored. Thus, we present ArgQA, a novel dataset of 3,807 multiple-choice questions based on authentic arguments from four distinct domains—product reviews, argumentative essays, e-rulemaking comments, and  medical research abstracts. Each question is designed to assess the ability to recognize and reconstruct one of three elementary logical structures—linear, convergent, and divergent—whose understanding is a prerequisite to both simple and complex reasoning. Experiments show that even the strongest LLMs still have considerable room for improvement with the overall 9-shot accuracy ranging from 29.2% (Qwen-2) to 61.8% (GPT-o3).", "tldr": "We present ArgQA, a novel dataset of multiple-choice questions to assess logical reasoning over elementary logical structures, based on authentic arguments from four distinct domains.", "keywords": ["Logical Reasoning Benchmark", "LLM evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/5a6262c569422a0786d476dc14779cf6b9a54d61.pdf", "supplementary_material": "/attachment/f6a212df936cf0f530a8da628836d60962b26af3.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ARGQA, a novel benchmark dataset designed to evaluate the logical reasoning capabilities of LLMs on authentic, real-world arguments. The authors argue that existing benchmarks are often inadequate, as they either rely on synthetically generated text or exam-style questions that use unnatural language, limiting their real-world applicability. ARGQA consists of 3,807 multiple-choice questions sourced from arguments across four distinct domains: product reviews, argumentative essays, e-rulemaking comments, and medical research abstracts. The dataset is specifically designed to assess an LLM's ability to recognize and reconstruct three elementary logical structures: linear, convergent, and divergent. The questions are standardized into nine distinct types, each corresponding to a specific logical relationship between the context sentences and the potential answers modeled by a graph, with correct alternative MCQ options constructed with the graph. Experiments on several modern LLMs (including the Mistral, Llama, and GPT series) demonstrate that this task remains challenging for modern models. The strongest model, GPT-03, achieved an accuracy of only 61.81% in a 9-shot setting."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Novel Problem Formulation: The paper identifies a clear and important gap in current LLM evaluation: the lack of benchmarks that test structural logical reasoning on natural, authentic arguments, constructed with consideration of standard logic/argument structures in philosophy.\n\nThoughtful Dataset Construction: The methodology and particularly the explanation for creating ARGQA is strong/ It is well-defended and transparently explained. \n\nFine-Grained Analysis: The paper's design, which is built around three elementary structures and nine specific question types, is a key strength. This allows for a better analysis than a single accuracy score, as demonstrated by the paper's ability to break down performance by structure type and logical role.\n\nClear Writing: The paper is well-written and the motivation is clearly established with a solid theoretical grounding in argumentation theory."}, "weaknesses": {"value": "The paper's primary weakness lies in a few missed opportunities for a more comprehensive evaluation, which are mostly captured in the questions below.\n\nThe evaluation is limited to 0-shot and 9-shot accuracy using greedy decoding. This leaves open questions about model consistency, the effects of different sampling strategies, and the potential for few-shot learning with more examples.\n\nThe paper does not fully explore the robustness of the models. For example, it is unclear how the models perform compared to human paraphrasing of the propositions or if their performance is brittle/varied across models."}, "questions": {"value": "How does model performance on ARGQA compare to performance on equivalent, purely symbolic logic problems (e.g., \"A supports B, B supports C\")? \n\nCan you provide more evidence for the claim that \"linked structures\" are infrequent in practice? Was their exclusion also motivated by the difficulty of data collection?\n\nWas the evaluation of more complex, composite logical structures (beyond the three elementary ones) considered as a next step?\n\nCould you provide an example of how \"the lack of explicit context often obscures the logic\" (Section 3.3) and clarify if this implies some questions are ambiguous without external knowledge?\n\nWhat was the rationale for choosing 0-shot and 9-shot settings? Since 9-shot provides only one example per question type, how does performance scale with more in-context examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p8TRXv6SbM", "forum": "O2w7hjFz1p", "replyto": "O2w7hjFz1p", "signatures": ["ICLR.cc/2026/Conference/Submission8171/Reviewer_XSW5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8171/Reviewer_XSW5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698786688, "cdate": 1761698786688, "tmdate": 1762920133559, "mdate": 1762920133559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thanks for the effort everyone has put into reviewing this work"}}, "id": "EJOvnpJgp7", "forum": "O2w7hjFz1p", "replyto": "O2w7hjFz1p", "signatures": ["ICLR.cc/2026/Conference/Submission8171/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8171/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763170085693, "cdate": 1763170085693, "tmdate": 1763170085693, "mdate": 1763170085693, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to evaluate the capability of LLMs in processing the basic logical structures in real-world text. It is claimed to overcome the problem of lack of natural language in existing datasets.\n\nIn this work, the authors focuses three types of basic logical structures, and then define 9 types of questions based on them by introducing some distractors into the three basic logical structures. \n\nThe question generation starts from source selection. Several datasets with desired logical labels are chosen. Then correct triplets are extracted from the text source and refined by paraphrasing. Distractors are then introduced to ad some noise. Finally, the questions are designed to query the logical relationship between the context and correct options."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper targets the reasoning capability of LLMs, and specifically focuses on the capability to analyze over the basic logical structures, which is a good intention. In addition, the work pointed out some unnatural problem in existing benchmark, which should be an important aspect of the research in this direction."}, "weaknesses": {"value": "1. The constructed questions only contain triplets extracted from original datasets with natural language usage, then the content is also not the original natural content we could encounter in daily life. In addition, the questions are simply determine the relationship between the context and the options, which does not look like a natural scenario either.\n\n2. Some concepts are not clearly defined before usage. For example, which propositions are context propositions in the three types of logical structures focused in this work? I was guessing that it should be the first two propositions in linear and convergent structures and the first one proposition in the divergent structure. However, around line 233, it is written '.... one or both of the context propositions ...', which seems to imply that each structure has exactly two context propositions. \n\n3. The explanation on the logical distractor is not clear enough. More concrete examples is needed to illustrate this.\n\n4. The distractors are generated by GPT-o3, while there seems to be no approach to validate and ensure the correctness."}, "questions": {"value": "1. Although there are unnatural questions in the existing benchmarks, are there also a sufficient amount of questions following natural expression? It seems possible that existing benchmarks contain both natural and unnatural expressions.\n\n2. The categorization on the logical structures does not follow common logical analysis frameworks, e.g. prepositional logic or first-order logic. How to justify that the adopted three type of logical structures are reasonable enough for evaluating the reasoning capability of LLMs. In addition, does this covers all possible logical structures?\n\n3. The selected source datasets already contain the relation annotations. Then it seems that the target logical structures are already available in these datasets, and what is the unique contribution of this work?\n\n4. The constructed questions only contain triplets extracted from original datasets with natural language usage, then the content is also not the original natural content we could encounter in daily life. How would the authors interpret this?\n\n5. How are the direct graph generated from the text, by LLM? It seems that sometimes the logical structure in the text could be complex or vague, which cause difficulty for extracting clear logical structure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eEZ536RYRR", "forum": "O2w7hjFz1p", "replyto": "O2w7hjFz1p", "signatures": ["ICLR.cc/2026/Conference/Submission8171/Reviewer_n2NV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8171/Reviewer_n2NV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790200380, "cdate": 1761790200380, "tmdate": 1762920133047, "mdate": 1762920133047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ARGQA, a new benchmark dataset designed to evaluate the logical reasoning capabilities of Large Language Models (LLMs) over elementary logical structures—namely linear, convergent, and divergent argument structures. The dataset contains 3,807 multiple-choice questions derived from real-world arguments across four domains: product reviews, argumentative essays, e-rulemaking comments, and medical research abstracts. Each question is crafted to assess the model’s ability to recognize or reconstruct a specific logical structure, with carefully designed distractors that reflect common logical misunderstandings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) ARGQA is built from authentic arguments across multiple domains, which makes it more representative of real-world reasoning tasks than synthetic or exam-style benchmarks.\n2) The dataset is carefully designed around three elementary logical structures (linear, convergent, divergent), each with three question types.\n3) The paper provides a detailed analysis of model performance across logical structures and domains."}, "weaknesses": {"value": "1) While the paper focuses on three elementary structures, it excludes linked structures and more complex argument forms. This limits the benchmark’s ability to assess higher-order reasoning or argumentative dynamics, which are common in real-world discourse. The exclusion is justified by practical concerns, but the dataset still falls short of capturing the full complexity of argumentation.\n2)  Authors argue that \"However, both synthetic and exam-style questions contain unnatural language, thereby limiting their applicability to real-world contexts.\" But actually,  unnatural language does not limit the applicability in the real-world context, as there are many scenarios using unnatural language, e.g., symbolic rules. Also, the dataset proposed by the authors is not more applicable than others.\n3) The use of multiple-choice questions (MCQs) simplifies evaluation but may not fully reflect reasoning depth. MCQs can be prone to guessing, surface pattern matching, or elimination strategies that do not require genuine understanding. A more open-ended format (e.g., argument reconstruction or generation) could provide a richer assessment of reasoning capabilities.\n4) The paper assumes transitivity of support relations, which may not always hold in natural language argumentation. This assumption can oversimplify the logical structure and may lead to misclassification of valid arguments or distractors. A more nuanced treatment of support would better reflect real-world reasoning."}, "questions": {"value": "In natural language, support is often contextual, defeasible, or non-transitive. Could this assumption lead to misclassification of valid arguments or distractors? Have you evaluated cases where transitivity breaks down?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dS8EKmE3fu", "forum": "O2w7hjFz1p", "replyto": "O2w7hjFz1p", "signatures": ["ICLR.cc/2026/Conference/Submission8171/Reviewer_MwTb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8171/Reviewer_MwTb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820196392, "cdate": 1761820196392, "tmdate": 1762920132780, "mdate": 1762920132780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ArgQA, a benchmark designed to evaluate logical reasoning. The authors argue that existing benchmarks are often limited because they either rely on automatic conversion of symbolic logic into natural language or use curated questions from standardized exams such as the LSAT. In contrast, ArgQA contains authentic arguments drawn from four domains and is designed to assess different reasoning structures, including linear, convergent, and divergent forms. Using this dataset, the authors evaluate the performance of Qwen and GPT-o3, concluding that both models show significant limitations in logical reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.The motivation for creating this dataset is clear and meaningful.\n\n2.The experimental analysis is broad and well designed."}, "weaknesses": {"value": "1.Figure 3 is difficult to interpret. How is the diagram constructed, and how does it relate to real-world reasoning scenarios?\n\n2.Several details in Step 2 need clarification. Do the triplets correspond to S1, S2, and the ground truth proposition? You mention that directed graphs are extracted from annotations: are these annotations provided by the source datasets or generated manually, and through what process?\n\n3.In Step 5, how do you ensure that GPT-3o actually produces distracting choices rather than arbitrary alternatives?"}, "questions": {"value": "Not a question, but the reviewer found table 1 is very similar to the table in [1], however this paper is not even been discussed or cited. \n\n[1]. Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bblr6vhTfb", "forum": "O2w7hjFz1p", "replyto": "O2w7hjFz1p", "signatures": ["ICLR.cc/2026/Conference/Submission8171/Reviewer_v3xo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8171/Reviewer_v3xo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762741222351, "cdate": 1762741222351, "tmdate": 1762920132443, "mdate": 1762920132443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}