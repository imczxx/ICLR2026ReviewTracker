{"id": "I5KWojfXxW", "number": 4572, "cdate": 1757709645214, "mdate": 1759898025913, "content": {"title": "InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation", "abstract": "Parameter-Efficient Fine-Tuning of Diffusion Transformers (DiTs) for diverse, multi-conditional tasks often suffers from task interference when using monolithic adapters like LoRA. The Mixture of Low-rank Experts (MoLE) architecture offers a modular solution, but its potential is usually limited by routing policies that operate at a token level. Such local routing can conflict with the global nature of user instructions, leading to artifacts like spatial fragmentation and semantic drift in complex image generation tasks.\nTo address these limitations, we introduce InstructMoLE, a novel framework that employs an Instruction-Guided Mixture of Low-Rank Experts. Instead of per-token routing, InstructMoLE utilizes a global routing signal, Instruction-Guided Routing (IGR), derived from the user's comprehensive instruction. This ensures that a single, coherently chosen expert council is applied uniformly across all input tokens, preserving the global semantics and structural integrity of the generation process. To complement this, we introduce an output-space orthogonality loss, which promotes expert functional diversity and mitigates representational collapse. Extensive experiments demonstrate that InstructMoLE significantly outperforms existing LoRA adapters and MoLE variants across challenging multi-conditional generation benchmarks. Our work presents a robust and generalizable framework for instruction-driven fine-tuning of generative models, enabling superior compositional control and fidelity to user intent.", "tldr": "InstructMoLE replaces flawed token-level routing in MoE models with a global policy guided by the user's instruction, achieving state-of-the-art performance on complex, multi-subject image generation and editing tasks.", "keywords": ["Mixture-of-Experts (MoE)", "Instruction-Guided Routing", "Conditional Image Generation", "Diffusion Transformers (DiTs)"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99b1115d1132a42ab119078c8c9b32d8cd9a9e7c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the efficient fine-tuning of Diffusion Transformer (DiT) models for multi-conditional image generation tasks. The authors point out that existing Mixture-of-Experts (MoE) based methods, such as MoLE, typically employ a localized, token-level routing strategy. This conflicts with the global nature of user instructions, leading to issues such as spatial incoherence and semantic drift in the generated images.\nTo address this core conflict, the paper proposes a novel framework named InstructMoLE. Its core contribution is an Instruction-Guided Routing (IGR) mechanism. Unlike per-token decision-making, IGR extracts a global routing signal from the user instruction to select a unified \"committee of experts\" for all spatial locations within the same layer of the model. This globally consistent expert selection strategy is designed to maintain the semantic and structural integrity of the generation process. Furthermore, to enhance functional diversity among experts and prevent \"expert collapse,\" the authors introduce an output-space orthogonality loss. Extensive experiments demonstrate that InstructMoLE significantly outperforms existing LoRA and MoLE variants on multiple challenging multi-conditional generation benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Novel Routing Paradigm: IGR redefines MoLE routing by tying expert selection to global instruction semantics, addressing a fundamental misalignment in prior token-level approaches. The fusion of T5 (compositional) and CLIP (holistic) embeddings via Perceiver attention is a creative combination of existing tools to solve a new problem.\n2.Targeted Regularization: Orthogonal loss fills a gap in MoE training—unlike load-balancing losses (which only ensure uniform expert usage), it directly enforces distinct expert functions, a unique design for low-rank MoE.\n3.Experimental Comprehensiveness: The paper validates performance across diverse tasks (in-context generation, multi-subject synthesis, single-image editing, spatial control) and includes ablation studies for routing policies, IGR components, and hyperparameters—leaving no critical design choice untested."}, "weaknesses": {"value": "1.Extreme Scenario Testing: No experiments on low-quality instructions (e.g., vague descriptions, typos) or ultra-complex instructions (e.g., 6+ subjects with overlapping spatial relationships). These scenarios are common in practice, and their omission limits the paper’s practical relevance.\n2. Ambiguity in the Expert Diversity Mechanism: While the orthogonality loss proves effective, the paper does not elucidate whether it functions solely as a regularizer or if it genuinely fosters functional differentiation among the experts. It remains unclear if the observed benefits stem from a general regularization effect or from the successful cultivation of specialized expert modules.\n3. Insufficient Analysis of Computational Overhead: The paper states that the output-space orthogonality loss requires computing the outputs of all experts. This implies that for calculating this loss, a forward pass through all N experts must be performed for the entire data batch, even for an MoE layer that only activates top-k experts during a standard forward pass. This approach could introduce significant computational and memory overhead, potentially negating some of the efficiency benefits of the MoE architecture. We recommend that the authors provide a concise analysis in the appendix, discussing the impact on training time and resource consumption and comparing these metrics against a baseline model trained without this loss."}, "questions": {"value": "1. On the Impact of IGR on Stochastic Diversity\n\nGiven that the Instruction-Guided Routing (IGR) mechanism selects a deterministic committee of experts based on the input prompt, does this fixed routing strategy impact the stochastic diversity inherent in a single generation instance? In other words, could it potentially reduce the random variations between different outputs generated from the same prompt and seed?\n\n2. Robustness and Dependency on the Global Signal\n\nThe model's decision-making process appears to be highly dependent on the single global signal extracted from the text instruction. Does this imply that the quality of routing is entirely contingent upon how accurately and comprehensively this global signal captures the user's intent? Could this create a robustness issue, making the model sensitive to ambiguous, noisy, or poorly formulated instructions?\n\n3. Router Behavior with Highly Complex Instructions\n\nWhen faced with exceedingly complex instructions that involve multiple attributes, is the router forced to make trade-offs? For instance, does it prioritize dominant instructions while neglecting minor details? Alternatively, does it default to selecting a generic, \"jack-of-all-trades\" committee of experts that performs sub-optimally on all aspects of the prompt?\n\n4. Potential Side Effects of Output-space Orthogonality Loss\n\nRegarding the Output-space Orthogonality Loss: could this constraint inadvertently hinder the model's ability to learn complex capabilities that require synergistic collaboration among multiple experts? By enforcing functional separation, does it risk producing images that, while accurate in individual components, may lack holistic harmony and coherence?\n\n5. Ablation Studies on the Orthogonality Loss\n\nHave ablation studies been conducted specifically on the orthogonality loss? For example, was an experiment run using only the orthogonality loss without the IGR mechanism to isolate its effect? Furthermore, is it possible that simply increasing expert capacity (e.g., adding more experts) could serve as an alternative to achieve expert diversity, thereby replacing the need for this loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9r1fiy7yJv", "forum": "I5KWojfXxW", "replyto": "I5KWojfXxW", "signatures": ["ICLR.cc/2026/Conference/Submission4572/Reviewer_qQ3z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4572/Reviewer_qQ3z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760619145059, "cdate": 1760619145059, "tmdate": 1762917448597, "mdate": 1762917448597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "InstructMoLE is a novel framework for instruction-guided fine-tuning of diffusion transformers. It replaces per-token routing in MoLE with global Instruction-Guided Routing (IGR), ensuring coherent expert selection across all tokens to preserve global semantics. An output-space orthogonality loss encourages expert diversity and prevents representational collapse. Experiments show that InstructMoLE outperforms existing LoRA and MoLE approaches on multi-conditional image generation tasks, offering robust and generalizable instruction-driven control."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Resolves the mismatch between token-level routing and the global semantic scope of instructions through InstructMoLE with Instruction-Guided Routing.\n2. Uses an output-space orthogonality regularizer to prevent expert collapse and improve compositional control.\n3. Achieves state-of-the-art performance on multi-conditional image generation, showing the superiority of global, instruction-guided routing."}, "weaknesses": {"value": "1. Lack of a deeper theoretical explanation for why global routing outperforms token-level routing.\n2. The author could improve the visual quality and aesthetics of the figures.\n3. What is the reason that Expert Race completely fails in Table 4 (Pose F1 = 0%)?\n4. The details of Perceiver Attention are not clearly specified, and the motivation for using it in this work is unclear.\n5. Scalability: How do performance and efficiency change as the number of experts N increases?\n6. Efficiency metrics such as total training cost and inference speed are not reported."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Yt5r2oSKBk", "forum": "I5KWojfXxW", "replyto": "I5KWojfXxW", "signatures": ["ICLR.cc/2026/Conference/Submission4572/Reviewer_CVZA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4572/Reviewer_CVZA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627471256, "cdate": 1761627471256, "tmdate": 1762917448306, "mdate": 1762917448306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel instruction-guided instance-level routing policy for MoLE in image editing tasks. This approach ensures that all tokens in an image share the same set of routed experts across different network layers, effectively addressing issues such as global inconsistency and semantic. Overall, the paper presents Instruction-Guided Routing (IGR) as a compelling innovation to align MoLE with global instructions, also effectively adopting existing methods such as Perceiver Attention, LoRA, and orthogonality regularization to solve information loss, computational overhead and expert collapse."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper successfully adapts the MoLE framework to DiT-based image editing tasks, specifically addressing the conflict between the global consistency requirements inherent in image editing and the limitations of token-level routing.\n2. It employs an output-space orthogonality regularization to mitigate the problem of expert collapse, and efficiently simplifies the associated computational cost by introducing the Gram matrix.\n3. The experimental setup is comprehensive, including comparisons across various image editing task types and their corresponding standard metrics, complemented by thorough ablation studies."}, "weaknesses": {"value": "1. Table 4 shows that the token-level Expert Choice (EC) method slightly outperforms IGR in the Multi-Subject task. This result is somewhat counter-intuitive, as the Multi-Subject task inherently demands the global consistency understanding that IGR is designed to provide. \n2. Some evaluation metrics rely heavily on GPT-4.1 assessments, which may introduce biases and inconsistencies. While this is common in the field, more diverse evaluation approaches or human studies would strengthen the validation.\n3. The paper focuses primarily on successful cases but provides limited analysis of scenarios where the proposed approach might fail or perform poorly."}, "questions": {"value": "1. What is the computational overhead of the Perceiver-style attention mechanism compared to standard token-level routing? \n2. How does performance change with different numbers of experts (N) and activation patterns (k)? Is there an optimal ratio, and how does this relate to task complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Oy9PowIFRK", "forum": "I5KWojfXxW", "replyto": "I5KWojfXxW", "signatures": ["ICLR.cc/2026/Conference/Submission4572/Reviewer_vXq2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4572/Reviewer_vXq2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883969686, "cdate": 1761883969686, "tmdate": 1762917447901, "mdate": 1762917447901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InstructMoLE, a Mixture of Low-Rank Experts (MoLE) framework with Instruction-Guided Routing (IGR) for multi-conditional image generation. It addresses token-level routing issues by leveraging global semantics from user instructions and introduces an orthogonality loss to ensure expert diversity. Extensive experiments show state-of-the-art performance across diverse benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Clear motivation: The paper clearly identifies the limitations of existing token-level MoE approaches and proposes an instance-level method.\n\n* Technical novelty: The introduction of orthogonal regularization specifically addresses the expert collapse problem in MoE models."}, "weaknesses": {"value": "1. Missing standard benchmarks: The evaluation did not include some widely-used T2I benchmarks such as GenEval and T2I CompBench. Including results on these benchmarks would make the comparison more comprehensive and convincing.\n\n2. Ablation study clarity: Although Table 5 presents ablation results, the signal is not very clear. The current format may give the impression that each row is an independent setting, but the description in the main text suggests that each row accumulates on the previous one. This should be made explicit in the table (e.g., using \"+\" notation or clearer legends).\n\n  Additionally, from the second row, the improvement from adding Z_global seems marginal, and in some tasks (e.g., single object), performance even drops. Could the authors analyze and explain why this happens?\nFurthermore, what are the effects of using only Z_global or only the orthogonal loss? Including these settings would give a more complete view of each component’s contribution.\n\n3. Analysis of orthogonal loss: The application of orthogonal loss is interesting and promising. However, it would be more convincing if the authors could provide a direct comparison of expert weighting distributions with and without orthogonal loss. Such visualization or quantitative analysis would better demonstrate the effectiveness of orthogonal regularization in mitigating expert collapse."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "maDzjWH9AS", "forum": "I5KWojfXxW", "replyto": "I5KWojfXxW", "signatures": ["ICLR.cc/2026/Conference/Submission4572/Reviewer_51YZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4572/Reviewer_51YZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977768783, "cdate": 1761977768783, "tmdate": 1762917447476, "mdate": 1762917447476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}