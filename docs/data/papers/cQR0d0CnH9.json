{"id": "cQR0d0CnH9", "number": 14291, "cdate": 1758232091185, "mdate": 1759897378453, "content": {"title": "Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach", "abstract": "Aligning large language models (LLMs) with human preferences usually requires fine-tuning methods such as RLHF and DPO. These methods directly optimize the model parameters, so they cannot be used in inference time to improve model performance, nor are they applicable when the model weights are not accessible. In contrast, test‑time methods sidestep weight updates by leveraging reward functions to guide and improve output quality. However, they incur high inference costs, and their one-shot guidance is often based on imperfect reward or value functions, leading to suboptimal outputs. In this work, we present a method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning (RL) framework that performs RL-style alignment of the (frozen) base model without touching its parameters. During training, each iteration {\\it (i)} samples candidates from the base model, {\\it (ii)} resamples using current value functions, and {\\it (iii)} trains a new lightweight value function that guides the next decoding pass. At inference time, the value functions are used to guide the base model generation via a search-based optimization process. We prove that under some mild conditions, IRO is a kind of policy iteration and attains the performance of Best‑of‑N (BoN) search with exponentially fewer tokens at inference time.  Experimental results demonstrate that IRO significantly improves length-controlled win rates on challenging instruction-following benchmarks, such as AlpacaEval 2.0, achieving a substantial performance boost (e.g., $30.71\\% \\to 43.80\\%$ for \\texttt{Llama-3-8B-Instruct} and $43.11\\% \\to 49.77\\%$ for $\\texttt{Llama-3-70B-Instruct}$ compared against GPT-4 responses). Further, IRO consistently outperforms SOTA inference-time alignment baselines such as BoN and weak-to-strong search, even when using much smaller value functions (of size 1B or 7B) to guide a large base model (of size 6.9B or 70B).", "tldr": "", "keywords": ["Inference-time alignment; Reinforcement Learning; RLHF"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bcb69ebf2b533a8370bae8ad75908aeb0d2a4e22.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the critical challenge of aligning Large Language Models (LLMs) when their parameters are frozen or inaccessible (e.g., API deployment). Standard alignment methods (RLHF, DPO) require weight updates, while existing inference-time methods (like Best-of-N) are often computationally expensive or provide only suboptimal, one-shot guidance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a crucial problem with high practical relevance: aligning and customizing frozen LLMs. The core idea of translating iterative RL updates into a sequence of external value functions, rather than internal weight changes, is novel and enables successive policy improvement where existing methods offer only one-shot correction.\n\nThe framework is well-motivated by established RL theory. Framing the iterative updates (Eq. 6) as successive solutions to KL-constrained optimization problems (TRPO-style) provides an elegant theoretical foundation (Sec 2.2), supported by convergence analysis (Theorem 1)."}, "weaknesses": {"value": "The paper presents IRO as an inference-time alignment method, but this framing obscures significant practical overheads.Training Overhead: IRO requires a substantial iterative training phase (Algorithm 3) to learn the sequence of value functions. This involves repeated cycles of generation from the \"frozen\" LLM, scoring, and VF training. The paper completely omits an analysis of the computational cost or time required for this pre-computation phase.Inference Overhead: At inference time, IRO requires loading and querying the entire sequence of learned value functions ($V_0, ..., V_{t-1}$) for every candidate in the beam search at every decoding step (Eq. 8). This significantly increases the memory footprint and latency. For example, using T=3 7B VFs to guide a 70B model adds 21B parameters of overhead. The paper lacks concrete measurements of inference latency and memory usage compared to baselines.\n\nThe value function training (Eq. 7) relies entirely on Monte Carlo (MC) regression, using the sparse, end-of-trajectory reward as the target for every prefix. MC methods are known to suffer from high variance in long-horizon tasks, leading to noisy value estimates and potential instability in the iterative training loop.4 The empirical observation that the $\\beta_t$ schedule needs careful adjustment for stability (L427) suggests that the optimization process is indeed sensitive. The paper does not explore lower-variance methods (e.g., GAE or TD-learning).\n\nThe theoretical framework and convergence guarantees (Sec 2.2, Sec 4.1) assume exact policy updates over the entire action space (Eq. 5). The implementation, however, relies on a localized beam search approximation (Sec 3). This introduces a significant gap; the beam search may greedily exploit inaccuracies in the value functions and fail to explore promising paths, potentially undermining the theoretical guarantees of monotonic improvement."}, "questions": {"value": "In addition to the weakness section, I have the below questions: \n\nHow does IRO handle the high variance inherent in using Monte Carlo returns for value estimation? Why were lower-variance methods like GAE or TD-learning not utilized or compared?\n\nThe paper mentions IRO is similar to Reinforcement Fine-Tuning (RFT). How does the performance and total cost of IRO compare to standard RFT applied to the same base model (assuming weights were accessible)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sH2D4tfU15", "forum": "cQR0d0CnH9", "replyto": "cQR0d0CnH9", "signatures": ["ICLR.cc/2026/Conference/Submission14291/Reviewer_57N6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14291/Reviewer_57N6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761452173184, "cdate": 1761452173184, "tmdate": 1762924738723, "mdate": 1762924738723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Iterative Reweight-then-Optimize (IRO), a novel framework for aligning large language models (LLMs) that are \"frozen,\" meaning their parameters cannot be directly updated. This work tackles the significant practical problem of how to improve or customize a deployed model (e.g., one accessed via API) using only an external outcome reward model (ORM). \n\nThe authors situate their work between two existing paradigms, highlighting the limitations of both:\n 1. training-time alignment (e.g., RLHF, DPO) is effective but static; models can't be adapted post-deployment without costly retraining\n 2. Inference-time alignment (e.g., Best-of-N, ARGS) is flexible but either computationally exorbitant (BoN requires a very large $N$) or suboptimal (relying on one-shot guidance or inaccurate token-level scores).\n\nIRO bridges this gap by proposing an RL-style iterative refinement process that does not touch the base model's weights. The core idea is to iteratively train a sequence of lightweight value functions. In the training phase, each new value function is trained on data generated by the base model guided by the previous value functions. At inference time, the base model's generation is guided by a search algorithm that reweights candidate tokens using a weighted sum of all the learned value functions.\n\nThe paper claims three primary contributions:\n1. A Novel Framework: A practical, multi-step alignment method for frozen LLMs.\n2. Theoretical Grounding: The authors prove that IRO is a form of policy iteration that converges to the optimal policy and is more token- and query-efficient than the standard Best-of-N (BoN) baseline for achieving the same performance.\n3. Strong Empirical Results: The paper demonstrates that IRO significantly outperforms strong baselines (including BoN and weak-to-strong search) on instruction-following (AlpacaEval 2.0) and summarization (TL;DR) benchmarks. Notably, it shows strong \"weak-to-strong\" capabilities, where small value functions (e.g., 1B or 7B) effectively guide much larger base models (e.g., 6.9B or 70B)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. High Significance and Novelty: The problem of aligning frozen or \"black-box\" models is extremely relevant. The core idea of using an iterative RL process to learn a sequence of value functions—effectively performing policy iteration without requiring weight updates—is a novel and powerful concept that significantly extends beyond existing one-shot inference methods.\n2. Strong Theoretical Foundation: The method is not just a heuristic. It is well-grounded in RL theory, with clear connections drawn to TRPO. The inclusion of a convergence proof (Theorem 1) and a formal cost-efficiency analysis against BoN (Proposition 1) adds significant rigor.\n3. Comprehensive Empirical Evaluation: The experimental validation is a key strength. The authors test IRO across multiple model families and scales (Pythia 1B/6.9B, Llama-3 8B/70B) on standard, challenging benchmarks (AlpacaEval, TL;DR). The inclusion of an extension to math reasoning (GSM8K) also successfully demonstrates the method's generality beyond continuous rewards.\n4. Strong Baselines and \"Weak-to-Strong\" Results: The paper compares IRO against relevant and strong baselines, including DPO, BoN-E, and Weak-to-Strong Search. The fact that IRO consistently outperforms them is impressive. The \"weak-to-strong\" results (e.g., a 7B value model guiding a 70B policy) are particularly impactful, suggesting a practical path for steering large models efficiently.\n5. Thorough Ablation Studies: The appendix provides excellent ablation studies that validate key design choices. These include the importance of the diversity-first search strategy, the method's robustness to imperfect reward models, and the impact of hyperparameters like chunk length $L$ and value function size."}, "weaknesses": {"value": "1. Training Cost Analysis: The paper heavily focuses on the inference-time efficiency gains over BoN. However, the IRO framework requires an iterative training process: $T$ iterations of generating a full dataset and training a (lightweight) value function. This \"offline\" training cost is not trivial and is not thoroughly compared against the one-time training cost of methods like DPO or the pure (but massive) inference cost of BoN with a very large $N$. A \"total compute\" comparison would make the efficiency claims more complete.\n2. Hyperparameter Sensitivity: The method introduces several new hyperparameters, including the number of iterations $T$, the $\\beta_t$ schedule, the search parameters $K$ and $B$, and the chunk length $L$. The ablations suggest that performance is quite sensitive to some of these (e.g., $\\beta_t$ in Fig. 8 and $L$ in Fig. 11). This may pose a challenge for practitioners seeking to apply IRO to new tasks, as it could require significant tuning.\n3. Inference Latency with $T$ Iterations: The inference process (Algorithm 4) requires running all $T$ (or $I$) learned value functions at each step of the beam search to compute the score (Eq. 8). While each VF is \"lightweight,\" this still means that inference latency scales linearly with the number of training iterations. This is noted in the complexity analysis (Table 1), but it is a practical limitation that could be discussed more."}, "questions": {"value": "Questions:\n\n1. Training Cost Comparison: Could you provide a more detailed analysis of the training cost of IRO? How does the total compute for $T=3$ iterations of value function training (including data generation) compare to the one-time training cost of DPO on the same dataset, or the total inference cost of BoN at a level where it achieves comparable performance (e.g., $N=64$ or $N=128$)?\n2. Choice of $\\beta_t$ Schedule: The paper uses a constant $\\beta_t=1$ for the TL;DR task but an increasing schedule ($\\beta_t = \\{1, 2, 2.5\\}$) for the UltraFeedback task. Theorem 1 also relies on an increasing $\\beta_t = \\mathcal{O}(\\sqrt{t})$. Could you elaborate on the empirical or theoretical intuition for this choice? Is an increasing schedule a form of regularization to ensure stability in later iterations, as suggested in the text?\n3. Performance Saturation: The experiments consistently show improvement from Iter 1 to Iter 3. Have you experimented with more iterations (e.g., $T > 3$)? Does performance continue to improve, or does it saturate or become unstable?\n4. Inference Latency: The inference cost scales with the number of value functions $I$ (Table 1), as all $I$ VFs must be queried. At $T=3$, this is likely manageable. But if $T$ were, for example, 10, would this become a significant latency bottleneck in practice compared to the base model's forward pass?\n\n5. Clarity on \"Diversity-First Principle\": This principle is noted as being critical for performance (Fig. 10), but it is only briefly described in the main text. Expanding this with a slightly more formal definition or a small algorithm block in the appendix would be very helpful for reproducibility.\n6. Figure 1 Clarity: Figure 1 is a bit dense. The \"Guided generation step\" and \"Value training step\" are clear, but the \"Generation in the IRO\" diagram at the bottom is complex and somewhat redundant with the top-left diagram. This could potentially be simplified to improve its role as an introductory illustration.\n7. Reward Model Quality: The paper rightly includes an ablation on imperfect reward models. It might be worth adding a concluding sentence to the main discussion (Section 5) to explicitly state that the final performance of IRO is, naturally, bounded by the quality of the ORM used for supervision, even as the method proves robust to weaker RMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fWirL2KoCx", "forum": "cQR0d0CnH9", "replyto": "cQR0d0CnH9", "signatures": ["ICLR.cc/2026/Conference/Submission14291/Reviewer_zS4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14291/Reviewer_zS4m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603242815, "cdate": 1761603242815, "tmdate": 1762924738330, "mdate": 1762924738330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel alignment algorithm for steering large language models without modifying their weights. The proposed approach, Iterative Reweight-then-Optimize (IRO), trains value functions, which are smaller than the base language model, to steer generation toward the desired reward. The proposed method brings significant improvements on the AlpacaEval 2.0 benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The exposition of the idea is generally clear.\n- Steering a large model without modifying its weight is an important problem.\n- Theoretical claims seem valid."}, "weaknesses": {"value": "- The primary concern is the computational overhead of the proposed method.\n    - IRO may require significant training compute, and, furthermore, non-trivial inference-time memory and latency overhead. Please correct me if I am wrong.\n    - For example, if we use the 7B value model and apply IRO for 3 iterations, does this mean that we need to run the 7B model three times during the inference, and the number of parameters being trained is 21B?\n- Discussion on the connection to Blockwise Best-of-N decoding (Mudgal et al., 2023), a closely related decoding method, is missing."}, "questions": {"value": "- Can you clarify the different to Blockwise Best-of-N approach? Empirical comparison will also be appreciated.\n- Can you elaborate on the computational overhead of IRO? Will there be any measure that can be taken to reduce this overhead? For example, by using LoRA?\n- From Appendix H.2.4, I found that the value function are initialized from the reward model. Is this process necessary? What if the reward model is a black box or not a neural network such that we can not initialize the value networks form the reward model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3DRejdkZrE", "forum": "cQR0d0CnH9", "replyto": "cQR0d0CnH9", "signatures": ["ICLR.cc/2026/Conference/Submission14291/Reviewer_uYrT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14291/Reviewer_uYrT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930078143, "cdate": 1761930078143, "tmdate": 1762924737925, "mdate": 1762924737925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training framework named IRO, aiming to align large language models during inference time to better adapt to human preferences while avoiding direct modification of model weights. IRO achieves multi-round policy improvements through a sequence of lightweight value models.\nThe authors provide theoretical proof at the level of convergence and complexity analysis. Experimental results suggest that this method is superior to standard inference-time alignment methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tIntroduces a novel approach for reweighting self-generated data to facilitate successive policy improvement. \n\n2.\tProvides thorough theoretical analysis, considering convergence and efficiency\n\n3.\tProvides insightful ablation studies, including β selection, chunk length, data volume, and reward model quality."}, "weaknesses": {"value": "1.\tThe article devotes a considerable amount of space to demonstrating the rationality of the method, but provides limited explanation of its operational process. \n\n2.\tThere may be overfitting or cumulative bias in multi-round iterative training of the value function, and the paper lacks quantitative analysis for this.\n\n3.\tWhile token efficiency is theoretically improved, runtime latency from multiple value evaluations and beam search is not quantified. For real-time systems, this trade-off is essential.\n\n4.\tThe method requires running multiple models simultaneously, which may not be practical in many real-world scenarios, especially for commercial models, which limits the practical application of this method."}, "questions": {"value": "1.\tIn Figure 3 and Figure 4, the experiments employ different β values across iterations. Do different β schedules lead to consistent convergence behavior across datasets, or is the performance highly sensitive to β tuning? \n\n2.\tIt is worrying that in Tables 8 and 9, the higher-quality reward models bring greater benefits in the first round of iterations, but as the number of iterations of the value model increases, the winning rate actually decreases. Could the author clarify the cause of this performance decline and whether you have considered any mechanisms to regulate subsequent iterations?\n\n3.\tIn practical deployment, how many iterations are typically required for the method to converge to optimal performance? \n\n4.\tHave the authors considered comparing their approach with other PRM-based methods to better demonstrate its effectiveness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mVqdPG9pwE", "forum": "cQR0d0CnH9", "replyto": "cQR0d0CnH9", "signatures": ["ICLR.cc/2026/Conference/Submission14291/Reviewer_wRVA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14291/Reviewer_wRVA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005666107, "cdate": 1762005666107, "tmdate": 1762924737523, "mdate": 1762924737523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}