{"id": "l9PRgLMqu9", "number": 22183, "cdate": 1758327303730, "mdate": 1763239615382, "content": {"title": "LASS-ODE: When Large Foundation Models Meet Small Unified ODE Representations", "abstract": "Foundation models have transformed language and vision through large-scale attention over discrete tokens, yet progress on continuous-time dynamical signals remains limited. A core challenge is the absence of a natural token-based representation for ODE trajectories, which evolve continuously, span multiple temporal resolutions, and are often partially observed. We introduce the Tokenized ODE Representation (TOR), which maps trajectories into latent tokens governed by local and linear Neural ODEs, leveraging their linearity for efficient scaling. To capture both temporal context and shared structure across systems, we design a hybrid attention architecture that alternates intra-system self-attention, modeling dependencies within each trajectory, and inter-system cross-attention, supported by a Dynamic ODE Hub (DOH) that serves as a shared repository for inter-system knowledge exchange. These components form LASS-ODE (LArge-Scale Small ODE), a foundation model with strong capacity for interpolation, extrapolation, probabilistic inference, and few-shot generalization across diverse ODE systems.", "tldr": "We introduce Lass ODE, a foundation model for continuous-time dynamical systems. It introduces tokenized linear ODE representations and a hybrid attention mechanism with a dynamic hub for inter-system knowledge sharing.", "keywords": ["Foundation model", "continuous-time ODE data", "interpolation", "extrapolation", "few-shot inference", "generalization"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ae63d4cddc522c327d046601139cd95debcb97cb.pdf", "supplementary_material": "/attachment/c2140b7e107b50294263c81c6d8b5f6f8db88ab8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel approach for zero-shot time series models that interpolate and extrapolate in continuous time, using multiple sequences as context. \n\nThe authors extend the common patching approach by deriving a tokenized ODE representation per patch.  After sharing information between patches and sequences, these tokens estimate a local linear neural ODE per patch in latent space. Solving the resulting piece-wise linear ODE and projecting it to data space yields a continuous-time model prediction. \n\nThe authors evaluate their method against existing zero-shot time series forecasting models, LatentODE and ContiFormer. Evaluation is performed on synthetic data of systems modeling real world phenomena."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors tackle an important issue: extending the existing zero-shot time series paradigm to continuous time (e.g. to better handle irregular grid data). \n2. The overall idea of sharing information between patches and learning local neural ODEs is interesting.  \n3. They provide a small but insightful ablation study on the architecture in Section 4.4."}, "weaknesses": {"value": "1. Major architecture *design choices* are not *explained*, or *not justified* by the empirical evaluation. For example: \n\n   a) There are *three* distinct time encodings: in equation (2), in equation (9) and in the fourier features (e.g. line 252).\n\n   b) Sharing information between sequences via the Dynamic ODE Hub is surely possible, but not an *immediate*, nor *intuitive choice*. \n\n   c) Choosing *linear neural ODEs* for each patch, instead of a more general, neural network based neural ODE. \n\n2. Empirical evaluation in Section 4. is seriously lacking. For example: \n\n   a) No evaluation on *real-world data*, just synthetic data from systems with real-world relevance. \n\n   b) No *details* about the complete train data sizes, dimensionality, or the training of baseline models LatentODE and ContiFormer. \n\n   c) No comparison against models trained on each system *individually*. An important baseline to evaluate the zero-shot model results against. \n\n3. An *important reference* (and probably baseline) is missing: \n\n    \"*Amortized Reparametrization: Efficient and Scalable Variational Inference for Latent SDEs*\" (Course, Nair; NeurIPS 2023)\n\n   Although presented for SDEs, the work is readily adapted to ODEs. It considers the piece-wise linear latent differential equations and mentions the idea of Transformer-based connection of the patches. Therefore, this work is very similar to the proposed Tokenized ODE Representation and intra-system attention and should be recognized as such. \n\n4. The authors do not *motivate their problem setup* (line 093) of pure ODE inference enough. Reframing (and training) the model as a general purpose, neural ODE based time series forecasting model should be considered."}, "questions": {"value": "1. How do you train LatentODE and ContiFormer? What architecture choices did you make for these two baselines and how many parameters do they have in total?\n2. How do you evaluate all baseline models? Do they receive the same inputs as your model, i.e. all available sequences?\n3. Why does your method require three distinct time series embeddings?\n4. What is the intuition behind the rather complex information sharing by inducing points, including additional gating functions and GRU updates? Surely, a simple MHA (without inducing points) to share information between sequences should be a much simpler idea. \n5. Do you explicitly train the extrapolation, and if so, how? By the remark on line 240, you highlight the sequence-to-sequence structure, yet you seem to be able to extrapolate 70% during evaluation (Section 4.3), just based on the last observed patch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ClFIjfmI4b", "forum": "l9PRgLMqu9", "replyto": "l9PRgLMqu9", "signatures": ["ICLR.cc/2026/Conference/Submission22183/Reviewer_HxGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22183/Reviewer_HxGY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729692129, "cdate": 1761729692129, "tmdate": 1762942106654, "mdate": 1762942106654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "f3t9IhLtnt", "forum": "l9PRgLMqu9", "replyto": "l9PRgLMqu9", "signatures": ["ICLR.cc/2026/Conference/Submission22183/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22183/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763239614572, "cdate": 1763239614572, "tmdate": 1763239614572, "mdate": 1763239614572, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LASS-ODE, a framework that bridges large-scale foundation models with continuous-time dynamical systems. The core innovation lies in the Tokenized ODE Representation (TOR), which decomposes continuous trajectories into piecewise linear latent ODE segments, preserving continuity and interpretability while reducing integration costs. Building upon this representation, the authors design a hybrid attention architecture combining intra-system self-attention and inter-system cross-attention via a Dynamic ODE Hub. Extensive experiments demonstrate that LASS-ODE outperforms existing time-series foundation models and Neural ODE-based methods in interpolation, extrapolation and zero-shot generalization across diverse ODE systems, including chaotic systems, biological networks, and power systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a Large-Scale Small philosophy that first scales down ODE dynamics into compact physics-aware tokens, then scales up via global attention for system-level integration. This design effectively unifies continuous-time modeling with foundation-style scalability.\n\n2. The combination of intra-system and inter-system attention modules with a dynamically updated ODE Hub is well-motivated and clearly described. The alternating ISA–ESA–Update mechanism effectively fuses local and global dynamics.\n\n3. Experiments cover diverse dynamical regimes—from chaotic and oscillatory systems to power grid and biological processes. Quantitative results show consistent performance gains across interpolation, extrapolation, and zero-shot settings."}, "weaknesses": {"value": "1. The paper does not provide a formal justification or error bound for the linear-token approximation used in TOR. A theoretical discussion on representational fidelity or stability would strengthen the contribution.\n\n2. Although TOR aims to reduce computational costs, the model still contains ~18 M parameters (Table 3, lines 740–755). There is no direct comparison of runtime or GPU memory usage against Latent ODE or ContiFormer, which limits claims of scalability.\n\n3. Figure 3 shows qualitative ablation results but lacks numerical values, variance, and dataset size, making it hard to assess statistical significance.\n\n4. Several datasets are custom-simulated or combined from heterogeneous sources. The paper does not specify normalization protocols, which may hinder reproducibility.\n\n5. While the related work discusses Physics-Informed Neural Networks (PINNs) and Hamiltonian NNs, these baselines are not included in experiments, weakening the empirical evaluation of “physics fidelity.”"}, "questions": {"value": "1. Can the authors provide an analysis or upper bound for the error induced by piecewise linearization in TOR compared to the underlying nonlinear dynamics?\n\n2. What is the actual computational gain (in FLOPs or wall-clock time) compared with traditional Neural ODE or ContiFormer models?\n\n3. How sensitive is LASS-ODE to the choice of token length or the number of tokens per horizon?\n\n4. Could TOR or DOH be extended to handle non-autonomous or controlled systems as mentioned in the conclusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xZZdAPiQFW", "forum": "l9PRgLMqu9", "replyto": "l9PRgLMqu9", "signatures": ["ICLR.cc/2026/Conference/Submission22183/Reviewer_EVL8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22183/Reviewer_EVL8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911602889, "cdate": 1761911602889, "tmdate": 1762942106376, "mdate": 1762942106376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presented LASS-ODE (Large-Scale Small ODE), an attention-based model designed for continuous ODE trajectories across multiple dynamical systems. LASS-ODE employees two types of attentions: intra-system attention for correlations within each system and inter-system cross attention for generalizability across systems.\nHowever, these attention mechanisms operate over discrete tokens, making it difficult to model the underlying continuous dynamics in ODEs. To address this limitation, the authors propose the Tokenized ODE Representation (TOR), a linear latent neural ODEs, that encodes local, small-scale, continuous physics inside each token. The model targets four downstream tasks: interpolation, extrapolation, probabilistic inference, and zero-shot generation across ODE systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed multiscale architecture, which integrates piecewise linear latent neural ODEs for local features with attention for global features, is innovative. This design effectively bridges discrete attention mechanisms with continuous-time system representations, enabling the model to capture both local physical evolution and global structural dependencies across systems.\n2. The approach is empirically assessed on a diverse set of ODE systems."}, "weaknesses": {"value": "1. While linear neural ODEs possess great properties, they typically face challenges when modeling multicomponent stiff nonlinear systems. Accurately representing such systems using TOR may require a significantly larger number of tokens with correspondingly smaller time segments per token. The paper would benefit from a discussion on how the proposed framework handle those scenarios and what strategies might mitigate the potential limitations. Additionally, it would be valuable to analyze how properties of the dynamics parameters A and b influence the stability and expressiveness. \n2. The experiment session could be more comprehensive. Notably, the evaluation does not include the probabilistic inference task mentioned in the problem setup. Furthermore, in the zero-shot generalization experiments, although LASS-ODE achieves lower MSE values compared to other methods, the absolute values remain high, raising concerns about the practical significance of the results. Additional clarifications on these aspects would strengthen the claims."}, "questions": {"value": "1. It would be helpful to clarify whether any stability analysis was performed for the token-specific latent ODE systems. In particular, were stability constraints considered for the latent dynamics parameters, or is stability expected to be achieved purely through learning?\n2. Could the authors comment on whether the seq-to-seq token reconstruction imposes a limit on the extrapolation horizon? It would also be useful to understand whether any studies were conducted on the time segments sizes for each token, especially the ones spanning beyond observation points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ajkyPVTnJd", "forum": "l9PRgLMqu9", "replyto": "l9PRgLMqu9", "signatures": ["ICLR.cc/2026/Conference/Submission22183/Reviewer_VNEo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22183/Reviewer_VNEo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964078880, "cdate": 1761964078880, "tmdate": 1762942106009, "mdate": 1762942106009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a latent ODE foundation model. The main ideas are to encode the systems into global embeddings, the timepoints into localised embeddings, piecewise linearise the latent trajectories, and incorporate attention between timepoints and systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The results of the paper are almost magical: the performance is dramatically better than competing methods, and the forecasting visualisations show almost magical fits. I don't really understand why the method works so well, but the results are impressive nevertheless. \n\nThe main ideas of the paper are sensible and put together well."}, "weaknesses": {"value": "The clarity of the work is ok, but could be improved. The relationships between the attentions and different embeddings could be clearer, and the fig1 is not particularly helpful.\n\nThe method is a black box, and it's difficult to see what the method learns, or why. Clearly the attention is the key part, but the paper doesn't give any insights into what the attention has learnt. \n\nThe method also feels a bit adhoc: stuff happens but not all of it is well motivated. For instance, the fourier encodings are thrown into the mix for little reason, and the ablations seem to say that they do nothing afterall."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cLPNH7FimX", "forum": "l9PRgLMqu9", "replyto": "l9PRgLMqu9", "signatures": ["ICLR.cc/2026/Conference/Submission22183/Reviewer_76yg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22183/Reviewer_76yg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762466949294, "cdate": 1762466949294, "tmdate": 1762942105289, "mdate": 1762942105289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}