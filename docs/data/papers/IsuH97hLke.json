{"id": "IsuH97hLke", "number": 3902, "cdate": 1757564284812, "mdate": 1759898063692, "content": {"title": "E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models", "abstract": "Spatial intelligence, encompassing 3D reconstruction, perception, and reasoning, is fundamental to applications such as robotics, aerial imaging, and extended reality. A key enabler is the real‑time, accurate estimation of core 3D attributes (camera parameters, point clouds, depth maps, and 3D point tracks) from unstructured or streaming imagery. Inspired by the success of large foundation models in language and 2D vision, a new class of end‑to‑end 3D geometric foundation models (GFMs) has emerged, directly predicting dense 3D representations in a single feed‑forward pass, eliminating the need for slow or unavailable precomputed camera parameters.\n\nSince late 2023, the field has exploded with diverse variants, but systematic evaluation is lacking. In this work, we present the first comprehensive benchmark for 3D GFMs, covering five core tasks: sparse‑view depth estimation, video depth estimation, 3D reconstruction, multi‑view pose estimation, novel view synthesis, and spanning both standard and challenging out‑of‑distribution datasets. Our standardized toolkit automates dataset handling, evaluation protocols, and metric computation to ensure fair, reproducible comparisons. We evaluate 17 state‑of‑the‑art GFMs, revealing their strengths and limitations across tasks and domains, and derive key insights to guide future model scaling and optimization. All code, evaluation scripts, and processed data will be publicly released to accelerate research in 3D spatial AI.", "tldr": "An emperical study of the effectiveness and efficiency of a new wave of end-to-end 3d geometric foundation models", "keywords": ["End-to-End 3D Geometric Foundation Models", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d81df640ecae7357862838c804625f84e0b318c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a benchmark for recent popular geometry foundation models, and evaluates the performance on various tasks with consistent metrics. The evaluation is very comprehensive, including indicators such as depth estimation accuracy, pose accuracy, and reconstruction accuracy, etc. After evaluating multiple methods, the authors also provide several insights to inspire further research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper provides a thorough evaluation on multiple geometry foundation models on a series of 3D related tasks, and further summarizes several key findings to help future researches.\n1.The included GFMs are rich, including pair-based, multi-view-based, image-sequence-based, and even diffusion models.\n2.The evaluation is comprehensive, including metrics such as depth estimation accuracy, pose accuracy, and reconstruction accuracy.\n3.The findings are insightful for future researches on GFMs."}, "weaknesses": {"value": "1. Training cost and deployment cost are metrics of concern for both researchers and industry. Quantitatively evaluating the relationship between performance and these costs is a potential direction for improvement in the E3D Benchmark. Training cost may include factors such as dataset collection and storage cost, GPU hours required for training, and memory consumption. Deployment cost, on the other hand, may include factors such as FLOPs and memory usage per inference.\n2. In the network architecture design section, a more in-depth analysis can be conducted for feed-forward architectures, particularly regarding how different network architectures handle multi-view images. For example, this includes the GA module in DUSt3R/MASt3R, the memory mechanisms in spann3r and cut3r, as well as the direct inference strategies adopted by Fast3R and VGGT."}, "questions": {"value": "No further questions for the authors. See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ycimfftGE6", "forum": "IsuH97hLke", "replyto": "IsuH97hLke", "signatures": ["ICLR.cc/2026/Conference/Submission3902/Reviewer_HUyc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3902/Reviewer_HUyc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727046345, "cdate": 1761727046345, "tmdate": 1762917090621, "mdate": 1762917090621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a thorough benchmark for 3D Geometric Foundation Models (GFMs), evaluating their performance across five different  tasks: sparse-view depth estimation, video depth estimation, 3D reconstruction, multi-view pose estimation, and novel view synthesis. It incorporates both conventional datasets and more challenging out-of-distribution scenarios, together with standardized evaluation protocols and metrics to ensure fair and reproducible comparisons. By analyzing 17 leading GFMs, the authors highlight each model’s strengths and limitations across diverse tasks and domains, offering valuable insights to guide future advancements in the field."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Following the seminal DUSt3R model, which introduced dense 3D representation prediction in a single feed-forward pass, numerous geometric foundation models (GFMs) have emerged, each proposing enhancements or variations of the original approach. A comprehensive and fair comparison of these methods across shared tasks, benchmark datasets, evaluation protocols, and metrics represents a highly valuable contribution to the research community.\n\n\nEvaluating the models in the context of novel view synthesis (NVS) is a valuable addition especially considering cross-domain scenarios. It is somewhat disappointing that the authors had to limit this experiment to generating a third view given two input views.  It would be interesting to additionally explore how accurate vanilla Gaussian Splatting could be built using the predicted poses and the 3D point-clouds obtained by different GFM models. To evaluate such scenarios, for instance, target images could be registered (if needed) in the new reference coordinate frame. Then starting from the registered pose, the views rendered could be compared directly to the ground-truth target images. This would offer an interesting alternative assessment of the models' capabilities in NVS."}, "weaknesses": {"value": "The paper includes a broad set of leading GFMs in its analysis, which is highly appreciated. However, it is somewhat unfortunate that MUSt3R (CVPR’25,ArXiv:2503.01661) and MV-DUSt3R+ (CVPR’25,ArXiv:2412.06974), two multi-view extensions of DUSt3R, were not considered in the study. Both have publicly released their code and demonstrated notable improvements over DUSt3R. Additionally, MV-DUSt3R+ introduces support for novel view synthesis (NVS) through lightweight prediction heads that regress 3D Gaussian attributes. \n\nThe paper is extremely dense, presenting results from a large number of experiment, which is on the one hand much appreciated. On the other hand, despite the authors’ efforts to summarize key insights, the overall presentation remains difficult to follow and, at times, somewhat monotonous. To enhance readability and better communicate the main findings, it would be beneficial to move Section 3 to the supplementary material as-is and instead expanding Section 4. This expanded section could highlight for each key observation the findings with simplified tables (by retaining only the top-performing methods globally or by showing results averaged across datasets). Additionally, it would be helpful if each original table included an average rank for each method across all metrics, making it easier to grasp overall performance at a glance. Illustrative plots like the ones in Figure 1 are particularly effective and could be used more extensively to support the narrative."}, "questions": {"value": "In the context of depth estimation, it would have been insightful to include results derived from the camera coordinate pointmap when available. It’s also important to note that for pairwise metric models such as MAST3R, global alignment does not guarantee scale preservation, which may explain the poor performance observed when evaluated without normalization. Although these models are referred to as metric due to their training with metric losses, the scale precision remains approximate (see for example Table 4 in MUSt3R paper). Consequently, the use of ground-truth intrinsics can influence depth predictions, as evidenced by the significantly improved results after normalization. For these reasons, I believe the inclusion of the bottom rows showing raw results may be misleading, and I would recommend removing them.\n\nLines 187-189: To more accurately assess how well metric scale is preserved by these methods, it would be preferable to derive depth directly from the camera coordinate pointmap. For models like MASt3R, this could be achieved by averaging the pointmaps of all image pairs in which the target image serves as the reference frame. \n\nIn Table 5, the results for MASt3R and DUSt3R appear quite similar, whereas the original MASt3R paper reports a more pronounced performance gap (see DTU results in Table 4). This discrepancy suggests that only coarse global alignment based on pointmaps was applied, without the more precise alignment using feature matching. For methods allowing matching maybe it would be interesting to also add results for refined reconstructions. \n\nIt would be interesting to also add a column to table 1 with the size of the model (number of parameters).\n\nTo improve readability and to better highlight the differences between methods, It would be helpful to  increase the size of the plots in Figure 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "prYnIXx6Dr", "forum": "IsuH97hLke", "replyto": "IsuH97hLke", "signatures": ["ICLR.cc/2026/Conference/Submission3902/Reviewer_oo1U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3902/Reviewer_oo1U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113547908, "cdate": 1762113547908, "tmdate": 1762917090340, "mdate": 1762917090340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces E3D-Bench, a comprehensive benchmark designed to evaluate end-to-end 3D Geometric Foundation Models (GFMs) across five core tasks: sparse-view depth estimation, video depth estimation, multi-view 3D reconstruction, multi-view relative pose estimation, and novel view synthesis. The authors benchmark 17 GFMs spanning prominent architectural families (feed-forward ViTs and diffusion-based models) using standardized protocols over diverse and challenging datasets, providing fair comparisons of effectiveness and efficiency. Alongside quantitative evaluation, the paper distills key trends and insights regarding model robustness, scalability, generalization, and real-time viability, supplemented by a public release of tools and data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strength:\n1. The paper is well-written and easy to follow.\n2. It benchmarks 17 recent 3D geometric foundation models (GFMs), covering both feed-forward transformer and diffusion-based architectures.\n3. The benchmark provides useful insights into how end-to-end 3D GFMs perform across multiple tasks, helping the community understand model strengths and weaknesses."}, "weaknesses": {"value": "Weaknesses\n\n1. The benchmark reuses existing datasets and introduces no new data. Therefore the main contribution is the findings provided to the community.\n\nFor the finding: There are confounding factors:\n1. When E3D-Bench come to the findings that “no single backbone is universally superior” but since those listed work's training objectives, data scales and additional modules are different. Without controlling for these factors, it is not solid to propose the finding.\n2. In 4.3 “stronger 2D feature extractors lead to substantially better performance” and cites VGGT’s superior results over Fast3R. However, VGGT and Fast3R differ in more than just the 2D backbone. VGGT is trained on a large and diverse set of around 17 datasets Fast3R uses a subset of six datasets (CO3D, ScanNet++, ARKitScenes, Habitat, BlendedMVS and MegaDepth) , so it is difficult to isolate the impact of the DINO‑based feature extractor"}, "questions": {"value": "Please see the weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "50cVkaiw3l", "forum": "IsuH97hLke", "replyto": "IsuH97hLke", "signatures": ["ICLR.cc/2026/Conference/Submission3902/Reviewer_HW5E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3902/Reviewer_HW5E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132224795, "cdate": 1762132224795, "tmdate": 1762917090103, "mdate": 1762917090103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents E3D-Bench, the first comprehensive benchmark for evaluating end-to-end 3D Geometric Foundation Models (GFMs). It addresses the rapidly growing field of GFMs that directly predict 3D geometry (depth, pose, pointmaps, tracks) from images without relying on precomputed camera parameters. The benchmark evaluates 17 recent GFMs across five core tasks: (1) Sparse-view depth estimation (2) Video depth estimation (3) Multi-view relative pose estimation (4) 3D reconstruction (sparse and dense) (5) Novel view synthesis.\nIt includes both standard datasets and challenging out-of-distribution settings (e.g., drone views, dynamic scenes, air–ground pairs). The benchmark also compares models’ efficiency (inference time & memory) to assess practicality for real-time deployment.\nThe authors provide a unified evaluation toolkit, standardized metrics, and summarize key empirical insights about task difficulty, generalization, architecture choice, and efficiency limitations. All code and processed data will be released."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) This is the first benchmark that systematically evaluates modern 3D GFMs in a unified framework across multiple tasks and data domains. It fills a clear gap in the community.\n(2) Evaluates 17 models across five different geometric tasks, including both feed-forward ViT-based and diffusion-based models. The scope is wide and well-curated.\n(3) Standardized evaluation protocols, consistent datasets, unified metrics, and fair hardware settings (A100 for all models). The commitment to releasing code/data is valuable.\n(4) Unlike many works that focus only on accuracy, this benchmark also evaluates inference latency and VRAM usage, which is crucial for robotics, AR/VR, and embedded deployment."}, "weaknesses": {"value": "(1) While the benchmark is comprehensive, the paper does not introduce new model architectures or learning paradigms. Its contribution is mainly infrastructural/empirical rather than methodological.\n(2) The paper provides extensive quantitative benchmarking but lacks deep qualitative or theoretical analysis of why certain models fail.\n(3) Although point cloud accuracy/completeness is reported, there is limited evaluation on mesh quality, surface continuity, or structural correctness.\n(4) The benchmark only compares a small set of appearance-aware GFMs and does not include strong baselines like NeRF, pixelNeRF, or Gaussian Splatting pipelines, making it difficult to position GFMs relative to classical generative reconstruction methods.\n(5) Important evaluation design decisions—such as depth scaling strategy, view selection for sparse input, or alignment methods—lack ablation or sensitivity analysis. It remains unclear how these settings influence conclusions."}, "questions": {"value": "(1) Include visual and geometric diagnostics for reconstruction errors, e.g., noisy depth edges, scale ambiguity, pose drift.\n(2) Include mesh-based metrics such as Chamfer-L1/L2, edge smoothness, manifoldness, or F-score on surfaces.\n(3) Add ablation on evaluation settings: Depth scaling strategies (median vs. least-squares), number/selection of views in sparse settings,\ninfluence of Umeyama vs. ICP for alignment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6cwepJCJ9C", "forum": "IsuH97hLke", "replyto": "IsuH97hLke", "signatures": ["ICLR.cc/2026/Conference/Submission3902/Reviewer_nKL8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3902/Reviewer_nKL8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150506441, "cdate": 1762150506441, "tmdate": 1762917089834, "mdate": 1762917089834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}