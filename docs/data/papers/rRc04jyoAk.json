{"id": "rRc04jyoAk", "number": 6206, "cdate": 1757958560499, "mdate": 1759897929918, "content": {"title": "Learning from Noisy Preferences: A Semi-Supervised Learning Approach to Direct Preference Optimization", "abstract": "Human visual preferences are inherently multi-dimensional, encompassing aspects of aesthetics, detail fidelity, and semantic alignment. However, existing open-source preference datasets provide only single, holistic annotations, resulting in severe label noise—images that excel in some dimensions (e.g., compositional) but are deficient in others (e.g., details) are simply marked as ``winner\" or ``loser\". We theoretically demonstrate that this compression of multi-dimensional preferences into binary labels generates conflicting gradient signals that misguide the optimization process in Diffusion Direct Preference Optimization (DPO). To address this label noise from conflicting multi-dimensional preferences, we propose Semi-DPO, a semi-supervised learning approach. We treat pairs with consistent preferences across all dimensions as clean labeled data, while those with conflicting signals are considered noisy unlabeled data. Our method first trains a model on a clean, consensus-filtered data subset. This model then acts as its own implicit classifier to generate pseudo-labels for the larger, noisy set, which are used to iteratively refine the model's alignment. This approach effectively mitigates label noise and enhances image generation quality, achieving better alignment with multi-dimensional human preferences. Experimental results demonstrate that Semi-DPO significantly improves alignment with multi-dimensional human preferences, achieving state-of-the-art performance without requiring additional human annotation or the need to train a dedicated reward models.", "tldr": "diffusion dpo， RLHF", "keywords": ["Diffusion Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/007e57c22aebf23bb1e9c34c65a9070aa889dfab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper is about generating T2I results that can incorporate human preferences by going beyond of collapsing the multi-dimensional preference to a single binary indicator. The paper proposed a method that by recognizing human preferences are of multi-dimensional and noisy (or in other words, emphasizing various aesthetics perspectives). The method is to divide a training set into clean and noisy subsets and start the training of a model with the clean subsets. The model then moves on to the noisy preference subset to train iteratively to align the model's generation with human preference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths of the work is the recognition of the noisy nature of human preference in assess AI-generated images. Building upon this strength is the strength of highlighting the drawback of simply classifying a generated image as winner or loser as do so would collapse the original multi-dimensional human preference to an overly too simple binary criterion. Another strength is the division of a training set to clean and noisy subsets and the design of method that iteratively trains on the noisy subset to align the model's output iteratively with human preference."}, "weaknesses": {"value": "It is not clear, given a training set, when it is divided into clean and noisy preference subsets along dimension k, does it mean for a different dimension, the clean and noisy subsets will be different.\nIt is unclear how the loss function of Eq. (8) was derived. It seems from the beginning of the paper, the iterative training only applies to the noisy preference subset, which is understandable, but then why for iterative refinement step, Eq. (8) includes L_labeled? Shouldn't Eq. (8) only need to be concerned with L_unlabeled^i for each iteration i?\nContrary to what the paper claimed in line 418, it seems there was not big improvement from iter 1 to iter 2 in ablation study. It seems that the improvement came from iter 0 to iter 1. From this perspective, it would likely suggest that iterative refinement is not necessary."}, "questions": {"value": "Please see weaknesses.\nIt would be helpful if the authors could give examples of \"noisy labels\", as well as some examples of multi-dimensional human preferences.\nDoes iter 0 mean no refinement and only the clean subset was used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jzLSNUuwix", "forum": "rRc04jyoAk", "replyto": "rRc04jyoAk", "signatures": ["ICLR.cc/2026/Conference/Submission6206/Reviewer_KSxh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6206/Reviewer_KSxh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761453898018, "cdate": 1761453898018, "tmdate": 1762918544547, "mdate": 1762918544547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper points out a key issue faced by the current image preference alignment methods based on Diffusion-DPO: human visual preferences are inherently multi-dimensional (such as composition, details, semantic alignment, aesthetics, etc.), but existing preference datasets only provide a single \"overall winner\" label, leading to significant label noise. The authors theoretically analyze that compressing such multi-dimensional preferences into binary labels will cause conflicting gradient signals, thereby hindering model optimization. To address this problem, the authors propose Semi-DPO, a new method that reconfigures preference alignment as a semi-supervised learning problem. This method first uses the consensus of multiple pre-trained reward models to filter out \"clean\" preference pairs as labeled data, and the rest are regarded as noisy unlabeled data. Then, through iterative self-training, it uses the model itself as an implicit classifier to generate fine-grained pseudo-labels at different time steps of the diffusion process, thereby decoupling conflicting signals. Experiments show that Semi-DPO significantly outperforms existing methods on multiple metrics, reaching state-of-the-art (SOTA) levels, without the need for additional manual annotations or explicit reward models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical analysis is solid: through the derivation of the lower bound of gradient variance, it rigorously proves how dimension conflicts lead to training instability, providing theoretical support for method design.\n\n2. The method design is ingenious: combining the DPO framework with semi-supervised learning, it utilizes the inherent discriminative ability of the diffusion model in DPO training to generate time-step conditional pseudo-labels without modifying the model architecture."}, "weaknesses": {"value": "1. The method relies on multiple rounds of self-training (2–3 rounds are used in the paper) and multi-reward model consensus filtering, making the training process more complex and time-consuming than standard DPO (see Appendix 6.4), which may limit its application in resource-constrained scenarios.\n\n2. The initial clean set only accounts for approximately 21% of the original data. If the consensus reward model itself has systematic biases (e.g., poor performance on certain prompt types), it may affect the quality of cold start."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CIlANfoW0n", "forum": "rRc04jyoAk", "replyto": "rRc04jyoAk", "signatures": ["ICLR.cc/2026/Conference/Submission6206/Reviewer_f4ae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6206/Reviewer_f4ae"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814349273, "cdate": 1761814349273, "tmdate": 1762918543747, "mdate": 1762918543747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors argue that compressing multi-dimensional human preferences into binary labels creates conflicting gradient signals during Diffusion-DPO training. They propose a two-stage method: 1) “Multi-Reward Consensus” filters a dataset into clean and noisy subsets based on the unanimous agreement of five pre-trained reward models, and 2) “Iterative Self-Training” uses a model trained on the clean set to generate timestep-conditional pseudo-labels for the noisy set, which are then used for further training. Experiments on Pick-a-Pic V2 show improvements over Diffusion-DPO and Diffusion-KTO baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The Semi-DPO framework is a clever and practical solution: Using a committee of diverse reward models for robust data partitioning. Leveraging the diffusion model itself as an implicit, timestep-wise preference classifier. The dynamic, timestep-conditional thresholding for pseudo-label selection, which accounts for the varying reliability of the model across the denoising process.\n2. The paper excels at identifying and formalizing a fundamental issue in preference learning for generative models. The argument that multi-dimensional preferences are a source of label noise is well-illustrated."}, "weaknesses": {"value": "1. The innovation is quite limited, and I believe the “Multi-Reward Consensus” in the first stage is very similar to CaPO. It involves using multiple rewards for evaluation and selecting samples where the winner outperforms the loser across all dimensions for learning. Additionally, the idea of phased training is somewhat similar to [5], relabel is somewhat similar to [8].\n2. There is a lack of comparison with many baselines.  like [1]SPO(SD1.5 SDXL), [2]DDIM-InPO(SD1.5 SDXL), [3]Flow-GRPO(SD3.5-Medium) [6]DSPO (SD1.5) [7] MaPO(SD1.5 SDXL). These baselines (checkpoints) are open-sourced.\n \n   [1]Aesthetic Post-Training Diffusion Models from Generic Preferences with Step-by-step Preference Optimization. CVPR2025\n\n   [2]InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment. CVPR2025\n\n   [3]Flow-GRPO: Training Flow Matching Models via Online RL\n\n   [4]Calibrated Multi-Preference Optimization for Aligning Diffusion Models. CVPR2025\n\n   [5]Curriculum Direct Preference Optimization for Diffusion and Consistency Models. CVPR2025\n\n   [6] DSPO: Direct Score Preference Optimization for Diffusion Model Alignment. ICLR2025\n\n   [7] Margin-aware Preference Optimization for Aligning Diffusion Models without Reference\n\n   [8] Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences. ICML 2025"}, "questions": {"value": "1. The SSL literature has many techniques for handling noisy labels (e.g., co-teaching, robust losses). Why was the self-training/pseudo-labeling paradigm chosen over others?\n2. Can further comparative experiments be conducted on T2ICompbench?\n3. Is the training time, memory footprint, or inference time comparable?\n4. Are there any failure modes observed?\n5. How does your model perform in controlled generation tasks, and could it lead to the loss of certain properties?\nThe final score is contingent upon the following revisions: For score 4 :supplemental baseline comparison mentioned，For score 6: Provide experimental efficiency metrics and comparative results on the T2ICompBench."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iDI8yTyrVb", "forum": "rRc04jyoAk", "replyto": "rRc04jyoAk", "signatures": ["ICLR.cc/2026/Conference/Submission6206/Reviewer_2XbV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6206/Reviewer_2XbV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893864289, "cdate": 1761893864289, "tmdate": 1762918543313, "mdate": 1762918543313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}