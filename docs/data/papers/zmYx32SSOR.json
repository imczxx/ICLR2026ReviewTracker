{"id": "zmYx32SSOR", "number": 1048, "cdate": 1756830016914, "mdate": 1759898230982, "content": {"title": "Advancing Equitable AI: A Comprehensive Framework for Individual Fairness Assessment", "abstract": "Ensuring fairness in machine learning (ML) models is essential for developing equitable and trustworthy AI systems. There has been extensive existing research on group-based fairness metrics such as the Statistical Parity Difference and Disparate Impact, but these group-based fairness metrics often fail to address fairness at the individual level. An ML model can achieve perfect group fairness, but produce discriminatory outcomes at the individual level or vice versa. In this paper, four novel individual-based fairness metrics are proposed: Proxy Dependency Score, Counterfactual Stability Rate, Attributional Independence Score, and Intra-Cohort Decision Consistency. These metrics are designed to assess different facets of individual fairness, including protected attributes’ influence on model predictions, model’s robustness to protected attribute perturbations, the independence of attributions from protected attributes, and the consistency within similar individuals. These four new individual-based metrics are empirically compared with group outcome-based fairness metrics on ML models trained on Adult and COMPAS datasets. The empirical results reveal that models deemed unfair by group metrics may exhibit individual-level fairness. Our work highlights the critical need for comprehensive individual fairness assessments in real-world applications. Our proposed framework can act as a complement to group-based evaluations towards a more complete understanding of AI fairness and the development of more equitable AI systems.", "tldr": "", "keywords": ["Social Welfare", "Justice", "Fairness and Equality", "Philosophical and Ethical Issues"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f15c77eadf982bc6dfb5a1650dfe7a3e97d689d5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors study the problem of individual fairness. To be specific, the authors criticize existing individual fairness measures for being not practical and not fine-grained, and introduce four measures. They compare these measures against commonly-used *group* fairness measures on ADULT and COMPAS datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "+ Individual fairness is a fundamental problem."}, "weaknesses": {"value": "Weaknesses:\n\n1. The paper's novelty and contributions over the literature are unclear. The paper fails to provide convincing arguments for why existing individual fairness measures are not practical or fine-grained. The authors state that: \"However, there is a lack of practical and fine-grained evaluation tools to capture different dimensions of individual fairness John & Saha (2020); Mukherjee et al. (2020); Zhang et al. (2023).\" => The provided references do not make such claims. If this claim is yours, you should justify why the existing measures are not practical or fine-grained. Therefore, it is not clear why new measures are introduced.\n\n2. There are fundamental problems with the \"individualness\" of the proposed measures and their novelties.\n\n2.1. Proxy Dependency Score utilizes accuracies. How is it then considered as a measure of individual fairness?\n\n2.2. Counterfactual Stability Rate: How is this different from existing counterfactual fairness measures?\n\n2.3. Attribution Independence Score: How about situations where a prediction should depend on whether the subject is male or female, e.g., a medical application considering menapause etc?\n\n2.4. Intra-cohort decision consistency: How is this different from existing individual fairness measures?\n\n3. Experimental evaluation is very very limited and unconvincing.\n\n3.1. It is not clear why the proposed measures are not compared against existing individual fairness measures.\n\n3.2. The evaluation should consider more diverse datasets than the ADULT and COMPAS datasets.\n\n\nMinor comments:\n\n- \"such decision-making systems Lious (2022).\" => \"such decision-making systems (Lious, 2022).\" If the reference is not part of the sentence, it should stay within parentheses. There is a way to do this in Latex (cite or citet)."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4ngXORjqdm", "forum": "zmYx32SSOR", "replyto": "zmYx32SSOR", "signatures": ["ICLR.cc/2026/Conference/Submission1048/Reviewer_VV7X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1048/Reviewer_VV7X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761203884191, "cdate": 1761203884191, "tmdate": 1762915663305, "mdate": 1762915663305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes four new fairness criteria:\n- ProxyScore, looking at the drop in accuracy when removing the sensitive attribute from the prediction model,\n- StabilityRate, quantifying the proportion of changed decisions when the sensitive attribute is flipped,\n- AIS, looking at correlation of sensitive attribute with its feature attribution,\n- IDS, looking at the variance of the prediction within clusters of similar individuals.\n\nTogether with these, ideas about how to address fairness challenges are discussed."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "(S1) The paper discusses an important topic in fair machine learning."}, "weaknesses": {"value": "(W1) The paper is clearly lacking mathematical formalism. All of the definitions are introduced through a single equation, after which some minor interpretation is provided in words. All of these statements would be much stronger if they were supported by formal propositions or theorems, which are completely absent in the paper.\n\n(W2) Relating to point (W1), in Line 138, a claim is made that “a low PDS indicates model’s minimal reliance on protected attribute proxies”. \n\nFirst, this is not true. Consider the following setting:\n\n$$\n\\\\begin{aligned}\nX &\\\\sim \\\\mathrm{Bernoulli}(0.5) \\\\\\\\\nW_i &\\\\sim \\epsilon_i + X \\quad \\text{for each } i \\in \\{1, \\dots, k\\} \\\\\\\\\nY &\\sim X + \\sum_i W_i + \\epsilon_y,\n\\\\end{aligned},\n$$\n\nwhere all $\\epsilon_i, \\epsilon_y$ are independent normal variables. Here, as the number of variables $k$ increases, we can infer $X$ almost perfectly from $W$. \n\nTherefore, regressing Y onto W (model $M’$) and Y onto (W, X) (model $M$) will have almost the same accuracy (with sufficiently many samples). However, in this case, the model $M’$ relies strongly on proxies of $X$, while model $M$ relies on $X$ itself and its proxies $W$? It is unclear whether this is in line with the description of the criterion.\n\nMore broadly, defining fairness in terms of accuracy seems unusual, and may be prone to counterexamples as the one above. \n\n(W3) The name counterfactual stability rate may be misleading; this notion is not related to counterfactual fairness [1]; in fact, under specific assumptions, this notion is related to a notion of _direct effect_ in the causal literature [2, 3]. These connections have not been drawn. Therefore, it is difficult to see if this notion presents any novelty compared to existing work.\n\n(W4) The third definition proposed, looks at the correlation of the feature attribution to the sensitive attribute S, and the sensitive attribute S itself. However, this proposal is difficult to justify — specifically, it seems to defer the problem of fairness to an explainable AI method. There is a huge literature on XAI, and there is a lack of agreement on what the best method for explanation is. Therefore, this proposal would be a lot more valuable if a specific XAI method was chosen, and if a formal result for the sentence “model primarily bases its decision on non-protected features” was provided. \n\n(W5) For the consistency metric, cohort(x) is not even defined in text. When looking at the algorithm, it seems some sort of clustering is performed first. The choice of the number of clusters etc. is not discussed properly. Therefore, this metric does not seem to be well-defined. Furthermore, comparing this notion with the existing notion of individual fairness would be quite important. Again, comparison with existing works and formal results are lacking.\n\n(W6) The algorithms pages 4,5 are taking up a lot of space; at the same time, they are performing rather simple computations. \n\n(W7) Pages 7 provides some overview of bias in high-stakes domain; pages 8-9 provide some high-level ideas about addressing issues of fairness. However, much of this discussion is in high-level terms, and does not convey a clear, actionable message.\n\n\nReferences:\n\n[1] Kusner, Matt J., Joshua R. Loftus, Chris Russell, and Ricardo Silva. “Counterfactual Fairness.” Advances in Neural Information Processing Systems 30 (2017).  \n\n[2] Di Stefano, Pietro G., James M. Hickey, and Vlasios Vasileiou. “Counterfactual Fairness: Removing Direct Effects through Regularization.” arXiv preprint arXiv:2002.10774 (2020).  \n\n[3] Plečko, Drago, and Elias Bareinboim. “Causal Fairness Analysis.” arXiv preprint arXiv:2207.11385 (2022)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3oCntKWvha", "forum": "zmYx32SSOR", "replyto": "zmYx32SSOR", "signatures": ["ICLR.cc/2026/Conference/Submission1048/Reviewer_ZbF4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1048/Reviewer_ZbF4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678720299, "cdate": 1761678720299, "tmdate": 1762915663198, "mdate": 1762915663198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes four individual ML fairness metrics to address the limitations of group-based metrics. The metrics are formally defined. Assessment algorithms are provided for each metric. Then, empirical assessment is performed on standard benchmarks. The results are analyzed to justify the usefulness of the metrics. The remaining sections are common recommendations for promoting fairness in responsible AI."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written. \nThe metric definitions are clear and intuitive."}, "weaknesses": {"value": "The proposed fairness metrics are very basic. According to me, they are too coarse-grained and do not allow an accurate assessment of bias if any. On the contrary, they can be misleading. The current description of the metrics, although clear, but very shallow. It is not enough to have a reliable assessment of their accuracy in measuring bias in practice. \nProxy Dependency Score is claimed to capture the indirect independence of the outcome to protected attributes. I disagree with that. Dropping the sensitive attribute from the model training is not enough to reveal the impact of proxy variables. The metric is useless, and even misleading.\nCounterfactul Stability Rate is using causality concepts in a very weired way. According to Algorithm 2, creating a counterfactual data point consists in simply flipping the protected attribute value. This is a naive and incorrect way for creating counterfactuals. What if there are confounder or collider variables between the protected attribute and the outcome ? This becomes misleading.\nDefinition (3) of Attribution Independence Score is not properly formalized as Corr, Protected, and even x are not properly defined. Algorithm 3 didn't help in understanding it.\nEmpirical results is clear, but again very shallow and is not enough to be conclusive about the metrics values.\nThe remaining sections (5 and 6) are completely useless as they rehash known issues about the field."}, "questions": {"value": "Why you didn't include a comparison with existing individual fairness metrics (Fairness through awarness, No Proxy Discrimination, Counterfactual fairness, etc.) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KZwdUJIbCv", "forum": "zmYx32SSOR", "replyto": "zmYx32SSOR", "signatures": ["ICLR.cc/2026/Conference/Submission1048/Reviewer_WN9V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1048/Reviewer_WN9V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729796098, "cdate": 1761729796098, "tmdate": 1762915663065, "mdate": 1762915663065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues group metrics can hide person-level unfairness and proposes four individual-fairness metrics—Proxy Dependency Score, Counterfactual Stability Rate, Attribution Independence Score, and Intra-Cohort Decision Consistency—which, on Adult and COMPAS, often reveal patterns that group metrics miss; the takeaway is to evaluate both group and individual fairness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear, the authors have made the code and formula readable.\n2. The authors made comprehensive and clear algorithms for each metrics."}, "weaknesses": {"value": "1. Motivation: Although the authors mentioned the motivation \"While group fairness provides a population-level insight, it can cause unfairness toward individuals within subgroups\",  it's not so clear as to why these proposed metrics could be addressing the problems that other fairness metrics may cause. Specifically, how do the proposed metrics compare with other individual metrics? It's of vital importance that the authors provide some intuitive/theoretical insights into how these metrics are different from the existing ones, aside from empirical studies, if they claim these metrics are complementary to the existing ones.\n2. Following on the previous one, I find it not so easy to understand why these metrics are intuitively correct. Are there simple examples explaining in what scenarios these metrics will succeed/fail (a simple counterexample)?  I am also curious about the relationships between these metrics, why are they independently proposed, how are they complementing each other? Again, I feel that the intuitive/theoretical justification is rather weak here.\n3. Experiments: I appreciate the real data analysis provided. However, these two datasets are rather limited in terms of their tasks (both are binary classification with a single sensitive attribute). Also, in recent years, there've been many studies investigating the limitations around these two well-used and debunked datasets (e.g., https://arxiv.org/abs/2108.04884 and https://arxiv.org/abs/2106.05498). It's important to evaluate these metrics in more reliable datasets and mention these limitations.\n4. Limitations: It's not so clear what the limitations of these metrics are, for example, in what task could they be used or generalized (e.g., binary classifcation/binary sensitive attributes/counterfactual fairness)? Also, it would be beneficial if the authors would include the code and running time for metrics involving non-trivial computation like this."}, "questions": {"value": "Please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MCy0q33V7H", "forum": "zmYx32SSOR", "replyto": "zmYx32SSOR", "signatures": ["ICLR.cc/2026/Conference/Submission1048/Reviewer_H3Wh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1048/Reviewer_H3Wh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918087203, "cdate": 1761918087203, "tmdate": 1762915662729, "mdate": 1762915662729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}