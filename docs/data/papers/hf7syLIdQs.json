{"id": "hf7syLIdQs", "number": 16244, "cdate": 1758262262419, "mdate": 1763577849095, "content": {"title": "How Should Corruption Be Used in SSL? Empirical Insights for Effective Pretraining", "abstract": "We study how corruption design—masking and additive noise—affects self-supervised pretraining of vision models. Although denoising diffusion models succeed in generation, noise-driven extensions of masked image modeling (MIM) achieve only marginal gains on recognition tasks, including fine-grained benchmarks. We thus investigate why this would be the case, seeking effective ways to combine masking and noising within the corruption-to-reconstruction (C2R) paradigm. We begin by analyzing prior noise-based MIM approaches, categorizing them into Substitutive Corruption (masked tokens replaced by noised ones) and Conjunctive Corruption (masked and noised tokens coexist), and further into Encoder- or Decoder-style depending on where corruption and restoration occur. Our study reveals that the literature trends toward a Decoder-style design. In contrast, we evaluate an Encoder-style alternative with a focus on transfer. Building on these analyses, we propose three principles for effective C2R pretraining: corruption and restoration should occur within the encoder, noise is most effective when injected at the feature level, and mask reconstruction and de-noising must be explicitly disentangled to avoid interference. By implementing these findings, we propose a framework that captures a broader frequency spectrum of representations and improves transferability, surpassing MIM by up to 8.1% and recent noise-driven pretraining methods by 8.0% across diverse recognition benchmarks. Code is available in the Supplementary Material.", "tldr": "We study how corruption should be used in SSL, focusing on C2R pretraining with masking and noise.", "keywords": ["Corruption", "C2R", "Self-Supervised Learning", "Pre-training", "Masked Image Modeling", "Denoising Diffusion Model"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/685d24d37250dd3c73dc6c67520758d9a0cd9127.pdf", "supplementary_material": "/attachment/7ea58b84598d0385b5f621405d6fc94b94e1988a.zip"}, "replies": [{"content": {"summary": {"value": "This paper empirically investigated the how masking and corruption should be used in visual SSL pretraining. The author proposed three principles: apply corruption/restoration in the encoder (encoder-style), inject feature-level noise early, and disentangle masked token reconstruction from de-noising. Following these principles, they built an encoder-style C2R framework using feature-space noise and a disruption loss to suppress mask–noise interference. Experiments show up to ~8% transfer gains over MIM on fine-grained tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper systematically derives a framework based on a clear definition of encoder/decoder styled and conjunctive vs substitutive corruption, which provides concrete grid of classifying different denoising and MIM based pretraining objectives. \n2. The authors provided controlled comparisons that isolate specific factors (corruption placement, noise injection stage, etc), which supports causal conclusions. \n3. The empirical findings are summarized into three clean design principles that are easy to understand."}, "weaknesses": {"value": "1. All experiments use a ViT-B backbone, pretrains only on ImageNet-1K for 400 epochs, and then fine-tunes on downstream tasks, which limits evidence that the conclusions hold at larger model scales or longer pretraining epochs (1600 epochs to have a fair comparison with result reported by DiffMAE and MAE). \n2. The motivation behind principle 1 needs to be expanded. Principle 1 favors encoder-style architecture for downstream transfer tasks and  was mainly described in section 4.1. However, the result of figure 4b only shows modest gains comparing to the decoder style, and the paper acknowledges that this alone “does not fully reveal its potential,” and immediately moved on to the principle 2 and 3. Without theoretical, empirical analysis, or reference to works that directly discusses the choices, it is unknown whether the following two principles will also help the decoder-styled C2R. Decoder-styled C2R could benefit from the efficiency (fewer token will be consumed by the encoder) and enable longer pretraining in the same computation budget. I encourage the author to provide more comprehensive investigations on the benefits of choosing encoder-style approach.\n3. An extension to weakness 2, the author should consider to show isolated gain from each principle as a separate ablation study.\n4. The paper does not discuss added compute cost using the encoder-style C2R. \n5. Appendix C discusses longer pretraining schedule but switched to FGVC as pretraining dataset. The author should consider to report on ImageNet1K in consistent with the result reported elsewhere in the paper. \n6. While I understand in this paper the author is trying to investigate how to make effective usage of denoising in SSL for vision tasks, it is not clear to me, both empirically and theoretically, why this is an important application. If the idea is to make C2R effective in both image understanding and generation tasks, the author should make this motivation clear in the paper. Otherwise, the current result is behind SOTA MIM, and showing the motivation somewhat ill-grounded."}, "questions": {"value": "1. The C2R model is pretrained with 400 epochs and the reported result for MAE is not matching up with the performance reported in MAE paper which is pretrained on 800/1600 epochs. I'm wondering whether the longer of the pretraining will also help C2R on downstream transfer. \n2. I'm wondering how quantitatively C2R performs on image generation/reconstoration tasks beyond qualitative examples presented in figure 11."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QzbhbPoPWZ", "forum": "hf7syLIdQs", "replyto": "hf7syLIdQs", "signatures": ["ICLR.cc/2026/Conference/Submission16244/Reviewer_jK7h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16244/Reviewer_jK7h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517282917, "cdate": 1761517282917, "tmdate": 1762926399016, "mdate": 1762926399016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper comprehensively studies the corruption issues used in self-supervised learning (SSL), especially for the masked image modeling (MIM) methods. Corruption is a common manipulation in SSL, instantiated as masking the input image in MIM methods. This paper analyzes the noising strategy (substitutive or conjunctive) and the location for corruption (encoder-style or decoder-style). With the analysis results, the paper proposes to perform training with the encoder style and feature-level noise, where the masked token reconstruction and denoising are disentangled."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. How to design the masking strategy is an interesting and important problem in MIM methods. This paper presents a systematic study that guides the design of the final training method based on the obtained conclusions.\n2. The proposed MIM method is evaluated on several tasks and benchmarks, presenting notable improvements."}, "weaknesses": {"value": "1. The biggest concern lies in the impact of the paper. Currently, the community primarily focuses on encoder pretraining for multimodal data and settings, such as CLIP and its successors. There also emerge other stronger pretrained encoders like DINOv3. How will this method benefit the self-supervised learning field? I recognize that the obtained conclusions can be useful for MIM methods, but MIM ones may be somewhat limited in current competitions. The main methods or baselines in this paper are from more than two years ago.\n2. Discussing the MIM methods with diffusion models may be inappropriate. Though some works (e.g. MaskDiT) do similar attempts, there indeed exist inherent differences, where sampling and (multi-step) denoising steps in diffusion models are not applicable in MIM."}, "questions": {"value": "1. An explanation about the potential impact of the paper is needed, considering MIM methods may be limited in the current SSL field.\n2. A better demonstration of the relationship between MIM and diffusion models needs to be provided. It is suggested to distinguish the two methodologies/tasks explicitly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JYZTBTZEd6", "forum": "hf7syLIdQs", "replyto": "hf7syLIdQs", "signatures": ["ICLR.cc/2026/Conference/Submission16244/Reviewer_n4jd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16244/Reviewer_n4jd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898594119, "cdate": 1761898594119, "tmdate": 1762926398677, "mdate": 1762926398677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes how to combine masking and additive noise for self-supervised pretraining in vision models. The authors introduce a unified corruption-to-reconstruction (C2R) framework that categorizes existing methods and show that encoder-style and conjunctive corruption lead to better transfer than the common decoder-style setups. Building on this analysis, the authors propose three principles: perform corruption and restoration within the encoder, add noise in feature space (early blocks), and disentangle de-masking from de-noising via a proposed disruption loss. The proposed method shows consistent gains over standard MIM and recent noise-based baselines across a range of downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The design choices are carefully motivated, each supported by clear hypotheses and empirical validation.\n- The proposed approach achieves consistent improvements over standard MIM and noise-based baselines across different downstream tasks.\n- The paper is well-written and structured, making the ideas easy to follow and the experimental results clear."}, "weaknesses": {"value": "- The main novelty seems to lie in the disentanglement (disruption) loss, as encoder-style and feature-level noise have been explored in prior works; the overall contribution feels more exploratory and incremental than conceptually new.\n- The paper lacks comparisons with some recent MIM baselines, for example, ColorMAE [A], HPM [B], and MixedAE [C], which would strengthen the empirical evaluation.\n- A comparison of the learned feature visualizations between the proposed method and baseline models would help highlight what new information or structure the encoder captures under the proposed framework.\n- The paper does not report computational cost or training time. Since the method combines masking and additive noise, an analysis of efficiency and resource requirements would be useful.\n- Main quantitative comparisons are mostly shown in figures; presenting them in a summary table would make it easier to assess the actual performance gains.\n\nMinor comments:\n- Some figures (e.g., Fig. 8) are pixelated and should be improved for clarity.\n\n[A] Carlos Hinojosa, Shuming Liu, and Bernard Ghanem. \"ColorMAE: Exploring data-independent masking strategies in Masked AutoEncoders.\" European Conference on Computer Vision (ECCV) 2024.\n\n[B] Kai Chen, et al. \"Mixed autoencoder for self-supervised visual representation learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR) 2023.\n\n[C] Haochen Wang, et al. \"Hard patches mining for masked image modeling.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023."}, "questions": {"value": "- Could the authors provide a comparison of their method with more recent MIM baselines such as ColorMAE, HPM, or MixedAE?\n- Could the authors provide comparative visualizations of the learned features to better illustrate what the encoder captures under the proposed framework?\n- What is the computational cost of the proposed approach compared to standard MAE or MIM baselines?\n- Could the authors clarify what is the final training objective and how the disruption loss is combined with others, if any?\n- Could the authors provide visualizations showing how the disruption loss actually changes the attention or affinity patterns in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Uw0qmL37Cj", "forum": "hf7syLIdQs", "replyto": "hf7syLIdQs", "signatures": ["ICLR.cc/2026/Conference/Submission16244/Reviewer_bBkf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16244/Reviewer_bBkf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983067189, "cdate": 1761983067189, "tmdate": 1762926398285, "mdate": 1762926398285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how different corruption strategies—specifically masking and additive noise—impact self-supervised pretraining of vision models within the corruption-to-reconstruction (C2R) paradigm. While denoising diffusion models have been highly successful in generative tasks, their noise-driven extensions to masked image modeling (MIM) have not yielded significant improvements for recognition tasks. The authors systematically analyze why this is the case, categorizing prior approaches into Substitutive Corruption (masked tokens replaced by noised ones) and Conjunctive Corruption (masked and noised tokens coexist), and further into Encoder- or Decoder-style frameworks depending on where corruption and restoration occur.\nThrough extensive empirical study, the paper proposes three key principles for effective C2R pretraining; Corruption and restoration should occur within the encoder (since the encoder is transferred to downstream tasks). Noise is most effective when injected at the feature level (especially in lower encoder layers).Mask reconstruction and de-noising must be explicitly disentangled to avoid interference, which is achieved by suppressing attention between masked and noised tokens.\nImplementing these principles, the authors design a new pretraining framework that captures a broader frequency spectrum of representations, leading to improved transferability. Their method outperforms standard MIM by up to 8.1% and recent noise-driven pretraining methods by 8.0% across a variety of recognition benchmarks, including fine-grained visual categorization, image classification, semantic segmentation, object detection, and instance segmentation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper provides a thorough empirical study and a clear taxonomy of corruption strategies (Substitutive vs. Conjunctive, Encoder- vs. Decoder-style), clarifying why previous noise-based C2R methods have limited effectiveness for recognition tasks.\n2) The authors distill their findings into three actionable principles for effective C2R pretraining, offering concrete guidance for the community on how to combine masking and noising for better transfer learning.\n3) By advocating for encoder-style corruption/restoration, feature-level noise injection, and explicit disentanglement of de-masking and de-noising objectives, the proposed method captures richer, more transferable representations—demonstrated both theoretically and empirically.\n4) The proposed approach achieves substantial improvements over both standard MIM and recent noise-based methods across a wide range of tasks and datasets, including challenging fine-grained recognition benchmarks. The results are robust, with statistical significance established through multiple trials."}, "weaknesses": {"value": "1) How will this method extend to the AiM-v2 method? AiM-v2 does unfined decoding for image and text, can similar ablations be shown for AiM-v2 as well?\n2) Comparison with CAN, in CAN contrastive loss resulted in good improvement in features and denoising loss also helped in representation learning. Comparison with CAN would also suggest what was the correlation between MIM loss, Contrastive loss and Denoising loss.\n3) In terms of gains, how much did it come from Disruption loss and vs other choices? We need systematic comparison of each component and where did the gains come from.\n4) The noise in different layers helps different datasets in different manners, which layer to finally apply noise to is not very clear. \n\nReferences.\n[1]Multimodal Autoregressive Pre-training of Large Vision Encoders.\n[2] A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS"}, "questions": {"value": "The implementation details for the final model are not very clear in the paper. Also there are other Contrastive MIM based methods that are not included in the paper which should be discussed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "I2qHGWkDRT", "forum": "hf7syLIdQs", "replyto": "hf7syLIdQs", "signatures": ["ICLR.cc/2026/Conference/Submission16244/Reviewer_vrdz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16244/Reviewer_vrdz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762226135044, "cdate": 1762226135044, "tmdate": 1762926397862, "mdate": 1762926397862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Paper Revision Summary"}, "comment": {"value": "We sincerely thank the Area Chair and all reviewers for their time and constructive feedback. We are encouraged that the reviewers found our empirical study thorough (vrdz, bBkf), our design choices carefully motivated by clear hypotheses (bBkf), the taxonomy and principles clear and actionable (vrdz, jK7h), our manuscript well-written and easy to follow (bBkf, jk7h), and the improvements substantial and consistent across diverse benchmarks (vrdz, bBkf, n4jd).\n\nWe would like to emphasize that **we have conducted all requested experiments and clarifications**, and have added these results in the Appendix of the revised manuscript. We note, however, that **all** of our new experiments are **in-line** with our findings, rooted from our initial detailed analysis in the original submission. We believe the new results strengthen our case, but the main substance of the paper, even revised, is the same as before, centered on the key insights we originally presented.\n\nKey revisions (all reflected in the Appendix) include:\n\n- **Appendix E, F, H**: Demonstration of generality and applicability; Extensions to Multimodal Pretraining (Appendix E), integration with Contrastive Learning (Appendix F), and complementarity with recent SOTA MIM methods (Appendix H).\n\n- **Appendix M**: Comprehensive scalability analysis with larger models and longer schedules.\n\n- **Appendix G, J**: Detailed analysis and efficiency; A systematic cumulative ablation study of each principle (Appendix G) and computational cost analysis (Appendix J).\n\n- **Appendix I, K**: Additional visualizations: Layer-wise attention maps illustrating learned features (Appendix I) and affinity matrix visualizations demonstrating the effect of the disruption loss (Appendix K).\n\n- **Appendix L, N**: Further clarifications on the relationship between MIM and Diffusion Models in our framework (Appendix L) and expanded implementation details/hyperparameter settings (Appendix N).\n\nWe believe these additions thoroughly address all reviewer concerns and significantly strengthen our manuscript. We thank the reviewers again for their engagement in improving our work."}}, "id": "cJALAi9wfR", "forum": "hf7syLIdQs", "replyto": "hf7syLIdQs", "signatures": ["ICLR.cc/2026/Conference/Submission16244/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16244/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission16244/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763579556577, "cdate": 1763579556577, "tmdate": 1763579556577, "mdate": 1763579556577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}