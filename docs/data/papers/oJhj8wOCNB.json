{"id": "oJhj8wOCNB", "number": 13245, "cdate": 1758215601553, "mdate": 1759897453059, "content": {"title": "PDMBench: A Standardized Platform for Predictive Maintenance Research", "abstract": "Predictive maintenance (PdM) is critical for industrial reliability and cost-efficiency, yet fragmented datasets, inconsistent evaluation protocols, and incompatible preprocessing pipelines hinder progress. We introduce PDMBench, a standardized and extensible platform for exploring and evaluating machine learning models on multimodal time-series data across diverse industrial settings. PDMBench integrates 14 curated datasets spanning bearings, motors, gearboxes, and multi-component systems, capturing real-world complexities such as irregular sampling, heterogeneous sensor modalities, and varying fault modes. To enable fair and reproducible comparison, we design a unified preprocessing pipeline that normalizes signal quality, extracts consistent features, and standardizes input representations, bridging the gap between models requiring handcrafted features and those operating on raw sequences. The benchmark covers two core tasks, fault classification and remaining useful life prediction, and includes 22 models ranging from traditional classifiers to cutting-edge transformers. Models are evaluated across three dimensions: prediction, uncertainty, and efficiency. The PDMBench web interface supports interactive dataset exploration, model comparison, and diagnostic analysis. Experimental results reveal no universal best model, with performance varying by dataset, task, and component type, underscoring the importance of standardized benchmarking. PDMBench enables rigorous, scalable, and interpretable research for real-world predictive maintenance by aligning data, models, and metrics in a reproducible platform.", "tldr": "", "keywords": ["Predictive maintenance", "benchmark", "evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd01d7ed3701e1a1d269e4418a2c5182b1963092.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Industry 4.0 needs a better solution for many tasks, such as RUL and fault classification. There has been a lot of work in the domain focusing on similar problems. In this paper, the author standardises the 14 datasets for building an RUL and classification problem. Overall paper need to improve on writting as well as bring some domain specific tools that has been developed in the domain for the same task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The importance of PdM and its relevant data preparation across multiple physical asset classes is interesting. \n- The preparation of data preprocessing pipeline and the various ML and DL models are collected. (the trend is in foundation model thought)"}, "weaknesses": {"value": "1. The introduction seems like a laundry list of related work. And many sections are like that. Ideally, we should use a Table to capture the key similarities and differences. This will help the reader to understand the key difference\n2. We understand there are 14 datasets being collected, but there is no single table that describes all of them in one place, for example \n    - number of rows\n    - number of columns \n    - number of failure modes\n3. There is a mismatch in the Figure and the text; the author needs to write the paper such that when we read the description, it refers to some content in the figure. Currently, many items in the figure are shown, but they were not discussed in the main text.\n4. Just collecting data and making it available in a unified format is an engineering work, and I believe almost all the research papers may have done that. For example, I can use the sktime package and write a small function to process the data, so such engineering work is not considered novel. Something that is laborious or requires special treatment, especially if novel, needs to be highlighted. \n5. Experimental results also need to be presented such that they create excitement. For example, these benchmark is created for whom? Who is the end persona? Is it an SME? Is it a DS? It is not very clear. Also, if any standardization is implemented at the API level, it needs to be discussed in the main paper. Currently, the paper's main body is entirely about discussion. \n6. Experimental observation needs to be claimed as either a novel discovery or an assertion of an existing claim. For example, is the observation about PatchTST that needs low resources, new or reported in the literature? Each and every claim made in the paper needs to be ack either novel or already known in the literature. \n7. There are many AI toolkits designed in the literature for a similar purpose, for example, AutoAI for time series forecasting, or Anomaly Detection, or Failure Pattern Analysis. Please look out for such related special-purpose toolkits designed for the Industry 4.0 domains. \n8. It is not very clear what is novel, as many papers in the literature typically use 6-8 different datasets to showcase their method. If that is the case, then the paper, in its current form, is just a few extra steps of data collection. In the case of Agentic AI, why do we not use LLM/CodeAgent to generate a solution?\n9. The chart needs to be organized such that the best performance models are easy to find."}, "questions": {"value": "Please look at all the weak points"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ORBeYVGzDh", "forum": "oJhj8wOCNB", "replyto": "oJhj8wOCNB", "signatures": ["ICLR.cc/2026/Conference/Submission13245/Reviewer_5bzq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13245/Reviewer_5bzq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517688288, "cdate": 1761517688288, "tmdate": 1762923926285, "mdate": 1762923926285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PDMBench, a benchmarking platform for evaluating machine learning models in predictive maintenance (PdM). The benchmark contains 22 time-series models and 14 datasets for 2 tasks (remaining useful life prediction and fault diagnosis)  across various fault types, sensor modalities and mechanical subsystems. The benchmark provides a unified preprocessing pipeline, a triadic evaluation framework (accuracy, uncertainty, efficiency) and an interactive web interface. The results show that no single model performs best across all settings, indicating important trade-offs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[Originality 1] The paper’s originality lies in the introduction of a standardized benchmarking platform for PdM, a domain that lacks unified evaluation protocols.\nWhile prior work has addressed dataset collection and isolated evaluations, this paper consolidates these efforts in a novel and extensible manner. \n\n[Originality 2] The inclusion of an interactive interface designed to support human understanding, is novel and particularly relevant for industrial applications such as PdM.\n\n[Quality 1] The paper has a strong selection of relevant datasets and models.\n[Clarity 1]  The paper is well-written and clearly structured around three conceptual levels (Data, ML and user) throughout.\n[Clarity 2] The results are properly communicated. The benchmark is designed with reproducibility in mind, all datasets are publicly available and an anonymized version of the code is provided.\n\n[Significance 1] The paper demonstrates that there is no universal best model and identifies key trade-offs."}, "weaknesses": {"value": "W1 – Parts of the multi-modal input and the required pre-processing steps are not fully clear to me yet.\nRelated questions: Q2, Q3, Q4, Q7, Q8, Q9\nW2 – Some aspects of the experimental procedure could be described in more detail. Related questions: Q1, Q5, Q6, Q10"}, "questions": {"value": "Q1 [line 252] Given that the benchmark includes datasets with heterogeneous sampling rates, how is temporal synchronization handled during preprocessing to ensure compatibility across models?\n\nQ2 [line 293] The paper refers to “baseline-specific preprocessing” but does not provide concrete examples. Could the authors elaborate on what these steps entail and how they influence model performance?\n\nQ3 [line 297] How is the fixed-length window size selected for each model? Is this choice based on validation performance, heuristic rules, or dataset-specific characteristics?\n\nQ4 [line 298] Could the authors provide a detailed list or representative examples of the handcrafted features used for traditional machine learning models (e.g., SVM, XGBoost)? This would help clarify the nature of the input representations and their consistency across models.\n\nQ5 [line 311] The paper states that models were selected based on their “relevance” to the PdM domain. Could the authors clarify how this relevance was assessed (e.g., based on prior usage, architectural diversity, or industrial applicability)?\n\nQ6 [line 348] What strategy was employed for hyperparameter optimization? Please specify the search method (e.g., grid, random, Bayesian), the hyperparameters tuned, and whether early stopping was used. Was tuning performed per dataset or globally?\n\nQ7 [line 940] The RUL prediction task is reformulated as a 10-class classification problem (Appendix D.2). Could the authors elaborate on the implications of this transformation? Specifically, does the labeling strategy assume that degradation begins at the first recorded measurement and progresses linearly? If so, how might this assumption affect the validity of the task formulation and introduce potential biases?\nFurthermore, after this transformation, accuracy and F1 score are used to evaluate RUL predictions. This may also introduce biases: for example, if a model predicts 100% remaining life while the ground truth is 0%, it receives the same penalty as a prediction of 10% remaining life.\n\nQ8 [line 1452] In the SHAP analysis (Appendix D.4), features are indexed (e.g., f297, f593), but their semantics are not fully explained. Do these indices correspond to raw time-series samples, frequency-domain components, or engineered features?\n\nQ9 [general] Which models in the benchmark operate directly on raw time-series inputs, and which rely on handcrafted or engineered features? A mapping of models to input types would help clarify the benchmark’s modality-agnostic claims.\n\nQ10 [general] What hardware setup was used to perform the evaluations reported in the paper? Were all models trained and evaluated under consistent computational constraints, particularly for the efficiency comparisons?\n\nQ11 [line 367] Is it possible to select different features for the feature-based models, or are the same handcrafted features always applied?\n\nQ12 [line 838] In the description of table 1, it is mentioned that there are three main tasks yet only two are listed (fault diagnosis and RUL prediction).\n\nQ13 [general] Currently, most citations are in-text, formatted as Author (Year). The paper would be more readable if citations were fully enclosed in parentheses, (Author, Year), when they are not grammatically integrated into the sentence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JWk2EhLYhp", "forum": "oJhj8wOCNB", "replyto": "oJhj8wOCNB", "signatures": ["ICLR.cc/2026/Conference/Submission13245/Reviewer_xUcH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13245/Reviewer_xUcH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826675969, "cdate": 1761826675969, "tmdate": 1762923925888, "mdate": 1762923925888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a comprehensive framework for benchmarking predictive maintenance (PdM) models across diverse industrial contexts. It integrates 14 datasets from different domains, organized around two main tasks: fault classification and remaining useful life (RUL) estimation (regression). It also includes 22 models and a set of metrics, including accuracy, calibration, and efficiency metrics. The proposed system is organized into a three-level architecture composed of data, model, and user levels, which enables consistent preprocessing, comparative model evaluation, and interactive exploration via a dashboard interface. The framework aims to promote standardization, reproducibility, and extensibility in PdM research by unifying data handling, model benchmarking, and visualization components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-organized, clearly written, and very detailed.\n- It addresses the current lack of standardization in the active research area of predictive maintenance.\n- The framework is extensible, allowing the inclusion of additional datasets, machine learning algorithms, and evaluation metrics in the future."}, "weaknesses": {"value": "- Overall, the issue of evaluation does not seem to be discussed in sufficient depth.\n- The paper does not clearly describe how hyperparameter tuning is performed for the models integrated into the framework.\n- Some ideas are repeated unnecessarily, which slightly affects conciseness."}, "questions": {"value": "- How is hyperparameter tuning performed for the models included in the framework? Is there a standardized procedure across datasets?\n- Is there any evaluation setup or consideration for timely fault classification, i.e., assessing model performance in terms of early or on-time detection of faults?\n- Some sections feel repetitive, while important implementation details are deferred to the appendix. Could the authors review the main body of text to improve readability and ensure that essential technical information is clearly presented?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dd6uADg7Ng", "forum": "oJhj8wOCNB", "replyto": "oJhj8wOCNB", "signatures": ["ICLR.cc/2026/Conference/Submission13245/Reviewer_x8B7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13245/Reviewer_x8B7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846811120, "cdate": 1761846811120, "tmdate": 1762923925459, "mdate": 1762923925459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a prototype system that aggregates 14 existing predictive maintenance (PdM) datasets, implements 22 existing time-series models, and evaluates their performance using standard metrics. It provides a web interface for result visualization. The authors claim this addresses fragmentation in PdM research by providing standardized preprocessing and evaluation protocols.\nThe authors outline their contributions as follows:\n* A curated dataset suite spanning 14 datasets across fault types, sensor modalities, and operational regimes\n* A unified toolbox for preprocessing, training, and evaluation across both handcrafted-feature and end-to-end models\n* A comprehensive evaluation framework encompassing accuracy, uncertainty (ECE, NLL, Brier Score), and efficiency (inference time, memory)\n* An interactive web interface that supports explainability, model diagnosis, and practitioner involvement"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The authors have undertaken substantial implementation work, integrating 22 diverse time-series models with consistent interfaces and running extensive experiments across 14 datasets.  The breadth of the applications covered by the datasets is notable and it includes bearings, motors, gearboxes, and multi-component systems."}, "weaknesses": {"value": "This paper is not appropriate for ICLR because it primarily presents engineering development rather than scientific research. There is no algorithmic innovation, theoretical insight, or methodological contribution that would move the field forward.The authors list four contributions: (1) dataset curation, (2) a unified toolbox, (3) comprehensive evaluation, and (4) an interactive interface. Unfortunately, none of these contributions represent a substantive advance in machine learning methodology beyond the aggregation of existing public datasets. For example, the authors do not present a novel curation framework. All their methods are reimplementations of capabilities already present in mature systems. I am not sure whether the authors are aware of tools such as MLflow that are designed to manage ML workflows and pipelines. \nFurthermore, the authors define fragmentation merely as the use of different datasets by different researchers.  This is something that is routine in engineering practice, especially in predictive maintenance. It is not a technical problem that requires new solutions. They also overlook the real fragmentation challenges addressed in prior work: irregular sampling, missing data, domain transfer across heterogeneous equipment, few-shot fault scenarios, and multi-fidelity sensor fusion. Ironically, their preprocessing pipeline removes these real-world complexities by enforcing uniform, fixed-length sequences, which makes the problem easier rather than addressing its inherent difficulties."}, "questions": {"value": "The paper in its current form is far from consideration in ICLR"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HPyDqn9Abz", "forum": "oJhj8wOCNB", "replyto": "oJhj8wOCNB", "signatures": ["ICLR.cc/2026/Conference/Submission13245/Reviewer_gza5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13245/Reviewer_gza5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032767498, "cdate": 1762032767498, "tmdate": 1762923924960, "mdate": 1762923924960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}