{"id": "1G34S0m9Sd", "number": 23385, "cdate": 1758342990868, "mdate": 1759896817676, "content": {"title": "DSPO: Stable and Efficient Policy Optimization for Agentic Search and Reasoning", "abstract": "Enhancing LLMs with the ability to actively search external knowledge is crucial for complex and real-world tasks. Current approaches either rely on prompting to elicit the model's innate agent capabilities, or suffer from performance ceilings and collapse when applying RL to complex interactive tasks, leaving their true agentic potential untapped. To address this, we introduce $\\textbf{D}$ynamic-filter $\\textbf{S}$equence-level $\\textbf{P}$olicy $\\textbf{O}$ptimization (DSPO), an improved RL algorithm designed for robust agent training through sequence-level optimization and dynamic sample filtering. We train our model purely through RL to interleave multi-turn search and reasoning, obviating the need for supervised demonstration data. Across multiple QA benchmarks, our DSPO-trained 7B model improves over a comparable previous work by \\textbf{34.1\\%}, and even outperforms the 14B model from previous work in complex multihop QA such as HotpotQA by nearly \\textbf{9\\% relative}, maintaining exceptional training stability.", "tldr": "We introduce DSPO, an improved reinforcement learning algorithm that solves the infamous instability problem when training LLM agents for search. Our model achieves exceptional results without the usual training collapse.", "keywords": ["Reinforcement Learning", "Large Language Models", "Training Stability", "AI Agents"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/021cdc6f97a3a596a58d2557d8eaa1417c6d1b91.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DSPO, an improved RL algorithm that overcomes the core instability and sample-inefficiency issues in training agentic search models. It achieves this by unifying two key principles into a single cohesive framework: sequence-level optimization for robust policy updates and dynamic outcome-based filtering for a dense and effective learning signal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1, Clear synthesis of sequence-level optimization with dynamic outcome filtering into a single, coherent algorithm tailored to agentic search with sparse terminal rewards. The sequence-level clipping with length-normalized ratio is well motivated and derived. \n2, Paper is well written and ablation studies do make sense to me."}, "weaknesses": {"value": "See questions"}, "questions": {"value": "1, Could you provide the results from different model family such as Llama3 to see how generalize the proposed method is?\n2, Could you also provide the scaled results for your proposed method on 14B models? I can get that the performance of 7B model is already good but I think it would be better to show your method scalability by using different size models from the same model family"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "12rKGHEc7v", "forum": "1G34S0m9Sd", "replyto": "1G34S0m9Sd", "signatures": ["ICLR.cc/2026/Conference/Submission23385/Reviewer_oBxZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23385/Reviewer_oBxZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761234064810, "cdate": 1761234064810, "tmdate": 1762942637176, "mdate": 1762942637176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dynamic-filter Sequence-level Policy Optimization (DSPO), a novel RL algorithm designed to train LLMs to act as stable and effective autonomous agents for search and reasoning tasks. The authors identify two critical failures in existing RL methods: training instability caused by a mismatch between token-level optimization and sequence-level rewards, and inefficient learning from sparse rewards where training batches often lack a useful learning signal. DSPO addresses these issues with two key innovations: first, it employs sequence-level policy optimization to align the training objective with the overall trajectory reward, which fundamentally stabilizes the learning process. Second, it incorporates a dynamic outcome-based filtering mechanism that ensures every training batch contains a mix of both successful and unsuccessful outcomes, guaranteeing a consistent and effective advantage signal. Through experiments, the paper demonstrates that DSPO not only prevents the catastrophic policy collapse common in other methods but also achieves state-of-the-art performance, with a 7B model outperforming even a 14B baseline model on complex question-answering benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Optimize the problems of GRPO in multi-round agent interaction scenarios\n2. Clear logic"}, "weaknesses": {"value": "1、The most significant issue with this paper lies in its experimental evaluation. The comparison is limited to the GRPO algorithm, overlooking numerous recent improvements such as DAPO and GSPO. These methods also tackle challenges related to token-level advantage calculation and gradient updates, yet the authors have not benchmarked their proposed algorithm against them.\n\n2、In the experiments, the authors only test the effectiveness of DSPO on search tasks and do not evaluate it on a broader range of tasks, such as mathematics. I am curious how DSPO would perform compared to GRPO in single-turn interaction scenarios, like those found in mathematical reasoning tasks.\n\n3、The final point concerns novelty. The second contribution proposed by the authors—filtering samples based on the reward distribution of group rollouts, such as selecting for groups with high reward variance—is very similar to existing methods. The authors need to clarify the specific innovation of their approach compared to prior work."}, "questions": {"value": "Stated in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "juG1wHnnDm", "forum": "1G34S0m9Sd", "replyto": "1G34S0m9Sd", "signatures": ["ICLR.cc/2026/Conference/Submission23385/Reviewer_Wt3z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23385/Reviewer_Wt3z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934433608, "cdate": 1761934433608, "tmdate": 1762942637004, "mdate": 1762942637004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DSPO (Dynamic-filter Sequence-level Policy Optimization), a new Reinforcement Learning (RL) algorithm designed to stably and efficiently train Large Language Models (LLMs) for tasks requiring agentic search and reasoning. The authors identify two main problems with existing methods like GRPO: Instability, where a mismatch between sequence-level rewards and token-level optimization objectives leads to high-variance gradients and policy collapse; and Inefficiency, where sparse rewards often lead to training batches where all trajectories fail or all succeed, providing no useful learning signal. DSPO tackles this by combining two key techniques into a single framework: Sequence-Level Optimization, which aligns the optimization objective with the reward unit by optimizing at the full-sequence level for a more stable gradient, and Dynamic Outcome-Based Filtering, which actively filters training batches to ensure every batch contains a mix of both successful and unsuccessful trajectories, guaranteeing an effective learning signal. The paper shows that their DSPO-trained 7B model significantly outperforms a 7B baseline (by 34.1%) and even surpasses a 14B baseline model on the complex HotpotQA benchmark, all while demonstrating superior training stability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper does an excellent job of explaining why training agentic search models with RL is difficult, pinpointing the specific issues of objective mismatch and sparse reward inefficiency.\n\n2. The performance looks good. A 7B model outperforming a 14B model on a complex multi-hop QA task (HotpotQA) is a very strong and compelling result.\n\n3. The appendix (Table A.1) provides concrete examples of the agent's learned behavior, such as recognizing irrelevant search results and reformulating queries, which shows it's learning a genuinely useful policy."}, "weaknesses": {"value": "1. The primary external baselines (GRPO and PPO) are from a single framework (Search-R1). While the authors do the right thing by re-training these models under their conditions for a fair comparison, they don't compare against a wider variety of recent RL-for-LLM algorithms. The paper builds on ideas from GSPO and DAPO, but the ablations (\"w/o filter\" and \"w/o seq-level\") act as stand-ins rather than a direct comparison against the original, fully-tuned implementations of those methods.\n\n2. The DSPO algorithm itself seems general, but its empirical validation is limited to the agentic search setting from the Search-R1 paper. It's unclear how it would perform on other complex RL tasks for LLMs, such as math reasoning or code generation, which also suffer from similar stability and reward issues.\n\n3.  All the evaluation datasets—2WikiMQA, HotpotQA, Bamboogle, MuSiQue, NQ, TriviaQA, and PopQA—are quite old, and the models have likely already been exposed to them during pretraining. Prior studies have also shown that models can often find the correct answer with only a few search turns (≤1). It would strengthen the evaluation to include more challenging and up-to-date datasets.\n\n4. The core contribution is a synthesis of two existing ideas (sequence-level optimization from GSPO and dynamic filtering from DAPO). While the paper proves their combination is synergistic and necessary, it does lower the fundamental novelty of the algorithm itself. After GSPO solved the core stability problem, adding a filtering mechanism could be seen as an incremental, though effective, step."}, "questions": {"value": "1. What is the computational and sample overhead of the dynamic filtering? What percentage of generated trajectory groups are discarded, and how does this change over the course of training?\n\n2. How does DSPO's performance (and the learned policy) change when it is paired with a state-of-the-art dense retriever instead of BM25?\n\n3. The paper shows a 7B model beating a 14B baseline. What are the results when DSPO is applied to the 14B model itself? Do the stability and performance gains scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CBjLGVrGFu", "forum": "1G34S0m9Sd", "replyto": "1G34S0m9Sd", "signatures": ["ICLR.cc/2026/Conference/Submission23385/Reviewer_fdXC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23385/Reviewer_fdXC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987651834, "cdate": 1761987651834, "tmdate": 1762942636810, "mdate": 1762942636810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DSPO (Dynamic-filter Sequence-level Policy Optimization), an RL algorithm for training agentic LLMs to interleave multi-turn search and reasoning without supervised demonstrations. DSPO combines (i) sequence-level optimization (GSPO) to align optimization with sequence-level rewards, and (ii) dynamic outcome-based filtering (DAPO) to ensure batches contain mixed success/failure trajectories, stabilizing group-relative advantages. On seven QA benchmarks with a BM25 retriever, a DSPO-trained 7B model reportedly outperforms a comparable 7B baseline by 34.1% (relative) and even exceeds a 14B baseline on HotpotQA. The paper emphasizes stability, sample efficiency, and generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper solves the problem of mismatch between sequence-level rewards and token-level optimization (GRPO/PPO), and sparse rewards leading to homogeneous groups and collapsed advantages.\n2.\tThe principle of the proposed algorithm is very clear: combining sequence-level ratios (geometric mean, length-normalized) with dynamic filtering directly targets instability and sparsity.\n3.\tThe results demonstrate notable advantage of DSPO’s performance on 7 datasets, with only BM25. 7B DSPO beats 14B baselines on HotpotQA.\n4.\tThe ablation study supports their claim: Removing dynamic filtering (GSPO) or sequence-level optimization (DAPO) degrades performance; the pattern aligns with the hypothesized failure modes."}, "weaknesses": {"value": "1.\tSome mathematical definitions and formulations are not very clear. What is the exact reference policy and KL term in the final loss. The KL regularization omits it in the final objective. The reward and environment are denoted as R. The definition of sequence and trajectory. \n2.\tAdvantage computation: Eq. 10 uses group-wise normalization with std(R)+δ for binary rewards; with small G, variance estimates are noisy. Sensitivity to G and δ is not discussed. Also, what is the exact reference policy and KL term in the final loss (Eq. 11 does not include KL, unlike Eq. 2)? The text mentions KL regularization earlier but omits it in the final objective.\n3.\tSome details are not mentioned: retrieved tokens is not detailed, the choice of  reference models, initialization checkpoint specifics and instruction format.\n4.\tHyperparameters (β for KL if used, δ, group size G, batch size B, learning rate, optimizer, entropy bonus), rollout depth limits, and early stopping criteria are not specified.\n5.\tReference model choice and freezing, as well as initialization checkpoint specifics and instruction format, are lightly described.\n6.\tThe filter rejects all-0 or all-1 groups, which changes the data distribution for policy updates. While it guarantees variance for A-^, it can bias the optimization toward decision boundaries, potentially harming calibration or robustness.\n7.\tThere is no analysis showing how filtering impacts exploration (e.g., encouraging diverse query reformulations vs. premature exploitation)."}, "questions": {"value": "1.\tThe content of “Agentic Search as a Markov Decision Process” is mainly a introduction of MDP, and should be rewritten.\n2.\tIn fig.3, what do thick curves and thin curves stand for? Why are the thin one so steep？why the green one (DSPO w/o Seq-level Opt.) only has 100 steps? Why the red one (DSPO) has a decease from step 110 to 160?\n3.\tThe baselines are only PPO, GRPO? Why DAPO is not compared? More related methods should be considered. Although seven datasets are considered, experiments seem a little insufficient. \n4.\tThe impact of δ should be discussed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aF4XBi5d5K", "forum": "1G34S0m9Sd", "replyto": "1G34S0m9Sd", "signatures": ["ICLR.cc/2026/Conference/Submission23385/Reviewer_skPz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23385/Reviewer_skPz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762624191765, "cdate": 1762624191765, "tmdate": 1762942636621, "mdate": 1762942636621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}