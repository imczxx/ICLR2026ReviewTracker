{"id": "K6IuPqU26C", "number": 7285, "cdate": 1758014278578, "mdate": 1763567384541, "content": {"title": "ELSE: An Extremely Lightweight Adapter of Simple Expression for Vision Transformer Fine Tuning", "abstract": "Inspired by Parameter-Efficient fine tuning (PEFT) methods in natural language processing, numerous efforts have been made in seeking lightweight plug-in modules to adapt Vision Transformer (ViT) to downstream applications. However, the majority of such endeavor is motivated from the architecture design point of view, while neglects the training dynamics of fine tune in terms of efficiency and stability. In contrast, this study aims to investigate how fine tune affects architecture design, and turns out a lightweight module by theoretical and experimental parsing of fine tune. As observed by us, the parameter initialization of fine tune has a significant influence on the training stability and efficiency. We notice that initializing all the parameters of Adapter to zeros can make the fine tuning start approximately from the original ViT with desired training dynamics, due to the universal representation of ViT learnt from big data. Our theoretical deduction further shows that such initialization will cause gradient vanishing of Adapter, making the large portion of it inactive,  which gives rise to the opportunity of simplifying the Adapter into an extremely lightweight equal form, that is, a single learnable vector instead of the full Adapter. To this end, an Extremely Lightweight adapter of Simple Expression (ELSE) can be approached. In the experiments, ELSE achieves superior or comparable transfer learning performance with less than 0.07% of the model’s parameters fine tuned while retaining the plug-and-play flexibility.", "tldr": "ELSE introduces a theoretically grounded, ultra-lightweight Adapter for Vision Transformers, achieving efficient and stable fine-tuning by leveraging zero-initialization and training dynamics analysis.", "keywords": ["PEFT", "Adapter", "transfer learning", "ViT", "computer vision"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73d310cb52f5f2266a0e7c65a37df91139f6bff0.pdf", "supplementary_material": "/attachment/2902142f3aacfede86ac92c0cafd473c11ff4c1c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new Parameter-Efficient Fine-Tuning (PEFT) method called ELSE (Enhanced Linear Shift Embedding). The approach introduces learnable additive vector (ELSE1 and ELSE2) after the attention and MLP layers in Vision Transformers, enabling efficient task adaptation with minimal additional parameters. The method is simple and compatible with pretrained ViTs such as MAE and MoCo. Experiments on VTAB-1K, CIFAR-100, SVHN, and Food-101 demonstrate consistent, though modest, performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of ELSE is extremely lightweight, introducing only two learnable additive vectors with almost no additional parameters or computational cost, making it highly suitable for large-scale vision tasks.\n2. The paper covers a broad range of experiments, validating the method on multiple datasets and showing consistent performance improvements, which demonstrates the generality of the approach.\n3. The paper is well-written and well-organized. The structure is logical, the language is clear, and the figures are easy to interpret. The appendix provides detailed experimental settings and hyperparameters, contributing to the overall high presentation quality."}, "weaknesses": {"value": "1. The paper has limited novelty. The proposed additive bias concept is conceptually similar to existing methods such as Bias-Tuning, LoRA (low-rank approximation), and Adapter-based bias injection. The authors do not provide sufficient justification to distinguish ELSE from these approaches in a fundamental way.\n2. The paper does not explain why the additive bias effectively facilitates task adaptation, nor does it provide feature visualizations or representational analyses to illustrate its underlying mechanism. The lack of theoretical or interpretability studies makes the method appear overly empirical.\n3. Table 7 is too wide and exceeds the typesetting scope."}, "questions": {"value": "The authors could further provide a theoretical explanation or mathematical analysis of the additive bias mechanism in the ELSE model, clarifying how it enhances feature representation and transferability. In addition, it is recommended to include visualization or attention map analyses to illustrate how the two learnable additive vectors behave under different inputs, thereby deepening the understanding of the model’s dynamic adjustment mechanism and interpretability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sh6RdX2kHW", "forum": "K6IuPqU26C", "replyto": "K6IuPqU26C", "signatures": ["ICLR.cc/2026/Conference/Submission7285/Reviewer_dizo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7285/Reviewer_dizo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653381771, "cdate": 1761653381771, "tmdate": 1762919409642, "mdate": 1762919409642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how fine-tuning affects the architecture design of Vision Transformers (ViT), focusing on training efficiency and stability. The authors find that initializing the Adapter's parameters to zero enables fine-tuning to start with the original ViT's training dynamics. This initialization causes gradient vanishing, making most of the Adapter inactive. As a result, the Adapter can be simplified into a single learnable vector, termed the Extremely Lightweight Adapter of Simple Expression (ELSE). Experiments show that ELSE achieves comparable or superior transfer learning performance while fine-tuning less than 0.07% of the model’s parameters, maintaining plug-and-play flexibility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes an extremely simple and lightweight adapter, achieving excellent performance.\n\n2. The method demonstrates comparable performance across various benchmarks."}, "weaknesses": {"value": "1. Architecturally, the proposed method is quite similar to SSF, with the key difference being the placement of a scale vector and the adapter. Therefore, ELSE can be viewed as a special case of SSF. Based on this observation, I would like to know whether the scale vector or the shift vector is more important in SSF, or if their placement plays a more significant role. I recommend a deeper analysis of this aspect.\n\n2. Although ELSE has a small number of parameters, its structure is constrained by the position of inserted neurons, which may limit its learning capacity. As the data scale increases, its performance bottleneck may become more apparent.\n\n3. Instead of adding a new bias vector, could we adjust some of the existing bias vectors in the model or fine-tune the learnable bias/shift vectors in the norm layer? How do these variants perform, and what is their stability?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rP0tWTj9vW", "forum": "K6IuPqU26C", "replyto": "K6IuPqU26C", "signatures": ["ICLR.cc/2026/Conference/Submission7285/Reviewer_zCXP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7285/Reviewer_zCXP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891110694, "cdate": 1761891110694, "tmdate": 1762919409315, "mdate": 1762919409315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ELSE, a minimal fine-tuning module for Vision Transformers. By analyzing training dynamics, the authors find that zero-initializing adapters preserves ViT representations and stabilizes training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and the presentation structures are organized suitably.\n- The discussion of zero-initialization is interesting but the novelty should be reconsidered.\n- This paper conducted extensive experiments on various downstream tasks, demonstrating its generalization."}, "weaknesses": {"value": "- In my opinion, a lot of PEFT methods are implemented with zero initialization in their open-sourced codes. The author should have this preliminary study before they conduct the study about the initialization in PEFT as if it a unwritten rule, there is not necessary to do this study.\nPlease give a table to show the initialization of all the baseline methods, such as ConvPass, NOAH, SSF, etc.\n\n- The novelty is limited. As ELSE propose to add two newly trainable vector plus with the features, it is a variant of SSF, such mean-shift design is not novel and discussed deeply in SSF, so it is more like do the same things again in this paper.\n\n- The citation style does not follow the standards of ICLR."}, "questions": {"value": "- Could you explain what is the bad effects when using the Xavier initialization as I found a little bit performance drops in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "prxjWQhuVz", "forum": "K6IuPqU26C", "replyto": "K6IuPqU26C", "signatures": ["ICLR.cc/2026/Conference/Submission7285/Reviewer_j5jV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7285/Reviewer_j5jV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975092582, "cdate": 1761975092582, "tmdate": 1762919408867, "mdate": 1762919408867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission introduces ELSE (Extremely Lightweight adapter of Simple Expression), a simplified Adapter fine-tuning method for Vision Transformers (ViTs). Motivated by an insightful analysis of the training dynamics of the popular Adapter module, this submission first observes that all-zero initialization for Adapter is beneficial for training stability, whilst causing a gradient vanishing issue. Then the authors dive into depth and point out that only the bias term of the up projection layer in the Adapter receives gradient updates in the initial training step, which causes gradient vanishing. Based on this observation, they propose to replace the entire complex Adapter structure with its simplest functional equivalent under these conditions: a single learnable vector. This vector, ELSE, is added as a bias-like term to the outputs of the MHSA and MLP modules in each Transformer block. The authors conduct extensive experiments on image classification, few-shot learning, and domain generalization, demonstrating that ELSE achieves performance comparable or superior to state-of-the-art PEFT methods while tuning a remarkably small number of parameters (< 0.07%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting and Practical Method: The initialization of different PEFT methods has explored by prior works[1-8]. Vanilla adapter[7] suggests that ``near-identity initialization’’ is a good choice. This submission first attempt to improves such initialization to all-zero initialization and then propose ELSE to further improve the training efficiency (balance between cost and performance) by analyzing the failure of all-zero initialization for Adapter.\n- Simple and Better Training Efficiency: The proposed method is simple to implement—it amounts to adding a learnable vector to the feature maps. Additionally, ELSE is lightweight, requiring only 0.06M parameters for ViT-B/16, which is significantly fewer than other competitive PEFT methods. The paper effectively highlights this advantage with the \"Performance-Per-Parameter\" (PPP*) metric, where ELSE achieves the highest score, demonstrating a superior trade-off between accuracy and parameter cost.\n- Good experimental results under Limited Data: The experimental results are comprehensive. ELSE achieves competitive or state-of-the-art performance across the 19 tasks in VTAB-1K. Its strength is evident in data-scarce scenarios. In few-shot learning (Figure 4), ELSE consistently outperforms other methods in 1-shot and 2-shot settings, supporting the hypothesis that minimal model modification is advantageous when training data is limited. Furthermore, its strong results on domain generalization tasks (Table 3) suggest that ELSE effectively adapts the model while preserving the robust, general-purpose representations learned during pre-training.\n\n[1] LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis\n\n[2] The Impact of Initialization on LoRA Finetuning Dynamics\n\n[3] $D^2LoRA$: Data-Driven LoRA Initialization for Low Resource Tasks\n\n[4] I2I: Initializing Adapters with Improvised Knowledge\n\n[5] Initialization Matters for Adversarial Transfer Learning\n\n[6] PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models\n\n[7] Parameter-Efficient Transfer Learning for NLP"}, "weaknesses": {"value": "- Further Theoretical Investigation of the failure of all-zero initialization: The gradient analysis in Section 4, which is the core motivation, is only strictly true for the very first backpropagation step. After the first update, the bias of the up-projection layer (bUp) is no longer zero, which means gradients will begin to flow to the up-projection weights (WUp) and subsequently to the rest of the Adapter. While the results of the ELSE are empirically successful, the theoretical claim that ELSE is an \"equal form\" of a zero-initialized Adapter is somewhat overclaimed without further proof. The success is likely due to ELSE being a highly effective form of regularization, rather than a direct functional equivalent.\n- Presentation Issues: 1) line 35: mean-stream -> mainstream; 2) the typeset of appendix is chaos;"}, "questions": {"value": "- Could you provide more intuition on why adding an external vector (ELSE) is substantially better than tuning the pre-existing biases within the ViT layers (as in BitFit)? Does adding the vector in a residual manner allow for a greater dynamic range of adaptation without interfering with the frozen weights' outputs?\n- The original Adapter was placed parallel to the MLP block. This submission places ELSE after both the MHSA and MLP blocks. Why do you think adding an adaptive bias after the self-attention mechanism is so effective? \n- Some works about the PEFT initialization and analysis could be considered as reference.\n\n[1] Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models\n\n[2] An Empirical Study of Parameter Efficient Fine-tuning on Vision-Language Pre-train Model\n\n[3] VL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks\n\n[4] LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis\n\n[5] The Impact of Initialization on LoRA Finetuning Dynamics\n\n[6] $D^2LoRA$: Data-Driven LoRA Initialization for Low Resource Tasks\n\n[7] I2I: Initializing Adapters with Improvised Knowledge\n\n[8] Initialization Matters for Adversarial Transfer Learning\n\n[9] PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models\n\n[10] Parameter-Efficient Transfer Learning for NLP"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ap8i33P6za", "forum": "K6IuPqU26C", "replyto": "K6IuPqU26C", "signatures": ["ICLR.cc/2026/Conference/Submission7285/Reviewer_xbHq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7285/Reviewer_xbHq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992333510, "cdate": 1761992333510, "tmdate": 1762919408375, "mdate": 1762919408375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}