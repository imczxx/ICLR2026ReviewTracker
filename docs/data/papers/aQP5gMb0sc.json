{"id": "aQP5gMb0sc", "number": 6868, "cdate": 1757998962140, "mdate": 1763619633857, "content": {"title": "Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration", "abstract": "Large language models (LLMs) are increasingly applied in diverse real-world applications, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These specifications, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we introduce SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several Test-Time Deliberation (TTD) methods, including Self-Refine, TPO, and MoreThink, show that SpecBench effectively reveals alignment gaps and that test-time deliberation improves specification alignment. Based on previous TTD methods, we further propose Align3, a lightweight method with hierarchical reflection and revision to reason over specification boundaries, advancing the safety-helpfulness trade-off frontier with minimal overhead. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries.", "tldr": "", "keywords": ["Specification Alignment", "Test-Time Deliberation", "Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/89d13bdb8ac23c64840244beea54b00596ca3901.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the specification alignment task, which aims at making LLMs adhere to dynamic, scenario-specific rules (specs) for both safety and behavior. It introduced ALIGN3, a lightweight test-time deliberation method that uses a 3-step reasoning process to navigate these boundaries. To evaluate this, the authors present SpecBench, a benchmark spanning five real-world scenarios. Experiments on 33 models demonstrate that test-time reasoning significantly boosts alignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ focus on a practical task\n+ good performance\n+ sound model architecture"}, "weaknesses": {"value": "1. evaluation bias:\n\nFor the evaluation, the authors used GPT-4.1 as the primary evaluator, while validated by a human study, which could be a potential source of bias. Models architecturally or functionally similar to GPT-4 might be unfairly favored. I recommend supplementing the LLM-based evaluation with a human-annotated gold standard, i.e., the authors can employ multiple human annotators to provide judgments and which can serve as an unbiased benchmark.\n\n\n2. lack of a systematic, generalizable approach for creating a complete set of specs for a given domain: \n\nThe methodology for creating specifications is a notable weakness, as it relies on a manual process that is difficult to scale and validate. The approach of curating specs through human-expert and LLM collaboration is vulnerable to the creators' own biases and knowledge gaps. This is especially problematic for novel domains where both the human experts' understanding may be incomplete and the LLM's training data may be lacking. Consequently, the \"completeness\" of any resulting spec set is fundamentally unknown. More critically, the paper provides no systematic framework or clear stopping criteria for practitioners to create their own comprehensive specs in a new domain. This lack of a generalizable protocol limits the reproducibility and broader application of the otherwise valuable \"specification alignment\" paradigm.\n\n3. limited addressment of dynamic and evolving specs:\n\nIn the introduction, the authors emphasized that real-world specifications are dynamic and evolve with changing user preferences and requirements. However, the proposed methods are fundamentally static, i.e.,  SpecBench is a fixed snapshot of 103 specifications. It does not provide a mechanism for testing a model's ability to adapt to new or modified specs after the initial evaluation. A model could be perfectly aligned with SpecBench's v1, but can fail completely on a later version, such as v2, where a key safety boundary has shifted. Also in ALIGN3, the TTD method required the specs to be provided at inference time. While this is more flexible than training-based methods, it does not address the core challenge of evolution. The method assumes the specs given are the correct, up-to-date set. It has no capability to reason about changes in specifications, handle conflicting rules from different versions, or identify when a spec is outdated."}, "questions": {"value": "How do you ensure you haven't missed a critical safety risk or behavioral nuance? What is the stopping criterion?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nZjRUoEv5G", "forum": "aQP5gMb0sc", "replyto": "aQP5gMb0sc", "signatures": ["ICLR.cc/2026/Conference/Submission6868/Reviewer_8wVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6868/Reviewer_8wVs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781511411, "cdate": 1761781511411, "tmdate": 1762919121953, "mdate": 1762919121953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work addresses the challenge of specification alignment for large language models (LLMs)—enabling LLMs to follow dynamic, scenario-specific safety and behavioral specifications (spec). It introduces a lightweight test-time deliberation (TTD) method called ALIGN3 and a unified benchmark SPECBENCH to advance this goal.\nSpecification alignment requires LLMs to adhere to two types of specs: safety-spec (defining scenario-specific safety boundaries, e.g., prohibiting harmful content in child storytelling) and behavioral-spec (guiding helpfulness, e.g., format constraints for code). The paper highlights a key \"safety-behavior trade-off\": strengthening one dimension often weakens the other (e.g., over-refusal ensures safety but reduces helpfulness).\nTo solve this, ALIGN3 uses three-step hierarchical TTD: 1) Behavior Optimization (maximizing compliance with behavioral-spec), 2) Safety-Guided Refinement (adjusting reasoning to avoid safety violations), and 3) Holistic Specification Audit (full spec review before final output). This minimizes token overhead while balancing safety and helpfulness.\nThe paper also presents SPECBENCH, a benchmark covering 5 scenarios , 103 specs, and 1,500 prompts. It uses the Specification Alignment Rate (SAR) metric, which prioritizes safety and rewards helpfulness for safe outputs.\nExperiments on 18 instruct and 15 reasoning models  show three key findings: 1) SPECBENCH effectively reveals alignment gaps (most models score <65% SAR); 2) TTD generally improves alignment; 3) ALIGN3 advances the safety-helpfulness trade-off—e.g., boosting Qwen3-14B’s SAR from 51.03% to 62.92% with minimal tokens, approaching GPT-4.1’s 69.20%.\nOverall, the work demonstrates test-time deliberation’s value for real-world specification alignment, providing a benchmark and method to guide scenario-specific LLM optimization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formally defines \"specification alignment\" as a critical challenge, emphasizing LLMs’ need to balance scenario-specific safety-spec (defining risk boundaries) and behavioral-spec (guiding helpfulness). This addresses a key gap in existing work that often overlooks dynamic, scenario-tailored requirements, providing a clearer framework for real-world LLM deployment.\n\n2. Innovative Lightweight TTD Method (ALIGN3): ALIGN3’s three-step hierarchical deliberation  effectively balances safety and helpfulness. Unlike multi-pass TTD baselines (e.g., Self-Refine) that incur high token overhead, ALIGN3 achieves substantial gains (e.g., 11.89% SAR boost for Qwen3-14B) with minimal tokens (~1800 per sample), making it efficient for practical use.\n\n3. SPECBENCH unifies evaluation across 5 realistic scenarios , 103 specs, and 1500 prompts. Its novel metric (SAR) prioritizes safety while rewarding helpfulness, enabling accurate measurement of the safety-helpfulness trade-off—filling a void in fragmented existing benchmarks."}, "weaknesses": {"value": "1. The paper primarily uses GPT-4.1 to calculate the Specification Alignment Rate (SAR), with Qwen3-32B-thinking as a substitute. It lacks validation with human evaluators across all scenarios—only a small-scale human study (300 samples) is conducted, and no cross-verification with other LLMs (e.g., Claude 3) is done. This may introduce evaluator bias, especially for subjective behavioral-spec compliance.\n\n2. the benchmark focuses on 5 English-language scenarios  but ignores low-resource languages or niche domains . This limits the framework’s applicability in non-English or specialized real-world contexts where scenario-specific specs differ significantly.\n\n3. While ALIGN3 shows short-term SAR gains (e.g., 11.89% for Qwen3-14B), the paper does not explore long-term performance—such as whether gains degrade with extended use, or if the method causes \"over-alignment\" ."}, "questions": {"value": "1. Your attack enhancement uses WildTeaming to rewrite unsafe prompts, but only verifies semantic preservation with Qwen3-32B-thinking and human review. Have you tested if the attacked prompts still effectively trigger model safety violations across more diverse models (e.g., closed-source ones like Claude 3)? And how do you ensure the attack tactics don’t introduce scenario-specific biases (e.g., overusing \"novel writing\" in the Child scenario)?  \n\n\n2. Qwen3-32B-thinking is proposed as a cost-effective evaluator alternative to GPT-4.1, but its scores are slightly lower. For scenarios with strict safety requirements (e.g., Biochem’s dual-use risks), have you validated if this score gap leads to misjudgments of critical safety violations?  \n\n\n3. ALIGN3’s three-step TTD improves SAR, but ablation shows omitting any step reduces performance. Have you explored why combining Step 1 (behavior optimization) and Step 2 (safety refinement) comes closest to full ALIGN3, and if adjusting the order of steps (e.g., safety first) would yield better results?  \n\n4. Most models score <65% SAR, with GPT-5-chat leading. Have you analyzed whether GPT-5-chat’s advantage stems from better handling of specific scenario types (e.g., Code’s vulnerability constraints) or a general edge in balancing safety-behavior trade-offs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B1f3iubQqB", "forum": "aQP5gMb0sc", "replyto": "aQP5gMb0sc", "signatures": ["ICLR.cc/2026/Conference/Submission6868/Reviewer_vrxC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6868/Reviewer_vrxC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917823906, "cdate": 1761917823906, "tmdate": 1762919121481, "mdate": 1762919121481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose to tackle the timely issue of specification alignment in LLMs, especially across a large variety of fields. First, as a comprehensive dataset seems to be missing in this area, they introduce SpecBench, a set of scenarios, specifications, and well-curated prompts, mixing synthetic and real-world examples as well as unsafe and safe prompts. This benchmark is used to identify and measure gaps in the specification alignment abilities of different LLMs. To quantify performance on this new benchmark, they propose the Specification Alignment Rate (SAR) as a new metric. Second, they introduce a new test-time approach for reasoning models, Align3, that significantly improves specification alignment without the need for additional fine-tuning through a \"Propose-Verify-Refine\" deliberation loop."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe construction of SpecBench is a notable contribution of the paper, providing a much-needed, structured benchmark for evaluating the nuanced task of specification alignment. Its inclusion of diverse scenarios, both synthetic and real-world, and its categorization of specifications (e.g., behavioral, stylistic, safety) make it a valuable resource for the community.\n-\tThe proposed Align3 algorithm is simple, intuitive, and shows significant gain in performance. A key advantage is that it is a test-time method, meaning it can improve the alignment of existing, pre-trained models without requiring costly retraining or fine-tuning."}, "weaknesses": {"value": "-\tThe novelty of the deliberation mechanism could be better situated within the existing literature on self-correction and iterative refinement. While the Propose-Verify-Refine framework is clean, the core idea of using an LLM to critique and improve its own output has been explored in prior work (e.g., Self-Refine), and the paper could benefit from a clearer differentiation.\n\n- The Align3 approach, by its nature, may introduce significant computational and latency overhead. It requires at least three full model inferences (propose, verify, refine) per prompt, which may make it impractical for real-time applications compared to single-pass generation. The trade-off between alignment performance and inference cost is a key consideration that we think could be discussed more."}, "questions": {"value": "1.\tIn the introduction, you state “SPECBENCH effectively reveals alignment gaps;” could you clarify whether this refers to gaps in performance between different models, or gaps between a model's high performance on general benchmarks (like MMLU) and its lower performance on specificational alignment tasks?\n2.\tThe Align3 process seems to treat all specifications with equal importance. Have you explored weighting specifications based on priority, or a mechanism for handling potentially conflicting specifications (e.g., \"be concise\" vs. \"include all these details\")? Perhaps using weight importance for different safety/behaviour instead of a “all-or-none” approach for safety properties and “uniform” importance for behaviour specifications ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lgEpBCxZGx", "forum": "aQP5gMb0sc", "replyto": "aQP5gMb0sc", "signatures": ["ICLR.cc/2026/Conference/Submission6868/Reviewer_hPxS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6868/Reviewer_hPxS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929696250, "cdate": 1761929696250, "tmdate": 1762919120982, "mdate": 1762919120982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Align3, a lightweight test-time deliberation method with three phases: behavior-oriented phase for maximizing helpfulness, safety-guided phase to remove risky content, and a spec audit before producing the final answer. To evaluate methods, it also releases SpecBench, a benchmark including 5 application domains with paired safety and behavioral specs per task, with a combined metric SAR that treats safety as a hard constraint and then scores usefulness. Experiments across different LLMs show Align3 advances the safety–usefulness frontier with small token overhead and competitive results versus other TTD baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied problem is interesting. This paper has clear problem formulation.\n\n2. This paper is well-written.\n\n3. The constructed benchmark is useful to the community."}, "weaknesses": {"value": "1. The proposed method needs a largely fixed prompt template/workflow, but prompt structure, and workflow can affect model benign performance; moreover, prompt optimization is an active research area (e.g., automated prompt search/rewriting, domain-specific templates). How Align3 would integrate with such optimization is not discussed in detail. Also, the evaluation of how the proposed method could influence the benign performance of the model is limited to the SpecBench. Broader results on more mainstream benchmarks would help quantify Align3’s effect on non-safety performance.\n\n2. The proposed method is simply prompt based workflow. Thus, the contribution and the novelty of it could be limited.\n\n3. Heavy reliance on LLM judge leaves room for evaluator bias, potentially reducing the quality of the benchmark comparing to the benchmark with ground-truth or verifiable answers.\n\n4. The failure analysis of Align3 and existing methods are not comprehensive enough. More fine-grained analyses and taxonomy of failure modes would clarify when the method breaks and why."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7LfvBzhVVi", "forum": "aQP5gMb0sc", "replyto": "aQP5gMb0sc", "signatures": ["ICLR.cc/2026/Conference/Submission6868/Reviewer_dFuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6868/Reviewer_dFuL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040788657, "cdate": 1762040788657, "tmdate": 1762919119012, "mdate": 1762919119012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}