{"id": "iKJbmx1iuQ", "number": 5368, "cdate": 1757904640124, "mdate": 1763571144525, "content": {"title": "Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations", "abstract": "Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce **C**ontractive **D**iffusion **P**olicies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.", "tldr": "We improve the performance of diffusion policies for offline learning by promoting contraction in the diffusion sampling process.", "keywords": ["Diffusion Policy", "Contraction Theory", "Robot learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a84a1c6208b881acb2bf5a2ee1da7a581c343410.pdf", "supplementary_material": "/attachment/0f2279bed5335265c883f5251022832a67833388.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies a weakness in modern diffusion policies. The authors argue that the iterative nature of the diffusion sampling process introduces compounding errors from two sources 1) inaccuracy of the learned score function, especially in low-data cases, and 2) discretization errors from the ODE solver. In control and robotics, these small and accumulating errors can lead to failed actions.\n\nTo solve this, the authors introduce Contrastive Diffusion Policy (CDPs). The core idea is to leverage contraction theory to make the reverse diffusion ODE more stable. The contraction theory establishes the relation between the ODE's Jacobian and the ODE. If this condition is satisfied, it can guarantee that the ODE trajectories converge to a more concentrated target distribution. This property makes the learned ODE more concentrated and the sampling process inherently robust to small perturbations like solver or model errors. \n\nThis paper leverages the theoretical analysis of contraction theory for ODE and modifies the original diffusion or flow policy by forcing the ODE's Jacobian to satisfy the contrastive drifting condition. This is learned along with the original flow objective. For the training details, they use the power iteration method to approximate the largest eigenvalue of the score Jacobian. Then a contraction loss is applied to penalize the model to be more concentrated. The final training loss is a simple weighted sum of the original diffusion loss and the new contraction loss. \n\nExperiments on D4RL, Robomimic, and real-world Franka robot tasks demonstrate that CDPs outperform the standard diffusion policy baseline, with particularly strong performance in low-data scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper leverages a solid foundation in contraction theory and gives a formal analysis of the ODE's stability, which is insightful and provides a clear, principled target for regularization\n- The paper has a good motivation. Compounding error is a valid issue in continuous control\n- The paper includes physical robot experiments in the real world, which demonstrates its robustness in practice"}, "weaknesses": {"value": "- The paper describes the added cost as \"minimal\" and \"negligible.\" However, the proposed method requires computing a Jacobian-vector product (K=3 or 4 times for the power iteration) for every item in the batch at every single training step. This is definitely more computationally expensive than a standard diffusion loss. A more transparent analysis (e.g., a wall-clock time comparison) would be helpful to quantify this overhead.\n- The advantage of diffusion models is to capture complex, multi-modal distributions (like the different ways to perform a task). Forcing the system to be \"contractive\" (pulling trajectories together) seems intuitively to contradict preserving this diversity. The paper mentions that \"excessive penalization could fuel a mode collapse,\" but does not investigate this trade-off in-depth. A quantitative study on how $\\gamma$ affects the variance or modality of the final action distribution would make the paper more complete, especially on a task showing multi-modal behavior.\n- The authors acknowledge this limitation of hyperparameter sensitivity. The new contraction loss weight, $\\gamma$, is a critical hyperparameter. As shown in Table 4, the optimal value for $\\gamma$ varies dramatically across different environments (from 0.001 to 100.0). This high sensitivity could make the method difficult to apply to new tasks without a costly hyperparameter sweep"}, "questions": {"value": "In addition to the questions in the weakness above, I have the following additional questions \n\n- Regarding Stochastic (SDE) Solvers: The paper focuses on stabilizing the deterministic ODE sampling process (and the experiments appear to use the deterministic ODE sampler). However, stochastic samplers (like DDPM) are well-known to have error-correcting properties due to their Langevin noise term, which also tends to concentrate the final distribution.\nCould the authors comment on why an ODE-based solver was chosen over an SDE-based one?\nDid the authors experiment with stochastic samplers? If so, how do they perform? Does the inherent noise of an SDE sampler already provide a similar \"contractive\" effect, making the explicit contraction loss less necessary?\n\n- Regarding Simpler Alternatives (e.g., Data Re-weighting): As an alternative to regularizing the model, could a similar \"concentrating\" effect be achieved through the training data or loss function? For example, one could identify actions that are \"central\" or \"high-quality\" (e.g., near the mean of a mode in the data) and apply a higher loss weight to these samples during training. This would intuitively \"tilt\" the learned score function to flow towards these more robust actions, without the computational cost of Jacobian-vector products. Could the authors comment on the pros and cons of their proposed method versus such a data re-weighting or distribution tilting approach?\n\n- In Table 2 (Robomimic), the proposed CDP (0.78 avg) underperforms the DP-Unet baseline (0.88 avg) significantly. Could the author provide additional results with CDP + Unet to show the improvements over different architectures?\n\n- I'm not quite sure why the method performs better for low-data regimes. if the score field ε_θ is inaccurate due to sparse data, why does forcing its Jacobian to be contractive (based on this inaccurate field) lead to a better outcome? Is there a theoretical explanation for this? \n\n*Typo: line 246 J_{\\epsilon_\\theta}^{sym} seems undefined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WPtq9vNEaq", "forum": "iKJbmx1iuQ", "replyto": "iKJbmx1iuQ", "signatures": ["ICLR.cc/2026/Conference/Submission5368/Reviewer_T4PE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5368/Reviewer_T4PE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962633433, "cdate": 1761962633433, "tmdate": 1762918025383, "mdate": 1762918025383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is improving general diffusion model policies through an additional class of losses that promote contractions in the sampling processes of the diffusion model policy. These losses are denoted Contractive Diffusion Policies (CDPs), which add a Jacobian-based loss so the reverse diffusion dynamics are contractive. The goal of this is to damp solver and score noise and reduce action variance. Experiments are done on D4RL, Robomimic, and a small real-robot setup."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Simple idea that integrates into standard diffusion-policy training without major architectural changes.\n- Well-presented theory around contraction for ODE’s and diffusion (or more generally flow-based) generative models.\n- Some positive results on D4RL benchmarks, especially where reducing variance helps."}, "weaknesses": {"value": "- Contraction seems to fundamentally contradict a central reason practitioners use diffusion model polices over e.g. Gaussian policies, namely multimodality. By pulling trajectories together, CDP can collapse valid modes that diffusion policies are meant to capture. It then seems in this setting that Gaussian policies would need to be benchmarked against, or at the very least explanation provided as to why either (a) multimodality is actually not that important for RL policies, or (b) why the contraction loss does not actually eliminate multimodality when it’s efficient for optimal policies to inhibit this.\n\n\n- There is no direct evidence that the spectral condition for contraction actually holds during training and sampling. Plots of eigenvalues through various time conditioning would give future readers a better idea of the effect of the contraction loss, and what parts of the diffusion model become more strongly contractive.\n\n- As Jacobian computations are typically expensive, a computation time analysis here seems relevant.\n\n\n- While some environments show CDP giving a non-negligible boost, none of the robomimic experiments show CDP giving a clear advantage over other methods. Additionally, some appendix figures are difficult to interpret because means and confidence intervals overlap heavily (e.g. Fig. 15). This makes it hard to assess some practical differences."}, "questions": {"value": "- How much action diversity is lost as the contraction weight increases? Is there a way to reconcile contraction-type losses with max-entropy RL?\n\n\n- What is the evidence that diffusion model policies suffer from the same score matching and integration errors that have been observed in e.g. image sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gE7a70n4gP", "forum": "iKJbmx1iuQ", "replyto": "iKJbmx1iuQ", "signatures": ["ICLR.cc/2026/Conference/Submission5368/Reviewer_rXr6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5368/Reviewer_rXr6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153762529, "cdate": 1762153762529, "tmdate": 1762918024885, "mdate": 1762918024885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is improving general diffusion model policies through an additional class of losses that promote contractions in the sampling processes of the diffusion model policy. These losses are denoted Contractive Diffusion Policies (CDPs), which add a Jacobian-based loss so the reverse diffusion dynamics are contractive. The goal of this is to damp solver and score noise and reduce action variance. Experiments are done on D4RL, Robomimic, and a small real-robot setup."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Simple idea that integrates into standard diffusion-policy training without major architectural changes.\n- Well-presented theory around contraction for ODE’s and diffusion (or more generally flow-based) generative models.\n- Some positive results on D4RL benchmarks, especially where reducing variance helps."}, "weaknesses": {"value": "- Contraction seems to fundamentally contradict a central reason practitioners use diffusion model polices over e.g. Gaussian policies, namely multimodality. By pulling trajectories together, CDP can collapse valid modes that diffusion policies are meant to capture. It then seems in this setting that Gaussian policies would need to be benchmarked against, or at the very least explanation provided as to why either (a) multimodality is actually not that important for RL policies, or (b) why the contraction loss does not actually eliminate multimodality when it’s efficient for optimal policies to inhibit this.\n\n\n- There is no direct evidence that the spectral condition for contraction actually holds during training and sampling. Plots of eigenvalues through various time conditioning would give future readers a better idea of the effect of the contraction loss, and what parts of the diffusion model become more strongly contractive.\n\n- As Jacobian computations are typically expensive, a computation time analysis here seems relevant.\n\n\n- While some environments show CDP giving a non-negligible boost, none of the robomimic experiments show CDP giving a clear advantage over other methods. Additionally, some appendix figures are difficult to interpret because means and confidence intervals overlap heavily (e.g. Fig. 15). This makes it hard to assess some practical differences."}, "questions": {"value": "- How much action diversity is lost as the contraction weight increases? Is there a way to reconcile contraction-type losses with max-entropy RL?\n\n\n- What is the evidence that diffusion model policies suffer from the same score matching and integration errors that have been observed in e.g. image sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gE7a70n4gP", "forum": "iKJbmx1iuQ", "replyto": "iKJbmx1iuQ", "signatures": ["ICLR.cc/2026/Conference/Submission5368/Reviewer_rXr6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5368/Reviewer_rXr6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153762529, "cdate": 1762153762529, "tmdate": 1763686951772, "mdate": 1763686951772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel diffusion policy called contractive diffusion policy. Comprehensive numerical experiments are conducted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The approach has strong theoretical foundations. The authors clearly provide conditions for contractive diffusion policy by leveraging the results in ODE theory and diffusion models.\n2. The numerical results are very comprehensive. The paper presents results across multiple benchmarks, showing the effectiveness of the proposed approach. \n3. Writing is easy to follow."}, "weaknesses": {"value": "Although I am familiar with diffusion models theory, I am not familiar with diffusion policy benchmarks. I only have one major concern:\n1. In Tang and Zhao (2024), the contractive condition excludes the application of usual diffusion model such as VP SDE. Do you face the same issue in your setup? If yes, what diffusion process are used and how to you compare with usual DDPM-based sampling process?"}, "questions": {"value": "The paper is clear to me. No further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9pRRQujMHd", "forum": "iKJbmx1iuQ", "replyto": "iKJbmx1iuQ", "signatures": ["ICLR.cc/2026/Conference/Submission5368/Reviewer_xLAA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5368/Reviewer_xLAA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762246039048, "cdate": 1762246039048, "tmdate": 1762918024625, "mdate": 1762918024625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to improve diffusion policy in the continuous control setup by inducing contractive behaviors. While the iterative sampling process of diffusion models encourages diverse outputs, it also hinders action generation in fields that require accurate signals (e.g., robotic control). The method introduces a penalty loss term on the score Jacobian to enhance robustness in the sampling process, as it effectively reduces action variance. The authors show that this offline learning method can improve the diffusion policy baseline on several robotic control problems, especially in the limited training data regime."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The method is simple and intuitive. The results look promising (especially the real-world experiments).\n+ The authors provide an in-depth theoretical analysis of their proposed method for inducing the contraction of diffusion models.\n+ The work is positioned well in a detailed discussion of related work."}, "weaknesses": {"value": "+ I am not fully convinced of the idea of unwanted action variance. Granted, excessive noise in the generated actions can lead to failure in continuous control. But the level of variance that is beneficial varies a lot from task to task, as pointed out in the appendix. It would be better if the authors could come up with a more principled tuning method (e.g., a self-adaptive coefficient). In particular, for tasks that require dynamic control (say, soft-body manipulation), it might be hard to justify the contraction-based regularization.\n+ I suspect that it might be harder for the contraction-regularized diffusion model to do transfer learning or further online fine-tuning. Basically, with the reduced action variance, it might be harder to adapt to various action distributions in a post-training manner. While the promise of a large diffusion policy for action generation lies in a foundation model + quick post-training fine-tuning paradigm, I wonder if the proposed approach might hinder this.\n+ The Jacobian (or in general Lipschitz) regularization approach has been popularized by a series of works in DNN generalization [1,2,3]. Please add them to the related work section.\n\nI am willing to raise my score after seeing the authors’ response.\n\n[1] Information-theoretic local minima characterization and regularization, ICML 2020\n\n[2] Sharpness-aware minimization for efficiently improving generalization, ICLR 2021\n\n[3] Understanding Gradient Regularization in Deep Learning: Efficient Finite-Difference Computation and Implicit Bias, ICML 2023"}, "questions": {"value": "Does the distillation process (e.g., consistency policy) effectively also induce a contraction? As the reduced sampling steps and the consistency distillation loss might also “pull” nearby diffusion flows closer. I suggest the authors study their proposed loss on distilled diffusion policies as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kqsc2DPo7W", "forum": "iKJbmx1iuQ", "replyto": "iKJbmx1iuQ", "signatures": ["ICLR.cc/2026/Conference/Submission5368/Reviewer_5Yr9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5368/Reviewer_5Yr9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762367879959, "cdate": 1762367879959, "tmdate": 1762918024362, "mdate": 1762918024362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revised Manuscript Uploaded"}, "comment": {"value": "The manuscript is updated to reflect the changes we outlined in the first round of responses. We will have a minor update only to add the additional experiment seeds to better support some responses."}}, "id": "2DFAVQETT9", "forum": "iKJbmx1iuQ", "replyto": "iKJbmx1iuQ", "signatures": ["ICLR.cc/2026/Conference/Submission5368/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5368/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission5368/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763571474263, "cdate": 1763571474263, "tmdate": 1763579224426, "mdate": 1763579224426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}