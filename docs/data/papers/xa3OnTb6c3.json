{"id": "xa3OnTb6c3", "number": 18821, "cdate": 1758291137989, "mdate": 1763747092033, "content": {"title": "MesaNet: Sequence Modeling by Locally Optimal Test-Time Training", "abstract": "Sequence modeling is currently dominated by causal transformer architectures that use softmax self-attention. Although widely adopted, transformers require scaling memory and compute linearly during inference. A recent stream of work linearized the softmax operation, resulting in powerful recurrent neural network (RNN) models with constant memory and compute costs such as DeltaNet, Mamba or xLSTM. These models can be unified by noting that their recurrent layer dynamics can all be derived from an in-context regression objective, approximately optimized through an online learning rule. Here, we join this line of work and introduce a numerically stable, chunkwise parallelizable version of the recently proposed Mesa layer (von Oswald et al., 2024), and study it in language modeling at the billion-parameter scale. This layer again stems from an in-context loss, but which is now minimized to optimality at every time point using a fast conjugate gradient solver. Through an extensive suite of experiments, we show that optimal test-time training enables reaching lower language modeling perplexity and higher downstream benchmark performance than previous RNNs, especially on tasks requiring long context understanding. This performance gain comes at the cost of additional flops spent during inference time. Our results are therefore intriguingly related to recent trends of increasing test-time compute to improve performance -- here by spending compute to solve sequential optimization problems within the neural network itself.", "tldr": "", "keywords": ["Sequence modeling", "test-time training", "RNN transformer alternatives"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44a3c8695dad1bb4d8da1b7b652e376b9de03b5b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Mesanet, which is a linear transformer based on the Mesa optimal regression update rule. The model is inspired by test-time training and the fact that the state update of linear transformers can be seen as an online learning objective. The model is sound, and the chunkwise parallel training makes the model efficient to train; however, the training is more time-consuming than other linear transformers such as GLA and Deltanet due to the solver update. The empirical results are extensive, and the model is evaluated on many different downstream language modeling tasks and length generalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper have several strengths:\n\n- **Model design**: The model design of Mesanet and the connection to test time training is elegant.\n\n- **Experimental setup:** The experimental setup for pre-training in language modeling tasks are sound and followed by the stablished setups such as [1,2].\n\n- **Fairness of comparison:** The paper tunes the learning rate for all different models which isa significant plus point as studies such as Gated Deltanet [3] specifically use the same learning rate for all models, which can indeed be not fair comparison between baselines.\n\n- **Chunk-wise parallel support for training:** In general the training paradigm and chunk wise form is designed nicely and helps parallel training.\n\n\n\n\n\n\n\n\n\n\n\n------\n\n###  References\n\n[1] Gated Linear Attention Transformers with Hardware-Efficient Training: Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim\n\n[2] Parallelizing Linear Transformers with the Delta Rule over Sequence Length: Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, Yoon Kim\n\n[3] Gated Delta Networks: Improving Mamba2 with Delta Rule: Songlin Yang, Jan Kautz, Ali Hatamizadeh"}, "weaknesses": {"value": "The main weakness of the paper lies in its presentation, which significantly reduces clarity regarding the approach and design. These issues can be summarized as follows:\n\n**1)** **Mesanet's online learning objective** is significantly under-explained, starting from Equation 4:\n\n$$\n\\mathcal{L}(\\Phi) = \\frac{1}{2} \\sum^t_{\\tau=1} |v_\\tau - \\Phi k_\\tau|^2 + \\frac{\\text{Tr} (\\Phi^\\top \\Lambda \\Phi)}{2}\n$$\n\nBy taking the gradient and setting it to zero, we arrive at:\n\n$$\n\\sum^t_{\\tau=1} (v_\\tau - \\Phi k_\\tau)k^\\top_\\tau + \\Lambda \\Phi = 0, \\quad\n\\sum^t_{\\tau=1} v_\\tau k^\\top_\\tau - \\Phi \\sum^t_{\\tau=1} k_\\tau k^\\top_\\tau + \\Lambda \\Phi = 0 \\quad \\text{(Eq. 1)}\n$$\n\nThe above result corresponds to the optimal $\\Phi$. Since in an autoregressive regime the summations above can be expressed recursively, one can write the following recurrences:\n\n$$\nG_\\tau = G_{\\tau-1} + v_\\tau k^\\top_\\tau, \\quad\nH_\\tau = H_{\\tau-1} + k_\\tau k^\\top_\\tau\n$$\n\nFrom here, one can substitute the states $G_t$ and $H_t$ into Equation (1) to obtain the optimal Mesa update rule:\n$\\Phi = G_t(H_t + \\Lambda_t)^{-1}$.\n\nMoreover, by including forget gates in the recurrences above, the online learning objective will also change, which is inconsistent with the paper’s notation. I suggest reformulating the online learning loss (Equation 4 in the paper) to explicitly include these forget gates, as also described in *Gated DeltaNet’s Table 1* [1]. Moreover, is $\\Lambda_t$ time-dependet or not since in equation 4 of paper is suggested as not but exactly on paragraph bellow it is depending on forget gates.\n\n----\n\n**2) The full recurrence of MesaNet is not shown and not compared with other baselines.**\nI recommend moving the recurrence equation of MesaNet (currently on page 21) into the main body of the paper, along with Table 2 for clarity and comparison with other methods.\n\n---\n\n**3) Motivation behind the Mesa layer.**\nSince the main motivation is the connection to test-time training (TTT), I wonder why one should use MesaNet instead of DeltaNet or Gated DeltaNet. My main question is: given that DeltaNet and Gated DeltaNet leverage TTT to improve recall abilities, and indeed show significantly better recall, does MesaNet outperform them in this aspect? MesaNet introduces additional complexity in both architecture and training. As Table 14 suggests, if the motivation for this design is to enhance recall ability, this should be clearly stated and justified in the paper.\n\n---\n\n**4) Hawk-Mesa underperforms significantly compared to many baselines and even MesaNet itself in recall tasks (Table 14).**\nIf the main motivation behind building a model based on the Mesa layer is to introduce an optimal way to structure memory (as suggested by the TTT framework), why does Hawk-Mesa perform substantially worse than MesaNet in Table 14? Why should one construct such a model if it does not improve recall? I ask this because I believe recall ability is one of the primary motivations behind the design of MesaNet and the TTT framework in general.\n\n---\n\n**5) Tables and plots are not clearly visible.**\nMany numerical results presented as bar plots are difficult to read. I highly recommend replacing them with tables, particularly Figure 5, to improve clarity.\n\n---\n\n**6) Missing baselines: Atlas and Titans.**\nI believe the baselines **Atlas** and **Titans** are missing from the paper’s comparisons. These models address a very similar problem and propose closely related solutions, especially Titans, which should be included for a fair evaluation.\n\n\n---\n\n### References\n\n[1] Gated Delta Networks: Improving Mamba2 with Delta Rule: Songlin Yang, Jan Kautz, Ali Hatamizadeh"}, "questions": {"value": "My most concerns are mentioned above and I mainly have 3 small questions:\n\n**1)** What is the effect of convoulution on q,k,v in Mesanet as it is a cruical component for Deltanet and Gated Deltanet and Mamba2. Also, since it has conections to the TTT paradigm as stated in: https://kexue.fm/archives/11320\n\n**2)** Why does $\\Lambda_t = \\frac{(1-\\gamma_t)}{\\beta_t}$ structured as so?\n\n**3)** How are forget gates $\\beta_t$ and $\\gamma_t$ defined and extracted? Specifically, what nonlinearities and feature mappings are used for them? Are they similar to those in Mamba-2, or do they use a temperatured sigmoid as in GLA or GSA?\n\nLastly, I am happy to increase my score if above presentation issues and clarifications regarding the paper are resolved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OfXhSUUzfa", "forum": "xa3OnTb6c3", "replyto": "xa3OnTb6c3", "signatures": ["ICLR.cc/2026/Conference/Submission18821/Reviewer_mKy1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18821/Reviewer_mKy1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761593194916, "cdate": 1761593194916, "tmdate": 1762930536898, "mdate": 1762930536898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a numerically stable, chunkwise-parallel Mesa layer. At each timestep it uses a conjugate-gradient (CG) solver to compute optimal fast weights, yielding a recurrent alternative to attention with dynamically adjustable test-time compute."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is well-motivated. It presents a numerically stable Mesa layer that solves $q^*_t=(H_t + \\Lambda)^{-1} q_t$ per timestep via a CG solver with gated state recurrences, yielding a well-posed recurrent formulation.\n2. Results are competitive with strong RNN-style baselines and broadly comparable to a Transformer at similar scale, and the accompanying analysis is generally sound."}, "weaknesses": {"value": "1. The paper shows per-layer timings and training throughput (Fig. 2), but there’s no single, end-to-end table that reports latency (ms/token), tokens/s, and GPU memory alongside quality across CG step counts or the stopping policy, and across multiple context lengths on the same hardware.\n2. It’s unclear when to prefer Mesa over MHA. Although the paper acknowledges that compute grows with CG steps and may exceed MHA past some step/key sizes, it lacks concrete recipes or tables mapping k/tolerance choices to context length, latency targets, and hardware, making deployment decisions difficult.\n3. On mean-so-far long-context perplexity, a strong SWA-1024 baseline is often competitive or better, which tempers any blanket \"long-range advantage\" message. Please expand the long-context suite and report where Mesa wins/loses with matched budgets."}, "questions": {"value": "Please see the concerns detailed in the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SgE2TRD9Go", "forum": "xa3OnTb6c3", "replyto": "xa3OnTb6c3", "signatures": ["ICLR.cc/2026/Conference/Submission18821/Reviewer_JXGc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18821/Reviewer_JXGc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750881259, "cdate": 1761750881259, "tmdate": 1762930483587, "mdate": 1762930483587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MesaNet, a sequence layer that replaces recurrent fast-weight updates with a per-token optimal linear regression solved by conjugate gradients (CG). The layer maintains two matrix states\n$G_t = \\\\sum_i \\\\zeta_{ti}, v_i k_i^\\\\top$ and $H_t = \\\\sum_i \\\\zeta_{ti}, k_i k_i^\\\\top$ (equivalently via gated recurrences $G_t=\\\\gamma_t G_{t-1}+\\\\beta_t v_t k_t^\\\\top, H_t=\\\\gamma_t H_{t-1}+\\\\beta_t k_t k_t^\\\\top$), and outputs $ o_t = G_t (H_t+\\\\Lambda)^{-1} q_t $ computed with a chunkwise-parallel CG solve implemented with GEMMs. Models at 140M/440M/1B on SlimPajama match or surpass strong linear-RNN baselines and are competitive with a transformer in average perplexity, with notably stronger early-sequence performance and solid results on some global-reasoning and few-shot tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Clear objective with a closed-form fast-weight map and practical CG solve; principled dynamic test-time compute via stopping criteria.\n* Strong empirical controls (same backbone/tokenizer/data order) enabling clean comparisons.\n* Useful diagnostics: early-sequence NLL gains, length extrapolation, grouped tasks; dynamic stopping achieves near-parity to larger fixed $k$ at substantially fewer steps on average."}, "weaknesses": {"value": "* Missing a concise accuracy–efficiency summary at inference (for one representative long-sequence setting), including per-token latency and peak memory under fixed batch, hardware, and precision.\n* Stability/conditioning analysis is qualitative; small ablations on the softplus scale for $\\\\Lambda$ and the diagonal preconditioner or $x_0$ initializer are needed to establish sensitivity and recommend defaults."}, "questions": {"value": "1. Accuracy–efficiency summary (1B). Using the existing 1B checkpoint and kernels, please provide a compact table for a representative long sequence reporting validation perplexity, mean per-token latency, and peak memory for a few $k$ values (e.g., $0$ and two others) or dynamic stopping with $\\\\epsilon$ (report average CG steps). Also state hardware, precision, batch size, sequence length, and measurement procedure (warmup, repeats). Throughput optional.\n2. Sensitivity/stability ablation. How sensitive are convergence and perplexity to the softplus scale on $\\\\Lambda$ and to the diagonal preconditioner or $x_0$ initializer? Please report CG steps to a fixed tolerance and validation perplexity, and provide recommended defaults."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iQOuWYj2O4", "forum": "xa3OnTb6c3", "replyto": "xa3OnTb6c3", "signatures": ["ICLR.cc/2026/Conference/Submission18821/Reviewer_oZt9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18821/Reviewer_oZt9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991636640, "cdate": 1761991636640, "tmdate": 1762930470839, "mdate": 1762930470839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a novel layer, called Mesa layer, guided by the theory of online learning/ test time training. The layer’s conception is similar to modern Linear RNNs/ SSMs like Gated DeltaNet, which allows for recurrent inference and chunk-wise parallel training. The difference lies in a principled online learning objective and, consequently, the recurrent state update rule, which features not just one, but two states and is computed via several steps of conjugate gradient descent. This yields competitive results in several benchmarks including large-scale LM with other models, such as the most related Gated DeltaNet. The authors conduct thorough theoretical analysis of their method and robust validation.\n\nI’m convinced this work is a valuable contribution to the field of efficient alternatives to Transformer/ online learning models and vote for its acceptance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "First, I stress that I liked this paper a lot and I'm excited about the latest developments in the field of linear-time Transformer alternatives, and in particular the direction where this and related works are steering the field. \n\n* The proposed method (online learning objective, corresponding update rule, chunk-wise and recurrent implementation algorithms) is novel and original.\n\n* The paper is very rich in content, be it new theory, connections to other literature, and especially experiments. \n\n* The scope of the experiments and ablations is very vast and comprehensive, to the extent that it leads to important and valuable observations about the other models and the field of linear alternatives to Transformers themselves (see e.g., summary in lines 81-84). \n\n* The paper is self-contained. For example, Appendix J explains the background from previous work leading to Mesa layer, so it’s not required to be familiar with it beforehand.\n\n* I’d like to highlight specifically that the paper provides a detailed description of the pre-training settings and hyperparameters, including e.g., dataset preparation, which allows for greater transparency and reproducibility."}, "weaknesses": {"value": "The below comments do not represent major weaknesses and don’t denigrate the quality of the paper. They aim for improvement of presentation or consider the pieces of text which the authors could just omit without detriment of the exposition. \n\n1. Please state in the beginning of the paper that all vectors are column vectors to avoid confusion because many related works such as GLA or Gated DeltaNet use row vectors as a convention.\n\n2. Lines 121-123: How do you derive the expression $\\gamma_t \\Phi_{t-1} + \\beta_t v_t k_t^T$ out of equation 3? Also, the statement on line 127 (“we recover DeltaNet”) is not supported.\nPlease provide full derivations or references to a specific place in the paper(s) where the statements are proved. In the same vein, could you describe how we get update rule (equation 5) out of online loss (equation 4)?\n\n3. Among several recent algorithms specifically designed with test-time training/ online optimization in mind, the paper discusses in depth only (Gated) DeltaNet variants, both theoretically (lines 1577-1603) and empirically. It would be great to explain in more detail MesaNet’s differences and similarities with such algos as Test-Time Training (https://openreview.net/forum?id=wXfuOj9C7L, not mentioned in the paper), LongHorn (https://openreview.net/forum?id=8jOqCcLzeO), Titans (https://openreview.net/forum?id=8GjSf9Rh7Z), Atlas (https://arxiv.org/abs/2505.23735), and Miras (https://arxiv.org/pdf/2504.13173). Lines 145-148 briefly touch on some differences, but further corroboration is required. E.g., could you provide formulas demonstrating why Atlas “corresponds exactly to a sliding-window variant of the Mesa layer”? Additionally, if you have the bandwidth and capacity, would it be possible to pre-train one of these related models and add it to comparisons (although I doubt that it would be significantly different in performance)? \n\n4. Also, It would be informative to include a table with comparison of online-learning objectives for different models, similar to the Table 4 in LongHorn paper (https://openreview.net/forum?id=8jOqCcLzeO).\n\n5. Lines 1452-1453 – The remark about Mamba 2 being non-gated is incorrect. Mamba 2 is gated in the same sense as GLA ($S_t = G_t * S_{t-1}$), but it calls the same mechanism “decay” and uses notation $A$ instead of $G$.\n\n6. There are no code listings for the new layer in the paper, nor accompanying code archive in the submission. The paper would benefit from an additional appendix with a high-level implementation of the core algo in PyTorch or Jax.\n\n7. I may have overlooked that but it seems the $O(T)$ training and $O(1)$ inference complexities aren't explicitly stated in the paper."}, "questions": {"value": "See section “weaknesses”. Also:\n\n1. I’m curious why the regularization term in equations 2 and 4 is $\\Phi^{\\top} \\Lambda \\Phi$ and not $\\Phi \\Lambda \\Phi^{\\top}$?\n\n2. What are your thoughts on why MesaNet becomes significantly superior to Gated DeltaNet on in-context recall tasks from Table 14 after prolonged training (50B tokens vs 15B)?\n\n3. Does MesaNet indeed have linear computational complexity w.r.t. sequence length T? Lines 449-450 highlight “the need for higher number of steps as t grows” indicating that it may not be the case.\n\n4. Lines 1399-1403: Do new sequences always start with fresh documents or new documents can cross sequence boundaries? I.e. if document A has more tokens than 2048 and therefore cannot be fitted into  sequence 1, will the next sequence 2 begin with the leftover part from document A?\n\n5. Do Transformer and SWA variants used in the experiments have any differences in architecture from LLama 2? Specifically, do the models employ RoPE?\n\n6. Why can't you use Woodbury identity for exact matrix inverse in chunk-wise parallel form instead of costly and approximate conjugate gradient descent steps?\n\n7. Will you release the data preparation/ configs/ pre-training code and, importantly, the weights of different models you've pre-trained for experiments in your paper? That would be greatly beneficial for the advancement of the field and provide very valuable playground and baselines for future research.\n\nEdit: Upon second thought, I see that Woodbury identity (https://en.wikipedia.org/wiki/Woodbury_matrix_identity) in Q6 wouldn't provide any benefits because the problem of finding inverse of $d \\times d$ matrix translates into finding inverse of $N \\times N$ matrix where d is head dimension and N is sequence length. However, I'm still not sure why you don't use exact matrix inverse algorithm with readily available PyTorch or CuBLAS methods (or some JAX/ TPU alternatives). For dense matrices, both conjugate gradient and ordinary exact matrix inverse methods have the same $O(d^3)$ complexity. Have you considered comparing speed of an exact method against your implementation with a commonly used head size d=128?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GlKdNgqVEi", "forum": "xa3OnTb6c3", "replyto": "xa3OnTb6c3", "signatures": ["ICLR.cc/2026/Conference/Submission18821/Reviewer_MDkC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18821/Reviewer_MDkC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141746457, "cdate": 1762141746457, "tmdate": 1762930464245, "mdate": 1762930464245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (Rebuttal)"}, "comment": {"value": "We would like to thank all the reviewers for their time and effort putting together these encouraging and helpful reviews. We are truly grateful for the many insightful comments that helped us to greatly improve our manuscript. We will gladly acknowledge the contribution of the anonymous reviewers in the final acknowledgements of our work.\n\nBased on your comments and suggestions, we ...\n- clarified our presentation which hopefully leads to a clearer and more accessible introduction of our method\n- added an additional token throughput comparison to related work\n- extended the related work discussion (including detailed derivations)\n\nPlease see the updated PDF where we highlighted all relevant changes in red. We remain open for any further questions and suggestions the reviewers might have. Thank you again!"}}, "id": "8EGSJs4aZk", "forum": "xa3OnTb6c3", "replyto": "xa3OnTb6c3", "signatures": ["ICLR.cc/2026/Conference/Submission18821/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18821/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission18821/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763751104464, "cdate": 1763751104464, "tmdate": 1763751104464, "mdate": 1763751104464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}