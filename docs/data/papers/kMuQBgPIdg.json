{"id": "kMuQBgPIdg", "number": 24514, "cdate": 1758357539258, "mdate": 1759896762236, "content": {"title": "Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search", "abstract": "Auto-bidding serves as a critical tool for advertisers to improve their advertising performance. Recent progress has demonstrated that AI-Generated Bidding (AIGB), which learns a conditional generative planner from offline data, achieves superior performance compared to typical offline reinforcement learning (RL)-based auto-bidding methods. However, existing AIGB methods still face a performance bottleneck due to their inherent inability to explore beyond the static offline dataset. To address this, we propose AIGB-Pearl (Planning with EvaluAtor via RL),  a novel method that integrates generative planning and policy optimization. The core of AIGB-Pearl lies in constructing a trajectory evaluator for scoring generation quality and designing a provably sound KL-Lipschitz-constrained score maximization scheme to ensure safe and efficient generalization beyond the offline dataset. A practical algorithm incorporating the synchronous coupling technique is further devised to ensure the model regularity required by the proposed scheme. Extensive experiments on both simulated and real-world advertising systems demonstrate the state-of-the-art performance of our approach.", "tldr": "", "keywords": ["auto-bidding", "offline reinforcement learning", "generative decision making"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ca5c019252ad292d0517b8b995cd1c44c417b66f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies how to integrate an offline reinforcement learning (offline RL) approach and conditional generative planning for the auto-bidding task of commercial advertisement. The proposed approach is akin to a conservative model-based RL method, where the evaluator is first trained in a pessimistic way with a Lipschitz constraint and then, controller is optimized to maximize the evaluator's score under the KL and Lipschitz constraints. The theoretical analysis is provided to bound the sub-optimality, and the experiment demonstrates that the proposed method works better than existing offline RL baselines and generative planning baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation for combining conditional generative planning and optimization (score maximization) is well-explained.\n\n- The proposed approach appears reasonable given the motivation of maximizing the score as an RL problem. In experiments, the proposed method is compared to offline RL baselines and works better than the compared method. \n\n- A good ablation study is provided. Especially, qualitative analysis showing how the trajectory of the proposed method differs between w/ and w/o KL explains why Lipschitz constraints are not sufficient even under the Lipschitz condition, which answers one of the questions I had."}, "weaknesses": {"value": "- I wonder where the conditional generative planning component is in the proposed method. While the proposed method works better than both traditional offline RL and generative planning-based baselines, given that the proposed method does not try to maximize the likelihood of demonstration, this looks more like a conservative model-based RL rather than a combination of planning and RL. The phrasing should be reconsidered, and in this sense, I wonder how impactful this paper is to the ICLR community (I acknowledge the contributions for the auto-bidding application, while in the offline RL context, Lipschitz constraints are another simple way to introduce conservativeness differently).\n\n- Theorem 2 seems to lack a necessary assumption. In my understanding, the evaluator's bias cannot be measured without having an assumption of the evaluator's accuracy or how the evaluator is trained (e.g., the evaluator is trained to satisfy the Lipschitz condition). It seems that he necessary assumption is not provided in advance.\n\n- Similarly, the paper claims that Theorem 1 is satisfied by \"the evaluator's design\" (i.e., by having $\\root{T} R_m$ constraints in the regression). However, if this is the reason, the theorem should be about the Lipschitz continuity of $\\hat{y}(\\tau)$, not that of $y(\\tau)$. There is no evidence presented regarding whether $y(\\tau)$ satisfies the Lipschitz continuity. I believe this should be an \"assumption\", instead of a \"theorem\". Overall, the theoretical analysis does not seem rigorous, and these details should be carefully reviewed."}, "questions": {"value": "- What is the sensitivity of the model like regarding the constraints (i.e., $\\delta_K$ and $L_p$)? How are these values determined in the experiment, and how much effort is needed to adjust these hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0jm40pB29b", "forum": "kMuQBgPIdg", "replyto": "kMuQBgPIdg", "signatures": ["ICLR.cc/2026/Conference/Submission24514/Reviewer_29sV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24514/Reviewer_29sV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761690850621, "cdate": 1761690850621, "tmdate": 1762943109298, "mdate": 1762943109298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AIGB-Pearl (Planning with EvaluAtor via RL), a method that augments generative auto-bidding (AIGB) with a learned trajectory evaluator and a KL–Lipschitz constrained score-maximization procedure to enable safe exploration beyond an offline dataset. The work combines theoretical analysis (evaluator bias bounds and a sub-optimality gap under KL/Lipschitz constraints), a practical planner/evaluator architecture (synchronous coupling, Lipschitz regularization), and extensive simulated and real-world A/B experiments that report consistent GMV/ROI improvements over strong baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Integrating an evaluator into a generative planner and constraining planning by KL + Lipschitz criteria to control evaluator bias is a principled and timely idea for offline auto-bidding.\n2. The paper gives provable bounds (evaluator bias → performance gap; sub-optimality bound under the proposed constraints) that make the method more convincing.\n3. Results include both simulated and large real-world A/B tests (TaoBao), showing consistent gains over state-of-the-art baselines and sensible ablations.\n4. The synchronous coupling and the proposed surrogate for the Wasserstein term are well-motivated and seem implementable in practice."}, "weaknesses": {"value": "1. In the problem statement the intrinsic impression value is introduced as v_i>0 (Section 2 / preliminaries). Later, in the experimental settings (Tables 5/6) the “value of impressions” is reported as ranging from 0 to 1. This inconsistency leaves readers unclear whether the theory (which uses v_i>0 and constants like R_m = max_i v_i/p_i) assumes arbitrary positive values or implicitly assumes normalized values in [0,1]. It also affects interpretability of constants such as R_m` Lipschitz constants that depend on reward magnitudes, and the hyperparameter choices.\n2. In the simulated-experiment paragraph the manuscript text says: “The maximum and minimum budgets of advertisers are 1000 Yuan and 4000 Yuan, respectively.” This phrasing is inverted relative to Table 5, which lists Minimum budget = 1000 and Maximum budget = 4000. In the real-world experiment paragraph a similar inversion appears: “The maximum and minimum budgets of advertisers are 50 Yuan and 10,000 Yuan, respectively.” This contradicts Table 6 (Minimum = 50, Maximum = 10,000). Such directional swaps are minor editorial errors but can confuse reproducibility and readers’ understanding of budget regimes used in experiments. Given that the budget ranges are central to the task formulation, they should be unambiguous."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "e1euXzRFWQ", "forum": "kMuQBgPIdg", "replyto": "kMuQBgPIdg", "signatures": ["ICLR.cc/2026/Conference/Submission24514/Reviewer_6mYT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24514/Reviewer_6mYT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786452248, "cdate": 1761786452248, "tmdate": 1762943109005, "mdate": 1762943109005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method (AIGB-Pearl) to enhance generative auto-bidding (AIGB) by integrating\nreinforcement learning (RL) into a trajectory-based generative modeling framework. The key idea is to\ntrain a trajectory evaluator to score the quality of generated trajectories and use this score to guide the generative\nplanner to find higher-performance trajectories beyond the offline dataset, while ensuring safety and\ngeneralization via KL and Lipschitz constraints. The method successfully leads to more stable training than\ntraditional offline RL, and is supported by theoretical guarantees on sub-optimality and generalization. The\nauthors conduct extensive experiments in simulated and real-world advertising environments to demonstrate\nstate-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: The integration of RL into a generative auto-bidding framework is innovative and addresses\nthe inability to explore beyond the offline dataset of existing AIGB methods. The introduction of a\ntrajectory evaluator and the use of KL-Lipschitz constraints to ensure safe exploration are also novel\ncontributions.\n\n2. Theoretical Rigor: The paper provides a thorough theoretical analysis, including bounds on evaluator\nbias, sub-optimality and generalization. The use of Wasserstein distance and synchronous coupling for\nLipschitz regularization is well-motivated and technically sound.\n\n3. Significance: The method is deployed in a real-world advertising platform and shows significant improvements\ncompared to previous methods, indicating high practical relevance.\n\n4. Ablation Analysis: Ablation studies clearly demonstrate the importance of both KL and Lipschitz\nconstraints. The analysis enhances the rigor and completeness of the article’s theoretical framework.\n\n5. Experimental Validation: Comprehensive and detailed experiments are conducted in both simulated\nand real-world environments with large-scale datasets and multiple budget levels, demonstrating the\nsuperior performance of the algorithm."}, "weaknesses": {"value": "Theoretical Assumptions: Some theoretical results in this paper rely on strong Assumption 1, which can be hard to verify on real-world dataset."}, "questions": {"value": "1. The paper considers the second-price bidding problem. How would the core formulation need to be adapted\nfor a first-price auction environment? Will the method still be effective under first-price settings?\n\n2. The theoretical analysis assumes the offline dataset satisfies LSI. What is the performance of AIGBPearl\nif the assumption does not hold? Are there any ways to relax the assumption?\n\n3. The paper considers offline RL baselines. Can the framework be extended to online cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U3nRGDG4n2", "forum": "kMuQBgPIdg", "replyto": "kMuQBgPIdg", "signatures": ["ICLR.cc/2026/Conference/Submission24514/Reviewer_wJct"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24514/Reviewer_wJct"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850264810, "cdate": 1761850264810, "tmdate": 1762943108716, "mdate": 1762943108716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AIGB-Pearl, a novel method designed to enhance AI-Generated Bidding (AIGB) by overcoming its limitation of imitating trajectories from a static offline dataset. AIGB-Pearl integrates generative planning with policy optimization by first constructing a trajectory evaluator through supervised learning to provides the necessary reward signal to guide the generative model. To ensure reliable utilization of this evaluator and to mitigate the Out-of-Distribution (OOD) issue, the method designs and enforces a provably sound KL-Lipschitz-constrained score maximization objective for the generative planner. AIGB-Pearl achieves greater training stability and demonstrates SOTA performance in extensive simulated and real-world advertising systems, consistently achieving improvement in GMV and other metrics compared to prior SOTA AIGB method"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty. AIGB-Pearl is the first work that integrates generative planning and policy optimization to address the performance bottleneck of existing AIGB methods. The integration of offline RL methods into the AIGB is important to guide the policy training and exploration of new trajectories\n2. Technical Contribution. The paper provides provable theoretical guarantees to ensure the reliability and safety of the policy optimization, which is critical when exploring beyond the static offline dataset. The core innovation is the design of a provably sound KL-Lipschitz-constrained score maximization objective that help mitigates the OOD problem\n3. Evaluation. In both the simulation and the real-world online A/B test, the methods achieve SOTA performance in the auto-bidding problem, out-performing the strong benchmark AIGB."}, "weaknesses": {"value": "1. The paper provides details on the overall dataset sizes: the simulated experiment uses 5k trajectories generated by 20 advertisers, while the real-world experiment uses 200k trajectories from 10k advertisers. However, it doesn’t explicitly state how many of these trajectories are used to train the evaluator model, and 5k/200k appears relatively small for training a stable evaluator capable of mitigating OOD issues.\n\n2. The paper states and assumes that the offline data distribution must satisfy the Logarithmic Sobolev Inequality (LSI) with a positive constant ρ, and this assumption could be satisfied through strategic adjustment of the trajectory sampling probability per condition y in the offline dataset D, the need to potentially alter the raw offline dataset distribution to ensure the theoretical bounds hold represents a limiting constraint or assumption on the data"}, "questions": {"value": "1. The effectiveness of AIGB-Pearl hinges on the reliability of the trajectory evaluator. The theoretical analysis bounds the gap between the planner's score and its true performance, where one of the core terms is the evaluator's bias on its training dataset. Could the authors provide the measured value of the evaluator's bias in both the simulated and real-world experiments?\n\n2. Related to weakness 2, Regarding “this assumption could be satisfied through strategic adjustment of the trajectory sampling probability per condition y in the offline dataset D”, Could the authors detail the specific nature and extent of this strategic adjustment applied to the offline dataset (e.g., which trajectories or conditions were adjusted)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5pHHIVrFr7", "forum": "kMuQBgPIdg", "replyto": "kMuQBgPIdg", "signatures": ["ICLR.cc/2026/Conference/Submission24514/Reviewer_3d4u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24514/Reviewer_3d4u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967331112, "cdate": 1761967331112, "tmdate": 1762943108394, "mdate": 1762943108394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}