{"id": "XjEmO6Znz9", "number": 19998, "cdate": 1758301324619, "mdate": 1759897007385, "content": {"title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation", "abstract": "Steering large language models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data, in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, which limits their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. \nIn this paper, we propose $\\textbf{Yet Another Policy Optimization (YaPO)}$, a $\\textbf{reference-free}$ method that learns $\\textbf{sparse steering vectors}$ in the latent space of a $\\textbf{Sparse Autoencoder (SAE)}$. \nBy optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. \nEmpirically, we show that sparse steering vectors converge faster, achieve lower training and evaluation loss, and remain more stable throughout training compared to dense counterparts. \nBeyond cultural alignment, YaPO generalizes to diverse alignment-related behaviors studied in BiPO, including truthfulness, hallucination mitigation, and jailbreak defense. \nOur results demonstrate that YaPO sparse steering provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad implications for controllability and domain adaptation.", "tldr": "We introduce YaPO, a reference-free method that learns sparse steering vectors via SAEs, enabling efficient, stable, and fine-grained alignment of large language models.", "keywords": ["Alignment", "Preference Optimization", "Activation Steering", "Sparse Autoencoders", "Domain Adaptation", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1d1789882f7cf9a6ab536418162a0e02d894fb3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper centers on activation steering, extending Bi-directional Preference Optimization (BiPO) by applying it within the sparse space of a Sparse Autoencoder. The experiments primarily target cultural adaptation, with results on Gemma 2 showing superior performance over BiPO."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach is intuitive and easy to understand.\n\n\n2. In cultural adaptation tasks, it achieves significant improvements over the baseline method, BiPO."}, "weaknesses": {"value": "1. The experiments were only conducted on Gemma2-2B, and the results need to be validated on more models to demonstrate generality.\n\n2. The baselines are limited. The paper only compares against BiPO, while there are many existing works on sparse activation steering that should be included for a more comprehensive comparison.\n\n3. The tasks are restricted to cultural adaptation, and although the authors created their own dataset, the description of the task is vague. It is difficult to understand the actual goal of this task.\n\n4. In Section 4.4 Generalization to Other Domains, the authors only report results on hallucination reduction without providing any details about the experimental setup, which makes this section confusing.\n\n5. Regarding method design, the authors follow BiPO’s bidirectional training framework, but they do not specify what the target behavior and opposite behavior are for the given task. Moreover, the evaluation does not mention bidirectional steering, leaving the purpose of this design unclear.\n\n6. Minor: Line 290 and 785 contain invalid pointers."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h7veck4xsJ", "forum": "XjEmO6Znz9", "replyto": "XjEmO6Znz9", "signatures": ["ICLR.cc/2026/Conference/Submission19998/Reviewer_Xmej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19998/Reviewer_Xmej"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761444156204, "cdate": 1761444156204, "tmdate": 1762932901723, "mdate": 1762932901723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces YaPO (Yet Another Policy Optimization), a method for learning sparse steering vectors in the latent space of Sparse Autoencoders (SAEs) to improve LLM alignment and domain adaptation. Unlike dense steering methods such as BiPO that operate directly in activation space and suffer from neuron multi-semanticity, YaPO optimizes sparse codes using a bi-directional preference optimization objective, producing disentangled and interpretable steering directions. The authors focus on cultural alignment as a case study, curating a new multilingual dataset covering 15 cultural contexts across 5 language families with both localized and non-localized prompts to measure the explicit-implicit localization gap.\n\nThe experimental results on Gemma-2-2B demonstrate that YaPO converges significantly faster than BiPO (under 150 steps vs. 600+ steps), achieves substantial performance improvements across multiple-choice questions (+14.7% average) and open-ended generation tasks, and remains more stable throughout training. YaPO also reduces the Performance-Normalized Localization Gap (PNLG) by 27.3% while improving Robust Cultural Accuracy (RCA) by 54.3%, indicating better consistency between localized and non-localized prompts. Beyond cultural alignment, the method generalizes to other alignment tasks such as hallucination mitigation, establishing sparse steering as a scalable approach for fine-grained LLM control."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- First method to combine preference optimization with sparse steering vectors in SAE latent space, addressing limitations of both dense steering (BiPO) and static sparse methods (SAS)\n\n- Demonstrates order-of-magnitude faster convergence and consistent performance improvements across all evaluated languages and settings\n\n- Curates a high-quality multilingual dataset (45,354 items) with careful controls for dialect, cultural validity, and localized/non-localized variants\n\n- Introduces PNLG and RCA metrics that appropriately measure both absolute performance and robustness to implicit cultural cues"}, "weaknesses": {"value": "- My biggest concern with this paper is the lack of baselines regarding steering with SAE. The authors did not compare against some new baselines like ReFT-r1, RePS, HyperSteer, and EasyEdit2. Since these methods also leverage SAE-based representations for steering, this omission makes it difficult to assess whether YaPO's improvements are genuinely novel.\n\n- I am a little bit concerned about the limited model coverage. YaPO is only evaluated on Gemma-2-2B (briefly mentions Gemma-2-9B), lacking evidence of scalability to larger models or different architectures (Llama, Qwen, etc.). There are also SAEs provided for models like Pythia and Llama in SAELens, and therefore I think more experiments are reasonable and necessary.\n\n- The interpretability claims are relatively unclear. While claiming \"interpretable\" steering, the paper lacks systematic analysis of what individual sparse features encode or how they differ from BiPO's dense features. Some automatic annotations with feature activations analyzed by LLMs could be a good supplementary material to strengthen these claims.\n\n- The cultural dataset focuses on specific countries but may not capture within-country diversity; Western \"control\" answers may introduce bias. The authors could consider using more datasets like those used in CAA and Axbench to demonstrate broader applicability."}, "questions": {"value": "1. Why did the author not compare against recent SAE-based steering methods like ReFT-r1, RePS, HyperSteer, and EasyEdit2?\n\n2. Can the author provide results on larger models (e.g., Gemma-2-27B, Llama-3) or different architectures (Qwen, Pythia) to demonstrate scalability?\n\n3. What specific concepts do the learned sparse features encode, and how do they differ from BiPO's dense features? \n\n4. Have the author tested YaPO on established cultural alignment benchmarks like those used in CAA and Axbench to validate broader applicability?\n\n5. How does the author method handle within-country cultural variations, given that your dataset focuses on country-level differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uN4JGwRPuM", "forum": "XjEmO6Znz9", "replyto": "XjEmO6Znz9", "signatures": ["ICLR.cc/2026/Conference/Submission19998/Reviewer_aqPy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19998/Reviewer_aqPy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729290719, "cdate": 1761729290719, "tmdate": 1762932901287, "mdate": 1762932901287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed YaPO (Yet Another Policy Optimization) which is an optimization-based approach to find steering vectors with the help of a pretrained SAE (sparse auto-encoder) of the target model. Specifically, YaPO trains the steering vector in a very similar way to BiPO but moves the steering vector from the model's hidden representation to the sparse features' activation, in hope that it will mitigate the multisemanticity issues in BiPO steering vectors. YaPO is only tested on Gemma-2 (and only results on the 2B variant are disclosed) with the off-the-shelf SAE Gemma Scope and primarily for a cultural localization task with a dataset the authors collected on their own, where BiPO yields noticeable improvement over both BiPO and the model w/o steering. The authors also shared some information on YaPO's performance in steering against hallucination which also outperforms BiPO and the model w/o steering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ It is good to see more research on joining SAE and steering vectors.\n+ The cultural localization problem the authors put forward and gathered a dataset for is an interesting problem and can be a good addition to existing tasks for benchmarking model behavior manipulation."}, "weaknesses": {"value": "+ The idea of bridging SAE and steering vectors are not exactly new. For instance [1] and [2] both have investigated how sparsity/monosementicity helps regularizes representation steering. In a way, YaPO can be considered merely using BiPO to achieve [2].\n+ While BiPO is a very good paper to base on, using it as the only baseline is inadequate, given that there are existing works that shared the same design as mentioned above.\n+ The experiments are also limited. \n    + Gemma is the only model being evaluated meaning that YaPO has never been evaluated on a model (mostly Llama, vicuna and their variants) that BiPO was originally tested on, even though there is off-the-shelf SAEs for them like Llama Scope as well. The performance on a single model is not as convincing as that across multiple models.\n    + The major experiments are conducted for the cultural localization task with datasets the authors built on their own without sharing a single example except for the 2 phrases in section 3.1. The authors also included some results about hallucination without any details other than the scores. BiPO was said to perform even worse than no steering which contradicts its reported performances on other models. The authors also fail to demonstrate if YaPO would undermine general utility of the models with e.g. MMLU benchmark.\n\n    If the authors do want to closely follow BiPO, they are suggested to at least do the experiments that BiPO has done with the exact settings. Simply be comparing Gemma Scope and Llama Scope it is not hard to find that they are very different in terms of how the sparse features look like, so it is possible that YaPO is only better for Gemma based models on very specific tasks.\n+ YaPO is said to be more efficient to train but that claim assumes that there is a pretrained SAE available for whichever model one want to use YaPO on. However in reality, training an SAE is actually way more consuming than either BiPO or YaPO and one cannot always expect a pretrained SAE to be readily available. People are interested in joining steering vectors and SAEs because they, to some extent, comprise a dual formulation of each other — one building up each single feature vectors bottom up from preference datasets, the other finds all possible feature vectors top down in an unsupervised fashion. So having a pretrained SAE at hand naturally gives YaPO a leverage by having the majority of the work done already so it is not surprising at all YaPO optimization could be faster.\n\n1. Chalnev, Sviatoslav, Matthew Siu, and Arthur Conmy. \"Improving steering vectors by targeting sparse autoencoder features.\" arXiv preprint1 arXiv:2411.02193 (2024).\n2. He, Zirui, et al. \"SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models.\" arXiv preprint arXiv:2505.16188 (2025)."}, "questions": {"value": "Please refer to the weakness for the concerns I have about this paper. Here I am just listing a few questions to facilitate the understanding of my concerns. \n+ Could it be 2B model is too small for the tasks?\n+ Could it be Gemma's SAE being different from Llama's?\n+ Did you redo the layer selection and hyperparameter search for BiPO?\n+ How does YaPO compare to other approaches to enhance steering vectors with SAE guidance?\n+ What if there is no off-the-shelf SAE? Can you train a sparse steering vector without a pretrained SAE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jBnrk1m76b", "forum": "XjEmO6Znz9", "replyto": "XjEmO6Znz9", "signatures": ["ICLR.cc/2026/Conference/Submission19998/Reviewer_AEFn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19998/Reviewer_AEFn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934302313, "cdate": 1761934302313, "tmdate": 1762932900659, "mdate": 1762932900659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Instead of learning a steering vector, they propose YaPO, where they instead learn to steer sparse features of a SAE. They show this on a cultural benchmark that they curate, and show that the method converges faster than and outperforms BiPO on that benchmark, as well as a hallucination benchmark."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The method converges much faster than BiPO and outpeforms BiPO in the cultural benchmark.\n* They also perform their method on BiPO's benchmarks, but only on the hallucinations dataset (which they note in their Limitations section)"}, "weaknesses": {"value": "* The work was done only on a single 2B model. The 9B variation was mentioned once in Limitations with no further details in the main body or Appendix.\n* The paper claims to produce more interpretable steering directions, but fails to do any work on interpreting the steering direction. They note that this is \"beyond the scope of this paper\" in the Limitations, but I disagree, as merely using the sparse autoencoder feature basis is not sufficient to make things more interpretable.\n* While the dataset/benchmark is claimed as a main contribution, there is barely any information about it in the main body."}, "questions": {"value": "In addition to the three weaknesses mentioned, I have the following questions:\n* How is this work different from [SAE TS](https://arxiv.org/abs/2411.02193)? (I note that SAE TS is not peer-reviewed and was not factored into my accept/reject decision, but nevertheless the paper should be cited and discussed as it predates this work by more than a year).\n* Is there any particular reason you choose to report the \"Egypt\" evaluation performance only for Figure 1? Is it possible to instead report the average difference over all categories between YaPO and BiPO across training epochs? What does that look like?\n\nI also have the following feedback:\n* **There are hallucinated citation authors**, like \"Steering llama 2 via contrastive activation addition\" being attributed to a \"Nathan Rimsky\". \"Steering Language Models With Activation Engineering\" also has hallucinated author names.\n* Appendix B is incomplete, especially its last paragraph which seems to be a broken transcript.\n* Line 290: Appendix reference missing\n* L418: Typo immediately after Activation Engineering (the full-stop).\n* L785: Figure missing\n* L302 Incomplete line \"we observe that the performance improvement is stable and consistent throughout the epochs for YaPO while BiPO\"..."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rx8pbKc0km", "forum": "XjEmO6Znz9", "replyto": "XjEmO6Znz9", "signatures": ["ICLR.cc/2026/Conference/Submission19998/Reviewer_Fp8b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19998/Reviewer_Fp8b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994639971, "cdate": 1761994639971, "tmdate": 1762932900138, "mdate": 1762932900138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}