{"id": "9R2iUHhVfr", "number": 21730, "cdate": 1758321053203, "mdate": 1759896906343, "content": {"title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?", "abstract": "We present SWE-Bench Pro, a comprehensive benchmark designed to evaluate software engineering capabilities through complex, realistic programming challenges. This benchmark extends beyond traditional algorithmic problems to encompass the full spectrum of professional software development tasks. The dataset comprises 1,865 problems sourced from 41 active software engineering repositories, spanning 123 unique programming languages and various application domains. The benchmark is structured into public and private components, with public access to problems from 11 repositories and private evaluation sets from 12 repositories across 4 distinct problem categories.\nSWE-Bench Pro addresses limitations of existing evaluation frameworks by incorporating problems that reflect real-world software engineering scenarios, including substantial codebases, complex enterprise applications, and multi-file projects requiring sophisticated reasoning and code modification skills. Problems range from early-stage startup environments to enterprise-level applications, with the private commercial set remaining inaccessible to maintain evaluation integrity while enabling public access to representative problems for professional development.\nOur evaluation methodology employs diverse coding approaches and models under controlled conditions, ensuring robust performance assessment across multiple programming paradigms. Results demonstrate significant performance variations across different problem categories, with traditional algorithmic challenges showing notably higher success rates compared to complex, multi-file engineering tasks. The benchmark reveals substantial gaps in current capabilities for handling real-world software engineering scenarios, particularly in areas requiring deep contextual understanding, cross-file reasoning, and integration with existing large-scale systems.\nThis work contributes a more comprehensive and realistic evaluation framework for assessing software engineering capabilities, providing insights into current limitations and establishing a foundation for future development in automated software engineering tools and methodologies.", "tldr": "We present SWE-Bench Pro, a benchmark for evaluating agent capability in long-horizon software engineering tasks.", "keywords": ["agent", "swe", "llm", "environment"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80c9d255234f96fafdf293e1a5184150ec8045d3.pdf", "supplementary_material": "/attachment/9ce9bdc3a9c3d2caff79275b5b46f4727ed49ab6.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces the SWE-bench Pro benchmark to address two issues in the evaluation of coding agents: \n(1) data contamination and (2) mismatch between the complexity of real-world software engineering tasks and those present in current benchmarks. \n\nFor mitigating data contamination, the authors choose repositories with copyleft licenses for public and held-out sets, and purchase private repositories from startups for a commercial set. They ensure complexity by filtering out <10 line edits, and claim that the tasks are diverse and industrially relevant because the repositories span a range of domains and no single repository has >100 tasks. They further manually enhance the dataset by augmenting the task descriptions and (manually) creating and verifying the execution environments. An evaluation of the SWE-agent scaffold with multiple models shows the challenging nature of SWE-bench Pro. They also ablate the human augmentations and show that unit tests have a high false-negative rate without them. Finally, an LLM-as-a-judge evaluation of the failure modes on SWE-bench Pro is given."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The problem tackled by the paper is impactful. Data contamination and limited complexity are important problems in current coding agent benchmarks, as shown by many prior works.\n- SWE-agent + Claude Sonnet 4 achieves ~66% on SWE-bench Verified, but only ~18% on SWE-bench Pro, indicating that the benchmark is challenging.\n- The human augmentation and verification workflow is well-designed to ensure the quality of the benchmark (although it requires human effort so may not be scalable). The ablation study shows that this is important to tackle the false-positive rate of the unit tests."}, "weaknesses": {"value": "- The claim that purchasing repositories from startups prevents data contamination is not entirely convincing. No details are provided on the contents of the agreements, e.g., can the startups sell the same repositories to another company developing a coding agent (or develop one themselves)? If so, it would raise fairness issues in the benchmark.\n- It is unclear why purchasing repositories is the best way to prevent contamination. Prior benchmarks like LiveCodeBench [1] and even SWE-bench Live [2] (particularly relevant to this work) instead use continuous evolution of the benchmark to address contamination. There is no discussion of why the authors' approach is better.\n- The evaluation is limited to a single agent scaffold (i.e., SWE-agent) with multiple models. While the results with SWE-agent do indicate a gap in difficulty between SWE-bench Verified and SWE-agent, more evidence is needed to show that the gap is not just for a particular scaffold.\n- The benchmark is limited in terms of language diversity, covering only 4 languages while prior works (SWE-bench Multilingual [3], Multi-SWE-bench [4]) cover 7 (or more) languages."}, "questions": {"value": "1. Could you please provide more details on the contents of the agreements with startups? Specifically, can they sell the same repositories to another company developing a coding agent (or develop one themselves)?\n2. How does this approach compare to using continuous evolution (like in SWE-bench Live) for preventing data contamination?\n3. Related to 2, what is the distribution of the dates of the PRs in the benchmark? How many are before/after the knowledge cutoff dates for the LLMs used in the experiments?\n4. Do you have evidence to show that other agentic scaffolds that score highly on SWE-bench Verified also score poorly on SWE-bench Pro?\n5. Related to 4, do you have a hypothesis for why SWE-agent performs well on SWE-bench Verified but not on SWE-bench Pro? Is it just the number of files/lines of code to be edited or are there other factors (e.g., unseen repositories, different domains etc.)? It would be great to have a qualitative comparison of the trajectories on both benchmarks.\n6. What is the distribution of the programming languages for the tasks in the benchmark?\n\n[1]: Jain, Naman, et al. \"Livecodebench: Holistic and contamination free evaluation of large language models for code.\" arXiv preprint arXiv:2403.07974 (2024).\n\n[2]: Zhang, Linghao, et al. \"SWE-bench Goes Live!.\" arXiv preprint arXiv:2505.23419 (2025).\n\n[3]: Yang, John, et al. \"SWE-smith: Scaling Data for Software Engineering Agents.\" arXiv preprint arXiv:2504.21798 (2025).\n\n[4]: Zan, Daoguang, et al. \"Multi-swe-bench: A multilingual benchmark for issue resolving.\" arXiv preprint arXiv:2504.02605 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8j9Gt1HPdA", "forum": "9R2iUHhVfr", "replyto": "9R2iUHhVfr", "signatures": ["ICLR.cc/2026/Conference/Submission21730/Reviewer_RmfJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21730/Reviewer_RmfJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884098426, "cdate": 1761884098426, "tmdate": 1762941908835, "mdate": 1762941908835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SWE-Bench Pro is a new benchmark of repository-level tasks for coding agents. The authors create 3 distinct splits, public, held-out and commercial, to avoid data leakage. The benchmark consists of relatively larger changes in more files that swebench, and shows reduced performance of LLMs compared to swebench. The key contribution of the paper is curating the benchmark with manual efforts to ensure quality of prompts and tests. The tasks also span a diverse class software-engineering activities like bug fixing, feature additions and code enhancements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents an enhanced version of repo-level coding tasks. It spans 4 different languages and variety of tasks (bug fixing, code enhancement, feature additions). Holding some repos private and collecting active repos from startups is useful to prevent data leakage or over-fitting.\n- The main technical contributions are in the preparation of the benchmark. The raw issue descriptions on GitHub can be under-specified or ambiguous. The authors manually add the necessary context/requirements to overcome this. This can provide clearer picture on the problem-solving ability of the LLMs, while avoiding confounding with ambiguity resolution. The authors also provide interface descriptions derived from tests, so that models are not penalized due to under-specification. Finally, they evaluate test suites and remove broad or irrelevant tests for more accurate evaluation.\n- The experimental evaluation shows that SOTA LLMs are not very effective on this benchmark."}, "weaknesses": {"value": "- The authors have spent extensive efforts to reduce ambiguity in task descriptions, which is helpful in evaluating some aspects of the LLMs/agents, but this de-emphasizes their performance in real-world scenarios where under-specified intents are the very common. The agents in such cases have to gather more context and do more exploration, e.g., to know what interfaces would be compatible with the rest of the repository. This is an important agentic capability, that is untested. The authors should discuss this.\n- I find the use of the term \"long-horizon\" in the title misleading. The changes to average 4 files and 100 LoC does not qualify as long-horizon in my view. The agents are given a maximum budget of 50 calls, which is pretty standard. The paper doesn't give the average number of turns used in successful trajectories, but that cannot cross 50 given the bound.\n- The evaluation is done using a single agent harness. Maybe the LLMs would do better with other harnesses. Many of them are also paired with their own agents. This evaluation is missing.\n- The authors do not evaluate the model performance with inference-time scaling or sampling. The numbers are pass@1. The model performance could improve substantially with scaling.\n- On the construction of the benchmark, the complexity of the benchmark is quantified by the number of files and lines changed. It is possible that some files have single line changes and some has a 100 line change because a single function is generated or edited. What would be useful is to provide the distribution by the number of diff hunks (disjoint code changes) and the average diff hunk size. The authors state that they drop broad or irrelevant tests. Are new tests added or were they deemed complete?"}, "questions": {"value": "- What makes the proposed benchmark long-horizon?\n- How does the model performance vary with different scaffolds, both open-source and commercial, e.g., openai codex or claude code?\n- How does the model performance vary with inference-time scaling? What are pass@10 numbers?\n- What are the average number of diff hunks and hunk sizes in the benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pjxjsikfh3", "forum": "9R2iUHhVfr", "replyto": "9R2iUHhVfr", "signatures": ["ICLR.cc/2026/Conference/Submission21730/Reviewer_RA2s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21730/Reviewer_RA2s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892698972, "cdate": 1761892698972, "tmdate": 1762941908610, "mdate": 1762941908610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SWE-bench pro, a new, large-scale benchmark designed to evaluate AI agents on complex, \"long-horizon\" software engineering tasks. The authors motivate this work by identifying critical limitations in existing benchmarks like SWE-Bench , namely their susceptibility to data contamination and their inclusion of many \"trivial\" problems (e.g., one- to two-line modifications) that do not reflect realistic, enterprise-level complexity.\nThe benchmark focuses on challenging, industrially relevant tasks by excluding trivial edits and curating problems that require substantial, multi-file modifications. \nThe authors evaluate several frontier models and find that their performance, which exceeds 70% on benchmarks like SWE-Bench Verified, drops significantly to below 25.9% on SWE-BENCH PRO (with GPT-5 achieving the highest score). The paper includes a detailed analysis of performance breakdowns and agent failure modes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The \"human-centered augmentation and verification workflow\" is a key strength that ensures high quality and resolvability. The decision to add requirements and interface specifications is crucial, as it provides a robust way to \"mitigate false negatives\" where a correct agent implementation might fail a unit test due to an unexpected API. This methodological rigor is commendable.\n\n- The paper demonstrates a clear and dramatic performance gap: models that excel on SWE-Bench Verified (often >70%) fail on SWE-BENCH PRO (<26%). This finding is significant as it \"highlights a critical gap between current agent capabilities and the demands of real-world development\"."}, "weaknesses": {"value": "- The paper's primary conceptual claim, as suggested by its title, is its focus on \"long-horizon\" tasks. While this is a well-motivated goal, the paper's operationalization of this concept is a significant weakness. \"Long-horizon\" is implemented as a filter for tasks requiring a large number of modified code lines. This conflates the size of the final patch with the complexity or duration of the task itself. A true long-horizon task implies a complex, multi-step reasoning and planning process, which is not necessarily correlated with the number of lines in the final diff. In the current landscape, where top agents are being developed to handle tasks that take human engineers many hours or even days to complete, using \"lines of code\" as the primary proxy for \"long-horizon\" feels reductive and outdated. While the engineering value of creating a more difficult benchmark is high, the conceptual novelty claimed by the \"long-horizon\" framing is not well-supported and oversells this aspect of the contribution.\n\n- Limited Language Coverage: The authors correctly identify this as a limitation. The benchmark includes Python, JavaScript, TypeScript, and Go, but it underrepresents other major languages in enterprise and systems development, such as Java, C++, and Rust. While the current selection is diverse, expanding this coverage in future work would be necessary to fully \"assess agent performance across the full spectrum of modern software development\"."}, "questions": {"value": "The field has recently seen a proliferation of benchmarks inspired by SWE-Bench, such as SWE-bench Multimodal and Multi-SWE-bench, which often feel iterative and focus primarily on expanding language coverage. This trend can be seen as offering diminishing returns, as it doesn't always address the more fundamental, unsolved problems in agent evaluation.\n\nMy initial reading of this paper was, 'This appears to be another, albeit higher-quality, multi-language SWE-Bench.' The human-annotated subset  certainly suggests a strong commitment to quality.\n\nHowever, the paper also explicitly criticizes prior work like Multi-SWE-Bench, stating its 'evaluation procedure remains superficial, focusing primarily on pass rates and thus limiting insights'. This paper, in contrast, introduces 'novel instance stratifications and retrieval metrics rooted in syntax tree analysis'\n\nGiven this context, I am curious about the authors' perspective:\n\n- How do you position your work against this backdrop of 'benchmark fatigue'? \n\n- Do you view the primary contribution as the dataset itself (a more diverse, multi-task, and rigorously verified resource) or as the new evaluation methodology (the file- and CST-node-level retrieval metrics )? \n\n- Following on that, how do you believe this new benchmark and its associated metrics will help researchers move beyond the current 'pass/fail' paradigm to gain the deeper insights that you rightly point out are often missing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8alXm1ezCo", "forum": "9R2iUHhVfr", "replyto": "9R2iUHhVfr", "signatures": ["ICLR.cc/2026/Conference/Submission21730/Reviewer_qF7u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21730/Reviewer_qF7u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974267639, "cdate": 1761974267639, "tmdate": 1762941908291, "mdate": 1762941908291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks? introduces an updated benchmark for evaluating how language models handle complex programming tasks. The dataset contains 1,865 verified issues from 41 real repositories, including both open-source and private ones. Each task requires larger, multi-file code edits, and comes with added human-written requirements and tests to ensure it can be solved and verified. The authors also focus on reducing data contamination by choosing repositories that are less likely to appear in model training data.\n\nIn their experiments, the authors find that even advanced models like GPT-5 and Claude Opus 4.1 can only solve around 25% of the problems, much lower than their performance on simpler benchmarks such as SWE-Bench Verified. Performance drops sharply when tasks span multiple files or become longer. Removing human-provided requirements and interface information leads to a large decrease in accuracy, showing that current models rely heavily on additional context to perform well.\n\nThe analysis shows that most model errors come from incorrect logic, syntax mistakes, and editing the wrong files. The paper concludes that existing models still struggle with complex, long-horizon programming work and that test-based evaluation alone does not fully reflect real software development tasks. SWE-Bench Pro is presented mainly as a more difficult and controlled benchmark for studying these limitations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper has a clear motivation and focuses on addressing two key issues with existing coding benchmarks: data contamination and lack of realistic task complexity. The authors clearly explain why these problems matter for evaluating AI coding models and position their work as a step toward more representative software engineering evaluation. This clarity in framing the problem gives the paper a solid foundation and makes its goals easy to understand.\n\nAnother strength is the careful and structured way the dataset is built. The authors combine open-source and proprietary repositories, verify all tasks with human input, and ensure that each problem is self-contained and reproducible. The inclusion of human-authored requirements, interface definitions, and verified test environments improves the dataset’s quality and reliability. These choices show a strong focus on making the benchmark both realistic and fair.\n\nThe experimental section is also thorough and systematic. The paper compares several major models under the same conditions, analyzes results across programming languages and levels of task complexity, and includes a detailed examination of model failure patterns. This goes beyond raw accuracy numbers and helps readers understand where and why models struggle. In addition, the effort to minimize contamination through strict repository selection adds credibility to the evaluation, and the clear reporting of dataset composition and evaluation setup supports reproducibility. Overall, the work stands out for its careful methodology, transparency, and depth of analysis."}, "weaknesses": {"value": "While the paper is well executed, its scientific contribution is relatively incremental. The benchmark primarily extends earlier work such as SWE-Bench and SWE-Bench Verified rather than introducing a fundamentally new methodology. Most improvements—such as stronger contamination control, larger multi-file edits, and human-verified augmentation—represent refinements rather than conceptual advances. From a research standpoint, this limits the originality of the paper, as it focuses more on scaling and cleaning existing benchmarks than on offering new theoretical understanding or experimental insight into model reasoning or autonomy.\n\nIn addition, the dataset’s scope remains narrow, covering only a few languages (mainly Python, JavaScript/TypeScript, and Go) and small to mid-sized repositories. This limits its claim of representing enterprise-level complexity. The use of startup codebases in the “commercial” subset is an interesting idea but falls short of demonstrating true industrial realism. Furthermore, the evaluation is tied to one scaffold—SWE-Agent—which, while standard, restricts the generality of the results. The authors could strengthen their claims by evaluating models under alternative frameworks such as OpenHands[1], AutoCodeRover[2], HyperAgent[3] or Agentless, which differ in interaction style, tool invocation, and code-editing strategy. Without such comparisons, it is unclear whether the observed limitations stem from model capability or from scaffold design.\n\nFinally, the paper’s analysis remains mostly descriptive. It identifies performance patterns and failure modes but provides limited causal interpretation or deeper reasoning about why these errors occur. The reliance on human-provided requirements and interface hints also undermines the claim of evaluating “autonomous” software engineering. Overall, while the benchmark is methodically strong and practically valuable, its scientific novelty and depth are modest, and its evaluation would benefit from broader experimental diversity and cross-framework validation.\n\n1) https://arxiv.org/abs/2407.16741, OpenHands: An Open Platform for AI Software Developers as Generalist Agents\n2) https://arxiv.org/abs/2404.05427, AutoCodeRover: Autonomous Program Improvement\n3) https://arxiv.org/abs/2409.16299, HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale"}, "questions": {"value": "1) The ablation study shows that removing human-written requirements and interfaces reduces accuracy sharply. Could the authors run intermediate levels of augmentation (e.g., partial requirements or inferred API hints) to better understand how much structured guidance is truly necessary for model success?\n\n2) Since the results are tied to the SWE-Agent scaffold, could the authors compare against other agent frameworks or planning strategies (e.g., multi-step reasoning, tool-chaining, or function-calling workflows)? This would clarify whether poor results are due to model limits or scaffold design.\n\n3) Can the authors test whether model performance correlates with repository size, complexity, or codebase age? This might help explain why some repositories have near-zero solve rates.\n\n4) Can the authors analyze model performance separately for bug fixes vs. feature additions vs. refactoring tasks? This breakdown would clarify which task types are most challenging and whether certain categories are overrepresented in the dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "avvyTbUHrO", "forum": "9R2iUHhVfr", "replyto": "9R2iUHhVfr", "signatures": ["ICLR.cc/2026/Conference/Submission21730/Reviewer_kCLY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21730/Reviewer_kCLY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132783631, "cdate": 1762132783631, "tmdate": 1762941907611, "mdate": 1762941907611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}