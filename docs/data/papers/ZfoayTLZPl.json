{"id": "ZfoayTLZPl", "number": 14826, "cdate": 1758244405760, "mdate": 1759897347026, "content": {"title": "Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization", "abstract": "Large language models (LLMs) are increasingly applied in legal practice, with case summarization being a key long-context task where cases often exceed 100K tokens across multiple documents. Existing evaluation methods rely on checklist comparisons but use coarse-grained extraction that merges multiple values into single text blocks, missing partial matches when comparing them. They also overlook content beyond predefined checklist categories and lack writing style evaluation. In this paper, we introduce Gavel-Ref, a reference-based evaluation framework that improves checklist evaluation through multi-value extraction with supporting text, and further incorporates residual fact and writing-style assessments. Using Gavel-Ref, we move beyond the single aggregate scores reported in prior work to systematically evaluate 12 frontier LLMs on legal cases ranging from 32K to 512K tokens, primarily from 2025. Our detailed analysis reveals Gemini 2.5 Flash, GPT-5, and Claude Sonnet 4 achieve the best performance (around 50 $S_{\\text{Gavel-Ref}}$), showing the difficulty of the task. These top models show consistent patterns: they succeed on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs keep improving and may eventually surpass human summaries, we also explore checklist extraction directly from case documents. We experiment with three different methods: end-to-end with long-context LLM, chunk-by-chunk extraction, and our newly developed autonomous agent scaffold, Gavel-Agent. Results show a trade-off between performance and efficiency: GPT-4.1 end-to-end performs best, while Gavel-Agent with Qwen3 reduces token usage by about 50\\%. We will release our code and annotations publicly to facilitate future research on long-context legal summarization.", "tldr": "We present Gavel-Ref for comprehensive legal summarization evaluation, benchmark 12 frontier LLMs, and explore direct checklist extraction with end-to-end, chunk-by-chunk, and our agent scaffold Gavel-Agent.", "keywords": ["Legal summarization", "Long-context evaluation", "Checklist-based evaluation", "Agent scaffold"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f31ee96a329e6e32f71b7fc0b02db3149577b6b7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduceGavel-Ref, a reference-based evaluation framework that aims to improve checklist evaluation on the long-context task of legal summarization. Their evaluation shows that frontier models succeed on simple checklist items but struggle on multi-value or rare ones such as settlements and monitor reports."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors defined a reference-based evaluation framework for comprehensively assessing legal summarization with checklist-style, residual facts, as well as writing style evaluations. This allows for more nuanced evaluation beyond a single-scaler score."}, "weaknesses": {"value": "- The amount of the cases (50) used in the evaluation is very limited. It is also unclear how such cases are selected. Given so many different legal areas, types of legal documents, juridictions etc., the representativeness of the 50 cases is questionable. For instance, a \"good\" summary for a contract law case could look very differently to a criminal law case.\n\n- Judgement from LLMs are used extensively in GAVEL-REF framework, yet it's unclear which model(s) are used in which step. There is no information as e.g. how self-bias and family-bias are taken into account in the evaluation.\n\n- In section 2.3, the authors \"recruit four in-house annotators with legal expertise\", yet it remains unclear what level of expertise the annotators acquire and in which jurisdiction. The recruitment criteria and procedure is also not presented. Given the high-stake nature of legal domain, this presents a significant limitation. \n\n- While GAVEL-Agent shows advantage over other methods such as end-to-end and chunk-by-chunk ones, and the tools defined in Section 4.1 may have great potentials as a product, I question the extent to which engineering efforts can translate into research insights."}, "questions": {"value": "- Given the difficulty of legal summarization, it would be very helpful to include qualitative analyses on what kind of cases lead to lower vs. higher scores.\n\n- In section 4.2, could you provide the rationale of the model selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "onSNyVeb63", "forum": "ZfoayTLZPl", "replyto": "ZfoayTLZPl", "signatures": ["ICLR.cc/2026/Conference/Submission14826/Reviewer_8q7c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14826/Reviewer_8q7c"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899401596, "cdate": 1761899401596, "tmdate": 1762925176141, "mdate": 1762925176141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies content selection and stylistic evaluation of text summaries generated from long legal documents. They combinechecklist-based evaluation with residual facts and writing style metrics. First, the authors evaluate a suite of open-source and proprietary LLMs on Gavel-Ref dataset. Second, they explored methods to automatically extract checklist answers from source documents.\n\nGavel-Ref is an extension of a prior benchmark (ExpertLongBench). ExpertLongBench includes a pre-defined set of 26 checklist items that are important for legal document summarization. Each input is already annotated with reference summary. Gavel-Ref makes three key extensions to ExpertLongBench. It modifies checklist answers to include multiple values. Additionally, Gavel-Ref includes atomic facts not covered by the checklists (residual facts) and an evaluation of the writing style on a Likert scale. The paper also includes a meta-evaluation of their LLM-based extraction of checklist answers from system-generated summaries and reference summaries. The paper evaluates a suite of 12 open-source and proprietary LLMs on Gavel-Ref. A notable result here is that the most LLMs often achieve higher scores at longer input lengths (>=128k) than shorter inputs (<64k) (Figure 2). This is a stark difference from prior evaluations on long-context benchmarks.\n\nThe second part of the paper explores three methods to automatically extract answers to the checklist questions from the source documents: end-to-end long-context model, chunk-by-chunk and a agent-based system. They evaluate these three method against reference-based checklist from Gavel-Ref. GPT-4.1 based long-context model performs the best, while the agent-based system (Qwen3-30B-A3B, gpt-oss-20B) significantly underperforms.\n\nOverall, the paper studies an important evaluation task but I have some concerns with the results (at varying context lengths) and agent-related experiments."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Checklist-based evaluation is a reliable option for certain technical domains such as finance and legal. This paper highlights key limitations with an existing checklist-based benchmark (ExpertLongBench) and proposes fixes through multi-value answers and residual facts.\n- The paper also explores the idea of extracting answers to the checklist directly from source documents. This is especially important for long input tasks because human written checklists are expensive to collect (if feasible). While I think an LLM agent is a strong option for this task, the results show that the agent approach underperforms a long-context model.\n- The context management component of Gavel-Agent is quite interesting, and well suited for the task."}, "weaknesses": {"value": "- The paper needs additional discussion of the results from Figure 2. Its unclear (and a bit surprising) why the models perform better at longer inputs than shorter inputs. This seems counter-intuitive and some qualitative analysis here could be helpful.\n- The proposed agent-based method significantly underperforms a strong long-context model (GPT-4.1). To show that the agent approach is reliable, it would be interesting to explore stronger models within the agent setup. I understand the argument around efficiency and performance, but for extracting reference checklists I think performance is very critical.\n- The paper doesn't discuss the effect of summary length. Longer system-generated summaries have a higher chance of including both checklist and residual facts. Do you control for summary length across the evaluated systems?"}, "questions": {"value": "Some additional questions and comments on the paper,\n\n- Gemini Flash outperforms Pro and Claude Sonnet outperforms Opus on Gavel-Ref – this is a bit unusual. I don't think I fully agree with the long-context argument provided in lines 252-254. Prior benchmarks shows that Gemini Pro has a longer effective context window length. Additional analysis here could be really helpful.\n- Is the size of checklist fixed to 26 items? Even when extracting they are automatically extracted from the source documents?\n- Human-curated checklists could be expensive (or even infeasible) for longer documents, so it would be interesting to check if automatically extracted (end-to-end or agent-based) checklist values improve on recall compared to human-curated checklist. Maybe a human expert could verify the automatically extracted checklists?\n- I am not sure I fully agree with the drawback of ExpertLongBench from lines 79-81. I believe it is possible for the user to breakdown the performance by task and example."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n9gchnURWU", "forum": "ZfoayTLZPl", "replyto": "ZfoayTLZPl", "signatures": ["ICLR.cc/2026/Conference/Submission14826/Reviewer_bwrh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14826/Reviewer_bwrh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971993612, "cdate": 1761971993612, "tmdate": 1762925175293, "mdate": 1762925175293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a reference-based evaluation framework for assessing legal summarization. It follows up on a checklist comparison approach to evaluating legal summaries and also proposes a new agent scaffold to assist the extraction of the checklist items. Results show that GPT 4.1 performs the best in the end-to-end scenario under a long context."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I love that the evaluations are very comprehensive, spanning across 12 frontier models. In addition, I appreciate your efforts in conducting a relatively large scale human annotation effort, which enhances the rigor of this study. There are also in-depth analyses of the failure modes and how top models succeed in this task, which gives us more insights."}, "weaknesses": {"value": "My biggest issue with the paper is that the contribution feels very incremental. The main method is a follow-up on an existing paper (https://arxiv.org/pdf/2506.01241) which is not a popular and widespread evaluation method as of today. Also, the existing method from Ruan et al. (2025) relied on 26 items from the legal experts in the study, and it really feels like the premise of Ruan et al. (2025) needs to be more general and robust. In addition, this paper presents a low inter-annotator agreement, which weakens the validity of evaluation."}, "questions": {"value": "Suggestions:\n1. Figure 2 does not have to go all green. You can go with a wider gradient of color given that for each task the colors in the heatmap are a bit indistinguishable. Widening the spectrum will help readers differentiate.\n2. Figure 3 feels a bit a convoluted and would be nice if a qualitative example or a clearer message is shown."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HNenrbVLtv", "forum": "ZfoayTLZPl", "replyto": "ZfoayTLZPl", "signatures": ["ICLR.cc/2026/Conference/Submission14826/Reviewer_W5hh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14826/Reviewer_W5hh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989378371, "cdate": 1761989378371, "tmdate": 1762925174661, "mdate": 1762925174661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GAVEL-REF to automatically evaluate LLM-generated summaries of legal cases. The paper focuses on long-context summarization, which aligns with the nature of legal documents. The paper reports results on several LLMs, such that GPT-4.1 performs the best while Qwen3 reduces token usage on 20 long case summaries."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper conducts a systematic evaluation of 12 frontier LLMs across five context length scales (32K–512K tokens), using predominantly 2025 cases (90% from 2025). This design offers credible insights into how state-of-the-art models process truly long contexts.\n\nInstead of reporting only aggregate performance scores, the study provides fine-grained, item-level analyses, revealing that top-performing models achieve near-perfect accuracy on simple single-value items (e.g., filing date: 0.99).\n\nThe proposed GAVEL-AGENT introduces an innovative long-context extraction strategy that emulates how human experts selectively navigate documents. Its demonstrated 40–60% reduction in token usage—while maintaining competitive performance (31.9 vs. 43.7 S_checklist)—addresses critical real-world challenges of computational efficiency and cost.\n\nThe integration of three complementary evaluation dimensions—checklist accuracy, residual facts assessment, and writing style analysis—provides a more holistic view of summary quality compared to content-only metrics. Notably, the finding that GPT-5 retains more residual facts but exhibits weaker narrative structuring, while Claude models excel in stylistic fidelity, highlights the strength of this multi-dimensional evaluation framework."}, "weaknesses": {"value": "The meta-evaluation in Section 2.3 is based on only 20 long summaries for checklist validation, with 15 receiving single annotations. Such a limited sample may fail to capture the full spectrum of edge cases and annotation disagreements across the 50 diverse cases evaluated in the main study.\n\nThe relatively low inter-annotator agreement for certain tasks (e.g., Krippendorff’s $\\alpha$ = 0.32 for style ratings) questions the reliability of these annotations as ground-truth references.\n\nAlthough the study aims to minimize data contamination by using mostly 2025 cases, it still includes five pre-2024 cases in the 512K-token bin due to “limited availability.” This exception weakens the overall contamination-control claim.\n\nThe GAVEL-REF metric (Equation 2) employs fixed hyperparameters ($\\alpha$ = 0.9) without justification or sensitivity testing. While the dynamic weighting between checklist and residual facts based on proportion r appears intuitively reasonable, the lack of empirical validation or ablation studies leaves uncertainty about whether this configuration truly captures optimal summary quality or affects model rankings. This also precludes reproducibility.\n\nAlthough the authors claim that GAVEL-AGENT is “fully customizable” for other domains, all experiments, evaluations, and validations are confined to U.S. civil rights cases from a single dataset (CRLC). The 26 checklist items are highly domain-specific, and the study offers no evidence that the framework, metrics, or agent scaffolding would generalize effectively to other legal systems, document types, or domains such as biomedical or financial texts."}, "questions": {"value": "Why is the evaluation on 20 documents only? Is this a statistically large sample to convey any meaningful insights?\n\nWhy are the inter-annotator agreements on the lower side? Is the task complex/subjective? Does it not question the evaluation correctness?\n\nWhy is $\\alpha$ set to 0.9?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DThS85b1Vk", "forum": "ZfoayTLZPl", "replyto": "ZfoayTLZPl", "signatures": ["ICLR.cc/2026/Conference/Submission14826/Reviewer_U3Kr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14826/Reviewer_U3Kr"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990814800, "cdate": 1761990814800, "tmdate": 1762925174070, "mdate": 1762925174070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}