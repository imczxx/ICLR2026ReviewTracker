{"id": "4Ha2srdhPN", "number": 16028, "cdate": 1758258834056, "mdate": 1759897266588, "content": {"title": "GRAID: Enhancing Spatial Reasoning of VLMs through High-Fidelity Data Generation", "abstract": "Vision Language Models (VLMs) achieve strong performance on many vision-language tasks but fail at spatial reasoning$\\textemdash{}$a prerequisite for many applications. Current training data generation pipelines have significant limitations: single-image 3D reconstruction introduces cascading modeling errors and requires wide answer tolerances, while caption-based methods require hyper-detailed annotations and suffer from generative hallucinations.\nWe present GRAID, built on the key insight that qualitative spatial relationships can be reliably determined from 2D geometric primitives alone. By operating exclusively on 2D bounding boxes from standard object detectors, GRAID avoids both 3D reconstruction errors and generative hallucinations, resulting in datasets that are of higher quality than existing tools that produce similar datasets as validated by human evaluations. We use our framework to implement 22 question templates spanning spatial relations, counting, ranking, and size comparisons, achieving 91\\% human-validated accuracy$\\textemdash{}$compared to below 59\\% by current methods.\nWe generate 8.5+ million high-quality VQA pairs using images from the BDD100k, NuImages, and Waymo datasets. Critically, we demonstrate that when trained on GRAID data, models do not simply memorize templates, but rather, learn spatial reasoning concepts that generalize and transfer: models fine-tuned on 6 question types improve on 10+ held-out question types, with accuracy gains of 47.5\\% on BDD and 37.9\\% on NuImages for Llama 3.2B 11B, and when trained on all questions types, achieve improvements on several existing benchmarks such as BLINK. \nWe will release the GRAID framework and datasets to accelerate research on VLM spatial reasoning.", "tldr": "A framework that generates high-quality Spatial Reasoning VQA datasets. Human evals show validity rates in the 90s and far above existing methods", "keywords": ["Vision Language Models", "Spatial Reasoning", "Synthetic Data Generation", "Human Validation", "Fine-tuning", "Multimodal Learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b30d32d54790f2cd602d5472e75837015db0fbf8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces GRAID, a framework that generates high-fidelity, qualitative spatial reasoning VQA data using only 2D bounding boxes and a predicate-based accelerator (SPARQ), avoiding errors from single-view 3D reconstruction and LLM-generated captions. Across BDD100k, NuImages, and Waymo, it produces 8.5M+ VQA pairs with ~91% human-validated accuracy and, after SFT on Llama 3.2-Vision-11B, yields strong generalization to unseen question types and external benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear and compelling motivation: the paper precisely identifies why prior pipelines underperform—single-image 3D reconstruction compounds modeling errors and forces wide tolerances, while caption-based methods demand hyper-detailed annotations and inherit generative hallucinations—and proposes a clean 2D, qualitative alternative.\n\n- RQ1 in Section 5 is crucial and is lacking in many current approaches."}, "weaknesses": {"value": "- Incomplete experiments\n  - The benchmarks used for testing are too few.\n  - More spatial understanding benchmarks are needed, especially to show that autonomous driving data can generalize to indoor scenes.\n  - Only Llama-3.2-Vision-Instruct was fine-tuned, which limits the persuasiveness of the results.\n  - There is no comparison against any methods specifically designed to improve VLM spatial reasoning (at the capability-improvement level).\n- Presentation issues\n  - The paper includes many unnecessary details in the main text that take up substantial space.\n  - There are too few experimental results.\n  - There is no table presenting results in the main text.\n  - Some images in the appendix are blurry."}, "questions": {"value": "- Add comparisons against methods explicitly aimed at improving VLM spatial reasoning (at the capability-improvement level).\n- Expand the benchmarking suite with more spatial understanding benchmarks, including indoor scene datasets, to assess cross-domain generalization.\n- Move non-essential implementation details to the appendix to improve clarity and flow.\n- Fine-tune and report results on additional base models beyond Llama-3.2-Vision-Instruct to strengthen evidence.\n- Provide clearer figures in the appendix.\n- Report more experimental results, including ablations on predicate sets, sensitivity to box quality, and training data size."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QicgcNWG58", "forum": "4Ha2srdhPN", "replyto": "4Ha2srdhPN", "signatures": ["ICLR.cc/2026/Conference/Submission16028/Reviewer_8c6y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16028/Reviewer_8c6y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760695669296, "cdate": 1760695669296, "tmdate": 1762926232083, "mdate": 1762926232083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a data generation pipeline to enhance the spatial reasoning capability of VLMs. In contrast to previous method, the proposed pipeline only relies on 2D bounding box detections (and monocular depth models in some cases), and only generate questions asking about qualitative spatial relationships. Therefore, the quality of the VQA pairs is much higher (confirmed by human studies). The authors run their data pipeline on 3 large-scale autonomous driving datasets to generate data for SFT on VLMs. Experiment results are very promising, as the finetuned model works significantly better on existing benchmarks such as BLINK."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a simple yet effective framework to generate high-quality data from only 2D bounding boxes. Although I don't find the data generation pipeline itself to be novel, it does solve the core problems of low data quality of the previous spatial VQA datasets in a simple and intuitive way.\n2. The experiments conducted are very sound and support the claim. I'm especially impressed by the human studies showing the flaws of previous VQA datasets, and the 95% accuracy of the proposed dataset. It is also impressive that the model finetuned on SPARQ can generalize to datasets such as BLINK, which has a large domain gap from the autonomous driving datasets that the model is finetuned on. This shows a good potential of the proposed approach."}, "weaknesses": {"value": "1. My main concerns of the proposed pipeline is that it is only evaluated on autonomous driving datasets. The authors claim in section 3.1 that GRAID can also work on detection-model-generated bounding boxes, but it is unclear how much the data quality will degrade when switching from GT detections to model detections. Therefore, I'm concerned about the generalization of the proposed method beyond autonomous driving scenes. One possible experiment the authors can do is: similar to L339-389, the authors can ask humans to judge the QA pair quality, but with bounding boxes from model detection."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The authors do human studies to evaluate the VQA data quality of several datasets. It is possible that the image content from some of them could make the human judge uncomfortable. Therefore, ICLR should check if the authors have obtained IRB approval or other similar regulations on human studies."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YOiQl9T4qa", "forum": "4Ha2srdhPN", "replyto": "4Ha2srdhPN", "signatures": ["ICLR.cc/2026/Conference/Submission16028/Reviewer_eQNk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16028/Reviewer_eQNk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934171413, "cdate": 1761934171413, "tmdate": 1762926231776, "mdate": 1762926231776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a way to build datasets and uses it to create an 8.5M-sample dataset for driving scenario, which focuses more on spatial relationships. Through experiments, the authors show that training on this dataset improves accuracy on the driving situation and also boosts performance on other external benchmarks that test spatial awareness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This dataset is very large and seems to be high-quality. If the authors open-source it, it would be a great help to the community. It's also impressive that even though the data is only from the driving domain, it helps improve performance on general tasks."}, "weaknesses": {"value": "1. The method in Algorithm 1 naively uses 2D bounding box alignment to infer \"left/right\" relationships, ignoring perspective. This is likely to introduce significant label noise in driving scenes by misinterpreting 3D \"front/back\" configurations as 2D \"left/right\" ones, leading to dataset inaccuracies.\n2. I'm concerned about whether a detector like YOLO can actually tell apart different objects of the same type. For example, can it handle several cars that look almost identical? This must happen all the time in driving scenes. If the detector can't distinguish between them, how can the system create correct questions about their relationships? This seems like a major issue.\n3. The reliance on an object detector for answer generation means that detection errors directly create factual inaccuracies (hallucinations) in the dataset. The paper lacks a quantitative evaluation of the answer generation accuracy."}, "questions": {"value": "Please refer to the issues detailed in the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6WWvhL9q1t", "forum": "4Ha2srdhPN", "replyto": "4Ha2srdhPN", "signatures": ["ICLR.cc/2026/Conference/Submission16028/Reviewer_kpWh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16028/Reviewer_kpWh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977928982, "cdate": 1761977928982, "tmdate": 1762926231335, "mdate": 1762926231335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GRAID, a data-generation framework that improves VLM spatial reasoning by operating on 2D detector boxes—avoiding single-view 3D reconstruction errors and caption-driven hallucinations. It scales via the proposed predicate-based engine (SPARQ) to create 8.5M+ VQA pairs across BDD100k, NuImages, and Waymo, with ~91% human-validated accuracy versus 57.6% for a recent pipeline. Fine-tuning on GRAID data yields some accuracy gains."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "I believe improving the spatial reasoning abilities for VLM is important. I am surprised this pipeline described in this paper hasn't not been proposed (if true). Overall, I believe leveraging 2D models on this purpose technical sounds.\n\nTheir experimental results show that there are some cross-type transfer (e.g., training on 6 question types improves >10 held-out types), and also boosts public benchmarks such as BLINK and A-OKVQA. \n\nAlso, I appreciate the human validation results."}, "weaknesses": {"value": "I was surprised to find that such a pipeline has not been studied before—or perhaps I am just not familiar with the relevant literature. I will double-check the related works and with other reviewers on this purpose. \n\nThe proposed template-based tasks are definitely limited the expression abilities for the datasets and the diversity. \n\nEven 91% is not desirable in my mind for dataset quality, and particularly, the spatial reasoning tasks shown in the paper is not that challenging. \n\nExtending the verified experiments beyond llama-3.2B 11B are helpful.\n\nAlso, the writing and figuring need to be improved."}, "questions": {"value": "Plz address the concerns raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "k9aHnXawoR", "forum": "4Ha2srdhPN", "replyto": "4Ha2srdhPN", "signatures": ["ICLR.cc/2026/Conference/Submission16028/Reviewer_Yb6x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16028/Reviewer_Yb6x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065966548, "cdate": 1762065966548, "tmdate": 1762926230953, "mdate": 1762926230953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}