{"id": "qsSIBK6nJX", "number": 6730, "cdate": 1757993734572, "mdate": 1763635343810, "content": {"title": "Glance and Focus Reinforcement for Pan-cancer Screening", "abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. We conduct training and validation on a large-scale pan-cancer dataset comprising 5,117 CT scans. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD). In addition, through discarding redundant regions, GF-Screen reduces the computation costs by 5.7 times, significantly improving inference efficiency. The superior performance of GF-Screen remarks a novel and practical breakthrough in pan-cancer screening. Codes will be available.", "tldr": "Inspired by radiologists' diagnostic strategy, we propose a Glance-and-Focus reinforcement learning framework for pan-cancer screening: localize tiny lesions in large CT and discard healthy regions, improving efficiency and reducing false positives.", "keywords": ["Pan-cancer screening", "AI for healthcare"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/676af555aaf71c77dddaf770ee4a0860c7cf2ea6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a novel end-to-end reinforcement learning framework for cancer screening. The proposed method consists of a sliding window-based approach (glance) to first identify CT sub volumes and a focus model to segment the lesions. This glance-and-focus approach demonstrated superior performance on multiple internal and public datasets, as well as the Miccai FLARE25 challenge."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The idea of applying group relative learning for sub-volume selection is innovative. Moreover, the reward design that incorporates segmentation results as reinforcement feedback is well-motivated. This strategy effectively distinguishes sub-volumes with partial lesion overlap, enabling more accurate and meaningful reward signals for reinforcement learning.\n\n2. The proposed method is fast in inference by discarding redundant regions, which reduces computational cost.\n\n3. The experimental evaluation is comprehensive, including comparisons with multiple strong baselines and demonstrating consistent superior performance across all tasks."}, "weaknesses": {"value": "1. Equation 4 has some jump in thoughts. I understand that it comes from the second order Taylor expansion of KL divergence and serves as a surrogate of KL divergence. However, it is not straightforward to readers. I recommend explaining this jump in the supplementary materials.\n\n2.The evaluation datasets only contain abdominal CT images. It is unclear how the model performs on other parts of the body, e.g., head CT. \n\n3. It would be beneficial to the paper to present the protocols and device of the CT images in use. This could benefit the robustness of the proposed method."}, "questions": {"value": "1. How is the size of sliding window decided? Different tumors, for example, liver tumor and lung tumor, differentiate in size. For the same type of tumor, it could also have inter-patient variations.\n\n2. Did the author perform any statistical analysis of the results in Tables 2 and 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UMfGohkEZn", "forum": "qsSIBK6nJX", "replyto": "qsSIBK6nJX", "signatures": ["ICLR.cc/2026/Conference/Submission6730/Reviewer_Cyrm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6730/Reviewer_Cyrm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511093053, "cdate": 1761511093053, "tmdate": 1762919017473, "mdate": 1762919017473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to AC and Reviewers"}, "comment": {"value": "We sincerely appreciate the AC and reviewers for your dedicated efforts in reviewing, which have significantly improved the quality of our manuscript. **We are genuinely encouraged by the generous commendations and positive feedback from the reviewers**:\n\n- Reviewer HUYR: *\"a novel framework, elegant and effective, truly remarkable achievement, addresses a critical bottleneck, practical breakthrough\"*.\n- Reviewer xJb3: *\"novel and meaningful way, clever conceptual and algorithmic adaptation, well-defined pipeline, clinically meaningful, strong validation, convincing quantitative results, notable contribution for real-world clinical deployment\"*.\n- Reviewer bZ4d: *\"practical step in accurate, scalable, and efficient pan-cancer screening, easy to follow, extensive evaluation\"*.\n- Reviewer Cyrm: *\"innovative and well-motivated idea, comprehensive evaluation, superior performance\"*.\n\nWe apologize that we overlooked highlighting some important parts in our main paper and appendix previously, which may have led to some reasonable concerns of our reviewers. Following your valuable comments, we have made thorough revisions to our manuscript. We are eager to engage with your professional insights in the rebuttal, which we believe represents an invaluable opportunity for academic exchange."}}, "id": "LFgSK2HeQj", "forum": "qsSIBK6nJX", "replyto": "qsSIBK6nJX", "signatures": ["ICLR.cc/2026/Conference/Submission6730/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6730/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6730/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763635456319, "cdate": 1763635456319, "tmdate": 1763635456319, "mdate": 1763635456319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents GF-Screen, a Glance-and-Focus reinforcement learning framework for pan-cancer screening in CT scans, inspired by radiologists’ diagnostic workflow. A Glance model identifies suspicious sub-volumes, and a Focus model segments lesions, with segmentation results rewarding the Glance model via a group relative learning strategy. Trained on 5,117 CT scans and evaluated across 23 datasets (9 lesion types), GF-Screen leads the MICCAI FLARE25 leaderboard, surpassing the prior champion by +25.6% DSC and +28.2% NSD, while reducing computation by 5.7× through efficient region selection. This marks a practical step in accurate, scalable, and efficient AI-based pan-cancer screening."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors effectively reframe the well-established coarse-to-fine paradigm as a \"glance-and-focus\" strategy, an evocative and clinically intuitive framing that demonstrates strong scientific storytelling.\n2. The figures are clean, well-designed, and complemented by clear, concise textual descriptions, making the methodology easy to follow.\n3. The empirical evaluation is extensive, with experiments across multiple internal and external datasets and diverse lesion types, providing thorough support for the paper’s claims."}, "weaknesses": {"value": "1. The motivation for using reinforcement learning (RL) to train the Glance model is underdeveloped. While extreme class imbalance is acknowledged as a key challenge, the authors do not sufficiently justify why conventional approaches, such as intelligent sampling, focal loss, or hard-negative mining, would be inadequate. Jumping directly to RL, a currently popular but complex paradigm, feels more like a methodological trend than a necessity.\n2. Within the RL framework, using segmentation performance from the Focus model as the reward signal for the Glance model introduces undesirable coupling between the two stages. Specifically, a high segmentation score may simply reflect an \"easy\" sub-volume (e.g., one with clear boundaries or canonical orientation), not necessarily one that was optimally cropped. Conversely, a low segmentation score might indicate a challenging but clinically critical case (e.g., partial lesions or suboptimal viewing angles), precisely the cases the Glance model should prioritize. The current reward scheme risks biasing the Glance model toward easy regions and discarding hard, ambiguous, yet important ones.\n3. The paper’s layout could be better optimized for impact. Given space constraints, Table 1 (dataset statistics) could be moved to the supplementary material to free up room for more critical content, particularly ablation studies and a dedicated conclusion section, which is notably absent and atypical for a conference submission.\n4. From a methodological standpoint, the work primarily constitutes an application of the GRPO algorithm to a new domain, rather than a significant algorithmic advance."}, "questions": {"value": "1. The term \"data curation\" is used, but simply aggregating public datasets does not constitute curation. Could the authors clarify their curation process? Specifically, how were label inconsistencies or annotation discrepancies across source datasets identified and resolved?\n2. Regarding the FLARE25 challenge results: the reported margin over the second-place method is substantial. Was this comparison conducted under equal conditions? In particular, did all participants use the same training data? Does the challenge permit the use of external data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SnKPBXsyJy", "forum": "qsSIBK6nJX", "replyto": "qsSIBK6nJX", "signatures": ["ICLR.cc/2026/Conference/Submission6730/Reviewer_bZ4d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6730/Reviewer_bZ4d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549854148, "cdate": 1761549854148, "tmdate": 1762919016958, "mdate": 1762919016958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GF-Screen, a novel reinforcement learning (RL) framework designed for pan-cancer screening from large-scale CT scans. The method mimics the radiologist’s diagnostic process — “glance” to locate suspicious regions and “focus” to perform detailed lesion segmentation. GF-Screen integrates a Glance model for sub-volume selection and a Focus model for segmentation. The Glance model is optimised through a Group Relative Learning (GRL) paradigm, where rewards are derived from segmentation outcomes of the Focus model. Experiments on 5,117 CT scans from 23 public datasets, covering nine lesion types, show substantial improvements over previous approaches, achieving the top rank on the MICCAI FLARE25 challenge, with reported +25.6% DSC and +28.2% NSD compared to the FLARE24 champion solution. The paper emphasises efficiency gains (5.7× faster inference) and reduced false positives through selective attention to diseased regions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths\n1. The “glance and focus” paradigm is a clever conceptual and algorithmic adaptation of clinical diagnostic reasoning to AI-based screening.\n2. The paper provides a well-defined pipeline linking reinforcement learning and segmentation, with precise mathematical formulations for the GRL optimisation objective.\n3. Addressing pan-cancer screening rather than single-organ segmentation is highly ambitious and clinically meaningful.\n4. The study presents extensive experiments on both internal and external datasets, including strong validation on the FLARE25 leaderboard.\n5. The demonstrated 5.7× reduction in computational cost without accuracy loss is a notable contribution for real-world clinical deployment.\n6. The quantitative results convincingly show that GF-Screen outperforms a range of established baselines, including nnUNet, SwinUNETR, VoCo, and PASTA.\n7. The adaptation of GRL to vision-based medical tasks is novel and extends recent RL developments in a meaningful way."}, "weaknesses": {"value": "Weaknesses\n1. While the proposed framework is well-engineered, its novelty may primarily lie in combining existing ideas (sub-volume selection, segmentation, RL reward optimisation) rather than introducing a fundamentally new theoretical concept.\n2. The reinforcement learning formulation, especially the reward design and advantage estimation, is largely empirical. There is little discussion of convergence, stability, or theoretical motivation for using group relative learning in medical imaging.\n3. The paper would benefit from more detailed ablation studies isolating the effects of the RL component, the GRL mechanism, and the segmentation backbone. Interpretability analyses (e.g., qualitative localisation or attention visualisations) are limited.\n4. While the authors state that code will be released, reproducibility remains uncertain. Essential hyperparameters (e.g., coefficients α, β, learning rates, or update frequency of G_ref) should be clearly stated in the main paper rather than deferred to the appendix.\n5. The dataset curation process aggregates 23 datasets with differing acquisition protocols and lesion distributions. Without normalisation or harmonisation details, there is a risk that model performance may rely on dataset-specific biases.\n6. The training process of the Glance model may be unstable due to non-differentiable sub-volume selection, but the paper does not report training curves or sensitivity analyses beyond one brief figure.\n7. Despite strong quantitative results, the absence of clinician-in-the-loop evaluation or human–AI comparison weakens the practical validation of the proposed clinical analogy.\n8. The paper is technically dense, with long paragraphs and numerous references, which may hinder readability for non-specialist reviewers. Some restructuring and pruning would improve clarity."}, "questions": {"value": "plz see my detaild comments above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nv3Z24Sn2x", "forum": "qsSIBK6nJX", "replyto": "qsSIBK6nJX", "signatures": ["ICLR.cc/2026/Conference/Submission6730/Reviewer_xJb3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6730/Reviewer_xJb3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964696253, "cdate": 1761964696253, "tmdate": 1762919016474, "mdate": 1762919016474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GF-Screen, a novel framework for pan-cancer screening in large-scale CT volumes. The core problem addressed is the significant challenge of locating small, diverse lesions within vast, mostly healthy, anatomical regions—an issue of extreme foreground-background imbalance. The proposed solution is inspired by the \"glance and focus\" strategy of radiologists. It consists of two main components: a Glance model that acts as a selector to identify promising sub-volumes, and a Focus model that performs detailed segmentation only on the selected regions. The key technical contribution is the method for training the Glance model. Since the selection action is non-differentiable, the authors employ a RL approach where the Focus model's segmentation output provides a reward signal to the Glance model. A novel GRL paradigm is introduced to optimize this policy. The method is validated on a large, diverse dataset aggregated from 23 public sources and shows outstanding results, notably leading the MICCAI FLARE25 challenge validation leaderboard by a significant margin and demonstrating a 5.7x reduction in computation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The results are the most significant strength. Leading the FLARE25 challenge validation leaderboard and outperforming the FLARE24 champion solution by such a large margin (+25.6% DSC) is a truly remarkable achievement.\n2. The method directly addresses a critical bottleneck in deploying AI screening models: computational cost and false positives. By intelligently discarding healthy regions, the 5.7x computational saving is a major practical breakthrough, making large-scale screening far more feasible.\n3. The use of RL to couple a detection/localization network (Glance) with a segmentation network (Focus) is elegant. The segmentation result (a binary overlap reward ) is used as a rich, task-specific feedback signal for the localization model, which is more sophisticated than training the localizer on simple binary \"lesion present/absent\" labels. The proposed Group Relative Learning (GRL) appears effective, as shown in the ablations."}, "weaknesses": {"value": "1. The paper introduces GRL as a \"novel\" paradigm, but its precise mechanism and distinction from existing policy gradient methods (like PPO, which is cited) could be clearer. The loss function in Eq. 3 looks like a PPO-style clipped objective. The novelty seems to be in the advantage estimation or the \"group relative comparison\", but this specific aspect is not explained in sufficient detail. This lack of clarity is particularly problematic concerning the core RL mechanism. For instance, the paper needs to explicitly detail the action space. The Glance model's role is to select (1) or discard (0) a sub-volume, which is a discrete, non-differentiable step. The paper should clarify how this binary decision is derived from the policy network. Is the model's output 'o' (as referenced around line 206) a probability (e.g., a sigmoid output) from which an action is sampled? If 'o' is treated as a discrete 0/1 output directly, it's unclear how the policy gradient can be backpropagated for optimization. A more detailed explanation is required of what exactly constitutes the 'action' in the RL framework (e.g., the binary selection), how this action is stochastically sampled from the policy during training to enable exploration, and how the underlying policy (the probability distribution over actions) is updated via the GRL objective.\n2. The primary justification for the RL approach is the failure of a standard classification (CE) baseline . However, the paper does not specify how this baseline was trained. Given the stated problem of \"severe foreground-background imbalance\", a simple CE loss is expected to fail. A fair comparison would require comparing GRL to a strong classification baseline"}, "questions": {"value": "See weakness 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T4xf0qpaAU", "forum": "qsSIBK6nJX", "replyto": "qsSIBK6nJX", "signatures": ["ICLR.cc/2026/Conference/Submission6730/Reviewer_HUYR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6730/Reviewer_HUYR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999621762, "cdate": 1761999621762, "tmdate": 1762919015579, "mdate": 1762919015579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}