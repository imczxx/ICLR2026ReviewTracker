{"id": "yFq9L5pTHY", "number": 18332, "cdate": 1758286504247, "mdate": 1759897110168, "content": {"title": "Geometric IB: Improving Information Bottleneck with Geometry-Aware Compression on Statistical Manifolds", "abstract": "We revisit the Information Bottleneck (IB) through the lens of information geometry and propose a Geometric Information Bottleneck (G-IB) that dispenses with direct mutual information (MI) estimation. We show that mutual information $I(X;Z)$ and $I(Z;Y)$ admit exact projection forms as minimal Kullback–Leibler (KL) distances from the joint distributions to their respective independence manifolds. Guided by this view, G-IB controls information compression with two complementary terms: (i) a distribution-level Fisher–Rao (FR) discrepancy, which matches KL to second order and is reparameterization-invariant; and (ii) a geometry-level Jacobian–Frobenius (JF) term that provides a local capacity-type upper bound on $I_\\phi(Z;X)$ by penalizing pullback volume expansion of the encoder. We further derive a natural-gradient optimizer consistent with the FR metric, proving the first-order equivalence between the geodesic update and the standard additive natural-gradient step. We conducted extensive experiments and observed that the G-IB achieves a better trade-off between prediction accuracy and compression ratio in the information plane than the mainstream IB baselines on popular datasets. G-IB offers a principled and scalable alternative that unifies distributional and geometric regularization under a single bottleneck multiplier, improving invariance and optimization stability. The source code of G-IB is released at \\url{https://anonymous.4open.science/r/G-IB-0569}.", "tldr": "We revisit the Information Bottleneck (IB) through the lens of information geometry and propose a Geometric Information Bottleneck (G-IB) that dispenses with direct mutual information (MI) estimation.", "keywords": ["Information bottleneck; information geometry"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9097c9a4af3e9a734c57392292aba8404d902d37.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Geometric Information Bottleneck (G-IB), which reinterprets the Information Bottleneck (IB) framework through the lens of information geometry. The authors claim to avoid direct mutual information estimation by introducing two geometry-aware regularizations. G-IB achieves a better trade-off between prediction accuracy and compression ratio."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of combining geometric considerations (via Fisher–Rao metrics) with IB is interesting and theoretically motivated."}, "weaknesses": {"value": "1. Extra computation for surrogates: computing JF uses Hutchinson probes + JVPs (adds forward-mode calls per sample). FR requires either closed-form KL or the FR distance proxy.\n\n2. Heavier optimizer: They propose natural-gradient updates via K-FAC with CG solves (Algorithm 1), which is more involved than standard Adam/SGD. No comparison is made with standard optimizers like Adam or SGD to demonstrate practical benefit.\n\n3. The reported accuracy gains (Table 1) are minor and inconsistent. For instance, on CelebA, G-IB underperforms compared to SIB, and the improvements on MNIST and CIFAR-10 are negligible.\n\n4. All datasets used (MNIST, CIFAR10, CelebA) are low-complexity and saturated benchmarks. There is no evidence that the proposed method scales to modern architectures or large-scale data (e.g., ImageNet)."}, "questions": {"value": "1. Please include experiments on more challenging datasets such as CIFAR-100 and Tiny-ImageNet to better demonstrate the scalability and generalization ability of the proposed method. In addition, an evaluation of adversarial robustness would significantly strengthen the empirical claims.\n\n2. The paper should discuss and compare with several closely related approaches, including NIB (Nonlinear Information Bottleneck) [1], DIB [2] (Deep Deterministic Information Bottleneck), and HSIC-Bottleneck [3], which are all relevant to the proposed framework.\n\n[1] Kolchinsky, Artemy, Brendan D. Tracey, and David H. Wolpert. \"Nonlinear information bottleneck.\" Entropy 21.12 (2019): 1181.\n\n[2] Yu, Xi, et,al. \"Deep deterministic information bottleneck with matrix-based entropy functional.\" ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021.\n\n[3] Ma, Wan-Duo Kurt, J. P. Lewis, and W. Bastiaan Kleijn. \"The HSIC bottleneck: Deep learning without back-propagation.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 04. 2020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hEcol9Q4w2", "forum": "yFq9L5pTHY", "replyto": "yFq9L5pTHY", "signatures": ["ICLR.cc/2026/Conference/Submission18332/Reviewer_E6Kx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18332/Reviewer_E6Kx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624893644, "cdate": 1761624893644, "tmdate": 1762928045381, "mdate": 1762928045381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors revisit the Information Bottleneck (IB) through information geometry. They show that both information terms can be written as minimal KL distances from the joint to independence sub-manifolds (“Pythagorean” projection view). Building on this, they propose Geometric IB (G‑IB), which avoids explicit MI estimation at training time and instead controls compression with two terms; Fisher–Rao discrepancy and Jacobian–Frobenius. G‑IB is compare to VIB, MINE‑based IB, a structured IB (SIB), and adversarial IB (AIB) on several classic vision datasets, demonstrating G-IBs consistent performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Originality**: the Geometric IB paper's primary contribution is the explicit synthesis and formalization of these concepts into a coherent, geometry-aware framework specifically designed to solve the Information Bottleneck problem.\n\n- **Quality**: the work presents strong theoretical foundation and accompanied by empirical validation. The reformulation of MI as a geometric projection provides a principled basis for the rest of the work. The derivations of the geometric penalties are well-motivated, and connected to the suggested optimization setup. Experiments span relevant baselines (VIB, MINE, SIB, AIB) on standard benchmarks (MNIST, CIFAR10, CelebA), and provide desired analysis for the IB setting -- the information plane. \n\n- **Clarity**: the paper construction is clear and easy to follow, the problem is motivated and the theoretical background/setting is provided in a an understoodable manner -- even for non-expert readers. \n\n- **Significance**: training without direct MI estimation alleviates known issues for IB based optimization."}, "weaknesses": {"value": "- **Scalability**: the authors claim for scalability, however computation is still quite expensive-- actual applicable scales are not presneted.\n- **Contributions**: Experiments are conducted only on classical vision datasets, presenting minimal improvement.\n- **Geometric motivation**: following the previous point, geometric aspects / problems are not explored within provided experiments."}, "questions": {"value": "Following the above weaknesses, could the authors:\n1. elaborate on scalability;\n2. extend the experiments to additional cases better representing the contribution and potentially highlighting the geometric aware setting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2HEYiwa7a1", "forum": "yFq9L5pTHY", "replyto": "yFq9L5pTHY", "signatures": ["ICLR.cc/2026/Conference/Submission18332/Reviewer_iDY5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18332/Reviewer_iDY5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633974656, "cdate": 1761633974656, "tmdate": 1762928044860, "mdate": 1762928044860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose the Geometric Information Bottleneck (G-IB), a revised version of the well-known Information Bottleneck principle. G-IB controls information compression using two additional terms based on geometric quantities. According to the authors, G-IB achieves a better trade-off between prediction accuracy and compression ratio in the information plane compared to the mainstream IB method (MINE)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Exploring the Information Bottleneck phenomenon from an information geometry perspective appears interesting and promising. Key components of the proposed approach include: 1) a geometry-level Jacobian–Frobenius (JF) penalty that provides a local capacity-type upper bound on $I_{\\phi}(X, Z)$ by discouraging pullback volume expansion; 2) a distribution-level Fisher–Rao (FR) discrepancy that locally matches KL divergence to second order and is invariant under smooth reparameterizations of the latent variable. The geometric perspective on mutual information represents a promising research direction in information theory. However, the paper has several limitations and concerns, detailed below."}, "weaknesses": {"value": "It would be valuable to conduct additional experiments exploring the dependencies between $I(X, Y)$ and $I(X, Z)$ during neural network training and to provide the resulting information planes, similar to those in (Shwartz-Ziv \\& Tishby, 2017).\n\nThe use of the Fisher-Rao metric in a natural Riemannian gradient descent method was first introduced in [1]. Proposition 1 (lines 254–260) in this paper appears to restate Theorem 1 from [1]. Moreover, [2] contains theoretical results that strongly intersect with those presented here: for instance, the proof of local second-order Fisher–Rao approximation (Appendix D) resembles Section 2.1.2 of [2], and Proposition 2 (lines 263–269) is similar to Proposition 1 in [2] (latter part of Section 2.1.2). Overall, [1, 2] already establish the relationship between the Fisher–Rao metric and Kullback–Leibler divergence from the perspective of “shortest path uphill.”\n\nI also have concerns regarding the experimental pipeline. How do the authors verify that the estimated value of $I(X, Z)$ is sufficiently accurate relative to the true mutual information? Table 1 compares several IB benchmarks but does not address the accuracy of MI estimation. It is important to demonstrate that the estimated MI values are good approximations of the true values. I recommend that the authors include experiments on synthetic datasets to provide empirical justification for their approach.\n\nAdditionally, in lines 480–482, the authors state: “we propose a natural gradient that aligns updates with the FR metric and prove that the standard additive natural-gradient step is first-order equivalent to the exponential-map (geodesic) update.” However, similar results have been established in prior works, such as [1, 3].\n\nFurthermore, many MI estimators have been proposed since MINE that should be included in the benchmarking. For example, [4–7] and references therein. To ensure a fair comparison, some recent MI estimation methods should be added to the experimental section. The paper would also benefit from more real-world experiments and a broader discussion of the method’s practical advantages (e.g., scalability and data efficiency).\n\nMinor issues:\n\n - The definition of Fisher information $F(\\theta)$ should be added to the main text (lines 161–162).\n - The quality of Figures 2–5 could be improved (e.g., increase font sizes of axis labels).\n\nReferences:\n\n[1] S. Amari. Natural Gradient Works Efficiently in Learning. Neural Computation, 10(2):251–276, 1998. \n\n[2] Y. Ollivier, L. Arnold et al. Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles. J. Machine Learning Research, 18(18):1–65, 2017. \n\n[3] S. Amari, H. Nagaoka. Methods of Information Geometry, 2000. \n\n[4] T. Nishiyama. A New Lower Bound for Kullback–Leibler Divergence Based on Hammersley–Chapman–Robbins Bound, 2019. \n\n[5] I. Butakov, A. Tolmachev et al. Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression. ICLR, 2024. \n\n[6] I. Butakov, A. Tolmachev et al. Mutual Information Estimation via Normalizing Flows. NeurIPS, 2024. \n\n[7] G. Franzese, M. Bounoua, P. Michiardi. MINDE: Mutual Information Neural Diffusion Estimation. ICLR, 2024."}, "questions": {"value": "1. How do you estimate the convergence rate of your method? Could you comment on the precision of the approximations suggested in Equations (6) and (11)?\n2. Why is $S = 1$ or $S = 2$ (line 220) typically sufficient for good trace estimation?\n3. Why are [1] (Amari, 1998) and [2] not cited, despite the significant overlap in theoretical content? Could you clearly highlight your contributions and theoretical novelty?\n4. How do you verify the quality of MI estimation using your method? Why was MINE chosen over more recent MI estimation approaches?\n5. What is the relationship between minimizing the Fisher–Rao and Jacobian–Frobenius terms in Equation (19)? How do they jointly affect the minimization of the overall G-IB objective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "77GY9NrpGT", "forum": "yFq9L5pTHY", "replyto": "yFq9L5pTHY", "signatures": ["ICLR.cc/2026/Conference/Submission18332/Reviewer_saLk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18332/Reviewer_saLk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776955609, "cdate": 1761776955609, "tmdate": 1762928043854, "mdate": 1762928043854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}