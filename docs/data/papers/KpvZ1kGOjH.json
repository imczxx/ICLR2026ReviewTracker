{"id": "KpvZ1kGOjH", "number": 25307, "cdate": 1758366536912, "mdate": 1759896725679, "content": {"title": "EvoCF: Multi-agent Collaboration with Memory-guided Evolutionary Counterfactual Planning", "abstract": "Planning collaboration strategies for multi-agent embodied systems remains a core challenge for LLM-based planners, which often fail to capture the physical and coordination constraints of real-world environments. To address this, we present \\textbf{EvoCF} (Evolutionary Counterfactual Planning), a memory-guided framework for discovering improved multi-agent collaboration strategies through counterfactual plan generation and evaluation. First, we induce a structured symbolic rule library from failure experiences, encoding reusable constraints of inter-agent dependencies and action feasibility. Then, we propose an evolutionary counterfactual plan generator that systematically explores semantically consistent plan variants through rule-guided mutations. This enables the discovery of robust multi-agent strategies beyond short-sighted LLM plans. Finally, we design an experience-driven evaluator that scores candidate plans along multiple metrics, using retrieval-augmented constraint matching. Across embodied simulation benchmarks, {EvoCF} consistently discovers more robust and executable plans compared to baseline approaches. Our results demonstrate that grounding multi-agent planning in structured memory and symbolic reasoning significantly enhances both reliability and adaptability.", "tldr": "", "keywords": ["Multi-Agent Collaboration", "Long-horizon Planning", "Large Language Models"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00a958c142661486301a48b13f1f3d1e831ce30f.pdf", "supplementary_material": "/attachment/7d2c284df23a2d24160b5de5683222e7fd1b7fa7.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a three part pipeline for multi-agent embodied planning: symbolic constraints induced from failure experiences, counterfactual plan generation using rule guided mutations, and a memory guided evaluator. The structure is clear and the MAP-THOR results are promising. \n\n**However, ablation coverage is incomplete.** The paper does not isolate the contribution of each component, does not study operator sensitivity, and does not report token or latency cost. \n\n**Positioning is also incomplete.** Closely related work in the Multi-LLM Agent Collaborative Intelligence literature is not cited or compared. This includes the MACI book (first edition March 2023, ACM Books release slated for end of 2025), SocraSynth and CRIT (2023) for counterfactual probing and rubric based judging, EVINCE (preprint) for modulation of contentiousness as a control dial in addition to information retrieval, ALAS (preprint) for disruption aware planning and compensation policies, SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning (PVLDB 2025) for persistent execution memory with validation, rollback, and compensation, and A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment (ICML 2025) for counterfactual reasoning in ethics alignment. \n\nPlease add a clear comparison and a novelty paragraph that states what is new here beyond those items, and provide module-level ablations, scaling beyond two agents, transfer to larger layouts, and basic cost reporting. See details in the rest of this review.\n\n**Provisional rating pending rebuttal.**"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Clear modularization: rule induction from failures, rule guided counterfactual generator, memory guided evaluator.\n\n* Concreteness: explicit mutation operators and an interpretable rule library that others can reuse.\n\n* Positive results on MAP-THOR with a clean experimental setup."}, "weaknesses": {"value": "**1. Missing citations and positioning:** The paper does not cite or compare against key MACI references that cover the same pillars the paper relies on. Please add and discuss:\n\n* MACI book, first edition March 2023, ACM Books release slated for 2025.\n\n* SocraSynth and CRIT (2023): counterfactual probing and rubric based judging.\n\n* EVINCE (preprint): modulation of contentiousness to balance exploration and convergence in addition to information retrieval.\n\n* ALAS (preprint): disruption aware planning with explicit compensation policies.\n\n* SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning (PVLDB 2025): persistent execution memory with validation, rollback, and compensation.\n\n* A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment (ICML 2025): counterfactual reasoning applied to ethics alignment.\n\n**2. Ablation coverage is incomplete:** no direct isolation of the symbolic constraint inductor; no sensitivity to rule-library size or quality; no operator-level analysis (swap, insert, delete, replace); no study of memory retrieval depth or neighborhood size; no comparison of evaluator variants beyond a single proxy; no discussion of failure modes tied to each module.\n\n**3. Limited scope:** results focus on two agents and AI2-THOR layouts. Scaling to more agents, larger environments, or real robots is not demonstrated. Transfer of induced rules across task families is not tested.\n\n**4. No cost reporting:** token counts and wall-clock latency for generation, mutation search, and evaluation are missing.\n\n**5. Limitations:** there is no dedicated limitations section that addresses generalization beyond hand-designed operators, robustness under perception noise or tool errors, and brittleness of induced rules.\n\n**Provisional recommendation** pending a clear novelty statement relative to MACI and the requested ablations, scaling, transfer, cost, and limitations. I will revisit after rebuttal.  **The MACI book is in the public domain.**"}, "questions": {"value": "**Encouragement to authors**\n\nThe core pipeline is clear and the MAP THOR results are promising. Although the current ablations are limited, a stronger related work section can materially improve the paper. Explicitly citing and positioning against the MACI literature will help readers see this work as a focused engineering integration rather than uncredited reinvention. It also lets you highlight what is uniquely yours in this setting, for example the specific mutation operators, the failure induced rule library, and the MAP THOR integration.\n\n1. Cite and compare against the MACI line listed above, and state precisely what is new here?\n\n2. Provide module-level ablations: rule induction on vs off with the same generator and evaluator; operator sensitivity; memory retrieval depth; evaluator variants with and without retrieval.\n\n3. Report token usage and latency for each phase (your competing papers did)\n\n4. Evaluate scaling to more agents and larger environments, and test cross-domain transfer of induced rules.\n\n5. Add a dedicated limitations section that covers generalization, robustness, and failure modes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Saquplbug9", "forum": "KpvZ1kGOjH", "replyto": "KpvZ1kGOjH", "signatures": ["ICLR.cc/2026/Conference/Submission25307/Reviewer_WJsZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25307/Reviewer_WJsZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761094618178, "cdate": 1761094618178, "tmdate": 1762943397419, "mdate": 1762943397419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EvoCF, a new framework for multi-agent collaboration planning that addresses the limitations of current LLM-based planners in complex environments. EvoCF leverages a structured memory of past experiences—particularly failures—to induce symbolic constraints that represent critical inter-agent dependencies and action feasibility. Using evolutionary algorithms, it systematically generates and explores alternative joint plans through counterfactual mutations, guided by these learned rules. A memory-driven evaluator then scores each candidate plan for robustness, efficiency, and coordination. Experimental results on simulation benchmarks show that EvoCF consistently produces more reliable, executable, and adaptable plans than existing state-of-the-art approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors present a creative integration of structured symbolic memory, rule induction from failure experiences, and evolutionary counterfactual planning for multi-agent collaboration. The combination distinguishes it from prior work. EvoCF is original in its systematic exploration of alternative joint plans using learned constraints, moving beyond conventional single-shot or heuristic LLM-based planning. Introducing explicit counterfactual reasoning and evolutionary search into the LLM multi-agent planning paradigm represents a fresh perspective. Overall I think the paper is highly original in its problem framing and solution design, maintains methodological quality and sound experimental validation, and communicates its ideas clearly."}, "weaknesses": {"value": "- The symbolic rule induction process appears fundamentally reliant on the presence and diversity of failure cases in structured memory, yet the paper does not address how EvoCF copes with sparse, unbalanced, or noisy episodic data. This should be a common scenario as environments grow complex or as agents encounter novel tasks. \n- The constraint induction mechanism, while formally defined, seems constrained to a set of relatively simple precondition and coordination rules, potentially missing high-level, non-obvious dependencies or temporally extended causal relations essential for advanced multi-agent coordination. Mutation operators in the evolutionary plan generator are limited to a small, discrete set of modifications (e.g., SwapAgent, InsertAction), which may not be sufficiently expressive or efficient to navigate the combinatorial explosion of plausible joint plans as agent and action space scales, or to handle rich, long-horizon tasks.\n- I'm not sure but the reliance on retrieval-augmented plan evaluation side-steps explicit world-modeling may fail catastrophically when retrieved experiences do not closely match the ongoing scenario. Critically, the paper does not report the computational efficiency of evolutionary plan search, nor does it clarify whether the constraint evaluation and plan ranking can be kept tractable in non-trivial settings.\n\nSome of these points may not reflect actual weaknesses, as certain technical details were not explicitly provided in the paper. I would be glad to reconsider my score if the authors can offer further clarification or additional evidence addressing these concerns."}, "questions": {"value": "- Can you provide more detail about the complexity of the simulation environments used in your benchmarks? How many agents, objects, and concurrent dependencies are typical, and how do they compare to real-world multi-agent scenarios?\n- How sensitive is EvoCF to the size, diversity, and quality of the episodic memory? What happens if failure cases are under-represented or the memory buffer is sparse or noisy?\n- What are the computational costs (in terms of time and resources) of EvoCF's evolutionary plan search and retrieval-augmented evaluation, especially as the number of agents or the complexity of tasks increases?\n- Can you share some concrete examples (or just some logs) of induced symbolic rules from real experiments? I'm curious how expressive or general are these rules compared to hand-crafted domain knowledge."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cfGRbGk4qH", "forum": "KpvZ1kGOjH", "replyto": "KpvZ1kGOjH", "signatures": ["ICLR.cc/2026/Conference/Submission25307/Reviewer_1hYG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25307/Reviewer_1hYG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623225123, "cdate": 1761623225123, "tmdate": 1762943397235, "mdate": 1762943397235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EvoCF, a memory-guided framework for discovering improved multi-agent collaboration strategies through counterfactual plan generation and evaluation. Specifically, the framework includes three processes: (i) the Counterfactual Plan Generator, which introduces evolutionary operators to explore diverse constraint-guided alternatives; (ii) the Retrieval-Augmented Counterfactual Evaluator, which grounds these candidates in past outcomes and symbolic constraints to assess their viability; and (iii) the Symbolic Constraint Inductor, which distills failure patterns into reusable rules that accumulate in memory. Experiments on MAP-THOR reflect the effectiveness of EvoCF compared to various baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The reliability of planning among multiple agents is a critical issue in real-world scenarios. This paper addresses this by leveraging historical experiences, constructing rule-based memory, employing retrieval mechanisms tailored to task execution, and dynamically updating memory based on new execution outcomes. This constitutes an effective paradigm of agent evolution. Experimental results also demonstrate a significant improvement of this approach compared to the baseline method."}, "weaknesses": {"value": "1. Section 3 employs an excessive number of concepts and symbols, and some definitions are not sufficiently clear. The relationships between symbols are ambiguous, making it very challenging for readers. For instance, what is the difference between $\\psi$ in line 201 and $\\phi$ in line 236? What distinguishes $\\mathcal{R}$ in line 219 from $\\Psi$ in line 210? How does the concept of memory differ from that of a library in the paragraph corresponding to line 192? How is $\\Psi^{\\rm{rule}}(m)$ operated in line 257, and what does the embedding function $e(.)$ specifically entail? What does the term \"identity information\" refer to in line 250? I recommend that the author avoid arbitrary noun substitutions, use consistent terminology and symbols for the same entity, and provide a complete example illustrating each part of the method with corresponding names and symbols to enhance the readability of the article.\n\n2. In line 058, it is mentioned that one of the motivations of this study is the increasing importance of reliable planning in multi-agent planning as the number of agents grows. However, it appears that the experiments in this study only involve 2-4 agents, which does not adequately demonstrate the scalability of the approach concerning the number of agents.\n\n3. The method in this paper is only validated on one dataset. Would the approach still be effective in a different scenario? Additionally, the experiments in the paper only use one model. How would the approach perform with open-source models?\n\n4. There are some typos in Figure 1: \"Experience-Given\" should be \"Experience-Driven,\" and \"Rlues\" should be corrected to \"Rules.\""}, "questions": {"value": "In many real-world scenarios, the consequences of actions are often irreversible. In such cases, what is the meaning of reflecting on and refining historical experiences? How can we enhance reliability by improving the accuracy of multi-agents' first-time actions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k1K9KbeUrW", "forum": "KpvZ1kGOjH", "replyto": "KpvZ1kGOjH", "signatures": ["ICLR.cc/2026/Conference/Submission25307/Reviewer_HnHQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25307/Reviewer_HnHQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021133107, "cdate": 1762021133107, "tmdate": 1762943396863, "mdate": 1762943396863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors presents EvoCF (Evolutionary Counterfactual Planning), a memory-guided framework designed to address the challenges of planning and coordination for multi-agent embodied systems in partially observable environments. The core contribution is a deliberative, iterative loop that moves beyond single-shot LLM plans by integrating three novel components:\n\n1. Symbolic Constraint Induction: A structured memory records failure experiences and extracts reusable symbolic rules that encode multi-agent coordination requirements and single-agent feasibility constraints.\n2. Evolutionary Counterfactual Plan Generator: This module systematically explores alternative joint action plans by applying mutation operators (e.g., SwapAgent, InsertAction) to a seed plan. Mutations are guided by the retrieved symbolic rules to ensure semantic consistency and task relevance.\n3. Experience-Driven Evaluator: Candidate plans are ranked based on memory-retrieved outcomes and learned symbolic constraints, effectively acting as a world-model-free reasoning layer to anticipate consequences and preemptively avoid failure.\n\n\nExperiments on the MAP-THOR benchmark show EvoCF achieves an $18\\%$ higher success rate than the strong baseline LLaMAR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The framework introduces a novel and robust method for improving multi-agent planning by integrating explicit symbolic rule induction from failure experiences into the LLM planning loop, ensuring that coordination constraints are reusable and transferable across tasks.\n\n• The use of evolutionary counterfactual search is a strong methodological step, moving beyond the inherent limitations of conventional one-shot LLM planners by systematically generating diverse, constraint-guided plan alternatives.\n\n• The paper demonstrates that the induced symbolic constraints encode structurally generalizable knowledge. A dedicated case study confirms that rules learned from one task significantly improve the success rate on structurally different, unseen target tasks.\n\n• The ablation study clearly validates the importance of both core modules: removing either the rule-guided generator or the experience-driven evaluator leads to a notable drop in performance, confirming their synergistic contribution to overall robustness.\n\n• EvoCF achieves significant empirical gains, outperforming state-of-the-art baselines like LLaMAR by $18\\%$ in success rate and substantially improving metrics across transport rate, coverage, and coordination balance in multi-agent embodied scenarios."}, "weaknesses": {"value": "- The Symbolic Constraint Induction process is not fully detailed as a black box. The generator that maps a failure-annotated transition to candidate rules relies on an implicit LLM reasoning step, making the induction quality and reliability difficult to assess or reproduce without knowing the underlying prompting or training.\n- The mutation operators are explicitly stated as manually designed (e.g., `SwapAgent`, `ReplaceObject`), and the ablation study confirms their effectiveness degrades under random selection. This reliance on pre-defined operators limits the approach's generalizability and ability to discover novel, unexpected collaboration strategies outside of a few common plan-editing patterns. Do the authors have experiments/scenarios where they saw this behaviour?\n- The Memory-guided Evaluation relies on retrieving transitions with similar object locations and interaction failures to judge the viability of a counterfactual plan. This suggests the framework's effectiveness may be sensitive to memory density and the quality of the embedding function ($e(\\cdot)$ in Eq. 1) used for retrieval, neither of which is thoroughly analyzed.\n- Scaling beyond three agents introduces diminishing returns, with performance slightly decreasing when moving from three to four agents. This suggests that the current constraint set or the evolutionary search complexity may struggle to manage the increased coordination complexity and load imbalance of larger teams."}, "questions": {"value": "1. Since the Symbolic Constraint Inductor is a critical component, could the authors provide a quantitative measure of its fidelity? Specifically, how often does the LLM-based $\\mathcal{C}_{gen}$ module successfully induce a rule that correctly prevents a recurrence of the specific failure type, and how often does it induce a rule that is structurally irrelevant or leads to a future constraint violation?\n2. The conclusion states future work involves extending counterfactual reasoning to the subtask level (e.g., goal reordering, subgoal decomposition). Given the success of the current joint action-level mutations, did the authors test a minimal experiment where they only allow mutations over the temporal ordering of the subgoals (ignoring agent assignment) to gauge the immediate benefit of this richer counterfactual space?\n3. The evaluator's design is invariant under monotonic transformation, implying only the ranking is relevant. However, the ranking is driven by retrieving relevant traces and using LLM reasoning to integrate rules and outcomes. Could the authors provide a case study where the LLM's Constraint-guided Evaluation successfully rejects a candidate plan that is structurally valid but known to lead to long-term failure based on a low-support rule (i.e., a rule with low confidence/support count $\\mu_r$)?\n4. Given the scaling challenge observed beyond three agents, have the authors investigated whether the performance drop at four agents is correlated with an increase in Load Balance (B) violations or an increase in the number of spatial feasibility constraints being retrieved during planning, suggesting a memory/coordination constraint saturation? Are there any mitigative measures for this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dJKb7YvjZa", "forum": "KpvZ1kGOjH", "replyto": "KpvZ1kGOjH", "signatures": ["ICLR.cc/2026/Conference/Submission25307/Reviewer_pAHc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25307/Reviewer_pAHc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762054021010, "cdate": 1762054021010, "tmdate": 1762943396696, "mdate": 1762943396696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EvoCF, a hybrid framework to improve multi-agent planning for embodied AI. It addresses the common failure of Large Language Model (LLM) planners to account for real-world physical and coordination constraints. EvoCF works in three stages: (1) it learns a library of symbolic rules by observing past failures; (2) it uses an evolutionary generator, guided by these rules, to create many \"what-if\" (counterfactual) plan variants; and (3) it uses an experience-driven evaluator to select the most robust plan."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It tackles the \"short-sightedness\" of LLM planners, which often fail to maintain context across multi-step tasks.\n\n- The hybrid approach of learning symbolic rules from failure and using them to guide an evolutionary search is a new and effective way to explore the planning space."}, "weaknesses": {"value": "- The paper's \"symbolic rules\" (Table 5) appear to be simple, correlational heuristics learned post-hoc from failures (e.g., 'being near X and Y causes interference'). This seems fundamentally different from a true causal understanding of why the failure occurred (e.g., 'Agent 1's path requires the same 3D space Agent 2 currently occupies'). Is the system simply learning brittle heuristics that only prevent seen failures, or can it prove it generalizes to novel failure modes? How would EvoCF handle an entirely new type of constraint (e.g., a \"social\" rule like \"don't enter the room while the user is in it\") that it has never seen fail before?\n\n- The 'evolutionary generator' uses simple mutation operators (Swap, Insert, Delete) to explore the plan space. This type of local search is prone to getting stuck in local optima; it might find a 'less bad' plan that avoids a known rule but could miss a globally optimal plan (e.g., one that is dramatically faster). Did the authors investigate whether EvoCF produces plans that are merely sufficient versus those that are provably optimal? How does this method compare to more traditional, exhaustive planners that can guarantee optimality?\n\n- The paper presents EvoCF as a hybrid LLM planner. However, the LLM's initial plan is explicitly 'distrusted' and immediately \"mutated\" by a symbolic, rule-based system. The \"intelligence\" of the solution seems to come entirely from the symbolic EvoCF component, which is a good thing. But, fundamentally, what value does the LLM provide, other than as a seed generator? Could the LLM be replaced with a simple, non-AI baseline planner (or even a hand-crafted template) with no loss in performance? What justifies this as an 'LLM-based' system?"}, "questions": {"value": "See the weaknesses. But other minor questions:\n\n- Could the authors provide data on the planning time required by EvoCF compared to the baselines? Is there a trade-off where EvoCF takes significantly longer to find a plan, even if that plan is more efficient to execute?\n\n- The experiments are limited to 2-agent scenarios. The evolutionary search for plans could become computationally infeasible with more agents. The authors should include some analysis of how the computational feasibility scales with the number of agents."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PjXbZAJVJZ", "forum": "KpvZ1kGOjH", "replyto": "KpvZ1kGOjH", "signatures": ["ICLR.cc/2026/Conference/Submission25307/Reviewer_Zhcg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25307/Reviewer_Zhcg"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission25307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762295501019, "cdate": 1762295501019, "tmdate": 1762943396528, "mdate": 1762943396528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}