{"id": "XxBR2KNWNh", "number": 2292, "cdate": 1757051628328, "mdate": 1763652864744, "content": {"title": "Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization", "abstract": "Multi‑agent debate (MAD) aims to improve large language model (LLM) reasoning by letting multiple agents exchange answers and then aggregate their opinions. Yet recent studies reveal that agents are not neutral: they are prone to identity‑driven sycophancy and self‑bias, uncritically adopting a peer’s view or stubbornly adhering to their own prior output, undermining the reliability of debate. In this work, we present the first principled framework that joins sycophancy and self-bias to mitigate and quantify identity bias in MAD. First, we formalize the debate dynamics as an identity‑weighted Bayesian update process. Second, we propose response anonymization: by removing identity markers from prompts, agents cannot distinguish \"self\" from \"peer\", which forces equal weights on agent identity, thereby reducing bias.  Third, we define the Identity Bias Coefficient (IBC), a principled metric that measures how often an agent follows a peer versus itself.  Empirical studies across multiple models, datasets and debate rounds confirm that identity bias is widespread, with sycophancy far more common than self‑bias. Our findings highlight the need to ``mask\" identity to ensure that MAD systems reason based on content rather than source identity.", "tldr": "", "keywords": ["Multi-Agent Debate", "Large Language Models", "Identity Bias"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/563e1cea698a962c54a3ef01f104711a68ade4ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies identity-driven biases in multi‑agent debate (MAD) with LLMs. Identity-driven biases are broken down to sycophancy (conformity, over-weighting peer opinions) and self-bias (obstanicy, over-weighting self opiniongs) which may distort collective reasoning. The authors formalize debate as a Bayesian update with Dirichlet-multinomial distribution, under which the gap = conformity - obstinacy  is a sum of a content‑driven belief difference term and a pure identity bias term. Then, a ‘response anonymization’ which removes all self / peer markers from the input prompts of next-round multi-agent debate will remove the identity bias term, allowing one to measure the term (named IBC). Experiments across different benchmarks and LLMs show mixed results, with tendency of language models to have positive IBC, indicating conformity typically has a larger effect than obstinacy."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Reproducibility and presentation quality. The paper provides detailed experiment settings including input prompts. Figures and tables are clean. It is a well-written and well-presented paper.\n\n2. The idea of formulating LLM agent collective reasoning as a Bayesian update of Dirichlet prior is intriguing, and a finding of disproportionality in opinion update is interesting. This idea and finding is worth further investigation."}, "weaknesses": {"value": "1. Statistical inference (Contribution 1, Line 76) is disconnected from the experiments and analysis. The paper’s core contribution is a Bayesian model of opinion updating in MAD. Yet the model yields no substantive insight beyond the self-explanatory intuition ‘masking identity in input prompts removes identity bias’. If the paper adopts a Bayesian framing (or more generally, statistical inference), it should lead to inference about the data-generating process: estimate parameters (e.g., \\(\\alpha, w_i, w_j\\)) from data and assess the realism of the model. Currently the formalism serves mainly to restate an obvious implication—if \\(w_i = w_j\\) by identity masking in input prompts, then \\(w_j - w_i = 0\\) (Equation 4, Line 258) without empirically validating that the statistical model explains observed behavior.\n\nA practical path would be to turn the formalism into an estimable model. One rough sketch might be like: under single-agent generation (Line 203) one could use empirical Bayes method to infer \\(\\alpha_{i,t}\\) (or, the normalized masses \\(\\alpha_{i,t} / \\sum \\alpha_{i,t}\\)) and estimate the identity-bias coefficient (Line 296) from result in Table 1. The paper could then test whether these inferred quantities jointly account for the observed conformity–obstinacy differences (Equation 4, Line 258). As it stands, one can skip Section 4 without losing interpretability of Tables 1-2 once conformity / obstinacy are defined (Sec. 3.1), which underscores the current disconnect. Demonstrating that the model fits and explains the generated data would substantially strengthen the contribution and value of the paper.\n\n2. There seems to be a logical gap in the interpretation of experiment results with the formalism. When measuring the conformity - obstinacy gap after identity masking, the paper claims “As established in Corollary 1, once identity bias is removed, the residual Δ reflects only the difference in the agent’s prior belief masses on the peer’s versus its own previous answer. Thus, small but nonzero values after anonymization are expected” (Line 341-344) indicating this result can be explained by the model, which is not justified. In fact, the expectation of ‘Δ in the absence of identity bias’ (Corollary 1, Line 275) over a joint distribution of y_{i,t-1} and y_{j,t-1} should be zero under the identical agent setting (i.e. same persona for i and j) as in Table 1. This part requires clarification.\n\n3. Identity masking hurts performance (Line 345; Table 3, Line 972). While the paper defends “eliminating identity bias remains essential” (Line 995) because it “makes the debate process more reliable and better aligned with the long-term goal of building trustworthy multi-agent” (Line 997), (1) the claim of reliability and trustworthiness is questionable and not demonstrated (2) some degree of identity-related behavior can be instrumental rather than purely harmful: for example, mild in-group / confirmation tendencies (e.g., confirmation bias) and socially productive forms of influence (e.g. accommodation in Communication Accommodation Theory, perspective-taking) can facilitate coordination and collective performance in human groups. Absent evidence that suppressing identity cues is essential, the paper’s blanket prescription to remove identity appears premature."}, "questions": {"value": "Dirichlet distribution as a conjugate prior for categorical distribution is well applied to discrete probability measure; however, how can this be extended to a countably infinite outcome space, for example, integer answer space? While several benchmarks have a finite answer space (like MMLU 4-option questions), others (e.g. AIME accepts integer answer 0-999) have much larger space, and since LLM is capable of open-ended generation task, the outcome space can be countably infinite. I would like to ask how the current framework can be extended in this case.\n\nRelated to Weakness 1, I would like to ask how the prior and weights, which are hyperparameters of the paper’s statistical model, can be chosen. Definition 1 treats α as free, but expectations of y_{i,t} (marginalized over thetas) are scale‑invariant in α while Δ and IBC depend on the sum of α (denominator in Theorem 1). How do you infer prior and weights in experiments?\n\nRecent work (e.g., Qian et al., To Mask or to Mirror: Human–AI Alignment in Collective Reasoning) examines identity bias in LLM multi-agent collective reasoning. I’m trying to understand the current state of this discussion: how well documented is identity-driven behavior in multi-agent setting, and to what extent is it a new observation versus an extension of earlier findings (e.g., LLMs favoring its own model generations) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bWb723HscY", "forum": "XxBR2KNWNh", "replyto": "XxBR2KNWNh", "signatures": ["ICLR.cc/2026/Conference/Submission2292/Reviewer_RJjL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2292/Reviewer_RJjL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761628413835, "cdate": 1761628413835, "tmdate": 1762916180377, "mdate": 1762916180377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the phenomenon of identity-bias in multi-agent debate (MAD) systems of large language models (LLMs). In such systems, multiple agents generate answers, see each other’s responses, revise, and then aggregate the result. The authors observe that agents tend to behave differently depending on whether they are looking at their own prior response (“self”) or a peer’s (“peer”) in particular exhibiting sycophancy (over-adopting peer responses) or self-bias (sticking too much to one’s prior answer). They propose a formal probabilistic Bayesian belief-update model that explicitly incorporates identity weighting to capture these biases. Based on that, they define interpretable metricsand a derived metric Identity Bias Coefficient (IBC) to measure how much identity influences agent behaviour. Their key intervention is “Response Anonymization” (i.e., removing identity markers so agents cannot tell whether a previous answer was from self or peer), thereby forcing symmetric weighting and reducing identity-bias. Empirical experiments across several model families and tasks show that identity bias (especially sycophancy) is widespread in MAD, and that anonymization substantially reduces the bias. The paper concludes that anonymization is a lightweight but effective method for making multi-agent debate more content-driven rather than identity-driven."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles a relatively under-explored issue in the emerging area of multi-agent collaborative LLM reasoning, namely how agent identity influences dynamics rather than purely content.\n\n- The formalisation is clear and elegant: modelling the agent’s belief updating as a Dirichlet-Multinomial (DCM) process and then deriving expressions that separate belief-driven update from identity-driven weight differences. \nThe proposed metrics (Conformity, Obstinacy, IBC) provide interpretable ways to quantify how much an agent is influenced by peer vs self, which is useful for analyzing such systems.\n\n- Response Anonymization is conceptually simple but practically appealing: it requires no retraining or architecture change, just modifying how prompts are constructed. That makes it widely applicable.\n\n- The empirical evaluation is reasonably broad: multiple model families, multiple datasets, both homogeneous and heterogeneous agent settings, and both single-peer and multi-peer setups. The results clearly show the presence of sycophancy and substantial reduction in bias under anonymization."}, "weaknesses": {"value": "- The evaluation focuses on many standard reasoning tasks (GPQA, MMLU, HellaSwag, GSM8K) but may not cover more open-ended or real-world debate scenarios (e.g., multi-turn dialogues, adversarial peers, diverse agent personas beyond simple roles). The generalisability to those contexts may be unclear.\n- The paper shows reduction in identity bias via anonymization but less discussion (or empirical depth) about how anonymization interacts with overall performance (accuracy, quality of final answer). There may be trade-offs (e.g., anonymization might reduce beneficial peer influence) that are not deeply explored.\n- The intervention (anonymization) treats all identity cues as equal, but there may be scenarios where knowing “this answer came from a more expert agent” is beneficial (i.e., the identity might encode expertise). The paper does not deeply explore heterogeneous expertise settings or when identity cues might be legitimately informative.\n- The paper missed some of the relevant reference such as https://arxiv.org/abs/2506.12657 in understanding how identity changes stances of multi-agent debate."}, "questions": {"value": "- How does anonymization affect the accuracy or quality of the final aggregated answer in multi-agent debate? If anonymization reduces identity bias but also reduces correct consensus or slows convergence, that would modify how strongly I view this as an unequivocal improvement.\n- In heterogeneous-agent settings where some agents are known to be more expert (e.g., a “doctor” vs “student” agent), how does anonymization affect outcomes? Does removing identity cues reduce the ability of the system to leverage expert peers?\n- How stable are the results across more challenging debate formats (longer chains, adversarial peers, mixed objectives) or with more than two rounds of debate? If identity bias re-emerges in more complex settings, that would temper the generality of the findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SZIWkEQ3kh", "forum": "XxBR2KNWNh", "replyto": "XxBR2KNWNh", "signatures": ["ICLR.cc/2026/Conference/Submission2292/Reviewer_eSo1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2292/Reviewer_eSo1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919422112, "cdate": 1761919422112, "tmdate": 1762916178948, "mdate": 1762916178948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors systematically examine identity biases in multi-agent debates, specifically sycophancy (the tendency to adopt the opinions of other agents) and self-bias (the tendency to maintain one's own opinion). They formalize debate dynamics as a Bayesian update process, which explains the aforementioned tendencies, and propose a simple intervention to reduce identity biases. They also introduce the Identity Bias Coefficient (IBC) as a metric to measure identity biases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well structured and easy to follow. \nResponse anonymization is a simple yet interesting and effective approach to reduce identity biases of LLMs in multi agent debates."}, "weaknesses": {"value": "Response anonymization appears to have no effect, or rather a negative effect, on the overall model's accuracy. \nThe related work section suggests that other mitigation strategies have been investigated by previous studies, but does not specify which ones. This information is important for the paper."}, "questions": {"value": "- What is the greatest benefit of removing identity biases in the case of multi agent debate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G63jTF3Q70", "forum": "XxBR2KNWNh", "replyto": "XxBR2KNWNh", "signatures": ["ICLR.cc/2026/Conference/Submission2292/Reviewer_u3fx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2292/Reviewer_u3fx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928925769, "cdate": 1761928925769, "tmdate": 1762916177970, "mdate": 1762916177970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses identity bias in multi-agent debate (MAD) systems, where LLM agents exhibit either sycophancy/conformity or self-bias/obstinacy. The authors formalize debate mechanics in LLMs as an identity-weighted Bayesian update process using a Dirichlet-Compound-Multinomial model. To mitigate the impact of identity bias on model responses, they propose response anonymization, which removes identity markers from prompts. The paper also introduces the Identity Bias Coefficient (IBC) to quantify the magnitude of identity bias. Comprehensive experiments demonstrate that identity bias is widespread, with sycophancy being more prevalent than self-bias, and that anonymization effectively reduces this bias."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The concept of unifying sycophancy and self-bias in MAD into identity bias is a novel proposal, which is supported by precise theoretical modeling using identity-weighted Bayesian updates in a principled and interpretable manner.\n\nThe response anonymization mitigation solution requires no retraining, architectural changes, or auxiliary losses and is shown to be consistent and effective across settings.\n\nThe authors demonstrate the robustness of their results across multiple models, tasks, and go beyond to include multi-peer and heterogeneous persona settings which makes the analysis compelling."}, "weaknesses": {"value": "While the theoretical derivation is able to isolate an identity-bias term, the empirical demonstrations are not fully convincing as causal in nature. When anonymization reduces the Conformity-Obstinacy gap, is this necessarily because identity bias was eliminated? Could this be instead because anonymization changes the cognitive load or prompt complexity? The ordering of responses in anonymised prompts could also introduce noise which could separately impact model reasoning. Comparing anonymisation to other intervention methods such as randomly swapping labels, applying counterfactual identity relabeling with content being held constant would help to strengthen the claims made in the paper.\n\nThe paper defers how anonymization impacts task accuracy to the appendix. Table 3 even shows mixed performance effects of anonymization, but the discussion is quite brief. While the paper argues that eliminating bias is valuable regardless of performance, a deeper analysis of when and why anonymization hurts performance would be valuable to readers.\n\nThe belief difference term which represents reasoning driven by actual content differences warrants more analysis. While references are made to the empirical behaviour of this term in Section 5.2, the calibration and quality of this term is not fully examined. Without this, we also don’t know when anonymization reduces bias while deference to more accurate peers could actually be more beneficial.\n\nIn the heterogenous setting, the role of adding multiple specialised personas is under-explored as they may be relevant to task success. Tied to the previous point, in this case, anonymisation may lead to more substantial performance tradeoffs as domain-expert personas should likely attract conformity on domain specific items."}, "questions": {"value": "Can you run alternate intervention experiments with counterfactual or random identity label swapping and compare the findings wrt point 1 mentioned above?\n\nCould you report more analysis of when anonymization can lead to trade-offs in actual task performance? Are there predictable patterns based on task characteristics, model size, or debate configuration?\n\nCould you empirically show how the belief difference term relates to actual content driven reasoning? Maybe a correlation analysis between proxies of model confidence (e.g. log probs) would make it more clear.\n\nWhat is the impact of anonymisation in cases where knowing the identity is actually beneficial for the outcome, such as in domain-relevant personas are used (e.g. a doctor in the medical domain)? A justification in the context of debate dynamics could also be useful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qAVWavSTfD", "forum": "XxBR2KNWNh", "replyto": "XxBR2KNWNh", "signatures": ["ICLR.cc/2026/Conference/Submission2292/Reviewer_quLh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2292/Reviewer_quLh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948647040, "cdate": 1761948647040, "tmdate": 1762916177713, "mdate": 1762916177713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers and Area Chair"}, "comment": {"value": "Dear Reviewers and Area Chair,\n\nWe extend our sincere gratitude for the time and effort you have dedicated to reviewing our manuscript. The thoughtful questions and constructive feedback from all the reviewers have been invaluable in improving our manuscript.\n\nWe are grateful for the reviewers' recognition of the *novelty* of our framework, and the  *practicality*, *effectiveness*, and *theoretical rigor* of our method. Below, we summarize the key strengths and concerns raised by the reviewers, along with our responses:\n\n---\n\n### Key Strengths noted by the reviewers\n1. A **novel framework with strong theoretical grounding** for explaining identity-driven multi-agent debate dynamics using the Dirichlet–Compound–Multinomial model. (Reviewers quLh, eSo1)\n3. The **simplicity and practicality of Response Anonymization**, requiring no retraining, architectural changes, or auxiliary objectives. (Reviewers quLh, u3fx, eSo1)\n4. **Demonstrated effectiveness of the proposed approach** in reducing identity bias across diverse multi-agent debate settings. (Reviewers quLh, u3fx)\n5. A **well-written manuscript and extensive analyses**, covering multiple model families, benchmarks, and evaluation dimensions. (All Reviewers quLh, u3fx, eSo1, RJjL)\n\n---\n\n### Key concerns and how we addressed them\n1. *How anonymization affects the overall accuracy in multi-agent debate*. We included additional experimental results analyzing the rates at which agents correct or subvert their answers. These results show that **anonymization improves trustworthiness by producing a substantially larger reduction in subversion than in correction**--indicating more reliable and trustworthy debate dynamics. (All Reviewers quLh, u3fx, eSo1, RJjL)\n2. *Whether identity cues (e.g., experts) can sometimes be beneficial.* We showed that \"expert\" identity labels **do not necessarily encourage conforming behaviors in both non-anonymized and anonymized settings**. We further emphasized that an agent's decision to defer or maintain its position should be grounded on content and quality of reasoning, without any reliance on identity cues. (Reviewers quLh, eSo1, RJjL)\n3. *How the Dirichlet-Compound-Multinomial model can be justified.* We showed that the DCM framework is empirically well-founded by **estimating its parameters** and demonstrating that the resulting Conformity and Obstinacy values closely match the observed ground-truth statistics. Moreover, we illustrate how these **estimated parameters enable principled calibration** of the Identity Bias Coefficient (IBC). (Reviewers quLh, RJjL)\n\nIncluding the concerns outlined above, we have addressed all reviewer questions in detail in the responses below, and updated the manuscript accordingly (updated parts are in blue font). We hope these clarifications fully resolve the outstanding issues.\n\n---\n\nThank you again for your time, careful evaluation, and for managing the review process. We appreciate your effort, and we hope this summary is helpful.\n\nSincerely,\n\nAuthors."}}, "id": "N8ueSlYuz8", "forum": "XxBR2KNWNh", "replyto": "XxBR2KNWNh", "signatures": ["ICLR.cc/2026/Conference/Submission2292/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2292/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission2292/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763656124652, "cdate": 1763656124652, "tmdate": 1763656605069, "mdate": 1763656605069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}