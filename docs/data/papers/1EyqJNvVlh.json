{"id": "1EyqJNvVlh", "number": 23589, "cdate": 1758346041728, "mdate": 1759896806259, "content": {"title": "Anchors Aweigh! Sail for Optimal Unified Multi-Modal Representations", "abstract": "A unified representation space in multi-modal learning is essential for effectively integrating diverse data sources, such as text, images, and audio, to enhance efficiency and performance across various downstream tasks.\nRecent binding methods, such as ImageBind, typically rely on a single, fixed anchor modality for aligning multi-modal data. We mathematically analyze these fixed anchor binding methods and uncover significant limitations: (1) over-reliance on the choice of the anchor modality, (2) inadequate capture of intra-modal information, and (3) failure to account for cross-modal correlation among non-anchored modalities. \nTo address these issues, we propose the need for adaptive anchor binding methods, exemplified by our framework CentroBind. \nThe proposed method uses adaptively adjustable centroid-based anchors generated from all available modalities, leading to a balanced and rich representation space.\nWe theoretically demonstrate that our approach captures three critical properties of multi-modal learning---intra-modal learning, inter-modal learning, and multi-modal alignment---while constructing a unified representation that spans all modalities. Experiments on both synthetic and real-world datasets show that adaptive anchor methods such as CentroBind consistently outperform fixed anchor binding methods, verifying our analysis.", "tldr": "", "keywords": ["Multimodal alignment", "mutual information"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/edf1f27ebe55e05e2ab62fe69196ae00789d8cce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper revisits the challenge of constructing a unified representation space across multiple modalities (e.g., text, image, audio) for multimodal learning. Existing methods, such as ImageBind, rely on a fixed anchor modality (e.g., images or text) as the target space into which all other modalities are aligned, thus leading to reliance on the anchor modality and loss of intra-model interaction. To overcome these issues, the paper proposes CENTROBIND by constructing a centroid-based, dynamically updated anchor embedding to act as the anchor for all modalities. Empirical results on synthetic and real-world datasets demonstrate consistent improvements over FABIND baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper studies a practical problem where the fixed anchor could be limited in some cases.\n- A theoretical framework is proposed to support the method."}, "weaknesses": {"value": "- The anchor generation strategy in CentroBind, which averages modality centroids, may not be robust when different modalities exhibit varying information densities. Modalities containing more or less discriminative information could disproportionately influence the centroid, potentially leading to biased or unbalanced representations.\n- The proposed method relies on high independence of modalities, which is not true in the real world. When modalities are highly correlated or exhibit strong synergy, the centroid could be overly biased toward certain modalities. Such correlation might hinder optimization, preventing the model from capturing true multimodal relationships. The paper does not clearly address how the optimization process mitigates these dependencies.\n- The combined use of multiple InfoNCE losses may introduce convergence instability or slow training. The effects of varying temperature parameters or weighting schemes among loss terms are not discussed, limiting understanding of the loss optimization dynamics and robustness.\n- The theoretical analysis appears to assume ideal conditions, leaving unclear the boundary cases where CentroBind may underperform—such as under uneven data distributions, severe modality imbalance, or non-independent modality structures. The generalizability of the theoretical claims thus remains uncertain."}, "questions": {"value": "Please see the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fG8tvm5sFR", "forum": "1EyqJNvVlh", "replyto": "1EyqJNvVlh", "signatures": ["ICLR.cc/2026/Conference/Submission23589/Reviewer_Ux9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23589/Reviewer_Ux9F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876001203, "cdate": 1761876001203, "tmdate": 1762942723526, "mdate": 1762942723526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is motivated by a problem in “bind-everything-to-images” style models (ImageBind, LanguageBind, etc.): if you always align other modalities to one fixed anchor (usually vision), the final joint space can only be as rich as that anchor, so any information that lives mostly in audio, text, or another modality is suppressed. Even worse, correlations between two non-anchor modalities (say, audio↔text) are never explicitly optimized, because the loss only looks at anchor↔others. So the authors ask: can we build a unified space that is co-defined by all modalities, not dictated by just one?\n\nTheir method, CENTROBIND, replaces the fixed anchor with an adaptive, batch-wise anchor: for each training batch, they compute a centroid from the available modalities embeddings and use that as the “anchor” everyone aligns to. Then they apply a contrastive / InfoNCE-style objective that pulls every modality toward this centroid, so the shared space is shaped by what actually appears in the data in that batch, not by a single handpicked modality. Because the center is recomputed each time, it can drift to accommodate different modality combinations and preserve modality-specific signals.\n\nEmpirically, on both synthetic and real multimodal setups, CENTROBIND outperforms fixed-anchor baselines, matching the theory that multi-modal alignment should be symmetric (all-to-one adaptive center) rather than asymmetric (all-to-one fixed image space)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Theoretical justification for the problem is solid, in standard multimodal contrastive learning the choice of anchor modality imposes a fixed ceiling\n\n- The propose CENTROBIND method is simple: compute a per-batch centroid and align to it, no additional architecture, so in principle you can drop it into existing multimodal contrastive setups"}, "weaknesses": {"value": "- Assumption that a single centroid per batch is a good proxy for the \"true\" shared semantics.\n- The method is also batch-dependent: the quality and stability of the anchor will depend on what modalities are present and how balanced the batch is.\n- Empirical evaluation is very limited, only consisting of results on a synthetic dataset and some limited set of tasks like sarcasm and speaker classification, dreambooth (image editing ?). Audioset is the only result comparable to prior baseline papers.\n\n- Some relevant references to related work are missing and should be discussed in paper:\n1. Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, and Du Tran. Self-supervised learning by cross-modal audio-video clustering. NeurIPS 2020.\n2. Brian Chen, Andrew Rouditchenko, Kevin Duarte, Hilde Kuehne, Samuel Thomas, Angie Boggust, Rameswar Panda, Brian Kingsbury, Rogerio Feris, David Harwath, et al. Multimodal clustering networks for self-supervised learning from unlabeled videos. ICCV 2021.\n3. Sirnam Swetha, Mamshad Nayeem Rizve, Nina Shvetsova, Hilde Kuehne, and Mubarak Shah. Preserving modality structure improves multi-modal learning. ICCV 2023."}, "questions": {"value": "Please provide results for all the datasets used in the ImageBind paper which is the main baseline. I am willing to raise my rating if comprehensive evaluation is demonstrated. Also add more recent results for models like LanguageBind and InternVideo2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qnvo7z9Tlb", "forum": "1EyqJNvVlh", "replyto": "1EyqJNvVlh", "signatures": ["ICLR.cc/2026/Conference/Submission23589/Reviewer_6D21"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23589/Reviewer_6D21"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970161784, "cdate": 1761970161784, "tmdate": 1762942723252, "mdate": 1762942723252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents CentroBind, a framework that adaptively determines the anchor modality for performing multimodal alignment. The paper provides a theoretical analysis showing that this approach can capture intra-modal, inter-modal, and multi-modal alignment components, and the empirical experiments demonstrate that CentroBind outperforms FABind baselines and other recent multi-modal alignment methods on retrieval and zero‐shot classification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear and Well-Structured: The paper is well-organized, with detailed explanations of the preliminary, intuition, and methodology.\n\n- Superiority in Alignment: The experimental results demonstrate that the proposed method achieves the best performance on the cross-modal retrieval and classification tasks compared to the baselines."}, "weaknesses": {"value": "- The paper provides clear intuition but presents the preliminary and methodological sections in an overly complex manner. I suggest that the authors reorganize the presentation flow to enhance readability and logical coherence. From my perspective, it is not necessary to include too many theoretical derivations or formal statements in the main text—these could be moved to the appendix, while keeping the main body focused on the core ideas, motivation, and empirical insights. \n\n- If some modalities’ encoders are significantly stronger or weaker, then the centroid might be dominated by high-quality modality embeddings, thus implicitly reintroducing an anchor bias. While the authors mention weighted aggregation as a workaround, empirical quantification of this phenomenon is minimal.\n\n- The technique introduced in this paper is relatively trivial and does not address the essential issue of anchor bias. While the proposed modification may bring marginal improvements, it fails to fundamentally eliminate the dependence on specific modalities or to provide a principled mechanism for balancing heterogeneous modality contributions.\n\n- The paper does not include experimental comparisons with other recent multimodal alignment methods, such as TRIANGLE [1] and GRAM [2]. Including these baselines would provide a stronger empirical validation of the proposed method's effectiveness. \n\n\n[1] A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity, NeurIPS 2025\n\n[2] Gramian multimodal representation learning and alignment, ICLR 2025"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jPNoLGQdaM", "forum": "1EyqJNvVlh", "replyto": "1EyqJNvVlh", "signatures": ["ICLR.cc/2026/Conference/Submission23589/Reviewer_EEA3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23589/Reviewer_EEA3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982244846, "cdate": 1761982244846, "tmdate": 1762942723021, "mdate": 1762942723021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CentroBind, an adaptive anchor binding method for multi-modal representation learning. \nThe authors argue that fixed-anchor binding (FABind) methods suffer from over-reliance on a single anchor modality, loss of intra-modal information, and failure to capture shared information among non-anchor modalities. \nCentroBind replaces the fixed anchor with a centroid-based anchor computed from all available modalities. \nThe method is evaluated on synthetic and real-world datasets and is shown to outperform FABind and several other baselines in tasks such as cross-modal retrieval and classification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and motivated by a relevant problem in multi-modal learning — the bias and inefficiency of fixed-anchor alignment.\n2. The proposed centroid-based adaptive anchor idea is simple, easy to implement, and potentially applicable to other multi-modal frameworks.\n3. Experiments are conducted on both synthetic and real-world datasets, covering multiple modalities and tasks."}, "weaknesses": {"value": "1. The theoretical novelty is relatively weak. \n\n(1) The key idea—constructing a centroid anchor from multiple modalities—is a minor variation of existing multi-modal alignment formulations. \n\n(2) Similar concepts of adaptive or learned anchors have been discussed in OmniBind[1] and UNIALIGN[2]. \n\n(3) The mathematical derivations in Section 3 mostly restate standard InfoNCE lower-bound properties without offering new theoretical insights or proofs that go beyond prior work. The formal results (Theorem 3.1, Propositions 2.2–2.3) repackage well-known properties of mutual information and data-processing inequality.\n\n2. While the authors evaluate on several datasets, the experimental validation is not convincing for a top-tier conference.\n\n(1) Baselines are not sufficient. The comparisons include FABIND, UniBind, AudioCLIP, and ViT-Lens, but exclude more recent and stronger baselines such as OmniBind [1] and UNIALIGN [2], which are both highly relevant and directly comparable.\n\n(2) Lack of large-scale evaluation. All reported results are on small or medium datasets (MUStARD, AudioSet subsets). For a method claiming general multi-modal unification, evaluations on larger benchmarks are expected.\n\n3. CENTROBIND introduces extra computation for per-batch centroid construction and additional InfoNCE terms. \nHowever, the paper does not provide complexity analysis or runtime comparison with strong baselines such as ViT-Lens and UniBind. \nWithout this, it is unclear whether the method scales to high-dimensional multi-modal tasks.\n\n4. The manuscript contains several minor formatting and reference errors (e.g., “Figure ??” on page 6, line 273).\n\n5. Appendix references are frequently cited for essential content (algorithms, proofs, and ablations), making it difficult to assess the main claims within the body of the paper.\n\n[1] Lyu, Yuanhuiyi, Xu Zheng, Dahun Kim, and Lin Wang. \"Omnibind: Teach to build unequal-scale modality interaction for omni-bind of all.\" arXiv preprint arXiv:2405.16108 (2024). \n\n[2] Zhou, Bo, Liulei Li, Yujia Wang, Huafeng Liu, Yazhou Yao, and Wenguan Wang. \"UNIALIGN: Scaling Multimodal Alignment within One Unified Model.\" In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 29644-29655. 2025."}, "questions": {"value": "1. Could the authors include direct experimental comparisons with OmniBind [1] and UNIALIGN [2]?\n2. Have the authors considered learning the centroid weights (e.g., modality-dependent coefficients) instead of computing a simple mean?\n3. What happens if modalities are partially missing during training or inference — can the adaptive anchor still be constructed robustly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MJVLlUkv3p", "forum": "1EyqJNvVlh", "replyto": "1EyqJNvVlh", "signatures": ["ICLR.cc/2026/Conference/Submission23589/Reviewer_Yya1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23589/Reviewer_Yya1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097110478, "cdate": 1762097110478, "tmdate": 1762942722766, "mdate": 1762942722766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}