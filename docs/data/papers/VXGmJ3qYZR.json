{"id": "VXGmJ3qYZR", "number": 8736, "cdate": 1758096542525, "mdate": 1759897766723, "content": {"title": "Unlocking the Pre-Trained Model as a Dual-Alignment Calibrator for Post-Trained LLMs", "abstract": "Post-training boosts the performance of large language models (LLMs) but systematically degrades their confidence calibration, making them frequently overconfident. Recent post-hoc LLM calibration methods circumvent the challenge by aligning the post-trained language model with its pre-trained counterpart; however, they treat calibration as a static output distribution matching problem, and thus fail to capture the complex dynamics of post-training induced on calibration. Our investigation into these dynamics reveals that calibration errors stem from two distinct regimes: (i) output drift, where final confidence is inflated while intermediate decision process remains consistent, and (ii) process drift, where the intermediate pathways themselves diverge. Based on this diagnosis, we propose DUAL-ALIGN, a dynamic unsupervised framework performing dual alignment for LLM confidence calibration. It applies output alignment to correct output drift by matching the final output distributions. For process drift, it introduces novel process alignment, a technique that first identifies the specific layer where the models' inference paths diverge and then realigns the stability of their subsequent trajectories. This dual strategy enables learning a temperature parameter that corrects both calibration error types that occur during post-training. Experiment results demonstrate that our method brings consistent improvement compared with representative baselines, reducing calibration error and approaching the performance of a supervised oracle.", "tldr": "", "keywords": ["LLM Calibration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/27e1871c36db64868352ca7e8c2c7acf8657f78d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Beside the widely acknowledged output drift, this paper reveals the existence of “process drift”, which is defined as the confidence discrepancy of pre-trained models (PLMs) and post-trained models (PoLMs) among intermediate layers. The authors recognize such drift as a source of over-confidence and thus causing miscalibration. To address this problem, this work proposes dual-align, a calibration method that learns a temperature parameter that can be applied to mitigate both output drift and process drift. Specifically, this method defines output alignment objective as the KL-Divergence between the temperature-scaled final-layer output distribution of the PoLM and the original distribution of the PLM, and defines process alignment objective as the squared difference between the Inferential Stability Entropy (the entropy computed with intermediate logits). Then the temperature parameter is learned with a combined loss of two objectives. Experiments shows consistent improvement on calibration when applied the learned temperature."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The revealing of process drift is refreshing.\n2. The design of identifying PDL for process alignment is novel but intuitive.\n3. Experimental results seem promising in the perspective of calibration."}, "weaknesses": {"value": "- My primary concern is that this work seeks to align the confidence of PLM and PoLM regardless of whether the confidence discrepancy is expected. Post-training itself is designed to alter model behaviors, and the confidence discrepancy between PLM and PoLM is a natural result, which may also indicate that expected behaviors are successfully injected. This work seems to consider this perspective neither theoretically (e.g. discerning what kind of discrepancy should be mitigated) nor empirically (e.g. displaying accuracy/behavior difference of baselines and the proposed method), which makes the contribution of this work questionable.\n- Other minor weaknesses lies in the organizing and demonstration of the paper, see Questions."}, "questions": {"value": "1. In section 3 Line 136-137, you are using the final token of input prompt. Does this mean that you expect the first generated token to be the answer token? How is this guaranteed, especially for pre-trained models? Also, this means that no CoT is generated in this process. As far as I know, CoT has an impact on model confidence, and reasoning models are more commonly used recently. Can method proposed in this work used for CoT inference (or it should also be converted into binary classification as stated in section 6)?\n2. Figure 5 is a little bit confusing for me. First, what are the meanings of the distribution curves above and aside the main plot? Is it the frequence distribution of ISE/Final Layer Confidence among all samples from some test set? If no, it would be of great help if you can explain further. If yes, high confidence itself does not indicate over-confidence (e.g. in the right figure if samples assigned 0.9 confidence are mostly correct, it could be well-calibrated), making the correlation between low ISE and over-confidence less convincing. Maybe briefly demonstrating relative experimental setting when introducing data plots would make the sections clearer.\n3. In Section 4.2 and 4.3, when calculating ISE, is temperature tau only used to divide final layer logits, or used among all layers after the PDL? If it is used among all layers after PDL during training, how it used during inference (applied to final layer or also intermediate layers) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o0uH7bdg4Z", "forum": "VXGmJ3qYZR", "replyto": "VXGmJ3qYZR", "signatures": ["ICLR.cc/2026/Conference/Submission8736/Reviewer_RrkG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8736/Reviewer_RrkG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708856419, "cdate": 1761708856419, "tmdate": 1762920529620, "mdate": 1762920529620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DUAL-ALIGN, an unsupervised output calibration method designed to address the previously overlooked issue of process drift. The authors’ motivation is that aligning PoLM and PLM only on consistent responses is insufficient for calibrating inconsistent cases where the underlying reasoning paths bifurcate. To this end, they introduce an unsupervised alignment approach that jointly accounts for output and process drift by minimizing (i) the KL divergence between the output distributions of the PLM and PoLM and (ii) the squared ISE discrepancy accumulated along the reasoning process. The method learns a single scalar temperature that is applied at inference time to calibrate the outputs. Empirically, DUAL-ALIGN outperforms baselines across multiple model scales and families, achieving performance close to supervised methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation: The authors identify and thoroughly analyze the process drift overlooked by prior work.\n2. Methodological novelty: They propose a new unsupervised algorithm that jointly addresses output drift and process drift, achieving performance that approaches supervised methods.\n3. Comprehensive experiments: Evaluations span multiple model scales and families, as well as diverse post-training paradigms."}, "weaknesses": {"value": "When fitting the temperature, the process-drift loss incorporates intermediate-layer logits; however, at inference time only the final layer (i.e., the output) is calibrated. This train–inference mismatch makes the source of the reported gains puzzling. In particular, because inference cannot account for process drift, the learned parameter does not actually stabilize the intermediate layers where such drift occurs, leaving the concrete mechanism behind the performance improvement unclear."}, "questions": {"value": "1. Section 5.3 ablations (Table 3): Table 3 shows that Output Only actually degrades performance. I’m curious whether this is because instances with answer inconsistency (i.e., process drift) were not filtered out—this leads to another question (see Q2).\n2. On loss weighting: For samples exhibiting output drift, weighting seems reasonable. However, for process-drift samples (i.e., cases with discrepant final answers), would assigning a weight to Loss output harm performance? Furthermore, if we forgo weighting altogether and simply stratify samples by the two drift types, how would performance change? I would like to see ablations specifically on the weighting strategy.\n3. Section 6 (open-ended setting): When the PLM and PoLM produce different answers, their self-evaluation contexts differ. In that case, is cross-model, layer-wise JSD still a robust indicator of “process divergence”? Alternatively, do you still compute it using the last token of the same prompt? This raises another concern: confidence can change during generation, so the prompt’s final token may no longer reflect the confidence at the time the decision is made. Could the authors clarify this? I am particularly interested in the open-ended scenario because it is the most common in practical tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aTu0usMnk0", "forum": "VXGmJ3qYZR", "replyto": "VXGmJ3qYZR", "signatures": ["ICLR.cc/2026/Conference/Submission8736/Reviewer_9k8u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8736/Reviewer_9k8u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727459229, "cdate": 1761727459229, "tmdate": 1762920529282, "mdate": 1762920529282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the calibration degradation that occurs when LLMs are post-trained (e.g., via instruction tuning or RLHF). The authors identify two empirical phenomena, i.e., output drift (the model’s confidence inflates while its intermediate reasoning remains consistent with the base model) and process drift (the model’s intermediate activations diverge sharply at a “peak divergence layer”).\nBuilding on this diagnosis, they propose DUAL-ALIGN, an unsupervised post-hoc calibration framework that performs (i) output alignment to correct surface-level overconfidence and (ii) process alignment to restore stability of intermediate inference dynamics. The method learns a single temperature parameter without labels and incurs no extra inference cost, achieving lower calibration errors across several model families (Llama-3.1, Qwen-2.5, Gemma-3) compared with existing unsupervised baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper offers an interesting empirical observation by decomposing post-training miscalibration into two distinct regimes (“output drift” and “process drift”). This provides a more granular view of how post-training affects model confidence than prior work, and it may inspire further interpretability-based calibration research.\n\n- The proposed method improves calibration with very low computational cost. It only learns a single scalar temperature parameter while exploiting existing model representations. There is no fine-tuning or additional inference overhead, making it practical for large-scale LLMs.\n\n- The approach is completely label-free. By leveraging the pre-trained model as a self-supervised reference, it eliminates the need for human-annotated validation data and remains effective across several architectures and datasets."}, "weaknesses": {"value": "- The two-drift observation is purely empirical and lacks a principled foundation. The distinction between output and process drift is derived from layer-wise diagnostics rather than foundamental analysis, leaving open questions about whether these patterns generalize to other architectures, training recipes, or datasets. Strengthening this with a more formal analysis or cross-model verification would make the argument more convincing.\n\n- The paper does not evaluate potential side effects on task performance. Although temperature rescaling does not alter argmax predictions, most generation setups rely on top-p sampling, where changing the temperature reshapes token probabilities and can influence output quality. Without reporting accuracy or generation-quality metrics, it remains unclear whether the calibration improvements come at the cost of degraded task performance."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lLAqHNKmi4", "forum": "VXGmJ3qYZR", "replyto": "VXGmJ3qYZR", "signatures": ["ICLR.cc/2026/Conference/Submission8736/Reviewer_6TZj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8736/Reviewer_6TZj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894988892, "cdate": 1761894988892, "tmdate": 1762920528892, "mdate": 1762920528892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}