{"id": "QOWYX3Q2XS", "number": 7714, "cdate": 1758033298676, "mdate": 1763676861917, "content": {"title": "MENLO: From Preferences to Proficiency – Evaluating and Modeling Native-like Quality Across 47 Languages", "abstract": "Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt–response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs' multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.", "tldr": "Massively multilingual preference evaluation, reward modeling, and post-training to improve LLMs' language proficiency", "keywords": ["multilingual", "reward modeling", "rl", "llm-as-judge", "human evaluation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6376677474af55dfc8dcf20edab86b3307d03ce1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for evaluating and improving whether large language models generate native-level natural responses across 47 languages. The goal is to evaluate whether models can converse like actual native speakers, beyond mere grammatical accuracy. To this end, the researchers applied the sociolinguistic principle of audience design, which is the concept that speakers adjust their language style according to their addressee or reference group. The framework divides native-level quality into four core dimensions. First, Fluency evaluates accuracy of vocabulary and grammar, consistency of sentence structure, and clarity. Second, Tone judges the overall writing style—whether it is helpful, insightful, engaging, and fair. Third, Localized Tone evaluates alignment with cultural, regional, and linguistic nuances, assessing appropriate use of local expressions and cultural sensitivity. Fourth, Localized Factuality evaluates factual accuracy and completeness, and whether information is provided based on local context. The researchers experimented with whether trained judge models could be used as generative reward models to directly improve the language capabilities of policy models. Using Qwen3-4B-RL-Judge as a reward model to subsequently train the base Qwen3-4B model, all three LLM evaluators (Qwen3-32B, gpt-4.1, Llama4-Scout-RL-Judge) confirmed improvements. Win rates ranged from 63.4% to 77.9%, with average score improvements from +0.80 to +1.16.However, there was an important finding. Human evaluation performed on 10 languages revealed that human evaluators were more conservative with a win rate of 55.7% and average score improvement of +10.9%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Methodological Rigor: Operationalizing sociolinguistic theory (Bell's Style Axiom and audience design) into a computational evaluation framework is a strong point. This goes beyond simply translated prompts or general naturalness evaluation to capture the actual phenomenon of native speakers adjusting language to specific addressees or reference groups. The decomposition into four dimensions is theoretically motivated, and developing detailed rubrics for each dimension effectively reduced subjectivity.\n\nData Quality and Scale: The MENLO dataset surpasses existing multilingual benchmarks in several aspects. The scale covering 47 language varieties, human-written and localized prompts, and Krippendorff's α of 0.84 are all impressive. Particularly, the fine-grained distinction of language varieties (e.g., European vs. Mexican Spanish, various English varieties) reflects actual linguistic diversity. Deploying expert annotators per language and developing customized annotation tools are also best practices in data quality management.\n\nSystematic Experimentation: Systematic comparisons of pairwise vs. pointwise evaluation, with/without rubrics, and SFT vs. RL clearly show the contribution of each design choice. Particularly, the finding that pairwise evaluation achieves up to 12.4% higher Macro-F1 than pointwise evaluation and is more effective than few-shot examples is significant. Ablation studies on reward shaping also quantify each reward component's contribution, demonstrating the importance of composite reward design in GRPO.\n\nPractical Value: Achieving actual model improvement by using trained judges as reward models demonstrates the framework's practicality. They closed the evaluation-improvement loop, suggesting real applicability beyond theoretical benchmarks. The commitment to releasing datasets and code will also contribute to reproducibility and facilitate follow-up research.\n\nTransparent Limitation Reporting: Honestly reporting the finding that LLM evaluators overestimate improvements by +0.5~0.6 compared to humans demonstrates academic honesty. Acknowledging the difficulty of the Localized Factuality dimension and suggesting alternative approaches (retrieval, tool use) is also positive."}, "weaknesses": {"value": "Insufficient Causal Verification: There's inadequate causal analysis of why pairwise evaluation is superior. It's not distinguished whether comparison itself is important, whether it's due to context richness from simultaneous exposure to two responses, or whether relative judgment is simply easier than absolute judgment. Without control experiments like pairwise evaluation of a single response (presenting the same response twice), mechanistic understanding is limited.\n\nLack of Statistical Rigor: Most results lack confidence intervals or significance tests. Especially given the high variability in language-specific performance (37.9%~82.1%), it's difficult to judge whether observed differences are statistically significant. Multiple comparisons across several models and settings were performed, so multiple testing corrections like Bonferroni are needed but not applied. Additionally, an average of 137 prompts per language may lack statistical power for some languages.\n\nMissing Comparative Experiments: There's no comparison with existing multilingual quality metrics (BLEURT, COMET, BERTScore), making it difficult to judge MENLO's relative advantages. Also, there's no comparison with other RL algorithms (PPO, DPO, RLAIF) or reward designs, so we cannot know if the proposed GRPO+composite reward is optimal. Human evaluation was only performed on 10 languages, limiting generalization to all 47 languages.\n\nMethodological Bias: The SFT baseline may be unfairly weak. Training without Chain-of-Thought doesn't provide fair comparison with RL, and SFT with reasoning should also be included. Additionally, justification for model selection (Qwen3-4B, Llama4-Scout) is lacking, and result generalization to other sizes or architectures is uncertain."}, "questions": {"value": "I want to express sincere appreciation for the authors' substantial effort in conducting this large-scale, ambitious research. Please consider the following recommendations to strengthen the paper:\n\n< Questions >\nCan you provide a more precise operational definition of 'native-level quality'? Please provide theoretical rationale for whether the four dimensions are sufficient or necessary conditions, and why other important aspects (pragmatics, humor, dialect) were excluded.\n\nThe distinction between Tone and Localized Tone sometimes appears ambiguous (e.g., fairness depends on cultural context). Can you clarify the conceptual boundary between these two dimensions?\n\nHow were reward function coefficients selected (e.g., partial reward +0.5, preference bonus +1, format penalty -0.2)? Did you perform sensitivity analysis? \n\n< Suggestions for Improvement >\nStrengthen Statistical Rigor: Perform and report statistical significance tests for all major claims. Particularly provide p-values, confidence intervals, and effect sizes for SFT vs. RL, pairwise vs. pointwise evaluation, and cross-language comparisons. Use bootstrapping or permutation tests to obtain robust estimates.\n\nClarify Causal Mechanisms: Conduct additional experiments revealing why pairwise evaluation is superior. Include presenting same response twice, systematically varying response order, and analyzing correlation between response quality differences and performance improvements.\n\nImprove Baselines: Add SFT baseline with reasoning to fairly compare with RL. Also include other RL algorithms like DPO or PPO to demonstrate GRPO is best.\n\nCompare with Existing Metrics: Compare MENLO with established multilingual metrics like BLEURT, COMET, BERTScore. Include correlation, relative strengths/weaknesses, and language-specific performance analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xa2aU8Fkth", "forum": "QOWYX3Q2XS", "replyto": "QOWYX3Q2XS", "signatures": ["ICLR.cc/2026/Conference/Submission7714/Reviewer_kNrK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7714/Reviewer_kNrK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962976302, "cdate": 1761962976302, "tmdate": 1762919771876, "mdate": 1762919771876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This works proposes a dataset and a LLM-judge based evaluation framework to evaluate whether an LMs' response is native-like quality. Specifically, the authors attempt to assess four aspects: (1) language quality/coherence, (2) alignment with cultural/lignuistic nuance, (3) factual correctness, (4) overall writing style and helpfulness. The dataset (Menlo) spans across 47 languages and includes 6423 instances. Then, the authors evaluate different models in point wise and pair wise settings when functioning as a judge to evaluate responses. Also, the authors train a judge model with RL and show that it could match or outperform the performance of Qwen3-32B and GPT-4.1. The authors also show that this judge model could also work as a generative reward model to improve LMs' ability to write fluent responses."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The Menlo dataset was annotated with 1934 annotators and also has high krippendorff's alpha value, indicating that it is a very high-quality dataset.\n\n2. The list of experiments is very extensive: starting from gathering a dataset, training a judge, and then using the judge to train a generator. Every step is supported by a experimental result, which is very helpful for future work in this area."}, "weaknesses": {"value": "1. Considering that the prompts consisting the Menlo dataset were derived using a prompt template, it could be possible that the prompts are synthetic in nature. There should be an explanation to which extent these prompts mirror what non-English speakers might prompt to frontier models in real-world scenarios. One possible way would be to gather the prompts on WildChat (that are non-English) and measure how similar the two groups are compared to other multilingual meta-eval benchmarks such as M-RewardBench or MM-Eval.\n\n2. The fact that \"pairwise ranking\" yields higher performance compared to \"pointwise\" is somewhat trivial - the caveat should be that it incurs higher inference costs. Yet, the paper simply concludes that pairwise has higher performance compared to pointwise. For a fair comparison, since pointwise is O(N) and pairwise is O(N^2), it is possible to prompt the LLM judge N times and apply either score averaging [1,2] or self-consistency [3].\n\n3. In Table 4, it shows that adding Rubrics help the models to predict better judgments. When looking into the rubrics in Appendix A.3., these are explanations of each criteria, not instance-specific rubrics [3]. I think the term \"context-specific rubrics\" should be adjusted to \"detailed rubrics\" for readers to not confuse with instance-specific rubrics. Also, it would be nice to add a qualitative example of how the chain-of-thoughts generated by the judges differ when including or excluding the rubrics.\n\n4. In Figure3, there are some languages that benefit more and other languages that do not. What is the difference? Is it the limitation of RL training (that means there should be more pretraining for some languages) or is there another reason behind it that the authors conjecture?\n\n5. Looking at the one of the qualitative example between Figure 9-16 (I won't specify for anonymity), I'm not sure if the grades make sense, which makes me a bit skeptical if the large group of annotators were instructed and managed properly. Can you give me details of how these people were recruited, how much they were incentivized, and how they were qualified?\n\n[1] Kim, S., Shin, J., Cho, Y., Jang, J., Longpre, S., Lee, H., ... & Seo, M. (2023, October). Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations.\n[2] Kim, S., Suk, J., Longpre, S., Lin, B. Y., Shin, J., Welleck, S., ... & Seo, M. (2024, November). Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (pp. 4334-4353).\n[3] Kim, S., Suk, J., Cho, J. Y., Longpre, S., Kim, C., Yoon, D., ... & Seo, M. (2025, April). The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) (pp. 5877-5919)."}, "questions": {"value": "1. In Table 2, Menlo is displayed with some benchmarks (e.g., MM-Eval and M-RewardBench), it might be good to specify the number of instances used as benchmarking (1776 instances).\n\n2. The Biggen-bench [1] also includes a multilingual category with 420 human judgments, it would be great if the authors could add it to Table 2.\n\n[1] Kim, S., Suk, J., Cho, J. Y., Longpre, S., Kim, C., Yoon, D., ... & Seo, M. (2025, April). The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) (pp. 5877-5919)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "edNRKAo4Hk", "forum": "QOWYX3Q2XS", "replyto": "QOWYX3Q2XS", "signatures": ["ICLR.cc/2026/Conference/Submission7714/Reviewer_M539"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7714/Reviewer_M539"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975367299, "cdate": 1761975367299, "tmdate": 1762919771336, "mdate": 1762919771336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript contributes to LLM-as-Judge by proposing MENLO, building a 6k+ human-annotated dataset, establishing four quality evaluation dimensions, showing RL's impact on LLM-as-judge and the dataset's benefit to multilingual RL, though the RL-enhanced LLM-as-judge still lags behind human annotation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper plans to open-source its dataset. A high-quality training dataset is currently in urgent need by the open-source community.\n- The paper’s dataset primarily contributes to the improvement of LLM-as-Judge — a critical need for the current RL paradigm, and a highly important topic that requires further advancement."}, "weaknesses": {"value": "The multilingual section of the paper is derived from direct translation, which may lack native-specific traits. For instance, the level of attention paid to art can vary across different countries, and this discrepancy may introduce a certain degree of bias."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3M0JOQaTDL", "forum": "QOWYX3Q2XS", "replyto": "QOWYX3Q2XS", "signatures": ["ICLR.cc/2026/Conference/Submission7714/Reviewer_yzH6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7714/Reviewer_yzH6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000884835, "cdate": 1762000884835, "tmdate": 1762919769895, "mdate": 1762919769895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MENLO, a framework for evaluating the native-like quality of multilingual LLM responses based on audience-design principles. MENLO defines four core dimensions, Fluency, Tone, Localized Tone, and Localized Factuality and provides localized prompt templates to elicit culturally grounded responses.\nMENLO further contributes a large scale multilingual human evaluation dataset comprising 6,423 prompt-response pairs with 81,014 annotations spanning 47 languages/varieties, achieving strong inter-annotator agreement (avg. Krippendorff’s α = 0.84).\nThe paper empirically demonstrates the superiority of pairwise over pointwise judgments for aligning evaluator preferences, and shows that incorporating structured rubrics improves pointwise reliability.\nAdditionally, the authors explore LLM-Judge training methods, including multi-task reinforcement learning and reward shaping to boost automatic judge models closer to human-level performance.\nFinally, the trained judges are shown to be effective as generative reward models, improving policy models’ multilingual proficiency. However, the study highlights that LLM-based evaluations can overestimate model performance relative to human raters, revealing an important limitation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Extensive Multilingual Coverage Demonstrates strong value by tackling the critical aspect of native-like linguistic quality, incorporating human validation across a broad set of languages.\nProvides substantial experimental diversity with 47 languages/varieties, 6,423 prompt–response pairs, and 81,014 annotations, ensuring wide multilingual representation.\n- Well-Structured and Systematic EvaluationOffers quantitative evidence that zero-shot pairwise evaluation consistently outperforms pointwise evaluation in both Macro-F1 and preference accuracy.\nEmpirically validates the practical utility of rubric-guided evaluation showing benefits for both absolute and comparative assessments, indicating strong applicability in real-world evaluation pipelines."}, "weaknesses": {"value": "- MENLO uses localized prompts and employs translators/annotators from the corresponding regions, there still exist diverse linguistic variations within a single locality across social groups, age ranges, and socioeconomic classes. Therefore, constructing gold labels based on a single demographic standard may introduce bias toward a particular perspective.\n- From a user perspective, even if a response appears native-like, its practical utility drops significantly if it is factually incorrect especially with respect to localized factual knowledge. If the reward model does not account for factuality, the system may optimize for style while increasing the risk of conveying incorrect information."}, "questions": {"value": "1. Do the four evaluation dimensions (Fluency, Tone, Localized Tone, Localized Factuality) include moral or behavioral conventions that are specific to certain regions or cultures?\n2. Pairwise evaluation involves comparing two responses at a time. Do the authors plan to conduct multi-sample comparison experiments, examining whether the presence or absence of rubrics affects performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wsYh2ouwF9", "forum": "QOWYX3Q2XS", "replyto": "QOWYX3Q2XS", "signatures": ["ICLR.cc/2026/Conference/Submission7714/Reviewer_kRGr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7714/Reviewer_kRGr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136102931, "cdate": 1762136102931, "tmdate": 1762919769108, "mdate": 1762919769108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}