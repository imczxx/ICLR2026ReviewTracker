{"id": "aKyBCsPOHB", "number": 7667, "cdate": 1758031267208, "mdate": 1763716712963, "content": {"title": "MedMT-Bench: Can LLMs Memorize and Understand Long Multi-Turn Conversations in Medical Scenarios?", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across various specialist domains and have been integrated into high-stakes areas such as medicine. However, systematically evaluating their capabilities remains a significant challenge, as existing medical-related benchmarks often focus on single-turn tasks or short dialogues and rarely stress-test the long-context memory, interference robustness, and safety defense required in practice. To bridge this gap, we introduce MedMT-Bench, a challenging medical multi-turn instruction following benchmark that simulates the entire diagnosis and treatment process, spanning pre-diagnosis, in-diagnosis, and post-diagnosis stages. Motivated by the practical problems observed in real-world implementations, MedMT-Bench operationalizes five core capabilities: 1) long-context memory and understanding; 2) resistance to contextual interference; 3) self-correction, affirmation and safety defense; 4) instruction clarification; and 5) multi-instruction response with interference. We construct the benchmark via scene-by-scene data synthesis refined by manual expert editing, yielding 400 test cases with an average of 22 turns (maximum 52), covering 24 departments and 9 sub-scenarios, including a multimodal subset. For evaluation, we propose an LLM-as-judge protocol with instance-level rubrics and atomic test points, validated against expert annotations with a human-LLM agreement of 91.94\\%. We test 17 frontier models, all of which underperform on MedMT-Bench (overall accuracy below 60.00\\%), with the best model reaching 59.75\\%. MedMT-Bench can be an essential tool for driving future research towards safer and more reliable medical AI. The benchmark is available in the supplementary materials.", "tldr": "We introduce MedMT-Bench, a new challenging benchmark for long medical conversations. We find that current SOTA LLMs severely fail in long-term memory, understanding and safety, revealing critical risks for real-world deployment.", "keywords": ["Mulit-Turn Instruction Following", "Benchmark", "Medical", "LLM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1d16e8e273f45bce801b48901e038308b490a6d.pdf", "supplementary_material": "/attachment/c0b777afe1323f0425299cbdef3ad6ca07d53f6d.zip"}, "replies": [{"content": {"summary": {"value": "This paper mostly focus on a 'multi-turn medical conversation' in LLM evaluation as contribution. The eval mostly focuses on instruction following on various medical scenarios that the authors generated. Things like prohibiting unsafe output, detect ambiguous queries, multiple queries handling etc. The generated data has 400 test cases about 22 turn average. And LLM as judge is used for atomic test points (ATPs). Some efficiency gains are discussed by using synthetic multi-agent generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The medical domain doesn't see much multi-turn conversation evaluation which makes the question at hand timely\n- The benchmark coverage is adequate, where they used pretty much all the frontier and open-source models so the numbers provided can help community to differentiate some of the models"}, "weaknesses": {"value": "- Overall the benchmark is only focusing synthetic data, i.e. no real patient data, no real world driven scenario. Although there are some expert reviews, the contribution is incremental. Take Healthbench as example, all of the scenarios are real world driven and are sourced globally to uncover some of the common issues LLMs have in medical domain. \n- Further, on some concepts, the authors equate “instruction following” with “medical safety and reasoning.” In high-stakes domains, those are distinct; a model can obey instructions yet make unsafe clinical inferences. This conflation limits theoretical clarity. The instruction following is solely deemed as the primary proxy for a model’s safety, reasoning, and reliability in medical contexts. This is a very narrow scope of the whole domain. For example, if the system prompt says “Do not provide drug brand names” and the model obeys, it is labeled “safe” — but it could still recommend a medically incorrect dosage or miss an urgent symptom, which the benchmark does not capture.\n- ATPs agreement was measured on a small subset of instances (unclear N). Statistical uncertainty is not reported. For a technical conference these things should be a given\n- Overall the benchmark may create a false sense of “medical readiness” since all dialogues are LLM-synthetic; true human–model interactive variance is not measured."}, "questions": {"value": "- Why use Gemini-2.5-Pro as both evaluator and one of the evaluated models? It risks model-family bias \n- What's the total number of rubrics across multi turn conversations?\n- Can you give more details on \"medical team\"? The paper lacks quantitative evidence of inter-annotator agreement or number of clinicians involved. “Professional medical data team” is vague and doesn't lend any credibility if no evidence"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hcMQDtn150", "forum": "aKyBCsPOHB", "replyto": "aKyBCsPOHB", "signatures": ["ICLR.cc/2026/Conference/Submission7667/Reviewer_FuhP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7667/Reviewer_FuhP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545127139, "cdate": 1761545127139, "tmdate": 1762919734197, "mdate": 1762919734197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new benchmark, MedMT-Bench, which has 400 samples, each with on average 22 turns. The benchmark asks questions along 5 key axis, including long context, self correction, and instruction clarification, with question coming from many different medical departments and scenarios. The authors propose adding atomic tests within each chat that are easier for the LLM auto-eval to judge, allowing the authors to get 92.18 percent agreement between human experts and Gemini 2.5 Pro."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I think the idea of the atomic test points is interesting, as a way to better use LLMs for automatic evaluation. \n\nI also think it's a valuable contribution to create more open source evaluation data along the key dimensions in the paper. \n\nThe paper is testing a large range of models (17) which is good to understand which models perform well in this domain."}, "weaknesses": {"value": "The most major weakness from my point of view of this work is that it's not possible to properly evaluate the contributions at in the paper as many details are missing, despite having a very long (60 pages) paper. In particular:\n\nIt's not clear if the accuracy metric is the fraction of the total test points that pass over the full corpus or if it's aggregated over the conversations.\n\nWhy does the models have a consistency of less than 50 % (random guessing if I understand the metric) without the atomic test points?\n\nThe atomic test points are insufficiently described in the main paper. How many are there per conversation? It's also unclear how many atomic test points the accuracy metric for each capability dimension represent.\n\nThere are no details on the human evaluation. Is this done on the full benchmark 400x(22 average turns). Who were the evaluators etc. \n\nIt would have been nice if there was an analysis how relevant/aligned performance on the test points was to performance on the downstream capabilities outlined in the paper. \n\nOther weaknesses: \n\nI think it's overselling the human-ai agreement when you report 92.18% as the human-AI agreement, as this is the highest agreement (human and Gemini 2.5 Pro) not the average between LLM and Human.\n\nPlease report the consistency rate for all the 7 models for which the human evaluation was conducted.\n\nMinor: some relevant recent work on Multi turn conversations is not cited: MMMT-IF: A Challenging Multimodal Multi-Turn Instruction Following Benchmark"}, "questions": {"value": "Clarification: Does the benchmark task that the LLM is asked to do corresponding to a real LLM use case in health care? Are LLMs already applied in this setting?\n\nDo we need a benchmark that combines multi-turn with complex instructions, at the same time as a medical domain specific one. Does it add value on top of having two evaluation benchmarks that already exist?\n\nPlease clarify how the consistency rate is computed, a simple equation would make it more clear.\n\nHave you considered the bias that may come from evaluating Gemini 2.5 Pro with Gemini 2.5 Pro also as the judge (this has for instance been documented in the paper MMMT-IF: A Challenging Multimodal Multi-Turn Instruction Following Benchmark)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HQLmjMY4xy", "forum": "aKyBCsPOHB", "replyto": "aKyBCsPOHB", "signatures": ["ICLR.cc/2026/Conference/Submission7667/Reviewer_B8DY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7667/Reviewer_B8DY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783106495, "cdate": 1761783106495, "tmdate": 1762919733851, "mdate": 1762919733851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors introduce MedMT-Bench to evaluate the long multi-turn instruction-following capabilities of LLMs on medical domain in five different dimensions as: (i) long-context memory and understanding, (ii) resistance to contextual interference, (iii) self-correction, affirmation and safety defense, (iv) instruction clarification; and (v) multi-instruction response with interference. Their evaluation data is generated by using a multi-agent pipeline and refined by medical experts for realism, resulting with 400 manually curated test samples averaging 22 turns.  Authors use an LLM-as-a-Judge (LaaJ) metric and evaluate 17 different LLM with both closed- and open-source version, where all score below 60%; revealing persistent weaknesses in long-context understanding and safety compliance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Authors focus on multi-turn evaluation in the medical domain, which is both timely and critically important. \n- Authors conduct comprehensive analysis across different LLMs and share insightful observations, providing possible opportunities for future work in both pure LLM research and medical domain.\n- Proposed data generation pipeline combines multi-agent synthesis with manual expert editing, resulting in 400 high-quality samples; which is promising for ensuring the reliability and accuracy of the benchmark."}, "weaknesses": {"value": "- The main limitation of the paper is its heavy dependence on LaaJ during evaluation, which poses additional concerns for specialized domains like medicine. Although the authors conduct human analysis, there is no guarantee that the LaaJ model (Gemini in this case) has sufficient domain knowledge to provide accurate judgments, especially when tasks increase in difficulty. In such situations, LaaJ may prove untrustworthy, especially for medical factuality. A multi-agent LaaJ approach could serve as an interim solution, where knowledge gaps in one model are compensated by others. Furthermore, authors do not provide robustness analysis for Gemini's evaluation performance, such as multiple evaluation runs with reported standard deviations, or cross-validation using different LLM judges with score correlation analysis. Additionally, since LLMs typically find ranking easier than absolute scoring, a ranking-based LaaJ approach might prove more reliable.\n\n- Another concern is evaluation cost and practicality. Given that Gemini is a proprietary model requiring paid API usage, and the benchmark involves long-context, multi-turn interactions, the token consumption per evaluation can be substantial. The paper would benefit from a cost analysis or discussion of the feasibility of large-scale use.\n\n- For practical deployment, such an environment should be user-friendly and easy to debug (for example for understanding LLM failure cases). How do the authors ensure that reviewers can confidently assess the evaluation environment's ease of use?"}, "questions": {"value": "1. Did the authors conduct statistical significance testing? Are the results statistically significant?\n2. How sensitive are the results to different prompt structures, temperature variations, or LaaJ model selection?\n3. How were safety-related turns (e.g., mental health, self-harm prompts) annotated and verified?\n4. How do the authors ensure medical factual accuracy, given that LLMs are prone to hallucination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "462Mguv3O1", "forum": "aKyBCsPOHB", "replyto": "aKyBCsPOHB", "signatures": ["ICLR.cc/2026/Conference/Submission7667/Reviewer_v846"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7667/Reviewer_v846"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932776745, "cdate": 1761932776745, "tmdate": 1762919733479, "mdate": 1762919733479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The papeer proposes MedMT-Bench, a benchmark for evaluating large language models in clinical multi-turn dialogue. It focuses on realistic medical consultation and treatment scenarios. The authors introduce multiple evaluation dimensions addressing long clinical conversation challenges. Data are generated using a multi-agent synthetic pipeline followed by expert medical editing. Evaluation uses fine-grained atomic test points, achieving up to 92.18% agreement with human judgments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Medical multi-turn conversations are both high-risk and frequent in real clinical workflows. The paper is well-motivated by identifying that existing medical benchmarks primarily test static knowledge via single-turn or short-dialogue formats.\n2. The benchmark is structured around the entire diagnosis and treatment process from pre-diagnosis, to post-diagnosis. This improves the validity and realism of the scenarios.\n3. The paper introduces an atomic test points mechanism for its LLM-as-judge protocol that does enhances evaluation reliability."}, "weaknesses": {"value": "1. While the benchmark covers 24 medical departments, the main results in Table 3, 4 only report aggregate performance. There is no analysis of which departments are more or less difficult, which should be common for medical benchmark studies and provide more clinical insights.\n2. Lack of modality ablation study: the paper presents results for text, image, and merged subsets but does not provide a quantitative breakdown of which capabilities are most impacted by the image modality. The analysis remains at a high level for model performance.\n3. Benchmark sample size 400 may be small relative to task diversity.\n4. LLM-based evaluation may reinforce model biases. Also, safety critical metrics false positives not measured in the experiments.\n5. Some texts in image are too small to recognize (Figure 5)."}, "questions": {"value": "1. The multi-agent synthetic data generation may inherit biases (e.g., style, over-sensitivity, or error propagation) from the models used for synthesis. Can you describe how to address this risk?\n2. The atomic test points are central to the evaluation , but the main text provides only one brief example (\"check whether... Asiplin\"). Could you clarify the full methodology to faciltate reader understanding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gJCpCP3GLZ", "forum": "aKyBCsPOHB", "replyto": "aKyBCsPOHB", "signatures": ["ICLR.cc/2026/Conference/Submission7667/Reviewer_uAto"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7667/Reviewer_uAto"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762281045359, "cdate": 1762281045359, "tmdate": 1762919733191, "mdate": 1762919733191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}