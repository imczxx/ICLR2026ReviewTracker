{"id": "6akKsczEYY", "number": 7566, "cdate": 1758027769201, "mdate": 1759897845982, "content": {"title": "Truncated Proximal Policy Optimization", "abstract": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds.  First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance.  We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5× and outperforms its existing competitors.", "tldr": "Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation.", "keywords": ["Reinforcement learning", "Proximal Policy Optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce4399ef46563731c4c1ecd86bf34f2a204fa793.pdf", "supplementary_material": "/attachment/046d373fc643918f4053e4b6e3157bb11b1a0a1a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Truncated Proximal Policy Optimization (T-PPO), an algorithm aimed at improving the training efficiency of LLM on reasoning tasks. The core idea is to mitigate the hardware underutilization caused by PPO's on-policy requirement to wait for full sequence generation. T-PPO introduces two main components: (1) Extended Generalized Advantage Estimation (EGAE), a method to compute advantage estimates from incomplete trajectories by truncating the advantage sum, and (2) a decoupled optimization strategy where the policy is updated frequently on partial data, while the value function is updated less frequently on completed trajectories. The authors test their method on the AIME 2024, showing speedup and performance improvement compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe paper studies a critical and widely recognized bottleneck in RL for LLMs: the inefficiency of on-policy sampling.\n\n2.\tThe paper reports good improvement and speedup over baselines on math reasoning tasks."}, "weaknesses": {"value": "1.\tThe prposed EGAE appears to be the standard truncated-trajectory GAE which is proposed in the original PPO paper [1]. Truncated fixed-length trajectory segments is long used for GAE in PPO. This is not novel.\n\n2.\tThe evaluation has significant weakness. The reliance on a single benchmark and a single model prevents any claims of general applicability. Although the paper claims faster convergence, but only the convergence steps for the proposed T-PPO and PPO-EWMA are reported, with other baseline methods’ statistics missing.\n\n3.\tThe paper's proposed T-PPO seems to be more related to asynchronous reinforcement learning methods for training LLMs, which also aims to decouple data collection and model updates to improve wall-clock efficiency [2]. The paper should add discussions and compare with these methods.\n\n[1] Proximal Policy Optimization Algorithms\n\n[2] Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL"}, "questions": {"value": "Please see Weaknesses Section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MtZqRS4QJD", "forum": "6akKsczEYY", "replyto": "6akKsczEYY", "signatures": ["ICLR.cc/2026/Conference/Submission7566/Reviewer_64VD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7566/Reviewer_64VD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760681785161, "cdate": 1760681785161, "tmdate": 1762919658091, "mdate": 1762919658091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an improved policy-gradient reinforcement learning method based on Proximal Policy Optimization (PPO), termed Truncated PPO (T-PPO). The primary contribution lies in the training system design, which substantially improves GPU utilization during training.\n\nHowever, the proposed Extended Generalized Advantage Estimation (EGAE) appears flawed. The value-function update can exhibit highly variable effective batch sizes, potentially leading to unstable updates.\n\nThe experimental evaluation is also quite limited: only one model and one dataset are considered. The paper appears to report a single training run, with no sensitivity analysis over random seeds, datasets, or model architectures. Key ablations (e.g., window length) are missing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and well organized, making the method easy to follow.\n2. The proposed approach appears computationally efficient for the chosen model and dataset."}, "weaknesses": {"value": "1. The paper lacks a Related Work section. The authors should review recent advances in related areas.\n2. The value function estimates the expected return under the current policy; the two should be tightly coupled. The current design partially decouples value training from policy updates.\n3. The experiments are too limited: only one model and one dataset are used, seemingly with a single training run. There is no robustness analysis across seeds, datasets, or models.\n4. Ablation studies on key components—such as the truncation window length—are missing."}, "questions": {"value": "1. The proposed \"Extended Generalized Advantage Estimation (EGAE)\" is flawed from my perspective. Before training start, value function should always give \"0\" for any trajectory. The KL penalty will not work as $\\pi_\\theta=\\pi_{ref}$. In such case, in GAE, $\\delta_t$ should be 0 for all $t\\in\\{1,2,3...,T-1\\}$, but for last token, $\\delta_T=r_T$. If the returns are truncated by a finite window, the estimated advantages may be substantially biased, potentially causing instability in the initial training stage. How is this addressed?\n2. Did the authors warm up the value function before commencing T-PPO training? If so, how many steps were used? If not, how do you mitigate the issue raised in Q1?\n3. In Figure 1, the reward curves increase monotonically and do not appear to converge. What happens if training continues for more steps?\n4. The response length in Figure 6 appears unstable. In my experience, such instability can lead to large variance across datasets or random seeds. Could the authors provide additional results to assess robustness?\n5. During policy updates, all generated tokens are included. However, the value-function update rule seems to induce large variability in the total number of tokens per batch. Since losses are averaged over the batch, could this introduce instability or bias the updates toward particular tokens? Have the authors considered or controlled for this effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qGhpd11Wrt", "forum": "6akKsczEYY", "replyto": "6akKsczEYY", "signatures": ["ICLR.cc/2026/Conference/Submission7566/Reviewer_MPdS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7566/Reviewer_MPdS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761129260496, "cdate": 1761129260496, "tmdate": 1762919657710, "mdate": 1762919657710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TPPO, an improved asynchronous version of PPO designed to enhance computational efficiency. TPPO achieves more than 2.5× faster computation while maintaining nearly the same performance as standard PPO."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "This is a method that the research community needs. Because Chain-of-Thought models produce very long token sequences, training them is practically infeasible for most researchers outside major companies. Although the proposed method is limited to PPO, a technique that accelerates training for long output sequences has significant practical value.\n\nThe method description is clear and easy to follow.\n\nThe experiments convincingly support the authors’ claims."}, "weaknesses": {"value": "Limited experimental comparisons.\nThe paper reports experiments on only one dataset. The core assumption of TPPO, \nV(s_l) = V(s_{l-1}), is a strong approximation whose theoretical error bound is not proven. Therefore, the authors should conduct experiments on multiple datasets to demonstrate that the approximation holds in general. In particular, it is unclear whether TPPO performs well on shorter datasets where the value function changes rapidly, or on non-mathematical domains. Based on the presented results alone, it is difficult to confirm the generality of TPPO.\n\nInsufficient ablation studies.\nThe paper presents results only on the Qwen2.5-32B model. While intuitively the method should work on other models as well, the paper should still provide empirical evidence—such as how much the speed improvement varies with model size. Moreover, there is even one unused page (the paper uses 8 out of 9 pages), so additional experiments could easily be included. More thorough ablation studies would greatly strengthen the paper’s credibility. Specifically, the effects of the Dual Update Mechanism and Token Filtering Strategy are underexplored. Since the Dual Update Mechanism could in principle be used even without TPPO, its independent contribution to performance should be analyzed.\n\nLimited novelty.\nThe proposed Token Filtering Strategy and Dual Update Mechanism seem more like engineering optimizations than conceptual innovations. Only the EGAE component appears to address a new issue in PPO, yet even that looks like a minor variant of GAE. While the paper has some engineering novelty, it lacks deeper insight into reinforcement learning or AI theory."}, "questions": {"value": "The Dual Update Mechanism appears applicable to standard PPO as well. What would be the results if applied to vanilla PPO? Would performance or speed differ?\n\nHow would the speedup of TPPO change with smaller models, e.g., a 4B model instead of 32B? Would the performance gap between PPO and TPPO still be significant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mSPc6WrvPE", "forum": "6akKsczEYY", "replyto": "6akKsczEYY", "signatures": ["ICLR.cc/2026/Conference/Submission7566/Reviewer_SM7P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7566/Reviewer_SM7P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935745087, "cdate": 1761935745087, "tmdate": 1762919657094, "mdate": 1762919657094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to extend on-policy RL methods such as PPO to the off-policy or partial-rollout setting. The authors propose T-PPO, which modifies the GAE formulation to handle truncated trajectories, enabling effective learning when only partial rollouts are available."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The target problem is important, as partial rollouts naturally arise in large-scale RL for LLMs. The presentation is clear, and the reported experimental results show promising improvements."}, "weaknesses": {"value": "The method is incremental. The proposed EGAE differs little from standard GAE, and the overall algorithm resembles prior partial-rollout implementations such as those used in Kimi. The paper does not clearly justify what is new or why the proposed adjustment yields improvement.\n\nIn addition, the analysis is limited. There is no theoretical or empirical discussion of bias introduced by truncation, nor comparison with other off-policy corrections such as partial rollout with importance sampling, which would be the most direct baseline."}, "questions": {"value": "Is the token filtering step equivalent to partial rollouts rather than an actual filtering mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0oUVwXBBQp", "forum": "6akKsczEYY", "replyto": "6akKsczEYY", "signatures": ["ICLR.cc/2026/Conference/Submission7566/Reviewer_dNEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7566/Reviewer_dNEk"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973162721, "cdate": 1761973162721, "tmdate": 1762919656644, "mdate": 1762919656644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}