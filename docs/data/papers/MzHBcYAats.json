{"id": "MzHBcYAats", "number": 1807, "cdate": 1756937676586, "mdate": 1759898185441, "content": {"title": "Complex Logical Instruction Generation", "abstract": "Instruction following has catalyzed the recent era of Large Language Models (LLMs) and is the foundational skill underpinning more advanced capabilities such as reasoning and agentic behaviors. As tasks grow more challenging, the logic structures embedded in natural language instructions becomes increasingly intricate. However, how well LLMs perform on such logic-rich instructions remains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a scalable, automated framework for generating verifiable instructions from code functions, which can naturally express rich logic such as conditionals, nesting, recursion, and function calls. We further curate a collection of complex code functions and use LogicIFGen to construct LogicIFEval, a benchmark comprising 426 verifiable logic-rich instructions. Our experiments demonstrate that current state-of-the-art LLMs still struggle to correctly follow the instructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the instructions, revealing significant deficiencies in their capacity to handle instructions that involve complex logical structures.", "tldr": "LLMs struggle with complex, logic-rich instructions, so we introduce an automated framework to generate verifiable logic-intensive tasksa benchmark of 426 such tasks, showing that even state-of-the-art models correctly follow fewer than 60% of them.", "keywords": ["Large Language Model; Instruction Following; Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b36b3601205b230e3c4c5afafbdd570a140e89c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes LogicIFGen, a framework which enables automatic generation of test instances for logic relations based on code. The framework is scalable and verifiable. Using this framework, the authors construct LogicIFEval, a benchmark curated to evaluate LLM capabilities in logic understanding and instruction following. Extensive empirical experiment results show that many of the models have poor performance on the benchmark, highlighting the issues in using LLMs to handle complex logic queries."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed framework, LogicIFGen, is scalable and verifiable. The authors also conduct human studies to show that the generation is of high quality.\n2. The research question itself is interesting and important: LLMs have to be able to follow various types of instructions, which potentially involve complex logic relations, to properly serve users.\n3. Writing and presentation are very clear. It is very easy to understand the authors' points."}, "weaknesses": {"value": "1. Related work section is not comprehensive enough. It reviews many works which are relevant but not closely relevant to the research question in the general LLM reasoning area. More strongly related works should be thoroughly discussed as I will elaborate below.\n\n2. The novelty of the research question is not extremely clear given many existing works in highly similar areas. This is my biggest concern. I think there are many existing works in code execution which are highly relevant, but not fully discussed in the related work section. For example, [1,2,3,4, inter alia] have done comprehensive analysis on the limitations of LLMs in simulating code execution result, trace, and executing natural language instructions which can be solved/verified by code. The authors should more carefully review relevant literature and argue for their novelty.\n\n[1] La Malfa, E., Weinhuber, C., Torre, O., Lin, F., Marro, S., Cohn, A., ... & Wooldridge, M. (2024). Code simulation challenges for large language models. arXiv preprint arXiv:2401.09074.\n\n[2] La Malfa, E., Weinhuber, C., Torre, O., Lin, F., Huang, X. A., Marro, S., ... & Wooldridge, M. (2025). Code Simulation as a Proxy for High-order Tasks in Large Language Models. arXiv preprint arXiv:2502.03568.\n\n[3] Sun, S., Hsieh, C. P., Ladhak, F., Arakelyan, E., Serano, S. A., & Ginsburg, B. (2025). L0-Reasoning Bench: Evaluating Procedural Correctness in Language Models via Simple Program Execution. arXiv preprint arXiv:2503.22832.\n\n[4] Liu, C., Dylan Zhang, S., Ibrahimzada, A. R., & Jabbarvand, R. (2024). Codemind: A framework to challenge large language models for code reasoning. arXiv e-prints, arXiv-2402."}, "questions": {"value": "1. It would be really helpful if the authors could kindly address concerns mentioned above.\n\n2. The theme of this paper is testing complex logic. What is the great advantage of using code as backbone to generating the questions instead of using classic formal logic frameworks such as first-order logic? The formal logic is also verifiable and easily programmable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9OS2maU6Up", "forum": "MzHBcYAats", "replyto": "MzHBcYAats", "signatures": ["ICLR.cc/2026/Conference/Submission1807/Reviewer_xW3R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1807/Reviewer_xW3R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673946346, "cdate": 1761673946346, "tmdate": 1762915896092, "mdate": 1762915896092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LogicIFGen, a method that uses an LLM to anonymize functional programs. This method further introduces state tracking variables and translates the programs into a natural language description of the function. The method is used to create a benchmark dataset, called LogicIFEval, for evaluating the ability of LLMs to follow instructions when asked to execute the provided natural language instruction step-by-step for several test cases. The complexity of the instructions is captured using a syntax-tree-based measure on the anonymized programs. Several frontier LLMs were tested on the benchmark, revealing declining performance with increasing instruction complexity. Further analysis of the models' different failure modes was conducted."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Overall, the paper is easy to follow. The experiments appear to be fully documented and reproducible, and the topic of assessing LLM abilities in relation to the logical complexity of the task is highly relevant."}, "weaknesses": {"value": "1.  The paper's main weakness is the limited relevance and coherence of the analysis conducted. Although assessing the abilities of LLM in instruction-following tasks by investigating dependency on increasing levels of logical complexity is promising, the main results focus on comparing the performance of different models. The overall pattern of declining performance with increasing instruction complexity is briefly discussed. However, the subsequent analysis of different failure modes does not consider varying difficulty levels. Hence, there is no insight into whether or how these failure modes depend on instruction complexity. This does not align with the paper's motivation and potential novelty. To significantly improve the paper's quality, the benchmark and related analysis should focus on the connections between LLM behavior and the logical complexity of instructions. This would allow the paper to fulfill the promises made in the abstract and introduction. \n2.  A second weakness is the lack of discussion or theory on the motivation behind the introduced dataset. Why is it important to explicitly prohibit the model from using programming or tools in general? Does the benchmark provide evidence for or against systematic compositionality in LLM behavior?\n\nIn summary, the paper appears to be immature in its current state. It presents an interesting idea for using LLM to process a selection of functions into datasets for assessing LLM instruction-following abilities in relation to task complexity. However, it lacks a sufficient theory explaining why and how these abilities can be assessed with the proposed dataset. Additionally, the analysis is insufficient for meaningfully evaluating the LLM behavior related to task complexity."}, "questions": {"value": "1.  Table 1 should specify whether performance is a percentage of test cases or questions. The number of questions in brackets after the difficulty levels seems to indicate a percentage of questions. However, it is unclear whether a question is counted if only some test cases are correct in terms of output, state, or both. The numbers appear to be a percentage of test cases, especially when compared to Figure 3. For example, GPT-5 has state tracking errors in 28.1% of questions but could not achieve a \"state\" performance of 89.44% correct answers; rather, it achieved this result for only a subset of test cases.\n2.  Why is the fourth result group labeled \"Average\" when the subtitle says \"Overall\"? The most intuitive average would be the weighted and unweighted means of the three columns on the left, respectively. However, that is not the case. Is this about overall performance rather than an average?\n3.  Figure 3 needs a subtitle and should specify if it is about at least one of the specific failure modes in some test case per question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kMftzIDIDb", "forum": "MzHBcYAats", "replyto": "MzHBcYAats", "signatures": ["ICLR.cc/2026/Conference/Submission1807/Reviewer_MrYZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1807/Reviewer_MrYZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737584889, "cdate": 1761737584889, "tmdate": 1762915895351, "mdate": 1762915895351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response 1: Why We Prohibit LLMs Using Tools"}, "comment": {"value": "The reason we prohibit the model from using programming or external tools is that our goal is to evaluate the **instruction-following ability of LLMs**, rather than their **coding ability**. Instruction following is a more general and fundamental capability because, in most real-world scenarios, LLMs are expected to follow the logic embedded in natural language through text generation.\n\nFor example, a simple instruction such as “When the user asks something harmful to society, refuse and redirect the conversation to a safer topic” requires the model to follow the logic purely via natural-language behavior. More complex cases can be found in the Claude system instructions (https://www.dbreunig.com/2025/05/07/claude-s-system-prompt-chatbots-are-more-than-just-models.html), which describe sophisticated agentic behaviors involving conditions, error handling, and reflection. Importantly, the execution of these behaviors is still realized primarily through text generation.\n\nWe acknowledge that real-world instructions may involve guidance on when and how to use external tools, and that LLMs are often expected to call tools in such scenarios. However, **allowing tool use during evaluation would make the evaluation both complicated and unfair**: there exists a vast number of possible tools, and different models have unequal tool-calling capabilities. Our objective is to **isolate and evaluate the instruction-following ability of LLMs**, without introducing potential confounding factors. To achieve this, we perform rigorous data filtering to guarantee that every substep of the instruction in our benchmark is simple enough for the model to complete internally, without the help of external tools (see Section 3)."}}, "id": "5AgahLzkc2", "forum": "MzHBcYAats", "replyto": "MzHBcYAats", "signatures": ["ICLR.cc/2026/Conference/Submission1807/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1807/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1807/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763238141638, "cdate": 1763238141638, "tmdate": 1763238141638, "mdate": 1763238141638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LogicIFGen and LogicIFEval. LogicIFGen is a method framework of generating natural language instructions based on code, and LogicIFEval is a benchmark derived by using the framework on hard coding problems. Then the authors tested the current LLMs on the benchmarks and shows it is very challenging for the current models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper studies an interesting problem of building complex natural language instructions from code and test the model's instruction following ability by using these code generated instructions.\n- The paper's main pipeline of building these instructions is interesting and solid. The paper also did a decent job in collecting the coding problems which could be a contribution to the community."}, "weaknesses": {"value": "- The naming is very confusing. Fundamentally LogicIFGen is the framework and LogicIFEval is derived from using this. But the naming made this very misleading. \n- I think the analysis part is not solid enough. For example, some simple reasoning baselines such as Program-of-Thought should be tested and analyzed. Since the instruction is derived from code, I think in general the paper should consider the aspect of reasoning with code."}, "questions": {"value": "- I would like to see how the current models w/ strong reasoning methods perform on this task.\n- I'm curious how do the author view the difference between using natural language instructions vs directly using code as instructions and asking the model to follow the logic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SDCvLswdrm", "forum": "MzHBcYAats", "replyto": "MzHBcYAats", "signatures": ["ICLR.cc/2026/Conference/Submission1807/Reviewer_fakE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1807/Reviewer_fakE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991002020, "cdate": 1761991002020, "tmdate": 1762915895058, "mdate": 1762915895058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}