{"id": "iYQgXETC1D", "number": 1651, "cdate": 1756901191536, "mdate": 1759898196985, "content": {"title": "iFusion: Integrating Dynamic Interest Streams via Diffusion Model for Click-Through Rate Prediction", "abstract": "Click-through rate (CTR) prediction is crucial for recommendation systems and online advertising, relying heavily on effective user behavior modeling. While existing methods separately refine long-term and short-term interest representations, the fusion of these behaviors remains a critical yet understudied challenge due to misaligned feature spaces, disjointed modeling, and noise propagation in short-term interests. To address these limitations, we propose iFusion, a diffusion-based generative user interest fusion method, which reformulates interest fusion as a conditional generation process. iFusion leverages short-term interests as conditional guidance and progressively integrates long-term representations through denoising, eliminating reliance on linear fusion assumptions. Our framework introduces two key components: (1) the Disentangled Classifier-Free Diffusion Guidance (DCFG) Mechanism, which adaptively disentangles core preferences from transient fluctuations, and (2) the Mixture AutoRegressive Denoising Network (MARN), which enables joint interest modeling and fusion through autoregressive denoising. Experiments demonstrate that iFusion outperforms baselines across public and industrial datasets, as well as in online A/B tests, validating its effectiveness in robust CTR prediction. This work establishes a new paradigm for generative user interests  fusion in CTR prediction.", "tldr": "", "keywords": ["User Behavior Modeling", "Diffusion Models", "Dynamic Interest Fusion"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a2bf6e5c5e2de49a75933ce38606228a13e9a78a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents iFusion method for CTR prediction. Instead of simply combining long-term and short-term user interests with methods like attention, it uses a diffusion model to generate a new, fused interest representation. The generation process starts with the long-term interest and is guided by the short-term behaviors, effectively denoising and integrating the signals. The authors introduce two key components: DCFG to separate core interest from noise, and MARN to process sessions sequentially."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The core idea of treating interest fusion as a conditional generation problem is a leap forward. It offers a way to handle challenges like misaligned feature spaces and noise.\n- Both DCFG and MARN are well-motivated solutions to specific problems in this new paradigm. DCFG's approach to disentangling signals is interesting , and MARN's autoregressive design is well-justified by theory.\n- The authors address the slow inference of diffusion models using a consistency loss, enabling effective one-step generation. The reported minimal increase in online latency makes it a practical solution, not just an academic one."}, "weaknesses": {"value": "- Justification for DCFG could be stronger. The analogy of using pooling as a low-pass filter and attention as a high-pass filter is intuitive, but a small empirical study (e.g., visualizing gradient directions) to support this \"functional orthogonality\" claim would make it more concrete.\n- The mixture in MARN is unclear. The paper explains the autoregressive part well, but the role of mixture in the name is not clearly defined. A brief clarification would be helpful.\n- Generalizability of one-step inference. The paper shows that one-step inference is optimal on their industrial dataset. It would be great to confirm if this finding also holds for the public datasets, which would strengthen the claim's generality."}, "questions": {"value": "Q1: The design of the DCFG mechanism is very insightful, drawing an analogy between AvgPool/Attention and low-pass/high-pass filters to disentangle core preferences from transient fluctuations. This is a strong intuition. Could you provide more empirical evidence to support this analogy and its effectiveness? For instance, have you measured the cosine similarity between the guidance gradients produced by the core preferences (g_cp) and transient fluctuations (g_tf) during training to verify their approximate orthogonality?\n\nQ2:The consistency loss is key to improving efficiency. To better understand the performance-efficiency trade-off, could you report the performance of the full iFusion model *without* the consistency loss but using multiple inference steps (e.g., 10 or 50 steps)? This would help quantify how much accuracy, if any, is traded for the significant speed-up.\n\nQ3:In the analysis of zero-data scenarios (Appendix N), you mention that iFusion can infer stable interests even when long-term historical data is missing. Could you provide more intuition on what the plausible interest points generated by the model look like in such cases? For instance, does it tend to generate representations corresponding to popular items, or does it leverage the limited short-term signals more heavily to make a best guess?\n\nQ4:The result in Figure 4a is critical, showing that a single inference step with a cosine schedule achieves the best performance on the industrial dataset. This is vital for practical deployment. Did you observe a similar trend on the public datasets (e.g., Amazon, Taobao)? Is one-step inference a generally optimal strategy for iFusion, or is its effectiveness dependent on the scale and distribution of the data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hAJosN7aaM", "forum": "iYQgXETC1D", "replyto": "iYQgXETC1D", "signatures": ["ICLR.cc/2026/Conference/Submission1651/Reviewer_h94K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1651/Reviewer_h94K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819010652, "cdate": 1761819010652, "tmdate": 1762915840875, "mdate": 1762915840875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents iFusion, a novel diffusion-based generative framework for click-through rate (CTR) prediction, addressing the problem about fusion of long-term and short-term user interests. Unlike traditional methods relying on late fusion and linear assumptions, iFusion reformulates interest fusion as a conditional generation process. The paper introduces two key components: DCFG to separate core preferences from transient fluctuations during generation and MARN to model short-term interest evolution in an autoregressive manner. Extensive empirical evaluations on public and industrial datasets (including online A/B testing) demonstrate that iFusion significantly outperforms state-of-the-art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a generative diffusion-based approach for interest fusion, which is a novel perspective in CTR modeling. Reformulating fusion as a conditional denoising process is conceptually strong and addresses key limitations of deterministic fusion.\n2. The paper provides rigorous theoretical analysis to justify the design of DCFG and MARN.\n3. The experiments span four datasets (public and industrial) and include online A/B tests. The reported AUC improvements and real-world CTR gains are substantial, supporting the practical value of the model."}, "weaknesses": {"value": "Please refer to the Questions part."}, "questions": {"value": "1. What is the long-term and short-term encoder the method is using? Are the encoders of LTI and SSI (in Figure 2 (a)) essential to the final performance? There seems no analysis or experiments on the encoder part.\n2. For the online A/B testing, what is the baseline model? Is it a traditional sequential RS model or a diffusion model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ebHu35l2Q8", "forum": "iYQgXETC1D", "replyto": "iYQgXETC1D", "signatures": ["ICLR.cc/2026/Conference/Submission1651/Reviewer_qLFV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1651/Reviewer_qLFV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896414280, "cdate": 1761896414280, "tmdate": 1762915839713, "mdate": 1762915839713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes iFusion, a novel CTR framework that reformulates interest fusion as a conditional generation process. The motivation of generative interest fusion is interesting. The proposed iFusion integrates long-term and short-term user interests through a denoising diffusion model, enhanced by a Disentangled Classifier-Free Guidance mechanism and an AutoRegressive Denoising Network. Experiments on several datasets demonstrate the effectiveness of the proposed iFusion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a well-motivated perspective by viewing interest fusion in CTR prediction as a conditional generation problem. This generative reformulation is conceptually appealing and offers a new way to understand how long-term and short-term user interests can be dynamically integrated.\n- The proposed framework introduces a diffusion-based fusion mechanism equipped with a Disentangled Classifier-Free Guidance (DCFG) and a Mixture AutoRegressive Denoising Network (MARN). Overall, these two designed components seem reasonable to implement a stable denoised diffusion process.\n- Experiments on both benchmarks and online tests verify the effectiveness of the proposed iFusion."}, "weaknesses": {"value": "- While the idea of reformulating interest fusion as a conditional generation process is interesting, the paper could better highlight its concrete technical contributions. The description of the proposed modules (DCFG and MARN) is somewhat difficult to follow, and their precise roles or advantages over standard diffusion variants are not always made explicit. A clearer explanation or ablation focusing on the novelty of each component would strengthen the paper.\n- The diffusion process is inherently time-consuming, which is a critical concern in CTR prediction scenarios where latency is a major constraint. Although the paper mentions a consistency loss to accelerate inference, it lacks a detailed discussion or quantitative analysis of complexity, inference time, and deployment feasibility.\n- The experiments mainly use a simple DNN backbone for CTR prediction. To fully demonstrate the generality of iFusion, it would be valuable to show results when integrating with more advanced or widely used architectures, such as DeepFM, DCN, or AutoInt, to confirm the robustness and adaptability of the proposed framework."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "D3fBlJVyMy", "forum": "iYQgXETC1D", "replyto": "iYQgXETC1D", "signatures": ["ICLR.cc/2026/Conference/Submission1651/Reviewer_n48p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1651/Reviewer_n48p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997321884, "cdate": 1761997321884, "tmdate": 1762915839502, "mdate": 1762915839502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to tackle interest fusion for CTR predictions. Given a long-term sequence and a short-term sequence partitioned into sessions, the goal is to learn a fusion map for downstream CTR. The key insight is to treat fusion as conditional generation with a diffusion model conditioned on the short-term sequence embeddings. The authors argue that such a generative fusion paradigm that disentangles stable vs. transient short-term behaviors during guidance and autoregressively conditions on multi-session contexts can yield consistent offline gains and online lieft with negligible latency. Specifically, this paper employs two mechanisms to structure the conditional guidance and how sessions are injected, namely DCFG splits guidance into core‑preference (low‑pass C‑Filter) and transient fluctuation (high‑pass T‑Filter) components with tunable strengths $\\gamma_{\\rm cp}, \\gamma_{\\rm tf}$, yielding an additive score decomposition supported by an energy‑based view and architectural assumptions about Hessian eigenspaces; MARN injects the $K$ shot-term sessions autoregressively in the reverse process via chain rule conditioning. For low latency, a consistency loss is used to encourage step‑invariance, which helps to enable one‑step inference. The proposed method demonstrate AUC improvements by 0.003-0.007 over the strongest baseline on multiple datasets; an online A/B tests reports improved CTR and eCPM with increased latency. The ablation study on the industrial set demonstrates the usefulness of the incorporated components."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is intuitive, based on the common understanding of user behaviors, and reasonably demonstrated.\n2. The DCFG formalizes the multi-scale guidance, and the MARN motivates AR conditioning for the dynamics across multi-session contexts. \n3. The breadth of empirical studies is commendable, which includes four datasets (with the industrial dataset included as well)."}, "weaknesses": {"value": "1. The core motivation targeting the claimed research gap. That is, the long- and short-term features are misaligned, while being intuitive,  was not empirically demonstrated, nor has solid support from literature.\n2. Theorem 1's orthogonality is assumed via architectural constraints, namely C-filter and T-filter can induce orthogonal dominant eigenspaces in the Hessians of their respective energy functions. This is a strong assumption that was not empirically verified.\n3. The bound presented in Theorem 2 omits definitions and conditions.\n4. All models were trained for one epoch without justification. In practice, many baselines typically benefit from longer training, nor the authors provide learning-curve evidence, which raises concerns regarding the small AUC margins as the claimed performance gain could shift given reasonable hyperparameter tuning. Eventually, this affects the strength of the offline comparison.\n5. No anonymous code or explicit reproducibility statement. While some details are partly included in the appendix but I personally don't think they are sufficient for full replication."}, "questions": {"value": "1. Why limit all baselines to one epoch? It is suggested to report learning curves or early-stopping results showing that additional epoch would not change the performance.\n2. In Figure 3b, AR-Attn slightly exceeds Ours. Which added components reduce AUC and why are they kept in Ours eventually?\n3. It is also suggested to report gradient/Hessian alignment between C-filter and T-filter over training to substantiate functional orthogonality claims.\n4. Can you please precisely define the NAR family and constants in Theorem 2 and estimate session mutual information on your datasets to check the stated dependence condition?\n5. What is the training $T$ and how sensitive are results to $T$ when using one-step inference?\n6. Beyond Appendix N, it would be interesting to show the method's robustness under somehow behavioral shocks (e.g., the promotion windows as shown in Figure 1), cold-start sessions, and heterogeneous event mixtures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vz3TjqkIAn", "forum": "iYQgXETC1D", "replyto": "iYQgXETC1D", "signatures": ["ICLR.cc/2026/Conference/Submission1651/Reviewer_yR4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1651/Reviewer_yR4B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762089793921, "cdate": 1762089793921, "tmdate": 1762915839303, "mdate": 1762915839303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}