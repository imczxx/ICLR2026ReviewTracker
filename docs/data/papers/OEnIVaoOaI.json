{"id": "OEnIVaoOaI", "number": 14443, "cdate": 1758235500650, "mdate": 1759897369729, "content": {"title": "Fusing Rewards and Preferences in Reinforcement Learning", "abstract": "We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that fuses both individual rewards and pairwise preferences (if available) into a single update rule. DFA uses the policy's log-probabilities directly to model the preference probability, avoiding a separate reward-modeling step. Preferences can be provided by human-annotators (at state-level or trajectory-level) or be synthesized online from Q-values stored in an off-policy replay buffer. Under a Bradley–Terry model, we prove that minimizing DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC) policy. Our simulation results show that DFA trained on generated preferences matches or exceeds SAC on six control environments and demonstrates a more stable training process. With only a semi-synthetic preference dataset under Bradley-Terry model, our algorithm outperforms reward-modeling reinforcement learning from human feedback (RLHF)  baselines in a stochastic GridWorld and approaches the performance of an oracle with true rewards.", "tldr": "", "keywords": ["Reinforcement Learning", "Reinforcement Learning from Human Feedback (RLHF)", "Preference Learning", "Human Feedback", "Bradley–Terry Model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3a5fc2aec30c7379e2db89991d0f67e916b2074.pdf", "supplementary_material": "/attachment/b5783be2e648d7028e25c8b955cdac9a718bddd8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Dual-Feedback Actor (DFA) algorithm for RL, which fuses both scalar reward signals and pairwise preferences into a unified training objective. DFA uses the agent’s policy log-probabilities to model preferences, bypassing explicit reward modeling. The authors prove that under the Bradley-Terry model, minimizing the preference loss recovers the Soft Actor-Critic (SAC) policy. Empirically, DFA matches or exceeds SAC in control tasks and outperforms preference-based baselines like RM+PPO and ZPG in stochastic settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow, providing enough background with appropriate notations.\n\n2. The proposed algorithm is well-motivated and theoretically grounded. The method is simple yet interesting."}, "weaknesses": {"value": "My main concerns lie with the experimental evaluation, which I believe is currently insufficient to fully support the paper’s claims. Specifically:\n\n1. The paper emphasizes the ability of DFA to handle both numerical rewards and preference feedback, but it is unclear whether the method remains effective when both signals are present simultaneously. It would be valuable to include an experiment that directly evaluates this mixed-signal scenario.\n\n2. In Section 6.1, comparing DFA only with SAC is insufficient and \"SAC the strongest baseline on many environments\" seems not correct. The authors should include additional baselines such as PPO, TD3 [1], and Rainbow [2] to establish a stronger empirical foundation.\n\n3. In Section 6.2, more preference-based baselines should be considered, especially DPO and IPL [3], which have become standard in RLHF research. Furthermore, experiments should extend beyond the synthetic GridWorld environment. Consider using more challenging and realistic continuous domains that incorporate stochasticity, such as risk-sensitive D4RL[4].\n\n4. Finally, the current experimental setup assumes stochasticity only in the environment’s transitions. A more realistic evaluation should consider noisy or inconsistent preferences, or noisy rewads like [4], which are common in real-world RLHF scenarios. Can DFA handle such noisy human feedback?\n\nOverall, the proposed method is theoretically promising and well-positioned within the literature. However, to fully demonstrate its practical value, the experimental section needs to be significantly expanded. I would be willing to consider increasing my score if the authors address these concerns in the rebuttal.\n\n[1] Hessel M, Modayil J, Van Hasselt H, et al. Rainbow: Combining improvements in deep reinforcement learning[C]//Proceedings of the AAAI conference on artificial intelligence. 2018, 32(1).\n\n[2] Fujimoto S, Hoof H, Meger D. Addressing function approximation error in actor-critic methods[C]//International conference on machine learning. PMLR, 2018: 1587-1596.\n\n[3] Hejna J, Sadigh D. Inverse preference learning: Preference-based rl without a reward function[J]. Advances in Neural Information Processing Systems, 2023, 36: 18806-18827.\n\n[4] Urpí N A, Curi S, Krause A. Risk-averse offline reinforcement learning[J]. arXiv preprint arXiv:2102.05371, 2021."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "44vQFFy4lX", "forum": "OEnIVaoOaI", "replyto": "OEnIVaoOaI", "signatures": ["ICLR.cc/2026/Conference/Submission14443/Reviewer_2Mgv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14443/Reviewer_2Mgv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625111847, "cdate": 1761625111847, "tmdate": 1762924848531, "mdate": 1762924848531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an RL method that combines two types of feedback: scalar rewards and human preferences to enhance prefromance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper claims dual compatibility with both reward signals and preference feedback, and shows that the method can be used in both on-policy and off-policy settings."}, "weaknesses": {"value": "The motivation for using dual feedback is not clearly explained. In addition, the experiments are not solid. The paper lacks strong baselines, it should compare against standard PbRL (preference-based RL) methods as well as common rl algorithms to properly demonstrate effectiveness. More diverse environments are also needed to support the claims."}, "questions": {"value": "I hope authors can clarify the motivation for using dual feedback. If scalar rewards are already available, why are human preferences still needed? If the goal is to show that DFA improves performance on difficult tasks, then the paper should include experiments on challenging environments. Currently, no such tasks are evaluated, which makes the claim insufficiently supported.\n\nThe paper spends substantial space discussing RLHF and LLM algorithm. Since this work focuses on PbRL for control tasks, I believe it would be more appropriate to review PbRL literature in robotics and control, rather than LLM-centric work.\n\nFor synthesizing preferences, are Q-values computed using the ground-truth reward, or with the reward learned from preferences? This detail is important for understanding the learning pipeline.\n\nHow many preference labels are used for each task?\n\nI also suggest including standard environments commonly used in PbRL research, such as MetaWorld and dm_control. In addition, more baselines should be included, such as classical PbRL methods like PEBBLE [1] and MRN[2], to provide a stronger and fairer comparison.\n\n[1] PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training. 2021.\\\n[2] Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning. 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k8wdRlaGTU", "forum": "OEnIVaoOaI", "replyto": "OEnIVaoOaI", "signatures": ["ICLR.cc/2026/Conference/Submission14443/Reviewer_Gbv4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14443/Reviewer_Gbv4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897631037, "cdate": 1761897631037, "tmdate": 1762924848043, "mdate": 1762924848043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Dual-Feedback Actor (DFA), a reinforcement learning algorithm that unifies scalar rewards and pairwise preferences into a single policy-update objective. The method directly models preference probabilities using policy log-probabilities without learning a separate reward model. The authors prove that, under a Bradley–Terry preference assumption on the soft-optimal Q-function, minimizing the DFA preference loss recovers the entropy-regularized Soft Actor-Critic (SAC) policy. Experiments demonstrate that DFA matches or exceeds SAC performance with more stable training on six MuJoCo tasks and outperforms reward-modeling baselines in a stochastic GridWorld with synthetic human feedback."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n- The method fuses scalar rewards and pairwise preferences into one loss and update rule without requiring a separate reward model. This problem is important for the related field.\n- Theorem 5.2 establishes that minimizing DFA’s state-wise preference loss recovers the SAC policy under Bradley–Terry preferences on the optimal Q-function.\n- Support for off-policy training with replay buffers."}, "weaknesses": {"value": "Weakness:\n- There is no real human feedback experiments. I strongly suggest that the authors to conduct studies with real human feedback with substantial subjects. If the method aims to use human feedback to improve the system, but no experiment is conducted on real human feedback, and an insufficient number of individuals are used to demonstrate generalizability. It's difficult to convince the audience that the method is an effective approach for leveraging human feedback without human or with a limited number of subjects.\n- Similarly, the theoretical guarantee relies on preferences exactly matching soft-optimal Q-value comparisons. This again limits the applicability to real noisy feedback.\n- Preference synthesis uses nearest-neighbor Q-comparison, which may introduce bias and lacks ablations or robustness analysis to study this part.\n- The experiments are not conducted in other environments, which leads to insufficient baseline comparisons with existing methods.\n\t- Some other environments to consider: \n\t\t- MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning (with interface to collect data.)\n\t\t- CREW: Facilitating Human-AI Teaming Research. GUIDE: Real-Time Human-Shaped Agents. (with human data provided and human interface).\n- Nearest-state lookup in the replay buffer may scale poorly. The authors acknowledge the cost but do not quantify it.\n- MuJoCo comparison only includes SAC; additional recent off-policy RL baselines (e.g., TD3, SPOT, or latent preference RL methods) would strengthen the claims."}, "questions": {"value": "Please address the weakness points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oPt2O5Kwvj", "forum": "OEnIVaoOaI", "replyto": "OEnIVaoOaI", "signatures": ["ICLR.cc/2026/Conference/Submission14443/Reviewer_JdPq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14443/Reviewer_JdPq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942525132, "cdate": 1761942525132, "tmdate": 1762924847595, "mdate": 1762924847595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the DFA, an RL algorithm designed to learn from both scalar rewards and pairwise preferences. DFA's core mechanism avoids a separate reward-modeling step, which is common in many RLHF methods. Instead, it models the preference probability directly using the policy's log-probabilities.\n\nWhen numerical rewards are available, DFA uses them to learn Q-values and then synthesizes preference pairs online by comparing Q-values of actions in the replay buffer. The paper provides a theoretical justification (Theorem 5.2) proving that, under a Bradley-Terry preference model based on the optimal Q-function ($Q^*$), minimizing DFA's preference loss recovers the optimal entropy-regularized SAC policy.\n\nEmpirically, the paper presents two main results (1) DFA (using its synthesized preferences) is shown to match or exceed SAC on six MuJoCo control tasks, exhibiting more stable training (2) DFA (using simulated human preferences) is shown to outperform RLHF baselines like RM+PPO and ZPG on a stochastic GridWorld."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proves that minimizing the DFA preference loss (under a BT assumption on $Q^*$) is equivalent to finding the optimal policy for the entropy-regularized SAC objective\n- Section 6.2 provides a convincing experiment demonstrating DFA's capabilities in a stochastic MDP where only preference feedback is available"}, "weaknesses": {"value": "- The claims of \"matching or exceeding SAC\" rest on an algorithm that is confounded by a heuristic: a nearest-neighbor state search in the replay buffer to find a comparison. This heuristic is computationally expensive (a $k$-NN search on the buffer per gradient step). What would the training curves look like when plotting against the wall clock?\n- The practical algorithm (Section 4.2) relies on synthesizing preferences from the current, noisy Q-estimate, $Q_k$. The theory, however, relies on the optimal, noise-free $Q^*$. The paper fails to analyze the impact of this noise. A noisy $Q_k$ will lead to noisy, incorrect preference labels, which could influence the stability during training.\n- The paper could benefit from a wider range of experiments\n- The state-wise preference (or action preference) setting is not natural in some settings (e.g., in robot learning, it is hard for a human to state whether a specific torque is better than another). The paper could benefit from a more thorough discussion of the per-trajectory preference as well as experimental results for this setting"}, "questions": {"value": "- The \"preference synthesis\" in 4.2 uses a nearest-neighbor state $s_i'$ to find the second action $a_i'$. What is the justification for this? Have the authors experimented with simpler, cheaper alternatives (e.g., sampling a random action $a_i'$ from the buffer, or sampling $a_i' \\sim \\pi_{\\theta}( \\cdot | s_i)$)?\n- How does the noise from a non-converged Q-function affect the synthesized preference labels?\n- Regarding the hyperparameters for Experiment 6.1, was $\\alpha$ tuned individually for each environment, or was a single value used? Table 2 in the appendix seems to suggest it was tuned per-environment (e.g., 0.3 for Swimmer, 0.4 for MountainCarC). Can the authors confirm this? Given the sensitivity to $\\alpha$ as shown in Figure 3 (b), it would seem that the convergence analysis should take the hyperparameter tuning into consideration when comparing with RM_2, for example. Similarly the the first point in \"Weaknesses\", it would be interesting to see a comparison with wall-clock time on the x-axis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xEToUDHxjX", "forum": "OEnIVaoOaI", "replyto": "OEnIVaoOaI", "signatures": ["ICLR.cc/2026/Conference/Submission14443/Reviewer_9sYc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14443/Reviewer_9sYc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977989680, "cdate": 1761977989680, "tmdate": 1762924846406, "mdate": 1762924846406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}