{"id": "X6mpiUEWPe", "number": 11892, "cdate": 1758204506987, "mdate": 1759897548263, "content": {"title": "TabX: X-cellent at Complex Tables and Beyond", "abstract": "Recent advances in table understanding have shifted from text-based large language model (LLM) methods to multimodal LLM (MLLM) methods like Table-LLaVA that directly process table images. Despite these advances, existing table MLLMs still exhibit limited robustness to complex table layouts and poor generalization to unseen tasks. We trace these failings to two fundamental issues in their development pipeline: (1) a low-quality dataset composed of instruction-table-answer triplets and (2) a lack of all-around understanding of table images. This predicament is analogous to a student learning from flawed material with no mechanism for self-correction. Typically, true understanding is not attained through passive study alone, but rather through iterative self-evaluation and the correction of errors under teacher guidance. Inspired by this cognitive process, we first curate a new dataset, MMTab-Pro, by introducing three challenging tuning tasks that encourage the model to perform a deeper understanding of table content and structure, while applying a reflection-based enhancement to refine low-quality triplets. We further propose a Self-Evolution with Teacher-Tuning (SETT) framework to fine-tune the model, which enables the model to evolve through self-feedback and the guidance of a stronger teacher model, continuously refining both data suitability and model comprehension. Finally, through the two-step pipeline developed above, we present TabX, a robust and generalizable table MLLM. Experiments on the MMTab-eval benchmark show that TabX outperforms existing models, particularly on structurally complex and unseen tasks.", "tldr": "", "keywords": ["Table Understanding", "Table MLLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6708d3082310c37b9fb5e96e724d57595c40b77d.pdf", "supplementary_material": "/attachment/7784dceabd890addbaf0e429940b119200a17821.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a solution encompassing high-quality dataset construction and a model fine-tuning framework. It addresses the limitations of existing multimodal large language models (MLLMs) for table understanding, specifically their insufficient robustness to complex table layouts and poor generalization to unseen tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- This paper directly targets the core pain points of existing table MLLMs—low-quality training data and inadequate table image understanding. \n- The two-stage reflection enhancement strategy of the added tasks achieves quantitative optimization of data quality, integrating model feedback into the data optimization process."}, "weaknesses": {"value": "There are two core concerns which from my respective are very important issues. Solving these concerns may have a great influence of my rating. Concerns are as follows:\n- It seems that there are some problems with experimental results. For instance, in Table 1, the performance on the WTQ dataset stands in striking contrast to findings from prior works. While existing studies typically report an average accuracy exceeding 70%—and in some cases reaching 80%—for this work, the accuracy achieved in the present work is merely 18.6%. Same problems exists in some other datasets. Moreover, the performance of TabX is not very good compared to other existing methods. I wonder how authors will explain this concern.\n- This paper claims that the pain points of table MLLMs are low-quality training data and inadequate table image understanding. \"Inadequate table image understading\" is vague, it's better to claim which aspect (semantic or structual). It seems that the method proposed in this paper only solve the low-quality training data. I wonder how the methods solve the problem of inadequate table image understanding."}, "questions": {"value": "Please refer to section Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1LMbuhssz8", "forum": "X6mpiUEWPe", "replyto": "X6mpiUEWPe", "signatures": ["ICLR.cc/2026/Conference/Submission11892/Reviewer_hFAt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11892/Reviewer_hFAt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649035718, "cdate": 1761649035718, "tmdate": 1762922908246, "mdate": 1762922908246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a multi-modal LLM method similar to TableLlava model on general table understanding. The authors identify two issues in the existing TableLlava model, including the quality in the training dataset, and lack of understanding for complicated table structures. The authors first complement the original MMTab dataset by introducing new tasks (header value matching, data imputation, and NL2SQL). After fine-tuning, the authors employ self evolution with teacher tuning framework to fine-tune the model, where the authors first leverage the student (the trained) model to produce an output and decide if it is a good or bad output. When the example is identified as a bad case, the framework introduces the teacher model to correct the output. The model is tuned in such a refined fashion."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies some issues in an existing multimodal table training dataset (MMTab).\n- The paper proposes to augment the existing dataset by proposing new tasks (in multi-modal table understanding setup).\n- The paper introduces a training framework where refinement is involved."}, "weaknesses": {"value": "### Content\n\n- The fundamental issue with this paper lies in the novelty. Instead of some creative idea in table modeling, the paper seems to be rather an ensemble of data engineering, training engineering. For instance, the three tasks introduced to augment the MMTab dataset, HeaderValueMatching, DataImputation, and NL2SQL, exist in the Table-GPT [1] paper from Microsoft. Though applied in multi-modal setup, I encourage the authors to consider a wider range of tasks discussed in [1]. The authors mentioned that the base model selection is important, which is not a new finding in table modeling aspect [2]. In terms of the feedback loop, [3] provides similar method on progressive data synthesis. In terms of the complicated multi-model table understanding, [4] crafted a nice benchmark which is not discussed or mentioned in this paper. Also, [5] is well worth mentioned in multi-modal table understanding (comparing vision versus text). There are also advances in table modeling both in terms of benchmarking and table LLMs that are worth mentioning.\n\n- The performance improvement seems limited. Of course, kudos to the authors for including such a comprehensive evaluation on a wide range of benchmarks. However, as we understand, most of the applications focus on TQA, TFV, T2T (academic tasks in Table 1), but it seems that the improvement on WTQ (18.6 v.s. Table-LLaVA's 18.4) is limited. And for cases in Table Fact Verfication (TFV), it is Table-LLaVA that takes the lead in most cases). I wonder the effort spent in data and training justify the improvement that we see in Table 1.\n\n- No ablation studies. In terms of all of the methods employed, the authors fail to provide a clear picture into what methods contribute to the improvement. For instance, can the introduction of DataImputation be the source of hallucination rather than the factor for performance improvement? Since in Table 1, no text-to-SQL tasks are involved, how does the introduction of text-to-SQL serve as the source of improvement? Failing to disentangle the core factors for performance improvement makes this paper less informative.\n\n\n----\n### Writing\n\n- Please fix the citations (e.g. Zheng et al. Zheng et al. (2024) at line 050).\n- In Table 1, HiT, Table-LLaMA + OCR achieves the best overall score, but the Table-LLaVA's performance is the one that is highlighted.\n- Please improve the presentation of the paper in general.\n\n\n----\n### References\n[1] Li, Peng, et al. \"Table-gpt: Table fine-tuned gpt for diverse table tasks.\" Proceedings of the ACM on Management of Data 2.3 (2024): 1-28.\n\n[2] Deng, Naihao, et al. \"Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models.\" arXiv preprint arXiv:2501.14717 (2025).\n\n[3] Zheng, Mingyu, et al. \"TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning.\" arXiv preprint arXiv:2506.08646 (2025).\n\n[4] Titiya, Prasham Yatinkumar, et al. \"MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning.\" arXiv preprint arXiv:2505.21771 (2025).\n\n[5] Deng, Naihao, et al. \"Tables as texts or images: Evaluating the table reasoning ability of llms and mllms.\" arXiv preprint arXiv:2402.12424 (2024)."}, "questions": {"value": "Other than the questions raised in the weakness parts,\n\n- Why using BLEU scores for the text-to-text generation tasks?\n- Does DataImputation lead to hallucinations for the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I4HCJOfgEJ", "forum": "X6mpiUEWPe", "replyto": "X6mpiUEWPe", "signatures": ["ICLR.cc/2026/Conference/Submission11892/Reviewer_jdan"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11892/Reviewer_jdan"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679521354, "cdate": 1761679521354, "tmdate": 1762922907611, "mdate": 1762922907611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TabX, a MLLM for table understanding. The authors identify two major bottlenecks in existing table MLLMs:\n\t(1) low-quality instruction–table–answer triplets, and\n\t(2) insufficient understanding of table images.\nTo address these challenges, they propose a two-part framework:\nSelf-Evolution with Teacher-Tuning (SETT) -- a fine-tuning process where the student model performs self-evaluation to identify bad cases, and a teacher model revises these cases to improve data quality and model robustness.\nExpanded Instruction-Tuning Dataset (MMTab-Pro) -- constructed by introducing three additional challenging table understanding tasks: HeaderValueMatching, DataImputation, and NL2SQL.\nExtensive experiments on the MMTab-eval benchmark demonstrate that TabX consistently outperforms prior table understanding models, showing strong robustness and generalization, particularly on complex and unseen tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) New methodological integration: Combines teacher–student reflection with iterative self-evolution, which is rarely explored in multimodal contexts.\n\n(2) Strong empirical validation: Comprehensive experiments across 17 tasks, held-in/out settings, robustness and generalization tests. Huge performance improvement.\n\n(3) High-quality dataset contribution: MMTab-Pro meaningfully improves existing instruction datasets both in scale and data clarity."}, "weaknesses": {"value": "(1)\tThe comparison set is outdated. Table-LLaVA is from early 2024 and no longer represent the current state of multimodal large language models.\n\nFor a fairer comparison, you may consider referencing or benchmarking against recent multimodal LLMs such as Qwen3-VL, InternVL2.5, or works like (1) HIPPO: Enhancing the Table Understanding Capability of Large Language Models through Hybrid-Modal Preference Optimization(arXiv:2502.17315) (2) Multimodal Tabular Reasoning with Privileged Structured Information (arXiv:2506.04088) \nIn addition, including results from strong closed-source MLLMs such as Gemini 2.5 Pro, GPT-4o, or SEED would provide a more comprehensive view of relative performance.\n\n(2)\tThe SETT framework heavily relies on a strong teacher model. If the teacher is weak or biased, its errors or stylistic preferences may propagate to the student, even after IFD filtering. The observed performance drop in later iterations in Figure7 may reflect this issue.\nThe reflection-based data enhancement may primarily optimize samples to better fit the student model’s IFD metric, rather than genuinely improving their objective quality. As a result, it is unclear whether the enhanced data are truly stronger or merely more compatible with the model’s existing biases, making the underlying mechanism conceptually ambiguous.\n\n(3)\tThe paper introduces three new fine-tuning tasks but provides limited discussion on why these particular tasks were chosen. A clearer explanation of the task selection rationale and how they relate to the overall table understanding objectives would strengthen the work.\n\n(4)\tThe paper mentions total training cost (~$200), but lacks analysis on training time, iteration efficiency, or scalability beyond 7B models."}, "questions": {"value": "(1) The SETT framework updates both the instruction and answer during the self-evaluation phase, but only the answer during the teacher-forced revision phase. Moreover, across iterations, the process alternates between revising instructions and answers.\nCould the authors clarify why this specific update order was chosen?\nHave alternative update strategies (e.g., revising both jointly or reversing the order) been explored?\n\n(2) How sensitive is the SETT framework to the choice of teacher model? Would smaller or weaker teachers other than Qwen-VL-Max still yield measurable improvement?\n\n(3) Given that the experiments were conducted on 4×A800 GPUs, could the authors comment on the computational efficiency of the SETT iterations? While the total reported cost is $200, the iterative self-reflection–retraining pipeline seems potentially expensive in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "m5FvkXuuhP", "forum": "X6mpiUEWPe", "replyto": "X6mpiUEWPe", "signatures": ["ICLR.cc/2026/Conference/Submission11892/Reviewer_HXbJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11892/Reviewer_HXbJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828692819, "cdate": 1761828692819, "tmdate": 1762922907092, "mdate": 1762922907092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TabX, a multimodal table understanding model designed to address the limited generalization ability of existing multimodal large language models in handling complex table structures and unseen tasks. Optimizations have been made across data, task definitions, and training methods.\nThe visual modality of tables represents an important subfield in table-related research, as tabular images can tackle certain challenges that text-based tables struggle with.\nWhile the paper is well-structured, the experimental work is somewhat insufficient, and the analysis lacks depth."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Accurate Problem Identification**  \nThe paper precisely identifies key limitations in previous multimodal table models and proposes targeted solutions to address them.\n\n2. **Knowledge Distillation and Self-Evolution**  \nDespite using a model with relatively few parameters, the proposed approach achieves strong performance by leveraging knowledge distillation from teacher models and a self-evolution algorithm.\n\n3. **Effective Visualizations**  \nThe paper includes a variety of clear and informative figures and tables that enhance the understanding of the methodology and results."}, "weaknesses": {"value": "1. **Limited Table Rendering Methods**  \n    The study relies solely on table rendering based on the Table-GPT dataset, with optimizations tailored exclusively to this single rendering approach.  \n\n2. **Insufficient Model Comparisons**  \n    The experimental section includes comparisons with only a limited number of baseline models.  \n\n3. **Lack of Justification for Teacher Model Selection**  \n    The rationale for selecting Qwen-VL-Max as the teacher model is not clearly explained, and its performance on the relevant benchmarks remains unstated.  \n\n4. **Superficial Experimental Analysis**  \n    The analysis lacks depth in several aspects—for example, it does not thoroughly investigate the performance drop during multi-round iterations, and the evaluation of TabX's performance on RW is weakly supported. Incorporating human evaluation or other detailed analytical methods could help better interpret the results."}, "questions": {"value": "1. **Prompt Format Specification for TableLLaMA**  \n    The performance of TableLLaMA is known to be highly sensitive to prompt formatting. Could the authors clarify whether the official recommended prompts were used in their experiments? If non-standard prompts were adopted, it would be important to explain how this may have influenced the results.\n\n2. **Scope of Baselines and OCR Techniques**  \n    As the experiments were likely conducted before some recent advances, the study does not include several newer models—particularly other multimodal methods for table understanding. Moreover, the OCR approach used is not described in detail. Many contemporary OCR systems extend beyond plain text recognition and support structured output such as LaTeX. We suggest the following additional comparisons:\n    - TableLLaMA enhanced with modern OCR,\n    - TableLLaMA with direct text-table input, and\n    - More comprehensive baselines covering both text-based and multimodal table models.\n    These comparisons are critical because, in the absence of (1) and (2), it remains unclear whether the reported superiority of the proposed method holds when TableLLaMA is provided with its optimal input (i.e., plain text tables). It is plausible that TableLLaMA could outperform the TabX under such conditions.\n\n3. **Selection of the Base Model**  \n    Given that Janus-Pro underperforms relative to Table-LLaVA, could the authors explain the rationale for choosing Janus-Pro as the base model?\n\n4. **Table Rendering Strategy**  \n    The rendering method used for tabular images may affect model performance. Have the authors considered whether some baseline MLLMs performed poorly due to suboptimal rendering? Additional experiments analyzing different rendering strategies would strengthen the study."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ah3tIxBiLQ", "forum": "X6mpiUEWPe", "replyto": "X6mpiUEWPe", "signatures": ["ICLR.cc/2026/Conference/Submission11892/Reviewer_xQgs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11892/Reviewer_xQgs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842116896, "cdate": 1761842116896, "tmdate": 1762922906568, "mdate": 1762922906568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}