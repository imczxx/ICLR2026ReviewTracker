{"id": "ZaPmQ0NHs4", "number": 16088, "cdate": 1758259724747, "mdate": 1759897262633, "content": {"title": "Large Language Model Compression with Global Rank and Sparsity Optimization", "abstract": "Low-rank and sparse composite approximation is a natural idea to compress Large Language Models (LLMs). However, such an idea faces two primary challenges that adversely affect the performance of existing methods. The first challenge relates to the interaction and cooperation between low-rank and sparse matrices, while the second involves determining weight allocation across different layers, as redundancy varies considerably among them. To address these challenges, we propose a novel two-stage LLM compression method with the capability of global rank and sparsity optimization. It is noteworthy that the overall optimization space is vast, making comprehensive optimization computationally prohibitive. Therefore, to reduce the optimization space, our first stage utilizes robust principal component analysis to decompose the weight matrices of LLMs into low-rank and sparse components, which span the low dimensional and sparse spaces containing the resultant low-rank and sparse matrices, respectively. In the second stage, we propose a probabilistic global optimization technique to jointly identify the low-rank and sparse structures within the above two spaces. The appealing feature of our approach is its ability to automatically detect the redundancy across different layers and to manage the interaction between the sparse and low-rank components. Extensive experimental results indicate that our method significantly surpasses state-of-the-art techniques for sparsification and composite approximation.", "tldr": "We present a two-stage method for compressing LLMs while maintaining performance. It tackles low-rank/sparse matrix interactions and global weight allocation.", "keywords": ["Low-rank and sparse approximations", "Model Compression", "Probabilistic Pruning", "Global Sparsity-Rank Co-optimization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e6d5693ac6f76e41cf8828116b3364ba9da5b6a0.pdf", "supplementary_material": "/attachment/db8fa0df3e6270b4fe55ea4113349eec453d8de4.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes CAP, a two–stage LLM compression framework that first decomposes each weight matrix with Robust PCA into a low-rank component and a sparse component, and then performs global, budget-aware selection by learning Bernoulli retention probabilities for singular values and entries using policy gradients on a small calibration set. The method targets layer-adaptive allocations of rank and sparsity, avoids manual thresholds, and factorizes the retained low-rank part for efficient inference. Experiments on OPT, LLaMA-1/2/3 and Phi-3 show better zero-shot accuracy and perplexity across different compression ratio compared to unstructured pruning methods, like SparseGPT, and joint pruning methods like JSQ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is intuitive and clear: the weight matrix is ​​decomposed through RPCA, and the two decomposed matrices are jointly optimized in a learnable manner, which solves the error accumulation caused by previous separate optimization.\n2. The proposed CAP only requires optimizing the diagonal elements of low-rank matrices and the nonzero elements of sparse matrices, significantly reducing the combinatorial space of the problem. Introducing RPCA to calculate the initial solution further simplifies the search difficulty for subsequent optimization. Furthermore, the policy-gradient step uses only 128 calibration sequences with 3–5 forward passes, reducing overall computational requirements and making it more efficient.\n3. The authors conducted comprehensive experiments on various models and tasks (different sparsity rates, combined with quantization, component ablation, etc.), which verified the robustness and versatility of the model in different scenarios."}, "weaknesses": {"value": "1. All calibrations use 128 C4 sequences regardless of task. It’s unclear how sensitive CAP is to domain shift or to the choice/size of calibration data, especially for models evaluated on different downstream tasks.\n2. While RPCA is solved via a convex surrogate, “globally optimal separation” holds under assumptions (e.g., incoherence, sparsity patterns). The paper alludes to this but reads as if CAP inherits guarantees end-to-end. Stage 2 remains a stochastic discrete optimization without convergence guarantees beyond standard REINFORCE properties. A clearer statement of conditions and limitations would improve correctness.\n3. When performing \"Thresholding masks and final factorization\", the authors choose the top-K parameters to keep, but the meaning of singular values ​​(corresponding to a column/row) in learned retention probabilities is different from that of entries (single elements) in sparse matrices. It would be helpful if the reason for this choice could be given.\n4. The authors claim that this approach will accelerate the model, but due to the existence of unstructured sparse matrices, the actual acceleration effect on real hardware (such as Nvidia A100, H100) is limited. The proposed CAP can support semi-structured sparse constraints to achieve true hardware acceleration."}, "questions": {"value": "1. Although Table 9 shows the time required for RPCA, the full time required for CAP is not provided. How does the time required for CAP change compared to other pruning methods as the model size scales?\n2. When stage 2 optimizes based on the initial solution obtained in stage 1, how does the optimization ensure that the overall distribution of learned retention probabilities is optimized towards a bimodal shape? How are sparse constraints added?\n3. Could the authors provide the evolution of the learned retention probabilities during the optimization process, and the difference compared to not using global optimization?\n4. The experiments in the paper lack the latest models and tasks. It would be better if the comparisons could be made on the latest models (such as Qwen3, DeepSeek-R1, etc.) and more tasks (such as reasoning tasks, long text tasks, etc.)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l5oWKAuBIZ", "forum": "ZaPmQ0NHs4", "replyto": "ZaPmQ0NHs4", "signatures": ["ICLR.cc/2026/Conference/Submission16088/Reviewer_qDUb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16088/Reviewer_qDUb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761275104874, "cdate": 1761275104874, "tmdate": 1762926270730, "mdate": 1762926270730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a two-stage compression framework for large language models (LLMs). In the first stage, robust principal component analysis (RPCA) is employed to decompose each weight matrix into low-rank and sparse components, effectively disentangling global structures from localized anomalies. In the second stage, the authors formulate a global, layer-aware pruning strategy as a probabilistic optimization problem over Bernoulli retention masks, optimized via policy gradients on a small calibration set. This design enhances compression efficiency by adaptively allocating compression ratios across layers and jointly optimizing the interaction between sparse and low-rank representations. Experiments across LLaMA-1/2/3, OPT, Phi-3, and BERT-base report better zero-shot accuracy and lower  WikiText-2 perplexity than SparseGPT/Wanda/DSNoT/OATS and competitive results versus joint methods (e.g., SLiM), with modest throughput overhead and a clear reproducibility statement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Principled search-space reduction: RPCA to form high-quality candidate subspaces before budgeted selection is elegant and well motivated. \n2. Global, budget-aware allocation: Bernoulli masking with policy gradients ties rank and sparsity to a single parameter budget K, avoiding heuristic thresholds and per-layer guessing. \n3. Consistent empirical gains: Tables show competitive or superior performance to strong sparsifiers (SparseGPT/Wanda/OATS) at 30–50% compression; \n4. Reproducibility: Implementation outline (files, steps), environment, calibration data, and hyperparameters are documented."}, "weaknesses": {"value": "1. Missing comparison with LoSparse: The paper explicitly points out that LoSparse suffers from manually selected ranks and lack of global coordination, yet the main experimental tables do not include LoSparse results. \n2. Lack of comparison with recent low-rank methods: To validate the claimed advantage of RPCA-based decomposition, the paper should include results on SVD-LLM v2(https://arxiv.org/abs/2503.12340), Basis Sharing(https://openreview.net/pdf?id=gp32jvUquq), and Dobi-SVD(https://openreview.net/pdf?id=kws76i5XB8), which represent the current state-of-the-art low-rank post-training compression methods."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mdz2OwP7F5", "forum": "ZaPmQ0NHs4", "replyto": "ZaPmQ0NHs4", "signatures": ["ICLR.cc/2026/Conference/Submission16088/Reviewer_6yto"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16088/Reviewer_6yto"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828823016, "cdate": 1761828823016, "tmdate": 1762926270232, "mdate": 1762926270232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CAP, a two-stage compression framework for large language models (LLMs) that jointly optimizes low-rank and sparse structures. In Stage 1, each weight matrix is decomposed using Robust Principal Component Analysis (RPCA) into a low-rank and a sparse component, providing structured subspaces. In Stage 2, Bernoulli sampling with policy-gradient optimization is used to select which singular values and sparse entries to retain under a global parameter budget."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper presents a conceptually unified view of low-rank and sparse compression and emphasizes global allocation of redundancy across layers.\n\n- The Bernoulli-policy optimization introduces a probabilistic pruning mechanism that is simple and can be trained without full fine-tuning."}, "weaknesses": {"value": "- The proposed CAP framework largely combines existing elements, RPCA-based decomposition, SVD thresholding, and REINFORCE optimization, without a fundamentally new algorithmic insight. The combination is technically straightforward and primarily an engineering integration of known methods.\n\n- Reported “no fine-tuning” performance comes at the cost of multiple RPCA and policy-gradient passes on calibration data. The wall-clock savings over simpler pruning or quantization approaches are not measured.\n\n- While RPCA is convex, the subsequent policy gradient step introduces nonconvex stochasticity, and no convergence or optimality guarantees are proven for the combined system. The “global optimization” claim is therefore misleading."}, "questions": {"value": "- How does CAP scale when applied to multi-GPU inference or distributed weight loading?\n\n- Can the method be integrated into prefix-caching or mixed-precision pipelines without recomputing decompositions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oXxIE2Ia2B", "forum": "ZaPmQ0NHs4", "replyto": "ZaPmQ0NHs4", "signatures": ["ICLR.cc/2026/Conference/Submission16088/Reviewer_vaH5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16088/Reviewer_vaH5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976176161, "cdate": 1761976176161, "tmdate": 1762926269745, "mdate": 1762926269745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a two-stage compression framework for LLMs: Stage 1 uses RPCA to decompose each weight matrix into low-rank and sparse components, and Stage 2 performs global, budget-aware selection via Bernoulli masks optimized with policy gradient on a small calibration set. The entire pipeline is similar to previous weight matrix decomposition methods; this work adopts the low-rank and sparse decomposition rather than the commonly used SVD decomposition and the learnable mask. This work uses a different decomposition method and combines it with the learnable mask method to form a joint compression method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing is clear and easy to follow. The appendix is a good preliminary for relevant techniques. \n2. This work provides enough details for reproducibility. \n3. Extensive ablation and analysis, including in the main content and Appendix K, show meaningful insights."}, "weaknesses": {"value": "1. While the paper reports zero-shot accuracy on eight benchmarks and perplexity on WikiText-2, it would be valuable to include tasks that require chain-of-thought or longer generations, such as those with more than 100+ tokens. Prior work has observed that models can retain perplexity and short-form QA accuracy, but degrade more sharply as generation lengths increase."}, "questions": {"value": "1. Why the 100 iterations' PPL is higher than 10 tierations. Does this mean the over-optimizaiton may harm the performance?\n2. Although the current hardware devices don't have a nice support of unstructured pruning, it's better to report the end-to-end runtime and the minimum GPU memory requirement. Even the baseline is not based on LLM inference engine is helpful. \n3. The choice of calibration set. I think sampling from C4 is unbiased, but it's better to have some additional experiments to evaluate the impact of the calibration set selection. Is the proposed method robust to the selection？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JM3AfWVkJl", "forum": "ZaPmQ0NHs4", "replyto": "ZaPmQ0NHs4", "signatures": ["ICLR.cc/2026/Conference/Submission16088/Reviewer_xDUq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16088/Reviewer_xDUq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986425515, "cdate": 1761986425515, "tmdate": 1762926268820, "mdate": 1762926268820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}