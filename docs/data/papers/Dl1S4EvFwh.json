{"id": "Dl1S4EvFwh", "number": 15519, "cdate": 1758252269203, "mdate": 1759897301493, "content": {"title": "PPTArena: A Benchmark for Computer-Use Agents on PowerPoint Tasks", "abstract": "Creating and editing slides is a rich, multimodal activity that is ubiquitous in professional and educational settings, making it an ideal testbed for real-world computer-use agents. Microsoft PowerPoint is among the most widely adopted and feature-rich environments for presentation creation. We introduce PPTArena, a benchmark of 120 diverse PowerPoint tasks covering both content creation and presentation editing, organized by difficulty. A central challenge in this domain is evaluation: tasks are complex, multimodal, and often admit many valid solution strategies. Moreover, today’s agents frequently make only partial progress, which binary success metrics fail to capture. To address this, we design a robust evaluation framework to help create task-specific scoring rubrics for PowerPoint tasks. These rubrics award partial credit for intermediate steps, penalize unnecessary changes and poor aesthetics, and provide natural language feedback. We release PPTArena together with this evaluation framework, along with an analysis of common failure modes and lessons for rubric design.", "tldr": "We introduce PPTArena, a benchmark featuring 120 diverse PowerPoint tasks and robust grading scripts for evaluating computer-use agents, spanning both presentation editing and content creation scenarios.", "keywords": ["computer-use agents", "benchmark", "rubric", "llm-as-a-judge", "slide editing", "powerpoint"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7fc47016bd8244eadcd921a29efdff5e85a325ff.pdf", "supplementary_material": "/attachment/5fd8064f29612e365aec95b79a474a492147d1ad.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces PPTARENA, a novel benchmark designed to evaluate computer-use agents on realistic, GUI-level editing tasks within Microsoft PowerPoint Online. The benchmark consists of 120 diverse tasks across 12 files, covering content creation and editing, and categorized by difficulty. The core contribution is a robust, semi-automatically generated rubric-based evaluation framework that addresses the challenge of complex, open-ended, and multimodal tasks. These rubrics provide partial credit for intermediate steps, penalize poor aesthetics/extraneous edits, and yield natural language feedback. The framework achieves strong correlation with human judgment. Initial benchmarking shows frontier agents (like Claude-4-Sonnet) struggle significantly (38% success rate, 0.58 average score), highlighting substantial room for improvement in GUI-based multimodal agents."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. PPTARENA focuses on the GUI of PowerPoint Online, granting agents access to the application's full feature set (Designer, animations, charts), which API-based benchmarks (like PPTC) fundamentally miss. This ensures the tasks reflect realistic, high-stakes professional workflows.\n\n2. The rubric-based scoring is excellent. By incorporating partial credit, penalties for extraneous edits, and natural language feedback, the evaluation captures nuanced agent performance far better than binary metrics. \n\n3. The failure analysis (Fig. 6b) is very insightful, clearly distinguishing between the trivial failures of smaller models (inactivity/looping) and the more sophisticated failures of frontier models (incorrect styling, partial but destructive changes). This provides clear targets for future agent improvement efforts."}, "weaknesses": {"value": "1. Lack of experiments. More base models and GUI agents should be evaluated for comparison to gain more evidence for the value of this benchmark.\n\n2. While the semi-automatic process is clever, the authors state that rubric development still required $\\sim 150$ hours of human effort and that model-generated drafts still need extensive human revision. This limitation suggests the current rubric creation process is not easily scalable for continuous training or very large benchmarks. A more detailed breakdown of why model drafts fail and what specific patterns human annotators fix would be highly beneficial for future work seeking to minimize human intervention.\n\n3. The performance of the open-source baseline (UI-TARS-1.5-7B) is extremely poor (0% SR, dominated by loops). While this sets a clear lower bound, a comparison to a more capable, recent open-source agent (e.g., a fine-tuned Llama/Mistral model with a modern ReAct or reflection architecture) would provide a more meaningful baseline for the majority of academic research."}, "questions": {"value": "1. Could you do more baseline experiments and analyse models' and agents' performance more detailedly?\n\n2. Why do you upload the PPTX file to OneDrive instead of keeping it as a desktop file? Will it be less realistic when all interactions and operations are done through web browser?\n\n3. The UI-TARS model frequently gets stuck in repetitive loops. Since the maximum step budget is 30, did the rubric automatically penalize this behavior, or was the penalty only applied once the budget was exceeded? Could the authors suggest a simple mechanism that could be added to the agent's action space (e.g., a \"reset\" or \"reflect\" action) that could have broken these loops?\n\n4. Regarding the extensive human effort ($\\sim 150$ hours), what would be the distinction and feature of PPTARENA compared to a small subset of OSWorld tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pIxfDPgApe", "forum": "Dl1S4EvFwh", "replyto": "Dl1S4EvFwh", "signatures": ["ICLR.cc/2026/Conference/Submission15519/Reviewer_MYKw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15519/Reviewer_MYKw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945265127, "cdate": 1761945265127, "tmdate": 1762925804374, "mdate": 1762925804374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PPTArena, a benchmark of 120 PowerPoint editing tasks. It also provides a rubric-based evaluation framework that gives fine-grained scores and  natural language feedback. Experiments show that this dataset is challenging for powerful models and agents such as Claude-4-Sonnet."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed dataset studies an important task (i.e., PPT editing). The sandboxed PowerPoint environment and tree-structured evaluation rubrics can be useful for future research."}, "weaknesses": {"value": "1. The experiments and analyses are very limited: (1) Only three models are evaluated in the proposed benchmark; (2) The dataset is designed to cover PowerPoint with diverse style (as mentioned in Section 3.3), but there is no qualitative analysis on these factors. Moreover, this paper did not discuss how these critical factors (e.g., density of images/charts, slide count, layout variations, topics) would influence performance, which is very important.\n\n2. The tasks are proposed by Claude, which could be artificial and not realistic.\n\n3. Details and results of human evaluation are missing. For example, the paper does not report the inter-annotator agreement, quantitative analysis of disagreement patterns beyond the qualitative examples provided. Moreover, the paper did not report the performance with human evaluators.\n\n4. Claude-4-Sonnet is used to propose task candidates during dataset construction, and then propose rubric tree during evaluation. Also, Claude-4-Sonnet is one of the three evaluated agents. This creates a risk of scorer and data generation bias. \n\n5. What is the reason for λ = 0.3 in rubric deign? How does this hyperparameter influence the score distributions and correlation with human judgements?"}, "questions": {"value": "See the \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vt4QVQlEm7", "forum": "Dl1S4EvFwh", "replyto": "Dl1S4EvFwh", "signatures": ["ICLR.cc/2026/Conference/Submission15519/Reviewer_gqPg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15519/Reviewer_gqPg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138643192, "cdate": 1762138643192, "tmdate": 1762925803958, "mdate": 1762925803958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PPTArena, a benchmark for evaluating computer-use agents on PowerPoint editing tasks. The benchmark comprises 120 diverse tasks across 12 presentation files, organized by increasing difficulty. They designed an evaluation framework to help create task-specific rubrics for PowerPoint tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is well-motivated. Working with a PowerPoint is a hard task that most of the current models are struggling with.\n- The evaluation framework and rubrics for each task are a strong contribution and improve the reliability of the evaluation."}, "weaknesses": {"value": "- The number of used files is minimal (12 files). A larger number of files with a diverse set of styles, topics, and versions would be a better metric of performance.\n\n- Adding some tasks, such as slide generation and critique, can be more representative of real-world tasks."}, "questions": {"value": "- How would you extend the current benchmark without extensive effort for rubric generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GHS4uoozea", "forum": "Dl1S4EvFwh", "replyto": "Dl1S4EvFwh", "signatures": ["ICLR.cc/2026/Conference/Submission15519/Reviewer_aLog"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15519/Reviewer_aLog"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762175379215, "cdate": 1762175379215, "tmdate": 1762925803432, "mdate": 1762925803432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PPTArena, a novel and well-motivated benchmark for evaluating computer-use agents on PowerPoint tasks. The benchmark is timely, addressing a significant gap in GUI-based agent evaluation for real-world productivity software. The authors provide a comprehensive dataset of 120 tasks across 12 diverse presentation files, with a strong emphasis on realistic, multimodal interactions. The introduction of a rubric-based evaluation framework with partial credit and natural language feedback is a notable contribution, and the meta-evaluation shows strong alignment with human judgment (Kendall’s τb = 0.77). The paper is clearly written, well-structured, and supported by thorough experiments and analysis."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty & Motivation: The benchmark fills a critical gap by focusing on GUI-native PowerPoint interaction, a ubiquitous yet under-benchmarked real-world task. The motivation, grounded in the high cost of slide editing, is compelling.\n\n2. Robust Rubric Design: The tree-structured evaluation rubrics, with dedicated scorers for leaf nodes (using programmatic checks, LLMs, and VLMs), provide a nuanced and robust framework for assessing complex, open-ended tasks. The critical/non-critical node distinction and penalties for extraneous edits are particularly well-justified.\n\n3. Human-Aligned Validation: The meta-evaluation convincingly demonstrates the rubric's reliability, showing strong correlation (Kendall’s τb = 0.77) with human judgments. This rigorous validation is a key strength of the proposed evaluation methodology.\n\n4. Clarity: The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The reported metrics (success rate, average score) are useful but do not fully disentangle failures due to perception (e.g., misreading UI), planning, or action execution. More fine-grained metrics could help guide future model development.\n2. The error taxonomy in Section 5.1 is coarse (“no visible edit”, “incorrect styling”, etc.). A more detailed mapping—e.g., labeling each trajectory with the first failure type and its UI context—would give developers clearer improving signals."}, "questions": {"value": "1. A suggestion: The task formulation in PPTArena remains largely **instruction-centric**—i.e., agents are given explicit, low-level directives such as “split Assets and Liabilities in slide 6 using Two Content layout.”, but do not yet capture the **higher-level, intent-driven** scenarios users will realistically pose, e.g., “add a comparative section that highlights the financial split between Assets and Liabilities.” In practice, users expect agents to autonomously decompose abstract goals into sequences of slide-level actions, balancing content planning, layout selection, and visual hierarchy without hand-holding. This gap implies that planning, abstraction, and co-reference resolution across multiple slides are under-tested. This limitation is acceptable for PPTArena’s role as a phased foundation, but essential for next-generation benchmarks.\n2. From the error analysis, what are the potential methods do you think for GUI agents to improve PPT editing performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f7M9HtX7UX", "forum": "Dl1S4EvFwh", "replyto": "Dl1S4EvFwh", "signatures": ["ICLR.cc/2026/Conference/Submission15519/Reviewer_iVTP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15519/Reviewer_iVTP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762248159217, "cdate": 1762248159217, "tmdate": 1762925802839, "mdate": 1762925802839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PPTARENA, a novel benchmark for evaluating the ability of AI computer-use agents to perform tasks in Microsoft PowerPoint. The authors argue that traditional binary success/failure metrics are insufficient for the PowerPoint domain, which is highly creative, multimodal, and has open-ended solutions.\n\nThe paper's core contribution is its innovative evaluation framework: a \"tree-structured rubric\" system. This system evaluates an agent's multimodal output using not only programmatic checks but also calls to VLMs (Vision Language Models) and LLMs (Large Language Models). Crucially, the framework employs a novel aggregation strategy to support \"partial credit,\" allowing for a more granular measurement of an agent's progress. The authors demonstrate that their evaluation framework correlates strongly with human judgment and show that current state-of-the-art agents still have significant room for improvement on this benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's most significant contribution is its novel evaluation framework. Moving beyond binary success/failure for complex, creative tasks like PowerPoint is a major challenge. The proposed \"tree-structured rubric\" (Sec 3.5.1) combined with an aggregation strategy (Sec 3.5.2) that enables \"partial credit\" is a valuable paradigm. It offers a more granular and realistic assessment of an agent's progress compared to the \"all-or-nothing\" gating rules of prior work.\n\n2. The benchmark is timely and relevant, addressing the growing research interest in GUI-based agents. By focusing on the PowerPoint Online GUI, it provides a more general and realistic testbed than benchmarks limited by programmatic APIs. This approach allows agents to be tested on the full, feature-rich surface of the application.\n\n3. A key strength is the multimodal evaluation design. The framework’s use of VLM and LLM calls to assess subjective visual and semantic aspects (e.g., the \"appropriateness\" of an emoji in Fig 9, or the correctness of element positioning) is a crucial step. It aligns the automated evaluation more closely with true human perception and user requirements."}, "weaknesses": {"value": "1. The paper's claims of \"sophisticated evaluation design\" and \"~150 hours of human effort\" (Sec 3.5.3) **appear significantly overstated upon inspection of the rubrics**. The rubrics demonstrate a severe lack of diversity and heavy reliance on templating, strongly suggesting they are primarily LLM-generated with only superficial human editing. This is evidenced by the data: 86.7% (104/120) of rubrics contain a near-identical \"extraneous changes\" checking node. The descriptive language is also formulaic, with phrases like \"Verifies that\" (97% of rubrics) and \"Evaluates whether\" (88%) clearly marking them as LLM-generated. Structurally, the rubrics are extremely shallow (average depth 1.12, max 2), with 49% following a simple \"main check + extraneous check\" pattern. This homogeneity contradicts the claim of \"task-specific\" design and undermines the credibility of the 150-hour effort.\n\n2. A major concern is the reproducibility and **stability of the evaluation**. The framework's core reliance on non-deterministic VLM/LLM calls for scoring is a critical issue for a benchmark. The authors commendably acknowledge this, citing a VLM hallucination in Appendix B.3.1 (misinterpreting a \"farmer emoji as a king emoji\"). However, this non-determinism means that re-running the evaluation on the same agent output could yield different scores. The paper critically fails to quantify this variance, which is essential for interpreting the results and comparing agent performance.\n\n3. The paper claims diversity by sourcing files from various domains (e.g., medicine, CS, history), but this is a **superficial metric**. True operational diversity for a GUI agent benchmark would come from variations in PowerPoint-specific structures (e.g., different master templates, complex layouts, multi-language text, font-specific challenges), not the slide content's topic. The paper does not demonstrate that it covers this more relevant and challenging operational diversity."}, "questions": {"value": "1. **Regarding the non-deterministic evaluation**: Have the authors tried running the evaluation 5-10 times on the same set of agent-modified files? What is the observed score variance? This data is essential to determine if the reported performance differences between agents are statistically significant.\n\n2. **Regarding task diversity**: When creating the 120 final tasks from the 471-task LLM-generated pool, what was the ratio of 'filtering' versus 'significant rewriting or original creation' by human annotators? Can the authors provide statistics on the original pool's quality to address concerns about a low-quality starting point limiting the final set's diversity?\n\n3. **Regarding the environment choice**: Why was PowerPoint Online chosen over the full-featured desktop version? The web version has known limitations (e.g., in complex animations, macros, or advanced font handling). How might these limitations affect the generalizability of the agents' evaluated capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6zIPvMevj4", "forum": "Dl1S4EvFwh", "replyto": "Dl1S4EvFwh", "signatures": ["ICLR.cc/2026/Conference/Submission15519/Reviewer_urpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15519/Reviewer_urpH"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762369152574, "cdate": 1762369152574, "tmdate": 1762925802529, "mdate": 1762925802529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}