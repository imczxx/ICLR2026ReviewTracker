{"id": "Nn3FUBhz2p", "number": 8278, "cdate": 1758077208703, "mdate": 1759897794683, "content": {"title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval", "abstract": "The rapid development of Vision-language models (VLMs) enables open-ended perception and reasoning. Recent works have started to investigate how to adapt general-purpose VLMs into personalized assistants. Even commercial models such as ChatGPT now support model personalization by incorporating user-specific information. However, existing methods either learn a set of concept tokens or train a VLM to utilize user-specific information. However, both pipelines struggle to generate accurate answers as personalized assistants. \nWe introduce **Jarvis**, an innovative framework for a personalized AI assistant through personal KV-Cache retrieval, which stores user-specific information in the KV-Caches of both textual and visual tokens. The textual tokens are created by summarizing user information into metadata, while the visual tokens are produced by extracting distinct image patches from the user's images. When answering a question, Jarvis first retrieves related KV-Caches from personal storage and uses them to ensure accuracy in responses. We also introduce a fine-grained benchmark built with the same distinct image patch mining pipeline, emphasizing accurate question answering based on fine-grained user-specific information. Jarvis is capable of providing more accurate responses, particularly when they depend on specific local details. Jarvis achieves state-of-the-art results in both visual question answering and text-only tasks across multiple datasets, indicating a practical path toward personalized AI assistants. The code and dataset will be released.", "tldr": "", "keywords": ["Vision-Language Model; Personalization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7531b8260e482e8f9e58d9e719a8b83dc32d0d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a framework named Jarvis for efficient VLM personalization using KV-Cache retrieval. It stores user-specific information in KV-Caches of both textual and hard visual tokens, which allows for accurate and efficient responses to personalized queries without the need for extensive fine-tuning. The authors also present a benchmark dataset for evaluating personalized VLMs on fine-grained visual question answering tasks. Experimental results show that Jarvis achieves good performance on both visual and text-only question answering tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of storing essential personalized information in a reusable KV cache is conceptually reasonable and can effectively reduce computational overhead during inference.\n2. The framework is lightweight and training-free, which makes it practical and easy to deploy for large VLMs.\n3. The experimental results show the effectiveness of the proposed framework on personalized VQA tasks."}, "weaknesses": {"value": "1. Several implementation details remain insufficiently explained. For example, it is unclear how textual similarity is computed for retrieval, how the proposed Yo'LLaVA++ and MC-LLaVA++ datasets are constructed, and what the question formats in these datasets look like. The authors should provide more detailed descriptions in the main text or appendices to improve clarity and reproducibility.\n2. The concepts of KV-cache and retrieval-based personalization have already been explored in prior works for both language and vision-language models. The paper should better clarify the technical differences between the proposed method and prior works, and ideally provide a fair empirical comparison to demonstrate its advantage.\n3. Additional baselines (e.g., RePIC, PLVM, or other recent personalization methods) should be included to provide a more comprehensive evaluation.\n4. The experiments are confined to question-answering tasks. It remains unclear how the proposed framework would perform on other multimodal tasks such as image captioning, recognition, or cross-concept interactions. The work also focuses only on single-concept personalization without discussing scalability to multi-concept personalization.\n5. Important hyperparameters such as the number of retrieved patches (top-k) or the system's robustness to retrieval errors are not thoroughly analyzed.\n6. The precomputed KV caches may fail to generalize to complex personalized queries that require reasoning beyond stored evidence. The authors should discuss this limitation and potential directions to improve adaptability."}, "questions": {"value": "1. The evidence retrieval process appears to rely primarily on textual similarity. How does the system handle cases where the target concept is not explicitly mentioned in the query text?\n2. Regarding the RAP-LLaVA baseline, is the model used in the experiments the officially released version, or was it retrained using the LLaVA-OneVision backbone for fair comparison? Clarifying this is important for assessing experimental consistency.\n3. In Figure 4, why is MC-LLaVA not included in the latency and throughput comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MAW6qRd7Ck", "forum": "Nn3FUBhz2p", "replyto": "Nn3FUBhz2p", "signatures": ["ICLR.cc/2026/Conference/Submission8278/Reviewer_tLpQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8278/Reviewer_tLpQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761680392571, "cdate": 1761680392571, "tmdate": 1762920212441, "mdate": 1762920212441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Jarvis, a training-free framework for personalizing Vision-Language Models. The core contribution is a retrieval-centric approach that externalizes user-specific concepts into a pre-computed Key-Value (KV) cache. This cache, composed of textual metadata and discriminative visual patches, is injected at inference time to ground the model's output.\nBy doing so, the method circumvents the need for parameter updates or the reconstruction of lengthy prompts, which the authors claim yields significant improvements in latency and throughput. The empirical evaluation, conducted on several benchmarks including newly proposed fine-grained variants, reports state-of-the-art performance, particularly on tasks requiring detailed, attribute-level recall."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces Jarvis, a novel and practical training-free framework for personalization. The core idea of externalizing user-specific concepts into a reusable KV-Cache is elegant. It directly addresses the high latency and context-length limitations of prompt concatenation methods without the overhead of maintaining per-user model parameters.\n\n-  The paper provides strong evidence of practical benefits, showcasing an order-of-magnitude reduction in latency and a corresponding increase in throughput (QPS). This efficiency makes the system viable for real-time, scalable deployment, a critical advantage for user-facing AI assistants."}, "weaknesses": {"value": "- The experiments primarily focus on single-concept personalization within a given session. A key challenge for a real-world AI assistant is handling queries that involve interactions between multiple personalized concepts (e.g., \"my dog,\" \"my daughter,\" \"my car\"). It is unclear how the proposed KV-Cache retrieval and concatenation would scale in complexity and performance when a query ambiguously references several distinct entities.\n\n- The paper presents evidence construction as a one-time, offline process. However, user-specific concepts are often dynamic; users provide new images or information over time. The paper does not discuss a mechanism for efficiently updating or augmenting the KV-Cache for an existing concept without re-running the entire heavy offline pipeline. This is a practical limitation for long-term personalization."}, "questions": {"value": "Plz answer my questions in the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper proposes Jarvis, a training-free framework for personalizing Vision-Language Models. The core contribution is a retrieval-centric approach that externalizes user-specific concepts into a pre-computed Key-Value (KV) cache. This cache, composed of textual metadata and discriminative visual patches, is injected at inference time to ground the model's output.\nBy doing so, the method circumvents the need for parameter updates or the reconstruction of lengthy prompts, which the authors claim yields significant improvements in latency and throughput. The empirical evaluation, conducted on several benchmarks including newly proposed fine-grained variants, reports state-of-the-art performance, particularly on tasks requiring detailed, attribute-level recall."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MFcBGjFwU2", "forum": "Nn3FUBhz2p", "replyto": "Nn3FUBhz2p", "signatures": ["ICLR.cc/2026/Conference/Submission8278/Reviewer_JB1P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8278/Reviewer_JB1P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793704249, "cdate": 1761793704249, "tmdate": 1762920212129, "mdate": 1762920212129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "this paper propose Jarvis, which is a personalized vlm model. it stores user-specific information as external key-value (KV) caches, which are precomputed and reused across turns without modifying the base model parameters. ​ This reduces latency and token usage while maintaining accuracy and responsiveness. ​\n\nthe main idea is that: user-specific information is summarized into compact text metadata and visual patches. ​ and the text metadata includes a canonical name, category, caption, and fingerprint attributes, while visual patches are mined from user images using techniques like GroundingDINO and SAM to extract discriminative regions.\n\nAt inference, Jarvis retrieves relevant evidence from the KV cache based on the user query and attaches it as external KV to the model. ​ This ensures responses are grounded in user-specific details without relying on lengthy prompts. ​\n\nThe authors introduce a benchmark that emphasizes accurate question answering based on fine-grained user-specific details, such as localized visual cues and abstract features"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I like the idea how they reduced latency: by precomputing and reusing external KV caches, Jarvis significantly reduces latency and improves throughput, making it suitable for real-time applications\n\nThe framework achieves state-of-the-art results in both text-only and visual question answering tasks, particularly excelling in fine-grained, user-specific scenarios. ​"}, "weaknesses": {"value": "Personalized VLMs have been extensively studied in recent years; therefore, the overall scope of this paper feels somewhat limited. I encourage the authors to propose something more novel or distinctive.\n\nthe approach and design is a bit complicated, which not sure about scalability."}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IC4ujdwMQT", "forum": "Nn3FUBhz2p", "replyto": "Nn3FUBhz2p", "signatures": ["ICLR.cc/2026/Conference/Submission8278/Reviewer_67Ra"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8278/Reviewer_67Ra"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858483762, "cdate": 1761858483762, "tmdate": 1762920211724, "mdate": 1762920211724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Jarvis, a method for addressing the problem of personalized LVLM. Jarvis is a retrieval and caching method for LVLM personalization, which retrieves the relevant information from the concepts corresponding to the user query and uses a caching method to insert it into the Query and Key cache of the LVLM model. Compared with training methods like YoLLaVA and MC-LLaVA, Jarvis can enable the personalization of LVLM without test-time fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed framework is lightweight for LVLM personalization. It is a training-free method that addresses the problem of LVLM personalization without spending a brutal concept embedding training in YoLLaVA and MC-LLaVA.\n2. The experiments indicate the superior performance of Jarvis compared to the personalization baselines."}, "weaknesses": {"value": "There are several weaknesses that should be addressed in this paper:\n1. Clarity: This paper does not have a clear presentation, especially in the Method presetation. In Algorithm 1, there are several terms that confuse the reader, including: box $B_m(u,v)$ (Line 287), OpenCLIP Relevance $R_m^{+}$ (Line 282), why we have the background map be the set $\\{R_{m,b}^{-}\\}_{b\\in\\mathcal{B}}$ (Line 283).\n2. The motivation behind the algorithm is not clearly explained: For example, Algorithm 1 contains a term called the difficulty map $C_m$, but it is not clearly explained in the main paper why taking the Diffusion Inversion can result in a difficulty map. Other terms such as background negative $\\mathcal{B}$ are not clearly explained in Algorithm 1. Also, why is the motivation behind the update in Lines 6, 10, and 11 in Algorithm 1?\n3. Restricted only to single concept personalization: Jarvis only addresses the problem of Single concept personalization. However, it is running on LLaVA-OV, which can accept multiple input images to the input, thus having the capability of multiple-concept personalization in the architecture of LLaVA-OV. In addition, conducting only single-concept personalization is not convincing, since most of the dataset for single-concept personalization contains only a single object of that concept, thus leading to the scenario that the LVLM can respond to the query input, while not being aware of what the user is referring to.\n4. In scenario 2 of Figure 1, the paper claims that Jarvis can clearly highlight the difference. However, from the answer, we do not know what the usual clothes <viruss> are. The difference between RAP-LLaVA and Jarvis only comes from \"Different from usual casual\nattire\"; however, it does not indicate that RAP-LLaVA cannot follow the instruction. Maybe with a different seed, the response of RAP-LLaVA includes the term \"Different from usual casual attire\"."}, "questions": {"value": "Based on the weaknesses, there are several follow-up questions to address my concerns:\n1. What is the definition of some terms: background negative $\\mathcal{B}$, background map $R_{m, b}^{-}$ in Algorithm 1? In this case, the paper should give a clearer definition of these terms, because it is hard to follow and understand the patch mining presented in Algorithm 1.\n2. Why does the difficulty map relate to the Stable Diffusion Inversion? What is the motivation in the equation at Lines 6, 10 and 11 in Algorithm 1?\n3. In Appendix D, what is the linearization of $T^{(c)}$ into short prefix $\\tau^{(C)}$ (Line 657), and how do we retrieve the small concept $S(q,I)$ in Line 668?\n4. Do we add patch $P^{(c)}$ for each concept to the LLaVA-OV in test time, and how do we add that? I cannot find that in the paper. Why do we need to select the top-k element in Algorithm 1? Do we conduct an ablation study for k?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iUapHexe3B", "forum": "Nn3FUBhz2p", "replyto": "Nn3FUBhz2p", "signatures": ["ICLR.cc/2026/Conference/Submission8278/Reviewer_DAdL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8278/Reviewer_DAdL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860945384, "cdate": 1761860945384, "tmdate": 1762920211362, "mdate": 1762920211362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}