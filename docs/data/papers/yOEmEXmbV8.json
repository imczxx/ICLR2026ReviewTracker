{"id": "yOEmEXmbV8", "number": 3459, "cdate": 1757435056464, "mdate": 1759898088997, "content": {"title": "Seeing Through Words: Controlling  Visual Retrieval Quality with Language", "abstract": "Text-to-image retrieval is a fundamental task in vision--language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, making them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queries with contextual details while incorporating explicit notions of image quality. Our key idea is to leverage a generative large language model as a query completion function, extending underspecified queries into descriptive forms that capture fine-grained visual attributes such as pose, scene, and aesthetics. We introduce a training framework that conditions query completion on discretized quality levels, derived from relevance and aesthetic scoring models, so that query enrichment is not only semantically meaningful but also quality-aware. The resulting system provides three key advantages: {1} flexibility, as it is compatible with any pretrained vision--language model without modification; {2} transparency, since enriched queries are explicitly interpretable by users; and {3} controllability, enabling retrieval results to be steered toward user-preferred quality levels. Extensive experiments demonstrate that our proposed approach significantly improves retrieval results and provides effective quality control, bridging the gap between the expressive capacity of modern vision--language models and the underspecified nature of short user queries.", "tldr": "", "keywords": ["Large Language Models", "Vision-Language Models", "Query Completion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b31d4d718dce207e0be440d1330b4e0b75295470.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the problem of Quality-Controllable Retrieval, which is an extension of the text to image retrieval problem where in the input query can have additional constraints such as high relevancy, low aesthetics, etc and the goal is to retrieve images from a gallery which satisfy the original query and the additional constraints as well. To enable existing VLMs such as CLIP to do this, they use a query augmentation pipeline where they train an LLM to generate modified queries based on certain additional constraints (relevancy, aesthetics in their case). The trained LLM is then used during inference time to change the query based on the additional constraints. Experiments on MS-COCO and a new Flickr2.4M dataset show the method can effectively steer retrieval results. Conditioning on \"High\" quality levels yields images with significantly better average aesthetic and relevance scores compared to \"Low\" conditions or baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a practical and significant problem. Users frequently use short, ambiguous queries and most existing T2IR systems just return the top-k images by cosine similarity rather than controlling for additional quality metrics. The formulation of \"quality-controllable retrieval\" is a strong and useful contribution. \n2. Simple and elegant solution: their approach allows to convert any existing VLM into a quality controllable one by using a fine-tuned LLM which can augment the input text query to constrain the search. \n3. Good visualizations / interpretable results: The paper contains several qualitative examples which show that by fine-tuning the LLM to augment queries, they are able to get precise augmented queries where the retrievals which match the users preferences."}, "weaknesses": {"value": "1. The same CLIP model and EV_A model are used to generate the training data quality labels and for measuring the average metrics during evaluation as well, this might lead to the fine-tuned LLM learning patterns which work only for a particular gallery/dataset and might overfit the query augmentations to that dataset. It would be great if the authors can either do a small human study or use other stronger SoTA multi-modal models as relative judges of the metrics. \n2. Missing per-condition re-rank baselines in table 6: the paper only compares with a retrieve -> sort by aesthetics post filtering pipeline in Table 6. This effectively is only covering the H/H sub-segment. Since the main idea behind the paper is QC^2, the table should also include the (L/M/H X L/M/H) segments like in Table 3/4 and also include stronger retrieval baselines like the following:\na) a weighted joint scorer which ranks the images based on a weighted average between rel and aes distance with the queries gt_rel and gt_aes\nb) constrained retrieval: since the rel and aes values are already binned, they can first be used to filter out all images which do not belong in the gt bin and then performing search over the reduced sub-space. \nThese results should help shed more light into how simple retrieval methods perform against more complex query augmentation methods \n3. Dataset dependency: As already mentioned by the authors, for each new dataset, they need to fine-tune the query augmentation LLM again so that the LLM can learn the specifics of rel and aes scores and the corresponding captions. This questions the scalability of the method if it needs to be deployed in a real scenario where web-scale retrieval is performed which has a lot of noise/distribution shift. \n\n4.Mode-collapse/diversity risk: QC^2 relies on an LLM to generate quality-conditioned completions, yet the paper reports no diversity or entropy analysis of those texts. Without such checks, the LLM may collapse to a few stylistic templates which led to low loss with a fixed rel and aes value, which can lead to poor diversity in the images which the users would see. It would be great if the authors can provide some metrics on the diversity of queries generated per (rel,aes) tuple. One such metric can be follows: \nfor a fixed instruction (query: q, rel: r, aes: a), sample n different completions from the llm with different temperature parameters, and then check how diverse these completions are (dispersion based on text-clip distance can be a good metric)."}, "questions": {"value": "refer to weaknesses section for questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WbBWbZtuG2", "forum": "yOEmEXmbV8", "replyto": "yOEmEXmbV8", "signatures": ["ICLR.cc/2026/Conference/Submission3459/Reviewer_PhGu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3459/Reviewer_PhGu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459904535, "cdate": 1761459904535, "tmdate": 1762916735182, "mdate": 1762916735182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Quality-Controllable Retrieval (QCR), which allows users to control retrieval results through quality dimensions such as relevance and aesthetics. The proposed approach uses an LLM as a query completion function conditioned on discretized quality levels. Experiments on multiple datasets demonstrate that quality-aware query completions can enhance retrieval performance, although only two quality metrics are explored."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper has a good motivation, addressing the challenge of underspecified queries that often lead to ambiguous retrieval results. Quality-controllable retrieval is both practically useful and of research interest.\n- The paper is well-structured, and the methodology is described in sufficient detail."}, "weaknesses": {"value": "- While the concept of quality-controllable retrieval is attractive, the current work only explores two quality dimensions (relevance and aesthetics). This limited scope is insufficient to demonstrate the true practical value of controllable retrieval in applications.\n- Despite the claim that LLM-based completions avoid irrelevant or hallucinated content, there is no explicit mechanism or evaluation to manage or detect query artifacts that could mislead retrieval or introduce out-of-distribution details.\n- The use of large language models for query rewriting or expansion is already a common and mainstream approach. The main innovation of this paper lies in introducing two quality indicators and training data to guide LLM-based query completions. My primary concern is that this level of novelty does not meet the expected standards of ICLR."}, "questions": {"value": "- The training data consists of concise sentence summarizing the main content of the image with annotated aesthetic and relevance levels. Through training, the LLM learns the relation between text descriptions and the two quality metrics. However, I am concerned **whether such concise sentences are sufficient for the LLM to truly capture the connection between these quality metrics and the actual visual content of the image.**\n\n- Although the numerical results suggest separability, they do not always match intuitive perception. For example, in Table 1 (teddy bear), the left image (aesthetic 4.788, relevance 0.359) seems more aesthetically pleasing and more representative of a teddy bear than the right image (aesthetic 5.818, relevance 0.437). This raises concerns about the reliability of the scoring process.\n\n- The authors claim that the method can be extended to many different metrics. However, the paper only provides experiments with relevance and aesthetics.  **I wonder how the LLM behaves when more metrics (three or more) are involved. Would the model prioritize certain metrics while ignoring others? Will the LLM's completion ability degrade under multiple metrics**？\n- Using LLM for query completion is a clever choice, but while the authors claim to avoid hallucinations, no explicit mechanism is provided. Have the authors  observed redundant, voague, or inaccurate queries, and were any filtering strategies considered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tZ37KrVteb", "forum": "yOEmEXmbV8", "replyto": "yOEmEXmbV8", "signatures": ["ICLR.cc/2026/Conference/Submission3459/Reviewer_syQK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3459/Reviewer_syQK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741900321, "cdate": 1761741900321, "tmdate": 1762916734982, "mdate": 1762916734982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method for text-to-image retrieval called quality-conditioned query completion . The method addresses the problem of short and underspecified user queries. It uses a large language model to expand short queries with descriptive and quality-aware details. The approach does not modify pretrained vision–language models and provides interpretable, controllable retrieval behavior. Experiments on MS-COCO and Flickr2.4M show that QC² consistently improves both semantic relevance and visual quality, outperforming existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a new task: quality-controllable text-to-image retrieval, which considers both semantic relevance and aesthetic quality during retrieval. This setting aligns well with real-world search scenarios.\n2. The method uses a large language model to generate query expansions with quality cues, requiring no modification to existing vision–language models and remaining relatively simple to implement."}, "weaknesses": {"value": "1. The paper mainly focuses on short queries but does not evaluate on datasets with long or descriptive queries. When the textual input already contains sufficient information, the effect of query completion may diminish or even introduce redundancy or semantic drift. Will this strategy still work for underspecified long user query?\n2. The paper's quantitative evaluation relies on a limited set of test queries, namely 80 concrete object nouns. Consequently, the method's performance in handling more abstract, complex, or queries involving emotional atmospheres (such as \"a sense of calm\"，\"a vintage vibe\") remains entirely unvalidated.\n3. This method requires fine-tuning a large language model for each specific image retrieval gallery. This means it is not a \"plug-and-play\" solution. On the contrary, if the gallery is replaced or updated, the dataset must be rebuilt and significant resources must be invested in retraining, which severely limits its scalability.\n4. How can this be extended to image-to-text retrieval?"}, "questions": {"value": "1. How does the method perform on the datasets from the LoTLIP[1]?\n2. How does the scalability of the QCR method which requires an LLM to be fine-tuned for each gallery, comparing to VISA [2], a \"plug-and-play\" method that performs re-ranking at test-time without training?\n\n[1] LoTLIP: Improving Language-Image Pre-training for Long Text Understanding\n\n[2] Visual Abstraction: A Plug-and-Play Approach for Text-Visual Retrieval"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jWBk3CLa96", "forum": "yOEmEXmbV8", "replyto": "yOEmEXmbV8", "signatures": ["ICLR.cc/2026/Conference/Submission3459/Reviewer_e8i9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3459/Reviewer_e8i9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912635932, "cdate": 1761912635932, "tmdate": 1762916734816, "mdate": 1762916734816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores using an LLM to rewrite short text queries into more detailed, quality-aware descriptions, aiming to control the aesthetic and relevance level of retrieved images without modifying the vision-language model. The method learns to expand queries based on quality labels and shows that higher-quality prompts lead to higher-quality retrieval results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear. The paper introduces aesthetic cues to explicitly control and improve retrieval quality. \n2. The method is plug-and-play and easy to apply in existing systems, requiring no modification to the visual model."}, "weaknesses": {"value": "1. Experiments mainly use single-word, single-object queries（e.g., “a dog”）. Real retrieval queries are usually longer, involve multiple objects and relations. Current setup looks more like keyword/entity retrieval.\n2. The paper doesn't report preprocessing cost or inference latency, so it's hard to judge the efficiency of method.\n3. The approach assumes the database has many visually similar images with different aesthetic qualities (like Flickr30k/COCO). In many datasets this won’t hold (VisualNews,fashion200k)."}, "questions": {"value": "1. In real retrieval scenarios, queries often contain multiple objects, relations, and context. How would the method scale to complex, natural multi-entity queries?\n2. How would the method perform when the image databse diversity is limited, or when aesthetic cues are less meaningful (news/fashion domain)?\n3. The theory suggests increased rank leads to better discrimination. But multiple  multiple LLM rewrites could also increase rank. On theoretical, what is the concrete advantage of your controlled rewriting over simple rewriting ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9M3z9Etnbn", "forum": "yOEmEXmbV8", "replyto": "yOEmEXmbV8", "signatures": ["ICLR.cc/2026/Conference/Submission3459/Reviewer_7fCB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3459/Reviewer_7fCB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919176021, "cdate": 1761919176021, "tmdate": 1762916734075, "mdate": 1762916734075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}