{"id": "WT9jewYFNQ", "number": 8319, "cdate": 1758078517142, "mdate": 1759897792061, "content": {"title": "FantasyPortrait: Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers", "abstract": "Producing expressive facial animations from static images is a challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, a diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both single- and multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the model's ability to render fine-grained emotions. For multi-character control, we design a spatial-masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts.", "tldr": "", "keywords": ["Diffusion Models", "Video Generation", "Human Animation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/000d4e407423a9069b197a06760c567d66454c93.pdf", "supplementary_material": "/attachment/4bdbd42606abec1ec6d9059738a3bcf92a9c7c56.zip"}, "replies": [{"content": {"summary": {"value": "FantasyPortrait is a diffusion transformer framework for single- and multi-character portrait animation. It introduces expression-augmented implicit learning to capture fine-grained, identity-agnostic facial dynamics and a masked cross-attention mechanism for coordinated multi-character control. With the new Multi-Expr dataset and ExprBench benchmark, it achieves state-of-the-art realism and emotion-rich animation quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.FantasyPortrait introduces a DiT-based framework for expressive multi-character animation using implicit, identity-agnostic expression control.\n\n2.Its Multi-Expr dataset and ExprBench benchmark fill a key research gap for multi-portrait training and evaluation.\n\n3.The masked cross-attention ensures independent character dynamics without interference.\n\n4.FantasyPortrait achieves the SOTA performance compared to current methods in both single- and multi-portrait animation.\n\n5.FantasyPortrait(Light) provide much faster inference speed and slight reduction in performance."}, "weaknesses": {"value": "1.The discussion of related work is insufficient. It only covers the limitations of explicit driving signal-based methods, while neglecting another line of approaches (e.g., X-Portrait [1], MegActor [2]) that construct paired datasets to model implicit, identity-free motion transfer. Moreover, methods such as FaceShot [3], which propose feasible solutions for handling ‚Äúsubstantial differences in facial structure,‚Äù are not discussed.\n\n2.The citation command \\cite was incorrectly. \\citet (or \\cite) is used when the author‚Äôs name is part of the sentence ‚Äî the citation is integrated into the text (e.g., Smith (2020) proposed...). \\citep is used when the citation is parenthetical, meaning it appears in brackets as supplementary information (e.g., ...as shown in previous work (Smith, 2020).).\n\n3.The design of the Expression-Augmented Encoder seems like adding a set of learnable parameters to the Expression Encoder rather than specific design for detail expressions.\n\n4.I notice that FantasyPortrait performs well on animals in your video demo. It is because the videos of dogs and cats are in your dataset, or the dogs and cats have the similar facial structure with human? Does it generalize to anime characters like Loopy and Peppa Pig.\n\nIf the authors solve all my concerns, I'd love to raise my score.\n\n[1]X-portrait: Expressive portrait animation with hierarchical motion attention, Siggraph.\n\n[2]Harness the Power of Raw Video for Vivid Portrait Animation.\n\n[3]FaceShot: Bring Any Character into Life, ICLR."}, "questions": {"value": "1.What is the limitation of proposed FantasyPortrait?\n\n2.Could your provide the visual results of FantasyPortrait(Light)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "46BGjdtOLH", "forum": "WT9jewYFNQ", "replyto": "WT9jewYFNQ", "signatures": ["ICLR.cc/2026/Conference/Submission8319/Reviewer_uYNe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8319/Reviewer_uYNe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760701910240, "cdate": 1760701910240, "tmdate": 1762920247135, "mdate": 1762920247135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel Diffusion Transformer framework named FantasyPortrait, which can generate high-fidelity and emotion-rich portrait animations in both single-character and multi-character scenarios. Its core innovations include:\n1. Utilizing implicit representations to capture identity-agnostic facial dynamics;\n2. Designing an Expression-Augmented Learning (EAL) module to model fine-grained emotional details;\n3. Introducing a Masked Cross-Attention mechanism to prevent feature interference among multiple characters;\n4. Constructing the Multi-Expr dataset and ExprBench benchmark for systematic evaluation of multi-character animation generation.\nExperimental results show that the proposed method outperforms existing approaches comprehensively in terms of FID, FVD, AED, and APD metrics, and achieves particularly strong performance in cross-identity reenactment scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Novel application of DiT: This is the first work to employ a Diffusion Transformer for multi-character portrait animation, filling a notable research gap in expressive video synthesis.\n2. Dataset and benchmark contribution: The creation of Multi-Expr dataset and ExprBench benchmark provides valuable community resources for evaluation and comparison in this emerging subfield.\n3. Sound methodological design: The combination of implicit expression representation and masked cross-attention is well-motivated and effectively mitigates identity leakage and inter-character interference during multi-subject animation.\n4. Clear ablation justification: The ablation study convincingly demonstrates why expression-augmented learning (EAL) is selectively applied only to non-rigid motion components (lip and emotion), showing clear empirical evidence that full augmentation brings little gain for rigid motions.\n5. Strong experimental performance: The model achieves competitive or superior results across multiple benchmarks (ExprBench, HDTF), with consistent improvements in FID, FVD, AED, and APD. The ablations are thorough and informative.\n6. Diverse generalization capability: The qualitative results include varied portrait styles (e.g., animals, cartoons) and complex real-world conditions (e.g., occlusions, accessories), indicating strong robustness and generalization ability.\n7. Efficiency and practicality: The proposed light version significantly accelerates inference (‚âà50√ó speedup) while maintaining nearly the same perceptual quality, enhancing practical usability."}, "weaknesses": {"value": "1. Methodology description is overly concise: Several critical modules‚Äîsuch as the expression-augmented encoder and masked attention pipeline‚Äîare only briefly introduced. The paper would benefit from more detailed architectural explanations or schematic illustrations.\n2. Unclear training details for learnable tokens: The paper introduces learnable tokens in the expression-augmented encoder, but their initialization, dimensionality, and optimization objectives are not described. This limits reproducibility and interpretability.\n3. Missing comparison with underlying components: Since the model is based on the Wan architecture and utilizes PD-FGC for implicit keypoint extraction, comparisons with these base methods are necessary to clearly separate FantasyPortrait‚Äôs contribution from prior foundations.\n4. Limited demonstration of fine-grained expression control: Although the paper claims ‚Äúfine-grained emotion synthesis,‚Äù the qualitative results lack close-up analyses or visualizations that clearly demonstrate per-region control (e.g., subtle lip or eyebrow motion).\n5. Minor technical issue in reporting: In Table 3, the ‚ÄúSpeed‚Äù unit should be seconds per frame (s/frame) rather than frames per second (frame/s), as the current interpretation conflicts with the numerical scale.\n6. Incomplete dataset release and demonstration: The paper mentions that the Multi-Expr dataset is curated, but the supplementary materials do not include visual samples or clear information about dataset accessibility, licensing, and annotation structure.\n7. Missing emotional diversity evaluation: The experiments mainly focus on overall motion and expression accuracy but lack systematic analysis across different emotion categories (e.g., happiness, sadness, anger). Quantitative emotion classification or perceptual user studies would strengthen the claim of emotional expressiveness.\n8. Stronger and more recent competitors such as VividPortraits, DiffPortrait, and AniFace should be included to strengthen the argument."}, "questions": {"value": "1. Is the maximum number of characters supported in multi-character animation generation limited? The experiments only demonstrate cases with 2‚Äì3 subjects.\n2. When will the Multi-Expr dataset be publicly available, and what will be the method of access?\n3. Can the proposed framework be extended to audio-driven multi-character animation generation?\n4. In the expression-augmented encoder, how are the learnable tokens initialized and optimized? Are they shared across emotion categories or dynamically adapted during training?\n5. Has the team tested the framework on long-form videos (e.g., >30 seconds)? How stable is temporal coherence over extended sequences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NKs0Ug3eyW", "forum": "WT9jewYFNQ", "replyto": "WT9jewYFNQ", "signatures": ["ICLR.cc/2026/Conference/Submission8319/Reviewer_coGC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8319/Reviewer_coGC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761300318224, "cdate": 1761300318224, "tmdate": 1762920246760, "mdate": 1762920246760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FantasyPortrait introduces a diffusion-transformer framework that synthesizes expressive, identity-preserving portrait animations from static images and driving videos, extending conventional single-person reenactment to multi-character scenarios.The model encodes driving signals as implicit facial representations, capturing emotion, lip motion, head pose, and eye movement. For complex non-rigid dynamics (emotion and lips), learnable tokens engage in cross-attention with video tokens, effectively decomposing subtle muscle and affective cues into a higher-dimensional expression subspace. To maintain spatial disentanglement, face masks extracted from the driving video are interpolated into the latent space and used to gate cross-attention, ensuring that each expression embedding modulates only its corresponding facial region."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Masked Cross-Attention mechanism enforces strict spatial gating, ensuring that each expression embedding only influences its corresponding facial region and completely prevents cross-character interference. In addition, by concatenating per-character embeddings with independent masks, the framework allows all characters to be animated synchronously yet independently, maintaining temporal coherence while preserving individual identity and expression consistency‚Äîan essential advancement for scalable, multi-person portrait animation."}, "weaknesses": {"value": "1. In Eq.4, ùëÄ‚äô(QK^T) zeros out cross-region logits through the mask ùëÄ, the subsequent softmax operation normalizes across all tokens, meaning each attention weight can still be indirectly influenced by the presence of others, leading to potential cross-region coupling. Moreover, the trilinear interpolation used to project pixel-level masks into latent space creates soft edges (values between 0‚Äì1), which further undermines the claim of achieving strict spatial isolation.\n2. The overall pipeline shows limited originality‚Äîits key component, Masked Cross-Attention, closely resembles mechanisms used in HunyuanVideo-Avatar, while the expression encoder is pretrained from PD-FGC paper.\n3. The method assumes that 3D VAE latent features are spatially aligned with the input video pixels‚Äîhow robust is this assumption under large head motion or occlusion?\n4. The Masked Cross-Attention module depends on precomputed facial masks‚Äîhow sensitive is the system to mask precision, boundary size? Furthermore, what occurs when faces overlap or partially occlude each other in multi-person scenes?\n5. Expression-Augmented Learning applies learnable tokens only to emotion and lip features; what empirical evidence supports the exclusion of head pose and eye dynamics, especially given that the supplementary video shows occasional misalignment in these components?\n6. it's not clarified how the number and dimensionality of learnable tokens are selected. are these empirically tuned, fixed by prior work, or determined through ablation?\n7. The proposed system builds on Wan2.1-I2V-14B, which is much larger than other baselines, making it difficult to attribute the reported performance gains solely to the proposed architectural innovations."}, "questions": {"value": "please check the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JKwgfoYiOh", "forum": "WT9jewYFNQ", "replyto": "WT9jewYFNQ", "signatures": ["ICLR.cc/2026/Conference/Submission8319/Reviewer_YcGk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8319/Reviewer_YcGk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973714968, "cdate": 1761973714968, "tmdate": 1762920246330, "mdate": 1762920246330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FantasyPortrait demonstrates strong empirical results in multi-character portrait animation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose FantasyPortrait, a system for portrait animation that has achieved strong empirical results, particularly in complex multi-character scenarios. The new \"ExprBench\" benchmark is a valuable resource contribution."}, "weaknesses": {"value": "The proposed method reveals that the work has limited algorithmic novelty.  \n\n- The paper's first key claim, an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics\" , depends on an existing component. The method explicitly employs a \"pretrained implicit expression extractor‚Äã\" from Wang et al. (2023a) to derive all core motion features (e_lip‚Äã, e_eye‚Äã, e_head‚Äã, e_emo‚Äã). The paper's sole algorithmic addition, the \"Expression-Augmented Learning (EAL)\" module, with an expression-augmented encoder, only refines two of inherited features. The ablation study in Table 2 shows removing EAL has no effect on head pose (APD) or eye motion (MAE). Control over these rigid dynamics is fully inherited from the work of Wang et al. (2023a). HunyuanPortrait (Xu et al. 2025) also utilize implicit represention to describe expression and disentangle appearance and motion.\n\n\n- The second key claim, a masked cross-attention mechanism, is an application of a well-established technique in generative models. The problem of \"feature interference\" or \"attribute entanglement\" in multi-subject generation is widely known. Consequently, using spatial masks to guide or constrain attention layers is a common solution, as documented in prior works, including but not limited to CustomVideo (arXiv: 2401.09962), arXiv: 2505.02823, MS-Diffusion (Wang et al. 2024b) and arXiv: 2505.05101. Masked cross-attention is no longer an innovation. FantasyPortrait applies this known method to its specific domain but does not invent the mechanism itself.\n\n\nThe paper's framing overstates its method contributions."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"]}, "details_of_ethics_concerns": {"value": "This paper is associated with facical generation, which may contain bias."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zKn8M5rvt8", "forum": "WT9jewYFNQ", "replyto": "WT9jewYFNQ", "signatures": ["ICLR.cc/2026/Conference/Submission8319/Reviewer_G4db"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8319/Reviewer_G4db"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002983312, "cdate": 1762002983312, "tmdate": 1762920245691, "mdate": 1762920245691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}