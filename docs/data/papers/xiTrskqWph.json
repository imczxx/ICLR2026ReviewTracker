{"id": "xiTrskqWph", "number": 8436, "cdate": 1758083451861, "mdate": 1759897784040, "content": {"title": "Bridging Generalization Gap of Heterogeneous Federated Clients Using Generative Models", "abstract": "Federated Learning (FL) is a privacy-preserving machine learning framework facilitating collaborative training across distributed clients. However, its performance is often compromised by data heterogeneity among participants, which can result in local models with limited generalization capability. Traditional model-homogeneous approaches address this issue primarily by regularizing local training procedures or dynamically adjusting client weights during aggregation. Nevertheless, these methods become unsuitable in scenarios involving clients with heterogeneous model architectures. In this paper, we propose a model-heterogeneous FL framework that enhances clients’ generalization performance on unseen data without relying on parameter aggregation. Instead of model parameters, clients share feature distribution statistics (mean and covariance) with the server. Then each client trains a variational transposed convolutional neural network using Gaussian latent variables sampled from these distributions, and use it to generate synthetic data. By fine-tuning local models with the synthetic data, clients achieve significant improvement of generalization ability. Experimental results demonstrate that our approach not only attains higher generalization accuracy compared to existing model-heterogeneous FL frameworks, but also reduces communication costs and memory consumption.", "tldr": "This paper presents a model-heterogeneous federated learning framework where clients share feature statistics to generate synthetic data for local fine-tuning, improving generalization while reducing communication and memory costs.", "keywords": ["Federated Learning", "Model Heterogeneity"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c3f4ed736806899aaca69844c10d28dec56afd8.pdf", "supplementary_material": "/attachment/e163623dc711c6f294bbe787c547e92947b8f3d8.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the generalization challenge in model-heterogeneous federated learning where clients have different model architectures, making standard parameter aggregation infeasible. The proposed framework, FedVTC, avoids parameter sharing and instead has clients exchange feature distribution statistics (mean and covariance) with the server. Each client then trains a local variational transposed convolutional network, using latent variables sampled based on the aggregated statistics, to generate synthetic data. By fine-tuning their local models on this generated data, clients improve generalization without needing a public dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation of the method is described accurately and in detail, the algorithm process is also explained clearly, and the specific ablation experiments are very comprehensive."}, "weaknesses": {"value": "1. Since each client needs to generate high-quality samples during training, please provide a more detailed analysis of the associated computational cost. Is this process computationally expensive, and if so, how does it affect training efficiency?\n\n2. The paper claims to address model heterogeneity. Please clarify whether the proposed method can handle heterogeneous architectures (e.g., ResNet, MobileNet, CNNs) rather than merely different-sized variants of the same backbone (e.g., ResNet-18 vs. ResNet-50). \n\n3. Since the primary goal of the proposed method is to improve generalization under heterogeneous model settings, the experiments should explicitly evaluate cross-domain generalization. In particular, results should be presented for each client model tested across all global test datasets. Currently, Figure 7 lacks details about the experimental setup, and Table 1 ambiguously refers to “all datasets and distributions.” Please describe these configurations more precisely to ensure reproducibility and clarity.\n\n4. The comparative analysis omits several VAE-based federated learning methods, which are highly relevant to the proposed approach. Including these baselines would provide a more comprehensive evaluation of the method’s relative advantages.\n\n5. In heterogeneous model scenarios, achieving generalization capability is likely related to class-specific feature representations. Even if data distributions differ across clients, a visualization (e.g., feature embeddings or attention maps) illustrating shared generalizable features would greatly enhance the interpretability of the results.\n\n6. In the privacy validation experiments, the visual differences between reconstructed and original samples are evident to the human eye. However, for a machine learning system, such differences should be quantitatively evaluated (e.g., via PSNR, SSIM, or feature similarity metrics). A quantitative privacy comparison would make the evaluation more rigorous.\n\n7. Several detail-related issues should be addressed. The abbreviation “TC” is undefined and should likely be “VTC”; please ensure consistency throughout the manuscript. Additionally, in line 195, “e.g.” should be written as “e.g.,” following standard academic style."}, "questions": {"value": "Please refer to above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kqs1iczfWO", "forum": "xiTrskqWph", "replyto": "xiTrskqWph", "signatures": ["ICLR.cc/2026/Conference/Submission8436/Reviewer_fKa4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8436/Reviewer_fKa4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636147778, "cdate": 1761636147778, "tmdate": 1762920328469, "mdate": 1762920328469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of data and model heterogeneity in federated learning (FL), where clients possess non-IID data and diverse model architectures, leading to limited generalization. The authors propose FedVTC, a model-heterogeneous FL framework that leverages variational transposed convolution (VTC) networks to generate synthetic data for fine-tuning local models. Instead of sharing model parameters, clients communicate feature distribution statistics (prototypes and standard deviations) with the server. The VTC model is trained using a composite loss combining evidence lower bound (ELBO) and distribution matching (DM) terms. Experiments on MNIST, CIFAR10, CIFAR100, and Tiny-ImageNet demonstrate improved generalization accuracy, reduced communication costs, and efficient memory usage compared to existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The use of variational transposed convolution to produce synthetic samples for fine-tuning local models is a distinctive approach, avoiding dependency on public datasets or parameter aggregation.  \n\nThe method is evaluated on four benchmark datasets under varying non-IID settings (Dir(0.1) and Dir(1.0)), demonstrating superior generalization accuracy over five state-of-the-art baselines.  \n\nFedVTC achieves lower communication costs than FedProto, FedTGP, FedGen, CCVR, and FedType and maintains comparable or reduced memory usage through alternating training of local and VTC models.  \n\nThe authors argue that transmitting prototypes and standard deviations does not expose raw data, supported by visual evidence of synthetic samples diverging from real images."}, "weaknesses": {"value": "Although FedVTC is compared to five baselines, the discussion overlooks potential overlaps or distinctions with contemporary approaches such as those using diffusion models or zero-shot learning.  \n\n Generalization performance is measured solely by accuracy; additional metrics like per-class precision/recall or robustness under extreme non-IID conditions would strengthen the claims.  \n\nThe requirement for homogeneous VTC models for aggregation may limit applicability in fully heterogeneous settings where clients cannot support the same VTC structure.  \n\nThe paper does not analyze cases where VTC fails to generate useful synthetic data (e.g., under high data skew or low client participation), nor does it explore the impact of varying latent dimensions on performance.  \n\nWhile memory usage is addressed, the additional computational cost of training VTC networks is not quantified or compared to baseline methods.  \n\nNo formal convergence guarantees or analysis are provided for the alternating optimization of local models and VTC networks, which is critical for FL methods."}, "questions": {"value": "Please respond to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8ILWUeJIIe", "forum": "xiTrskqWph", "replyto": "xiTrskqWph", "signatures": ["ICLR.cc/2026/Conference/Submission8436/Reviewer_MpGA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8436/Reviewer_MpGA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653310277, "cdate": 1761653310277, "tmdate": 1762920328181, "mdate": 1762920328181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedVTC, a model-heterogeneous federated learning framework based on Variational Transposed Convolution. The framework generates synthetic data to fine-tune local models, thereby improving the generalization of heterogeneous clients without relying on public datasets. The paper compares FedVTC with several existing methods, but it lacks comparisons against strong works from 2025."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes using FedVTC to generate synthetic data, which can be used to fine-tune local models and improve their generalization ability, thereby eliminating the reliance on public datasets.\n\n2. FedVTC avoids exposure of raw data by transmitting prototypes, covariances, and VTC models, thus preventing additional privacy risks.\n\n3. A new objective function is designed for training the VTC model, which includes the standard negative ELBO loss and a distribution matching (DM) loss, regularizing the training process and improving the quality of the generated synthetic data."}, "weaknesses": {"value": "1. Some symbols in the formulas are not defined, such as the symbol $V_i$ in Formula 5, which lacks a definition.\n\n2.  The proposed FedVTC framework does not introduce an entirely new solution or framework, but rather builds upon existing methods, which results in a somewhat limited level of innovation.\n\n3. The comparative experiments seem to only compare with works published before 2025, without including comparisons with excellent works from 2025.\n\n4. The flowchart in the paper is somewhat simplistic. For example, in Figure 1, the elements, colors, and arrows are used in a rather basic way, and the relationships and operational details between the steps are not fully demonstrated."}, "questions": {"value": "1. I would like to know the meaning of the symbol $V_i$ in Formula 5.\n\n2. Could you add experiments comparing with the outstanding works from 2025 to provide a clearer comparison of experimental results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tKLgmUJMED", "forum": "xiTrskqWph", "replyto": "xiTrskqWph", "signatures": ["ICLR.cc/2026/Conference/Submission8436/Reviewer_xMVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8436/Reviewer_xMVi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657889227, "cdate": 1761657889227, "tmdate": 1762920327851, "mdate": 1762920327851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to improve the generalization of personalized Federated Learning, specifically enhancing accuracy on unseen data. The authors suggest generating synthetic data at the client side to fine-tune the global model after the federated training phase. This approach improves the performance of local models without requiring access to a public global dataset. To achieve this, the paper introduces a variational transposed convolutional neural network for synthetic data generation. Experimental results demonstrate significant improvements in the average client accuracy across multiple settings, including four datasets and two non-IID scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper is clear to understand and follow\nS2. The approach of fine-tuning the local models of clients using synthetic data is not new but the method using to generate the synthetic data is novel. \nS3. Significant improvement in average accuracy among clients are shown."}, "weaknesses": {"value": "W1. Computation and Communication Overhead: The proposed method requires the server to train and distribute the VTC model to clients. In addition, clients must generate synthetic data locally. These steps introduce non-trivial computational overhead compared to standard federated learning. Please discuss the communication and computation requirements of the proposed method and compare them with those of other existing approaches.\n\nW2. Privacy Preservation: Although the authors discuss privacy preservation in the paper, the examples shown in Figure 4 do not convincingly demonstrate that data privacy is maintained. It would be beneficial to quantitatively assess the similarity between the original samples and the corresponding generated data. For instance, the authors could use the method proposed in Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600–612, 2004, which introduces the Structural Similarity Index (SSIM).\n\nW3. The authors compares FedVTC with 5 other state of the art. However, the authors should discuss the approach in this paper \"Enhancing the Generalization of Personalized Federated Learning with Multi-head Model and Ensemble Voting\" (https://ieeexplore.ieee.org/abstract/document/10579121) which also try to improve the generalization of Personalized Federated Learning.  It will be better if the author could show that proposed method outperform such kind of approach."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CXgfhBBqWW", "forum": "xiTrskqWph", "replyto": "xiTrskqWph", "signatures": ["ICLR.cc/2026/Conference/Submission8436/Reviewer_Ltp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8436/Reviewer_Ltp9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960703158, "cdate": 1761960703158, "tmdate": 1762920326633, "mdate": 1762920326633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}