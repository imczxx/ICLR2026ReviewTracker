{"id": "eieKrzjolP", "number": 4426, "cdate": 1757678427285, "mdate": 1759898033175, "content": {"title": "Epipolar Prompt: A Simple Baseline for Motion Segmentation", "abstract": "Reconstructing dynamic 3D/4D scenes from uncalibrated videos remains challenging due to moving objects violating the static assumptions required for multi-view consistency. While motion segmentation can resolve this, existing methods struggle to generalize across datasets and ignore 3D geometry cues. To this end, we propose **Epipolar Prompt**, a zero-shot framework that synergizes epipolar geometry with foundation segmentation models (e.g., SAM) to achieve robust motion segmentation. Our approach first computes epipolar error maps from optical flow correspondences to localize regions that violate static scene assumptions. These error maps then guide an iterative prompt selection strategy to generate precise segmentation from SAM. Surprisingly, our simple yet effective prompt-based method outperforms both supervised and unsupervised approaches on standard benchmarks (e.g., +9.3 IoU over DAVIS2017) and demonstrates strong generalization to in-the-wild videos. Furthermore, we show that our motion masks serve as a plug-and-play enhancement for existing dynamic 4D reconstruction methods, leading to improved performance.", "tldr": "", "keywords": ["motion segmentation", "SAM", "3D", "4D"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0eb5511c17efc53749ced6230661213934ce17f1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Epipolar Prompt, a training-free network for robust motion segmentation between pairwise images. The authors leverage a pretrained optical flow model to predict correspondences, which are then used to estimate the relative camera pose and compute epipolar error maps. The epipolar error is then used as a cue to prompt the foundation segmentation model SAM to achieve precise motion segmentation. The authors show that the proposed pipeline outperforms both supervised and unsupervised approaches on DAVIS2017 and in-the-wild videos. It can also be used with 4D reconstruction models for better camera tracking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a simple and training-free solution for motion segmentation, which leverages pretrained optical flow and segmentation models SAM.\n- The authors use an epipolar-guided error map to prompt the SAM model, providing useful dynamic cues by estimating camera pose and further improving segmentation through the foundation model.\n- The authors demonstrate state-of-the-art performance on multiple benchmarks, outperforming both supervised and unsupervised methods."}, "weaknesses": {"value": "- Limited novelty. As the authors mention in the introduction, the idea of using epipolar maps has also been explored in RoMo (Goli et al., 2024) and other previous works. The processing steps are similar: (1) use optical flow to obtain correspondences; (2) use correspondences to recover the fundamental matrix and compute epipolar error. This paper uses the epipolar error map to prompt the SAM model, while RoMo uses the error map to train a classifier. \n- Reliability of the epipolar error map in dynamic scenes. The authors propose recovering the fundamental matrix via RANSAC. However, on challenging dynamic scenes (e.g., Sintel), RANSAC often fails to capture static correspondences correctly, which leads to inaccurate camera poses. As a result, the epipolar error map may not provide useful cues for these scenes.\n- Comparison to MonST3R masks and runtime. As shown in Table 3, the proposed mask performs similarly to the MonST3R mask, which doesn’t require a foundation segmentation model. Also, as mentioned in the supplementary, the runtime with VLM-based filtering takes about 2–3 seconds per pair, which is slow compared to other methods like the MonST3R mask."}, "questions": {"value": "- It would be helpful if the authors compared their approach with RoMo (Goli et al., 2024) and clarified the specific contributions. In addition, the authors should include a comparison with RoMo in Table 2.\n- Since RANSAC can handle only limited amounts of dynamic motion, how robust is the proposed method for predicting motion segmentation if RANSAC fails to obtain the static correspondences?\n- In Table 3, the authors show that incorporating the motion segmentation mask improves depth estimation. The authors should further explain how the mask improves depth performance in DUSt3R/MonST3R, as this is not trivial.\n- The proposed method relies on pairwise optical flow and camera pose estimation for motion segmentation. How does the method extend to video sequences? Can it produce temporally consistent segmentation of the same object?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bm75riMQoR", "forum": "eieKrzjolP", "replyto": "eieKrzjolP", "signatures": ["ICLR.cc/2026/Conference/Submission4426/Reviewer_eqwm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4426/Reviewer_eqwm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694178074, "cdate": 1761694178074, "tmdate": 1762917358173, "mdate": 1762917358173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Epipolar Prompt, a zero-shot motion segmentation framework that integrates epipolar geometry with the Segment Anything Model (SAM). The key idea is that violations of epipolar geometry in optical flow correspondences can identify moving regions in a scene. These regions are then used to prompt SAM to produce motion segmentation masks without training or fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper smartly combines epipolar geometry—a classical geometric constraint—with the Segment Anything Model (SAM). \n2. The method does not require training or fine-tuning, making it lightweight and adaptable. \n3. The results show the effectiveness of the proposed method."}, "weaknesses": {"value": "1. Dependence on heuristic thresholds: The approach relies heavily on manually tuned parameters such as the epipolar error threshold and confidence/stability scores. This dependence can make performance sensitive to dataset variations and limit robustness in uncontrolled environments.\n2. Limited performance on older benchmarks: Although strong overall, the method performs slightly worse than FlowP-SAM on SegTrack v2 and FBMS59, suggesting that it may struggle with low-quality or complex motion data.\n3. Vulnerability to optical flow errors: Since the method builds on optical flow correspondences, inaccuracies from flow estimation (due to blur, occlusion, or lighting changes) can directly affect segmentation quality, producing false motion regions.\n4. Engineering-heavy, limited theoretical novelty: The contribution lies mainly in the integration of existing tools (epipolar geometry, SAM, heuristic filtering) rather than a fundamentally new theoretical framework, which may reduce its perceived novelty for top-tier conferences.\n5. Although the paper provides quantitative benchmarks and qualitative frame examples, the absence of supplementary video results (or at least a more extensive collection of visual examples in the appendix) makes it difficult to fully assess the temporal consistency and perceptual quality of the motion segmentation. Providing such results would strengthen the empirical evidence and transparency of the work."}, "questions": {"value": "1. The authors mostly follow the evaluation datasets and metrics used in [1]. However, it is unclear why the MOCA dataset [2] was not included in the evaluation. Could the authors explain the reason for excluding this dataset?\n2. The combined FlowP-SAM + FlowI-SAM model in [1] demonstrates higher performance than the individual models. It would strengthen the comparison if the authors included the results of FlowP-SAM + FlowI-SAM in Table 2. Since this combined model is likely larger, comparing the number of parameters would further highlight the efficiency advantage of the proposed method.\n3. Could the authors also report the model size and inference time to better illustrate the computational efficiency of the proposed approach?\n4. How sensitive (or robust) the performance is with respect to the thresholds (confidence, stability, IoU)?\n\n\nReferences\n[1] Xie, Junyu, et al. \"Moving Object Segmentation: All You Need is SAM (and Flow).\" Asian Conference on Computer Vision. Singapore: Springer Nature Singapore, 2024.\n[2] Lamdouar, Hala, et al. \"Betrayed by Motion: Camouflaged Object Discovery via Motion Segmentation.\" Proceedings of the Asian Conference on Computer Vision, 2020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ngxoet5J9o", "forum": "eieKrzjolP", "replyto": "eieKrzjolP", "signatures": ["ICLR.cc/2026/Conference/Submission4426/Reviewer_Q59N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4426/Reviewer_Q59N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801539008, "cdate": 1761801539008, "tmdate": 1762917357950, "mdate": 1762917357950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a learning-free approach to moving object segmentation via so-called epipolar prompt. Given a pretrained SAM, VLM, and optical flow model, the method first calculate epipolar error map and detect regions that violates the epipolar constraints (ie, moving part). Then it samples few points from the region, prompts SAM using those points, and gets moving object mask. It iteratively updates the mask while being validated via VLM. The method achieves the best/competitive accuracy on the benchmark.\n\n---\n\nThe proposed method sounds great with good accuracy. However, there are concerns on the novelty, paper fit, and overstating of 'no training needed' while using more advantageous setups than others, which can mislead. Thus the recommendation is **4: marginally below the acceptance threshold**. However, any thoughts and justifications regarding these concerns would be appreciated!"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Clarity**\n\n  The paper is written clearly and read really well. It includes necessary technical details to fully understand the method. It further includes in depth analyses of the method, such as ablation study, hyperparameter choices, limitations, justifications on the design choices, etc.\n\n* **Good results**\n\n  The methods achieves the competitive/best accuracy among other methods in the benchmark. In the downstream task (camera pose evaluation and depth estimation), the setup with the proposed methods achieves the best accuracy over other setups using other methods."}, "weaknesses": {"value": "* **Novelty concern / paper fit**\n\n  Though the paper shows good accuracy on the benchmark, I cannot erase an impression that the method is likely a well-composed pipeline of established methods using heuristics (one could interpret it as a contribution of the method though, ie simple composition of off-the-shelf methods beat learning-based algorithms). The idea is neat indeed. However, I am not so sure if there can be any new novel finding or learning representation learned from the paper. \n\n  Call of papers in the ICLR webpages says that it accepts applications in vision, but the paper seems quite at the end of the application side (close to WACV, for example). I was wondering if the paper would fit to the ICLR's interests. \n\n\n* **Argument on 'no training needed'**\n\n  In Table 2, the paper classifies its method as `No training needed`, which is true. Though the proposed technique itself is `No training needed`, the whole pipeline is based on a more advantageous setup using several off-the-shelf methods including RAFT, SAM, and VLM (with 7M parameters). This argument might mislead others. \n\n  Also, without the usage of VLM, how much does the accuracy drop on each dataset in Table 2? (Table 8 shows only partial results)\n\n  Also in Table 2, some numbers (on SegTrack v2 and FBMS59) underperform FlowP-SAM. It's fine because the method still outperforms on the other benchmarks and downstream tasks as well. However, given its advantageous setup, it weakens the strength of the paper."}, "questions": {"value": "* **Minor**\n\n  In Table 4, the number on Sintel, `DUSt3R w/ FlowP-SAM mask`, I think the RMSE 0.5111 might be a typo. \n\n\n* **Accuracy of the epipolar map**\n\n  I was wondering if there are any analyses on the accuracy of the epipolar error map and curious how accurate/reliable it is. (it may not be necessary but good to evaluate the accuracy on a dataset with GT camera pose)\n\n* **Extreme scenario**\n\n  I was also wondering if there is any limitation coming from the epipolar map. For example, it's a rare case that other methods can fail as well, but let's assume an image pair where a foreground object dominate most of the image part, and the epipolar error map highlights the background region. Then how will the method behave?\n\n* **What's the runtime of the method?**\n\n  Especially it's curious how much time does each stage take."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MAW0Nsmzcv", "forum": "eieKrzjolP", "replyto": "eieKrzjolP", "signatures": ["ICLR.cc/2026/Conference/Submission4426/Reviewer_cN3G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4426/Reviewer_cN3G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886911019, "cdate": 1761886911019, "tmdate": 1762917357694, "mdate": 1762917357694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}