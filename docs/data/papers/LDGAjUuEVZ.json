{"id": "LDGAjUuEVZ", "number": 16708, "cdate": 1758267889159, "mdate": 1759897223661, "content": {"title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese", "abstract": "As large language models (LLMs) evolve into web-interacting agents, their ability to retrieve and reason over real-time information has become a crucial benchmark for general intelligence. However, existing benchmarks such as BrowseComp focus solely on English, neglecting the linguistic, infrastructural, and retrieval-specific challenges posed by other information ecosystems—particularly the Chinese web. We present BrowseComp-ZH, a high-difficulty, natively-constructed benchmark designed to assess LLM agents’ web browsing abilities in Chinese. Rather than translating from English, all questions in BrowseComp-ZH are written from scratch by native speakers to reflect authentic information-seeking behaviors and cultural contexts. The dataset comprises 289 multi-hop questions across 11 diverse domains, each reverse-engineered from a short, verifiable answer and filtered through a twostage quality control pipeline to ensure retrieval hardness and answer uniqueness. We evaluate over 20 leading LLMs and agentic search systems. Despite strong language and retrieval abilities, most models perform poorly: many score below 10% accuracy, and only a few exceed 20%. Even the best system achieves just 42.9% accuracy. These results highlight the considerable difficulty of BrowseComp-ZH, where success requires not only robust retrieval strategies but also advanced multihop reasoning and information reconciliation—abilities that remain challenging for current models. BrowseComp-ZH thus serves as a stress test for web-interactive LLMs beyond English, offering a rigorous and linguistically diverse evaluation framework to guide future research on multilingual agent capabilities.", "tldr": "", "keywords": ["Large Language Models", "Web Browsing", "Retrieval and Reasoning", "Benchmark Dataset", "Chinese Web"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87c1f7ab97266c5f92c7c3d587b18381aa2f10bb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BrowseComp-ZH, a benchmark for evaluating LLM agents’ web browsing and information-seeking abilities specifically within the Chinese web ecosystem. The dataset comprises 289 human-authored, multi-hop, multi-constraint questions across 11 knowledge domains, meticulously reverse-engineered from verifiable answers. The paper also provides thorough statistics, calibration analysis, multiple failure case studies, and benchmark reproducibility details."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper fills a clear gap by constructing the first natively-annotated Chinese web browsing benchmark, avoiding translation artifacts and capturing authentic linguistic, cultural, and infrastructural challenges unique to the Chinese internet.\n- The reverse-design pipeline is impressively meticulous, resulting in a high-quality, stress-test-style dataset."}, "weaknesses": {"value": "- At only 289 questions, BrowseComp-ZH is considerably smaller than many contemporary benchmarks. This small scale could weaken robustness for fine-grained model assessment or pre-train/fine-tune settings in the future.\n- Despite employing multi-agent and human-in-the-loop processes, the guarantee of a single unique answer still depends on current model/retrieval limitations and annotator creativity."}, "questions": {"value": "- How were the final domain-specific question counts determined? Is there a risk of over-representation of certain topics?\n- How can the annotators ensure that the difficulty of the problem is sufficient and that this benchmark has the ability to evaluate the entire RAG system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w0rfk7vqQq", "forum": "LDGAjUuEVZ", "replyto": "LDGAjUuEVZ", "signatures": ["ICLR.cc/2026/Conference/Submission16708/Reviewer_8svw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16708/Reviewer_8svw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821580012, "cdate": 1761821580012, "tmdate": 1762926761208, "mdate": 1762926761208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BrowseComp-ZH, a new, high-difficulty benchmark designed to evaluate the web browsing and information retrieval capabilities of Large Language Models (LLMs) specifically within the Chinese web ecosystem.\n\nKey Contributions:\n* Natively Constructed: It consists of 289 questions written from scratch by native Chinese speakers to reflect authentic user behavior, rather than being translated from English.\n\n* High Difficulty: The questions are multi-hop, requiring complex reasoning and the ability to synthesize information from multiple sources.\n\n* Rigorous Design: Questions were reverse-engineered from verified answers and passed through a two-stage quality control process to ensure they are difficult to solve and have unique answers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Fills a Critical Gap: It’s the first benchmark for evaluating LLMs’ web-browsing abilities in the Chinese ecosystem, avoiding flaws of translated English benchmarks by using native Chinese questions that reflect linguistic, cultural, and Chinese web-specific traits. \n\n2. Rigorous Construction: Its 289 multi-hop questions (11 domains) go through strict quality control—reverse-engineered from answers, tested on major search engines to ensure difficulty, and validated for answer uniqueness. \n\n3. Insightful Evaluation: It assesses over 20 systems (open/closed-source models, commercial tools) and includes human baselines (17.6% accuracy), revealing key findings (e.g., reasoning and multi-round retrieval boost performance) to guide LLM improvement. \n\n4. Practical Guidance: Serves as a targeted stress test for non-English web LLMs, aiding the development of multilingual AI agents for real-world Chinese web scenarios."}, "weaknesses": {"value": "1. Small Dataset Scale: With only 289 questions, it is smaller than English counterparts like BrowseComp, though the authors note this stems from high curation costs. \n\n2. Incomplete Answer Uniqueness: Despite strict checks, the reverse-design approach cannot fully guarantee no alternative valid answers exist for some questions. There are also some obvious bugs in the dataset that need to be fixed.\n\n3. Compared to BrowseComp, BrowseComp-ZH is insufficiently challenging. It is essentially a simplified Chinese adaptation that lacks originality."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lpKvKG3fT2", "forum": "LDGAjUuEVZ", "replyto": "LDGAjUuEVZ", "signatures": ["ICLR.cc/2026/Conference/Submission16708/Reviewer_yxRg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16708/Reviewer_yxRg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882570380, "cdate": 1761882570380, "tmdate": 1762926760654, "mdate": 1762926760654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BrowseComp-ZH, a native Chinese benchmark for evaluating LLMs’ web browsing and reasoning abilities. The dataset contains 289 expert-authored, multi-hop questions across 11 domains, validated for retrieval difficulty and answer uniqueness. Over 20 models—including open-, closed-source, and commercial agents—are evaluated, showing generally poor performance (most <10% accuracy), while multi-round retrieval systems like DeepResearch achieve the best results. The benchmark offers a rigorous, culturally grounded evaluation of LLM agents in the Chinese web ecosystem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark offers a rigorous, culturally grounded evaluation of LLM agents in the Chinese web ecosystem.\n  - The benchmark is carefully constructed with dual validation for difficulty and answer uniqueness, ensuring high data quality.\n  - It systematically compares over 20 systems, including human baselines, revealing meaningful performance gaps."}, "weaknesses": {"value": "- The paper’s main contribution appears incremental rather than novel. From a task construction perspective, BrowseComp-ZH closely mirrors the original BrowseComp benchmark, with the primary difference being its adaptation to the Chinese web environment rather than a fundamentally new methodology or task design.\n  - The benchmark contains only 289 queries, which may not be sufficient to capture the full linguistic, structural, and retrieval complexity of the Chinese internet. Such a limited number of examples could constrain the benchmark’s representativeness and the statistical robustness of the reported results.\n  - The paper lacks a deeper analysis of why models perform differently on BrowseComp-ZH. It remains unclear whether the observed performance gaps stem mainly from the change of language (English → Chinese) or from the distinct characteristics of the Chinese web ecosystem. A more detailed ablation or cross-lingual comparison would strengthen the interpretation of results."}, "questions": {"value": "- Have you compared model performance between BrowseComp-ZH and the original English BrowseComp benchmark? Specifically, do models exhibit consistent relative rankings or performance gaps across the two languages? Including such a cross-lingual comparison or correlation analysis would help clarify whether the observed difficulties stem from linguistic or retrieval-ecosystem differences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "41a8c0pYEq", "forum": "LDGAjUuEVZ", "replyto": "LDGAjUuEVZ", "signatures": ["ICLR.cc/2026/Conference/Submission16708/Reviewer_3iN7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16708/Reviewer_3iN7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904016610, "cdate": 1761904016610, "tmdate": 1762926759046, "mdate": 1762926759046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce BrowseComp-ZH, a new benchmark for evaluating the web-browsing ability of LLMs in Chinese. The authors created reverse-engineered, multi-constraint questions that push models to do multi-step reasoning across real Chinese web sources like Baidu, Zhihu, and WeChat articles. Expert annotators curated the dataset through several rounds of filtering to keep only challenging questions with clear, unique answers. The final dataset covers 15 diverse domains. Evaluation results show that DeepResearch hit 42.9% accuracy, human searchers averaged 17.6%, and most general-purpose LLMs stayed below 10%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work faithfully reimplements the BrowseComp methodology in the Chinese-language ecosystem, where the data reflects how Chinese users search online.\n- All queries, evidence chains, and browsing steps are authored natively in Chinese, not translated.\n- Reporting calibration alongside accuracy improves interpretability of model performance."}, "weaknesses": {"value": "- The benchmark creation process almost exactly mirrors BrowseComp’s methodology, and the contribution of this work feels somewhat incremental. The core takeaway is essentially: current LLMs (agents) perform poorly when browsing in Chinese.\n- The idea of cultural groundedness is not empirically verified. Checking how often agent trajectories only involve Chinese sources would provide some insights on how much of the task is really dependent on navigating Chinese sources. The paper claims all queries, evidence chains, and browsing steps are “authored in Chinese,” but does not verify that all necessary evidence exists exclusively on Chinese websites.\n- There is no analysis of failure modes. An important question to explore is how often errors stem from retrieval shortcomings versus misinterpretations of Chinese cultural or linguistic context."}, "questions": {"value": "- Could you provide more details on the human baseline experiment? How were the annotators hired, and how were examples distributed among them? Were there cases where people gave up?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O3YgcbhsTo", "forum": "LDGAjUuEVZ", "replyto": "LDGAjUuEVZ", "signatures": ["ICLR.cc/2026/Conference/Submission16708/Reviewer_bkpL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16708/Reviewer_bkpL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972915929, "cdate": 1761972915929, "tmdate": 1762926758544, "mdate": 1762926758544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}