{"id": "neE8pqIqyR", "number": 21379, "cdate": 1758316868376, "mdate": 1759896925076, "content": {"title": "PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs", "abstract": "Text watermarking for large language models (LLMs) is important for model owners to verify the origin and protect the intellectual property of AI-generated text. While watermarking methods for closed-source LLMs' text generation are relatively mature, watermarking open-source LLMs' text generation remains challenging. Closed-source model developers typically embed text watermarks during decoding; however, this approach is ineffective for the text generation of open-source models, where developers have no control over how decoding occurs. As a result, owners of open-source LLMs still lack practical methods to verify whether a given piece of AI-generated text originated from their models. The primary challenge lies in embedding watermarks directly into model weights without compromising detection accuracy. One possible solution is first to create a text generation watermark in the closed-source setting, then distill that watermark information into the publicly released model's weights.  However, this approach faces two critical challenges: (i) Reduced detectability due to inconsistency between the watermark patterns learned by the model and the predefined patterns used during detection. This inconsistency arises because existing closed-source watermark patterns are difficult for models to learn effectively. (ii) Vulnerability to modifications by downstream users, such as fine-tuning or model merging, which may weaken or completely remove the embedded watermark. To address these challenges, we propose ***PRO***, a precise and robust text watermarking method for open-source LLMs. First, we introduce a trainable watermark policy model, which is jointly optimized with the LLM during training. This co-optimization helps generate watermark patterns that are easier for the model to learn, significantly reducing inconsistencies between generated patterns and predefined detection criteria. Additionally, we incorporate a regularization term into the watermarking loss, which simulates various perturbations (e.g., fine-tuning, model merging) and penalizes any degradation in watermark detectability under these modifications. This approach ensures that the embedded watermark remains resilient even after downstream model alterations. Our evaluation on mainstream open-source LLMs (e.g., LLaMA-3.2, LLaMA-3, and Phi-2) demonstrates that our approach significantly outperforms prior methods in terms of both watermark detectability and robustness against model modifications. The code is publicly available at https://anonymous.4open.science/r/PRO-DE2A/README.md.", "tldr": "", "keywords": ["Watermark algorithms", "Large Language Models", "Open-source LLMs"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d80c5cff058ac2a786b2482db12c4c43e0280554.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes PRO, a framework for embedding text watermarks directly into the weights of open-source large language models.\nIt introduces two key components: 1) Co-Adaptive Watermark Policy (CAWP), which jointly trains a learnable watermark mapping and the model to improve generation–detection consistency, and 2) Forgotten Perturbation-aware Learning (FPL), which simulates fine-tuning perturbations during training to enhance robustness against model modifications such as merging, pruning, and quantization.\nExperiments on LLaMA and Phi models show that PRO achieves high AUC and low perplexity while maintaining strong robustness compared to prior watermarking methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a watermarking for open-source LLMs. The proposed PRO framework—with its co-adaptive watermark policy (CAWP) and forgotten perturbation-aware learning (FPL) is empirically effective, showing strong robustness against fine-tuning, merging, and pruning while maintaining good text quality."}, "weaknesses": {"value": "1. The convergence analysis in Appendix D essentially restates the standard smoothness and gradient-descent convergence results of KL-based distillation. However, it does not establish any theoretical link between the proposed objectives and the final detection metrics (e.g., AUC or TPR at low FPR), nor does it formally connect to the claimed robustness objective (detectability under fine-tuning, merging, or pruning). For FPL, what is the bound the forgotten perturbation space? In reality, users perform diverse modifications such as LoRA/adapters, parameter averaging or smoothing, and even full re-distillation. It remains unclear how this simplified perturbation approximates such real-world updates.\n\n2. In Table 1, the paper reports AUC which measures overall separability across all thresholds and TPR @ 5%FPR. However, in watermark detection, the low-FPR region is the only practically relevant regime but high AUC does not imply good performance at low FPR. Table 1 should report TPR @ 1% FPR (and ideally TPR @ 0.1\\% FPR). This would align with common practice in watermarking papers (e.g., KGW [1]) and make the results interpretable for real-world deployment.\n\n3. The robustness evaluation currently covers only full-parameter fine-tuning, model merging, quantization, and pruning.\nHowever, in the open-source ecosystem, attackers typically use cheaper and more practical operations such as distillation, i.e., training a new student model using an nonwatermarked teacher. This strong modification could completely rewrite the output distribution and is feasible for professional attackers.\n\n[1] A Watermark for Large Language Models. John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein. ICML 2023"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F9XzwgafJO", "forum": "neE8pqIqyR", "replyto": "neE8pqIqyR", "signatures": ["ICLR.cc/2026/Conference/Submission21379/Reviewer_2HJA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21379/Reviewer_2HJA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884918196, "cdate": 1761884918196, "tmdate": 1762941733145, "mdate": 1762941733145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRO, a Precise and Robust watermarking framework for open-source large language models (LLMs) that embeds watermarks directly into model weights rather than at the decoding stage, making it effective even when users modify or re-implement decoding. The approach features a trainable watermark policy model jointly optimized with the base LLM to encourage watermark-friendly generation patterns, and a robustness regularization term that simulates downstream perturbations such as fine-tuning and model merging while penalizing any degradation in watermark detectability. Experiments on LLaMA-3.2, LLaMA-3, and Phi-2 show that PRO achieves higher watermark detectability and stronger robustness than prior methods, with publicly released code supporting reproducibility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a practical and growing problem: watermarking open-source LLMs where owners lack control over decoding.\n- The experiments across multiple open-source models demonstrate that PRO yields higher watermark detectability and improved resistance to post-training modification compared to baseline methods.\n- The paper is clearly structured, with intuitive figures."}, "weaknesses": {"value": "- The paper does not provide a formal analysis or theoretical guarantee on why the joint optimization leads to higher detectability or robustness. A more rigorous treatment (e.g., gradient alignment or mutual information perspective) would strengthen the claims.\n- While PRO aims for “precise and robust” watermarking, the authors do not systematically evaluate how the approach affects the model’s general performance (e.g., perplexity, generation quality, reasoning accuracy) on more diverse and broader tasks.\n- The paper does not isolate the contributions of the trainable policy model and the robustness regularizer.\n- While PRO simulates perturbations from fine-tuning and model merging, it overlooks other crucial real-world perturbations, such as reinforcement learning–based post-training (e.g., RLHF or DPO)."}, "questions": {"value": "In general, the paper is very comprehensive. Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vKstk9Jx0h", "forum": "neE8pqIqyR", "replyto": "neE8pqIqyR", "signatures": ["ICLR.cc/2026/Conference/Submission21379/Reviewer_LnXy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21379/Reviewer_LnXy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888041141, "cdate": 1761888041141, "tmdate": 1762941732740, "mdate": 1762941732740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework to embed resilient watermarks directly into model weights. It introduces Co-Adaptive Watermark Policy (CAWP) to jointly learn watermark patterns aligned with the model’s behavior, reducing generation–detection inconsistency, and Forgotten Perturbation-aware Learning (FPL) to enhance robustness against fine-tuning and model merging. Experiments on LLaMA3 and Phi2 models show that PRO achieves higher detectability, better text quality, and stronger robustness than existing open-source watermarking methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Identify the problem of Generation-Detection Inconsistency. The mappings of watermarked tokens are arbitrary.\n2. Provide a novel method co-adapting the watmeark model with the real model to better align the watermark with the model's innate performance. And innovatively devise the FPL module to properly solve the weakness of the current open-source model watermark to finetuning.\n3. carry out experiment validating the performance of PRO."}, "weaknesses": {"value": "1. Using model merging as an attack to evaluate learning-based watermarking may be inappropriate, since such attacks assume access to an unwatermarked model. In my opinion model merging shouldn't be considered as a valid attack.\n2. Because a key component of CAWP relies on an MLP that extracts semantic information through a BERT encoder, it would be important to include comparisons with prior semantic-invariant distillation method to demonstrate the necessity and contribution of the co-training design.\n3. The FPL's loss function seems to only prevent local curvature, lack theoretical and empiraicla exepriment for its efficacy when the model's being trained on more anti-watemrakr token. Does it means the watermakred model itself is hard to be finetuned and gain downstream capability?"}, "questions": {"value": "1. The experiment detail is missing in 3.2's motivation study, which shows the inconsistency between the teacher model and student model, According to prior work [1], it seems to me that the reported AUC drop is unexpectedly large and may require further clarification or replication details.\n\n2. While AUC is reported as the main detection metric, it is not the most interpretable measure for practical watermark detection. It is recommended to include TPR at a given FPR in the main paper rather than relegating it to the appendix, as this provides a clearer assessment of real-world detection performance.\n(also see my questions in weakness)\n\n[1] Gu, C., Li, X. L., Liang, P., & Hashimoto, T. (2024). On the Learnability of Watermarks for Language Models. In Proceedings of the Twelfth International Conference on Learning Representations (ICLR 2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vl5lkqCBZr", "forum": "neE8pqIqyR", "replyto": "neE8pqIqyR", "signatures": ["ICLR.cc/2026/Conference/Submission21379/Reviewer_gnTk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21379/Reviewer_gnTk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948060304, "cdate": 1761948060304, "tmdate": 1762941732354, "mdate": 1762941732354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a text watermarking method for open-weight LLMs. Because users have control over inference procedures, a watermark for open-weight LLMs must be learned into the weights, instead of relying on decoding-based watermarking. Towards this goal, this paper proposes PRO, a method which jointly trains a watermark policy alongside the LLM, and also includes a loss term that penalizes a decrease in watermark detectability under a weight perturbation. Experiments claim that PRO outperforms baseline methods in terms of text quality, watermark detectability, and robustness against model modifications (e.g., fine-tuning, model merging)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Effective and robust watermarking for open-weight LLMs is an important open problem. As open-weight LLMs become more capable and widely used, combating LLM misuse via methods such as watermarking become more important.  \n2. The proposed method seems like a natural way to approach the problem. It simultaneously optimizes the watermark policy to increase detectability, along with optimizing against degradation in detectability from a simulated gradient update step on red tokens.  \n3. The code is publicly released, enhancing the transparency and reproducibility of the results. Training configurations and hyperparameters are also included in the appendix."}, "weaknesses": {"value": "1. Watermark detectability still drops significantly in PRO after fine-tuning. The TPR@5 decreases from 0.99 to 0.37 after 1500 fine-tuning steps on OpenMath Instruct, which I’m not sure I would call “robust”.  \n2. The numbers reported for the Gloaguen et al. (2025) method in Table 1 do not match up with the numbers they reported, even though the experimental setups seem to be mostly the same. [Gloaguen et al. (2025)](https://arxiv.org/abs/2502.10525) (Table 1\\) reports 0.69 TPR@5 after 2,500 fine-tuning steps on OpenMathInstruct (and considers this **not** robust), whereas the PRO paper reports 0.222 TPR@5 after 1,500 fine-tuning steps on OpenMathInstruct. This significant discrepancy is strange.  \n3. [Gloaguen et al. (2025)](https://arxiv.org/abs/2502.10525) find that watermark durability can be improved by increasing the distillation dataset size. In their Table 2, they report that this method can achieve 0.91 TPR@5 after 2,500 fine-tuning steps on OpenMathInstruct. The PRO paper does not mention this method at all, and does not run any experiments on how the dataset size/number of training tokens affects durability.  \n4. The baseline perplexity of the original model before training is not included. It would be good to have the original model in Table 1, Figure 6, etc. in order to evaluate how much PRO impacts the text quality, compared to no watermarking. It might also be nice to compare with the performance of decoding-based watermarking (perhaps slightly less necessary).  \n   * Line 144 claims that PRO “can even match the performance of closed-source counterpart.” But I do not see any experimental comparisons with decoding-based watermarking to support this claim.  \n5. Watermark detection is now more expensive, as it requires running an embedding model and MLP. Most existing detectors are non-neural and can be run on CPU only.  \n6. It would be ideal to have I.4 Robustness Against Paraphrasing Attack in the main paper, as robustness to text modifications is an important aspect of evaluating watermarking methods. But I understand that the main paper is already at the page limit.  \n7. Appendix F: \"Watermarking for Closed-Source LLMs\" in this paper appears to be mostly taken from Appendix A: \"Additional Details on Watermarking Strategies\" in [Gu et al. (2023)](https://arxiv.org/abs/2312.04469), yet there is no citation. Some parts are nearly verbatim identical. A citation appropriately indicating the source and extent of reuse should be added to avoid potential plagiarism concerns.  \n   * Similarly, some parts of Appendix G: \"Model Modification\" appear to be paraphrased from Section 3: \"Durability of Open-Source LLM Watermarks\" of [Gloaguen et al. (2025)](https://arxiv.org/abs/2502.10525), again with no citation. The overall structure/organization of topics is very similar. An appropriate citation should also be added here."}, "questions": {"value": "1. Why doesn’t the PRO method appear in Figure 2 (right)?  \n2. There should be citations for the models used, e.g., Llama 3 (line 139), Phi2 (line 140), BERT (line 264), etc.  \n3. Equation 3: KL divergence should be computed on the probabilities, not the raw logits. Also, $\\\\pi$ represented probabilities earlier, so it is inconsistent to have it denote logits now.  \n4. Line 280 describes ensuring half the tokens are more likely and the other half are less likely, i.e., $\\\\gamma \\= 0.5$. But what if the developer wants to use some other value of $\\\\gamma$, such as 0.25?  \n   * Also, zero mean does not necessarily ensure that half are positive and half are negative. If $1/3$ are positive with logits 1 and $2/3$ are negative with logits \\-0.5, then the mean is still zero.  \n5. The labels in equation (4) for the terms for (i) unbiased token preference and (ii) balanced watermark logits appear to be swapped.  \n   * The first term incentivizes zero mean across the vocabulary for each input embedding. (label should be balanced watermark logits)  \n   * The second term incentivizes each token to have zero mean across input positions. (label should be unbiased token preference)  \n6. Another related work that uses neural networks to generate watermark logits is [Liu et al. (ICLR 2024\\)](https://openreview.net/forum?id=gMLQwKDY3N).  \n7. What is the perplexity of the original model before any training? And the original model with decoding-based watermarking?  \n8. How many tokens are the methods in Table 1 trained on?  \n9. What are the training details for the OpenMath/OpenCode fine-tuning experiments, e.g., batch size, learning rate, training time, etc.?  \n10. Several references are cited as arXiv preprints, but they have been published in conferences. For example, [Frantar el at. (ICLR 2023\\)](https://openreview.net/forum?id=tcbBPnfwxS), [Gu et al. (ICLR 2024\\)](https://openreview.net/forum?id=9k0krNzvlV), [Kirchenbauer et al. (ICLR 2024\\)](https://openreview.net/forum?id=DEJIDCmWOz), [Kuditipudi et al. (TMLR)](https://openreview.net/forum?id=FpaCL1MO2C), [Sun et al. (ICLR 2024\\)](https://openreview.net/forum?id=PxoFut3dWW), [Zhao et al. (ICLR 2024\\)](https://openreview.net/forum?id=SsmT8aO45L). Please update these references, and check for any other papers that are incorrectly cited as preprints.\n\n### Minor notes\n\n1. The font size in equations (1) and (2) seem abnormally small.  \n2. The absolute values in equation (4) should be made larger so they are the same size as the enclosed expression (can use `\\left` and `\\right`).  \n3. I suggest using $|x|$ instead of $N$ for the sequence length in equation (3), to avoid confusion with $N$ as the number of samples.  \n4. Per the [ICLR Author Guide](https://iclr.cc/Conferences/2026/AuthorGuide), the Ethics Statement and Reproducibility Statement should appear before the References section, right after the main paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "98fwcPGpQi", "forum": "neE8pqIqyR", "replyto": "neE8pqIqyR", "signatures": ["ICLR.cc/2026/Conference/Submission21379/Reviewer_2PZW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21379/Reviewer_2PZW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954759893, "cdate": 1761954759893, "tmdate": 1762941731986, "mdate": 1762941731986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}