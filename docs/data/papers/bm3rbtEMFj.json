{"id": "bm3rbtEMFj", "number": 25001, "cdate": 1758362981972, "mdate": 1759896738635, "content": {"title": "ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL", "abstract": "Real-world robotic agents must act under partial observability and long horizons, where key cues may appear long before they affect decision making. However, most modern approaches rely solely on instantaneous information, without incorporating insights from the past. Standard recurrent or transformer models struggle with retaining and leveraging long-term dependencies: context windows truncate history, while naive memory extensions fail under scale and sparsity. \nWe propose $\\textbf{ELMUR}$ ($\\textbf{E}$xternal $\\textbf{L}$ayer $\\textbf{M}$emory with $\\textbf{U}$pdate/$\\textbf{R}$ewrite), a transformer architecture with structured external memory. Each layer maintains memory embeddings, interacts with them via bidirectional cross-attention, and updates them through an $\\textbf{L}$east $\\textbf{R}$ecently $\\textbf{U}$sed $\\textbf{(LRU)}$ memory module using replacement or convex blending. \nELMUR extends effective horizons up to 100,000 times beyond the attention window and achieves a 100% success rate on a synthetic T-Maze task with corridors up to one million steps. In POPGym, it outperforms baselines on more than half of the tasks. On MIKASA-Robo sparse-reward manipulation tasks with visual observations, it nearly doubles the performance of strong baselines. These results demonstrate that structured, layer-local external memory offers a simple and scalable approach to decision making under partial observability.", "tldr": "ELMUR is a transformer model with layer-local external memory and LRU-based memory updates for long-horizon reasoning in POMDPs", "keywords": ["RL", "POMDP", "Memory", "Transformer", "Robotics"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cfd2fdf517449e0e46ee727f55b775b0e2846745.pdf", "supplementary_material": "/attachment/42c0b9b6dddf2f4dcff1141421581f361f5f5da6.zip"}, "replies": [{"content": {"summary": {"value": "ELMUR (External Layer Memory with Update/Rewrite) is a Transformer architecture that introduces structured, layer-wise external memory. Each layer maintains and updates memory embeddings through bidirectional cross-attention and an LRU-based update mechanism, enabling extremely long-term dependency modeling: up to 100,000× beyond the standard attention window. ELMUR achieves superior performance on long-horizon, partially observable tasks such as T-Maze, POPGym, and MIKASA-Robo, demonstrating a simple yet scalable solution for decision-making with long-range dependencies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a new ELMUR mechanism and demonstrates its strong generalization ability through experiments."}, "weaknesses": {"value": "1. **Table Placement and Layout:**\n\nTable 1 is placed too low and is visually disconnected from the surrounding text. It would be better to reposition the table for improved readability and consistency. The overall layout of the paper requires further optimization.\n\n2. **Ablation Study and Update Formula:**\n\nThe ablation study shows that the parameter λ is unstable when M<N. This instability might stem from the blending update formula. The motivation for choosing the specific update rule in Eq.8 should be explained in greater depth.\n\n3. **Uniform Update Weight Across Segments:**\n\nThe paper applies the same update weight to all segments, which seems somewhat unreasonable. In practice, different segments may have varying importance — for example, key frames might require longer retention (i.e., a smaller λ). This issue is not discussed in the current analysis or in the future work section.\n\n4. **Applicability and Generalization:**\n\nHow generalizable is this improvement? Is it limited to the Transformer architecture used in this paper’s experiments? The authors should apply ELMUR to more model architectures to demonstrate its generality or at least analyze its applicable scenarios.\n\n5. **Bidirectional Update Visualization:**\n\nThe bidirectional update mechanism would benefit from a visual analysis. For instance, when processing a specific frame’s token, is the most relevant portion of memory indeed assigned the highest weight? Otherwise, the observed performance gain might simply result from adding a new learnable external module.\n\n6. **Comparison with Prior Memory Methods:**\n\nA comparison table should be provided to clearly illustrate the differences between ELMUR and prior memory-related approaches."}, "questions": {"value": "1. What is the core motivation behind selecting the specific update rule presented in Eq. 8?\n\n2. Why not consider adaptive strategies that adjust the update behavior based on the relative importance of different segments?\n\n3. How generalizable is the observed improvement across different architectures？\n\n4. Why not include a visual analysis illustrating the proposed bidirectional update mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "47EH3ziK7J", "forum": "bm3rbtEMFj", "replyto": "bm3rbtEMFj", "signatures": ["ICLR.cc/2026/Conference/Submission25001/Reviewer_AzD4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25001/Reviewer_AzD4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830377034, "cdate": 1761830377034, "tmdate": 1762943278589, "mdate": 1762943278589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ELMUR to solve POMDPs by allowing a transformer to read and write to external memory, reminiscent of a Differentiable Neural Computer. \n\nELMUR consists of two blocks that feed each other. The token block is essentially a standard transformer with an additional cross attention module between the encoded observations and a cache. At each timestep, the token block outputs both an action and writes to the cache. The memory block consists of an LRU cache that produces memory vectors for cross attention with the token block. Importantly, the memory block is able to update its stored embeddings. These embeddings are written using a convex update $\\lambda m_{new} + (1-\\lambda)_{old}. The paper also presents a positional encoding that functions across multiple segments. The approach section ends with a short analysis on the decay rate of memories using ELMUR.\n\nThe paper evaluates ELMUR across three benchmarks and compares to other baselines such as RATE, DT, and DMamba. The results show that ELMUR achieves higher returns than other models. A T-Maze experiment demonstrates ELMUR generalizes to long trajectories. The paper performs ablation studies on the update weight $\\lambda$, random initialization of memory slots, number of slots, and segment length."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is organized well and pleasant to read. \n- The contributions are clearly explained.\n- The approach performs better than previous methods."}, "weaknesses": {"value": "-  RQ1 and RQ4 experiments appear to be an unfair comparison, as DT does not have segment-level recurrence while the proposed method does. I would like to see a comparison between ELMUR and a DT+GTrXL method instead. This is the main weakness of the paper.\n- It is unclear why $\\lambda$ is selected rather than learned, an additional ablation would be useful.\n    - A learned, input-dependent $\\lambda$ would potentially remove the need for an LRU-eviction scheme."}, "questions": {"value": "- My understanding is the LRU eviction is based on writes, not reads. Could this result in older but important memories being evicted?\n- Why is $\\lambda$ a hyperparameter and not learned?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5jwFQsgP8a", "forum": "bm3rbtEMFj", "replyto": "bm3rbtEMFj", "signatures": ["ICLR.cc/2026/Conference/Submission25001/Reviewer_1eVV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25001/Reviewer_1eVV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964914697, "cdate": 1761964914697, "tmdate": 1762943278269, "mdate": 1762943278269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ELMUR, which addresses long-horizon decision-making under partial observability by augmenting each transformer layer with structured external memory updated via a Least Recently Used (LRU) mechanism with convex blending. Through offline imitation learning experiments, ELMUR achieves 100% success on T-Maze corridors up to one million steps, nearly doubles baseline performance on MIKASA-Robo visual manipulation tasks, and ranks first on half of the POPGym benchmark tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Provides theoretical guarantees on memory retention horizons and embedding boundedness\n* The T-Maze result (100% success at 1M steps) is genuinely impressive and demonstrates extreme memory retention"}, "weaknesses": {"value": "* Title claiming \"Long-Horizon RL\" while only evaluating offline imitation learning is misleading and inappropriate.\n* Lacks online RL experiments and comparisons with strong online RL baselines (e.g., R2I [1]). While the authors claim that unlimited interaction would significantly increase training time, sample efficiency should also be considered in online RL settings, which is a core evaluation metric for RL methods.\n\n[1] Mastering Memory Tasks with World Models, ICLR 2024"}, "questions": {"value": "* Could the authors provide experiments on more 3D vision memory-intensive tasks (e.g., MemoryMaze)? Most memory-intensive tasks in the paper appear to be in simple observation environments.\n* How do author determine memory capacity $M$ for new tasks?\n* What is actually stored in memory slots? Could the authors provide qualitative results to demonstrate what information the memory embeddings capture?\n* In the ablation studies (Table 3), the advantages of DeepSeek MoE over MLP are not evident, with both achieving identical performance. Could the authors demonstrate whether MoE outperforms MLP across all benchmark environments, or provide quantitative results regarding computational efficiency advantages (FLOPs, training time, memory usage)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p8awNhbMph", "forum": "bm3rbtEMFj", "replyto": "bm3rbtEMFj", "signatures": ["ICLR.cc/2026/Conference/Submission25001/Reviewer_bpxq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25001/Reviewer_bpxq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967416972, "cdate": 1761967416972, "tmdate": 1762943278039, "mdate": 1762943278039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ELMUR (External Layer Memory with Update/Rewrite), a transformer policy for long-horizon control under partial observability. Each transformer layer owns a fixed-size external slot memory and two explicit pathways: mem→tok (read) and tok→mem (write) cross-attention. Memory is managed with a usage-aware LRU + convex-blend rule and temporal bias on writes, enabling controllable, analyzable forgetting via a closed-form half-life/effective horizon. Experiments demonstrate (i) retention far beyond the attention window on long-horizon probes (e.g., T-maze/key→door), (ii) strong performance across POPGym, and (iii) gains on pixel-based manipulation with sparse rewards. Ablations indicate per-layer memory and the LRU-blend update are key drivers. A direct comparison with RMT is included; I recommend adding an RMT-L diagnostic (layer-local RMT) to fully isolate where the gains come from."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**\n* Combines per-layer fixed slot memory, explicit read/write paths, and LRU-blend with temporal write bias plus a half-life analysis—a configuration not present in prior explicit-memory policies.\n\n**Quality**\n\n* Method is precisely specified (blocks/pseudocode/hparams), and experiments span long-horizon probes, POPGym, and pixel manipulation, with ablations (M, λ, layer-local vs shared) supporting design choices.\n\n**Clarity**\n\n* Clean separation of self-attn vs mem→tok / tok→mem improves interpretability; slot contents/usage are inspectable. Figures make the two-track flow and LRU/blend easy to understand.\n\n**Significance**\n\n* Tackles long-horizon POMDP memory at fixed compute (scales with #layers×slots, not history), yielding gains on memory-heavy control and visual manipulation and aiding debugging/safety via controllable forgetting."}, "weaknesses": {"value": "* **Standpoint/structure unclear.** The paper under-organizes its narrative: Introduction/Background don’t explicitly position ELMUR along the key memory axes (global vs layer-local, implicit vs explicit/usage-aware), and the method opener (“GPT + memory”) obscures the real novelty (explicit read/write, LRU-blend, temporal write bias, half-life).\n* **Memory-centric analysis is insufficient.** Although RMT is compared, there’s no RMT-L (layer-local RMT, same total memory) to isolate layer placement vs explicit management effects. More diagnostics—slot-usage/overwrite rates, write-location histograms, retention curves under capacity pressure—would clarify how memory is used."}, "questions": {"value": "1. **Add an RMT-L diagnostic (optional but recommended).** \n2. **Clarify standpoint in Intro/Background.** Provide a compact “**Memory for long-horizon control**” subsection contrasting TXL/Compressive/Ring (history-scaled caches), RMT (shared tokens; implicit updates), RATE (recurrent state), test-time memory (e.g., Titans), and ELMUR (layer-local slots; **explicit read/write**; **LRU-blend** with temporal bias)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ofmyOY7Yrr", "forum": "bm3rbtEMFj", "replyto": "bm3rbtEMFj", "signatures": ["ICLR.cc/2026/Conference/Submission25001/Reviewer_PqH3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25001/Reviewer_PqH3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984193014, "cdate": 1761984193014, "tmdate": 1762943277773, "mdate": 1762943277773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}