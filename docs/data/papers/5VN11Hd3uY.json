{"id": "5VN11Hd3uY", "number": 14951, "cdate": 1758246003864, "mdate": 1759897339597, "content": {"title": "From Fields to Random Trees", "abstract": "This study introduces a novel method for performing Maximum A Posteriori (MAP) estimation on Markov Random Fields (MRFs) that are defined on locally and sparsely connected graphs, broadly existing in real-world applications. We address this long-standing challenge by sampling uniform random spanning trees(SPT) from the associated graph. Such a sampling procedure effectively breaks the cycles and decomposes the original MAP inference problem into overlapping sub-problems on trees, which can be solved exactly and efficiently. We demonstrate the effectiveness of our approach on various types of graphical models, including grids, cellular/cell networks, and Erdős–Rényi graphs. Our algorithm outperforms various baselines on synthetic, UAI inference competition, and real-world PCI problems, specifically in cases involving locally and sparsely connected graphs. Furthermore, our method achieves comparable results to these methods in other scenarios.The code of our model can be accessed at \\url{https://anonymous.4open.science/r/From-fields-to-trees-iclr-EB75}.", "tldr": "", "keywords": ["MAP estimation", "Markov Random Fields", "random spanning trees"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58bd3171c2d05b1e7bcd44d781151353211ed4cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a spanning-tree–based MAP inference algorithm (called SPT) for Markov Random Fields (MRFs) on locally and sparsely connected graphs, which is a regime common in power grids, communication networks, and transportation networks. The key idea is to sample uniform random spanning trees (RSTs) from the original graph; then run exact BP on each sampled tree (cycle-free so inference is tractable). Experiments on both synthetic an real datasets show the better performance of the proposal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper has clear motivation and introdution for the problem setup and the proposal.\n- The complexity analysis and convergence guarantee is given and offer insights into the dependence on the problem parameters.\n- The proposed algorithm is scalable thus supports inference for large graphs.\n- The empirical validation on synthetic + UAI + real PCI datasets shows promising gains of the proposed approach."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "- In Theorem 1, the error bound decreases as the number of spanning trees $\\mathcal{K}$ increases. The proposed algorithm is taliored for sparse graphs. However, a sparse graph should have less spanning trees compared to a dense graph, which leads to a worse error bound. How do we understand this?\n- In the experiment of Figure 1, where we compare energy against number of iteration, I wonder if this is a fair comparison. As these methods can be doing very different things and computation in each iteration step.\n- Maybe a better comparison is to look at the wall clock time needed for each baseline to reach a certain energy accuracy.\n- Why is the proposed algorithm inconsistent on ER graph?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fNVEVMDsDl", "forum": "5VN11Hd3uY", "replyto": "5VN11Hd3uY", "signatures": ["ICLR.cc/2026/Conference/Submission14951/Reviewer_Vpu6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14951/Reviewer_Vpu6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575119216, "cdate": 1761575119216, "tmdate": 1762925291383, "mdate": 1762925291383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores sparsity and local connectedness in MAP estimation of Markov Random Fields (MRFs) proposing to combine multiple (spanning) subtrees learned exactly by sampling multiple spanning trees from the original graph and applying conventional belief propagation (BP). These exactly solved spanning subtrees are combined through an inverse weighting function accounting for the probability of sampling given edges to provide aggregated approximate solutions to the entire MRF. The approach is simple and straightforward to implement and appears to work well in practice as highlighted in the papers experimental section."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tThe approach is well related to the existing literature in the literature review.\n\n•\tThe approach appears to work well in practice and is simple to implement.\n\n•\tThe weighting scheme to correct for how edges in the spanning tree are sampled compared to uniform sampling appears as a simple, elegant, and valid practical approach to correct for biases by the sampling procedure.\n\nOriginality:\nThe approach combining multiple spanning subtrees appears new and original.\n\nQuality:\nThe paper is generally clear and easy to follow. The experimentation considers comparison to a limited number of baselines which can be improved.\n\nClarity:\nThe paper can be improved in its writing - please also see the minor comments under Weaknesses, however, the developed methodology is clear. \n\nSignificance:\nMinimizing pairwise MRFs are an extensively studied field with many contributions and approaches developed over the years. The paper here develops an interesting new approach which could warrant publication, but the significance of the results are unclear as no error bars are reported and the paper only compared to a limited number of alternatives."}, "weaknesses": {"value": "The present procedure relies on estimating the spanning three exactly requiring O(N^3) which limits the approach to small graphs and subtrees. While scalable approaches are discussed it is unclear how well they perform in practice.\n\nWhereas applications where the methodology is important is discussed in the motivation it is unclear how the solutions benefit practical applications. It would strengthen the paper to consider the solutions for at least one of the given problem domains highlighted in the motivation (introduction) and the practical implications.\n\nThe method is compared to very few competing methods, whereas the literature and approaches minimizing the energy of MRF including for the considered pairwise MRFs is vast as also highlighted in the rather old survey:\n\nWang, Chaohui, Nikos Komodakis, and Nikos Paragios. \"Markov random field modeling, inference & learning in computer vision & image understanding: A survey.\" Computer Vision and Image Understanding 117.11 (2013): 1610-1627.\n\nWhere submodular random fields have also been considered for pairwise random fields such as the papers cited in the related works section:\n\nH. Ishikawa, Exact Optimization for Markov Random Fields with Convex Priors, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 25 (10) (2003) 1333-1336.\n\nD. Schlesinger, B. Flach, Transforming an Arbitrary Minsum Problem into a Binary One, Tech. Rep. TUD-FI06-01, Dresden University of Technology (2006)\n\nFurthermore MRF estimation has been scaled using massively parallelized implementations as in:\nhttps://download.mmag.hrz.tu-darmstadt.de/media/FB20/GCC/paper/Thuerck-2016-HPG.pdf\n\nIn this context, I find the experimentation to only include a very limited  number of comparisons to existing methods, i.e. Mean field, LBP, and TRBP whereas the literature is vast with many proposed methods. \n\nMinor issues of the paper needing some proof-reading:\nProblem equation 1 is known NP-hard in general -> The problem in equation 1 is known to be NP-hard in general\n\nsolve equation 1 can date back 80’s of the previous century -> solve equation 1 dates back  to the 80’s of the previous century\napproach to infer on MRFs  -> approach to infer MRFs \n\nStrange use of past tense of “could” in the Methods section which should be “can” throughout please check. \n\nSince Acquire the true tree -> Since Acquiring the true tree"}, "questions": {"value": "Consider include scalable approaches based on approximate spanning tree estimation procedures and compare how the exact to such scalable approaches compare.\n\nPlease provide error bars in Figure 1 and the Tables across multiple runs. As the procedure is non-deterministic it will be good to see how much variability this induces on the results.\n\nMRF estimation has been demonstrated to benefit from massively parallelization – how does the proposed approach compare to such parallelized implementations as in:\n\nhttps://download.mmag.hrz.tu-darmstadt.de/media/FB20/GCC/paper/Thuerck-2016-HPG.pdf\n\nFurthermore, how does the procedure compare to other methods covered in the related works section such as \n\nH. Ishikawa, Exact Optimization for Markov Random Fields with Convex Priors, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 25 (10) (2003) 1333-1336.\n\nD. Schlesinger, B. Flach, Transforming an Arbitrary Minsum Problem into a Binary One, Tech. Rep. TUD-FI06-01, Dresden University of Technology (2006)\n\nIn equation 12 the product does not produce as I understand it a valid conditional distribution \\tilde{p}(x_i|X\\{x_i}). Please clarify how samples are drawn from the distribution, is it simply renormalized by \\tilde{p}(x_i|X\\{x_i})/(\\sum_{x_i} \\tilde{p}(x_i|X\\{x_i})) and shouldn’t \"=\" then be \"\\propto\" (if it is not a normalized distribution)?\n\nIn summary, I consider this a borderline leaning accept paper but with room for improvements in particular in terms of establishing their approach to more alternatives by providing a wider comparison and include error bars for the assessments of results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CJWR27hQJr", "forum": "5VN11Hd3uY", "replyto": "5VN11Hd3uY", "signatures": ["ICLR.cc/2026/Conference/Submission14951/Reviewer_inkb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14951/Reviewer_inkb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817170560, "cdate": 1761817170560, "tmdate": 1762925290346, "mdate": 1762925290346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studied the problem of finding the minimum energy state in a Markov random field (MRF). The problem is NP-hard in general because it can in-code the max-cut problem. The Belief Propagation (BP) is a famous algorithm for computing marginal distributions (inference) of MRFs. The algorithm can solve the inference problem exactly if the graph is a tree. The main contribution of this paper is a heuristic algorithm for finding the minimum energy state of a MRF. \n- The algorithm generate some i.i.d. samples of uniform random spanning trees.\n- For each tree (re-weight the edge according to the marginal of random spanning trees), run BP to compute marginals of every nodes. \n- Merge the results of all trees by taking the product in (12) line 237.\n- Given the marginals, using a greedy algorithm and a simple sampling (sample each marginal independently) algorithm to find a state. Update the current best answer.\n\nThe paper gives theoretical analysis on some step of the algorithm. The paper then did a lot of experiment on both synthetic data and real-world data.\n\nTo summarize, I think the theory part is of this paper is simple. The idea share some similarity with graph sparsification algorithm, which also use the random tree to sparsify the graph and change the edge weight (you may add some discussion). Some main theoretical results is the correctness of expectation and a concentration bound. However, it is interesting to see a simple algorithm works in many real application."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposed to use random trees to approximate the MRF, which connects the concept of graph sparsification with the MAP (Maximum A Posteriori) inference problem in probabilistic graphical models. Intuitively, by constructing a collection of randomly sampled tree structures, one can run existing algorithms on trees while random trees preserves some information of the original MRF.\n\n- The experimental evaluation on synthetic data and real-world data is comprehensive and well-structured. Detailed tables and visualizations are provided to illustrate the results, including comparisons with baseline methods. The findings shows that their algorithm can achieve a good performance in many situations."}, "weaknesses": {"value": "- The MAP problem is trivial in trees because one can use a dynamic programming (the algorithm called max-product or min-sum algorithm). In this paper, the authors use BP to compute marginal distributions on trees and use marginal on trees to approximate marginal on MRFs.  The connection between computing marginal distributions and identifying the minimum energy configuration is not stated in the paper.  Even if this is a heuristic, it would be good to explain some intuition here. (See more detailed questions in the next section)\n\n- Some statement in the theorems and lemmas and some proofs look confusing.  (See more detailed questions in the next section)"}, "questions": {"value": "Consider a Markov Random Field (MRF) defined on a general graph, inducing a Gibbs distribution $\\mu$ over $\\mathcal{X}^V$ (assume $\\mathcal{X} =$ {0,1}). Although computing marginals is generally intractable, suppose we have access to an oracle that provides the marginal distribution for each node. For any $v \\in V$, the oracle returns $p_v = \\text{Pr}_{X \\sim \\mu}[X_v = 0]$. With this information, we can directly run the GibbsSampler and GreedySelector as described in Algorithm 1. The question is: does the algorithm find a good state with low energy?\n\nHere is a simple example. Consider the energy function defined in (1), with $\\mathcal{X}$ = {0,1} and $\\theta_i(x_i) = 0$ for all $i$ and $x_i$. For the pairwise interaction term $\\theta_{ij}$, define:\n- $\\theta_{ij}(x_i, x_j) = 1$ if $x_i \\neq x_j$\n- $\\theta_{ij}(x_i, x_j) = 0$ if $x_i = x_j$\n\nClearly, the minimum energy configuration corresponds to the MAX-CUT of the graph, which is NP-hard. In this case, due to the symmetry between values $0$ and $1$, the marginal distribution for each node is uniform over {0,1}. This suggests that marginals alone may not help in finding the minimum energy state.\n\n**Question:** In general, what is the relationship between computing marginals and finding the minimum energy state? Since the problem itself is NP-hard, we cannot expect the algorithm work for all cases. It is better to give some intuition and explanation on which situation this idea could provide a good solution. \n\n---\n\n**Lemma 1** relies on an unrealistic assumption. The set $\\mathcal{K}$ is a randomly sampled set in the algorithm and may contain duplicate elements. The lemma assumes $\\mathcal{K} = \\mathcal{T}$, but this assumption is problematic for two reasons:\n1. Let $N = |\\mathcal{T}|$ be the number of spanning trees. Since $N$ can be exponential in $n$, i.e., $N = \\exp(O(n))$, it is infeasible to generate that many samples.\n2. Even if it were feasible, the condition $\\mathcal{K} = \\mathcal{T}$ implies no duplicates in $\\mathcal{K}$ (otherwise, equation (17) in the proof of Lemma 1 fails). However, the algorithm samples with replacement, making this event extremely unlikely.\n\n---\n\n**Lemma 2** is more reasonable. It states that the expected energy is correct. Some notational improvements can be made in equation (13):\n- You can remove the factor $\\frac{1}{|\\mathcal{K}|}$ and the summation over $T_k \\in \\mathcal{K}$, and simply replace $T_k$ with $T$. It suffices to show that the expectation over a single random spanning tree $T\\sim \\Omega(\\mathcal{T})$ is correct. Since $\\mathcal{K}$ consists of i.i.d. samples, the expectation of their average is also correct.\n- If you prefer to keep the set $\\mathcal{K}$, then the expectation should be taken over the randomness of all i.i.d. samples in $\\mathcal{K}$, where each $T_k \\sim \\Omega(\\mathcal{T})$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xAeL35QElV", "forum": "5VN11Hd3uY", "replyto": "5VN11Hd3uY", "signatures": ["ICLR.cc/2026/Conference/Submission14951/Reviewer_14zd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14951/Reviewer_14zd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886086796, "cdate": 1761886086796, "tmdate": 1762925288922, "mdate": 1762925288922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}