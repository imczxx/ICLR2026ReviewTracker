{"id": "Fx8AtzGlTS", "number": 8250, "cdate": 1758076270193, "mdate": 1763627563010, "content": {"title": "Flow-Disentangled Feature Importance", "abstract": "Reliably quantifying both feature importance and its statistical uncertainty is a fundamental goal for interpretability, yet classical model-agnostic methods produce unreliable attributions that compromise statistical validity when features are correlated. Statistical approaches that address correlation through feature decorrelation have shown promise but remain restricted to $\\ell_2$ loss, limiting their applicability across diverse machine learning tasks. We introduce Flow-Disentangled Feature Importance (FDFI), a model-agnostic framework that resolves these limitations by combining principled statistical inference with computational flexibility. FDFI leverages flow matching to learn flexible disentanglement maps that not only handle arbitrary feature distributions but also provide an interpretable pathway for understanding how importance is attributed through the data's correlation structure. The framework generalizes the decorrelation-based attribution to general differentiable loss functions, enabling statistically valid importance assessment for black-box predictors across regression and classification. We establish statistical inference theory, deriving semiparametric efficiency of FDFI estimators, which enables valid confidence intervals and hypothesis testing with Type I error control. Experiments demonstrate that FDFI achieves substantially higher statistical power than existing removal-based and conditional permutation approaches while maintaining robust, interpretable attributions under severe multicollinearity on synthetic and biomedical datasets.", "tldr": "", "keywords": ["Interpretability", "Feature Importance", "Statistical Inference", "Correlation Distortion", "Uncertainty Quantification"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb0e07d70afc8582127537a1a5bba631f6e4691d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Flow-Disentangled Feature Importance (FDFI), a model-agnostic interpretability framework designed to provide statistically valid feature importance estimation even in the presence of strong feature correlations. The key idea is to use flow matching—a flexible generative modeling technique—to learn a disentanglement map that transforms correlated features into an approximately independent latent space. Once features are decorrelated in this latent space, feature importance is computed and then projected back to the original space, along with uncertainty quantification based on semiparametric efficiency theory.\nFDFI generalizes earlier Disentangled Feature Importance (DFI) work, extending it from the restrictive $\\ell_2\\text{-loss}$ setting to arbitrary differentiable loss functions, thereby supporting both regression and classification tasks. The paper provides theoretical results proving the asymptotic normality and semiparametric efficiency of the proposed estimators, along with empirical results on synthetic and biomedical datasets (e.g., Cardiotocography, Diabetes, and MicroMass). Empirically, FDFI achieves higher AUC, power, and more robust inference than competing model-agnostic methods such as LOCO, CPI, and DFI, especially under multicollinearity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Sound theoretical framing: The paper provides a clear unification of LOCO, CPI, and SCPI under general differentiable loss functions, and introduces a principled FDFI definition that subsumes DFI as the ℓ₂ special case. The semiparametric efficiency and asymptotic normality proofs are technically rigorous and well-documented.\n2. Flexible, scalable transport: The flow-matching approach yields expressive and stable mappings with a well-defined training objective.\n3. Compelling empirical results: FDFI consistently achieves higher AUC and statistical power while maintaining Type-I error control across correlated and non-Gaussian settings (Figures 2–3). The CTG, Pima, and MicroMass studies further confirm interpretability and clinical relevance, including recovery of known physiological patterns and robustness in high-dimensional regimes (d ≈ 1300)."}, "weaknesses": {"value": "1. Because the disentanglement map $T$ is learned via flow matching rather than optimal transport, different flow paths could, in principle, lead to distinct ‘independent’ latent representations $Z$. While Appendix B establishes uniqueness of the ODE solution for a chosen interpolation path, semantic identifiability of the latent axes remains uncertain. This limits how literally one can interpret the Jacobian heatmaps as explanatory structures. Incorporating quantitative independence diagnostics (e.g., distance correlation or energy tests on $Z$) would strengthen interpretability claims.\n2. Although the asymptotic theory accommodates $n^{-\\frac{1}{4}}$ convergence rates, finite-sample behavior may be sensitive to inaccuracies in the estimated flow $T$. The paper could be strengthened by an ablation over auxiliary-sample size $m$ and architectural choices (depth, width) to examine effects on MMD independence, feature scores, and variance estimates. Appendix D.2 provides training details, but robustness to nuisance quality remains empirically under-explored."}, "questions": {"value": "1. Latent independence diagnostics: Can you report quantitative independence scores (e.g., pairwise HSIC or distance correlation) for the learned $Z$ in the main experiments, and relate them to attribution stability and power?\n2. Sensitivity to flow training: In Appendix D.2 you select hyperparameters with MMD; could you add (a) an ablation on auxiliary set size $m$, (b) $M$ (latent resamples) vs. variance/power, and (c) model depth/width vs. AUC/power?\n3. Interpretability of Jacobians: The CTG heatmap (Fig. 4a) is compelling. Can you provide case‑wise attributions (e.g., conditional on ranges of clinical variables) to illustrate how Jacobian structure plus latent scores produce the final importance?\n4.When FDFI may fail: Are there data‑generating processes (e.g., strong higher‑order interactions that can’t be linearized locally) where the squared‑Jacobian weighting is systematically biased, even if the latent scores are well estimated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ySOvnxYYxN", "forum": "Fx8AtzGlTS", "replyto": "Fx8AtzGlTS", "signatures": ["ICLR.cc/2026/Conference/Submission8250/Reviewer_akD3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8250/Reviewer_akD3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653369324, "cdate": 1761653369324, "tmdate": 1762920193247, "mdate": 1762920193247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Flow-Disentangled Feature Importance (FDFI), a new model-agnostic framework for estimating feature importance and its statistical uncertainty in ML models. Unlike standard model-agnostic attribution methods (e.g., permutation or removal-based approaches) that become unreliable when features are correlated (correlation distortion), FDFI provides statistically valid and interpretable attributions even under severe multicollinearity.\n\n\nPrevious decorrelation-based methods (e.g., Disentangled Feature Importance, DFI) were limited to l2 loss, but FDFI generalizes this to any differentiable loss function, allowing its use in both regression and classification settings. FDFI employs flow matching, a recent generative modeling technique (Lipman et al.), to learn a disentanglement map that transforms correlated features X into statistically independent latent variables Z. This approach replaces the restrictive Gaussian otpimal transport assumption used in prior work. It enables flexible modeling of arbitrary feature distributions while preserving interpretability of the transformation.\n\nThe authors derive a semiparametric inference theory showing that FDFI estimators are asymptotically efficient and allow valid confidence intervals and hypothesis testing with controlled Type I error. The FDFI estimator remains sqrt(n)-consistent even when the flow map T is estimated nonparametrically at slower rates. Furthermore, FDFI quantifies how each feature’s contribution to model loss propagates through the correlation structure of the data. It provides both latent-level and original-feature-level importance scores, linking disentangled representations back to raw variables.\n\nEvaluations on synthetic and biomedical datasets compare FDFI with LOCO (Leave-One-Covariate-Out), CPI (Conditional Permutation Importance), and DFI. FDFI achieves higher statistical power in detecting relevant features, controlled type I error, and robust performance under high feature correlation. FDFI also recovers correlated yet predictive features that other methods fail to identify."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main contribution of the paper is well-motivated, clearly situating its novel approach within an interesting problem setting. The authors provide a compelling analysis of existing methods, outlining its strengths and weaknesses, clearly identifying specific gaps and limitations that effectively frame the need for their proposed framework. This strong introduction is complemented by a solid theoretical foundation. The authors build their first theoretical results on reasonable assumption (A1) to establish strong initial guarantees, such as the tight bounds presented in Theorem 2.1, ensuring the soundness of their method.\n\nThe primary theoretical contribution is Theorem 3.1, establishing the semiparametric efficiency of the latent FDFI estimator, $\\hat{\\phi}^{\\text{FDFI}}_{Z_j}$. This is a solid result. It proves that the estimator achieves the optimal $\\sqrt{n}$-convergence rate. A significant practical implication of this efficiency is that this $\\sqrt{n}$-consistency holds even when using a flexible, nonparametrically-estimated nuisance map, $\\hat{T}$, which itself may converge at a slower rate.\n\nThis property is important indeed as it allows for the use of complex machine learning models to estimate the nuisance components without sacrificing the statistical validity of the final importance score. Finally, the authors complete their framework by providing the necessary inferential tools in Proposition 3.2. This proposition grounds the theory in practice, allowing for the construction of valid confidence intervals and hypothesis tests for the FDFI scores, making the entire method statistically sound and practically applicable.\n\nBefore proceeding to the weaknesses I would like to point out that, although the theoretical results seem sound, I have not checked their validity (especially regarding the proofs)."}, "weaknesses": {"value": "The paper extends the DFI framework and identifies two of its key limitations: its reliance on the optimal transport map for disentanglement, which can be computationally expensive and less flexible for complex, high-dimensional distributions. However, this claim seems somewhat not well tackled by FDFI. From the appendix, it becomes evident that the proposed FDFI methods are actually slower in terms of computation time, and the experiments are applied only to relatively simple settings—Gaussian and mixture-of-Gaussian data, along with a real-world dataset of max 21 dimensions. When reading this section, I was hoping that FDFI would be faster than DFI or at laset that the authors would provide stronger evidence demonstrating its advantages on complex, high-dimensional data. Neither of these expectations is convincingly addressed.\n\nFigure 1 focuses primarily on highlighting the benefits of FDFI compared to LOCO, CPI, and SHAP, yet it would be more informative if it also emphasized improvements over DFI. This addition would help clarify and disentangle which of the observed gains are due to the new FDFI framework itself and which are inherited from DFI.\n\nIn line 233, the authors claim that DFI is limited because it cannot be applied to classification tasks or models using general differentiable loss functions. However, in Assumption 1 they assume that the loss is M-smooth, which already implies differentiability. It is unclear whether this assumption applies only to the first two theoretical results or to the broader framework. If the former case is the true one, it would be useful for the authors to clarify this connection and more explicitly relate Section 2.1 to the FDFI formulation.\n\nIn line 260, the authors mention that Equation (4) recovers the DFI formulation under the l2 loss. It would strengthen the paper if they explicitly demonstrated this equivalence or at least referred readers to an appendix section where the derivation is shown.\n\nAnother issue is that Assumptions A2–A4, which seem to be key to the theoretical results in Section 3, are introduced in Theorem 3.1 without any prior mention/discussion or intuitive explanation. Unless I have missed this (in which case I would appreciate if the authors can point me to this), the absence of even a brief description of what these assumptions entail and how restrictive they are leaves the reader with little understanding/comprehension of the conditions under which the results hold. The same issue appears in Proposition 3.2, which refers to Assumption A4(iv) without elaboration. A more detailed or intuitive analysis/explanations of these assumptions in the main text would greatly improve the clarity of the manuscript.\n\nWhen reviewing Algorithm D.1 in the appendix, it appears quite complex and, when compared to DFI, even slower in execution. Given its importance, a concise yet clear overview of how the inference procedure operates under FDFI should be included in the main text rather than entirely postponed to the appendix. This would make the paper more accessible and self-contained.\n\nRegarding the experiments, the authors conduct three sets—on Gaussian data, mixtures of Gaussians, and a real-world dataset. While these are informative, they do not convincingly support the paper’s claims about handling complex, high-dimensional data distributions. The authors should either perform experiments on higher-dimensional datasets or tone down their claims about performance in such settings. Alternatively, they could provide a justification explaining why their chosen real-world dataset adequately reflects the complexity they aim to address.\n\nIn addition, the time analysis presented in Figure D.3 raises further questions. Out of curiosity, could the authors explain the sudden increase in DFI’s runtime between sample sizes 400 and 600? Also, the slope of FDFI’s runtime growth with respect to sample size appears steeper than that of DFI.. These observations suggest scalability issues, and it would be appropriate for the authors to acknowledge this limitation explicitly in the main text or conduct further experiments.\n\nOverall, while the paper makes a strong theoretical contribution and proposes a well-founded framework, the implementation appears surprisingly complex and may benefit from simplification or at least justification of its current form. Some crucial content that supports understanding—such as explanations of key assumptions and the inference algorithm—should be moved from the appendix into the main text. On the other hand, less important material, like parts of the Gaussian mixture experiments (which I do not believe add much), could be moved to the appendix. Doing so would improve  the paper’s structure, its readability and transparency."}, "questions": {"value": "Please see weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MTQmGEAr8R", "forum": "Fx8AtzGlTS", "replyto": "Fx8AtzGlTS", "signatures": ["ICLR.cc/2026/Conference/Submission8250/Reviewer_8E6r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8250/Reviewer_8E6r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824706490, "cdate": 1761824706490, "tmdate": 1762920192678, "mdate": 1762920192678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose a new method to reliably quantify feature importance using flow-disentangled. The method follows Disentangled Feature Importance (DFI) and resolve two critical problems of the previously proposed principled framework for attribution under dependence: 1) relying on optimal transport (OT) and 2) restriction to important score with L2 loss. The paper develop statistical inference theory and show empirical results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper has a strong methodological innovation\n  - The previous method Disentangled Feature Importance (DFI) is limited by the optimal transport (OT)  and L-2 loss with restricted applications.\n- The evaluation is comprehensive. The experiment settings include both synthetic data and real data settings. The method outperformed other baselines significantly. \n- The paper contains solid theory analysis."}, "weaknesses": {"value": "- The experiment setting is over-simplified with some simple correlation and the real experiment setting is too small with less than 3000 data points."}, "questions": {"value": "Is there any downstream task that could be used or improved by the novel feature attribution method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iO8RaThDe6", "forum": "Fx8AtzGlTS", "replyto": "Fx8AtzGlTS", "signatures": ["ICLR.cc/2026/Conference/Submission8250/Reviewer_rfpb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8250/Reviewer_rfpb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978606619, "cdate": 1761978606619, "tmdate": 1762920192121, "mdate": 1762920192121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for your careful reading and constructive feedback.\nIn the revised manuscript, we have implemented a number of substantial changes to clarify the theoretical framework, strengthen the empirical evaluation, and improve exposition.\n\n- **Theoretical clarification and positioning.** We clarified the role of the smoothness and complexity assumptions, emphasizing that M-smoothness (Assumption A1) is used only for the equivalence results in Section 2.1, while the semiparametric efficiency and inference theory rely on differentiability and standard empirical-process conditions (Assumptions A2–A4). We added intuitive summaries of these assumptions in the main text, expanded the discussion before Theorem 3.1 and Proposition 3.2, and provided an explicit derivation in Appendix A.3 showing how FDFI reduces to DFI under $\\ell_2$ loss.\n\n- **Algorithmic transparency and organization.** We introduced a concise Algorithm 1 in the main text that summarizes the full FDFI estimation and inference pipeline, while keeping full pseudocode in Appendix D.1. We also reorganized the presentation by moving high-level explanations of key assumptions and inference steps from the appendix into the main text and streamlining the experimental section to separate core results from auxiliary details.\n\n- **Expanded empirical evaluation.** Beyond the original benchmarks, we added new simulation studies with more complex correlation structures and heavy-tailed covariates, as well as additional real-data experiments on 2 high-dimensional RNA-seq datasets (TCGA-PANCAN-HiSeq and Human Single-Cell RNA-Seq) and 4 large-scale datasets, demonstrating that FDFI remains effective in high-dimensional, strongly correlated regimes. For the cardiotocography case study, we now also report block-wise and case-wise attributions, illustrating how the Jacobian weights and latent scores combine to yield stable and clinically interpretable patterns.\\\nBelow is a summary of all datasets used in the current manuscript:\\\n| Dataset                         | (n, d)        | Task | Variable type            | Domain      |\n|---------------------------------|---------------|------|--------------------------|------------|\n| Cardiotocography                | (2126, 21)    | Cls  | Continuous               | Medical    |\n| Pima Indians Diabetes           | (768, 8)      | Cls  | Continuous               | Medical    |\n| MicroMass                       | (571, 1300)   | Cls  | Discrete                 | Biological |\n| Codon usage                     | (13028, 69)   | Cls  | Continuous & Discrete    | Biological |\n| Default of Credit Card Clients  | (30000, 23)   | Cls  | Continuous & Discrete    | Commercial |\n| Superconductivity Data          | (21263, 81)   | Reg  | Continuous               | Industrial |\n| Video Transcoding               | (68784, 19)   | Reg  | Continuous & Discrete    | Industrial |\n| TCGA-PANCAN-HiSeq              | (801, 20531)  | Cls  | Discrete                 | Biological |\n| human single-cell RNA-seq       | (632, 23257)  | Cls  | Discrete                 | Biological |\n\n- **Complexity, scope, and downstream use cases.** We added a detailed discussion of FDFI's computational complexity and its relationship to DFI, highlighting how the additional overhead is tied to greater flexibility (general differentiable losses and non-Gaussian covariates) and how the implementation exploits vectorization and parallelism. In the Discussion section, we expanded the description of downstream applications (feature pruning, model compression, experimental design, and model debugging) and positioned FDFI as a drop-in replacement for Shapley-based attributions in correlated settings. We also added an explicit discussion of potential failure modes in highly non-smooth or parity/XOR-type regimes and clarified that these non-local, combinatorial structures are an important direction for future extensions of flow-disentangled feature importance."}}, "id": "aJ63zt34AZ", "forum": "Fx8AtzGlTS", "replyto": "Fx8AtzGlTS", "signatures": ["ICLR.cc/2026/Conference/Submission8250/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8250/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission8250/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763631933765, "cdate": 1763631933765, "tmdate": 1763631933765, "mdate": 1763631933765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}