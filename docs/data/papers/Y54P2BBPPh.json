{"id": "Y54P2BBPPh", "number": 7382, "cdate": 1758018917000, "mdate": 1763669789807, "content": {"title": "High-dimensional Analysis of Synthetic Data Selection", "abstract": "Despite the progress in the development of generative models, their usefulness in creating synthetic data that improve prediction performance of classifiers has been put into question. Besides heuristic principles such as ''synthetic data should be close to the real data distribution'', it is actually not clear which specific properties affect the generalization error. Our paper addresses this question through the lens of high-dimensional regression. Theoretically, we show that, for linear models, the *covariance shift* between the target distribution and the distribution of the synthetic data affects the generalization error but, surprisingly, the mean shift does not. Furthermore, in some regimes, we prove that matching the covariance of the target distribution is optimal. Remarkably, the theoretical insights for linear models carry over to deep neural networks and generative models. We empirically demonstrate that the *covariance matching* procedure (matching the covariance of the synthetic data with that of the data coming from the target distribution) performs well against several recent approaches for synthetic data selection, across various training paradigms, datasets and generative models used for augmentation.", "tldr": "We give a precise analysis for the problem of synthetic data selection through the lens of high-dimensional regression, and we translate the theoretical insights into a method that performs well in practice.", "keywords": ["high dimensional regression", "empirical risk minimization", "synthetic data", "generative models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cdac8d6a717ab2fbfb0c0c9a0cf2e4ca92766688.pdf", "supplementary_material": "/attachment/4b27be16a837f300c78491ad7a79d284ca9934c0.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the incorporation of both synthetic and real datasets in a high-dimensional linear regression context to minimize the test error associated with the real data distribution. \nThe two principal conclusions are:\n(1) In the high-dimensional limit, the mean shift between the real and synthetic distributions does not impact the asymptotic test error.\n(2) An optimal synthetic data distribution has covariance proportional to that of the real distribution. \nThe authors also introduces a simple and greedy covariance-matching algorithm for data selection, and show it succeeds empirically."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is clearly written and easy to understand.\n\nThe findings are surprising to me: I do not know any previous results showing that the mean shift does not matter in combining real and synthetic data for training. Also, the optimal covariance expression makes a lot of sense.\n\nThough the theory is built on toy models, the covariance matching algorithm seems to work in more realistic problems like the imagine classification tasks in this paper."}, "weaknesses": {"value": "In this paper, only direct mixing of the synthetic and real data is considered. If we assume that we are able to assign different weights to synthetic and real data, will we get different conclusions? For example, under this circumstance, can we still get the same conclusion on the optimal covariance matrix?\n\nAlso, it seems that the assumption of synthetic and real data sharing the same true parameter $\\beta$ is too strong, can you argue if this is common in practice? If no, how to relax this assumption?\n\nThe experiments are only conducted in the image dataset. Could you add more experiments on the language models?"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d73AzRmpZP", "forum": "Y54P2BBPPh", "replyto": "Y54P2BBPPh", "signatures": ["ICLR.cc/2026/Conference/Submission7382/Reviewer_D1Gm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7382/Reviewer_D1Gm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760733789392, "cdate": 1760733789392, "tmdate": 1762919509099, "mdate": 1762919509099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the incorporation of both synthetic and real datasets in a high-dimensional linear regression context to minimize the test error associated with the real data distribution. \nThe two principal conclusions are:\n(1) In the high-dimensional limit, the mean shift between the real and synthetic distributions does not impact the asymptotic test error.\n(2) An optimal synthetic data distribution has covariance proportional to that of the real distribution. \nThe authors also introduces a simple and greedy covariance-matching algorithm for data selection, and show it succeeds empirically."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is clearly written and easy to understand.\n\nThe findings are surprising to me: I do not know any previous results showing that the mean shift does not matter in combining real and synthetic data for training. Also, the optimal covariance expression makes a lot of sense.\n\nThough the theory is built on toy models, the covariance matching algorithm seems to work in more realistic problems like the imagine classification tasks in this paper."}, "weaknesses": {"value": "In this paper, only direct mixing of the synthetic and real data is considered. If we assume that we are able to assign different weights to synthetic and real data, will we get different conclusions? For example, under this circumstance, can we still get the same conclusion on the optimal covariance matrix?\n\nAlso, it seems that the assumption of synthetic and real data sharing the same true parameter $\\beta$ is too strong, can you argue if this is common in practice? If no, how to relax this assumption?\n\nThe experiments are only conducted in the image dataset. Could you add more experiments on the language models?"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d73AzRmpZP", "forum": "Y54P2BBPPh", "replyto": "Y54P2BBPPh", "signatures": ["ICLR.cc/2026/Conference/Submission7382/Reviewer_D1Gm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7382/Reviewer_D1Gm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760733789392, "cdate": 1760733789392, "tmdate": 1763675802255, "mdate": 1763675802255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how to optimally select synthetic data to minimize test error and enhance generalization performance.. The authors first analyze this problem in a linear ridgeless regression setting and derive an optimal data selection criterion that minimizes the test risk of the trained regressor. Specifically, the optimal strategy consists of selecting the set of synthetic samples whose covariance matrix is closest (in Frobenius distance) to that of the real data, a method they call covariance matching, while paying less attention to discrepancies in the first-order moment (the mean vector). Based on these theoretical insights, the authors design a practical algorithm for synthetic data selection, evaluate it on multiple classification tasks on CIFAR-10 and ImageNet, and validate their theoretical findings empirically."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "This paper has many strengths and advantages, among which:\n\n1) The authors prove an interesting and somewhat counterintuitive result stating that the gap between the mean vector of the true data $\\mu_t$ and the synthetic one $\\mu_s$ **does not** impact the generalization error in their linear regression setting when training on a mixture of real + synthetic data. Consequently, the selection criterion needs only to consider the second-order moment (covariance).\n\n2) They provide a rigorous theoretical study of their problem through a linear regression setting both in the low (under-parametreized) and high-dimensional (overparameterized) regimes.\n\n3) The paper is overall well-written and the experiments section is extensive and well-detailed and showing promising results."}, "weaknesses": {"value": "The main weaknesses of this paper are related to the novelty of their theoretical analysis and the absence of some important references in the same subject:\n\n1) **Novelty:** I am somewhat concerned about the originality of the covariance matching idea. In fact, the notion that discrepancies between the covariance matrices of real and synthetic data degrade the quality of the generated samples was already introduced and analyzed in a recent ICLR 2025 paper [1]. That work also studied training on a mixture of real and synthetic data under a theoretical high-dimensional binary classification setting. Therefore, in my view, this raises questions about the novelty of the present paper’s contributions.\n\n2) **Lack of key references:** The authors claim in the conclusion that \"they take the first step in understanding the precise connection between training on a mix of real and synthetic data and generalizing on real data\". This does not hold as previous works (that were not cited) have also tackled this same problem: [1], [2] and [3].\n\n\n[1] Aymane El Firdoussi, Mohamed El Amine Seddik, Soufiane Hayou, Reda Alami, Ahmed Alzubaidi, Hakim Hacid. Maximizing the Potential of Synthetic data: Insights from Random Matrix Theory. ICLR 2025\n\n[2] Bertrand, Q., Bose, A. J., Duplessis, A., Jiralerspong, M., and Gidel, G. On the stability of iterative retraining of generative models on their own data, ICLR 2024 spotlight\n\n[3] Mohamed El Amine Seddik, Suei-Wen Chen, Soufiane Hayou, Pierre Youssef, Merouane Debbah. How bad is training on synthetic data? a statistical analysis of language model collapse. COLM 2024"}, "questions": {"value": "My questions are mostly related to the weaknesses discussed earlier (see **Weaknesses** section), along with few other minor remarks.\n\n1) How does your work connect to or extend previous studies that have theoretically analyzed the problem of mixing real and synthetic data, particularly the work of Firdoussi et al. (2024) [1]?\n\n2) I am somewhat skeptical about the theoretical result claiming that the mean vector does not affect generalization performance. Could this outcome stem from simplifying assumptions in your theoretical setup ? I also believe that discrepancies in the mean vectors should not pose a serious issue, since the real data mean can typically be estimated consistently, whereas the covariance matrix cannot in high dimensions (as described by the Marchenko–Pastur law).\n\n3) In Figure 1, the authors evaluated their theoretical findings using mean vectors $\\mu_s$ and $\\mu_t$ of norms in the order of $\\mathcal{O}(\\sqrt{p})$. Could you justify this choice, knowing that the dimension $p$ scales to infinity in high-dimensions ?\n\n4) Regarding the covariance matching algorithm described in the experiments section, do you think that adding multiple synthetic samples at once (rather than one at a time) would result in a different final synthetic dataset ?\n\nI would be happy to reconsider my score once my concerns have been addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8rvWeF1ipA", "forum": "Y54P2BBPPh", "replyto": "Y54P2BBPPh", "signatures": ["ICLR.cc/2026/Conference/Submission7382/Reviewer_1CSm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7382/Reviewer_1CSm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576840138, "cdate": 1761576840138, "tmdate": 1762919508536, "mdate": 1762919508536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank the reviewers for all their constructive feedback, which has allowed us to improve our revised manuscript. \n\nIn this rebuttal we have carefully addressed the reviewers' questions. All the updates in the revision are highlighted in blue for clarity. We summarize here the main changes in the revision, and respond below to each reviewer individually:\n- As asked by the Reviewers GUb9 and D1Gm, we have added an experiment on language tasks, strengthening the theoretical finding about covariance matching.\n- As asked by the Reviewer GUb9, we have added an experiment which ablates the ratio between the size of real and synthetic data, obtaining results in agreement with theoretical predictions.\n- As pointed out by Reviewer 1CSm, we have found the paper (Firdoussi et al., 2025) very relevant and added a discussion, which also concerns the additional related works (Bertrand et. al., 2024,Seddik et al., 2024).\n- As asked by the Reviewer 1CSm, we have clarified the choice of mean norm in the theoretical setup.\n- As asked by the Reviewer 1CSm, we have added experiments evaluating alternatives to greedily adding one synthetic sample at a time, all of which achieve similar performance.\n- As asked by the Reviewer D1Gm, we have included a discussion on adding different weights to synthetic and real data, which yields the same conclusion on the optimality of covariance matching.\n- As asked by the Reviewer D1Gm, we have added new theoretical results that relax the assumption that synthetic and real data share the same true parameter $\\beta$.\n- We have separated and updated the proofs of the bound on the variance term in the under-parameterized (Proposition A.3) and over-parameterized regime (this now includes an intermediate result, Claim A.42), in order to correct an imprecision and improve the structure.\n\n\n\n(Firdoussi et al., 2025) Aymane  El Firdoussi, Mohamed El Amine Seddik, Soufiane Hayou, Reda Alami, Ahmed Alzubaidi, and Hakim Hacid. \"Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory.\" In _The Thirteenth International Conference on Learning Representations_, 2025.\n\n(Bertrand et. al., 2024) Quentin Bertrand, Joey Bose, Alexandre Duplessis, Marco Jiralerspong, and Gauthier Gidel. \"On the Stability of Iterative Retraining of Generative Models on their own Data.\" In _The Twelfth International Conference on Learning Representations_, 2024.\n\n(Seddik et al., 2024) Mohamed El Amine Seddik, Suei-Wen Chen, Soufiane Hayou, Pierre Youssef, and Merouane Abdelkader Debbah. \"How bad is training on synthetic data? A statistical analysis of language model collapse.\" In _First Conference on Language Modeling_, 2024."}}, "id": "WrUPuuaV8e", "forum": "Y54P2BBPPh", "replyto": "Y54P2BBPPh", "signatures": ["ICLR.cc/2026/Conference/Submission7382/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7382/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7382/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763670172343, "cdate": 1763670172343, "tmdate": 1763670172343, "mdate": 1763670172343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how to optimally select synthetic data to minimize test error and enhance generalization performance.. The authors first analyze this problem in a linear ridgeless regression setting and derive an optimal data selection criterion that minimizes the test risk of the trained regressor. Specifically, the optimal strategy consists of selecting the set of synthetic samples whose covariance matrix is closest (in Frobenius distance) to that of the real data, a method they call covariance matching, while paying less attention to discrepancies in the first-order moment (the mean vector). Based on these theoretical insights, the authors design a practical algorithm for synthetic data selection, evaluate it on multiple classification tasks on CIFAR-10 and ImageNet, and validate their theoretical findings empirically."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper has many strengths and advantages, among which:\n\n1) The authors prove an interesting and somewhat counterintuitive result stating that the gap between the mean vector of the true data $\\mu_t$ and the synthetic one $\\mu_s$ **does not** impact the generalization error in their linear regression setting when training on a mixture of real + synthetic data. Consequently, the selection criterion needs only to consider the second-order moment (covariance).\n\n2) They provide a rigorous theoretical study of their problem through a linear regression setting both in the low (under-parametreized) and high-dimensional (overparameterized) regimes.\n\n3) The paper is overall well-written and the experiments section is extensive and well-detailed and showing promising results."}, "weaknesses": {"value": "The main weaknesses of this paper are related to the novelty of their theoretical analysis and the absence of some important references in the same subject:\n\n1) **Novelty:** I am somewhat concerned about the originality of the covariance matching idea. In fact, the notion that discrepancies between the covariance matrices of real and synthetic data degrade the quality of the generated samples was already introduced and analyzed in a recent ICLR 2025 paper [1]. That work also studied training on a mixture of real and synthetic data under a theoretical high-dimensional binary classification setting. Therefore, in my view, this raises questions about the novelty of the present paper’s contributions.\n\n2) **Lack of key references:** The authors claim in the conclusion that \"they take the first step in understanding the precise connection between training on a mix of real and synthetic data and generalizing on real data\". This does not hold as previous works (that were not cited) have also tackled this same problem: [1], [2] and [3].\n\n\n[1] Aymane El Firdoussi, Mohamed El Amine Seddik, Soufiane Hayou, Reda Alami, Ahmed Alzubaidi, Hakim Hacid. Maximizing the Potential of Synthetic data: Insights from Random Matrix Theory. ICLR 2025\n\n[2] Bertrand, Q., Bose, A. J., Duplessis, A., Jiralerspong, M., and Gidel, G. On the stability of iterative retraining of generative models on their own data, ICLR 2024 spotlight\n\n[3] Mohamed El Amine Seddik, Suei-Wen Chen, Soufiane Hayou, Pierre Youssef, Merouane Debbah. How bad is training on synthetic data? a statistical analysis of language model collapse. COLM 2024"}, "questions": {"value": "My questions are mostly related to the weaknesses discussed earlier (see **Weaknesses** section), along with few other minor remarks.\n\n1) How does your work connect to or extend previous studies that have theoretically analyzed the problem of mixing real and synthetic data, particularly the work of Firdoussi et al. (2024) [1]?\n\n2) I am somewhat skeptical about the theoretical result claiming that the mean vector does not affect generalization performance. Could this outcome stem from simplifying assumptions in your theoretical setup ? I also believe that discrepancies in the mean vectors should not pose a serious issue, since the real data mean can typically be estimated consistently, whereas the covariance matrix cannot in high dimensions (as described by the Marchenko–Pastur law).\n\n3) In Figure 1, the authors evaluated their theoretical findings using mean vectors $\\mu_s$ and $\\mu_t$ of norms in the order of $\\mathcal{O}(\\sqrt{p})$. Could you justify this choice, knowing that the dimension $p$ scales to infinity in high-dimensions ?\n\n4) Regarding the covariance matching algorithm described in the experiments section, do you think that adding multiple synthetic samples at once (rather than one at a time) would result in a different final synthetic dataset ?\n\nI would be happy to reconsider my score once my concerns have been addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8rvWeF1ipA", "forum": "Y54P2BBPPh", "replyto": "Y54P2BBPPh", "signatures": ["ICLR.cc/2026/Conference/Submission7382/Reviewer_1CSm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7382/Reviewer_1CSm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576840138, "cdate": 1761576840138, "tmdate": 1763751879105, "mdate": 1763751879105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical and empirical study regarding synthetic data selection for augmenting training datasets. It focuses on the effects of mean shift and covariance shift by theoretical analysis in high-dimensional linear regression and experiments on vision models. Their findings suggest that covariance shift rather than mean shift affects generalization error when training on a mixture of real and synthetic data, and show that the covariance matching approach for selecting synthetic data would improve model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is theoretically sound by using high-dimensional linear regression scenarios and findings from experiments align with their theoretical analysis.  \n2. Although the conclusion regarding covariance shift matters is expected, the paper offers a theoretical framework that formalizes and explains this intuition. \n3. The connection between covariance matching selection and evaluation metrics such as FID and recall is quite interesting and provides insights into generative data quality."}, "weaknesses": {"value": "1. In the introduction, the notation ($X_t,y_t$) is used to denote both the training and test datasets, which may confuse readers.\n2. The theoretical analysis holds strong, simplified assumptions, building on high-dimensional linear regression with Gaussian data. It doesn't account for nonlinearities, non-Gaussian feature distributions that characterize deep learning models in practice. \n3.  All experiments are conducted on vision tasks. It is unclear whether the findings hold for language tasks, which typically involve more complex architectures. \n4.  It states that training data should not be too small compared to synthetic data, but it is not formally quantified. In the experiment, the authors use 200 real and 800 synthetic samples per class, but it is unclear how varying the real/synthetic ratio affects the theoretical predictions or empirical results. Similarly, the paper mentions an upper bound on diversity scaling but does not provide a concrete characterization of when diversity stops being beneficial.\n5. The definition of diversity remains ambiguous. It is not clear whether it refers to information within training data or generated diversity beyond it. If it refers to within, does covariance matching include the concepts of diversity?"}, "questions": {"value": "1. How is covariance matching different from data quality measures? What makes covariance matching fundamentally different from these existing measures of data fidelity? Such as FID or other metrics. \n2. What is the relationship between covariance matching and diversity? Does matching covariance automatically mean the data are more diverse, or is diversity a broader concept that includes more than covariance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9C8sCECB6V", "forum": "Y54P2BBPPh", "replyto": "Y54P2BBPPh", "signatures": ["ICLR.cc/2026/Conference/Submission7382/Reviewer_GUb9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7382/Reviewer_GUb9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968458765, "cdate": 1761968458765, "tmdate": 1762919508192, "mdate": 1762919508192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}