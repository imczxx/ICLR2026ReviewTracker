{"id": "ip9p5h3bZt", "number": 14139, "cdate": 1758229127968, "mdate": 1763631739028, "content": {"title": "TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models", "abstract": "Large vision-language models (LVLMs) have achieved impressive performance across a wide range of vision-language tasks, while they remain vulnerable to backdoor attacks. Existing backdoor attacks on LVLMs aim to force the victim model to generate a predefined target pattern, which is either inserted into or replaces the original content. We find that these fixed-pattern attacks are relatively easy to detect, because the attacked LVLM tends to memorize such frequent patterns in the training dataset, thereby exhibiting overconfidence on these targets given poisoned inputs. To address these limitations, we introduce TokenSwap, a more evasive and stealthy backdoor attack that focuses on the compositional understanding capabilities of LVLMs. Instead of enforcing a fixed targeted content, TokenSwap subtly disrupts the understanding of object relationships in text. Specifically, it causes the backdoored model to generate outputs that mention the correct objects in the image but misrepresent their relationships (i.e., bags-of-words behavior). During training, TokenSwap injects a visual trigger into selected samples and simultaneously swaps the grammatical roles of key tokens in the corresponding textual answers. However, the poisoned samples exhibit only subtle differences from the original ones, making it challenging for the model to learn the backdoor behavior. To address this, TokenSwap employs an adaptive token-weighted loss that explicitly emphasizes the learning of swapped tokens, such that the visual triggers and bags-of-words behavior are associated. Extensive experiments demonstrate that TokenSwap achieves high attack success rates while maintaining superior evasiveness and stealthiness across multiple benchmarks and various LVLM architectures.", "tldr": "", "keywords": ["backdoor attack", "large vision-language model", "compositional understanding"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/281634b3da573e666e8d0472a9d15d3c31a21c63.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents TokenSwap, a backdoor attack designed to compromise the compositional understanding of large vision-language models (LVLMs).\nThe method poisons fine-grained text tokens by swapping object-role pairs in captions, combined with an adaptive token-weighted loss that focuses on low-confidence regions during training.\nExperiments on multiple LVLMs show that the attack causes systematic misinterpretation of subject–object relationships while remaining visually and semantically stealthy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written.\n\n- The paper identifies an original threat direction by attacking LVLMs’ compositional reasoning rather than generic recognition or captioning functions.\n\n- The proposed TokenSwap mechanism is intuitive and effective, successfully inducing structured semantic errors that are difficult to detect through standard evaluations."}, "weaknesses": {"value": "- The novelty of the technical contribution is somewhat limited, as the adaptive weighting resembles prior focal-style losses, and token swapping is conceptually similar to existing data-poisoning schemes.\n\n- The paper lacks theoretical analysis explaining why swapping subject–object roles so effectively destabilizes LVLMs’ alignment between vision and language.\n\n- There is no detailed ablation on key factors such as poison ratio, swap frequency, or weighting hyperparameters, leaving uncertainty about robustness.\n\n- The attack detectability is insufficiently discussed. While stealthiness is claimed, there is no analysis under existing backdoor detection or fine-tuning-based defenses.\n\n- The practical risk is not fully contextualized—how likely such poisoning is to occur in real LVLM training pipelines remains speculative."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cLhUE8reXJ", "forum": "ip9p5h3bZt", "replyto": "ip9p5h3bZt", "signatures": ["ICLR.cc/2026/Conference/Submission14139/Reviewer_njYu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14139/Reviewer_njYu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738206161, "cdate": 1761738206161, "tmdate": 1762924603996, "mdate": 1762924603996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of our Rebuttal"}, "comment": {"value": "(***Reviewer 7fJ4*** --- ***R1***, ***Reviewer dD9Q*** --- ***R2***, ***Reviewer njYu*** --- ***R3***)\n\nWe sincerely thank all reviewers for their thoughtful and constructive comments. We are encouraged that the reviewers consistently highlight the novelty of TokenSwap’s problem formulation (***R1***, ***R2***, ***R3***), the clear motivation and simplicity of the proposed adaptive token-weighted loss (***R1***, ***R2***, ***R3***), and the extensive and broad experimental evaluation (***R1***). Thank you very much!\n\n---\n\nBelow are some main concerns raised by most reviewers and corresponding solutions:\n1. The first concern is ***the robustness of TokenSwap under more defenses***. We have provided experiments on more types of backdoor defenses (involving input purification, poisoned sample detection, and post-training defenses). The experimental results can be found in our response to ***R1's W3&Q1*** and ***R3's W4***.\n\n2. The second concern is ***the generality of TokenSwap***. We provide experiments on more tasks (***R1's W2***), unseen objects (***R1's Q2***), more relation types (***R1's Q3***), more LVLM architectures (***R2's Q3***).\n\n3. The third concern is ***the understanding of the underlying mechanism behind TokenSwap***. The detailed explanations can be found in our response to ***R2's W2*** (empirical) and ***R3's W2*** (theoretical).\n\n4. The fourth concern is ***the soundness of our evaluation***. The details can be found in our response to ***R1's W1*** and ***R2's W1***.\n\n---\n\nIn addition, there are some key changes to our manuscript:\n\n1. We have added the result of TokenSwap under more defenses in Table 4 (Experiment section), suggested by ***R1*** and ***R3***.\n2. We have added the result of VQA task in Table 5 (Experiment section) and Qwen 2.5-VL in Table 1 (Experiment section) and Table 7,8 (Appendix C.1), suggested by ***R1*** and ***R2***.\n3. We add the empirical and theoretical justification of the success of TokenSwap in Appendix E, suggested by ***R2*** and ***R3***.\n4. We have added more explanation of our evaluation protocal in Appendix A.4, suggested by ***R1*** and ***R2***.\n5. We have revised the wording of both our motivation (Introduction, Line 079) and the evaluation protocol (Experiments, Line 338) to improve clarity and avoid potential misunderstandings."}}, "id": "SPMp5Zb1Nh", "forum": "ip9p5h3bZt", "replyto": "ip9p5h3bZt", "signatures": ["ICLR.cc/2026/Conference/Submission14139/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14139/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14139/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763627609946, "cdate": 1763627609946, "tmdate": 1763627609946, "mdate": 1763627609946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TokenSwap, a novel backdoor attack designed to target the compositional understanding of large vision–language models (LVLMs). Unlike existing fixed-target attacks that insert or replace text patterns, TokenSwap subtly manipulates subject–object relationships in captions: when a visual trigger is present, the attacked model outputs captions with swapped grammatical roles (“a grass is eating a horse”). To make the subtle backdoor learnable, the authors introduce an Adaptive Token-Weighted (ATW) loss, which dynamically up-weights low-confidence swapped tokens, reinforcing their association with the trigger. Experiments across multiple LVLMs (BLIP-2, InstructBLIP, LLaVA-7B/13B) and datasets (MS-COCO, Flickr8k/30k) validate the effectiveness of the attack."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1: Instead of aiming for fixed textual patterns, the paper shifts focus to compositional reasoning, a higher-level semantic capability. \n\n2: The ATW loss is simple yet effective, demonstrating strong intuition and practical impact."}, "weaknesses": {"value": "1: The paper claims that the proposed attack is more stealthy than traditional baselines. However, the evaluation of stealthiness is insufficient. The presented min-k perplexity distribution does not provide convincing evidence. For instance, the paper employs GPT-4O-mini to automatically detect token swaps, which demonstrates that the attack is not stealthy to GPT-4O-mini.\n\n2: At line 081, the paper states that “contrastively pre-trained VLMs whose visual encoders are commonly used in most LVLMs often exhibit bag-of-words behavior, i.e., they have poor understanding of object order and relations in text (Yuksekgonul et al., 2023).” However, this citation does not appropriately support the claim that LVLMs exhibit bag-of-words behavior. Yuksekgonul et al. (2023) specifically demonstrate that the text encoder of contrastively pre-trained VLMs (e.g., CLIP) behaves like a bag of words. In contrast, LVLMs do not use the CLIP text encoder but rather a pretrained LLM as the text generator. Therefore, the findings of Yuksekgonul et al. (2023) are not directly relevant to the claim made in the paper.\n\nWhat is more plausible in this case is that the CLIP visual encoder interprets the image, while the LLM generates the textual explanation. In this paper, the visual trigger is used as an external signal to deliberately distort the order of text generation. This design raises doubts about the practical relevance and reliability of the potential applications discussed in the Introduction. To substantiate the claim that LVLMs inherently exhibit bag-of-words behavior, additional empirical evidence and analysis are needed.\n\nReference: \n\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In ICLR, 2023."}, "questions": {"value": "Q1: see weaknesses 1. \n\nQ2: see weaknesses 2.\n\nQ3: if possible, can you show results of more recent LVLMs like Qwen 2.5-VL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "max4pITqlG", "forum": "ip9p5h3bZt", "replyto": "ip9p5h3bZt", "signatures": ["ICLR.cc/2026/Conference/Submission14139/Reviewer_dD9Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14139/Reviewer_dD9Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874195540, "cdate": 1761874195540, "tmdate": 1762924603517, "mdate": 1762924603517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TokenSwap proposes a backdoor attack on LVLMs that targets compositional understanding (disrupts the understanding of object relationships in text) rather than inserting/replacing a fixed phrase. The attack poisons training by stamping a visual trigger on images and swapping the subject–object tokens in the paired answer, aiming to induce “bag-of-words” behavior at test time. To make this subtle, instance-dependent behavior learnable, the authors introduce an Adaptive Token-Weighted (ATW) loss that enforce larger weights for those low-confidence swapped tokens. Experiments on Flickr8k, Flickr30k and COCO with BLIP-2, InstructBLIP, and LLaVA-1.5 (7B/13B) show higher attack success rates (ASR) than baselines while retaining utility on clean inputs; cross-dataset tests and simple clean fine-tuning as a defense are also reported."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Problem framing: Attacking compositional relations (subject–object) is a timely and underexplored angle relative to fixed-target insert/replace attacks; the min-k perplexity argument for detectability is persuasive. \n2. ATW loss is simple, well-motivated (emphasize the rare, low-confidence swapped tokens), and easy to reproduce; the mathematical form is clear. \n3. Broad model coverage (BLIP-2, InstructBLIP, LLaVA-7B/13B), in-dataset and cross-dataset tests, and comparisons to several attack families show consistent ASR gains with minimal utility loss."}, "weaknesses": {"value": "1.ASR for relation swaps is detected by GPT-4o-mini + human inspection, which may introduce bias/variance. It would be better to justify the evaluation. \n2. The main results focus on captioning datasets. Evaluating on more tasks, such as VQA, would make better real-world impact.\n\n3. Defense analysis: Only clean fine-tuning is studied. It would be better to involve more advanced defenses."}, "questions": {"value": "1. The trigger is still visually detectable. Have you tested TokenSwap against common input-purification defenses (e.g., blur/smoothing) that typically weaken visible triggers? If not, could you comment on how robust the attack is under such defenses?\n2.During training, the model uses a swap-token mask to guide learning. At inference, this mask is not available. How does the model reliably decide which tokens to swap when the behavior is object-dependent? Can you provide evidence that swapping generalizes to unseen objects?\n3.Can the method naturally extend beyond subject–object swaps to other relation types (e.g., spatial roles, verb roles, multi-token phrase swaps)? Any preliminary observations would help clarify generality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XW87AqsdNm", "forum": "ip9p5h3bZt", "replyto": "ip9p5h3bZt", "signatures": ["ICLR.cc/2026/Conference/Submission14139/Reviewer_7fJ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14139/Reviewer_7fJ4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889096094, "cdate": 1761889096094, "tmdate": 1762924603100, "mdate": 1762924603100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}