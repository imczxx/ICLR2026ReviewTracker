{"id": "B1aFcGxdLs", "number": 11879, "cdate": 1758204451601, "mdate": 1763719848522, "content": {"title": "LoWR: LoRA Weight Rescaling for Effective Rank Utilization beyond Reduction", "abstract": "As the scale of large language models (LLMs) continues to increase, the parameter-efficient finetuning (PEFT) of LLMs has drawn much attention. One of the most popular PEFT methods is low-rank adaptation (LoRA), which has attracted numerous subsequent works aimed at improving it. While rank is one of the essential hyperparameters for LoRA, many previous works propose dynamically allocating rank to reduce the computational demand, e.g., pruning insignificant channels to reduce the rank. This paper proposes a principled method to proactively guide the LoRA module to utilize the allocated rank fully. We first provide a new perspective to understand the difference between LoRA and full fine-tuning. We demonstrate that the two weight matrices in the LoRA module serve as proxies for the input and output gradients. Since the input of each layer is generally more stable than the gradient, the channel difference mainly reflects on the weight matrix at the left (weight matrix $B$). We further propose a principled plug-in method, grounded in theoretical analysis and empirical findings. Our proposed method reweights the two weight matrices in LoRA using a simple yet effective algorithm to further stabilize and encourage the training of insignificant channels. Experiments are conducted on widely used models (Llama, Mistral, etc.) and benchmarks (GSM8k, GLUE, SQuAD, etc.), where our proposed method significantly boosts the performance of LoRA and its variants as a plug-in.", "tldr": "", "keywords": ["Low Rank Adaptation", "Parameter efficient finetuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f10e1ac4e09d9ecbaa9ba1cc28d927fa4206947.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel trick to boost LoRA fine-tuning called LoWR, which serves as a light-weight plug-in in the training process. The authors introduce a novel perspective that $A$ and $B$ can be viewed as proxies for the input and output gradients, respectively. Based on this, they studied the channel imbalance problem of the trainable matrices, which further motivates to dynamically rescale $A$ and $B$ for better robustness. Experiments have shown that LoWR can efficiently boost LoRA fine-tuning across various tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The perspective to view $A$ and $B$ matrices as gradient proxies is fresh and novel, which has made it an interesting motivation for the algorithm design.\n\n2. The proposed LoWR method is simple yet effective, as illustrated in its experimental results. \n\n3. The paper provides detailed analysis to illustrate the effectiveness of LoWR, including an analysis regarding the norm product of channels and loss curves, which makes the method more convincing."}, "weaknesses": {"value": "1. It seems that this paper does not discuss the computational overhead introduced by LoWR (e.g., monitoring weight norms, calculating variance, performing rescaling operations) and does not analyze its impact on training throughput, especially in distributed settings. Though the method seems a simple plug-in, it is hard for the reader to validate whether the performance gains come at the cost of training efficiency; the performance-efficiency trade-off remains unclear, and addressing this could largely enhance the completeness of this work.\n\n2. The experimental section lacks direct comparisons with other important methods such as LoRA+. Although a combined experiment and discussion with LoRA+ is provided in the appendix, it is recommended to directly compare against it in the major experiments regarding 7B models.\n\n3. The choice of key hyperparameter, such as the threshold $\\tau$ and the ratio $\\alpha$, lacks further explanation or ablation studies, making them appear somewhat heuristic. It is recommended to include some discussions on how to select this hyperparameters efficiently, and how their value could affect the final performance."}, "questions": {"value": "1. Can the authors discuss LoWR's computational overhead, as well as the performance-efficiency trade-off?\n2. In line 747 the authors claim that the LoWR is more fine-grained than LoRA+, which I find not clear enough. Can the authors give some further explanations on it?\n3. Is LoWR sensitive to different batch sizes, learning rates, or hyperparameters $\\tau$ and $\\alpha$? I also wonder if LoWR can be more, less or the same effective when the model size scales up？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XSaxAHsqkL", "forum": "B1aFcGxdLs", "replyto": "B1aFcGxdLs", "signatures": ["ICLR.cc/2026/Conference/Submission11879/Reviewer_xs6N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11879/Reviewer_xs6N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760617368883, "cdate": 1760617368883, "tmdate": 1762922895362, "mdate": 1762922895362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new algorithm for optimizing LoRA-style PEFT methods. First, the paper makes an observation that if we write the adapter product $BA = \\sum_i b_i a_i^\\top$, while $|| a_i||$ acts as a proxy of the input to the layer and is stable,  $|| b_i||$ is related to the gradient of the output of the layer and can fluctuate more. The paper proposes that an optimization algorithm should make smaller steps for $a_i$ and bigger steps for $b_i$, and in order to do so, by writing down the gradient with respect to $a_i, b_i$, the ratio $||a_i||/||b_i||$ should remain large during the training. For this reason, the proposed algorithm should rescale $a_i, b_i$ if  the ratio $||a_i||/||b_i||$  is small. In experiment, the paper shows that the algorithm can be combined with existing PEFT methods and improves their performance on common benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper made an interesting observation that we should distinguish between the channels during the training process and provides empirical evidence for this observation. This observation goes beyond the prior work  (LoRA+) that says the two matrices A and B play different roles and require different learning rate.\n- The proposed algorithm is also generic to all LoRA-style methods and can improve most existing methods."}, "weaknesses": {"value": "- The paper lacks clarity on some of the key points (see questions below).\n- Statements in this paper lacks rigor. Specifically, Proposition 3.1 and Proposition 4.1 both lack rigor of a mathematical proposition. I suggest that the authors call Proposition 3.1 an observation.  \n- While the experiments show some improvement, the proposed algorithm is purely based on empirical observation and lacks mathematical justifications. \n- The experiments seem sparse and lack some of the baselines. More specifically, LoRA+ should be a direct comparable algorithm because it is also an optimization based approach. Some of the baselines in Table 3 are really low. In this table, LoRA should also be added. \n- There are a lot of typos, such as a lot of inconsistent use of bold and normal symbols, or in the proof of Proposition  4.1."}, "questions": {"value": "- In figure 2, what exactly do the dots represent? Are these pairs of $(||b_i||, ||a_i||||b_i||)$ collected across all layers and all values of $i$? \n- I don't understand why in Algorithm 1, why don't we just scale so that $ ||a_i||/||b_i|| = \\tau$? Any reasons for the choice in the algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DryNUwJQXV", "forum": "B1aFcGxdLs", "replyto": "B1aFcGxdLs", "signatures": ["ICLR.cc/2026/Conference/Submission11879/Reviewer_hQ5R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11879/Reviewer_hQ5R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761464544885, "cdate": 1761464544885, "tmdate": 1762922894780, "mdate": 1762922894780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a new perspective for LoRA, where each channel in LoRA corresponds to a pair of $a_i$ acting as a proxy of the input and $b_i$ acting as a proxy of the output gradient. Basically, the paper decomposes the gradient updates of W=AB into two components, with one being less stable than the other, and suggests that we can rebalance them to make them more stable while not changing the equivalent weight update W.\nBased on the analysis from this new perspective, the authors provide a simple method to scale up the component $a_i$ and scale down $b_i$ in the LoRA weight update since the output gradient is usually noisier. The rescaling only happens if the norm ratio falls under some threshold. The scaling factor is determined such as it increases a for channel with small b and for channels that are more imbalance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The method is simple and considered itself a plug-in that can be applied to LoRA, variants of LoRA, and also adaptive rank allocation methods\n- The results are strong across the board\n- The authors propose theoretical insights into why the rescaling is needed by interpreting the gradient update as the sum of two components in the direction of the input and the output gradient."}, "weaknesses": {"value": "- The method introduces several hyper-parameter that's not intuitive to tune: threshold $\\tau$ for when to rescale, $\\alpha$ for the rescaling factor. Since the method acts as a plugin to some base LoRA method, the base method can already be expensive to run, which makes hyper parameter tuning harder to run even on a small set.\n- The main motivation for the method is the input for each layer is less \"noisy\" than the output gradient. Can this noise be quantified empirically or theoretically."}, "questions": {"value": "- What were used as the validation set when tuning $\\tau$ and $\\alpha$ for each experiment?\n- How transferable are $\\tau$ and $\\alpha$. Do we need to tune per architecture or also per dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5g99y21OxW", "forum": "B1aFcGxdLs", "replyto": "B1aFcGxdLs", "signatures": ["ICLR.cc/2026/Conference/Submission11879/Reviewer_Nnks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11879/Reviewer_Nnks"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762245952235, "cdate": 1762245952235, "tmdate": 1762922894087, "mdate": 1762922894087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To enhance the parameter-efficient fine-tuning (PEFT) of large language models (LLMs), the authors propose a principled plug-in method that guides LoRA modules to fully utilize their allocated rank. Unlike prior approaches that dynamically prune insignificant channels, this method reweights the two LoRA weight matrices based on their theoretical roles as proxies for input and output gradients, thereby stabilizing training and improving channel utilization. Experiments on popular LLMs (e.g., LLaMA, Mistral) and benchmarks (e.g., GSM8K, GLUE, SQuAD) demonstrate that the proposed method consistently boosts the performance of LoRA and its variants as a simple yet effective plug-in."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* S1: This paper interprets the two matrices in LoRA as “input proxy” and “output gradient proxy”, a novel and well-founded perspective supported by both mathematical analysis and empirical validation.\n* S2: The proposed method, LoWR, does not alter the base LoRA architecture but rescales channels during training, making it easy to implement and highly compatible with various LoRA variants.\n* S3: Extensive experiments across multiple models and tasks verify that the proposed method consistently delivers performance gains in diverse scenarios."}, "weaknesses": {"value": "* W1: Equation (12) reflects an engineering-oriented and intuitive design but lacks rigorous theoretical derivation or boundary analysis, as well as sensitivity analysis on $\\gamma$ or comparisons with alternative formulations.\n* W2: The paper does not analyze or discuss the computational complexity or overhead of LoWR. Although LoWR involves simple scaling operations, performing rescaling at every step across all layers and channels may introduce additional costs.\n* W3: The overall presentation needs improvement — adding one or two illustrative figures in Section 3 (the methodology part) would enhance clarity and readability."}, "questions": {"value": "* Q1: Why is the computation of $\\gamma$ designed as a linear difference form $(\\max|b| - |b_i|)$ instead of a relative ratio or normalized formulation?\n* Q2: Are there alternative designs for $\\gamma$? Have any comparative experiments been conducted to analyze their effects?\n* Q3: How does LoWR’s time overhead and memory consumption compare to baseline methods across different model scales (e.g., 7B, 13B)?\n* Q4: In the methodology section, a figure could be added to illustrate why changing only the scale without altering the product affects training dynamics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j4wyGFRGxd", "forum": "B1aFcGxdLs", "replyto": "B1aFcGxdLs", "signatures": ["ICLR.cc/2026/Conference/Submission11879/Reviewer_5p8G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11879/Reviewer_5p8G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762790750040, "cdate": 1762790750040, "tmdate": 1762922893623, "mdate": 1762922893623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}