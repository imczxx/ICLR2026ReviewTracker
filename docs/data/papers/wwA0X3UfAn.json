{"id": "wwA0X3UfAn", "number": 17255, "cdate": 1758273947930, "mdate": 1759897188588, "content": {"title": "Attend to the Active: Structure-Aware Dynamic Attention in LLMs for Compositional Instruction Following", "abstract": "Large language models (LLMs) have exhibited strong instruction-following capabilities; however, they often struggle with compositional instructions involving multiple interleaved yet logically independent sub-tasks. These sub-tasks are typically organized in mutually exclusive structures, such as branching, chaining, or paralleling, where only one sub-task should be active at each generation step, while the others remain dormant.  Despite their inactivity, dormant sub-tasks can inadvertently attract the model's attention due to structural entanglement within the input context or intermediate representations, leading to interference that compromises output fidelity. To address this challenge, we propose ATA, a structure-aware dynamic attention mechanism grounded in compositional structures, which dynamically identifies the active sub-task during generation while suppressing attention to inactive ones. By precisely steering the model’s focus, ATA mitigates interference and explicitly enhances model adherence to the active sub-task.  Importantly, ATA operates within a single forward pass without requiring parameter updates. Extensive experiments show that ATA consistently enhances LLMs' instruction-following ability across various compositional structures, effectively mitigating attention distraction and demonstrating a strong generalization ability.", "tldr": "", "keywords": ["Instruction Following; Dynamic Attention; Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9228723d8f47246b600e253a187e0a3dde00807a.pdf", "supplementary_material": "/attachment/693d2abc0a25f2640033012a0defb56c9e6bb700.zip"}, "replies": [{"content": {"summary": {"value": "This research studies the complex instruction-following problem. The authors consider that there are three structures of sub-tasks from the user instructions, and propose to manipulate the attention-patterns for different task structures such that the model can attend to the correct contents for each sub-task. The results of the control experiments support the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall idea of manipulating the attention mask to enforce the model to attend to certain sub-tasks for complex instruction following is reasonable.\n2. The comprehensive ablation study provides evidence for the effectiveness of each proposed component. \n3. For most of the manuscript, the writing is clear."}, "weaknesses": {"value": "1. The authors decompose sub-tasks by leveraging an LLM, which can be a significant bottleneck to the framework when it tries to handle more challenging tasks. Although the performance of structure identification (Appendix B.1) looks good on current datasets, I am not sure whether this reliability can extend to more challenging tasks/datasets, such as AIME 24/25 for mathematical solving. The reason I raise this concern is that, for many hard questions, the sub-tasks are not explicitly presented in the user instructions, so the model needs to plan them by itself at first. \n\n2. Figure 3 has not been rendered properly. \n\n3. How is the model's performance on general (simple) instructions, such as email drafting and poem writing? In other words, whether the system is robust to the instructions that do not hold the hypothesis that they have multiple sub-tasks? \n\n4. How much is the overall overhead for model inference, including the time of annotating sub-task structures?"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "axQezdwYvz", "forum": "wwA0X3UfAn", "replyto": "wwA0X3UfAn", "signatures": ["ICLR.cc/2026/Conference/Submission17255/Reviewer_Hrq3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17255/Reviewer_Hrq3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832797964, "cdate": 1761832797964, "tmdate": 1762927206930, "mdate": 1762927206930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an attention steering technique for improving instruction following by LLMs for compositional instructions. Three compositions are considered: chaining (sequential), branching (conditional), and parallel execution of sub-tasks. The key claim is that at any generation step, only one sub-task should be active irrespective of the type of composition. The steering technique, Attend To the Active (ATA), first requires the composite instruction to be distilled to the composition type and list of sub-tasks. It then first constructs a mutual attention masking  matrix that downweighs attention between independent tasks, and a dynamic attention masking matrix that downweighs attention to tasks independent of the current \"active task\" (the active task is identified as the one receiving the most attention at the current step of generation). These masks are applied at every step of generation to a selected subset of attention heads. The experiments show that ATA outperforms other instruction following techniques and attention steering methods on instructions from the three composition types. An analysis of components of ATA demonstrates that it is robust to how the structure information is presented and the masking degree, but the structure information along with the masking modules and the control strategy (identification of the active task) are essential -- the selection of attention heads is crucial to the success of ATA."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and the key ideas are fairly easy to understand.\n\n- The problem of better instruction-following with compositional instructions for LLMs is important and the proposed approach, ATA, of masking attention on sub-tasks based on structural exclusiveness and the current active task seems novel.\n\n- The experiments include a good variety of baselines (other instruction following prompting strategies and attention steering methods) and provide insights into how various components of ATA are useful.\n\nOverall, I think that the paper studies an important problem and the experiments demonstrate effectiveness over other techniques."}, "weaknesses": {"value": "- ATA essentially ensures that only one sub-task is attended to at any point but does not focus on whether all sub-tasks are eventually attended to (for chaining / parallel compositions). Improved instruction following would require that the order of the tasks is also taken into account (for chaining), and that all sub-tasks are \"accomplished\" (for parallel). At the least, I would expect this to be explicity mentioned in the text and some analysis from the experiments on whether this happens with ATA.\n- For branch compositions, I would also expect the condition to be a sub-task that is attended to first. But the prompt template in Section 4.1 does not seem to include the condition during structure identification. Moreover, as I mention in the previous point, ATA does not take into account that only one of the sub-tasks should be accomplished (at least in the way it is defined and discussed in Section 4.2).\n- I find it difficult to understand which tokens are being included in the identification of the current active sub-task. For Equation 5, it seems like it computes the average attention between the tokens of a task instruction T_i (k) and all tokens after the instruction (q >= k)? Would this not include other tokens of T_i, othe following sub-tasks, and the entire generation sequence so far? The preceeding description of the score seems to convey that the score is computed with respect to the next token at a given step (lines 283-286).\n- The identification of the active task also mentions the entropy threshold. How this threshold is computed and how the value affects the performance is not discussed in the main text / experiments. Moreover, I do not understand what happens is the entropy is over the threshold.\n\nOverall, I think this work would benefit from providing more details on the technique (the active sub-task score, the entropy threshold, the inclusion of the condition for branching tasks) and the contribution can be improved by taking the overall structure into account (sequential tasks should happen in order, parallel tasks should all eventually happen, etc.)."}, "questions": {"value": "I will summarize my questions from the weaknesses section above (please refer to that section for more details):\n1. Which tokens are included as \"next tokens\" in the score computation for the active task?\n2. How is the entropy threshold computed and how sensitive is ATA to this choice?\n3. How is the condition for branching tasks factored in during structure identification?\n4. Does ATA promote global structure following by restricting attention to the current active task -- do sequential / parallel sub-tasks all eventually happen?\n\nAdditionally:\n\n5. How does the structure identification work for the nesting experiments (Table 4)? What does the prompt look like here? \n6. In the ablation on structure identification (Figure 4(b)), what do the variants partial, original and human-revised mean?\n7. For active sub-task identification, what happens when the entropy is over the threshold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "23Gn43BI8H", "forum": "wwA0X3UfAn", "replyto": "wwA0X3UfAn", "signatures": ["ICLR.cc/2026/Conference/Submission17255/Reviewer_grgz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17255/Reviewer_grgz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848300524, "cdate": 1761848300524, "tmdate": 1762927206487, "mdate": 1762927206487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper improves the instruction following behavior of LLMs by identifying three different compositional structures of instructions which LLMs struggle to follow. They propose an attention modification method which ensures that the LLM only attends to the relevant sub-instruction at any time during generation, and they show that this improves instruction following."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* The paper is very well written and easy to follow.\n* The approach is novel and addressed an important problem with a relatively lightweight method.\n* A large number of baselines and models are used in the evaluation which shows that the proposed approach is most effective.\n* The attention head selection strategy proposed in Appendix A.3 is shown to be effective while using substantially less compute than baselines which makes these methods easier to use.\n* Results on nested composition also show that the method remains effective."}, "weaknesses": {"value": "* Missing standard deviations for results.\n* The datasets used for evaluation are designed to have the three types of composition, but a traditional instruction following dataset with more “real” tasks is not used for evaluation."}, "questions": {"value": "* Figure 4a shows that ATA reduces the identified generation errors, but it doesn’t go to 0. Why do you think some of these errors remain? Are the wrong subtasks selected by the model’s attention?\n* How often is active subtask selection incorrect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ir6nII0jLA", "forum": "wwA0X3UfAn", "replyto": "wwA0X3UfAn", "signatures": ["ICLR.cc/2026/Conference/Submission17255/Reviewer_TRL7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17255/Reviewer_TRL7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954846773, "cdate": 1761954846773, "tmdate": 1762927206069, "mdate": 1762927206069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Attend to the Active (ATA), an inference-time method that steers a model’s attention toward the currently “active” sub-task within a complex instruction. ATA masks attention between mutually exclusive sub-tasks and then boosts attention to the detected active sub-task. The authors evaluate on chain, branch, and a synthetic parallel setup and report consistent gains over prompting and planning baselines, with ablations on masking strength and head selection. They also include robustness checks for structure identification quality and some results on nested compositions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Good idea and implementation. The focus on compositional instruction following is timely. Doing it with attention masking at inference is interesting because it avoids retraining and aims to localize interference between sub-tasks.  \n\n2. Clear writing and organization. The paper explains the three structure types, gives concrete examples, and walks through the masking and steering pipeline in a way that is easy to follow.   \n\n3. Strong experiments and analysis. The experiments cover two base models, three composition types, and include ablations for masking degree and number of steered heads, as well as component removals. The figures show sensitivity to α and head counts, and tables report the impact of removing structure info, mutual masks, dynamic masks, and active control."}, "weaknesses": {"value": "1. Parallel instruction data is simplistic - The “parallel” benchmark is built by concatenating independent GSM8K problems. This construction makes sub-tasks independent by design, which favors attention masking because there is little need for cross-task sharing or entity linking. A harder parallel set, closer to the realistic prompts illustrated in figure 2, would be better test.  \n\n2. General capability preservation is under-tested - Because ATA modifies attention patterns, it is important to show that language abilities are not degraded. Following evaluations in [1] it would be good to measure side effects on general text quality.  \n\n\nReferences: \n[1] Stolfo et al., “Improving Instruction-Following in Language Models through Activation Steering,” ICLR 2025."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lXCpMLFUNu", "forum": "wwA0X3UfAn", "replyto": "wwA0X3UfAn", "signatures": ["ICLR.cc/2026/Conference/Submission17255/Reviewer_qkLE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17255/Reviewer_qkLE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974931857, "cdate": 1761974931857, "tmdate": 1762927205466, "mdate": 1762927205466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}