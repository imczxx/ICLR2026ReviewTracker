{"id": "djwumu36TX", "number": 21698, "cdate": 1758320627957, "mdate": 1759896908094, "content": {"title": "UNSUPERVISED CONFORMAL INFERENCE: BOOTSTRAPPING AND ALIGNMENT TO CONTROL LLM UNCERTAINTY", "abstract": "Deploying black-box LLMs requires managing uncertainty in the absence of token-level probability  or true labels. We propose introducing an unsupervised conformal inference framework for generation, which integrates: generative models, incorporating: (i) an LLM-compatible atypical score derived from  response-embedding Gram matrix, (ii) UCP combined with a bootstrapping variant (BB-UCP) that aggregates residuals to refine quantile precision while maintaining distribution-free, finite-sample coverage, and (iii) conformal alignment, which calibrates a single strictness parameter $\\tau$ so a user predicate (e.g., factuality lift) holds on unseen batches with probability $\\ge 1-\\alpha$. Across different benchmark datasets, our gates achieve close-to-nominal coverage and provide tighter, more stable thresholds than split UCP, while consistently reducing the severity of factuality severity, outperforming lightweight per-response detectors with similar computational demands. The result is a label-free, API-compatible gate for test-time filtering that turns geometric signals into calibrated, goal-aligned decisions.", "tldr": "We propose an unsupervised conformal framework for black-box LLMs: Gram-geometry scoring ,batched bootstrap calibration and conformal alignment, yielding near-nominal coverage and reliable factuality improvements.", "keywords": ["conformal prediction", "uncertainty quantification", "large language models", "bootstrapping", "Gram matrix", "hallucination detection", "calibration", "alignment"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5241c97f61de2aeb8c2f8b08d9afb0b9da593ddb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors (1) derive a new conformity score for LLMs, (2) propose batched and bootstrap-batched variants of UCP, and (3) perform conformal alignment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is written well and all results are clear and concise. The advantages of their bootstrapping approach is clear."}, "weaknesses": {"value": "While I think the paper is well-written, it does not do a good job in conveying background information or motivation, especially whe compared to many other works that apply forms of uncertainty quantification to language models. For example, the conformal LLM papers mentioned by the authors (Quach et al., 2024; Mohri & Hashimoto, 2024) make a substantial effort in conveying to readers what practical LLM problems they are trying to solve and how the problems can be framed under conformal prediction.\n\nThe title and intro of this paper suggest that the purpose of this work is to show how to better control LLM uncertainty, but it reads as if the audience is only people who are familiar with and interested in improving split UCP. While I believe the authors that this is important for generative models/LLMs, I think major revisions in writing are required to make that point clear (or the paper just needs to be reframed)."}, "questions": {"value": "I thinking adding a section with diagrams outlining how UCP can be used for LLM QA would be very helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "F95rUjaNpi", "forum": "djwumu36TX", "replyto": "djwumu36TX", "signatures": ["ICLR.cc/2026/Conference/Submission21698/Reviewer_1god"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21698/Reviewer_1god"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939735524, "cdate": 1761939735524, "tmdate": 1762941893590, "mdate": 1762941893590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a conformal prediction framework that utilizes their proposed Gram-similarity matrix scoring. Their frameworks work under the batch exchangeability assumption. They evaluate their coverage guarantees on several datasets with various use case scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The paper focuses on an important and timely topic.\n- Their proposed method looks like it has novelty over prior works (which I may be wrong about because I might misunderstand the main message of the paper ).\n- The experimental evaluation is concrete."}, "weaknesses": {"value": "- The main weakness of the paper, in my opinion, is its presentation. As someone who is highly familiar with the UQ-LLM literature but only moderately knowledgeable about conformal prediction, I struggled to grasp the main idea of the paper. It uses several terms that can have multiple meanings without providing clear definitions, such as “residuals” (which can mean different things in machine learning), “alignment,” and “modality.” Phrases like “prompts are not quantifiable covariates” are also confusing. While some of this may be due to my limited familiarity with the specific subfield, I suspect that readers from a general machine learning background would also find the paper difficult to follow.\n\n- There is no Figure 1 explaining the main message of the paper.\n\n- Please explain key terms (e.g., Split UCP) in the main text rather than only in the appendix.\n\n- Fix the Gemini citation, it currently spans almost three pages."}, "questions": {"value": "- Why do you use a \"batched\" setting? How is it different from the classical CP settings, and what is the advantage of it?\n- What is the main novelty of the paper over existing LLM CP frameworks?\n- What is the main purpose of the knob in a practical application?\n- What is conformal alignment in general?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SmAK1p3imS", "forum": "djwumu36TX", "replyto": "djwumu36TX", "signatures": ["ICLR.cc/2026/Conference/Submission21698/Reviewer_h8Go"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21698/Reviewer_h8Go"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124135176, "cdate": 1762124135176, "tmdate": 1762941893382, "mdate": 1762941893382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to answer this question: When you only have black-box access to an LLM (no logits, no hidden states, no labels at deploy time), how can you reliably decide which generated answers to keep vs. filter out--with statistical guarantees? The paper builds an unsupervised conformal inference pipeline that works directly from multiple sampled responses per prompt."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Look at questions."}, "weaknesses": {"value": "Look at questions."}, "questions": {"value": "The core (unsupervised) guarantee is about the rank of an unlabeled score, not task correctness: with probability ≥ 1−α, the next response’s Gram-energy-based score falls below a calibrated threshold. Formally, Pr{Yₙ₊₁ ∈ Cₙ} ≥ 1−α, proved for BB-UCP (Theorem 3.2).  This is distribution-free because it uses exchangeability and order statistics, not labels. However, for a decision-maker, this by itself does not guarantee accuracy, factuality, or utility --- only that the gate’s acceptance event (defined by an unlabeled similarity/typicality score) occurs at least (1−α) of the time.\n\nI would like the authors to explain what is the point of this guarantee. Particularly, why and how a downstream decision maker might benefit from this algorithm/guarantee? I.e., t does not guarantee that kept items are correct, less harmful, or even better than rejected ones. \n\nIf the only goal is to control the throughput of the model, like when there is human-in-the-loop, one could do so many things. Then the scope of experiments should be on a variety of pruning methods and decided which on keeps more valuable outputs.\n\n\nAlternatively, there is an important line of work (e.g. look at [1] and [2]) where they also treat the LLM as pure black-box and they also sample multiple time and keep only a fraction of samples, but instead they have meaningful correctness guarantees. Of course, this comes at the cost of a need for some calibration ground truth labels. But anyhow, this is an important and very relevant line of work which has to be discussed in the paper. To me, asking for the order of 100 ground truth labels, but then providing a meaningful guarantee, is better than avoiding it by making the guarantee less useful for the user. Do the authors have an example against this statement?\n\n[1]: Conformal Language Modeling, Quach et. al.\n\n[2]: Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models, Noorani et. al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9X3TkAC6W9", "forum": "djwumu36TX", "replyto": "djwumu36TX", "signatures": ["ICLR.cc/2026/Conference/Submission21698/Reviewer_2eaX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21698/Reviewer_2eaX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762291078395, "cdate": 1762291078395, "tmdate": 1762941893103, "mdate": 1762941893103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}