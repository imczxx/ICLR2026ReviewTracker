{"id": "Bw4G5ftscn", "number": 10160, "cdate": 1758162600515, "mdate": 1763292613845, "content": {"title": "Beyond Extrapolation: Knowledge Utilization with Bidirectional Inference for Time Series Forecasting", "abstract": "Time-series forecasting has recently attracted considerable attention due to its crucial role in domains such as energy demand prediction, traffic flow management, and public health.  However, most existing forecasting models rely solely on unidirectional inference from history to target. While effective in relatively stable scenarios, this paradigm often lacks explicit, data-driven constraints that reflect how trajectories tend to evolve beyond the prediction horizon, making it difficult to stabilize predictions under complex dynamics. \nThere is a natural chain in time series, namely ``history (model input)–target (ground-truth output)–future continuation''. Rather than further deepening the model’s capacity to capture only the relationship between history and target, we leverage another piece of information that is intrinsically linked to the target, namely the future continuation. In other words, we extend the conventional unidirectional inference toward a knowledge utilization paradigm with bidirectional inference, which introduces a future continuation stream alongside the current input stream to impose active constraints during prediction.\nTo instantiate this idea, we provide a simple yet efficient reference framework. For each chain in the training set, we first construct a ratio matrix that characterizes how the future continuation varies relative to its history, and build a retrieval library that stores these matrices together with their corresponding historical segments. Given a current input, we retrieve similar historical segments from this library, collect their associated ratios, and use them to estimate the future continuation of the current input. We then extract features from both the current input and the estimated future continuation separately, and fuse them via a lightweight gating module before feeding them into the backbone model.\nThis design transforms forecasting from a purely extrapolative task into an interpolation-like process over learned trajectories, enabling the model not only to rely on static knowledge stored in its parameters but also to dynamically reference learned future continuation streams as compensation, thereby reducing uncertainty in volatile scenarios. Across six benchmarks and several state-of-the-art models, our paradigm consistently improves forecasting performance with small overhead.", "tldr": "", "keywords": ["Time-series forecasting", "Bidirectional inference", "Structural future prior"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a5847a21a1269ad5fe5e94c1bee616adfdd23958.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes using what they call a approximate future prior in addition to the predictive forward model for forecasting.\nThe paper motivates the problem, goes through a number of derivations of the method they use and then demonstrates the method on a number of testbeds, comparing with standard MSE training methods, but not particularly looking towards state of the art approaches, or considering any of the normal regularisation methods that one would reasonably employ in these settings for the size of forecasts being considered. They focus on long prediction length, but do not consider autoregressive unrolling, or consider methods that are more targeted at long horizon distributional forecasts (e.g. probabilistic basis function approaches)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I am uncertain of the strengths of the paper."}, "weaknesses": {"value": "The motivation for this paper is somewhat puzzling to me. In a forecasting model P(future|past), the prior is already included in the forecast distribution, in that it moves from the unconditional prior P(future) to the conditional posterior P(future|past), that captures the whole future distribution. Combining on an additional prior component to this breaks the rules of probability. Hence the whole process seems somewhat dubious. I see nothing in the motivation or the problem description that either relates to this or addresses this. The writing of this paper seems somewhat lacking in understanding of the probabilistic foundations of forecasting altogether. \n\nThe formulation itself seems fundamentally broken, and is certainly unmotivated. The authors define R as the division of two matrices, without defining what is meant by that. The motivation for this division (assumed component-wise) is also problematic, and not provided and seems entirely arbitrary. The paper introduce an undefined query vector X_q, and do not explain the purpose here: it appears to be to search histories to find nearby examples that can be used to build a distribution for the future, but this is not a future prior, as it is history dependent, and there is no loss function element. The paper then goes on with what, to my reading, seem like fairly meaningless calculations to produce something that is called a future prior generation, but really seems broken to me. I think what the paper is actually trying to do is some crude form of backoff, akin to that in n-gram language models, but this neither clear nor properly formulated.\n\nThe rest of the paper rests on all this, and sadly I think this sits on very shaky ground.\n\nIf the authors really believe there is merit in this work they need to found it properly in a proper probabilistic framework, and explain each step of the approach, why each step is done, what impact each step has on the probabilistic formulation of the problem and how that all complies with the rules of probability. This should start with the foundations. As it stands it is a list of apparently-arbitrary and potentially ill-motivated set of computations. That the authors might be able to run some experiments that enable them to bold a column in a table, but that does not, in itself, demonstrate that the approach has any merit. At the end of the paper, I am left with no insight, no change in understanding that I did not have before, and little understanding of the point or process of the procedure they follow.\n\nFundamentally IMO the premise is wrong: \"most existing forecasting models rely solely on unidirectional inference from history to future. While effective in stable scenarios, this paradigm lacks explicit structural constraints about the future, which makes it challenging to stabilize predictions under complex dynamics.\" - forecasting models rely on inference from history to the future because the history is the only place where data is given - there is no inferential information from the future as it has not happened yet. But the claim that the \"paradigm lacks explicit structural constraints about the future\" is patently false - that is what the forecasting model does - it is predicting the future evolution, either as a joint distribution over the future states, or as an unrolling of the next-step predictions via the chain rule. These future predictions have an implicit prior already (simply sum out the conditional distribution over the histories to get the prior). These future predictions then already incorporate the \"structural constraints about the future\". The argument might be that current approaches do that badly (I am not sure they do, and methods such as classifier-free guidance tackle this significantly), but then that needs demonstrating, reasoning about and a clearly formulated, and mathematically justified fix applied. This is not happening here."}, "questions": {"value": "I do not believe there are answers to any questions that would really clarify or fix this paper for me sufficiently beyond a fundamental rewrite. However the basic questions hold: what is the context, what is the definition of the problem space, what is the current failure-mode that you see in this problem space? In what way and why do current state-of-the-art approaches fail to address this, and how have you demonstrated this? What is the fundamental methodologically grounded insight that you bring to this that allows you to overcome this problem? How is this implemented? How have you reliably demonstrated that this really does do what you expected it to and complies with the methodological expectations in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FNbTrzoUEJ", "forum": "Bw4G5ftscn", "replyto": "Bw4G5ftscn", "signatures": ["ICLR.cc/2026/Conference/Submission10160/Reviewer_1s5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10160/Reviewer_1s5C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738918712, "cdate": 1761738918712, "tmdate": 1762921529788, "mdate": 1762921529788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes using what they call a approximate future prior in addition to the predictive forward model for forecasting.\nThe paper motivates the problem, goes through a number of derivations of the method they use and then demonstrates the method on a number of testbeds, comparing with standard MSE training methods, but not particularly looking towards state of the art approaches, or considering any of the normal regularisation methods that one would reasonably employ in these settings for the size of forecasts being considered. They focus on long prediction length, but do not consider autoregressive unrolling, or consider methods that are more targeted at long horizon distributional forecasts (e.g. probabilistic basis function approaches)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I am uncertain of the strengths of the paper. Though I have upped my score from 0 to 2, just because there might be some hope that the paper can be changed later into a feasible paper. I am still very strongly opposed to publication in this conference and in any approximation to its current form."}, "weaknesses": {"value": "The motivation for this paper is somewhat puzzling to me. In a forecasting model P(future|past), the prior is already included in the forecast distribution, in that it moves from the unconditional prior P(future) to the conditional posterior P(future|past), that captures the whole future distribution. Combining on an additional prior component to this breaks the rules of probability. Hence the whole process seems somewhat dubious. I see nothing in the motivation or the problem description that either relates to this or addresses this. The writing of this paper seems somewhat lacking in understanding of the probabilistic foundations of forecasting altogether. \n\nThe formulation itself seems fundamentally broken, and is certainly unmotivated. The authors define R as the division of two matrices, without defining what is meant by that. The motivation for this division (assumed component-wise) is also problematic, and not provided and seems entirely arbitrary. The paper introduce an undefined query vector X_q, and do not explain the purpose here: it appears to be to search histories to find nearby examples that can be used to build a distribution for the future, but this is not a future prior, as it is history dependent, and there is no loss function element. The paper then goes on with what, to my reading, seem like fairly meaningless calculations to produce something that is called a future prior generation, but really seems broken to me. I think what the paper is actually trying to do is some crude form of backoff, akin to that in n-gram language models, but this neither clear nor properly formulated.\n\nThe rest of the paper rests on all this, and sadly I think this sits on very shaky ground.\n\nIf the authors really believe there is merit in this work they need to found it properly in a proper probabilistic framework, and explain each step of the approach, why each step is done, what impact each step has on the probabilistic formulation of the problem and how that all complies with the rules of probability. This should start with the foundations. As it stands it is a list of apparently-arbitrary and potentially ill-motivated set of computations. That the authors might be able to run some experiments that enable them to bold a column in a table, but that does not, in itself, demonstrate that the approach has any merit. At the end of the paper, I am left with no insight, no change in understanding that I did not have before, and little understanding of the point or process of the procedure they follow.\n\nFundamentally IMO the premise is wrong: \"most existing forecasting models rely solely on unidirectional inference from history to future. While effective in stable scenarios, this paradigm lacks explicit structural constraints about the future, which makes it challenging to stabilize predictions under complex dynamics.\" - forecasting models rely on inference from history to the future because the history is the only place where data is given - there is no inferential information from the future as it has not happened yet. But the claim that the \"paradigm lacks explicit structural constraints about the future\" is patently false - that is what the forecasting model does - it is predicting the future evolution, either as a joint distribution over the future states, or as an unrolling of the next-step predictions via the chain rule. These future predictions have an implicit prior already (simply sum out the conditional distribution over the histories to get the prior). These future predictions then already incorporate the \"structural constraints about the future\". The argument might be that current approaches do that badly (I am not sure they do, and methods such as classifier-free guidance tackle this significantly), but then that needs demonstrating, reasoning about and a clearly formulated, and mathematically justified fix applied. This is not happening here."}, "questions": {"value": "I do not believe there are answers to any questions that would really clarify or fix this paper for me sufficiently beyond a fundamental rewrite. However the basic questions hold: what is the context, what is the definition of the problem space, what is the current failure-mode that you see in this problem space? In what way and why do current state-of-the-art approaches fail to address this, and how have you demonstrated this? What is the fundamental methodologically grounded insight that you bring to this that allows you to overcome this problem? How is this implemented? How have you reliably demonstrated that this really does do what you expected it to and complies with the methodological expectations in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FNbTrzoUEJ", "forum": "Bw4G5ftscn", "replyto": "Bw4G5ftscn", "signatures": ["ICLR.cc/2026/Conference/Submission10160/Reviewer_1s5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10160/Reviewer_1s5C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738918712, "cdate": 1761738918712, "tmdate": 1763325287708, "mdate": 1763325287708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel algorithm of using future priors developed independently to improve time series forecasting by leveraging both the historical values and the future priors. The paper shows improvements in the forecasts based on this methodology across standard benchmark datasets within the time series foundation models community using three forecasting models, namely, PatchTST, DLinear and TimesNet."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Paper presents a novel algorithm by reframing the forecasting problem as  an interpolation problem rather than the conventional extrapolation problem to improve time series forecasting using future priors across a wide range of benchmark datasets. \n\nAlgorithm also allows for any other future prior to be utilized and plugged into the algorithm. \nSignificant gains in performance have been shown for DLinear algorithm and modest gains for PatchTST and TimesNet."}, "weaknesses": {"value": "It is not clear what are the advantages of this method as compared to existing methods mentioned in the literature review by the authors in Section 2.2. \n\nI think a comparison with state of the art methods is missing. I think comparing the algorithm mentioned in this paper with RAFT model in Section 2.2 or other models in Section 2.2 will illustrate the benefits of this work better (if any). \n\nState of the art models can be improved. TTM and Chronos are probably more standard models employed right now as compared to PatchTST."}, "questions": {"value": "As stated in the weaknesses, I think the authors can expand on this section to make the applicability of this method more clear with reference to prior art in Section 2.2.\n\n1. Where would this method work better than the existing methods? \n2. Are there any computational benefits compared to the existing methods?   \n3. How does this method compare with other state-of-the-art methods like RAFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ptCtSxTxof", "forum": "Bw4G5ftscn", "replyto": "Bw4G5ftscn", "signatures": ["ICLR.cc/2026/Conference/Submission10160/Reviewer_qV9Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10160/Reviewer_qV9Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813183444, "cdate": 1761813183444, "tmdate": 1762921529179, "mdate": 1762921529179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces KUP-BI (Knowledge Utilization Paradigm with Bidirectional Inference), a framework that augments standard unidirectional time-series forecasting models with an auxiliary future-prior stream. Instead of predicting solely from historical data (past → future), KUP-BI retrieves approximate future priors from a library of historical patterns (constructed as history–target–future chains). These priors represent structural cues about plausible future trajectories and are fused with the current input’s representation via a lightweight gated module. The design reframes forecasting from a purely extrapolative problem into an interpolation-like one, using approximate structural knowledge from correlated historical patterns to stabilize predictions.\nEmpirical results on six datasets (ETTh1/2, ETTm1/2, Exchange, and ILI) with three backbones (DLinear, TimesNet, PatchTST) show consistent improvements in MSE/MAE, particularly for lightweight models (e.g., DLinear). The authors also provide theoretical motivation (an interpolation vs. extrapolation error bound under Lipschitz continuity) and detailed ablations on retrieval quality, fusion strategies, and hyperparameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The conceptual novelty lies in explicitly incorporating an estimated “future prior”—learned from training histories that resemble the current input—as a structural anchor to guide prediction. This moves beyond existing retrieval-augmented or exogenous-enhanced methods, which either concatenate retrieved outputs or use external signals. The bidirectional reasoning idea—combining historical and prospective cues—is fresh and supported by a solid theoretical argument that interpolation is less error-prone than extrapolation for Lipschitz functions.\n\nWhile related to retrieval-augmented forecasting (RAFT, TS-RAG), future prior modeling as a separate learnable stream with a ratio-based representation (rather than directly fusing retrieved outputs) appears original. The method’s simplicity and plug-and-play nature increase its potential impact—especially since it can attach to diverse backbones with negligible overhead.\n\n- The ratio-style operator ($R = (F-H)/(H+\\epsilon,\\text{sign}(H))$) is clearly defined, and the retrieval and softmax-weighted fusion mechanisms are mathematically specified.\n\n\n- The Lipschitz interpolation theorem is formally derived in Appendix A and conceptually motivates the approach. While idealized, it reinforces the intuition that adding a bounded-fidelity future anchor reduces variance.\n\n\n- The gated fusion module and harmonic residual design (convex combination controlled by α and per-channel gate γ) are reasonable and stabilizing.\n\n\n- The experiments seem to be methodically conducted with non-leaking retrieval libraries (constructed only from training data)."}, "weaknesses": {"value": "I believe \n\n- the retrieval-based prior depends on correlation measures that may fail under phase shifts or abrupt regime changes. Although the authors acknowledge this, no quantitative robustness analysis is included.\n\n- the “ratio operator” assumes relative amplitude continuity between history and future, which might not hold in nonstationary series (financial or event-driven).\n\n\nWhile results are statistically consistent, the improvements are modest (1–7% MSE reduction). It would help to report statistical significance or confidence intervals. Also, the approach may implicitly leak periodic information if future segments overlap with nearby training windows—clarifying how window offsets are handled would strengthen reproducibility, given that code is not shared yet."}, "questions": {"value": "- Can you quantify retrieval precision (e.g., correlation between estimated and true future segments) to better understand when the prior helps?\n\n\n- How does KUP-BI perform on non-periodic or abrupt-shift datasets (e.g., stock tick data)?\n\n\n- Could a learned retriever (e.g., embedding-based) outperform correlation-based matching?\n\n\n- The ratio operator assumes elementwise alignment—could phase-shift alignment or dynamic time warping improve the prior’s fidelity?\n\n\n- Can you include some runtime comparisons (ms per batch) to substantiate claims of negligible overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BftARVitgc", "forum": "Bw4G5ftscn", "replyto": "Bw4G5ftscn", "signatures": ["ICLR.cc/2026/Conference/Submission10160/Reviewer_yB2k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10160/Reviewer_yB2k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915884244, "cdate": 1761915884244, "tmdate": 1762921528637, "mdate": 1762921528637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}