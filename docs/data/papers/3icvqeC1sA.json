{"id": "3icvqeC1sA", "number": 17691, "cdate": 1758279350860, "mdate": 1759897159882, "content": {"title": "ChaosNexus: A Foundation Model for Universal Chaotic System Forecasting with Multi-scale Representations", "abstract": "Accurately forecasting chaotic systems, prevalent in domains such as weather prediction and fluid dynamics, remains a significant scientific challenge. The inherent sensitivity of these systems to initial conditions, coupled with a scarcity of observational data, severely constrains traditional modeling approaches. Since these models are typically trained for a specific system, they lack the generalization capacity necessary for real-world applications, which demand robust zero-shot or few-shot forecasting on novel or data-limited scenarios. To overcome this generalization barrier, we propose ChaosNexus, a foundation model pre-trained on a diverse corpus of chaotic dynamics. ChaosNexus employs a novel multi-scale architecture named ScaleFormer augmented with Mixture-of-Experts layers, to capture both universal patterns and system-specific behaviors. The model demonstrates state-of-the-art zero-shot generalization across both synthetic and real-world benchmarks. On a large-scale testbed comprising over 9,000 synthetic chaotic systems, it improves the fidelity of long-term attractor statistics by more than 40% compared to the leading baseline. This robust performance extends to real-world applications with exceptional data efficiency. For instance, in 5-day global weather forecasting, ChaosNexus achieves a competitive zero-shot mean error below 1°C—a result that further improves with few-shot fine-tuning. Moreover, experiments on the scaling behavior of ChaosNexus provide a guiding principle for scientific foundation models: cross-system generalization stems from the diversity of training systems, rather than sheer data volume.", "tldr": "We introduce a foundation model on a vast and diverse corpus of thousands of chaotic systems, enabling zero-shot forecasting for previously unseen chaotic systems.", "keywords": ["Chaotic Systems", "Foundation Model", "Zero-shot Generalization"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/245244422c5237489b1b89022ab7bd22a1753a6f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors develop a foundation model for forecasting chaotic systems, ChaosNexus, and show that it outperforms existing time-series and dynamical-systems foundation models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Foundation model for dynamical systems is an exciting direction for scientific machine learning. The paper proposes a novel architecture that could be useful for future foundation models in different scientific domains. Their results also highlight the importance of building domain-specific foundation models, which can be valuable for guiding research efforts in the field of AI for Science."}, "weaknesses": {"value": "* The key insight that the diversity of the training data is more important than quantity has already been established in the Panda paper.\n* The baseline models are incomplete. Other than Panda, there are other dynamical-systems foundation models, for example, see Hemmer and Durstewitz 2025 (https://arxiv.org/abs/2505.13192).\n* The differences between existing models (e.g., Panda) and ChaosNexus are quite small in many metrics."}, "questions": {"value": "* It was claimed in the paper that \"the strong performance of ChaosNexus in these statistical metrics is therefore compelling evidence that it can infer intrinsic dynamics of new systems from the contexts rather than superficial pattern memorizing.\" There is a recent paper that showed simply parroting from the context data can outperform many time-series foundation models (Zhang and Gilpin 2025, https://arxiv.org/abs/2505.11349). Have the authors compared ChaosNexus to context parroting? Beating this naive baseline could be a signal that ChaosNexus is doing more than pattern matching.\n* Have the authors observed any failure modes of ChaosNexus when forecasting chaotic systems? Identifying these failure modes can inform the development of better foundation models for dynamical systems.\n* Why didn't the authors fine tune the base or large Chronos model, which are much better than the small Chronos model in general time series tasks? There is a chance that a fine-tuned large Chronos model can outperform ChaosNexus."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Qqg0JLh6nB", "forum": "3icvqeC1sA", "replyto": "3icvqeC1sA", "signatures": ["ICLR.cc/2026/Conference/Submission17691/Reviewer_R3dg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17691/Reviewer_R3dg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760743655687, "cdate": 1760743655687, "tmdate": 1762927537532, "mdate": 1762927537532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The study introduces ChaosNexus, a pretrained foundation model for forecasting chaotic dynamical systems.The architecture is based on ScaleFormer, comprising a hierarchical encoder-decoder Transformer that operates on temporal patches using dual axial attention to separately capture variable- and time-axis dependencies. Each Transformer block is augmented with Mixture-of-Experts (MoE) layers to enable specialization, and a wavelet scattering-based frequency fingerprint provides invariant spectral conditioning, aiming to stabilize cross-system transfer. ChaosNexus is pretrained on a corpus of synthetically-generated differential equations.The training uses a composite loss combining MSE, an MoE load-balancing term, and a Maximum Mean Discrepancy (MMD) regularizer to align predicted and true attractor distributions. The authors evaluate zero-shot forecasts on held-out data from the ODE corpus, and they also fine-tune their model on a weather dataset. The authors report strong zero-shot generalization on both simulated systems and on weather forecasts, using measures of both local and global accuracy. Scaling experiments show that performance improves with the amount of data available to the model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The authors fully describe their methods, architecture, and training information. The paper reads well, and the organization is clear. The authors provide sufficient background about their “chaos” dataset that a time series practitioner can understand that it is synthetic data with certain desirable properties.\n\n+ The paper is timely. Many current time series foundation models appear to be close to saturating available data, and so many works have turned towards using synthetic datasets. For example, Chronos-1, one of the earliest high-performing time series foundation models, leveraged random Gaussian processes as training data. I like the idea of instead using data from random differential equations, which might help cover certain types of time series not covered by stochastic processes.\n\n+ The use of a ScaleFormer architecture is well-motivated. The authors argue that their dataset has many interacting scales, requiring an architecture resembling a U-net, which was originally developed to capture many scales in image-to-image models. The choice of downsampling and then upsampling the patches to capture scale invariance is therefore well-motivated by the data.\n\n+ The wavelet transform is an appropriate preprocessing choice for this dataset, which the authors emphasize is a multiscale dataset. The ablations show that this step contributes, and it appears that this could be a compelling standard featurization for models of this kind.\n\n+ The use of MMD in the loss function ensures that long-term statistics are preserved. This could help generally improve time series foundation models for stationary systems.\n\n+ Multiple metrics are used to ensure that the generated forecasts are both point-wise accurate, and that they capture the long-term statistics of the systems of interest."}, "weaknesses": {"value": "**Significant overlap with a prior study.** I am confused regarding how this paper relates to the earlier “PANDA” paper cited by the authors. The title of and abstract that paper is very similar: “A pretrained forecast model for universal representation of chaotic dynamics” versus “A Foundation Model for Universal Chaotic System Forecasting with Multi-scale Representations.”  Both papers use a genetic algorithm to evolve the same starting set of 135 ODE into a new set of 10^4 new dynamical systems, both papers report scaling laws with the number of systems, and both papers plot attention maps with Toeplitz structure. Both papers benchmark against Chronos, and fine tune a Chronos checkpoint with a lookback of 512 timepoints for 300k iterations. Both papers use the same metrics: SMAPE, KL Divergence, and Fractal Dimension. The code included with this submission also contains a directory called “panda” and exhibits nearly identical directory structure to the PANDA code. \nWhat is the relationship between these two papers? Is this a follow-up study? Is this a contemporaneous effort that happened to arrive at a similar approach (evolving ODEs with a genetic algorithm)? The authors need to clearly outline what parts of their paper are new, and what parts come from earlier work. It’s extremely difficult to disentangle their contributions here relative to the earlier work.\n\n**Novelty.** The author’s description of a “new paradigm for chaotic system forecasting” has been made earlier in earlier works, first in PDE models (Jiao et al. 2021), and recently in DynaMix (Hemmer & Durstewitz 2025) and the PANDA paper described above (Lai et al 2025), as well as in neural operators. Since the authors are not the first to address this problem, the primary points of novelty, in my reading, would either be the dataset or the architecture itself (along with its benchmarks). Regarding the novelty of the dataset, as I note above, it is very unclear in my reading whether the authors introduced a new pretraining dataset, or whether this dataset fully comes from the He et al. 2025 paper or the Lai et al. 2025 papers. The description at times refers to a dataset in the earlier study (He et al. 2025), but Appendix D references Lai et al 2025, but then describes generating the dataset. Did the authors evolve a set of systems using a similar approach? Did they re-run the Lai et al 2025 evolutionary process on their own data to make a larger pretraining dataset? As far as I can tell, the He et al. 2025 study does not use the full dataset. Did the authors run their evolutionary algorithm on that study’s original, smaller dataset? Without clarification from the authors about the relationship between their study and prior works, it’s not clear to me that the dataset itself is the contribution (however, I may be mistaken, see Q1 below). \n\n**Methods.** The model architecture (Fig 1) is intricate and overly elaborate for a standard multivariate forecasting task. The authors first apply a bespoke patching scheme, then axial and temporal attention within a stacked Transformer model, then an MoE layer, then patch merging, then patch expansion, then joint scale readout with a wavelet scattering transform. The training procedure then combines standard MSE error on the forecasts with additional regularizers like MMD and MoE balancing. This is a lot of moving parts, each with their own hyperparameters and user choices. This overall seems mismatched to the task itself, which is a standard multivariate forecasting setting that a simple transformer like Fedformer ought to be able to cover. Why does this particular multivariate forecasting dataset require this level of complexity? I’m less interested in empirics here (I don’t think that ablating all possible combinations of hyperparameters is necessary). I’m more interested in, conceptually, why these particular datasets would require this much model engineering. My concern is that this architecture’s complexity renders it brittle.\n\n**Experiments.** I am confused that the chaotic systems dataset and the weather dataset use a completely different set of models and metrics. This reads as if a completely different study has been merged into this paper. Why were these models and metrics chosen in each case? Additionally, for the fine-tuning experiments, were FEDformer et al pretrained at all? Are they fully-trained on the weather data on which ChaosNexus is fine-tuned? It’s not clear to me that all baselines had access to the same amount of training data, which makes it difficult to evaluate these results.\n\n**Reproducibility.** I am unable to see how I could reproduce the results reported in this paper with the included code. In particular, the provided code does not include any of the evaluation scripts needed to reproduce the results in the figures, and the weather benchmark results are missing. I could not find the Chronos-S-SFT zero-shot and Fedformer fine-tuned models that the authors fine-tuned. I very much understand if the authors prefer not to share these checkpoints, but please at least provide more information about how you trained these baselines."}, "questions": {"value": "+ How did you create the pretraining dataset? I looked at the reference cited for the dataset (He et al. (2025)) and I don’t see that study uses the same chaotic systems dataset or a different one than described here. Reading Appendix D, the authors describe generating the chaotic systems dataset. Can you clarify how this relates to the work in He et al. 2025? Another reference cited in the text, Lai et al. 2025, also describes generating a chaotic ODE dataset with a genetic algorithm. What is the relationship between that dataset and this one? I’m supportive of this synthetic data generation strategy in general, but I find the origin of the data in this paper very confusing.\n\n+ How were Fedformer and Chronos-S-SFT trained? Can you please provide more information about these baseline models? Which checkpoint did you start from? How did you choose the context of 512 and 300k training iterations for the baseline models?\n\n+ Why are the benchmark models and metrics on the weather dataset different from the benchmark models on the dynamical systems dataset? \n\n+ The authors appropriately point out that reservoir computers are currently considered to be a leading architecture for creating digital twins of dynamical systems. However, the next-generation reservoir computer that they benchmark (NVAR) appears to underperform on this dataset. Can the authors comment on why this occurs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DYExNP09qQ", "forum": "3icvqeC1sA", "replyto": "3icvqeC1sA", "signatures": ["ICLR.cc/2026/Conference/Submission17691/Reviewer_fspm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17691/Reviewer_fspm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783058098, "cdate": 1761783058098, "tmdate": 1762927537087, "mdate": 1762927537087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ChaosNexus, a foundation model for forecasting chaotic dynamical systems. The model employs a multi-scale transformer architecture (ScaleFormer) augmented with Mixture-of-Experts layers and wavelet-based frequency fingerprints. Pre-trained on ~20K synthetic chaotic systems, ChaosNexus achieves zero-shot and few-shot generalization to novel systems. The authors report improvements over baselines on synthetic benchmarks and demonstrate competitive zero-shot performance on a real-world weather dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-motivated architecture**: The multi-scale encoder-decoder design is well justified for the broadband spectra of chaotic systems.\n2. **Evaluations**: The evaluation metrics are thorough and the results strong.\n3. **Presentation**: The paper is geneally well written."}, "weaknesses": {"value": "### Major Weaknesses\n1. **Misleading performance claims**: The abstract claims that ChaosNexus \"improves the fidelity of long-term attractor statistics by more than 40%.\" Line 328 states ChaosNexus \"demonstrates a consistent advantage in both short-term and long-term point-wise forecasting accuracy.\" However, Figure 2 shows substantial overlap between ChaosNexus and Panda for these statistics. I think the results are being unnecessarily overstated; they're very good in their own right and worth reporting, even if they don't beat necessarily Panda. The paper would be improved if these claims were toned somewhat.\n2. **Missing Panda from weather experiments**: Figure 3 compares against multiple baselines but omits Panda, the most directly comparable model, even though it's used elsewhere in the paper.\n3. **Missing training details**: No specific details of training setup or timing.\n4. **Missing computational analysis**: No benchmarking of inference speed versus, e.g., Panda.\n5. **Incomplete long-term dynamics evaluation**: For chaotic systems, the Lyapunov spectrum is a fundamental characterization of the system's long-term dynamics. I would find the claims about long-term statistical properties much more convincing if the model is able to reproduce plausible Lyapunov spectra.\n6. **Frequency Fingerprint**: The ablations state \"Removing the wavelet transform-based frequency fingerprint results in a noticeable decrease in model performance,\" but Table 1 does not support this at all. Why not just remove this rather convoluted feature that apparently offers no benefit?\n\n### Minor Weaknesses\n1. Section 4.1 cites \"He et al. (2025)\" for the dataset, but this should cite Lai et al. (2025) (the Panda paper)\n2. The description of reservoir computing (lines 70-72) is technically inaccurate: the randomly initialized reservoir _is_ the high-dimensional state space; the read-in weights perform the lifting\n3. The finding that diversity of training systems matters more than data volume per system was already demonstrated in the Panda paper, which the authors cite. However, in lines 86-88, it's presented as a novel contribution: \"Moreover, our scaling analysis reveals a key insight for future work: generalization is driven more by the diversity of systems in the pretraining corpus than by the sheer volume of trajectories per system.\"\n4. The paper attempts to identify a gap in the literature (lines 153-156) that ChaosNexus fills, but the cited Panda paper already addresses this exact gap—pre-training a foundation model on diverse chaotic ODEs for zero-shot forecasting. Again, no need to overstate the contribution here."}, "questions": {"value": "1. Achieving zero-shot <1°C MAE for 5-day temperature forecasting is a remarkable result. Have you analyzed this further? For example, how does this result vary by latitude, say, tropics versus mid-latitudes? This is a very strong result, and I think a big claim like this requires more detailed analysis.\n2. Why is Panda not included for the weather experiment?\n3. What is the training setup for ChaosNexus?\n4. What is computational cost comparison between ChaosNexus and Panda for training and inference?\n5. Why include the frequency fingerprint if there is no apparent benefit?\n6. Did you try to compute Lyapunov exponents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hsrnu45TCB", "forum": "3icvqeC1sA", "replyto": "3icvqeC1sA", "signatures": ["ICLR.cc/2026/Conference/Submission17691/Reviewer_bASM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17691/Reviewer_bASM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882515209, "cdate": 1761882515209, "tmdate": 1762927536722, "mdate": 1762927536722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ChaosNexus, a large pre-trained model designed as a foundation model for chaotic dynamical systems.\nIt aims to learn generalizable representations of chaotic behaviors, those exhibiting sensitive dependence on initial conditions and complex attractor structures, so that the model can forecast unseen systems (“zero-shot”) and maintain accurate long-term statistical properties.\nChaosNexus is trained on a large synthetic dataset of thousands of ODE-based chaotic systems generated through evolutionary algorithms and tested both on unseen synthetic systems and on real-world meteorological time series (global temperature, wind, pressure)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work frames chaotic dynamics as a foundation-model problem, which is conceptually bold and aligns with current “AI for Science” trends. The combination of U-Net-style Transformer (ScaleFormer), Mixture-of-Experts (MoE), and frequency-domain fingerprinting is technically coherent. The paper reports both large-scale synthetic ODE datasets and real meteorological data, demonstrating promising zero- and few-shot results. It provides meaningful evaluation metrics, including attractor-based metrics (Dfrac, Dstsp), beyond pointwise MSE, which is insightful and physically relevant."}, "weaknesses": {"value": "1. Authors seem to be overstating the “foundation model” claim. The paper repeatedly positions ChaosNexus as a foundation model for chaotic systems, yet the evidence presented does not support this scale or generality. The model (~50M parameters) is smaller and less diverse in data exposure than true foundation models. Its pretraining corpus consists mainly of synthetically generated ODEs, which may not capture the breadth of real-world chaotic processes (e.g., fluid dynamics, plasma, turbulence).\n\n2. The theoretical justification for cross-system generalization is insufficient. Specifically, the work relies heavily on empirical evidence but lacks a clear theoretical explanation of why ScaleFormer + MoE + wavelet fingerprints should learn invariant measures or attractor structures across systems. There is also minimal discussion of connections to operator theory (e.g., Koopman embeddings, spectral decompositions) or chaos invariants (Lyapunov exponents, entropy).\n\n3. The synthetic dataset lacks transparency and validation. The “evolutionary algorithm” used to generate 20k synthetic ODEs is not described in enough detail to ensure reproducibility or assess diversity. It is unclear whether these systems cover a range of dynamical behaviors (limit cycles, quasi-periodic, chaotic) or merely parameter variants of a few archetypes.\n\n4. The reviewer doubts that the zero-shot evaluation may truly test out-of-distribution generalization. The synthetic “zero-shot” test systems appear to be drawn from the same procedural generation process as training data, which likely share structural priors. The meteorological evaluation, although impressive, still belongs to a similar temporal forecasting regime, not an entirely new dynamical class.\n\n5. The paper claims that the Mixture-of-Experts (MoE) layer learns to specialize different experts for distinct physical regimes or system families, suggesting that ChaosNexus internally disentangles dynamical mechanisms. However, this conclusion is drawn only from a few qualitative visualizations, e.g., heatmaps of expert activation frequencies or attention patterns, which are anecdotal and visually suggestive but not statistically supported. \n\n6. The paper does not provide sufficient details on the training setup, which makes it difficult for other researchers to reproduce the reported results or assess scalability. For example, training costs and compute resources requirements, as well as hardware configuration, though such information is crucial for large-scale models that claim “foundation” status. Hyperparameter schedules are not listed or discussed. These parameters directly affect model stability and convergence, especially under chaotic data distributions. As a result, the current presentation prevents meaningful verification of results and undermines claims of scalability or general applicability."}, "questions": {"value": "1. Could the authors clarify what qualifies ChaosNexus as a foundation model? Any criteria? Is it the scale (parameters, data volume), the breadth of dynamical regimes, or the ability to transfer across physical domains? How does this compare to true foundation models in other scientific areas (e.g., WeatherBench-2)?\n\n2. Could the authors provide the details of the synthetic ODE dataset generation? What is the search space (number of variables, nonlinear terms, parameter ranges)? What is the search space (number of variables, nonlinear terms, parameter ranges)? How is chaos ensured (e.g., via Lyapunov exponents)? Are non-chaotic systems included for contrast? Sharing statistical properties (distribution of Lyapunov exponents, attractor dimensions) would make the dataset’s diversity and representativeness clearer.\n\n3. To clarify true zero-shot versus parameter-shift generalization, how distinct are the training and test systems in structure? Are they drawn from the same family of equations but with different coefficients, or from entirely different equation topologies? Could the authors quantify structural overlap (e.g., via symbolic similarity or graph edit distance of ODE terms)? \tIf overlap is high, how can we be sure the model is generalizing rather than interpolating?\n\t\n4. Regarding cross-domain transfer and universality, have the authors attempted transferring from synthetic ODEs to PDE-based or experimental datasets (e.g., turbulence, reaction–diffusion, plasma)? If not, do they expect the architecture to extend naturally to spatiotemporal fields? What modifications (e.g., 2D attention, convolutional encoders) would be required?\n\t\n5. What is the role of the wavelet scattering fingerprint? How much does the frequency fingerprint contribute relative to standard positional encodings or learned embeddings? Did the authors test alternative spectral representations (e.g., STFT, learnable Fourier features)?\n\t\n6. For MMD regularization and attractor preservation, how sensitive is the model to the choice of MMD kernel or weighting coefficient λ₂? Could the authors provide ablation or visual examples showing how this term improves long-term distribution alignment?\n\n7. The paper claims that experts specialize in different physical mechanisms. How was this assessed? Could the authors provide quantitative evidence (e.g., entropy of gating distribution, clustering by system type)? Would freezing or pruning certain experts degrade performance in predictable ways?\n\n8. Regarding fairness and scope of baselines, why were physics-based baselines such as Neural ODE, Koopman networks, or DeepONet not included? Would the authors expect ChaosNexus to outperform them on canonical chaotic benchmarks (Lorenz-63, Rössler, Lorenz-96)?\n\t\n9. Could the authors report total training compute (GPU type, training time, FLOPs, or energy use)? How does the efficiency of ScaleFormer compare to conventional Transformers for the same sequence length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9G3bg9xshF", "forum": "3icvqeC1sA", "replyto": "3icvqeC1sA", "signatures": ["ICLR.cc/2026/Conference/Submission17691/Reviewer_DRuT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17691/Reviewer_DRuT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915562979, "cdate": 1761915562979, "tmdate": 1762927536274, "mdate": 1762927536274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}