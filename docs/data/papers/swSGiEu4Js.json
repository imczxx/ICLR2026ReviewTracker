{"id": "swSGiEu4Js", "number": 5916, "cdate": 1757946151501, "mdate": 1759897945091, "content": {"title": "TAMP: Task-aware Multimodal Pre-Interaction for fine-grained Large Language Models", "abstract": "Current Multimodal Large Language Models (MLLMs) primarily rely on image-level visual-linguistic alignment, limiting their capability in fine-grained visual perception tasks. Existing solutions either serialize coordinates as text inputs, which lose spatial semantics, or introduce specialized expert modules that increase inference latency and exhibit task bias. To address these limitations, we propose TAMP, a Task-aware Multimodal Pre-Interaction for Fine-Grained Multi-modal LLMs, that automatically recognizes key task-relevant information from instructions and extracts corresponding region features through an  unified and detector-free paradigm. A task-aware region connector with a dual-branch is designed that dynamically handles both referring and grounding tasks. By introducing a instruction template with region placeholders, we seamlessly integrate fine-grained region features into the LLM's reasoning process. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on both referring and grounding benchmarks while maintaining strong general VQA capabilities.", "tldr": "", "keywords": ["Fine-grained", "Multimodal Large Language Model", "Detector-free"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed5517eb0ddca646bc100788bd604d7f2c188f18.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a Task-aware Multimodal Pre-Interaction Framework (TAMP) aimed at enhancing fine-grained visual perception in multimodal large language models (MLLMs).  The proposed task-aware region connector with a dual-branch architecture allows for dynamic extraction of task-relevant region features. Experiments are conducted on several referring and grounding benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The writing is good. The paper is easy to follow."}, "weaknesses": {"value": "[1] In Fig. 1, the authors mention that the position response of existing methods is inherently adept at processing discrete symbols, but\nlacks capabilities for modeling continuous spatial coordinates. However, the proposed methods still rely on the discrete symbols to encode coordinates with text. Moreover, compared with ROIAlign-based method, the differences of the proposed method are the cross-attention on the visual features. The proposed method cannot address the challenges of existing methods.\n\n[2] A task-aware instruction template cannot be a contribution.\n\n[3] It seems that the proposed task-aware region connector cannot be used for conventional VQA, limiting the application of the proposed methods.\n\n[4] The method still relies on the 224px visual encoder. However, the more recent MLLM can deal with high resolution, e.g., Qwen-VL 2.5, LLava-ov. The compared methods are old."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vr6GqAt2VH", "forum": "swSGiEu4Js", "replyto": "swSGiEu4Js", "signatures": ["ICLR.cc/2026/Conference/Submission5916/Reviewer_aVSo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5916/Reviewer_aVSo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761049305224, "cdate": 1761049305224, "tmdate": 1762918348633, "mdate": 1762918348633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a task-aware multimodal pre-interaction for fine-grained MLLMs, which extract key task-relevant information from instructions and according region features. By employing a instruction template with region placeholders, fine-grained region information is integrated into the reasoning process. Extensive experiments demonstrate the effectiveness of the proposed method on referring and grounding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to understand. The figure 1 and 2 are clear to understand the motivation and the whole picture of the proposed method.\n\n2. The proposed method is simple yet effective to improve the fine-grained capability of MLLMs. The proposed task-aware region connector is somewhat novel to integrate fine-grained region feature.\n\n3. Extensive experiments demonstrate the effectiveness of the proposed method on referring and grounding benchmarks."}, "weaknesses": {"value": "1. The base model for experiments is out of date. CLIP ViT-L-224px and LLaMA-2-7B suffers very limited performance for evaluating the effectiveness of the proposed method. I suggest the author to conduct experiments with siglip2-384 and qwen2.5-7b to truly validate the effectiveness.\n\n2. In addition to Lora training, I suggest the authors to conduct the full training to prove the effectiveness in commonly used training settings."}, "questions": {"value": "How much data was used for pre-training and sft? Is there some grounding VQA data used for training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UKTNps2DnN", "forum": "swSGiEu4Js", "replyto": "swSGiEu4Js", "signatures": ["ICLR.cc/2026/Conference/Submission5916/Reviewer_DaGz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5916/Reviewer_DaGz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891314361, "cdate": 1761891314361, "tmdate": 1762918348039, "mdate": 1762918348039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the fine-grained visual perception capabilities of multimodal large language models (MLLMs) and proposes a dual-branch, task-aware region connector to enhance performance. The module automatically identifies task-relevant information from user instructions, extracts corresponding regional features, and integrates them into the instruction following a task-specific template. Experimental results demonstrate improved performance on downstream tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is intuitive and easy to implement."}, "weaknesses": {"value": "**Limited applicability:**  The method is only designed and evaluated on grounding and referring expression tasks. However, modern MLLMs are expected to handle a wide range of complex tasks, such as reasoning, generation, or editing. The proposed approach lacks generalizability and may not be easily adapted to these scenarios.\n\n\n**Lack of complexity analysis:** The paper does not analyze the computational or memory overhead introduced by the additional module and training. A thorough comparison of training cost and inference efficiency with other methods is necessary to fairly evaluate the trade-off between performance and complexity.\n\n\n**Insufficient experimental evaluation:** The experiments are mainly conducted on VQA and RefCOCO datasets. Performance on other widely used benchmarks such as MMMU, POPE, or MMBench is not reported. A more comprehensive evaluation across diverse datasets is needed to validate the general effectiveness of the method.\n\n\n**Limited novelty:** The proposed method resembles an attention mechanism over image regions based on textual input. However, the design is relatively straightforward and requires additional training and inference overhead, which diminishes its novelty and practical appeal."}, "questions": {"value": "Can the proposed method be compared with saliency-based approaches? For instance, instead of using the proposed connector, could one directly use attention maps between text and image regions to select relevant features, thereby avoiding extra modules and training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wOga88UP0E", "forum": "swSGiEu4Js", "replyto": "swSGiEu4Js", "signatures": ["ICLR.cc/2026/Conference/Submission5916/Reviewer_psuK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5916/Reviewer_psuK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894287398, "cdate": 1761894287398, "tmdate": 1762918347750, "mdate": 1762918347750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies fine-grained visual recognition based on Multimodal Large Language Models (MLLMs). The basic idea is to generate a task-aware region token as input to LLM. Experiments show boosted performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-motivated and fig.1 clearly shows the differences of illustrated frameworks. \n2. The proposed method is reasonable, i.e., to acquire more regional cues which might be important for fine-grained visual recognition.\n3. Experiments are conducted on multiple datasets, and shows better performance than many existing works."}, "weaknesses": {"value": "1. One of concerns is the computational overhead, which is not discussed in depth in the paper. \n2. The strong performance is achieved based on strong baselines. Compared with the baseline, the performance enhancement seems marginal in some cases. \n3. Another important concern is the limited generalization capability. This paper is limited to distinguishing between referring and grounding. This degrades the generalization capability of MLLM, although boosts its performance in specific tasks. It would be important to see if this framework could be extended to other fine-grained tasks."}, "questions": {"value": "1. Need to provide more indepth discussion and comparison on efficiency and computational overhead.\n2. It is important to show illustrations of the effectiveness of learned task-aware region token, and compare the learned tokens of referring and grounding.\n3. Need to clarify the limited performance enhancement and generalization capability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YWTKIUyh0U", "forum": "swSGiEu4Js", "replyto": "swSGiEu4Js", "signatures": ["ICLR.cc/2026/Conference/Submission5916/Reviewer_vZBM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5916/Reviewer_vZBM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046065837, "cdate": 1762046065837, "tmdate": 1762918347303, "mdate": 1762918347303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}