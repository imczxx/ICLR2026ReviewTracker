{"id": "EXiQr9FBqH", "number": 11455, "cdate": 1758199422843, "mdate": 1759897574404, "content": {"title": "CausalKANs: interpretable treatment effect estimation with Kolmogorov-Arnold networks", "abstract": "Deep neural networks achieve state-of-the-art performance in estimating heterogeneous treatment effects, but their opacity limits trust and adoption in sensitive domains such as medicine, economics and public policy. Building on well-established and high-performing causal neural architectures, we propose _causalKANs_, a framework that transforms neural estimators of _conditional average treatment effects_ (CATEs) into Kolmogorov–Arnold Networks (KANs). By incorporating pruning and symbolic simplification, causalKANs yields interpretable closed-form formulas while preserving predictive accuracy. Experiments on benchmark datasets demonstrate that causalKANs perform on par with neural baselines in CATE error metrics, and that even simple KAN variants achieve competitive performance, offering a favorable accuracy–interpretability trade-off. By combining reliability with analytic accessibility, causalKANs provide auditable estimators supported by closed-form expressions and interpretable plots, enabling trustworthy individualized decision-making in high-stakes settings. We release the code for reproducibility.", "tldr": "We propose causalKANs, a framework that transforms neural-based causal estimators into KAN-based estimators that are interpretable by construction.", "keywords": ["causality", "interpretability", "causal inference", "KAN", "treatment effects", "potential outcomes"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b663e4d125cdcad7f6eeb9c62f738960930def13.pdf", "supplementary_material": "/attachment/c10d3a62d4b184b3647f4a355405ecdd7fc89a74.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces causalKANs, a framework that transforms deep neural network estimators of conditional average treatment effects (CATEs) into Kolmogorov–Arnold Networks (KANs) to improve interpretability. By applying pruning and symbolic simplification, causalKANs produce closed-form, interpretable formulas while maintaining predictive accuracy and achieving a balance between performance and transparency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- S1: Replacing MLPs with KANs in causal inference is well-suited, as KANs can model complex relationships with fewer parameters and improved interpretability, which supports more transparent estimation of treatment effects.\n\n- S2: The proposed probability radar plots (PRPs) and partial dependence plots (PDPs) effectively illustrate the causal interpretability of the model in a clear and intuitive manner.\n\n- S3: The paper is clearly written and easy to follow, with precise explanations and well-documented details throughout."}, "weaknesses": {"value": "- W1: In Section 4, the paper’s main framework contribution includes a function simplification pipeline based on pruning and auto-symbolic search, which iteratively modifies model edges and performs revalidation on the validation set or even retraining. This procedure appears to impose substantial computational overhead, yet the paper provides no discussion of its complexity or efficiency. Including a complexity analysis similar to that used in tree-pruning methods would make the proposed approach appear more rigorous and practical.\n\n- W2: The paper argues that simplifying KANs improves interpretability but implicitly introduces new assumptions about the dataset, such as the additivity assumption discussed at line 957. However, as shown in the IHDP-B experiment at line 395, replacing MLPs with KANs leads to poorer performance on heterogeneous datasets. The paper does not address how to validate these assumptions or guide model selection. Incorporating validation methods such as residual analysis or interaction tests could help assess their validity more systematically.\n\n- W3: At line 1050, the paper uses test loss as the main model selection criterion. However, this metric may overemphasize outcome prediction accuracy and risk overfitting, rather than reflecting true treatment effect accuracy. As shown in Figure 11, PEHE appears to plateau as test loss increases, suggesting limited causal improvement despite better predictive fit."}, "questions": {"value": "- Q1: At line 255, multiple regularization terms and $\\lambda$ coefficients are introduced to control model complexity, but their selection is not discussed. Could the authors clarify how these values were determined or tuned?\n\n- Q2: At line 1006, the experiment shows a weak correlation between model complexity and PEHE error. However, the definition of complexity lacks clear quantitative specification across different model settings. Could the authors provide more explanation, for instance, why “the weight penalty $\\lambda$ contributes 1 if set to 0.01 and 2 otherwise”?\n\n- Q3: At line 1407, replacing MLPs with KANs in the S-learner results in significantly slower performance compared to other models. Could the authors provide an explanation for this behavior?\n\n- Q4: At line 1350 (Table 2), pruning appears to cause disastrous prediction errors on some datasets. Could the authors explain whether such degradation is expected or reasonable under their pruning criteria and stopping rules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u6GLdfhyFC", "forum": "EXiQr9FBqH", "replyto": "EXiQr9FBqH", "signatures": ["ICLR.cc/2026/Conference/Submission11455/Reviewer_aDc3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11455/Reviewer_aDc3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378636571, "cdate": 1761378636571, "tmdate": 1762922567544, "mdate": 1762922567544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors integrate Kolmogorov–Arnold Networks (KANs) into causal learning frameworks including the S-learner, T-learner, TARNet, and DragonNet to enhance interpretability while maintaining competitive accuracy in estimating conditional average treatment effects (CATEs). The proposed approach is empirically validated on benchmark datasets, including ACIC and IHDP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well-written, and the interpretability analyses such as the radar plots and partial dependence plots are clear, intuitive, and effectively communicate the model’s behavior to the audience."}, "weaknesses": {"value": "Despite its strengths, the paper exhibits several significant weaknesses.\n\nWeakness 1 – Limited baseline comparisons: although the paper presents an interesting approach, it includes only a small set of baseline models—specifically TARNet, S-learner, T-learner, and DragonNet for evaluating estimation accuracy. However, numerous well-established alternatives exist, such as the DML, DR-learner, Ganite, and X-learner frameworks. For a paper without a solid theoretical contribution and aiming for a top-tier machine learning venue, it is essential to conduct a more comprehensive empirical comparison against a broader range of state-of-the-art methods to substantiate the claimed performance advantages.\n\nWeakness 2 – Limited dataset diversity: The paper evaluates the proposed method solely on IHDP and ACIC, both of which are tabular, semi-synthetic datasets. This narrow experimental scope limits the generalizability and robustness of the findings. To strengthen the empirical contribution, the authors should consider incorporating a wider range of datasets, such as TCGA (additional tabular data), EEEC (text-based data), and Uganda (image data), following the precedent set by Wu et al. (2024), “Discret: Synthesizing Faithful Explanations for Treatment Effect Estimation,” Proceedings of Machine Learning Research, 235, 5359–97. Again, for a paper positioned as an applied contribution at a top-tier machine learning venue, it is crucial to demonstrate comprehensive experimentation across diverse data modalities.\n\nWeakness 3- Limitations of KANs: following the weakness 2, one of the key motivations for using Kolmogorov–Arnold Networks (KANs) is their potential interpretability, as each edge represents a learnable univariate function that can be visualized and analyzed. However, the flexibility of per-edge spline functions may lead to overfitting or highly irregular learned mappings, particularly when applied to complex modalities such as image or text data. As discussed in Somvanshi et al. (2025), “A Survey on Kolmogorov–Arnold Networks,” ACM Computing Surveys, 58(2), 1–35, this challenge highlights the importance of validating the proposed framework on more diverse datasets including image and text-based benchmarks to ensure the robustness and scalability of causalKANs beyond simple tabular settings.\n\nWeakness 4 – Limited novelty of CausalKANs: the core idea of enhancing interpretability in treatment effect estimation through additive neural architectures is not entirely novel. Similar concepts have been explored in prior work, such as Chen, K., Yin, Q., & Long, Q. (2023). “Covariate-Balancing-Aware Interpretable Deep Learning Models for Treatment Effect Estimation,” Statistics in Biosciences, 17(1), 132–150, where the authors employed neural additive models to estimate treatment effects. Since neural additive models (NAMs) can be viewed as a special case of Kolmogorov–Arnold Networks (KANs), the contribution of CausalKANs appears incremental rather than fundamentally new. A clearer discussion of how CausalKANs advance beyond existing NAM-based causal estimators would strengthen the paper’s methodological originality.\n\nWeakness 5 – Limited interpretability across other estimator types: following the weakness 1, the main advantage of causalKANs seems to be improving interpretability for difference-in-mean estimators of treatment effects. However, it is not clear how this approach would enhance interpretability for other common estimators that rely on inverse probability weighting or doubly robust methods, such as the X-learner and DR-learner. Even for DragonNet with target regularization (see Section 3 of Shi, C., Blei, D., & Veitch, V. (2019), NeurIPS), the paper does not clearly explain how causalKANs make these models more interpretable. Providing a clearer discussion or examples showing how causalKANs could be applied to these estimators would make the paper stronger and more convincing.\n\nWeakness 6 – Unclear reporting of evaluation metrics: the paper does not specify whether the reported empirical results reflect in-sample or out-of-sample errors. This distinction is important for assessing model generalizability and potential overfitting. It would strengthen the paper to report both in-sample and out-of-sample performance for all experiments, allowing readers to better understand the robustness of the proposed approach.\n\nOverall, this is a clearly written and interesting paper. The proposed idea is promising, and the experiments are presented in an organized way. However, the paper could be significantly strengthened by either expanding the experimental evaluation to include a wider range of baseline methods and data modalities or by adding a solid theoretical component, such as a generalization error bound. In its current form, I do not think the paper meets the standard required for acceptance at ICLR 2026."}, "questions": {"value": "Several points:\n\n1.) Figure 2 presents box plots comparing MLP- and KAN-based models. From my perspective, there is no clear evidence that KANs consistently outperform MLPs, except perhaps for the T-learner. In some cases, such as DragonNet, KANs even appear to exhibit higher variance. It would be helpful if the authors could provide a deeper discussion of the observed bias–variance trade-off between MLP and KAN architectures in the context of treatment effect estimation.\n\n2.) In Line 143, the paper states that $E[y \\mid x, t]$ is an unbiased estimator of $\\mu_t(x)$.  This appears to be a typo or conceptual inaccuracy, as an estimator should be data-driven  and cannot be expressed in terms of an expectation. The authors may wish to revise this statement  to clarify that $E[y \\mid x, t]$ equals $\\mu_t(x)$ under the standard causal assumptions,  rather than serving as its estimator."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "olsRfJ1kMS", "forum": "EXiQr9FBqH", "replyto": "EXiQr9FBqH", "signatures": ["ICLR.cc/2026/Conference/Submission11455/Reviewer_M51h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11455/Reviewer_M51h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725402320, "cdate": 1761725402320, "tmdate": 1762922567136, "mdate": 1762922567136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel framework for causal inference, transforming opaque neural CATE estimators into closed-form formulas using Kolmogorov-Arnold Networks (KANs). The claim is extracting human-readable (accept/reject gates, explicit budgets) causal effect formulas from neural networks. The algorithm works with standard architectures e.g. s-learner, r-learner, DragonNet. The paper claims competitive predictive performance that matches or exceeds neural baselines. The authors also provide analysis and visualization across multiple benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes an innovative algorithm with KANs which replace neural networks in causal learning architectures. This makes the algorithm more interpretable \n- The algorithm with KAN generally performs better or matches the performance of neural networks, tested on multiple datasets \n- The overall approach has significant potential, and i'd be interested to know how KAN can contribute to solving novel tasks that are difficult for NN based causal learning"}, "weaknesses": {"value": "- The paper may be seen as slightly incremental due to the plug-in of KANs to replace neural networks, and the algorithm itself didn't have other innovations. The contribution seems more towards interpretation and validation in experiments \n- More datasets quoted in previous work may help strengthen the paper \n\ne.g. \n[1] Rankability-enhanced revenue uplift modeling framework for online marketing. He, Bowei and Weng, Yunpeng and Tang, Xing and Cui, Ziqiang and Sun, Zexu and Chen, Liang and He, Xiuqiang and Ma, Chen. \n\n- ablation studies and more examples of interpretability offered by KAN's specific to the task/dataset will strengthen the paper."}, "questions": {"value": "- could more dataset comparison be included in the paper \n- I think the interpretation with accept/reject gate etc. is a bit less well explained. Could you give examples and interpret the architecture or signals given a dataset and task? \n- are there tasks you think KAN with causal learning/HTE may particularly excel at, ones that are challenging for NNs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SLSN7Wlb52", "forum": "EXiQr9FBqH", "replyto": "EXiQr9FBqH", "signatures": ["ICLR.cc/2026/Conference/Submission11455/Reviewer_CfPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11455/Reviewer_CfPa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977992954, "cdate": 1761977992954, "tmdate": 1762922566702, "mdate": 1762922566702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents CausalKAN framework with code contributions that address treatment effect estimation with interpretable results. This has applications in sensitive decision making systems to understand covariates relevant to individual treatment effects."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Originality: While the paper is well written and the interpretability angle is valuable, the core technical ideas appear close to KANITE [1], which targets performance rather than interpretability. \n\nQuality & clarity: With its key contribution as interpretable results, I found the exposition clear and well organized. The manuscript delineates the symbolic simplification and pruning techniques in a reproducible way (definitions, criteria, and algorithmic steps are explicit) and demonstrates how they lead to interpretable models via semi-synthetic experiments.\n\nSignificance: The work is a useful step toward interpretable effect estimation for sensitive decision-making (e.g., healthcare, lending, education). While the technical novelty is not groundbreaking, the paper’s pipeline makes ITE models more transparent and could aid auditing and deployment. The current evidence (largely semi-synthetic) is promising but would benefit from stronger identifiability statement for interpretable symbolic regression (see below for details). \n\n[1] E. Mehendale, A. Thorat, R. Kolla, and N. Pedanekar. Kanite: Kolmogorov-arnold networks for ite estimation. arXiv preprint arXiv:2503.13912, 2025"}, "weaknesses": {"value": "*Evaluation for interpretability/faithfulness.* I recommend including symbolic CATE benchmarks: construct SCMs with known closed-form symbolic CATE $\\tau(x)$ and report whether KAN’s simplified/pruned representation recovers the underlying expression (up to algebraic equivalence) alongside prediction error. This directly evaluates faithfulness, which is key for trustworthy use in sensitive decisions, and would make the interpretability claim much more convincing."}, "questions": {"value": "See above. I would be interested to see how CausalKAN address the faithfulness of the interpretable results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iVbwYqKXvI", "forum": "EXiQr9FBqH", "replyto": "EXiQr9FBqH", "signatures": ["ICLR.cc/2026/Conference/Submission11455/Reviewer_Gp2j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11455/Reviewer_Gp2j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988196201, "cdate": 1761988196201, "tmdate": 1762922566160, "mdate": 1762922566160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}