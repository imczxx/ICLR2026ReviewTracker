{"id": "L5llQD0nMf", "number": 12022, "cdate": 1758205234594, "mdate": 1759897538756, "content": {"title": "TP-Spikformer: Token Pruned Spiking Transformer", "abstract": "Spiking neural networks (SNNs) offer an energy-efficient alternative to traditional neural networks due to their event-driven computing paradigm. However, recent advancements in spiking transformers have focused on improving accuracy with large-scale architectures, which require significant computational resources and limit deployment on resource-constrained devices. In this paper, we propose a simple yet effective token pruning method for spiking transformers, termed TP-Spikformer, that reduces storage and computational overhead while maintaining competitive performance. Specifically, we first introduce a heuristic spatiotemporal information-retaining criterion that comprehensively evaluates tokens' importance, assigning higher scores to informative tokens for retention and lower scores to uninformative ones for pruning. Based on this criterion, we propose an information-retaining token pruning framework that employs a block-level early stopping strategy for uninformative tokens, instead of removing them outright. This also helps preserve more information during token pruning. We demonstrate the effectiveness, efficiency and scalability of TP-Spikformer through extensive experiments across diverse architectures, including Spikformer, QKFormer and Spike-driven Transformer V1 and V3, and a range of tasks such as image classification, object detection, semantic segmentation and event-based object tracking. Particularly, TP-Spikformer performs well in a training-free manner. These results reveal its potential as an efficient and practical solution for deploying SNNs in real-world applications with limited computational resources.", "tldr": "We propose a simple yet effective token pruning method for spiking transformers, termed TP-Spikformer, that reduces storage and computational overhead while maintaining competitive performance. TP-Spikformer performs well in a training-free manner.", "keywords": ["Spiking Neural Networks", "Efficient Spiking Neural Network", "Pruning Spiking Neural Network"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1fb5186031978f401a2aa9638ca5a7f732768e4b.pdf", "supplementary_material": "/attachment/76afff00baa067af1806b2f064a430eb39b19362.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents TP-Spikformer, a method to speed up Spiking Transformers by pruning unimportant tokens. Its main ideas are: 1) a new scoring rule called IRToP, which finds important tokens by checking if they look different from their neighbors (space) and if they change over time (time); and 2) a pruning architecture called IR-Arc, which makes unimportant tokens \"skip\" the heavy computation in a block instead of deleting them. This keeps the model's structure intact. Experiments show the method works well on many models and tasks, and impressively, it boosts speed with almost no accuracy loss even without any retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is very versatile. It works on simple Spiking Transformers, complex ones with convolutions (like QKFormer), and on many different tasks like classification and detection. This shows it's a useful, general tool.\n2. The biggest plus is its \"training-free\" ability. This is very practical for real-world use where retraining is too expensive.\n3. The paper backs up its claims with strong experiments. The ablation studies clearly show why each part of the design is necessary, and the results on big datasets like ImageNet prove that the speedup is real."}, "weaknesses": {"value": "1. The paper doesn't have a good, automatic way to decide how many tokens to prune in each layer. For some models, they just set the numbers by hand. For others, they run a slow and expensive \"grid search\" beforehand. The reported speedup doesn't include this search time, which makes the method seem faster than it is to set up.\n2. The method prunes at the whole \"block\" level. A token either skips both the Attention and MLP parts, or does both. It might be better to prune them separately (e.g., prune more tokens for Attention and fewer for MLP) to get a better speed-vs-accuracy balance.\n3. The paper only considers skipping tokens. Another popular technique is \"token merging,\" where several similar tokens are combined into one. This might preserve more information than just letting old tokens pass through unchanged. The paper should have compared its method to token merging.\n4. All experiments were on \"directly trained\" SNNs. It's unclear if the method would work on ANN-to-SNN conversion."}, "questions": {"value": "1. How long does the grid search for finding the pruning ratios actually take? Have you thought about a simpler or automatic way to find these numbers?\n2. Could you explain why you chose to skip tokens instead of merging them?\n3. Do you think your method would work for SNNs that are converted from ANNs? For example, you can apply your method to \"Masked Spiking Transformer\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4WLKSPClx4", "forum": "L5llQD0nMf", "replyto": "L5llQD0nMf", "signatures": ["ICLR.cc/2026/Conference/Submission12022/Reviewer_BtDR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12022/Reviewer_BtDR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761431001998, "cdate": 1761431001998, "tmdate": 1762923007138, "mdate": 1762923007138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed an information retaining token pruning framework for spiking transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposed an information retaining token pruning framework for spiking transformers. \n\n2. The writing in this paper is good.\n\n3. The experiments are quite extensive, including classification, segmentation, detection, and tracking tasks."}, "weaknesses": {"value": "1. My main concern lies in the motivation. Currently, directly trained spiking Transformers are relatively small- or medium-scale models. Although pruning slightly reduces performance while improving throughput（eg, TP-Spikformer with SDT-V1-8-768 on imagenet: -1.53% acc, thr 29%）， this trade-off does not constitute a strong motivation.\n\n2、There is a lack of discussion on overall model training costs, such as training time and memory consumption.\n\n3、There is a lack of spike-driven characteristics and energy consumption discussion on the heuristic spatiotemporal information-retaining criterion."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Z9m1HPTMe7", "forum": "L5llQD0nMf", "replyto": "L5llQD0nMf", "signatures": ["ICLR.cc/2026/Conference/Submission12022/Reviewer_G33Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12022/Reviewer_G33Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724370054, "cdate": 1761724370054, "tmdate": 1762923006690, "mdate": 1762923006690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TP-Spikformer, a simple and effective token pruning method designed to reduce the high computational and storage costs of large-scale spiking transformers, making them more suitable for resource-constrained devices. The core of the method consists of two main contributions. First, it proposes a heuristic Information-Retaining Token Pruning (IRToP) criterion, which scores and identifies important tokens by evaluating both their spatial distinctiveness from neighbors and their temporal variation across time steps . Second, it introduces the Information-Retaining Architecture (IR-Arc), which, instead of permanently dropping unimportant tokens, applies a block-level early stopping strategy—uninformative tokens bypass computation within a block and are then reassembled with the processed informative tokens. A significant advantage of this approach is its versatility and efficiency, as it requires no retraining or architectural modifications, achieving competitive performance even with zero fine-tuning. The authors demonstrate the method's effectiveness, efficiency, and scalability across diverse architectures (like Spikformer, QKFormer, and SDT) and a range of visual tasks, including classification, detection, segmentation, and tracking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents a robust and highly impactful contribution, demonstrating exceptional strengths across originality, quality, clarity, and significance. The work's originality is outstanding, introducing a novel, training-free token pruning framework for spiking transformers, which stands in stark contrast to prior methods that require costly retraining and architectural modifications. The core ideas are creative and well-motivated: the bio-inspired IRToP criterion offers a new heuristic for identifying important tokens based on both spatial saliency and temporal dynamics, while the IR-Arc architecture cleverly uses a block-level early stopping and reassembly strategy instead of direct token removal, making it compatible with modern hierarchical SNNs. The research quality is superb, validated through extensive experiments across a diverse set of architectures (Spikformer, SDT-V1/V3, QKFormer) and a wide range of tasks, including classification, detection, segmentation, and tracking. The results consistently show significant reductions in computational overhead with minimal performance degradation, and thorough ablation studies rigorously justify the design choices. The paper is presented with excellent clarity, logically progressing from a well-defined problem to an elegant solution, with effective visualizations and a clear algorithm to aid understanding. Finally, the work is of high significance as it provides a simple, practical, and broadly applicable tool that addresses a critical bottleneck in deploying large-scale SNNs. By enabling efficient compression without retraining, TP-Spikformer dramatically lowers the barrier for applying spiking transformers in resource-constrained, real-world scenarios, marking a key step forward for the field of energy-efficient neuromorphic computing."}, "weaknesses": {"value": "1. The paper would be strengthened by providing further experimental results, such as from an entropy or visualization perspective, to more rigorously justify the effectiveness of the Spatial and Temporal token scorers in measuring token importance.\n2. The authors state that TP-Spikformer performs well even without training, but its accuracy drops significantly when using QKFormer and SDT-V3."}, "questions": {"value": "1. Can the proposed method be applied to train a spiking transformer from scratch?\n2. How could the proposed method be generalized to non-grid-like modalities?\n3. How well does the proposed method preserve key information during pruning?What are the key differences between token pruning in spiking transformers and conventional ANN-based transformers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KF6kfA1mVV", "forum": "L5llQD0nMf", "replyto": "L5llQD0nMf", "signatures": ["ICLR.cc/2026/Conference/Submission12022/Reviewer_r4rD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12022/Reviewer_r4rD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793080442, "cdate": 1761793080442, "tmdate": 1762923006335, "mdate": 1762923006335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a token pruning method for spiking transformers. First, a heuristic spatio-temporal information-retaining criterion is proposed to evaluate token importance. Second, based on the importance scores, a block-level early stopping strategy is proposed for uninformative tokens. Experiments are conducted on Spikformer, QKFormer and Spike-driven Transformer V1 and V3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The narrative is very convincing that token-level sparsification is a very effective  way for transformer-style models to boost computational  efficiency.\n\n2. This paper's experiments are very solid and extensive. The token sparsification method is verified across different tasks and multiple datasets, which makes the effectiveness very convincing.\n\n3. This work demonstrates the most advanced token  pruning results for spiking ViT in comparison with previous spiking token  pruning methods."}, "weaknesses": {"value": "The substantive contributions of this paper significantly overlap with previous work on ANN transformer sparsification, demonstrating limited novelty. It essentially replicates the success of existing ANN transformer sparsification approaches, with even the narrative framework bearing striking resemblances.\n\n1. The spatial token scorer is highly similar to the ANN token-pruning one [1]. Thus, the novelty of this paper is significantly challenged.\n\n2. The ablation study in Table 6 is conducted on ImageNet, a static dataset with no temporal disparity, thus I cannot confirm the effectiveness of the temporal pruning scorer. The visualization in Figure 5 does not entirely prove the effectiveness of the temporal pruning scorer. Instead, it exhibits limitations in spatial perception of the spatial scorer. \n\n3. It is very counterintuitive that the temporal scorer is used to prune spatial token rather than prune temporal token. Moreover, temporal pruning is applied regardless of whether the dataset is dynamic or static.\n\n4. The retaining is already existent. This is also called token emerger technique. Or, retaining can be seen as a naive token emerger that can restore non-informative tokens for future use. I can easily give an example of such retaining [2].\n\nIn short, this paper shows severe duplication of ANN token-pruning domain. In essense, the success of this paper's method is inherited from the ANN  token-pruning  works. Such that this paper's contribution is limited. Moreover, the spatial token pruning may be even better if the most advanced ANN token pruning is applied.\n\nThis is more like a very good technique report concerning the verification of ANN methods applied to SNNs.\n\n[1] Similarity-Aware Token Pruning: Your VLM but Faster\n\n[2] HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers"}, "questions": {"value": "What's the main contributions from the methodology perspective?\n\nWhy not cite the ANN transformer sparsification papers which this paper's main techniques originate from? Similarity-based pruning has already existed, even the token-level sparsification has already been researched for years. But no citation is found in this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XKpf7CpCFB", "forum": "L5llQD0nMf", "replyto": "L5llQD0nMf", "signatures": ["ICLR.cc/2026/Conference/Submission12022/Reviewer_TA1d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12022/Reviewer_TA1d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888418319, "cdate": 1761888418319, "tmdate": 1762923005956, "mdate": 1762923005956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}