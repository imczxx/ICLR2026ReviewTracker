{"id": "omxSBcv8TF", "number": 16139, "cdate": 1758260491558, "mdate": 1759897259030, "content": {"title": "Teaching Language Model to Act Efficiently", "abstract": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with the ability to invoke external tools during long-form reasoning, such as search engines and code interpreters, to solve tasks beyond the capabilities of internal reasoning.\nWhile reinforcement learning (RL) has shown promise in training such agents, most of existing approaches typically optimize only for final correctness without considering the efficiency or necessity of external tool use. This often leads to excessive tool calling, leading to increased computational costs and additional latency, and may also shift reliance toward external tools rather than the model’s own reasoning -- a phenomenon referred to as \\textit{cognitive offloading}. To this end, we propose Optimized Tool Call-controlled Policy Optimization (OTC-PO), a simple yet effective RL-based framework that encourages models to produce accurate answers with less tool calls. Our method introduces a tool-integrated reward that jointly considers answer correctness and corresponding tool use behavior of model to reach that answer. To validate the effectiveness, we introduce the additional metric of \\textit{tool productivity}, defined as the ratio between the number of correct answers and the total number of tool calls across all test cases. This metric reflects how efficiently and effectively tool usage contributes to successful task completion, with higher values indicating more productive external tool calls with the help of internal reasoning. We then instantiate this framework within both Proximal Policy Optimization (PPO) and Group Relative Preference Optimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach reduces tool calls by up to 68.3\\% and improves tool productivity by up to 215.4\\%, while maintaining comparable answer accuracy, especially for the larger models.", "tldr": "A simple, scalable, and generalizable OTC-PO algorithm to encourage the model to use less tool calls to solve the problem and maximize the tool productivity", "keywords": ["Language Agents", "Tool-integrated Reasoning", "Inference Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0357e77cab700338cad8d5cafae53a2bde84b508.pdf", "supplementary_material": "/attachment/cbfef989f0084b46965da9bc0e545d08501e69f3.zip"}, "replies": [{"content": {"summary": {"value": "The authors investigate tool-integrated reasoning large language models, and introduce the Optimized Tool Call-controlled Policy Optimization (OTC-PO) method to handle the cognitive offloading issue. The core to OTC-PO is a novel reward function that relates to tool calls. By using the proposed reward function, OTC-PO  not only improves the performances but also reduce the number of tool calls."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and easy to understand.\n- The authors provide sufficient motivation for the proposed reward function.\n- Extensive experiments are conducted to validate the effectiveness of **OTC-PO**."}, "weaknesses": {"value": "- The reviewer would like to know the practical cost of tool invocation. According to **Table 1**, **Search-R1-PPO** calls the tool up to three times, while **OTC-PPO** also requires at least one call. What is the actual time savings achieved in this case? This investigation is essential to assess the real-world significance of the proposed method.\n- The reviewer suggests conducting experiments on larger-scale models, if computational resources permit, to further verify the scalability of the approach.\n- An important related work on **tool-augmented LLMs** [1] is missing from the discussion in the paper.\n\n[1] Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. NeurIPS 2024."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jwbZUG7HM3", "forum": "omxSBcv8TF", "replyto": "omxSBcv8TF", "signatures": ["ICLR.cc/2026/Conference/Submission16139/Reviewer_oxjH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16139/Reviewer_oxjH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662375610, "cdate": 1761662375610, "tmdate": 1762926307764, "mdate": 1762926307764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OTC-PO, an RL framework that trains tool-using LLM agents to optimize both answer correctness and efficiency of tool use. The key idea proposed by the authors is a tool-integrated reward that multiplies a correctness term with a tool-efficiency coefficient that favors trajectories reaching correct answers with fewer tool calls.\n\nTool-integrated agents increasingly dominate practical workflows. Optimizing only EM produces agents that over-call tools, inflate latency, and offload trivial reasoning to external systems. This paper reframes training as a compute-aware objective and introduces a concrete, RL-compatible shaping that pushes models toward judicious tool use without hand-crafted rules or heavy SFT. The result is a simple, plug-and-play recipe that system builders can directly try in search and code agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The multiplicative tool-efficiency factor is easy to add to existing RL pipelines, and the GRPO variant provides a data-driven estimate of the “optimal” call budget per question. This is engineering-friendly and aligns with system constraints. \n\nThe paper shows large TC reductions and strong TP gains on both search and code tool settings, including OOD QA, while preserving accuracy on larger models. This is the right way to argue for deployment. \n\nThe case study and the “reasoning versus acting” analysis are insightful, demonstrating that discouraging gratuitous tools increases internal reasoning, which is exactly the intended effect."}, "weaknesses": {"value": "EM and TP are informative, but many decisions hinge on end-to-end cost. Please add wall-clock latency, prompt token counts including tool observation length, and number of forward passes so that compute is matched across baselines. Some comparisons could still be explained by budget differences. \n\nThe reward uses alpha and a smooth constant c. Provide ablations showing stability, convergence speed, and EM-TP trade-offs as these vary, ideally on two datasets and two backbones. The text hints at PPO versus GRPO stability but does not quantify failure rates or variance. \n\nThe “minimal calls among correct trajectories” is a moving target. Analyze how often the proxy changes during training, whether it biases toward under-search, and how robust it is to spurious lucky trajectories. A small controlled synthetic task where the true optimum is known would help."}, "questions": {"value": "(1) Under equalized prompt tokens, number of model calls, and observation lengths, how do OTC-PPO and OTC-GRPO compare to Search-R1 and ToRL on EM and TP, including wall-clock? A clean, matched table would increase confidence that gains are not budget artifacts. \n\n(2) Show how often the estimated n changes per question during training, and the effect on stability. Include a diagnostic on under-search errors created by too-aggressive penalties. \n\n(3) Can you demonstrate the same shaping works with larger tool menus or multi-tool plans, for example search, code, and calculator together, where the notion of “optimal calls” may be more complex. A small ablation would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The work uses public benchmarks, releases code snippets for reward design, and does not process sensitive personal data or human subjects."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CIs2W4SKrS", "forum": "omxSBcv8TF", "replyto": "omxSBcv8TF", "signatures": ["ICLR.cc/2026/Conference/Submission16139/Reviewer_y9uq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16139/Reviewer_y9uq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687699258, "cdate": 1761687699258, "tmdate": 1762926307336, "mdate": 1762926307336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Optimized Tool Call-controlled Policy Optimization (OTC-PO), a RL framework that trains language models to use external tools more efficiently while maintaining accuracy. It introduces a new reward design and metric, tool productivity, which balances correctness with the number of tool calls. Experiments on multiple benchmarks with Qwen models show that OTC-PO reduces tool calls by up to 68% and improves tool productivity by over 200%, without significant loss in performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- OTC-PO can be plugged into existing RL pipelines with minimal modification. Its reward design is lightweight yet conceptually impactful.\n- Evaluated on multiple tasks (search and code), datasets, and model scales (3B–7B), showing consistent improvements."}, "weaknesses": {"value": "- The main novelty lies in the reward design, specifically the cosine-decay modulation to penalize excessive tool use. While effective, this idea feels heuristic and somewhat ad hoc. It would be stronger if compared with or generalized to other decay strategies (e.g., linear, exponential, adaptive).\n- The paper lacks comparisons with existing length-control methods such as Length-Control Policy Optimization (LCPO) or budgeted reasoning RL approaches.\n-"}, "questions": {"value": "- Could the authors justify why cosine decay is preferable to simpler alternatives (e.g., linear, exponential, or adaptive decay)? Including ablations with different decay types or visualizations of reward curves would help clarify robustness.\n- In Section D, why do the authors revise the system prompt of Search-R1 by adding the instruction “You need to make every search call count and gain helpful results”? What motivates this modification, and does it influence the accuracy, training stability, or comparability of baselines?\n- In Table 1, OTC-GPRO shows huge improvement over Search-R1-GRPO. Could the authors explain it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZIKbqMD27p", "forum": "omxSBcv8TF", "replyto": "omxSBcv8TF", "signatures": ["ICLR.cc/2026/Conference/Submission16139/Reviewer_d63o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16139/Reviewer_d63o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687930792, "cdate": 1761687930792, "tmdate": 1762926306759, "mdate": 1762926306759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Optimized Tool Call-controlled Policy Optimization (OTC-PO), a simple yet effective RL-based framework that encourages models to produce accurate answers with less tool calls. Tool integrated reasoning is framed as jointly optimizing both correctness and efficiency by modulating the reward signal with a scaling coefficient that reflects tool efficiency. Two instantiations of the framework, OTC-PPO and OTC-GRPO, are extensively evaluated on two tools namely search and code to validate the effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally clear and well structured. Figure 1 is a good illustration of the proposed method.\n- The proposed method is simple yet effective and is compatible with various RL methods.\n- Conducted experiments validate the effectiveness in reducing total tool calls with minimal to no loss of efficacy."}, "weaknesses": {"value": "- The technical contribution of OTC-PO centers on scaling the correctness reward by a tool-efficiency coefficient that penalizes trajectories with a larger number of tool calls. While the prompt-adaptive estimation of minimal tool usage is a useful heuristic, conceptually it is an incremental extension of standard length penalties and reward shaping techniques for tool-efficiency.\n\n- A follow-up to Search-R1 [1] demonstrates that incorporating a format reward can substantially improve performance, particularly when training from base LLMs. For example, using Qwen2.5-3B-Base, Search-R1 with a format reward reaches **0.428** (NQ) and **0.371** (HotpotQA) under PPO, and **0.429** (NQ) and **0.372** (HotpotQA) under GRPO. With Qwen2.5-7B-Base, the PPO variant attains **0.488** (NQ) and **0.436** (HotpotQA), while the GRPO variant achieves **0.458** (NQ) and **0.412** (HotpotQA); refer to Table 1 in [1]. These results are competitive with, and in some settings exceed, the performance of OTC-PO variants. A more extensive comparison against such methods would better contextualize the contribution.\n\n- The paper reports single-run results and notes that hyper-parameters are reused from Search-R1. However, LLMs are known to exhibit high run-to-run variance, and sampling parameters can significantly influence outcomes. Without repeated trials and confidence intervals, it is difficult to assess the stability and practical significance of the reported improvements.   \n\n---\n\nReferences\n\n [1] Jin et al., An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents"}, "questions": {"value": "1. Could the authors clarify the sampling parameters used during inference for all reported experiments? Such details are important for reproducibility and for understanding potential performance sensitivity to decoding settings.\n\n2. Minor typos.  Line 106: “Only few of studies” should be revised to “Only a few studies.” Line 157: “generate outputs consists” should be revised to “the generated output, consisting.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Aytm6q8kaW", "forum": "omxSBcv8TF", "replyto": "omxSBcv8TF", "signatures": ["ICLR.cc/2026/Conference/Submission16139/Reviewer_nebb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16139/Reviewer_nebb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937416512, "cdate": 1761937416512, "tmdate": 1762926306360, "mdate": 1762926306360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}