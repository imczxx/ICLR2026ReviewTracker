{"id": "1e42ofAFnO", "number": 448, "cdate": 1756740175863, "mdate": 1759898260620, "content": {"title": "A^2-TFG: Analytical and Adaptive Training-Free Diffusion Guidance", "abstract": "Training-free diffusion guidance is a strategy that uses an unconditional diffusion model and an off-the-shelf property predictor to generate samples with desired characteristics without further training. Typical predictors here are any differentiable functions (\\eg classifiers) that can be used to evaluate the quality of the generated samples. Existing works design the weights of the guidance term using heuristic rules, resulting in fixed guidance weights for different samples. In this paper, we propose Analytical and Adaptive Training-free Guidance (T^2-AFG), which improves upon prior approaches in two aspects: \\textbf{(1) Analytical}: We formulate an optimization objective that provides a closed-form solution for the guidance term weights;\\textbf{ (2) Adaptive}: This closed-form solution varies with different inputs rather than being fixed. Compared to heuristic rules or grid search, these improvements lead to generally better performance, and the closed-form solution also reduces computational costs. We extensively validate the effectiveness of T^2-AFG across six tasks combining three models such as Cat-DDPM, Stable Diffusion, and Audio-Diffusion with various task-specific targets, achieving superior performance over the vanilla TFG across most metrics.", "tldr": "", "keywords": ["Training-Free Diffusion Guidance", "Analytical Solution", "Adaptive Guidance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c892c355ed1a08f4e93a19c3672bf7322d6b2d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an enhancement to a training-free guidance method (TFG) for pretrained diffusion models. The core contribution is a novel technique that replaces TFG's costly hyperparameter search for guidance strength with an analytical, per-sample solution. By deriving the optimal guidance strength at each sampling step, this method offers a valuable trade-off, shifting computational load from a \"pre-inference\" search phase to inference time. This approach has the potential to significantly improve the efficiency of TFG, particularly in scenarios where a hyperparameter search is prohibitive."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-motivated. The efficiency of training-free guidance is a significant practical challenge, and the computational overhead of hyperparameter tuning in the original TFG is a clear bottleneck. The proposed solution—deriving an analytical optimum for the guidance strength—is an elegant and novel approach to this problem. The theoretical derivation appears sound and directly addresses the stated limitation of the prior work."}, "weaknesses": {"value": "1. (False claim of TFG) A critical issue is the characterization of the TFG baseline's hyperparameter search. The paper repeatedly refers to it as a **grid search** (e.g., line 192), which appears to be a misunderstanding. The original TFG paper (Sec. 4, final paragraph) specifies the use of **beam search**. This distinction is crucial and impacts several key aspects of the paper. The authors should revise all related descriptions (including figures) and re-calculate the efficiency comparisons based on the correct baseline search algorithm.\n\n2. (Experiments) The experimental section lacks the necessary detail and support for its claims. \n\n   (1) The paper claims to evaluate on \"vision, audio, and molecular domains\" (line 351), but there are no molecular generation experiments presented. This claim should be removed or the corresponding experiments and results must be included. \n\n   (2) The text asserts that audio manipulation is a \"more complex\" task to justify the method's superior performance in that domain (line 418-419). This claim requires substantiation. What metric or reasoning is used to define its complexity relative to the vision tasks? Without justification, this reads as post-hoc reasoning. \n\n   (3) Sections 5.2 and 5.3 report superior metrics but are not very informative beyond that. Crucially, the computational efficiency analysis is presented without context. To be verifiable, it must include details on the **specific tasks, models, datasets, and hardware** used to arrive at the reported figures.\n\n3. (Visualization) For a paper focused on conditional generation, the qualitative results are insufficient. Figure 1 provides only a few examples. The quality and diversity of generated samples are critical evaluation criteria, and the current visualizations do not allow for a thorough assessment. The authors are encouraged to add a comprehensive set of generated samples, perhaps in an appendix, similar to the approach in Appendix D of the original TFG paper.\n\n4. (Writing) The manuscript needs a thorough proofreading. \n\n   (1) There appears to be a mix-up between `\\citet` and `\\citep` commands throughout the text, leading to incorrect citation formatting. \n\n   (2) Remnants of Markdown syntax (e.g., `*conditional generation*` in lines 120-121, `*local, linearized*` in lines 318-319) should be corrected to the appropriate LaTeX formatting. \n\n   (3) The symbol $\\rho$ is used for a DDIM coefficient (lines 359-360) after being defined as a TFG hyperparameter. This should be disambiguated to avoid confusion. \n\n   (4) Please explicitly define the term \"ascending configuration\" (lines 361-362). \n\n   (5) The methodology described in lines 362-363 (\"if missing, we optimize scaling factors on 1/8 of the data with up to 6 search iterations over [0, 1000]\") is unclear. What is potentially \"missing\"? What specific hyperparameters are being optimized here? This process needs to be explained in more detail."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vQNnHqo4k8", "forum": "1e42ofAFnO", "replyto": "1e42ofAFnO", "signatures": ["ICLR.cc/2026/Conference/Submission448/Reviewer_vw8Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission448/Reviewer_vw8Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760512353642, "cdate": 1760512353642, "tmdate": 1762915522716, "mdate": 1762915522716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes A²-TFG, a new perspective on training-free guidance for diffusion models that aims to address both the computational bottleneck and performance limitations of existing methods. The authors formulate training-free guidance as an optimization problem and derive analytical guidance weights that depend on the sampling time. Experimental results on multiple benchmarks demonstrate the effectiveness of A²-TFG compared to standard training-free guidance baselines such as TFG."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The performance of the method is good. Given the benefits of identifying suitable parameters for various training-free guidance methods, the proposed framework could offer valuable computational and performance advantages without relying on costly grid search.\n\n* The additional cost per iteration step is minimal. Thus, A²-TFG improves efficiency without significantly increasing the per-step generation cost.\n\n* The idea of deriving the guidance weight based on condition alignment at each step is novel and interesting, to the best of my knowledge.\n\n* The experiments cover a wide range of setups, including both vision and audio generation tasks.\n\n* The paper is well written and easy to follow. The intuition behind the method is explained clearly, at least to some extent."}, "weaknesses": {"value": "* The method compares its optimization with the grid search used in TFG; however, I believe the current setups are not directly compatible. TFG optimizes final quality metrics such as FID and LPIPS, while the proposed method focuses on finding guidance directions that maximize the \"condition alignment loss\" in the update. I believe additional explanation or intuition is needed to better clarify the contribution of each step of the method.\n\n* Although the method performs well in practice, the theoretical assumptions are somewhat strong** (e.g., linear model, lack of joint optimization, errors in $x_0$, etc.). I suggest reformulating the claims to present the method primarily as a practical approach with theoretical justification, rather than as an exact theoretical framework that happens to work well in practice. \n\n**Minor comments:**\n-  $x$ is not defined in Algorithm 1 (Line 274).\n- Line 384 should be revisited.\n-  Some references on using adaptive weights in classifier-free guidance (CFG) [1, 2, 3, 4] could be included. Although not identical to the proposed approach, time-dependent guidance weights are common in CFG and should be discussed as related work.\n\n[1] Sadat S, Buhmann J, Bradley D, Hilliges O, Weber RM. CADS: Unleashing the diversity of diffusion models through condition-annealed sampling. arXiv preprint arXiv:2310.17347. 2023 Oct 26.\n\n[2] Kynkäänniemi T, Aittala M, Karras T, Laine S, Aila T, Lehtinen J. Applying guidance in a limited interval improves sample and distribution quality in diffusion models. Advances in Neural Information Processing Systems. 2024 Dec 16;37:122458-83.\n\n[3] Wang X, Dufour N, Andreou N, Cani MP, Abrevaya VF, Picard D, Kalogeiton V. Analysis of classifier-free guidance weight schedulers. arXiv preprint arXiv:2404.13040. 2024 Apr 19.\n\n[4] Sadat S, Hilliges O, Weber RM. Eliminating oversaturation and artifacts of high guidance scales in diffusion models. InThe Thirteenth International Conference on Learning Representations 2024 Oct 3."}, "questions": {"value": "1. Could you provide results or analysis comparing joint optimization with separate optimization per parameter? This would help illustrate the performance–efficiency trade-off that arises when using separate optimization instead of a joint one.\n\n2. How does the method perform if SGD is used to optimize Equation (5) instead of relying on the linear approximation?\n\n3. How do different choices of $\\mathcal{F}$ affect the final results?\n\n4. Is it possible to optimize the parameters over multiple steps, rather than optimizing them independently at each sampling step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CRGVyToAuQ", "forum": "1e42ofAFnO", "replyto": "1e42ofAFnO", "signatures": ["ICLR.cc/2026/Conference/Submission448/Reviewer_SyHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission448/Reviewer_SyHn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750969517, "cdate": 1761750969517, "tmdate": 1762915522605, "mdate": 1762915522605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel approach to training-free diffusion guidance that is well-motivated. The paper is closely based on the TFG paper, and its novel idea is to compute the $\\rho,\\mu$ hyper-parametrers by the estimation of a optimization problem, which is analytical and can remove hyper-parameter search."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Overall, I think this paper is well-motivating and the proposed algorithm is intuitive, reasonable, and somewhat grounded. Generally speaking, the sample-wise hyper-parameter selection could be a pareto improvement over TFG, and I appreciate that the authors figure out a way to conduct sample-wise selection by linear estimation. In addition, it is also an advantage to remove two hyper-parameters vectores ($\\rho_t, \\mu_t$) and remain only the scalar hyper-parameters (which is related to computational budget)."}, "weaknesses": {"value": "I am giving a conditional weak rejection, and depending on whether the authors can address these issues, I will raise / reduce my score accordingly.\n\n- The first issue is a over-claim of the efficiency improvement compared with TFG. The authors intentionally replace the \"beam search\" strategy in the original paper to \"grid search\" and ignore the smaller search size, making the comparison unfair at all. According to the TFG paper, \"values are determined via searching with 1/8 of the sample size and a maximum search step of 6.\", which means that the search cost is approxiamtely $3 * 6 * 1/8 =2.25$ times of the real experiments, and is significantly smaller than the 70h claims in the paper. In addition, the comparison in Table 3 is also unfair: either you should amortize the serach cost, or you should report the inference cost over the entire dataset. It is also unclear under what datasets and configurations numbers in Table 3 are generated. For instance, I can imagine that for large NN-based guidance function $f$, the JVP computation is at least as heavy as a gradient compute and should induce much larger cost. \n- Due to the issue of incorrect search configurations, Figure 3 is not convincing. Based on my experience of TFG, the search trajectory of TFG in the figure is **incorrect**. Their beam search mechanism should be able to expand to $\\rho = 8$ and $\\mu = 2$ conditioned on the heat map. This misleading figure will confuse readers and the authors should carefully explain the actual message conveyed in this figure with correct experiments. Again, I completely agree that the proposed method could have better performance due to sample-wise optimization. However, this should not be demonstrated via a incorrect way of not implementing existing algorithms.\n- TFG has provided a codebase comprehensive enough to run different experiments all at once, and it is unclear to me why the authors do not conduct on all datasets: classifier guidance in vision domains is removed, combined guidance is removed,  and all molecular experiments are removed as well (despite the authors claim that they have the experiments). In addition, numbers in the paper are inconsistent with the original paper, despite that they are the same task. The authors do not include appendixs / code and I have no way to check in details. This is skeptical and make it less convincing whether the theoretical merit is indeed realized by the estimation prposed in this method. The authors should address all these experimental concerns.\n- Lastly, I think the writing of the paper could be substantially improved in order to meet the accpetance bar. I normally do not evaluate a paper based on writing but in this paper the writing has been influencing understanding. For instance,  you should use `citep` (with brackets) when citing a paper; $\\mathcal F$ and $f$ are used interchangeably without explanations; claims are inconsistent across papers, and in L107: \"the first analytical solution for training-free guidance within a unified theoretical framework, providing rigorous mathematical proofs of optimality.\" is obviously false (not a theoretical optimality, not the first solution, not the first unified framework), in L101: \" provides theoretical optimality guarantees\" is over-claiming (the linearized formula and the step-wise local search cannot be theoretiicaly optimal). The authors are strongly encouraged to better polish their writings."}, "questions": {"value": "Presentation: The first question is that I think the Figure 2 are conveying less information that the space it occupies. A correct version of Figure 3 + Figure 1 (with all tasks) should be sufficient to experss the information that Figure 2 tends to convey."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DVr4EtURmc", "forum": "1e42ofAFnO", "replyto": "1e42ofAFnO", "signatures": ["ICLR.cc/2026/Conference/Submission448/Reviewer_etPR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission448/Reviewer_etPR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797435688, "cdate": 1761797435688, "tmdate": 1762915522499, "mdate": 1762915522499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents $A^2$-TFG, a method that improves upon existing Training-Free Guidance (TFG) framework. The authors proposed an analytical solution to the parameter optimization problem formulated in the Unified TFG paper (https://arxiv.org/abs/2409.15761). The novel approach has both a mathematical proof as well as an empirical validation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured and easy to follow\n2. The paper has a strong mathematical background\n3. Thanks to the analytical solution a ton of time parameter optimization time is saved in comparison to TFG"}, "weaknesses": {"value": "1. Very limited qualitative evaluation is included in the paper (only figure  2b). The majority of the tasks are done on the visual models and I believe at least a qualitative comparison should included in the paper (ideally, a human evaluation; however, I believe it would be hard to do during the AC discussion period)\n2. Lack of examples on the SOTA models (e.g., SDXL, Stable Diffusion 3.5, FLUX.1-dev, Qwen-Image). I know that the majority of the tables in the previous works is done using Stable Diffusion 1.5; however, It would be extremely beneficial for the community if at least a comparison with a current SOTA baseline would be included (I see no reasons why the proposed method wouldn't work on a new model. Please correct me if I'm wrong)\n3. No ablation study. I'd be glad if authors include an empirical validation of the method's compatibility with LoRAs, ControlNets, IP Adapters etc."}, "questions": {"value": "see weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F3PrF9wM3H", "forum": "1e42ofAFnO", "replyto": "1e42ofAFnO", "signatures": ["ICLR.cc/2026/Conference/Submission448/Reviewer_UeHf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission448/Reviewer_UeHf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138450471, "cdate": 1762138450471, "tmdate": 1762915522401, "mdate": 1762915522401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}