{"id": "0b6a2SE23v", "number": 18244, "cdate": 1758285556767, "mdate": 1763744723158, "content": {"title": "Adapting Self-Supervised Representations as a Latent Space for Efficient Generation", "abstract": "We introduce Representation Tokenizer (RepTok), a generative modeling framework that represents an image using a single continuous latent token obtained from self-supervised vision transformers. Building on a pre-trained SSL encoder, we fine-tune only the semantic token embedding and pair it with a generative decoder trained end-to-end using a standard flow matching objective. This adaptation enriches the token with low-level, reconstruction-relevant details, enabling faithful image reconstruction. To preserve the favorable geometry of the original SSL space, we add a cosine-similarity loss that regularizes the adapted token, ensuring it remains smooth and suitable for generation.Our single-token formulation resolves the spatial redundancies of the 2D latent space, simplifies architectures, and significantly reduces training costs. Despite its simplicity and efficiency, RepTok achieves competitive results on class-conditional ImageNet generation and extends naturally to text-to-image synthesis, reaching competitive zero-shot performance on MS-COCO under extremely limited training budgets. Our findings highlight the potential of fine-tuned SSL representations as compact and effective latent spaces for efficient generative modeling. We will release our model to facilitate further research.", "tldr": "We show that fine-tuned self-supervised tokens can serve as compact latents, enabling faithful single-token reconstruction and efficient generation.", "keywords": ["generative models", "visual synthesis", "diffusion", "flow matching"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2ba80e02c5e8f5674b51397b16e380b5e3b1c8ae.pdf", "supplementary_material": "/attachment/f342dbd577b3ad24e628d4506e30aabbe2c9b5ec.pdf"}, "replies": [{"content": {"summary": {"value": "This paper adapts pre-trained self-supervised representations into a single continuous latent token space and trains a lightweight latent generator with a generative decoder. This idea is potentially impactful for compute-efficient generation. However, at the reported compute and model sizes, the performance falls short of the current SOTA, which lacks a matched-budget comparison. Also, the Flops reported in Table 3 appears to exclude the generative decoder, making the efficiency claims for end-to-end generation unclear. The paper also lacks a scaling study of compute resource and paramters which is an important property."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is a noval approach that uses a single continuous latent token adapted from self-supervised representations with a lightweight latent generator.\n2. RepTok shows flexibility as it extents to text-to-image synthesis.\n3. The paper is generally clear and well organized."}, "weaknesses": {"value": "1. Although the compute and model sizes of latent generator is smaller, the gFID in Table 2 and FID in Table 3 are not competitve with SOTA. It remains unclear how RepTok performs under matched evaluation settings against baselines.\n2. Table 3 appears to report their compute resources and parameters only for the MLP Mixer, excluding the stage-one Generative Decoder. This undermines the fairness of the compute comparison and weakens the efficiency claim.\n2. The paper does not provide a scaling study for the MLP-Mixer generator (e.g., FID vs. parameters/train steps). Table 3 reports only a single 276M model, whereas the only scaling evidence shown concerns the frozen language backbones in Figure 7. Since parameter scaling is a key feature of DiT/SiT, reporting two training-step settings is insufficient to establish scaling behavior.\n3. There are many instances of incorrect citation formatting: using author-in-text citations (\\citet{...}) instead of the required parenthetical author-year style citations (\\citep{...})."}, "questions": {"value": "1. What are the total compute resources and paramters combining the Generative Decoder with the MLP Mixer? Please report FID, GFlops/Iter, total PFlops in Table 3 under matched evaluation settings and include the stage-one generative decoder in compute resources and parameters.\n2. Can RepTok exceed SOTA FID when scaled up in Table 3? Please include a scaling analysis of the MLP-Mixer sizes and longer training, reporting FID vs. total PFLOPs/params under matched evaluation settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9dXvZsFMeJ", "forum": "0b6a2SE23v", "replyto": "0b6a2SE23v", "signatures": ["ICLR.cc/2026/Conference/Submission18244/Reviewer_kicm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18244/Reviewer_kicm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760584305335, "cdate": 1760584305335, "tmdate": 1762927975284, "mdate": 1762927975284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear reviewers,\n\nThank you for your constructive feedback. We are glad that you found RepTok “efficient” (**wivx, TyLM**) and “flexible” (**GhVu, kicm**), and appreciate that you find our method “well designed” and “intriguing” (**wivx, TyLM**). We are also encouraged that you found the idea novel (**GhVu, kicm**), described the paper as “clear” and “well written” (**TyLM, kicm**), and regarded the ability to recover an image from a single token as particularly promising (**wivx**).\n\nWe also appreciate that reviewers emphasized the broader potential of our approach, noting that it may “inspire subsequent researchers” (**TyLM**), offer “practical applications” (**TyLM**), and “encourage more work on compact visual tokenizers” (**GhVu**).\n\nWe highlight updates to the paper in $\\color{blue}{\\textit{blue}}$. In the updated version, we corrected the citation formatting, added a MLP-Mixer scaling experiment (Fig. 10), and included first-stage training cost and large MLP-Mixer FID results (Tab. 3). We updated the generative FID numbers in Tab. 2, added concurrent work to the related-works section, and added a brief description and qualitative results for the model adaptation to $512^2$ resolution.\n\nBelow we address each individual comment."}}, "id": "IHuurfl3QM", "forum": "0b6a2SE23v", "replyto": "0b6a2SE23v", "signatures": ["ICLR.cc/2026/Conference/Submission18244/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18244/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18244/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763740909065, "cdate": 1763740909065, "tmdate": 1763740909065, "mdate": 1763740909065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RepTok, a generative modeling framework that leverages the [cls] token from a pre-trained self-supervised vision transformer as a single-token, continuous latent space useful for image generation tasks. \n\nThe paper fines only the [cls] token from a pretrained SSL network and regularizing it with a cosine similarity loss. The authors enable faithful image reconstruction and efficient generative modeling. The approach achieves competitive results on class-conditional ImageNet generation and zero-shot text-to-image synthesis on MS-COCO.\n\nThis paper, to my knowledge, is the first single-token encoder paper that achieves reasonably high performance on image genertion tasks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "(+) novelty: using a single [cls] token to represent the image in the continuous space to my knowledge is novel (and neat), what is better is that it can be minimally fine-tuned from a pretrained SSL network. The paper can encourage more work for compact visual tokenizer that improves both token redundancy and improve tokenization/reconstruction quality\n\n(+) Competitive performance: the performance is good for the first paper with a single-token compression. The experiements are also relatively thorough\n\n(+) Generalization: can use different SSL models, such as DINOv2, MAE, CLIP"}, "weaknesses": {"value": "(-) The method requires careful tuning of the cosine similarity loss parameter and careful choices of freeze/unfreeze tokens to balance between reconstruction fidelity and generative quality. This trade-off may be non-trivial in practice and may need tuning for different backbones\n\n(-) A single-token representation is neat, but it will be nice to see the tradeoff between more tokens vs single token."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h63uQUYGbs", "forum": "0b6a2SE23v", "replyto": "0b6a2SE23v", "signatures": ["ICLR.cc/2026/Conference/Submission18244/Reviewer_GhVu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18244/Reviewer_GhVu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549550757, "cdate": 1761549550757, "tmdate": 1762927974847, "mdate": 1762927974847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RepTok, an approach for training image generative models using pre-trained self-supervised learning (SSL) models. During the encoder-decoder training phase, the SSL model is fine-tuned efficiently via a learnable [CLS] token parameter, and a flow matching head is trained. In the image generation training phase, images are generated in the SSL model's latent space using flow matching based on an MLP model. The paper validates the feasibility of using SSL as an encoder and demonstrates impressive performance in both image reconstruction and generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The paper offers several intriguing insights in its model architecture design. These include the indirect fine-tuning of the pre-trained SSL model via a learnable [CLS] token, the use of cosine similarity as a latent space regularizer, and the employment of an MLP-Mixer as the backbone for the generative stage. I believe these interesting design choices could provide inspiration for subsequent researchers.\n2.  The proposed method is notably lightweight. On one hand, the model architecture does not require training an excessive number of parameters; on the other hand, building upon a pre-trained SSL model keeps the training cost for the generative model relatively low, suggesting potential for practical application.\n3.  The paper is generally well-written and clear, and the experiments are reasonably comprehensive."}, "weaknesses": {"value": "1.  Although the structure is lightweight, the scaling potential of the method is concerning. Firstly, while the reconstruction results in Figure 3 appear superior to FlexTok, they do not seem to surpass those of the VAE. Furthermore, as shown in Table 3, RepTok's final performance is lower than that of REPA-CFG 1.5. Therefore, I would like the authors to provide an analysis of how this method could be further scaled, including a discussion of performance bottlenecks, and to present additional experimental results demonstrating the method's potential at larger scales.\n2.  The authors' proposed method (two-level flow matching) bears formal resemblance to Transition Matching [1]. Furthermore, the idea of replacing VAEs with self-supervised models has also been explored, including in works such as RAE [2] and SVG [3]. While direct comparison with concurrent work is not strictly necessary, I encourage the authors to supplement the discussion with a clarification of the distinctions and a more in-depth comparative analysis.\n3.  Since the authors' core claim also includes that \"SSL provides a better semantic space than VAE\" (not merely lightweight), additional experimental results are needed to substantiate this point, particularly regarding performance aspects. This could include evaluations on avoiding distortions and handling challenging, complex, or compositional prompts.\n4.  The citation format is informal; it should be (Ho et al. 2020), not Ho et al. (2020).\n\nref:\n\n[1] Transition Matching: Scalable and Flexible Generative Modeling\n\n[2] Diffusion Transformers with Representation Autoencoders\n\n[3] Latent Diffusion Model without Variational Autoencoder"}, "questions": {"value": "Refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q3PvamoiF1", "forum": "0b6a2SE23v", "replyto": "0b6a2SE23v", "signatures": ["ICLR.cc/2026/Conference/Submission18244/Reviewer_TyLM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18244/Reviewer_TyLM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901037419, "cdate": 1761901037419, "tmdate": 1762927974464, "mdate": 1762927974464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for tokenizing an image into a single continuous token using an SSL-pretrained model. Based on this single token, the authors design a generative decoder and a representation generator to perform reconstruction and generation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides strong evidence that SSL models can provide strong generative information using only a single image token.\n2. The corresponding generative decoder and representation generator are well designed.\n3. The proposed method shows very high training efficiency."}, "weaknesses": {"value": "1. Lack of representation comparison: As the work is heavily based on the image representations, it would be helpful to show the correspondence between generation quality and representation performance under different [CLS] settings (e.g., frozen or fine-tuned with varying $\\lambda$).\n2. The authors experimented with different SSL models for the proposed method. How does the performance change when combining multiple models, such as training a lightweight linear projection on top of the concatenated [CLS] tokens (or through other fusion strategies)?\n3. The proposed method heavily relies on image SSL models, which may limit its generalizability to video generation or multimodal generation tasks.\n4. The paper only conducts experiments at a resolution of 256x256. Using a single token to encode an image may be ineffective for higher-resolution settings."}, "questions": {"value": "As the generation process involves two flow-matching steps, I am curious about the inference speed of the proposed model. Could the authors provide a comparison of inference time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gHx8tXzKbV", "forum": "0b6a2SE23v", "replyto": "0b6a2SE23v", "signatures": ["ICLR.cc/2026/Conference/Submission18244/Reviewer_wivx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18244/Reviewer_wivx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014347019, "cdate": 1762014347019, "tmdate": 1762927974065, "mdate": 1762927974065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}