{"id": "scgtQSpROE", "number": 16881, "cdate": 1758269864601, "mdate": 1759897213777, "content": {"title": "Logit‑KL Flow Matching: Non‑Autoregressive Text Generation via Sampling‑Hybrid Inference", "abstract": "Non-autoregressive (NAR) language models offer notable efficiency in text generation by circumventing the sequential bottleneck of autoregressive decoding. However, accurately modeling dependencies in discrete sequences remains challenging in this paradigm. In this work, we advance the field of NAR generation by applying conditional flow matching (CFM) methods grounded in geometrically principled interpolation, specifically leveraging Kullback-Leibler (KL) divergence geodesics, which correspond to linear interpolation in logit space. We rigorously establish that maximizing conditional likelihood in this setting precisely recovers the flow matching velocity field, supplying the theoretical justification for this approach in sequence modeling. To address practical performance gaps of \\emph{basic} inference, we propose a novel empirical \\emph{sampling} strategy that iteratively denoises and re-noises, along with a \\emph{hybrid} scheme that integrates our \\emph{sampling} method with \\emph{basic} procedure. Across unconditional and conditional text and code infilling, the approach improves perplexity and downstream metrics over prior NAR baselines under matched settings.", "tldr": "", "keywords": ["Flow Matching", "NAR text generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed98752bf5376f7636477a1ce430885cc54475e1.pdf", "supplementary_material": "/attachment/6fa5f36e3bfa641b1435355ee3dda88fc3dc7ce8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Logit-KL Flow Matching (KL-Flow), a non-autoregressive (NAR) text generation framework that performs flow matching in logit space rather than probability space.\nBy interpreting the KL divergence geodesic as a linear path in logits, the authors provide a theoretically grounded interpolation scheme for discrete sequence modeling. They further introduce an iterative sampling–hybrid inference procedure combining deterministic ODE integration and stochastic denoising steps.\nEmpirically, KL-Flow shows consistent gains over prior discrete flow and diffusion baselines (DFM, Dirichlet Flow, Fisher Flow, SEDD) on unconditional, conditional, and code infilling benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper rigorously connects conditional likelihood maximization to flow velocity recovery in logit space.\n2. The token-wise conditional likelihood formulation is both simple and tractable, providing a bridge between discrete diffusion and flow models.\n3. The hybrid inference scheme demonstrates consistent improvements across several datasets (TinyStories, FineWeb, WMT14, MBPP), showing competitive results for both language and code tasks."}, "weaknesses": {"value": "1. While the paper covers some discrete flow and diffusion models, it does not include strong diffusion-based text generation baselines such as MDLM (Masked Diffusion Language Model) and conditional NAR transformers such as Tracformer (https://arxiv.org/pdf/2502.07616?)\n2. Most experiments focus on small to mid-scale models (≤1.5B parameters); Would scaling the model or using stronger backbones (e.g., Llama-2 or Mistral) improve performance beyond the 1.5B-parameter setting?\n3. Although empirically effective, the hybrid scheme’s transition point t* is largely heuristic. Could the authors provide intuition or ablation results for how the choice of the deterministic–sampling switch point t* influences diversity and perplexity across datasets?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AwaObmaEbM", "forum": "scgtQSpROE", "replyto": "scgtQSpROE", "signatures": ["ICLR.cc/2026/Conference/Submission16881/Reviewer_5rVW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16881/Reviewer_5rVW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993729588, "cdate": 1761993729588, "tmdate": 1762926911422, "mdate": 1762926911422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a non-autoregressive (NAR) text generator built on conditional flow matching (CFM) in logit space. Instead of interpolating token probabilities linearly on the simplex or along Fisher–Rao geodesics, it interpolates logits between a simple Dirichlet-like start and the target one-hot token, which the authors argue is the KL geodesic; they show that, under this path, maximizing the conditional likelihood ( $\\log p_\\theta(x_1 \\mid x_t, t)$ ) exactly recovers the desired flow velocity field. On top of that, they introduce a hybrid inference scheme that runs a deterministic ODE-style update in the early time steps and switches to an iterative sampling / re-noising procedure in later steps to fix token-level errors. On several text, conditional, and code-infilling benchmarks, the method outperforms earlier discrete / Dirichlet / Fisher flow-matching baselines and comes close to, but does not fully match, similarly sized AR models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear geometric diagnosis + concrete fix. The paper identifies a real failure mode of earlier probability-space paths — linear on the simplex, Fisher-Rao sphere, even some Dirichlet settings — namely that $KL((x_{\\text{data}}|x_t))$ collapses too quickly so mid-time supervision vanishes on large vocabularies. The logit-space (KL-geodesic) interpolation directly targets this and shows improved calibrations against those baselines. This is well aligned with prior observations in Dirichlet Flow Matching that “naïve linear FM on the simplex is pathological.”\n2. Bridging “train a denoiser” and “learn the flow field” for sequences, not just single tokens. Earlier CFM/DFM papers had versions of “conditional likelihood recovers the field,” but mostly in single-site or weaker sequence assumptions; this paper pushes the argument specifically for logit-KL paths and uses it to justify a very practical objective (just NLL on corrupted sequences). That reduces the gap between elegant flow theory and what people actually train.\n3. Inference is engineered rather than hand-waved. Many discrete flow papers stop at “we have the field, integrate it”; here they run a 3-way comparison (deterministic / stochastic / hybrid) and show that pure ODE is insufficient and pure sampling collapses entropy, while a staged hybrid fixes both. That’s a useful empirical lesson for the whole discrete-flow community."}, "weaknesses": {"value": "1. Novelty margin over very close contemporaries is thin. At least two 2024–2025 papers already investigated conditional text generation via KL-geodesic / logit-space flow matching and even proposed almost the same empirical “sampling + noise re-injection + hybrid” recipe to fix the underperforming basic sampler. The descriptions in Sevriugov & Oseledets (2024) and its 2025 extensions match this work’s geometric choice and sampling intuition almost line-for-line. If the contribution here is meant to be “we prove the conditional-likelihood = exact field under this path and scale it to larger datasets,” the paper needs to separate itself much more crisply from those concurrent KL-geodesic efforts, especially since they also claim better results over discrete FM. Right now the delta looks incremental.\n2. Key equivalence rests on a factorized / per-position view that is not obviously valid in early timesteps. The derivation leans on the idea that the optimal vector field at time (t) can be written as an expectation of target logits under ($p(x_1 \\mid x_t)$) token-wise. But in NAR text, ($p(x_1 \\mid x_t)$) is usually not well factorized when (t) is small: tense, agreement, long-range topic constraints all couple positions. The paper’s fix is “do deterministic updates early, sampling later,” which is an empirical workaround, not a proof that the factorization is OK. So a central theoretical selling point (“likelihood = flow”) is relying on a data-distribution property that’s weakest exactly where the model needs guidance most. That’s a structural, not cosmetic, gap.\n3. Evaluation uses LM-perplexity proxies and medium-scale AR baselines, so the true competitiveness is unclear. Measuring perplexity by scoring NAR outputs with an external LM is standard for discrete flows, but it is a proxy; it is known to favor models that mimic the scorer’s style rather than models that are truly diverse or controllable. And the main AR point of comparison is GPT-2-class models, not the 2025-era instruction-tuned or code-tuned LLMs that NAR methods would actually have to replace. In other words, the paper shows “better than prior discrete flows” but not “this can plausibly replace competitive AR models under identical training budgets.” That’s a material, not rhetorical, limitation.\n4. Hybrid inference is admitted to be heuristic and under-analyzed. The whole motivation of the paper is “basic ODE flow isn’t good enough on text,” which is fair; but the proposed fix (early deterministic, late sampling + noise) is only justified by curves. There is no stability analysis, no guarantee of staying close to the learned KL-geodesic, and no complexity comparison with recent few-step DFM / consistency-trained flows that do target a fixed number of steps directly. A reviewer can reasonably ask why we shouldn’t just adopt FS-DFM-style step-budget-aware training or consistency distillation to get the same effect with a cleaner theory."}, "questions": {"value": "1. How does this behave under true few-step regimes (e.g. 8–16 NFE) against step-consistent discrete flows like FS-DFM or consistency-trained DFM? Right now the advantage is shown largely when you afford hundreds of steps or a tailored hybrid schedule; but the practical motivation for NAR is low latency. A head-to-head with step-budget-aware models is missing. Can the KL-geodesic path still deliver better gradients than Dirichlet / Fisher when you compress the time discretization that aggressively?\n2. What exactly is the uniqueness claim over existing KL-geodesic / logit-FM papers? Several public works from late 2024 onward already stated (i) “logit-space interpolation is the KL geodesic,” (ii) “maximizing ($p_\\theta(x_1 \\mid x_t,t)$) gives you the right field,” and (iii) “basic deterministic inference is weak; add an iterative sampling-and-noise scheme; hybridize.” If this paper’s contribution is a stronger sequence-level derivation or better scaling to open-domain data, please spell out the technical gap (e.g. a specific lemma about token-wise optimality under KL-paths, or a complexity advantage in hybrid inference) that is not present in Sevriugov & Oseledets (2024) or the concurrent KL-geodesic variants. Right now it reads more like a consolidation than a breakthrough."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E74OgT9M3R", "forum": "scgtQSpROE", "replyto": "scgtQSpROE", "signatures": ["ICLR.cc/2026/Conference/Submission16881/Reviewer_eLVC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16881/Reviewer_eLVC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046969915, "cdate": 1762046969915, "tmdate": 1762926910917, "mdate": 1762926910917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel non-autoregressive text generation framework that uses a Kullback-Leibler (KL) divergence geodesic for interpolation, which is shown to be equivalent to linear interpolation in logit space. The objective function is to minimize the negative log-likelihood of the target distribution at the sequence level. Experiments demonstrate that the proposed method outperforms other non-autoregressive (NAR) baselines such as DFM and SEDD."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method achieves strong performance against other NAR baselines across various tasks.\n\nThe discussion of the inference process is insightful."}, "weaknesses": {"value": "It is unclear why the deterministic inference process performs poorly, given that the \"Logit-KL Flow Matching\" objective recovers the velocity field.\n\nThe efficiency of the proposed method is not thoroughly discussed, particularly in comparison to methods that are trained with an MSE loss (in training) and solve ODEs using numerical techniques (in inference).\n\nPerplexity is measured using samples from GPT-2, GPT-3, and Llama-2, which may introduce bias from these reference models."}, "questions": {"value": "In the experiments, the authors state that all models use a bidirectional transformer backbone. Was the GPT-2 baseline trained in an autoregressive manner, and was its causal attention mechanism replaced with bidirectional attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MH7PZYlbxF", "forum": "scgtQSpROE", "replyto": "scgtQSpROE", "signatures": ["ICLR.cc/2026/Conference/Submission16881/Reviewer_qQ8c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16881/Reviewer_qQ8c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762421759875, "cdate": 1762421759875, "tmdate": 1762926910459, "mdate": 1762926910459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a non-autoregressive text-generation framework that performs conditional flow matching on the probability simplex using KL-divergence geodesics. The authors show (in their way) that maximizing the token-level conditional likelihood exactly recovers the flow-matching velocity field, providing a principled training objective."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The mathematical descriptions in this paper are relatively accurate.\n2. The paper is well structured."}, "weaknesses": {"value": "1. The used base model in the experiments seems obsolete, which makes it unclear whether the proposed method still works for the SOTA models nowadays.\n2. Lines 226 ~ 228 seem confusing. A bidirectional attention is used to model sequence-level NLL, but the condition variable $x_t$ represents a single token, not a total sequence. I am not convinced of this modelling approximation.\n3. Line 309 says all models used in the experiments are bidirectional backbones. How do you apply this to text generation, where you do not have access to any future token information?\n4. I see some autoregressive models have been used in the experiments (GPT-2), so you replaced the causal attention in the model with a bidirectional attention? If so, how do you initialize your model weights? Is it the same with the pre-trained weights?\n5. The experiment setting does not include baseline introductions, which makes the reader very hard to get familiar with the relevant work.\n6. The proposed method in this paper has two inference methods, namely, basic inference and sampling inference. However, there is no formal section in the paper to systematically compare these two inference methods and discuss the corresponding advantages and disadvantages. The experiment section also missed this."}, "questions": {"value": "No, see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8YapN2rGE5", "forum": "scgtQSpROE", "replyto": "scgtQSpROE", "signatures": ["ICLR.cc/2026/Conference/Submission16881/Reviewer_VSxC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16881/Reviewer_VSxC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762436905725, "cdate": 1762436905725, "tmdate": 1762926909941, "mdate": 1762926909941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}