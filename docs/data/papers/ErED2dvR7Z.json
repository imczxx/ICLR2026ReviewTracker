{"id": "ErED2dvR7Z", "number": 18697, "cdate": 1758290227516, "mdate": 1759897086893, "content": {"title": "Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features", "abstract": "Advances in generative modeling have recently been adapted to heterogeneous tabular data. However, generating mixed-type features that combine discrete values with an otherwise continuous distribution remains challenging. \nWe advance the state-of-the-art in diffusion-based generative models for heterogeneous tabular data with a cascaded approach.\nAs such, we conceptualize categorical variables and numerical features as low- and high-resolution representations of a tabular data row. We derive a feature-wise low-resolution representation of numerical features that allows the direct incorporation of mixed-type features including missing values or discrete outcomes with non-zero probability mass.\nThis coarse information is leveraged to guide the high-resolution flow matching model via a novel conditional probability path.\nWe prove that this lowers the transport costs of the flow matching model. \nThe results illustrate that our cascaded pipeline generates more realistic samples and learns the details of distributions more accurately.", "tldr": "A cascaded flow matching framework that generates details in tabular data conditioned on low-resolution features.", "keywords": ["tabular data", "flow matching", "generative modeling", "synthetic data"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b02dc0bf5379e607878c80d87a2eb2d2c6bd8c9c.pdf", "supplementary_material": "/attachment/e38234e6f9efd4a819a3442a6e36fca83c8dcd01.zip"}, "replies": [{"content": {"summary": {"value": "This paper's main aim is to answer the research question “How can we generate realistic heterogeneous tabular data that includes mixed-type numerical features (continuous values with discrete point masses such as missings or inflated values), by reducing transport costs and improving fidelity via a cascaded flow-matching framework that leverages low-resolution information?”"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **[Important] Novel model design.** Cascaded factorisation of tabular generation. The paper proposes TabCascade, which factors the joint into a low-resolution model and a high-resolution flow-matching model, which is conceptually attractive.\n2. **Theoretical claim.** With a DT encoder, the authors seem to provide convincing evidence that data-dependent coupling lowers an upper bound on transport cost compared to independent couplings.\n3. **[Important] Empirical gains on standard metrics.** On six datasets, TabCascade (DT) achieves state-of-the-art Detection scores (C2ST) and competitive/better Shape/Trend scores."}, "weaknesses": {"value": "1. **[Important] Two-stage factorisation may under-capture cross-type dependencies.** Since $x_{\\text{cat}}$ and $x_{\\text{num}}$ do not seem to be generated jointly (high-res is conditioned on low-res outputs), subtle dependencies might be missed.\n2. **[Important] Limited coverage of benchmark generators.** Many competitive models are missing in the current results, such as foundation models, CTSyn [1] and TabPFN [2]. I would suggest the authors refer to relevant literature [3, 4] for a broader context of the benchmark setups. Existing coverage seems limited to reach conclusive results.\n3. **Privacy is only analysed superficially (DCR share) and no guarantees are claimed.** For sensitive tabular data, the absence of *any* privacy mechanism or DP ablation limits practical adoption.\n4. **Metric dependence and detector sensitivity.** The strong wins are most pronounced for Detection score (gradient-boosted C2ST); while Shape/Trend also improves, they are already near ceiling for many baselines. This raises questions about how general the gains are across orthogonal metrics and downstream utility.\n\n[1] Lin, Xiaofeng, et al. \"Ctsyn: A foundational model for cross-tabular data generation.\" *arXiv preprint arXiv:2406.04619* (2024).\n\n[2] Hollmann, Noah, et al. \"Accurate predictions on small data with a tabular foundation model.\" *Nature* 637.8045 (2025): 319-326.\n\n[3] Ma, Junwei, et al. \"TabPFGen--Tabular Data Generation with TabPFN.\" *arXiv preprint arXiv:2406.05216* (2024).\n\n[4] Margeloiu, Andrei, et al. \"Tabebm: A tabular data augmentation method with distinct class-specific energy-based models.\" *Advances in Neural Information Processing Systems* 37 (2024): 72094-72144."}, "questions": {"value": "Please refer to \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JAIKKYmSW7", "forum": "ErED2dvR7Z", "replyto": "ErED2dvR7Z", "signatures": ["ICLR.cc/2026/Conference/Submission18697/Reviewer_oVg1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18697/Reviewer_oVg1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761501075019, "cdate": 1761501075019, "tmdate": 1762928400032, "mdate": 1762928400032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Cascaded Flow Matching, where the authors leverage ideas from cascaded diffusion and apply it to flow matching by first generating low resolution features (categorical features) then generating high resolution information (continuous features) conditioned on the generated categorical features. Experiments highlight that their generative paradigm provides competitive results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**. The paper integrate concepts from cascaded diffusion onto tabular data. As far as I know, this is the first paper that applies a multi-resolution generation concept to tabular data (low = categorical and high = numerical). \n\n**Quality**. Idea is interesting and results demonstrated are competitive.\n\n**Clarity**. Overall, the paper is easy to follow as it conveys the message and method well.\n\n**Significance**. Tabular data generation is important in many aspects such as privacy preservation."}, "weaknesses": {"value": "In the work, the authors claim that:\"this is the first work to address mixed-type feature generation, i.e., features following a mixture of categorical and continuous distributions\". However, this is not expressed very clearly. There are methods that unify the data representation including TabRep [1] that explores various encoding, TabbyFlow [4] that represents heterogeneous data types using a general exponential family distribution, TabSYN that projects the data onto a latent space, and StaSy that applies continuous diffusion to a unified data space via one-hot encoding.\n\nThe results seem to be underwhelming compared to the baselines reported in the paper. Not-so-recent baselines including TabRep and TabbyFlow that explores flow matching on tabular data should also be included for comparisons. Considering that TabRep-Flow and TabbyFlow both outperform TabDiff across the board, the margins for TabCascade will shrink.\n\nWhat is the motivation of using flow matching in the framework? TabRep and TabbyFlow leverages its sampling speed. If so, are there experiments to demonstrate this?\n\nOne of the most important use-cases of tabular relational data generation is privacy preservation. DCR experiments are conducted and demonstrates that it underperforms against its competitors. Additionally, existing literature in tabular data generation [1] and computational privacy [2] [3] have also highlighted the inadequacy of DCR in evaluating privacy preservation. Hence, its important to assess the privacy preservation via Membership Inference Attacks too.\n\n[1] Si, Jacob, et al. \"TabRep: Training Tabular Diffusion Models with a Simple and Effective Continuous Representation.\" arXiv preprint arXiv:2504.04798 (2025).\n\n[2] Georgi Ganev and Emiliano De Cristofaro. The inadequacy of similarity-based privacy metrics: Privacy attacks against \"truly anonymous\" synthetic datasets, 2024.\n\n[3] Joshua Ward, Chi-Hua Wang, and Guang Cheng. Data plagiarism index: Characterizing the privacy risk of data-copying in tabular generative models, 2024.\n\n[4] Guzmán-Cordero, Andrés, Floor Eijkelboom, and Jan-Willem van de Meent. \"Exponential Family Variational Flow Matching for Tabular Data Generation.\""}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DNXZAn6OzB", "forum": "ErED2dvR7Z", "replyto": "ErED2dvR7Z", "signatures": ["ICLR.cc/2026/Conference/Submission18697/Reviewer_5gde"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18697/Reviewer_5gde"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763063372, "cdate": 1761763063372, "tmdate": 1762928398579, "mdate": 1762928398579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TabCascade, a cascaded flow matching framework for heterogeneous tabular data with mixed type features. Low resolution information, that is categorical variables and a discretized view of numeric variables, is generated first, and a high resolution conditional flow matching model then fills in continuous numeric details. The high resolution model uses a guided conditional probability path with feature specific time schedules and a data dependent Gaussian source distribution, together with a theorem showing that the tree based encoder can lower a transport cost bound. Experiments on six public datasets show strong detection scores and competitive Shape and Trend metrics, with an ablation over decision tree depth. However, the study does not include direct flow matching baselines, so it is hard to tell how much of the reported gains come from the cascade versus simply switching from diffusion to flow matching."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### **Strengths**\n\n- Clear and practical framing for mixed type numeric features, with an ancestral sampling procedure that decides coarse states first and fills in numeric details when needed (eq 2.)\n- Guided conditional probability path with feature specific time schedules and a compact supervised target for the velocity field (eq 5 and 6).\n- Theorem showing reduced transport cost under the tree encoder, which gives non trivial support for the chosen coupling, see Appendix A.\n- Consistent improvements on detection scores and competitive Shape and Trend across six datasets, with a simple depth ablation."}, "weaknesses": {"value": "### **Weaknesses**\n\n- Too many changes at once, limited component ablation. The system introduces several factors at the same time, switch from diffusion to flow matching, data dependent source with mean and variance from z, learned feature specific time schedules, and the cascaded split with a strong low resolution model. The study does not sufficiently isolate the contribution of each factor. The current ablation varies only the depth of the tree encoder, which mainly changes the difficulty of the high resolution stage rather than the learning rule itself.\n- Missing flow matching baselines. The core high resolution component is a flow matching model, but Section 5 compares against diffusion models and non diffusion models, not against direct flow matching baselines. At minimum, include a straight flow matching baseline for tabular data with a linear path and a standard normal source. In addition, please compare to recent flow matching baselines suggested by the community (eg [1, 2]), or explain why they are not compared against.\n- Encoder dependence and leakage of difficulty. With deeper trees, integer valued features can be fully captured at low resolution, effectively removing them from the high resolution task, which can inflate joint realism without demonstrating stronger continuous modeling. A per dataset analysis of how often the high resolution stage is masked would help.\n\n\n### References \n\n- [1] **Exponential Family Variational Flow Matching for Tabular Data Generation** - Andrés Guzmán-Cordero, Floor Eijkelboom, Jan-Willem van de Meent\n- [2] **Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees** - Alexia Jolicoeur-Martineau, Kilian Fatras, Tal Kachman"}, "questions": {"value": "### **Questions**\n\n- Add a direct flow matching baseline for tabular data with a linear path and a standard normal source, and also a rectified flow baseline, both under the same architecture and training budget as your high resolution model, then report the main metrics in Tables 1 to 3 to separate the effect of the cascade from the effect of the objective. Ideally also compare against existing FM approaches.\n- Provide component wise ablations that keep the encoder fixed and switch off each ingredient in turn, no learned time schedules, set gamma t equal to t, no data dependent source, set mu equal to zero and sigma equal to the identity, keep the cascade but replace the high resolution flow matching with diffusion as in CDTD with the same model size and budget, and keep the high resolution flow matching but remove the cascaded conditioning to test unconditional flow matching, then report all three main metrics. \n- Report, for each dataset, the fraction of features and rows for which the high resolution model is masked by z, and how this fraction changes with encoder depth, to calibrate how much work is delegated to the low resolution stage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TWntNoruim", "forum": "ErED2dvR7Z", "replyto": "ErED2dvR7Z", "signatures": ["ICLR.cc/2026/Conference/Submission18697/Reviewer_SS8m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18697/Reviewer_SS8m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975550667, "cdate": 1761975550667, "tmdate": 1762928398064, "mdate": 1762928398064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Mixed-type tabular data generation with cascaded flow matching so it condition on categorical features and latent continuous features.\n\n\"To the best of our knowledge, this is the first work to address mixed-type feature generation,\ni.e., features following a mixture of categorical and continuous distributions, within diffusion-based\nmodels.\" This is an overclaim. You mentioned yourself prior reference of work doing mixed-type generation. You don't need this claim for your work to be relevant. Okay, after reading more I get what you are trying to see, please rephrase it to talked about diffusion cascade because right now it looks like an overclaim.\n\nMissing reference: \n- https://arxiv.org/abs/2309.09968\nhandles mixed-type generation and missing data using xgboost\n- https://openreview.net/forum?id=LFCSTy6MYe#discussion\nuses kernel density integral quantization (KDI) which sounds quite similar to your use of distributional trees\n\nProblem statement Inflated values: Why be limited to dirac (aka binary categorical feature)? There are multi-class categories. I'm not sure I get why this paragraph is needed.\n\n\"Previous diffusion models for tabular data can be trained on numerical features with missing values, but are\nnot designed to generate such instances.\" Nobody wants to generate data with missing values, its not useful. Then you have to discard those samples anyways if using something like linear regression or logistic classification (which is what people use for small data in medecine/psychology).\n\n\"he simplicity of learning categorical features\": Can you show Figure 2 with other methods included, e.g., TabDDPM, ForestDiffusion? This fits the figure and would strengthen the argument. One thing though to keep in mind is that categorical data can be very hard to model properly, it could just that for this data, getting the category right is easy. In retrospect, to be honest, I dont buy that claim either that categorical features are easier, it really depends on the dataset. I really dont buy it, you need a stronger argument or more proof or to remove that paragraph. Maybe multiple datasets with multiple methods.\n\nGeneral comment:\n- I feel like there is a lot of flafla text that could be trimmed down and a lot of unnecessary math equations that could be removed and replaced with a figure or one paragraph.\n- its overcomplicated, make it more simple\n- results sections need major rework\n\nCan you explain why you need a fifth-degree polynomial for the time schedule. This seems extremely overengineering, like a simple linear line from 0 to 1 would work. And why do we need feature-specific path?\n\nThe coupling and factorization make sense. I can see how it could help produce better data. I like the idea of learn z from GMM or DT.\n\n10% MNAR is extremely low. Real world data that data scientists deal with have 25-50% MNAR. Having an example with 25 or 50% would be important because its closer to the real world and will test the methods to their limit.\n\n SDMetrics shape and trend are not great metrics, they are not accounting for the whole distribution. The other metrics are good. But you need a distribution metric to really tackle distance in distribution. You can use something like the Wasserstein distance (see https://arxiv.org/abs/2309.09968). It's important to have such a metric. In my opinion it would be better to show your results as the average across datasets or the rank, this way you can have a single figure with all the metrics. You can leave the current tables to the appendix. Also right now it makes you look like you chose the best metrics to show in the paper and left the rest in appendix; from looking at the appendix, it sure looks that way. Make one table with all metrics in the paper, everything else is appendix info. Please also add a small table showing the N, N_cat_features, N_cont_features of each dataset included. I'm sorry to say this, but 6 datasets is not a lot with tabular data. I know that this is extra work, but ideally having more datasets would really help. When you switch to average or ranking, it will make it easy to add new datasets without getting tables that are too big.\n\nThe ablation is too small, you need a real ablation where components are removed including whether to go cascaded or not, GMM vs distributional trees vs Quantile-Transform, linear schedule versus your super complicated feature-dependent 5-degree polynomial, etc.\n\nIf the authors make the major changes requested, I'll revisit my score."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "cascaded structure is promising"}, "weaknesses": {"value": "- There is a lot of flafla text that could be trimmed down and a lot of unnecessary math equations that could be removed and replaced with a figure or one paragraph.\n- its overcomplicated for no reason, explain in a simpler way\n- results sections need major rework\n- showing only good numbers in the paper and leaving bad ones in the appendix\n- lack of true ablation\n\nSee the \"Summary\""}, "questions": {"value": "Can you explain why you need a fifth-degree polynomial for the time schedule. This seems extremely overengineering, like a simple linear line from 0 to 1 would work. And why do we need feature-specific path?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BXQ9NHnc2f", "forum": "ErED2dvR7Z", "replyto": "ErED2dvR7Z", "signatures": ["ICLR.cc/2026/Conference/Submission18697/Reviewer_5EfX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18697/Reviewer_5EfX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184414834, "cdate": 1762184414834, "tmdate": 1762928397403, "mdate": 1762928397403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}