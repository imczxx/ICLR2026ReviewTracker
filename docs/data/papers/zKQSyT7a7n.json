{"id": "zKQSyT7a7n", "number": 21189, "cdate": 1758314723984, "mdate": 1759896936777, "content": {"title": "Visuo-Tactile World Models", "abstract": "We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot–object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33\\% better performance at maintaining object permanence and 29\\% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35\\% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM shows data efficiency when targeting a new task, outperforming a behavioral cloning policy  by over 3.5$\\times$ in success rate with limited demonstrations.", "tldr": "Visuo-Tactile World Models (VT-WM) combine vision and touch to capture contact dynamics, yielding more faithful imagination and up to 35% higher zero-shot planning success in real robot manipulation.", "keywords": ["world models", "robotics", "tactile sensing"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10ae9451be3e3d43fa0069a28f740ab6176ec26a.pdf", "supplementary_material": "/attachment/4118bf8ce891cb53ec839a804c8617ade5d32856.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes Visuo-Tactile World Model (VT-WM) for contact-rich robot manipulation. The model encodes RGB images and images from fingertip vision-based tactile sensors, performs action-conditioned latent rollouts with a transformer, and plans via CEM toward a vision-only goal image."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- paper is well structured and easy to read.\n- motivation is good. Vision only world models do lack the capability to handle contact-rich scenarios\n- The examples are compelling and convincing, eg. the robot arms grasping blue cube.\n- The experiment design and results are interesting."}, "weaknesses": {"value": "- The paper presents a visuo-tactile world model, but the tactile stream is limited to vision-based tactile images (Digit 360). That is a narrow slice of touch compared to other modalities (force/torque, capacitive arrays, pressure taxels, vibration, thermal, shear/strain, proprioception at contact, etc.). As written, the title/claims risk overgeneralizing beyond the evaluated sensing class.\n\n- Most data come from similar tabletop setups and Digit 360 configuration. This couples performance to a specific sensor, mount, and fingertip geometry, which can limit transfer. See prior work (e.g., AnySkin, Bhirangi et al., ICRA 2025; see Fig. 7) for the cross-sensor gaps for vision-based tactile."}, "questions": {"value": "- Could you clarify the taxonomy (e.g., “vision-based tactile”) in title/abstract or limitations; discuss what would change for non-vision tactile sources (signal statistics, encoder design, synchronization, calibration)?\n- The rollout method is quite compute-heavy, how long does it take to finish a simple task?\n- How does the performance of VT-WM compare with VT diffusion policy baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1knP8yCsUq", "forum": "zKQSyT7a7n", "replyto": "zKQSyT7a7n", "signatures": ["ICLR.cc/2026/Conference/Submission21189/Reviewer_8r8W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21189/Reviewer_8r8W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761270343333, "cdate": 1761270343333, "tmdate": 1762941598042, "mdate": 1762941598042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes training world models with tactile sensing to enable world models that remain grounded in physical contact. The paper validates the superiority of tactile world model in contact-rich scenarios and also shows the usefulness of such world models for real world planning through CEM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important problem of developing physically grounded world models using tactile sensing as an additional signal.\n- The experiments in the paper are nicely organized with an effort to assess statistical significance of the results.\n- The authors also show the ability to perform zero-shot planning using their proposed world model using CEM.\n- Further, the authors highlight the data efficiency of training models models for real world planning as opposed to specialized task-specific policies."}, "weaknesses": {"value": "Including both weaknesses as well as questions tied to the weaknesses below.\n- In Section 4.3, what if one trained a multitask BC policy instead of a task-specific policy. I am assuming  the authors have actions available from the training data collected for training the world model. I am curious to see if multitask BC training exhibits similar data efficiency.\n- I would be curious to see the difference in performance if the world model that takes tactile readings as input but doesn’t predict tactile outputs. This will throw some light on the need for reconstructing all observation modalities versus only reconstructing vision (with all modalities as input)."}, "questions": {"value": "It would be great if the authors could address questions in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kDtz2S7UYl", "forum": "zKQSyT7a7n", "replyto": "zKQSyT7a7n", "signatures": ["ICLR.cc/2026/Conference/Submission21189/Reviewer_KZLz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21189/Reviewer_KZLz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850042566, "cdate": 1761850042566, "tmdate": 1762941597145, "mdate": 1762941597145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduce a visuo-tactile world models which help capture physics of contact through touch. The authors motivation based on the leverage tactile sensing to complement the vision. The authors aim to train a general purpose multi-modal world models for planning. The tactile is not in the final planning goal but in a more implicit way to help the planning.\n\nWhile the zero-shot planning is appreciate, the paper failed to show the benefit of using visuo-tactile world model over BC policy training from scratch. The paper do not fully support the \"data efficiency\" due to baseline selection(baseline train from scratch). Also, the paper do not fully justify the usage of tactile information due to task selection(tasks are simple which could be completed with vision only policy)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well presented, with clear illustrations and a coherent narrative. It is easy to read and effectively conveys the authors’ key ideas. The results are clearly reported."}, "weaknesses": {"value": "Minor weakness:\n\n1. Additional visualization of the tactile signals would help readers grasp the Digit 360 modality. In Fig. 2, the four tactile images are difficult to distinguish. Consider including the object’s CAD model and/or overlays highlighting contact regions to clarify how the signals correlate with surface geometry.\n\n2. L139–140: ‘For instance, when manipulating an object in-hand, touch provides context about forces, slip, and subtle pose changes.’ It’s true that humans can interpret gel deformation or temporal changes to understand forces, slip, and subtle pose changes. Do the model or the current architecture actually capture this information? I would be interested to see further evidence that such physical information is leveraged by the model, instead of only reporting success rates over several simple tasks.\n\n3. Would be interested to see the tactile signals for both t_k timestep and t_{k+1}. This could better help reader to understand how your model predicted based on the history.\n\n4. Its hard to say reach button, push fruits, reach and push, stack cubes are tasks that require tactile information to complement the vision. More contact-rich tasks are appreciate.\n\n5. Images in figure 5 are hard to recognize. It make reader hard to get the information the author want to carry out.\n\n6. In figure 5, the author mentions in the vision only, the object less visible. Actually, its still less \"visible\" for both methods in the camera view. The author could show tactile signals for VT-WM, if it can be clearly reflected in tactile images, we can consider its more visible in tactile view.\n\n7. Line 358-360. \"This highlights the V-WM’s difficulty to distinguish between contact and non- contact  states  based  on  visual  input  alone.\" I agree the statement for vision-only. Would be appreciate what levels of contact information can digit 360 and the model could extract from the tactile signals, contact or not only, slip, shear force, normal force. A detailed description of tactile raw signals' performance and the model's capabilities to extract tactile information are appreciate.\n\nMajor weakness:\n\n1. Would be better to specify the dataset used to train the world model in the main text. Eg. details in A.0.1 in Appendix. I still curious how large the dataset is and how many data pair(proprioception, vision, tactile) you used for the training. As the allegro hand generally is less stiff, a small control difference will largely change the tactile signals in fingertip( but the object may still be grasped in hand). I assume it may require large data to capture such physics between hand control and tactile.\n\n2. It is hard to claim ‘data efficiency,’ since a large amount of data is required to train the world model and it includes the tasks used in the downstream evaluation.\n\n3. Regarding the eight tasks in Appendix Fig. 9 used to train the world model, five of those tasks are used for final fine-tuning and for the results reported in the main text.  It would be better to exclude those tasks from world-model training and try more contact-rich tasks outside these eight to see whether it performs better or requires less data. Currently, I only see a new task. placing a plate in a dish rack, which is a simple pick-and-place and does not require tactile sensing.\n\n4. When reporting the numbers in Figure 8, five of the eight training tasks are included. I assume the tasks ‘scribble with marker,’ ‘insert lampshade,’ and ‘insert table leg’ are more complex, which is understandable. Rather than avoiding report their results, I’m more curious about what limits these tasks from succeeding, or what should be added to make them work, for example, adding more data to train the world model or adding more data for fine-tuning.\n\n5. It is actually unfair to compare a fine-tuned world model with a simple BC policy trained from scratch, since training the world model requires a lot of additional data, and the downstream tasks are also included in the world model. A more comparable baseline would be preferable. For example, using the same dataset to train a pretrained model [1], such as a simple cross-modal vision-and-tactile model, and then fine-tuning a state-of-the-art BC like diffusion policy or ACT. This would be fairer because it uses the same dataset for pretrain, rather than training BC from scratch.\n\n[1] ViTaMIn: Learning Contact-Rich Tasks Through Robot-Free Visuo-Tactile Manipulation Interface.\n\n6. Are there any baseline comparisons for the tactile encoder? For example, why use the Sparsh-X tokenizer? Baselines to justify the Cosmos tokenizer and the Sparsh-X tokenizer would be appreciated."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZSCSeVX1Do", "forum": "zKQSyT7a7n", "replyto": "zKQSyT7a7n", "signatures": ["ICLR.cc/2026/Conference/Submission21189/Reviewer_MMZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21189/Reviewer_MMZP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892134940, "cdate": 1761892134940, "tmdate": 1762941595969, "mdate": 1762941595969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an action-conditioned world model that simultaneously takes visual and tactile inputs and predicts future visual and tactile observations. To demonstrate effectiveness, the authors conduct experiments to assess object permanence and causal compliance. Additionally, the authors apply the model to robotic manipulation and report data efficiency gains over behavior cloning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Novelty: To the best of my knowledge, this paper presents one of the first visuo-tactile world models.\n* Thorough experiments: The paper includes experiments on video generation, robotic manipulation, and data efficiency. These comprehensive studies make the work quite solid."}, "weaknesses": {"value": "* The explanation for causal compliance is not entirely satisfactory. As shown in Fig. 7, VT-WM appears to understand contact and predicts the cloth to be static. However, in theory, the model could also hallucinate incorrect tactile images and predict contact. Adding a tactile modality does not necessarily guarantee causal compliance. Based on Fig. 7, it is difficult to rule out cherry-picking, and the evidence does not fully establish causal compliance. For example, in Fig. 7, it seems plausible that VT-WM could imagine contact and predict object motion.\n* The robot planning formulation is unclear. The authors claim the search space is R^7, covering the robot hand pose and gripper closure. First, this search space is still large; considering the planning horizon H, the problem remains challenging, and more details are needed on how the authors tackle it. Second, a dexterous hand has many more degrees of freedom than simple closing/opening. Reducing high-DoF dexterous control to a single DoF significantly simplifies the problem and may undermine the motivation for using a dexterous hand; a parallel-jaw gripper might suffice.\n* The robotic tasks are relatively simple. This likely reflects a broader challenge for the community and may be outside this paper’s scope.\n* Comparing VT-WM + planning to behavior cloning (BC) is questionable. The outcome depends heavily on task design. For challenging, long-horizon, contact-rich tasks, VT-WM + planning may not outperform BC. Therefore, the comparison offers limited insight because results are highly task-dependent and sensitive to design choices. Additionally, VT-WM + planning can leverage prior datasets, whereas BC cannot."}, "questions": {"value": "* What is the inference time of the world model, and what is the speed of robot planning? Real-time performance is important for practical robotics.\n* How consistent are the tactile and visual predictions? Is it possible for the imagined tactile signal to indicate contact while the visual prediction suggests otherwise?\n* I may have missed this, but what is the control horizon for each robotic task?\n* Could the authors provide more details on the CEM method, such as pseudocode?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L72SdoXOQr", "forum": "zKQSyT7a7n", "replyto": "zKQSyT7a7n", "signatures": ["ICLR.cc/2026/Conference/Submission21189/Reviewer_A4Jw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21189/Reviewer_A4Jw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021413484, "cdate": 1762021413484, "tmdate": 1762941595289, "mdate": 1762941595289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}