{"id": "HZ0YvjVzpj", "number": 8169, "cdate": 1758072210987, "mdate": 1759897802204, "content": {"title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation", "abstract": "Model merging aims to integrate multiple task-specific models into a unified model that inherits the capabilities of the task-specific models without additional training. Existing model merging methods often lack consideration of the varying contribution ratios of different task-specific models to the final merged model. In this paper, we propose Mixup Model Merge ($M^3$), a simple yet effective method inspired by the randomized linear interpolation strategy from the Mixup data augmentation technique. $M^3$ performs randomized linear interpolation in parameter space between two task-specific LLMs, where interpolation coefficients are sampled from a Beta distribution to explore diverse contribution ratios. This controllable randomness allows $M^3$ to outperform standard equal-ratio merging by discovering better contribution ratio combinations. Extensive experiments show that $M^3$ significantly: improves merged LLM performance across tasks; enhances out-of-distribution and adversarial robustness; outperforms the positive effects of the sparsification method DARE on model merging and can be further combined with DARE to achieve superior results; and by tuning the Beta distribution’s shape parameters, $M^3$ balances exploration efficiency and diversity in contribution ratios. The code is provided in the supplementary materials.", "tldr": "Mixup Model Merge (M³) is a simple yet effective method that improves model merging by performing randomized parameter interpolation between task-specific LLMs using a Beta distribution to optimize contribution ratios.", "keywords": ["Model Merging", "Mixup Interpolation", "Large Language Models (LLMs)"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4633e2c9bb33da8467f4fc1633544c9c1a7f5925.pdf", "supplementary_material": "/attachment/ea314a5094b6c5c29ac7c9c65a57dd597af94778.zip"}, "replies": [{"content": {"summary": {"value": "This paper propose Mixup Model Merge to perform linear interpolation between two task-specific LLMs. The interpolation coefficients are sampled from a Beta distribution, which is highly motivated by Mixup. The experiments and discussions are sound."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. An interesting perspective on the coefficients selection of model merging.\n\n2. The experiments reveals that the proposed method not only can improve the merging performance but also have positive influence on the robustness of the merged model.\n\n3. The proposed method is simple yet effective. The conflict-cancellation observation (interpolated deltas can nullify opposing signs at specific parameters) offers an intuitive mechanism for stability/benefit."}, "weaknesses": {"value": "1. It seems that the method can only work on model merging between two models. Can the authors provide further discussions on model merging between multiple models? Currently the contribution’s generality is implied but not evidenced. Model merging between two models is quite an easy problem. \n\n2. The method linearly mixes all parameters. An ablation that interpolates only selected layers/blocks could reveal where most gains arise and reduce risk when models are heterogeneous.\n\n3. For completeness, the authors may consider adding instruction-jailbreak to diversify robustness claims.\n\n4. Stability across seeds: What is the variance over multiple λ_m draws for a fixed α? Please report mean±std over (say) 5–10 draws to quantify reliability and avoid cherry-picking."}, "questions": {"value": "See weakness. I'm welling to increase the rating once my concerns are well addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EAcm2l3NHd", "forum": "HZ0YvjVzpj", "replyto": "HZ0YvjVzpj", "signatures": ["ICLR.cc/2026/Conference/Submission8169/Reviewer_t1C3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8169/Reviewer_t1C3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797949121, "cdate": 1761797949121, "tmdate": 1762920132143, "mdate": 1762920132143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Mixup Model Merge (M³), a method for merging task-specific large language models (LLMs). The key idea is to perform a randomized linear interpolation between the parameters of two fine-tuned models, where the interpolation coefficient is sampled from a Beta distribution. This approach explores different contribution ratios between the two models, moving beyond the standard practice of using fixed or equal ratios. The authors demonstrate through extensive experiments that M³ can improve the performance of merged models across various tasks, enhance their out-of-distribution (OOD) and adversarial robustness, and work well alongside existing methods like sparsification (DARE). The method is presented as simple, plug-and-play, and effective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of applying a Mixup-inspired, randomized interpolation strategy directly in the parameter space for model merging seems novel and intuitive.\n\n2. The paper is well-supported by comprehensive experiments. The evaluation covers multiple tasks (instruction following, math, code), multiple merging methods (Average, Task Arithmetic, TIES, DARE), and, importantly, extends to OOD and adversarial robustness, which are crucial for real-world applicability. The results consistently show performance gains, making a strong case for the method's effectiveness.\n\n3. The paper is well-written and easy to follow. The motivation is clearly explained, and the figures (like the Beta distribution visualization and the potion-mixing analogy) help in understanding the methodology."}, "weaknesses": {"value": "1. The experiments focus exclusively on merging pairs of models. A key question is how well the proposed method scales to merging more than two models simultaneously. The current formulation for two models is clear, but its extension to multiple models is not discussed and could be more complex.\n\n2. While the effect of \\alpha on the distribution shape is well-explained, the paper lacks clear guidance or an intuitive strategy for choosing \\alpha for a given pair of models or tasks. The choice seems to be made empirically via a sweep. A deeper analysis or heuristic for selecting \\alpha would make the method more user-friendly.\n\n3. The process for selecting the final merged model from the multiple samples is not explicitly stated. Is the best model on a held-out validation set chosen? Or is it selected based on average performance? This important practical detail should be clarified."}, "questions": {"value": "How can the proposed method be used to integrate more than two models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Pix8D8nUMr", "forum": "HZ0YvjVzpj", "replyto": "HZ0YvjVzpj", "signatures": ["ICLR.cc/2026/Conference/Submission8169/Reviewer_489T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8169/Reviewer_489T"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808088397, "cdate": 1761808088397, "tmdate": 1762920131771, "mdate": 1762920131771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Mixup Model Merge, a plug-in that replaces the usual equal weighting when fusing two task-specific LLMs with a single random coefficient drawn from a Beta distribution. However, it lacks clear motivation, presents limited novelty, and provides an insufficiently explained methodology."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Mixup Model Merge is a plug-and-play enhancer that grafts onto four existing merge methods with zero re-training and only seven forward passes; it systematically boosts every method on seven standard benchmarks (20 tables), yielding a merged model that beats both specialists on 4/5 datasets, and its validity is underpinned by the flat-basin observation that linear interpolation of same-pre-checkpoint fine-tunes stays in low loss."}, "weaknesses": {"value": "1)Statistical significance and uncertainty quantification missing: Performance Drop Rate (PDR) for adversarial sets is based on a single attack seed (Table 2; Appendix G). No direct evidence found in the manuscript that results persist with more λm draws or different seeds.\n\n2)Scalability and practical constraints unaddressed: Experiments restricted to 13-B Llama-2 pairwise merges; no evidence on larger models, >2 parents, or different architectures (Sec. 4). \n\n3)Robustness claims overstated without distributional detail: “Enhances adversarial robustness” (Abstract) rests on ≤3.2 pp PDR reduction in StressTest but absolute accuracies after attack remain low. No transfer evaluation across attack types; gains vanish under DeepWordBug for LM & Math (Table 6).\n\n4)The symbols δt1, δt2 appear without prior definition in Eq. 3, forcing the reader to infer their meaning from later text; introducing them explicitly before first use would remove ambiguity."}, "questions": {"value": "1)Could you clarify the core motivation behind this approach? Specifically, how does it address the challenge of merging more than two models effectively? \n\n2)While the theoretical analysis is presented, its significance isn't fully clear: how do these theoretical insights directly support or validate the final empirical conclusions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JngmffQrY1", "forum": "HZ0YvjVzpj", "replyto": "HZ0YvjVzpj", "signatures": ["ICLR.cc/2026/Conference/Submission8169/Reviewer_6rPz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8169/Reviewer_6rPz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983175676, "cdate": 1761983175676, "tmdate": 1762920131117, "mdate": 1762920131117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Mixup Model Merge (MMM), a novel method for parameter-level model merging that leverages data-level Mixup augmentation to enhance generalization and robustness across varying models. Experiments demonstrate its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and clearly presented.\n\n2. The experiments are extensive and provide strong empirical support for the proposed method."}, "weaknesses": {"value": "1. Novelty (a) The design of the Beta distribution sampling in M³ appears arbitrary and lacks justification. The chosen values of the shape parameter $\\alpha$ vary widely across different base methods (from 0.4 to 2), yet the authors provide no explanation for this phenomenon or its potential impact on model merging performance. (b) the proposed weighted method seems to be a general formulation of task arithmetic, not novel enough.\n\n2. The paper claims that the proposed design enhances robustness against adversarial attacks, but no detailed analysis or theoretical reasoning is provided to explain why this design would be effective in such scenarios.\n\n3. There are no ablation studies exploring the sensitivity of key hyperparameters $\\alpha$ and $\\lambda$ Moreover, the scalability of the proposed method is questionable, as these parameters vary significantly across different base methods (see Table 1), suggesting limited generalizability.\n\n4. lack baselines.  There are some merging methods based on expert routing [1]. Including such baselines would strengthen the empirical evaluation and contextualize the contribution.\n\n[1] MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1vDq4VqeAu", "forum": "HZ0YvjVzpj", "replyto": "HZ0YvjVzpj", "signatures": ["ICLR.cc/2026/Conference/Submission8169/Reviewer_fajz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8169/Reviewer_fajz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008068230, "cdate": 1762008068230, "tmdate": 1762920130483, "mdate": 1762920130483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}