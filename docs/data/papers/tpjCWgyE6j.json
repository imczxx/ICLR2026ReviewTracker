{"id": "tpjCWgyE6j", "number": 21484, "cdate": 1758318086500, "mdate": 1763587340606, "content": {"title": "Policy Regret Minimization in Partially Observable Markov Games", "abstract": "We study policy regret minimization in partially observable Markov games (POMGs) between a learner and a strategic adaptive opponent who adapts to the learner's past strategies. We develop a model-based optimistic framework that operates on the learner-observable process using \\emph{joint} MLE confidence set and introduce an Observable Operator Model-based causal decomposition that disentangles the coupling between the world and the adversary model. Under multi-step weakly revealing observations and a bounded-memory, stationary and posterior-Lipschitz opponent, we prove an $\\mathcal{O}(\\sqrt{T})$ policy regret bound. This work advances regret analysis from Markov games to POMGs and provides the first policy regret guarantee under imperfect information against an adaptive opponent.", "tldr": "Policy Regret Minimization in Partially Observable Markov Games", "keywords": ["Partially Observable Markov Games", "Policy Regret", "Weakly-Revealing", "Observable Operator Model"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43fad34526654796e77c781565f686a8c5c9fef4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the first unified framework for policy regret minimization in partially observable Markov games (POMGs) against adaptive opponents. The authors achieve sublinear policy regret under bounded-memory and weakly revealing conditions. Some new techniques, such as the Posterior-Lipschitz assumption and operator decomposition for POMGs are provided."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a theoretical policy regret analysis about POMGs. The theoretical analysis is solid and comprehensive. \n\n2. The algorithm in this paper successfully combines the Optimistic MLE in [1] for POMDP, some assumptions and techniques in [2] for policy regret minimization, and batch analysis in [3] for policy low-switching. The final algorithm successfully solves Partially Observed Markov Games.\n\n3. The paper is well-structured. It also contains some sketches for the Appendix. The algorithms are presented in a clear way. \n\n[1]. Qinghua Liu, Alan Chung, Csaba Szepesva ́ri, and Chi Jin. When is partially observable reinforcement learning not scary?\n\n[2]. Thanh Nguyen-Tang and Raman Arora. Learning in markov games with adaptive adversaries: Policy regret, fundamental barriers, and efficient algorithms.\n\n[3]. Nuoya Xiong, Zhaoran Wang, and Zhuoran Yang. A general framework for sequential decisionmaking under adaptivity constraints."}, "weaknesses": {"value": "1. Although the paper is technically dense, its methodological novelty appears limited. The main proof largely combines the OMLE techniques from [1] with the policy regret algorithm from [2], without introducing substantial new technical contributions. The main difference between OMLE and this paper is that it contains the class of adversarial channel $g$ in the MLE oracle. However, it will not introduce intrinsic difficulty since MLE analysis still works. \n\n2. The proposed algorithm seems difficult to implement in practice. It involves solving a constrained optimization problem whose structure does not appear to lend itself to tractable solution methods. The algorithm also contains the adversarial channel in the MLE oracle, which can make this constraint optimization harder to solve. \n\n3. Several technical definitions are introduced without sufficient motivation or intuitive explanation. For example, Section 4.3 presents the definitions of the eluder dimension and the function class directly, which may be challenging for readers unfamiliar with prior work on POMDPs and RL theory. Providing intuitive explanations or brief context for these definitions would improve readability and accessibility.\n\n\n[1]. Qinghua Liu, Alan Chung, Csaba Szepesva ́ri, and Chi Jin. When is partially observable reinforcement learning not scary?\n\n[2]. Thanh Nguyen-Tang and Raman Arora. Learning in markov games with adaptive adversaries: Policy regret, fundamental barriers, and efficient algorithms."}, "questions": {"value": "1. Could the author explain why the Posterior-Lipschitz assumption is necessary? It seems that this assumption is used in Lemma 7 to get an upper bound $\\Delta_\\sigma(\\pi,\\upsilon)$, which then reappears in Lemma 10 as an upper bound term. After that, the paper seems to treat this term as a constant. Then, why the Posterior-Lipschitz assumption necessary? It would be helpful if the authors could explain the role and necessity of this assumption more explicitly.\n\n2. In Line 216, is it correct that the conditional distribution of $\\tau_A$ given $\\tau_B$ should also depend on the policy of both agent and the adversarial? \n\n3. Some notations like $g(\\cdot \\mid \\tau_B, \\pi^{1:m})$ should be clarified before they are used. The paper only defines $g$ as a function that maps $\\Pi^M$ to the adversarial policy space.\n\n4. Do the authors have any insights or suggestions on how to solve the constrained optimization problem in practice? Are there some potential empirical applications for this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MsVpb95Iuo", "forum": "tpjCWgyE6j", "replyto": "tpjCWgyE6j", "signatures": ["ICLR.cc/2026/Conference/Submission21484/Reviewer_9NX7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21484/Reviewer_9NX7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878622389, "cdate": 1761878622389, "tmdate": 1762941799927, "mdate": 1762941799927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies policy-regret minimization for a learner playing a partially observable Markov game (POMG) against an adaptive, bounded-memory, stationary opponent. The authors propose a batched, model-based algorithm (MOMLE) that maintains a single joint confidence set over the world model and the opponent’s response model via optimistic MLE on the learner-observable process. A key technical ingredient is an OOM-based “causal decomposition” of per-step operators into a world channel and an adversary channel, enabling a telescoping analysis. Under multi-step weakly revealing observations and a posterior-Lipschitz opponent, the method achieves a sublinear policy-regret bound."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. First policy-regret result in POMGs (as far as I know). Extends recent MG results to imperfect information with adaptive opponents.\n2. The OOM factorization into world/adversary channels plus a two-stage telescoping bound is technically interesting and seems reusable."}, "weaknesses": {"value": "1. Although the policy regret setup makes sense in general, it makes less sense in POMG, which features decentralized information. Specifically, how can the adversary response map depend on the learner's past policies since such learner's information is almost never available to the adversary in a decentralized setup.\n\n2. Although I understand Assumption 1.1&1.2 is necessary for tractable algorithms, it again makes less sense to me. In my opinion, it is almost saying that the opponent is ``stationary''. This kind of defeats the purposes of considering adaptive opponents. In fact, the policy regret considered in this paper is much weaker than the standard external regret which allows adversarial opponents. As a side note, the regret considered by liu et al, 2022 is not by definition external regret in my opinion. It is a self-play setting where both players are controlled by a certain algorithm. For the actual external regret guarantee, plz refer to [1, 2], which is in most cases hard. This further raises questions regarding how interesting it is to study policy regret.\n\n3. Based on the intuition that Assumption 1.1&1.2 make the opponent almost stationary, it is kind of expected that the problem reduces to a single-agent POMDP (up to extensions on the state space to incorporate the finite memory dependence of the adversary).\n\n4. Assumption 1.3 also lacks justifications and requires explanations. It is unclear whether it is fundamental or only makes analysis possible.\n\n[1]. Liu, Qinghua, Yuanhao Wang, and Chi Jin. \"Learning markov games with adversarial opponents: Efficient algorithms and fundamental limits.\" International Conference on Machine Learning. PMLR, 2022.\n\n[2]. Foster, Dylan J., Noah Golowich, and Sham M. Kakade. \"Hardness of independent learning and sparse equilibrium computation in markov games.\" International Conference on Machine Learning. PMLR, 2023."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T9LJ1YHtgr", "forum": "tpjCWgyE6j", "replyto": "tpjCWgyE6j", "signatures": ["ICLR.cc/2026/Conference/Submission21484/Reviewer_6LiP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21484/Reviewer_6LiP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020315768, "cdate": 1762020315768, "tmdate": 1762941799611, "mdate": 1762941799611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of minimizing policy regret against an adaptive adversary in Markovian Games with partial observability. As noted by this paper, this is a technically challenging problem at multiple levels, with policy regret even under full observability requiring structural conditions for sample efficient learning. The main contribution is in identifying reasonable conditions under which policy regret minimization is possible in POMG — specifically restrictions on the nature of the adaptive adversary - i.e. memory bound, stationary over time and a novel assumption about posterior-Lipschitzness, which is a condition about the stability of the adversary’s repsonses to different learner sequences that induce similar posterior beliefs about the learner’s future behavior. Additional assumptions are made about the nature of observability (required for the any reasonable inference about the hidden state) and the Eluder dimension, a complexity measure of the joint world state - adversary trajectories that can generate the observed states.  Under these assumptions, they provide an algorithmic result, building upon existing tools for POMGs and policy regret minimizing along with novel technical analysis to break through unique roadblocks due to the combination of an adaptive adversary and partial observability. Specifically, they adapt the OOM framework of Liu et al. after using a novel technical tool to causally disambiguate the world state from the adversary actions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper solves a genuinely difficult problem, stitching together tools for two problems with different sources of complexity -- partial observability and policy regret. Based on my limited experience in this field, this result appears to be a significant technical advancement of the field and is worthy of acceptance on those grounds."}, "weaknesses": {"value": "NA"}, "questions": {"value": "Is there any interplay between the assumptions about the adaptive adversary and about the nature of the observability and complexity of the POMG or are they uncoupled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yn3sMM1MXq", "forum": "tpjCWgyE6j", "replyto": "tpjCWgyE6j", "signatures": ["ICLR.cc/2026/Conference/Submission21484/Reviewer_eex2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21484/Reviewer_eex2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150682002, "cdate": 1762150682002, "tmdate": 1762941799358, "mdate": 1762941799358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We have updated the paper and highlighted the changes in blue, based on the current suggestions from our reviewers. We are happy to make further revisions based on our ongoing discussions with the reviewers.\n\nThe revisions consist of (i) expanding Section 1.1 (Overview of Techniques) to better explain the new tools used in our analysis, (ii) clarifying the definition of Posterior-Lipschitz in Section 3.1, (iii) adding an introduction and intuitive explanation of the Eluder condition in Section 4.3, and (iv) adding to Appendix F.1 a counterexample illustrating that, in the absence of Posterior-Lipschitz, multi-step weakly revealing dynamics alone are insufficient to guarantee sublinear policy regret."}}, "id": "bpyOWy3iuK", "forum": "tpjCWgyE6j", "replyto": "tpjCWgyE6j", "signatures": ["ICLR.cc/2026/Conference/Submission21484/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21484/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission21484/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763590108432, "cdate": 1763590108432, "tmdate": 1763643564841, "mdate": 1763643564841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}