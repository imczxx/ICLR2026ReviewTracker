{"id": "zF0A0xw3HZ", "number": 14066, "cdate": 1758227869039, "mdate": 1763718101172, "content": {"title": "vCache: Verified Semantic Prompt Caching", "abstract": "Semantic caches return cached responses for semantically similar prompts to reduce LLM inference latency and cost. They embed cached prompts and store them alongside their response in a vector database. Embedding similarity metrics assign a numerical score to quantify the similarity between a request and its nearest neighbor prompt from the cache. Existing systems use the same static similarity threshold across all requests to determine whether two prompts can share similar responses. However, we observe that static thresholds do not give formal correctness guarantees, can result in unexpected error rates, and lead to suboptimal cache hit rates. This paper proposes vCache, the first verified semantic cache with user-defined error rate guarantees. It employs an online learning algorithm to estimate an optimal threshold for each cached prompt, enabling reliable cache responses without additional training. Our experiments show that vCache consistently meets the specified error bounds while outperforming state-of-the-art static-threshold and fine-tuned embedding baselines with up to 12.5$\\times$ higher cache hit and 26$\\times$ lower error rates. We release the vCache implementation and four benchmarks to support future research.", "tldr": "", "keywords": ["Semantic Prompt Cache"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b5401bd1799e2c142906fc3f645d016f455e64c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes using dynamic thresholds for different prompts in semantic caching, thereby providing an error rate guarantee. Based on a sigmoid parametric model and observed samples, the proposed online learning algorithm continually estimates the optimal thresholds for cached prompts. Experimental results demonstrate its effectiveness in controlling error rates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well structured and easy to follow.\n- Guaranteed error rate is a crucial issue for semantic caching, and the proposed online-learning-based dynamic threshold method provides a reasonable solution."}, "weaknesses": {"value": "- Although vCache can guarantee the error rate of the semantic cache, it heavily relies on the observation of correctness (Algorithm 1, Line 8). In the experimental section, the authors propose using exact matching for short prompts and LLM inference for long prompts to determine correctness. The additional LLM inference introduces extra cost for the semantic cache, which undermines its practical value. Moreover, the LLM could make mistakes in judgment, making the guarantee unreliable.\n- Why is the actual error rate in Figure 4 much lower than $\\delta$? According to the guarantee, they should not differ by such a large margin.\n- Despite the controlled error rate, the risk of private data leakage still exists, which limits its practical adoption in industry.\n- Since LMArena contains many testing prompts, the high hit rate shown in Figure 4 might result from these testing cases. Can the authors provide details about the hit prompts for the three benchmarks?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cNO1vGEK2T", "forum": "zF0A0xw3HZ", "replyto": "zF0A0xw3HZ", "signatures": ["ICLR.cc/2026/Conference/Submission14066/Reviewer_sFQ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14066/Reviewer_sFQ7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595759262, "cdate": 1761595759262, "tmdate": 1762924546545, "mdate": 1762924546545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes vCache, a caching system that optimizes GPT-Cache and dynamically returned the cached responses subjected to the pre-defined error rate. Compared to the previous works that relies on a static threshold for all incoming queries and all cached responses, this paper adopts online learning to estimate a threshold for each cached response. To help evaluate the performance of GPT-cache style caching systems, the author generates three benchmarks to help evaluate the performance. Abundant empirical experiments on different models show the effectiveness of this method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper tackles on an important topic of GPT-cache and provides unique perspectiives on how to optimize this problem. Static threshold is a fundmental problem that prevents GPT-cache for better selecting cached responses.\n- The evaluation is holistic and abundant. The author also presents large-scale benchmarks for helping research in this field.\n- Author also presents good theoretical guarantees for showing the effectiveness of this method,"}, "weaknesses": {"value": "- Dataset Generation neglects no match scenario. I checked the appendix about how the data is generated. For instance, for the SemCacheLMArena, 1 to 23 similar prompts will be generated. In reality, there could be many prompts where no similar answer in the cache can be fetched directly. I would suggest adding many prompts where no similar one variants are included should be helpful.\n- Though the experiments regarding error rate is abundant, more analysis of latencies and throughputs should be added."}, "questions": {"value": "- For creating the dataset SemCacheLMArena, how to ensure the sampled prompts are not really distinct? \n- Where do you define? I guess it's time per response?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wZtCJsq6kt", "forum": "zF0A0xw3HZ", "replyto": "zF0A0xw3HZ", "signatures": ["ICLR.cc/2026/Conference/Submission14066/Reviewer_e1XS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14066/Reviewer_e1XS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949880974, "cdate": 1761949880974, "tmdate": 1762924545973, "mdate": 1762924545973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes vCache, a novel semantic caching system for LLMs that provides user-defined error rate guarantees while learning embedding-specific thresholds online. The key innovation is replacing static global thresholds with dynamic, per-embedding thresholds estimated through an online learning algorithm that models the probability of correctness using sigmoid functions. The method is evaluated on three benchmarks across different embedding models and LLMs, demonstrating superior performance compared to static threshold baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a practical challenge of the semantic prompt caching \n- The proposed method is well-motivated"}, "weaknesses": {"value": "- The writing is a bit repetitive and could be streamlined"}, "questions": {"value": "Thank you for your submission. I appreciate the motivation behind providing formal correctness guarantees for semantic caching systems, which is indeed a significant limitation of existing approaches. The experimental validation across multiple benchmarks and the introduction of new evaluation datasets are valuable contributions. However, several aspects of the paper require clarification and the technical approach raises some concerns:\n\n- The sigmoid modeling assumption (Equation 9) is quite strong but not well justified. Why should the relationship between similarity and correctness follow a sigmoid specifically? Have you experimented with other parametric families or non-parametric approaches?\n- The confidence band computation for parameters t and Œ≥ (mentioned in Section 4.2 and relegated to Appendix C) is crucial for the guarantees but insufficiently explained in the main text. How sensitive are the guarantees to the choice of confidence level Œµ?\n- Algorithm 2 shows that œÑ is computed by minimizing over Œµ ‚àà [0,1], but this seems computationally expensive for online inference. What is the actual computational overhead of this optimization step?\n- The paper claims vCache \"consistently meets the specified error bounds\" but Figure 4 shows the actual error rate is noticeably below the specified Œ¥. This suggests the method might be overly conservative, potentially sacrificing cache hit rate for unnecessary safety margins. Can you quantify this conservatism?\n- The comparison with GPTCache is somewhat unfair since GPTCache doesn't attempt to provide error guarantees. A more relevant baseline would be other adaptive thresholding methods from the retrieval literature adapted to this setting.\n- The evaluation focuses on relatively simple benchmarks (classification, search queries). How does vCache perform on more complex scenarios like multi-turn conversations or reasoning tasks where semantic similarity becomes more nuanced?\n- The i.i.d. assumption for incoming prompts is quite restrictive in practice. Real-world query distributions often exhibit temporal correlations, user-specific patterns, and concept drift. How robust is vCache to violations of this assumption?\n- Figure 3's motivation is compelling, but the connection to the proposed solution could be clearer. It shows the problem but doesn't intuitively explain why sigmoid modeling would solve it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BpYLamEbbs", "forum": "zF0A0xw3HZ", "replyto": "zF0A0xw3HZ", "signatures": ["ICLR.cc/2026/Conference/Submission14066/Reviewer_yfEW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14066/Reviewer_yfEW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973607230, "cdate": 1761973607230, "tmdate": 1762924545547, "mdate": 1762924545547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes vCache, a semantic LLM caching method that guarantees a user-defined error rate, based on the previous work of GPTCache, a semantic LLM caching method to optimize LLM cost and latency. This work contributes to the domain of LLM caching by introducing per-embedding dynamic threshold, which was improved from the static threshold of GPTCache."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis study algorithmically improves from the existing threshold-based retrieval methods such as GPTCache using a novel approach. It theoretically guarantees a desired error rate and demonstrates improved performance across several metrics by introducing a verified semantic cache.\n2.\tAs an online learning algorithm, the proposed method achieves strong results without fine-tuning the embedding model, thus requiring no additional training.\n3.\tTo validate the effectiveness of the proposed methodology and foster future research, the authors have constructed and publicly released three new benchmark datasets (SemCacheClassification, SemCacheLMArena, and SemCacheSearchQueries) that reflect real-world caching scenarios."}, "weaknesses": {"value": "1.\tSince vCache is still a semantic caching technique, there is an insufficient evaluation of its effectiveness concerning the capability and performance across the underlying embedding model (e.g., BERT). A comparative analysis using various embedding models is needed to substantiate the \"Model-Agnostic\" claim, which currently lacks sufficient analysis.\n2.\tWhile the paper discusses the trade-off between accuracy and cost, the benchmark results show a trade-off between Cache Hit Rate and Error Rate when compared to GPTCache. For instance, in Appendix D.5, the best-case for GPTCache (GS) shows a 5.2% error rate with a 67% hit rate, whereas vCache (LD3) achieves a 2.0% error rate with a 54% hit rate. Although vCache has the distinct advantage of reliably guaranteeing a user-defined error rate (e.g., 2.0%), the lack of in-depth analysis on this trade-off makes it difficult to conclude its superiority across all scenarios. Combined with Weakness 1, this property raises questions about whether the improvement of vCache against GPTCache is marginal or not.\n3.\tAs mentioned in the limitations section, the evaluation relies on an LLM-as-a-judge for benchmarks except SemCacheClassification, which has a clearly defined correctness criterion."}, "questions": {"value": "‚Ä¢\tWere comparative experiments conducted with different embedding models? An analysis of the relationship between the performance of the embedding model and the performance gains from vCache would better substantiate the \"Model-Agnostic\" claim. Beyond the GteLarge and E5-large models presented, were experiments with other BERT models‚Äîsuch as multilingual variants or those employing improved techniques‚Äîconsidered?\n‚Ä¢\tWhen a new embedding is added to the cache, how many observations are required for vCache to learn a stable threshold? I am curious about the analysis of performance degradation during the initial learning phase (the cold-start problem)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZH52wsqNxp", "forum": "zF0A0xw3HZ", "replyto": "zF0A0xw3HZ", "signatures": ["ICLR.cc/2026/Conference/Submission14066/Reviewer_5kcm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14066/Reviewer_5kcm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987396414, "cdate": 1761987396414, "tmdate": 1762924545140, "mdate": 1762924545140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all reviewers"}, "comment": {"value": "We thank all reviewers for their time, constructive comments, and positive evaluations. Below we summarize the main clarifications, new experiments, and additional evidence added in the rebuttal and revised manuscript.\n\n- **New benchmark SemCacheCombo** [Appendix D.2]. We add SemCacheCombo, which combines SemCacheClassification and SemCacheLMArena to explicitly cover workloads with and without cache hits. On this benchmark, vCache consistently outperforms state-of-the-art baselines, achieving up to 12.5√ó higher cache hit rates while satisfying the user-defined error-rate bound.\n- **Additional embedding model** [Appendix D3]. We evaluate a third embedding model, text-embedding-3-small, alongside GPT-4.1-nano, complementing the two embedding models already studied. vCache outperforms the GPTCache baselines with up to 26√ó lower error rates while respecting the user-specified error bound.\n- **Overhead of $\\tau$ computation** [Appendix D.4]. We measure the latency of computing ùúè and show that, across all sample sizes, the per-query overhead remains below 1.5 ms, indicating that the computation of $\\tau$ is negligible compared to LLM latency.\n- **Justification of the sigmoid model** [Appendix H.]. We add experiments analyzing $Pr ‚Å° (c(x) = 1 | s(x))$ as a function of similarity, both per-embedding and aggregated across embeddings, and show that the empirical curves are monotone and S-shaped, supporting our sigmoid-based model in Eq. (9)."}}, "id": "We2kLsKwR9", "forum": "zF0A0xw3HZ", "replyto": "zF0A0xw3HZ", "signatures": ["ICLR.cc/2026/Conference/Submission14066/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14066/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission14066/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763718560223, "cdate": 1763718560223, "tmdate": 1763718560223, "mdate": 1763718560223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}