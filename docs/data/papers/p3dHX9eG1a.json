{"id": "p3dHX9eG1a", "number": 21858, "cdate": 1758322724367, "mdate": 1759896899675, "content": {"title": "How Does Layer Normalization Improve Deep $\\boldsymbol{Q}$-learning?", "abstract": "Layer normalization (LN) is among the most effective normalization schemes for deep $Q$-learning. However, its benefits remain not fully understood. We study LN through the lens of _gradient interference_. A gradient interference metric used in prior works is the inner product between semi-gradients of the temporal difference error on two random samples. We argue that, from the perspective of minimizing the loss, a more principled metric is to calculate the inner product between a semi-gradient and a full-gradient. We test this argument with offline deep $Q$-learning, without a target network, on four classic control tasks. However, counterintuitively, we find empirically that first-order gradient interference metrics _positively_ correlate with the training loss. We empirically show that adding a second-order gradient interference term gives more intuitive results. Theoretically, we provide supporting arguments from the linear regression setting.", "tldr": "", "keywords": ["deep RL", "deep learning", "normalization", "gradient interference", "reinforcement learning", "RL", "off-policy RL", "offline RL"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b8808ac34f5dd271ccfebe5d6e0cecca98ed678.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates why Layer Normalization (LN) improves stability and performance in deep Q-learning. They argue that prior \"gradient interference\" metrics are incomplete. It proposes a mixed-gradient perspective and, via a second-order Taylor analysis of TD-loss decrement, introduces GI2, a metric combining first and second-order terms. Empirically, across four classic offline control tasks trained without a target network, first-order inference metrics counterintuitively increase with training loss, while GI2 decreases with loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear: LN is effective while other normalization often do not work in deep Q-learning.\n- GI2 adds a prinicpiled second-order term to loss-decrement reasoning, aligning theory with observed loss trends better than first-order methods."}, "weaknesses": {"value": "- GI2 omits the Hessian term involving $\\nabla^2 h$. While the paper argues that this term is complex but leaves its contribution untested.\n- MountainCar consistenly breaks the main trends, but brief analysis is given."}, "questions": {"value": "- Can you explain more on the deviation of the abnormal performance of the MountainCar example from the correlation patterns for GI2 and returns?\n- Given GI2's negative correlation with loss, can practitioners monitor a proxy online to decide when to apply LN or adjust learning rates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ckSbf3ez6c", "forum": "p3dHX9eG1a", "replyto": "p3dHX9eG1a", "signatures": ["ICLR.cc/2026/Conference/Submission21858/Reviewer_Y589"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21858/Reviewer_Y589"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760852414178, "cdate": 1760852414178, "tmdate": 1762941958054, "mdate": 1762941958054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides an in-depth investigation into how Layer Normalization enhances performance and stability in Deep Q-Learning, primarily from the perspective of Gradient Interference. The authors present an interesting insight: the first-order gradient interference metric shows a positive correlation with training loss, whereas a second-order corrected metric offers a more intuitive explanation for LN's benefits. This claim is supported by extensive experiments."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The analysis of gradient interference, particularly the introduction of the second-order corrected term (GI₂) to explain LN's role in DQN, demonstrates significant insight to the existing literature. Investigating the impact of LN specifically on Deep Q-Learning is a valuable research direction."}, "weaknesses": {"value": "The logical flow of this article is quite confusing to me, which may be due to my limited familiarity with the field of Q-learning. I hope the author can provide detailed classifications on the following points.\n\n1. The narrative logic is somewhat confusing. The stated goal is to understand how LN improves Deep Q-Learning performance. However, Section 3.1 primarily discusses the effects of different optimizers on various metrics, and Section 3.3 only briefly mentions LN's performance improvements on several benchmarks without delving into the underlying reasons. The core theoretical attempt to answer the \"how\" question is concentrated in Section 3.4. However, the theoretical setting in 3.4 is highly simplified. Using data normalization as a direct analogy for analyzing Layer Normalization might be too trivial and lacks rigor; it feels more like a toy example than a formal theorem.\n\n2. The paper is in a good direction but requires some revisions for presentation. \n- (a) In Table 1, the number of decimal places for the \"random\" column is inconsistent.\n- (b) Phrases like \"second-order–corrected metric in line 61\" are awkwardly phrased. The article uses a large amount of \"–\", which can easily confuse readers with \"-\".\n- (c) The formatting appears unpolished. For instance, there is excessive whitespace around Equation (11) on Page 7."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CTnbkLWaCS", "forum": "p3dHX9eG1a", "replyto": "p3dHX9eG1a", "signatures": ["ICLR.cc/2026/Conference/Submission21858/Reviewer_htXn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21858/Reviewer_htXn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761628271475, "cdate": 1761628271475, "tmdate": 1762941957775, "mdate": 1762941957775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates gradient interference in deep Q-learning, where gradients of the Q function for one state-action pair also affect other state action pairs.\nTo this end, the paper introduces several ways to measure gradient inference.\nThey look at how these correlate with return and loss, and attempt to use this to show that layer normalization helps Q-learning."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper introduces several interesting metrics for gradient interference in Q-learning.\n* There are some surprising results, especially about the relevance of the second order term in gradient interference."}, "weaknesses": {"value": "* The relation to layer normalization is weak. Most of the paper is about gradient inference, and is independent of the architecture. The experiments are done with and without LN, but this is only a comparison, and doesn't give any insights into how LN is actually helping.\n* There are several experiments where the learning rate is too high (figure 2 and figure 3), which causes the training loss to increase to huge values (>1e10). This then also makes some of the other plots useless, for example GI₂ has values around -1e50 for these failed runs.\n  It also makes me question the conclusions that \"SGI and MGI also positively correlate with training loss in our experiments\" and \"GI2 negatively correlates with training loss\". The axes are so stretched that I can't clearly see any such correlations. And these absurd values would dominate any calculated correlation coefficient.\n* If you make a claim of correlation, then this should be supported with some correlation coefficient.\n* Theorem 2: \"Under fixed $R(θ_t)$, $A^2_t/B_t$ is maximized when ∥x∥ is a constant in the data distribution.\"\n  In this theoretical dataset, you can't just normalize $x$ without also affecting $y=x^Tθ$. So this is not a realistic representation of layer normalization. If $y$ was fixed, then scaling $x$ down for example would require $θ$ to scale up, which would then scale up $R(θ)$."}, "questions": {"value": "* \"However, counterintuitively, we find empirically that first-order gradient interference metrics positively correlate with the training loss.\"\n  Why is this unexpected? I would think that less interference = better = lower loss. Or is that not what you meant by a positive correlation?\n* The presentation of deep Q-learning using a \"semi-gradient\" is slightly nonstandard.\n  What this paper calls a semi-gradient is just the gradient of the squared TD-error, no?\n  Is this called a semi-gradient because the gradient is not propagated through the TD-target?\n * With a continuous state space, the self-interference term ($(s,a)=(s_t,a_t)$) in QGI, SGI, and MGI has measure 0, so it does not contribute relevantly to the expectation. Is that understanding correct? And if so, why is a different choice made for QGI and SGI/MGI?\n * \"a standard single-layer neural network\"\n   Clarify this as a \"single hidden-layer neural network\", since some people will say that this network has two layers.\n * \"absolute cosine similarity notation cos+\"\n   Is this definition necessary? You could define normal cosine similarity and write $|\\cos(x,y)|$.\n * in the interpretation of theorem 2: \"$\\max_η\\{2ηA_t − η²B_t\\} = A_t^2/B_t^2$\"\n   Should be $A^2/B$.\n * The definition of $l_\\theta$ is important, but it is split up by a page break and a figure, making it harder to find.\n * There is also a page break in the middle of (*) in section 3.2. The first line of that equation could be incorrectly read as the whole equation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Mnl9M4I1gz", "forum": "p3dHX9eG1a", "replyto": "p3dHX9eG1a", "signatures": ["ICLR.cc/2026/Conference/Submission21858/Reviewer_4CDy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21858/Reviewer_4CDy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848624119, "cdate": 1761848624119, "tmdate": 1762941957578, "mdate": 1762941957578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of Layer Normalization (LN) in improving optimization dynamics of deep neural networks. The authors attempt to provide both theoretical and empirical insights by analyzing gradient scaling, variance stabilization, and optimization landscape smoothness under LN. They present several simplified derivations suggesting that LN reduces gradient variance and improves isotropy, followed by small-scale experiments on MLPs and Transformers to illustrate these effects. The paper aims to unify existing intuitions about LN’s benefits into a coherent explanation framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a fundamental and relevant question in deep learning — understanding why Layer Normalization helps training stability and convergence.\n\nThe overall motivation is clear, and the paper provides a structured narrative linking theory, gradient analysis, and empirical visualization.\n\nThe presentation is relatively readable, and figures (e.g., gradient norm distributions and optimization trajectories) help illustrate the main intuition."}, "weaknesses": {"value": "Symbols like $\\mu_i$, $\\sigma_i$, and $\\gamma$, $\\beta$ switch between layer and neuron-level contexts without explicit indexing. This makes it difficult to follow what the normalization is actually applied to.\n\nLN “preserves the direction of gradients while adjusting their scale,” but no formal proof or Lipschitz analysis is given. This is hand-wavy and lacks formal support.\n\nThe comparison with BatchNorm and RMSNorm is descriptive, not analytical. The same conclusions already appear in several existing studies.\n\nExperiments are only on small-scale models (2-layer MLPs and tiny Transformers). This severely limits the claim of generality.\n\nReported improvements are within 0.2–0.4% on average accuracy — no significance testing or error bars are shown."}, "questions": {"value": "See WeakNess, but I don't really know enough about this area of ​​research, so my evaluation may not be accurate. Perhaps the author's innovation lies more in the theoretical level."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "H1vtFjhup6", "forum": "p3dHX9eG1a", "replyto": "p3dHX9eG1a", "signatures": ["ICLR.cc/2026/Conference/Submission21858/Reviewer_axoo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21858/Reviewer_axoo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986116379, "cdate": 1761986116379, "tmdate": 1762941957356, "mdate": 1762941957356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}