{"id": "31Mr6wLBeF", "number": 4265, "cdate": 1757651004993, "mdate": 1759898042985, "content": {"title": "OPPO: Accelerating PPO-based RLHF via Pipeline Overlap", "abstract": "Proximal Policy Optimization (PPO)-based reinforcement learning from human feedback (RLHF) is a widely adopted paradigm for aligning large language models (LLMs) with human preferences. However, its training pipeline suffers from substantial inefficiencies due to sequential multi-model dependencies (e.g., reward model depends on actor outputs) and long-tail response lengths, where a few long responses straggle the stage completion. We present OPPO, a novel, lightweight, and model-agnostic PPO-based RLHF framework that improves training efficiency by overlapping pipeline execution. OPPO introduces two novel techniques: (1) Intra-step overlap, which streams upstream model outputs (e.g., actor model) in right-sized chunks, enabling the downstream model (e.g., reward) to begin prefill while the upstream continues decoding; and (2) Inter-step overlap, which adaptively overcommits a few prompts and defers long generations to future steps, mitigating tail latency without discarding partial work. OPPO integrates easily with existing PPO implementations with a few lines of code change. Extensive evaluations show that OPPO accelerates PPO-based RLHF training by $1.8\\times$--$2.8\\times$ and improves GPU utilization by $1.4\\times$--$2.1\\times$ without compromising training convergence.", "tldr": "", "keywords": ["Reinforcement Learning from Human Feedback", "Training Efficiency"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e0b1413f48ba1c23e161d3af9793b75fe08d788.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper examines training system efficiency in PPO-based RLHF algorithms, revealing significant GPU underutilization and latency bottlenecks in current synchronous approaches. To overcome these limitations, the authors introduce a dual-strategy solution: First, intra-step overlap enables pipelined execution by chunking upstream model outputs (e.g., actor model) and streaming them to downstream models (e.g., reward model), allowing prefill operations to commence before upstream decoding completes. Second, inter-step overlap employs adaptive prompt overcommitment, strategically postponing lengthy generations to subsequent steps to reduce tail latency while preserving computational progress. Experimental results on the TRL framework demonstrate 1.8×–2.8× efficiency gains over baseline implementations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Provides valuable empirical analysis of inefficiencies in practical PPO-based training systems.\n- Introduces effective actor generation and reward scoring overlap strategies to improve pipeline efficiency.\n- Demonstrates promising empirical results on small models, achieving reduced running time without performance degradation."}, "weaknesses": {"value": "- The paper lacks systematic analysis of system efficiency. A more principled approach to analyzing the dimensions for reducing time complexity and examining related hyperparameters would strengthen the contribution.\n\n- The improvements are demonstrated on small-scale models with limited training data. It remains unclear whether these advantages extend to large-scale models or architectures beyond those tested in the paper. A theoretical framework and analysis would make the claims more convincing.\n\n- The paper lacks comparison with existing systems such as Verl, OpenRLHF, AReaL, and RoLL. These systems have explored numerous resource management and load-balancing strategies for addressing challenges like long-tail generation.\n\n- The improvements are restricted to PPO algorithms in the RLHF setting, which limits the paper's impact. A significant portion of modern alignment training uses GRPO and RLVR approaches, where no reward model is involved, and the proposed overlap strategies may not be applicable."}, "questions": {"value": "- Regarding the intra-step overlap strategy, could this be implemented entirely at the sequence level? Specifically, would it be feasible to use an asynchronous approach where the generation batch is split into mini-batches, with each completed sequence immediately forwarded to the reward model for scoring upon generation completion?\n- Could the authors provide a detailed comparison with recent advances in Verl, AReaL, RoLL, and OpenRLHF? These represent more modern, industry-grade systems with their own optimization strategies. A comparison would help clarify the unique contributions of this work relative to the current state-of-the-art, as TRL may not reflect the most competitive baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "99HvxX1ZCm", "forum": "31Mr6wLBeF", "replyto": "31Mr6wLBeF", "signatures": ["ICLR.cc/2026/Conference/Submission4265/Reviewer_FKvv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4265/Reviewer_FKvv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707152906, "cdate": 1761707152906, "tmdate": 1762917264649, "mdate": 1762917264649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on improving the training efficiency of PPO algorithms. The authors propose two overlapping techniques:\n- Intra-step overlap: allows the downstream model (e.g., reward model) to begin prefill while the upstream model continues decoding.\n- Inter-step overlap: adaptively overcommits a few prompts and defers longer generations to future steps.\nExperiments demonstrate that the proposed methods significantly accelerate PPO-based RLHF training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and easy to follow.\n\n- Training efficiency of PPO algorithms is an important problem. The proposed two overlapping techniques are intuitive and well motivated.\n\n- Experiments across different tasks are conducted to demonstrate the acceleration achieved by the proposed algorithm."}, "weaknesses": {"value": "- For intra-step overlap, in reasoning scenarios, difficult problems often require generating long responses. With this technique, is there a risk that updates for difficult problems will always be deferred, while the algorithm converges quickly on easier problems?\n\n- Could intra-step overlap affect the distribution being learned, as discussed in the first question?\n\n- There are other techniques, such as fine-grained parallelism adopted in VERL for accelerating PPO, but these are not included in the experiments. Could the authors elaborate on the reason for this omission?"}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wd9QiJGjUT", "forum": "31Mr6wLBeF", "replyto": "31Mr6wLBeF", "signatures": ["ICLR.cc/2026/Conference/Submission4265/Reviewer_ty1D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4265/Reviewer_ty1D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986970646, "cdate": 1761986970646, "tmdate": 1762917264190, "mdate": 1762917264190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and addresses two major sources of inefficiency in standard PPO-based RLHF training pipelines: (1) sequential dependencies between the actor, critic, and reward models, where downstream models must wait for the upstream actor to fully complete generation , and (2) long-tail latency, where a few long \"straggler\" responses in a batch delay the completion of the entire step .\n\nThe authors propose OPPO, a lightweight systems optimization framework that introduces two novel overlap techniques:\n\nIntra-step Overlap: The actor model streams its generated tokens in adaptive chunks to the downstream critic and reward models. This allows the downstream models to begin their computationally expensive \"prefill\" stage concurrently while the actor is still decoding, effectively hiding the scoring latency .\n\nInter-step Overlap: The pipeline \"overcommits\" by starting with a batch of B + Δ prompts. It sends the first B responses that complete to the training stage, while the Δ unfinished straggler responses are deferred and resumed in the next iteration. This mitigates tail latency without discarding partial work .\n\nEmpirically, OPPO is shown to accelerate end-to-end PPO training by 1.8×–2.8× and improve GPU utilization by 1.4×–2.1×, all while achieving identical step-to-reward convergence and final model quality"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Massive Efficiency Gains: The headline result of a 1.8×–2.8× speedup on a notoriously slow and expensive pipeline is a major strength. This is a huge practical win.\n\nPreservation of Convergence: The paper's strongest evidence is that this speedup comes \"for free.\" Figure 4 and Table 1 show that the step-to-reward convergence and final model quality are nearly identical to the baseline. This proves it's a true systems optimization, not an algorithmic trade-off.\n\nOrthogonal Solution: The method attacks two different, orthogonal bottlenecks: pipeline bubbles (via intra-step overlap) and straggler latency (via inter-step overlap). The ablation in Figure 6 clearly shows that both components are necessary for the full speedup.\n\nSmart Adaptive Control: The heuristic for dynamically controlling the overcommitment level Δ is very clever. It adaptively becomes more aggressive when training is improving and more conservative (decaying Δ) as training converges, balancing speed with stability."}, "weaknesses": {"value": "\"Lightweight\" Claim: The paper claims the method is \"lightweight\" and requires \"a few lines of code change\". This seems like a significant overstatement. Implementing a streaming data buffer, inter-model communication for chunks, and the dynamic Δ scheduler (Algorithm 1)  is a non-trivial systems engineering task that deeply modifies the training loop's data flow.\n\nHeuristic-Driven Controls: The method's dynamic controls are entirely heuristic. The chunk size is tuned by \"periodically... appl[ying] a few candidate chunk sizes\" , and the Δ level is based on the reward slope. While these heuristics are smart and work well, they are not theoretically derived and may require careful tuning (e.g., the window size W, bounds Δmin/Δmax).\n\nUnanalyzed Staleness: The inter-step overlap introduces a form of staleness. While the policy used for generation is consistent (the work is just deferred), the prompts that are deferred are from an older data distribution. The paper shows that high staleness is bad (Fig 2c)  but doesn't analyze if this prompt-distribution staleness has any subtle, negative effects on the converged model, especially if Δ is large."}, "questions": {"value": "Could you elaborate on the \"few lines of code change\" claim? This appears to require a complex streaming and scheduling data structure (Algorithm 1) . What is the practical engineering lift to integrate OPPO into an existing TRL-based pipeline?\n\nThe dynamic control for Δ is based on the reward slope st. How sensitive is this heuristic to a noisy reward signal? If the reward oscillates (common in RL), would this cause Δ to become unstable and harm performance?\n\nThe paper mentions OPPO \"generalizes to other paradigms such as DPO\". How would this work? The DPO pipeline does not have the same multi-stage, multi-model dependency as PPO. What exactly would be overlapped?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XswQzZ3UTS", "forum": "31Mr6wLBeF", "replyto": "31Mr6wLBeF", "signatures": ["ICLR.cc/2026/Conference/Submission4265/Reviewer_Hdb3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4265/Reviewer_Hdb3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114652001, "cdate": 1762114652001, "tmdate": 1762917263938, "mdate": 1762917263938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new pipeline methodology for PPO. The methodology is illustrated for single-node training and results in significantly faster training times."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The impacts on the speed of post training are quite significant, and the implementation appears to be easy to add to existing code bases. The paper is well-written with detailed experiments and clear plots. I also really appreciate the annotated algorithm."}, "weaknesses": {"value": "> Among RLHF methods, Proximal Policy Optimization (PPO) (Schulman et al., 2017) has been the de facto standard due to its training stability and flexibility across diverse reward models and objectives. PPO underpins the training of state-of-the art LLMs such as GPT-4 (OpenAI et al., 2024), Gemma (Team et al., 2024), and LLaMA (Touvron et al., 2023; Grattafiori et al., 2024). \n\nThe quoted passage is wrong. As far as I am aware there is no public information on the RLHF strategies used in GPT-4 or Gemma 3, and while LLaMA 2 (Touvron et al., 2023) used PPO, LLaMA 3 and 4  (Grattafiori et al., 2024; Meta, 2025) use DPO. I do not believe that PPO is the current standard anymore, with both DPO and GRPO being significantly more popular. Qwen 2 used DPO, Qwen 2.5 used both DPO and GRPO, and Qwen 3 used just GRPO. GRPO is used by DeepSeek in all their models. DPO is used by Tulu, Hermes3, and several other models as well. I am unaware of any major model trained in 2024 or 2025 that discloses that it uses PPO. None of this means that improving PPO's efficiency isn't useful, but this paper heavily oversells how widely used PPO is and uses that as a major motivator.\n\nMore importantly, this paper appears to exclusively examine single-node training. While that's not inherently problematic it is a strong limitation to the impact of the methodology since it would be inapplicable for large models. In the absense of any discussion or results of multinode training, I'm going to assume that the method doesn't work in a multinode settting. I strongly recommend including this as a limitation in the discussion of the paper."}, "questions": {"value": "Do you have any results in a multinode setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fjw0G3Dtz6", "forum": "31Mr6wLBeF", "replyto": "31Mr6wLBeF", "signatures": ["ICLR.cc/2026/Conference/Submission4265/Reviewer_bNZ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4265/Reviewer_bNZ4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762200525100, "cdate": 1762200525100, "tmdate": 1762917263444, "mdate": 1762917263444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}