{"id": "rZVOroOwLA", "number": 10571, "cdate": 1758176010224, "mdate": 1759897642814, "content": {"title": "MASDiff: Large Scale Multi-Agent System Emergence Control via Evolutionary Diffusion", "abstract": "Emergence control aims to learn the microscopic decision policies of individual agents within a system. Based on these individual policies, macroscopic properties emerges. This paper aims to learn the individual policies of 1000+ agents based on multi-agent reinforcement learning (MARL). Although CTDE and DTDE frameworks are capable of training decentralized individual policies for multiple agents, when the number of agents scales to 1000+, both frameworks can hardly work. For the CTDE framework, although credit assignment methods can be used to distribute macroscopic goals into individual agents' microscopic rewards, the combinatorial explosion of large-scale agents makes the learning process nearly impossible. As for the DTDE framework, it lacks the capability to facilitate bottom-up learning from microscopic policies to macroscopic phenomenon. To handle these challenges, this paper proposes a diffusion model driven DTDE MARL framework MASDiff which shift the focus from exploring the optimal policies to exploring the optimal sampling data. MASDiff leverages diffusion models to learn from data sampled by multi-agents. Then it employs evolutionary mechanism to generate optimized sample data that can steer the offline training of policies to promote controllable emergence. In the experiments, the emergent macroscopic data simulated by the learned policies matches well with the ground truth data. Furthermore, we demonstrate that the ability to achieve controllable emergence would enable counterfactual reasoning in such systems. We propose several 'What if' questions to indicate the change of scenarios and obtain relatively accurate counterfactual reasoning results.", "tldr": "This paper aims to learn the individual policies of 1000+ agents based on multi-agent reinforcement learning (MARL).", "keywords": ["Emergence Control; Multi-agent reinforcement learning; conditional diffusion model; Evolutionary strategy;"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a158e93f4c5e3ee2c92197fb3ff1d483baab1a1.pdf", "supplementary_material": "/attachment/1936b468c12b3c00eb68ffe6625a2574a8152d06.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes **MASDiff**, a diffusion-model-based decentralized multi-agent reinforcement learning (MARL) framework aiming to control large-scale emergent behaviors in systems with over 1,000 agents.\nThe authors argue that existing CTDE (Centralized Training Decentralized Execution) and DTDE frameworks cannot effectively scale to such large systems.\n\nTo address this, **MASDiff** introduces:\n\n* An **evolutionary diffusion model** that generates and optimizes sample trajectories instead of directly learning policies.\n* An **offline RL component** that learns agent policies guided by diffusion-generated samples.\n* A **dual-population evolution mechanism** combining policy and trajectory evolution for scalability.\n\nExperiments are conducted in a **traffic signal control scenario** with over 1,500 simulated vehicles. The results suggest that the learned microscopic policies reproduce target macroscopic traffic patterns and allow limited counterfactual reasoning (“What if” scenarios)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* **Ambitious research goal:**\n  The paper targets the challenging and important problem of controlling emergent behavior in *large-scale* multi-agent systems, which remains an underexplored area in MARL.\n\n* **Creative methodological combination:**\n  The integration of *diffusion models*, *evolutionary optimization*, and *offline reinforcement learning* is novel and conceptually appealing, bridging generative modeling with MARL.\n\n* **Attention to scalability:**\n  The dual-parallelism structure (policy and trajectory populations) is an interesting architectural idea that could, in principle, improve scalability to thousands of agents.\n\n* **Structured methodology:**\n  The derivation from an ideal oracle-based framework to the practical algorithm (Algorithm 2) is clearly explained and logically consistent.\n\n* **Application relevance:**\n  The traffic control domain provides a concrete and intuitive example to visualize emergent phenomena and control."}, "weaknesses": {"value": "Despite its interesting premise, the paper falls short of ICLR standards in terms of **clarity, completeness, and experimental rigor**.\n\n### (1) Lack of clear problem formulation\n\nThe introduction does not adequately explain what *core difficulty* arises when scaling MARL to thousands of agents, nor does it justify how the proposed method addresses it *in a generalizable way*.\nThe term “emergence control” is vaguely defined, with no formal or quantitative evaluation criterion.\n\n### (2) Incomplete literature coverage\n\nThe related work section overlooks multiple important baselines and recent studies:\n\n* In **online MARL**, key methods such as *RACE*[1], *SMPE*[2], and *Revisiting Off-policy MARL*[3] are not cited.\n* In **diffusion-based RL**, critical algorithms like *EDP*[4], *IDQL*[5], *DPPO*[6], *OMAR*[7], *CFCQL*[8], *DOM2*[9], and *DoF*[10] are omitted.\n  This incomplete coverage weakens both novelty and context.\n\n[1]RACE: Improve Multi-Agent Reinforcement Learning with Representation Asymmetry and Collaborative Evolution. ICML 2023\n\n[2]SMPE: Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration. ICML 2025\n\n[3]Revisiting Cooperative Off-Policy Multi-Agent Reinforcement Learning. ICML 2025\n\n[4]EDP: Efficient diffusion policies for offline reinforcement learning. NeurIPS 2023\n\n[5]IDQL: Implicit q-learning as an actor-critic method with diffusion policies.\n\n[6]DPPO: Diffusion Policy Policy Optimization\n\n[7]OMAR: Plan better amid conservatism: Offline multi-agent reinforcement learning with actor rectification. 2022 ICML\n\n[8]CFCQL:Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning. 2023 NeurIPS\n\n[9]DOM2:Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning.\n\n[10]DoF: A Diffusion Factorization Framework for Offline Multi-Agent Decision Making. ICLR 2025\n\n### (3) Latent-space diffusion issues\n\nThe diffusion process is trained in a *latent space*, but the paper provides no analysis or ablation regarding how the encoder/decoder capacity affects model stability or performance.\nThis omission is crucial, as limited representation power could severely constrain trajectory generation and downstream policy learning.\n\n### (4) Domain-specific and limited experiments\n\nAll evaluations are conducted on a **custom traffic control simulation**, without comparison on any **standard MARL benchmarks** (e.g., SMAC, MPE, Multi-Agent Mujoco).\nConsequently, the claimed “general” scalability and emergence control ability lack empirical support.\n\n### (5) Writing and presentation quality\n\nThe overall writing is **unpolished and difficult to follow**.\nMany paragraphs repeat similar ideas, and key terms (“macro phenomenon”, “oracle”, “type III emergence”) are not precisely defined.\nFigures and tables lack quantitative metrics that directly validate the claimed improvements.\n\n### (6) Issues about the offline setting\nI am curios about the issues of the offline setting and the algorithm seems hard to online finetuning or training from scratch based on the evoluational diffusion method. It seems a detailed explanation or experiments about offline-to-online and online setting.\n\nWhile the paper presents a somewhat concept, it lacks the **conceptual clarity, comprehensive experimentation, and polished exposition** expected at ICLR. This paper is lack of novelty.\nSubstantial revisions, including clearer problem framing, expanded benchmark evaluations, and in-depth analysis of diffusion training, are needed before this work can be considered competitive."}, "questions": {"value": "To improve the work, the authors are encouraged to address the following points:\n\n1. **Generalization:**\n   Can MASDiff be evaluated on standard MARL benchmarks (e.g., SMAC, MPE)?\n   How does its performance scale with increasing agents under different task dynamics?\n\n2. **Diffusion modeling details:**\n   What is the dimensionality of the latent space? How sensitive are results to encoder architecture or noise schedule?\n\n3. **Baseline coverage:**\n   Why are methods like *Diffusion-QL*, *EDP*, *OMAR*, *IDQL*, etc., excluded from comparisons?\n\n4. **Ablation studies:**\n   What is the contribution of the *evolutionary mechanism* versus the diffusion model itself?\n\n5. **Definition of emergence control:**\n   How exactly is success measured? Is there a quantitative metric for macroscopic alignment?\n\n6. **Reproducibility:**\n   Will the authors release their simulation code, dataset, and hyperparameters for replication?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yt0yup2ETd", "forum": "rZVOroOwLA", "replyto": "rZVOroOwLA", "signatures": ["ICLR.cc/2026/Conference/Submission10571/Reviewer_BhSg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10571/Reviewer_BhSg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204283848, "cdate": 1761204283848, "tmdate": 1762921841272, "mdate": 1762921841272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MASDiff, a framework for controlling emergent behavior in large-scale multi-agent systems. Instead of directly optimizing policies, MASDiff learns to generate high-quality data samples—trajectories and rewards—that can lead to desired macroscopic outcomes. A conditional diffusion model is trained to approximate an oracle $O(\\tau)$ mapping trajectories $\\tau$ to optimal rewards $R$ that minimize macro-level error. An evolutionary sampling process (population-based selection, mutation via truncated diffusion, and denoising reconstruction) refines these samples iteratively. Agents then learn policies offline from the generated data using standard offline RL methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The design of the proposed Oracle is novel, which reframes large-scale MAS control as learning to generate trajectory–reward pairs rather than directly learning policies. \n2. The diffusion-based oracle approximation is an intuitive way to model emergent control.\n3. Experiments at the 1k–4k agent level within SUMO city network demonstrates good scalability."}, "weaknesses": {"value": "1. Some of notations in this paper is confusing. For example, the $MQL$ appears for the first time in Line 414 without any explanation, which  makes section 5.4 hard to read. \n2. Some experimental details need further clarification. The authors denote $LCC$ in Line 372 as the largest functional cluster of the traffic network. However, this definition is confusing for me. How is $LCC$ obtained? What kind of data is $LCC$? Is $LCC$ like the state sequences $X_{t=1}^T$?\n3. Since this paper use the mse loss between the output of the proposed method and $LCC$ to evaluate the model, a correct and detailed definition of $LCC$ is important. \n4. It seems the proposed MASDiff is similar to model-based RL. I suggest the authors to discuss the differences between MASDiff and model-based methods. \n5. For evaluation, why didn't the autors use the training return curves? It would be good if the authors provide the learning curves for MASDiff and baselines. \n\nI am willing to increase my score if the authors make some clarifications."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pBMFzucVm0", "forum": "rZVOroOwLA", "replyto": "rZVOroOwLA", "signatures": ["ICLR.cc/2026/Conference/Submission10571/Reviewer_dVau"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10571/Reviewer_dVau"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638238956, "cdate": 1761638238956, "tmdate": 1762921840770, "mdate": 1762921840770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MASDiff, a novel framework for controlling emergent macroscopic phenomena in large-scale multi-agent systems (with over 1000 agents). The core challenge addressed is that traditional multi-agent reinforcement learning (MARL) frameworks like CTDE and DTDE struggle at this scale. The key contribution is a paradigm shift from searching in the policy space to searching in the space of training data. MASDiff uses a conditional diffusion model, guided by an evolutionary algorithm, to generate optimal reward and trajectory data. Individual agent policies are then trained using offline RL on this generated data. The framework is validated in a large-scale traffic simulation, demonstrating its ability to steer the system towards a target macroscopic state and perform counterfactual reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a highly significant and challenging problem: controlling emergence in massive multi-agent systems. The core idea is clear. Shifting the search from the high-dimensional, unstable space of policy parameters to the manifold of \"good\" training data is an approach that sidesteps online large-scale MARL.\n\n- The proposed MASDiff framework is interesting, as it coherently integrates diffusion models for generative search, evolutionary strategies for exploration, and offline RL for policy learning. The \"Actor-Diffusion-Learner\" architecture is a clean and scalable conceptualization.\n\n- The experimental validation is another strong point. The use of a realistic, large-scale traffic simulator (SUMO) with real-world map data lends credibility to the applicability of this work. The experiments are designed to answer specific research questions, and the demonstration of counterfactual reasoning (RQ2) is a particularly compelling showcase of the framework's potential utility beyond simple pattern matching."}, "weaknesses": {"value": "1.  The paper only compares MASDiff against its own ablations. The central claim is that existing CTDE / DTDE MARL frameworks \"can hardly work\" at this scale, but this claim is not experimentally substantiated. Without comparisons to established, state-of-the-art MARL algorithms (e.g., MAPPO, QMIX), it is very difficult to gauge the performance and efficiency gains of MASDiff.\n\n2.  Algorithm 2 presents a very high-level loop that is hard to follow, with multiple simulation, update, and evolution steps. A more detailed diagram illustrating the flow of data between the policy population, the sample population, and the diffusion model could improve understanding. \n\n3.  The proof for Proposition 1 is hand-wavy; equating this complex iterative process with \"classical policy iteration\" needs a more rigorous justification and references.\n\n4.  The framework has many associated hyperparameters (e.g., population size `M=500`, diffusion steps `t`, scaling factor `γ`). The paper provides no discussion on the sensitivity to these parameters or the process for tuning them. For a framework this complex, understanding sensitivity is crucial for reproducibility and practical application."}, "questions": {"value": "1.  The goal of this paper is a bit confusing, it might be good to discuss the relation of this work to inverse reinforcement learning? \n\n2. Could you elaborate on the choice of baselines? The main claim is that existing MARL frameworks are insufficient, but no such frameworks were included in the comparison. Why not compare against adapted versions of established algorithms like MAPPO or PBT, or with inverse RL?\n\n3.  The framework has a large number of hyperparameters (population size, evolutionary parameters, diffusion steps, etc.). Could you provide some insight into their tuning? Is the framework's performance highly sensitive to these choices, for instance, the population size `M`?\n\n4. The methodology appears to be quite related to mean field methods such as mean field games and mean field control [1-3], which describes the macroscopic behavior resulting from microscopic agent behavior and was also similarly applied to traffic control [4]. Is there potential for a synthesis of both worlds?\n\n[1] Perrin, S., et al. Mean Field Games Flock! The Reinforcement Learning Way. IJCAI (2021).\n\n[2] Carmona, R. et al. Model-free mean-field reinforcement learning: mean-field MDP and mean-field Q-learning. The Annals of Applied Probability 33.6B (2023): 5334-5381.\n\n[3] Cui, K., et al. Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior. ICLR (2024).\n\n[4] Wu, M., et al. Participatory traffic control: Leveraging connected and automated vehicles to enhance network efficiency. Transportation Research Part C: Emerging Technologies 166 (2024): 104757."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rYTJBrozvt", "forum": "rZVOroOwLA", "replyto": "rZVOroOwLA", "signatures": ["ICLR.cc/2026/Conference/Submission10571/Reviewer_qWsX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10571/Reviewer_qWsX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720967366, "cdate": 1761720967366, "tmdate": 1762921840052, "mdate": 1762921840052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}