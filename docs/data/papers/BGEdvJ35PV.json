{"id": "BGEdvJ35PV", "number": 10075, "cdate": 1758159947934, "mdate": 1763671921184, "content": {"title": "Diffuse and Steer: Corrective Sampling for Stable 3D Molecular Generation", "abstract": "Diffusion models have achieved state-of-the-art performance across diverse domains, yet their application to molecular generation remains challenging. Unlike many data types where values can tolerate slight variations, such as pixel intensities in images, molecules are governed by strict geometric and chemical constraints: minor variations in the atomic coordinates of even a single atom can lead to totally invalid or unstable molecules. \nThese constraints give rise to *highly concentrated* data distributions, forming sharp probability peaks. Moreover, these peaks are *densely packed* in configuration space: changing one atom's type, along with small but precise adjustments to its position and that of its neighbors, can result in a distinct molecule, whereas images generally require much larger perturbations to change semantic meaning. \nThis dense-concentrated structure makes diffusion modeling fragile: because valid regions are narrow and tightly clustered, even small deviations at intermediate timesteps can easily cross validity boundaries. Once entering the invalid regions, the generative process provides unreliable guidance, causing errors that accumulate over timesteps and drift generative trajectories off-distribution, ultimately leading to irreparable structural violations.\nTo address this challenge, we formalize the notion of dense-concentrated structure in molecular distributions\nand analyze how discrepancies at intermediate steps propagate under reverse inference. \nBuilding on this insight, we propose **DIST**, a plug-in corrective method that **DI**ffuses and **ST**eers the intermediate distribution, thereby realigning inference trajectories toward a valid molecular distribution. Our method is model-agnostic and can be integrated into a wide range of existing diffusion models, achieving significant improvements in performance while reducing the computational cost to nearly half the standard number of timesteps.", "tldr": "", "keywords": ["Diffusion models", "molecular generation", "generative models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01e0d81c289f41f12f654850381ff4681b780925.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the instability of diffusion models in 3D molecular generation. The authors identify that molecular data exhibits a dense-concentrated (DC) structure—valid molecules correspond to narrow, isolated peaks in distribution space separated by low-density regions. Standard diffusion models often drift into invalid regions due to this concentration, leading to unstable or chemically invalid molecules. To address this, the authors propose DIST, a plug-in corrective sampling module. DIST selectively corrects intermediate distributions during reverse diffusion by evaluating sample batches, discarding off-distribution trajectories, and steering remaining samples back toward high-density, valid regions. They provide theoretical guarantees and show that DIST improves molecular validity and stability across several backbone models on QM9 and GEOM-Drugs datasets, while cutting inference timesteps nearly in half."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well motivated and has some theoretical insights. The empirical results show consistent improvement across diverse backbones and datasets."}, "weaknesses": {"value": "1. While the application to 3D molecules is new, the idea of filtering intermediate states resembles rejection or guidance sampling.\n2. No comparison to alternative corrective techniques (e.g., score rescaling, gradient correction, or classifier guidance).\n3. Scalability to very large molecules or protein-level systems is not demonstrated.\n4. Dependence on heuristic thresholds (τ) and pilot sampling design may reduce reproducibility or generality."}, "questions": {"value": "1. How sensitive is DIST’s performance to the choice of threshold and batch radius?\n2. Is the theoretical bound (Proposition 3.1) empirically validated—e.g., by measuring total variation between qt and pt estimates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G1cHC0kOb0", "forum": "BGEdvJ35PV", "replyto": "BGEdvJ35PV", "signatures": ["ICLR.cc/2026/Conference/Submission10075/Reviewer_5oDq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10075/Reviewer_5oDq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569032158, "cdate": 1761569032158, "tmdate": 1762921466667, "mdate": 1762921466667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on the \"dense-concentrated (DC) structure\" inherent to molecular data: chemically valid structures form sharp, densely packed peaks in the representation space, separated by regions of near-zero density. This structure makes diffusion modeling fragile. To mitigate this, this paper propose DIST (DIffuse and STeer), a selective correction method. DIST filters and rescales intermediate distributions during inference, steering trajectories toward the valid molecular peaks. Experimental results demonstrate that DIST consistently enhances performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The identification of the \"dense-concentrated structure\" in molecular data as a critical challenge is valuable.\n\n2. The theoretical analysis enhances the plausibility of the proposed approach."}, "weaknesses": {"value": "1. A thorough survey and comparison to the literature on \"exposure bias in diffusion models\" (such as [1], [2], [3], and recent works)  are highly necessary. The proposed solution shares conceptual similarities with existing methods—for example, Proposition 2 in [2] reaches conclusions analogous to this work’s Cor. 3.1 and Prop. 3.1, albeit with different metrics. The absence of a clear comparison from existing methods makes it difficult to assess the novelty and uniqueness of its contribution.\n\n2. Lack of experimental validation for the issue of dense-concentrated data: The phenomenon observed in Table 1 mirrors exposure bias and which is also found in image generation with diffusion models. To substantiate the arguments in Section 3.1, additional experiments should be conducted to demonstrate that dense-concentrated data exhibit more pronounced exposure bias than smoother distributions. \n\n3. Clarity and rigor in writing need improvement: \n    - Line 230: The derivation of \\(\\|\\nabla \\log p(z_t)\\|\\) and Equations 6, 7 needs rigorous mathematical justification. \n    - Section 3.1 claims that the DC-structure causes the \"reverse update to step past the peak and cross into high-density opposite regions.\" I suggest including a 1D Gaussian mixture simulation example, illustrating how specific choices of \\(\\sigma_*\\), \\(\\Delta\\), and \\(m_k\\) lead to sampling falling into low-density regions.\n    - I suggest including a pseudocode implementation of the DIST algorithm for reproducibility.\n    - Minor typo: Line 306 \"the the reverse process\"."}, "questions": {"value": "1. The DIST algorithm involves multiple manually tuned hyperparameters (intermediate timestep t, filtering threshold, batch division, pilot sample ratio). What guidelines exist for tuning them? If time permits, an ablation study for each parameter is recommended.\n\n2. Intuitively, transforming the data (e.g., stretching the \"x-axis\") could reshape the distribution to be sparser and smoother, eliminating the dense-concentration. Is the DC-structure an inherent property of molecular data, or an artifact of representation? A discussion on this would strengthen the work’s motivation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VDozzUaE4F", "forum": "BGEdvJ35PV", "replyto": "BGEdvJ35PV", "signatures": ["ICLR.cc/2026/Conference/Submission10075/Reviewer_wDAK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10075/Reviewer_wDAK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881728917, "cdate": 1761881728917, "tmdate": 1762921466385, "mdate": 1762921466385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the vulnerability of diffusion models in 3D molecular generation caused by intermediate-time drift and extremely narrow effective domain. The authors first formally propose the data distribution hypothesis of \"dense-centralized (DC) structure,\" and based on this, analyze the overshoot problem in back-inference (Equations (6)(7)). They then propose a pluggable bias-correcting sampling framework, DIST (Diffuse and Steer): at intermediate time points, replication-perturbation forms local \"batches,\" which are scored using pilot inference and inconsistent/low-quality batches are filtered out to obtain the corrected intermediate distribution q_{t}^{c} before continuing back-sampling. Empirical results on QM9 and GEOM-Drugs show improved stability and legitimacy for various backbones (EDM/GeoLDM/RADM), and claim a nearly 50% reduction in the average number of steps."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.Abstracting the molecular distribution into a DC structure of \"multiple narrow peaks + low-density spacing\" provides a theoretical explanation and intuitive illustration of the trajectories intersecting in the intermediate distribution and the misleading effects of the mean-based score field.\n\n2.Without altering the backbone weights and hyperparameters, the algorithm directly selects the best and discards the worst in the inference process, empirically demonstrating stable performance improvements on multi-backbone architectures and two datasets.\n\n3.The author provides a step count calculation and shows in a table the significant decrease in average steps.\n\n4.Cor. 3.1 gives the conclusion that \"closer intermediate distributions lead to closer final distributions\" as the TV distance does not increase; Prop. 3.1 gives the upper bound of the error after selective correction as dependent on α(τ), β(τ) and the conditional TV deviation."}, "weaknesses": {"value": "1.Cor. 3.1 actually utilizes the general fact that \"any Markov kernel is 1-Lipschitz (non-expansion) on TV\" (k∈[0,1] often only takes the value 1), but does not guarantee strict contraction (k<1). Calling it \"TV–contraction\" is misleading to the reader into thinking it means \"inevitable contraction.\" It is suggested to change it to \"non-expansive step / TV non-expansion,\" and clarify the typicality of k=1.\n\n2.The upper bound of Prop. 3.1 depends on the quality of the true distribution, but in practice the true distribution is unavailable. The authors approximate it with \"pilot inference results (stability/legitimacy)\"; however, the consistency of this approximation and the confidence bound (the estimation error of α, β) are not analyzed, so the upper bound is difficult to instantiate.\n\n3.The paper calculates the average number of steps (e.g., (T−t)/|B|+t) based on \"replicating batches up to t + full replay of a small number of pilot samples + continuing only for batches that pass the threshold.\" However, the description of the complete reverse cost of the pilot samples is somewhat vague. The \"pilot cost of discarded batches\" should also be included in the total budget before comparing the end-to-end total cost with the baseline's fixed 1000 steps. It is recommended to provide the precise accounting formula and pseudocode in Appendix D, and report the wall-clock time with the same hardware and parallelism.\n\n4.Only rule-based indicators such as stability, legitimacy, and uniqueness are reported. It is recommended to add: skeleton diversity , ring structure and heteroatom distribution, QED/SA/synthetic feasibility, chirality and geometric configuration preservation, etc., to reflect the impact of the \"screening + replication\" strategy on diversity and pattern coverage.\n\n5.The settings and scales of \"radius r\", the trade-off between quality and diversity in \"replication count/perturbation intensity\", the specific definition and threshold selection of pilot score s_{j} (which seems to be based on stability/legitimacy in the final draft), and the sensitivity to random seed/batch size are currently less detailed in the main text."}, "questions": {"value": "1.The abstract claims that \"computational overhead is reduced to nearly half that of the standard number of steps.\" Please provide the total end-to-end time (including pilot cost and the overhead of dropped batches) and the equivalent GPU hours, and compare it with the baseline on the same hardware and concurrency level.\n\n2.The image comparison in Fig. 1 is intuitive, but could the differences be exaggerated due to variations in image task semantics, etc.? Could you provide a 1D/2D toy density comparison to more purely represent the mechanism of \"multiple narrow peaks + overlap + overshoot\"?\n\n3.The reverse update of Equation (5) uses ϵ_{θ} to approximate the score; in molecular tasks, the SE(3) isovariant network or coordinate alignment is often used to reduce ambiguity. The authors' subsequent experiments included isovariant and non-isovariant backbones, but did they examine the sensitivity differences of DIST under canonicalization/coordinate normalization schemes?\n\n4.Please change \"TV–contraction\" to \"TV non-expansion\" and clarify that κ=1 in general. Furthermore, providing sufficient conditions under which κ<1 would be more convincing.\n\n5.Please provide accurate accounting (including pilot costs for failed batches) in Appendix D, and supplement with wall-clock and throughput to avoid substituting actual time with just \"steps\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6t44XEMw5h", "forum": "BGEdvJ35PV", "replyto": "BGEdvJ35PV", "signatures": ["ICLR.cc/2026/Conference/Submission10075/Reviewer_DXTg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10075/Reviewer_DXTg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888745883, "cdate": 1761888745883, "tmdate": 1762921466061, "mdate": 1762921466061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}