{"id": "bPN1c10U3P", "number": 10485, "cdate": 1758173335848, "mdate": 1759897647912, "content": {"title": "BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning", "abstract": "Prototype-based federated learning (PFL) has emerged as a promising paradigm to address data heterogeneity problems in federated learning, as it leverages mean feature vectors as prototypes to enhance model generalization. However, its robustness against backdoor attacks remains largely unexplored. In this paper, we identify that PFL is inherently resistant to existing backdoor attacks due to its unique prototype learning mechanism and local data heterogeneity. To further explore the security of PFL, we propose BAPFL, the first backdoor attack method specifically designed for PFL frameworks. BAPFL integrates a prototype poisoning strategy with a trigger optimization mechanism. The prototype poisoning strategy manipulates the trajectories of global prototypes to mislead the prototype training of benign clients, pushing their local prototypes of clean samples away from the prototypes of trigger-embedded samples. Meanwhile, the trigger optimization mechanism learns a unique and stealthy trigger for each potential target label, and guides the prototypes of trigger-embedded samples to align closely with the global prototype of the target label. Experimental results across multiple datasets and PFL variants demonstrate that BAPFL achieves a 33%-75% improvement in attack success rate compared to traditional backdoor attacks, while preserving main task accuracy. These results highlight the effectiveness, stealthiness, and adaptability of BAPFL in PFL.", "tldr": "We propose BAPFL, a novel backdoor attack that significantly improves the attack success rate in prototype-based federated learning via dual-direction prototype optimization.", "keywords": ["Federated learning", "prototype-based federated learning", "backdoor attacks", "adversarial attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37238d2a8e6998f5900222c5a20bf2f6b4c7e664.pdf", "supplementary_material": "/attachment/7e255b8d3a5e5099fe083c95ab24019aebac1e4d.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the backdoor vulnerability of Prototypical Federated Learning (PFL). This paper first analyzes PFL's natural resistance to traditional attacks, then proposes BAPFL, the first backdoor attack specifically designed for PFL. BAPFL combines a Prototype Poisoning Strategy (PPS) to manipulate global prototypes via prototype flipping, and a Trigger Optimization Mechanism (TOM) to learn stealthy triggers aligned with target-class prototypes. Experiments show BAPFL achieves a high ASR while maintaining ACC."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This paper is the first to systematically study the backdoor security issues in PFL, an important variant of FL, filling a research gap. \n\n2.The paper fully validates the effectiveness of BAPFL's components and its robustness under various settings through extensive internal comparative experiments.\n\n3.This paper is well written and easy to follow."}, "weaknesses": {"value": "1. Fails to evaluate BAPFL against SOTA general-purpose backdoor defenses (e.g., FLAME [1]), leading to an insufficient assessment of its actual threat.\n\n2.  Lacks evaluation against more complex, PFL-specific backdoor defense strategies, failing to demonstrate robustness against such targeted defenses.\n\n3.  Lacks comparison against other SOTA attacks (e.g., Chameleon [2], A3FL [3]).   This is needed to benchmark BAPFL's relative strength, even if those attacks require adaptation to PFL.\n\n4. The stealthiness claim relies solely on ACC, overlooking statistical anomaly analysis of the poisoned prototypes themselves.\n\nReference:\n\n[1] Nguyen et al. FLAME: Taming backdoors in federated learning.USENIX Security’ 2022.\n\n[2] Yanbo Dai, Songze Li. Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning. ICML2023.\n\n[3] Zhang, Hangfan, et al. A3FL: adversarially adaptive backdoor attacks to federated learning. NeurIPS’ 2023."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K9Ol3MA1Pb", "forum": "bPN1c10U3P", "replyto": "bPN1c10U3P", "signatures": ["ICLR.cc/2026/Conference/Submission10485/Reviewer_EUSX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10485/Reviewer_EUSX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893721476, "cdate": 1761893721476, "tmdate": 1762921775258, "mdate": 1762921775258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates security vulnerabilities in Prototype-based Federated Learning (PFL) under backdoor attacks. The authors identify that PFL’s prototype-based learning and data heterogeneity provide inherent resistance to conventional attacks. To address this limitation, the paper introduces BAPFL, a novel attack framework combining a Prototype Poisoning Strategy (PPS)and a Trigger Optimization Mechanism (TOM). PPS manipulates global prototypes to mislead benign clients, while TOM learns stealthy triggers aligned with target prototypes. Extensive experiments on MNIST, FEMNIST, and CIFAR-10 across multiple PFL variants demonstrate that BAPFL achieves a substantial improvement in attack success rate (33%–75%) over traditional backdoor attacks while maintaining comparable main-task accuracy. The results further highlight BAPFL’s robustness against advanced defenses and its adaptability to heterogeneous data settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Presents the first systematic investigation of backdoor attacks in PFL and identifies its intrinsic resistance to conventional attacks.\n2. The integration of PPS and TOM for dual-direction prototype optimization is well-motivated and significantly enhances both attack effectiveness and stealthiness.\n3. Appendix I demonstrates that BAPFL introduces minimal computational and communication overhead, suggesting its practical feasibility."}, "weaknesses": {"value": "1. While ``stealthiness'' is discussed as a key objective, the paper lacks visual examples of trigger-embedded samples. Figure 3 visualizes prototypes but does not illustrate the actual modified inputs seen by benign clients. Including such examples would strengthen the claim of imperceptibility.\n2. The statement that robust aggregation defenses exhibit ``limited effectiveness'' against BAPFL would benefit from deeper analysis. A theoretical discussion clarifying *why* prototype manipulation circumvents these defenses would provide stronger insight.\n3. Appendix G.3 asserts that BAPFL is relatively insensitive to hyperparameters after normalization, yet this claim relies on limited perturbation testing. Presenting quantitative results (e.g., plots or tables) for small variations of λ₁, λ₂, and λ₃ would substantiate this conclusion.\n4. The federated learning setup deviates from standard backdoor attack settings, which typically involve 100–200 clients with partial participation (e.g., 10% per round). The current configuration of 20 clients with full participation is non-standard; clarification and evaluation under typical settings are necessary.\n5. The related work section omits discussion of several defense strategies proposed for FL backdoor attacks, particularly outlier-detection-based methods [1, 2]. Evaluation beyond robust aggregation would provide a more comprehensive comparison.\n6. Experiments on more complex datasets (e.g., CIFAR-100 or Tiny-ImageNet) would strengthen claims of generalizability across diverse data distributions.\n7. The large performance gap in ASR (33%–75%) compared to ``traditional backdoor attacks'' raises concerns about baseline adaptation. The paper should clarify whether baselines were properly adjusted for the PFL context or justify why they cannot be effectively adapted.\n8. Minor: The meaning of ``attack rate'' (e.g., proportion of malicious clients) should be explicitly defined in Section 5.1 for clarity.\n\n[1]. Nguyen, Thien Duc, et al. \"{FLAME}: Taming backdoors in federated learning.\" *31st USENIX Security Symposium (USENIX Security 22)*. 2022.\n\n[2]. Rieger, Phillip, et al. \"Deepsight: Mitigating backdoor attacks in federated learning through deep model inspection.\" arXiv preprint arXiv:2201.00763 (2022)."}, "questions": {"value": "Please address the aforementioned weaknesses and clarifications, particularly regarding experimental setup consistency and baseline adaptation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KYzzGueQ1K", "forum": "bPN1c10U3P", "replyto": "bPN1c10U3P", "signatures": ["ICLR.cc/2026/Conference/Submission10485/Reviewer_aqg4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10485/Reviewer_aqg4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904629331, "cdate": 1761904629331, "tmdate": 1762921774868, "mdate": 1762921774868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the security of Prototype-Based Federated Learning (PFL), a paradigm that addresses data heterogeneity in federated learning (FL) by aggregating class prototypes (mean feature vectors) instead of full model parameters. PFL exhibits inherent robustness against existing backdoor attacks, attributed to the limited influence of poisoned prototypes (only affecting the embedding layer of benign models) and client data heterogeneity (some clients lack target label samples, breaking trigger-target mappings). To fill the gap of PFL-specific backdoor attacks, the authors propose BAPFL. Selects high-impact trigger samples via Euclidean distance and flips trigger prototypes to manipulate global prototypes away from trigger prototypes, pushing benign prototypes to diverge from trigger prototypes. Expands the target label space to cover benign clients’ local labels, learns label-specific stealthy triggers, and aligns trigger prototypes with the global prototype of the target label."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1-It addresses a critical, underexplored problem—PFL’s security against backdoor attacks. Prior work primarily focuses on vanilla FL’s backdoor vulnerabilities, while PFL’s unique prototype aggregation mechanism creates a distinct security landscape; the authors are the first to systematically analyze PFL’s resistance and design a dedicated attack, which fills an important research gap.\n\n2-BAPFL’s dual-direction optimization (PPS + TOM) is well-motivated and tailored to PFL’s characteristics. PPS targets the prototype aggregation process (a core of PFL) to disrupt benign prototype learning, while TOM addresses client heterogeneity to ensure attack coverage—this design effectively overcomes PFL’s inherent resistance and is more innovative than adapting vanilla FL attacks to PFL.\n\n3-The paper provides formal theoretical analysis, including two key assumptions (prototype-based classification rules) and two theorems (proving PPS’s ability to increase misclassification probability and TOM’s role in activating trigger-target mappings), which strengthens the methodological validity."}, "weaknesses": {"value": "1-The paper claims PFL is applied in smart healthcare and autonomous driving, but all experiments use standard image datasets (MNIST, CIFAR-10) with synthetic data heterogeneity. There is no validation on domain-specific datasets (e.g., medical imaging datasets like ChestX-ray14 for healthcare) or under real-world constraints (e.g., limited client resources, intermittent connectivity), making it hard to assess BAPFL’s practical impact.\n\n2-The paper assumes the adversary controls multiple compromised clients and has full access to global prototypes. In practice, adversaries may only control a single client or have incomplete knowledge of global prototypes (e.g., due to partial participation in aggregation rounds). The paper does not evaluate BAPFL’s performance under weaker adversary models (e.g., 1 compromised client, partial global prototype knowledge), limiting its generalizability.\n\n3-The experiments only run for 200 training rounds. In real PFL systems, training may last for thousands of rounds, and benign clients’ continuous updates could dilute the backdoor effect. The paper does not test BAPFL’s persistence over extended training, leaving uncertainty about its long-term effectiveness."}, "questions": {"value": "1-The paper claims PFL is applied in smart healthcare and autonomous driving, but all experiments use standard image datasets (MNIST, CIFAR-10) with synthetic data heterogeneity. There is no validation on domain-specific datasets (e.g., medical imaging datasets like ChestX-ray14 for healthcare) or under real-world constraints (e.g., limited client resources, intermittent connectivity), making it hard to assess BAPFL’s practical impact.\n\n2-The paper assumes the adversary controls multiple compromised clients and has full access to global prototypes. In practice, adversaries may only control a single client or have incomplete knowledge of global prototypes (e.g., due to partial participation in aggregation rounds). The paper does not evaluate BAPFL’s performance under weaker adversary models (e.g., 1 compromised client, partial global prototype knowledge), limiting its generalizability.\n\n3-The experiments only run for 200 training rounds. In real PFL systems, training may last for thousands of rounds, and benign clients’ continuous updates could dilute the backdoor effect. The paper does not test BAPFL’s persistence over extended training, leaving uncertainty about its long-term effectiveness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i6feOGef6N", "forum": "bPN1c10U3P", "replyto": "bPN1c10U3P", "signatures": ["ICLR.cc/2026/Conference/Submission10485/Reviewer_b8XZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10485/Reviewer_b8XZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918681608, "cdate": 1761918681608, "tmdate": 1762921774415, "mdate": 1762921774415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies that Prototype-based Federated Learning (PFL) is resistant to existing backdoor attacks due to its prototype mechanism and data heterogeneity. To overcome this, the authors propose BAPFL, which integrates a Prototype Poisoning Strategy (PPS) and a Trigger Optimization Mechanism (TOM). Experimental results show that BAPFL is effective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-structured.\n\n* This is the first backdoor attack method designed for Prototype-based Federated Learning (PFL).\n\n* The experimental results validate the attack performance of the proposed method."}, "weaknesses": {"value": "* The experimental settings are rather unusual. The clients use a very small local batch size of 4. What does the attack rate (AR) mean? Is it the proportion of malicious clients? Based on Table 1, the proposed attack method appears to reduce the model's main task accuracy (ACC). The datasets and models used are also simplistic. The authors should report the results on large-scale datasets and models for a more convincing evaluation. \n\n* A sensitivity analysis for $\\lambda$ is missing.\n\n* Some findings presented in Section 3.3 appear to overlap with findings in Bad-PFL.\n\n* The paper lacks a discussion about potential defense mechanisms."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XV1JQkVSXa", "forum": "bPN1c10U3P", "replyto": "bPN1c10U3P", "signatures": ["ICLR.cc/2026/Conference/Submission10485/Reviewer_CdtN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10485/Reviewer_CdtN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965948664, "cdate": 1761965948664, "tmdate": 1762921774039, "mdate": 1762921774039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}