{"id": "D4xSuGvLZA", "number": 15330, "cdate": 1758250334805, "mdate": 1759897313210, "content": {"title": "BriLLM: Brain-inspired Large Language Model", "abstract": "We introduce BriLLM, the first brain-inspired large language model that establishes a genuinely biology- and neuroscience-grounded machine learning paradigm. Unlike previous approaches that primarily mimic local neural features, BriLLM implements Signal Fully-connected flowing (SiFu) learning—the first framework to authentically replicate the brain's macroscopic information processing principles at scale. Our approach is uniquely validated by two core neurocognitive facts: (1) _static semantic mapping_ to dedicated cortical regions, and (2) _dynamic signal propagation_ through electrophysiological activity. This foundation enables transformative capabilities: inherent multi-modal compatibility, full node-level interpretability, context-length independent scaling, and global-scale simulation of brain-like language processing. Our 1–2B parameter models demonstrate stable learning dynamics while replicating GPT-1-level generative performance. Scalability analysis confirms feasibility of 100–200B parameter variants. BriLLM represents a paradigm shift from representation learning toward biologically-validated AGI foundations, offering a principled solution to current AI's fundamental limitations.", "tldr": "", "keywords": ["Brain-Inspired LLM", "Non-Transformer Architecture"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80f8dc67d668711f39049c238fe592a835d030fc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BriLLM, a brain-inspired neural architecture for language modeling. The architecture is a fully-connected graph where vector messages are passed in a bidirectional manner. The authors show that the loss decreases over training on a language modeling task."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The topic of submission (brain-inspired language modeling) is highly aligned to the conference."}, "weaknesses": {"value": "This manuscript is not ready for publication at ICLR. Unfortunately, the neuroscientific motivation, grounding in past literature, theory, and experiments are preliminary. In particular, BriLLM is not the first brain-inspired architecture out there-- the submission mentions spiking neural networks but otherwise ignores a vibrant line of recent work including, e.g., TopoLM [1], Topoformer [2], Mixture of Cognitive Reasoners [3] etc. \n\nOtherwise, the paper makes quite a few meandering and unsupported claims. For instance:\n\n- \"Cognition emerges from electrophysiological signal flow (e.g., EEG patterns)\"\n- \"The brain's direct semantic mapping to dedicated components represents a fundamentally simpler mechanism than representation learning's indirect vector encoding, aligning with evolutionary efficiency.\"\n\nExperimentally, it is insufficient to show the loss curve and amount of sparsification-- a thorough comparison to other architectures, incl. GPT, TopoLM, etc, on at least the text perplexity is needed. Furthermore, the current method produces incoherent continuations, see Appendix A table, which makes the conclusion that \"BriLLM is a transformative framework for genuine AGI development\" (line 480) highly implausible. \n\nI recommend rejection as the manuscript does not demonstrate proper literature review and experimental practice. \n\n[1] TopoLM (Rathi et al., ICLR 2025)\n\n[2] TopoFormer (Binhuraib et al., ICLR Representational Alignment Workshop 2024)\n\n[3] Mixture of Cognitive Reasoners (AlKhamissi et al., 2025)"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bi7cBVkl6N", "forum": "D4xSuGvLZA", "replyto": "D4xSuGvLZA", "signatures": ["ICLR.cc/2026/Conference/Submission15330/Reviewer_WWZM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15330/Reviewer_WWZM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760696817267, "cdate": 1760696817267, "tmdate": 1762925624848, "mdate": 1762925624848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "rebuttal 2025-11-16"}, "comment": {"value": "We thank the reviewers for their time. However, core criticisms stem from a fundamental misapprehension: evaluating a paradigm-shifting machine learning contribution through the narrow lens of the very paradigm it seeks to transcend. Why is AGI so elusive? Precisely because of too many such limited-scope assessments.\n\nBriLLM is not an incremental improvement to the Transformer; it is a foundational alternative grounded in distinct first principles. Our rebuttal centers exclusively on this point.\n\nTo Reviewer fyDE:\n\nWe appreciate your recognition of the work’s high originality, but your concerns are misplaced.\n\nOn Performance Comparisons: Demanding direct benchmarks against GPT models on standard tasks is a category error. BriLLM introduces a new biological-principle-based computing paradigm. Judging it solely by representation-learning metrics is analogous to evaluating the integrated circuit by vacuum tube benchmarks. The primary contribution is the framework itself, delivering inherent properties (full interpretability, multimodal compatibility) that Transformers fundamentally lack. The “GPT-1-level” claim is a scaling law reference, not the objective.\n\nOn Autoregressive Processing: SiFu’s sequential nature is a deliberate design reflecting the continuous dynamics of cognitive signal flow. We are engineering a principled AGI path, not optimizing for GPU utilization. Linear complexity with context length is a decisive theoretical advantage over the Transformer’s quadratic bottleneck.\n\nTo Reviewer wo3a:\n\nYour review contains significant factual inaccuracies and fails to engage with the core innovation.\n\n“Mischaracterization of non-representation learning”: Incorrect. An ML “representation” is a distributed, latent code; a BriLLM node is a localist symbol with dynamic activation. This is the critical distinction between a biologically plausible symbolic activation model and distributed representation learning. Conflating the signal tensor with an RNN state reveals a fundamental misunderstanding.\n\n“Superficial neuroscientific grounding” & “Weak experimental validation”: Your critique is superficial. Neurocognitive facts provide a computational blueprint, not literal biological simulation. The proof-of-concept demonstrates stable learning in a novel, non-differentiable architecture. Fixating on one imperfect generation sample while ignoring robust scaling analysis is unconstructive. Standard metrics are secondary for a first demonstration of a new paradigm.\n\nTo Reviewer kCXf:\n\nYour demand for EEG/fMRI validation reflects a profound misunderstanding of this work’s contribution. BriLLM is a machine learning model, not a brain simulation.\n\nEEG/fMRI Validation Is Irrelevant: This paper establishes a new ML paradigm inspired by macroscopic brain principles—a computational theory, not a neuroscientific study. Requiring biological data validation is as misplaced as demanding Transformer papers validate attention against primate visual cortex recordings. The contribution lies in the mathematical framework and its computational properties (interpretability, scalability), presented and analyzed on their own merits.\n\nOn Methodological Flaws & Experimental Gaps: The model’s soundness is evaluated by internal consistency and ability to learn language-like structures, not fitting biological noise. Scalability to 100–200B parameters is theoretical analysis based on graph structure—standard practice for introducing new architectures. Energy consumption questions, while valid for future engineering, are premature for a foundational paradigm paper.\n\nTo Reviewer WWZM:\n\nYour dismissal based on literature is incorrect.\n\n“Ignores recent work”: False. Cited works (TopoLM, etc.) are sophisticated extensions of representation learning. BriLLM is a non-representation, non-differentiable, brain-inspired alternative—philosophically and architecturally incommensurate. Our contribution lies in stepping outside this lineage to propose a radical new foundation. Applying your citation standard would require citing all post-Transformer/GPT papers, which is riduculously impractical.\n\n“Meandering claims” & “Preliminary experiments”: Claims are supported by established computational principles. Initial text generation results are proof-of-concept for a novel architecture, demonstrating feasibility. The significant contribution is the framework itself.\n\nSummary Rebuttal:\n\nReviewers have largely evaluated a paradigm-shifting proposal using obsolete old-paradigm criteria on a poor machine learning commonsense basis. Criticisms on benchmarking and biological validation are conceptually invalid. BriLLM’s significance lies in opening a new AGI pathway—principled, interpretable, and biologically plausible."}}, "id": "51IyReGOO6", "forum": "D4xSuGvLZA", "replyto": "D4xSuGvLZA", "signatures": ["ICLR.cc/2026/Conference/Submission15330/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15330/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15330/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763269752369, "cdate": 1763269752369, "tmdate": 1763269752369, "mdate": 1763269752369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BriLLM, a brain-inspired large language model based on Signal Fully-connected flowing (SiFu) learning, the first framework claimed to authentically replicate the brain’s macroscopic information processing principles.  It leverages two neurocognitive facts: static semantic mapping to dedicated cortical regions and dynamic signal propagation via electrophysiological activity.  BriLLM achieves multi-modal compatibility, full interpretability, context-length independent scaling, and simulates brain-like language processing.  With 1–2B parameter models matching GPT-1 performance and scalability to 100–200B parameters, it proposes a shift from representation learning toward biologically-validated AGI foundations."}, "soundness": {"value": 4}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality**: Attempts a biologically-grounded paradigm shift, though derivative of SNNs and neurocognitive models.\n- **Quality**: Some conceptual alignment with neuroscience principles, but experimental validation is absent.\n- **Clarity**: Limited by poor figure annotation and vague methodology.\n- **Significance**: Targets AGI limitations, but lacks practical impact without evidence."}, "weaknesses": {"value": "- **Methodological Flaws**: The SiFu graph (Definition 1) and signal tensor (Definition 2) lack validation against EEG data or cortical activation patterns.  The competitive activation formula ignores biological noise and synaptic delays.\n- **Experimental Gaps**: No performance metrics (e.g., perplexity, BLEU) compare BriLLM to GPT-1 or modern LLMs.  Scalability claims to 100–200B parameters are theoretical without training data or hardware details.\n- **Oversight**: Ignores computational cost trade-offs and energy efficiency compared to Transformers.  Potential biases in semantic mapping (e.g., cultural variability) are unaddressed.\n- **Validation**: Claims of multi-modal compatibility and interpretability lack demonstration with real datasets or tasks."}, "questions": {"value": "1.  Can the authors provide EEG or fMRI data (for example, brain encoding perfermance, or brain-like perfermance) comparisons to validate SiFu’s signal propagation against biological patterns?\n2.  What are the specific perplexity, BLEU, or other metrics for BriLLM’s 1–2B models versus GPT-1, and why were modern LLMs excluded?\n3.  How were the 100–200B parameter scalability estimates derived, and what training infrastructure supports this claim?\n4.  Can the authors quantify the impact of biological noise or synaptic delays on SiFu’s competitive activation mechanism?\n5.  Why were energy consumption and hardware requirements not compared to Transformer-based models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "06WfSexpym", "forum": "D4xSuGvLZA", "replyto": "D4xSuGvLZA", "signatures": ["ICLR.cc/2026/Conference/Submission15330/Reviewer_kCXf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15330/Reviewer_kCXf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761372455074, "cdate": 1761372455074, "tmdate": 1762925624401, "mdate": 1762925624401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BriLLM, a brain-inspired large language model based on Signal Fully-connected flowing (SiFu) learning, the first framework claimed to authentically replicate the brain’s macroscopic information processing principles.  It leverages two neurocognitive facts: static semantic mapping to dedicated cortical regions and dynamic signal propagation via electrophysiological activity.  BriLLM achieves multi-modal compatibility, full interpretability, context-length independent scaling, and simulates brain-like language processing.  With 1–2B parameter models matching GPT-1 performance and scalability to 100–200B parameters, it proposes a shift from representation learning toward biologically-validated AGI foundations."}, "soundness": {"value": 4}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality**: Attempts a biologically-grounded paradigm shift, though derivative of SNNs and neurocognitive models.\n- **Quality**: Some conceptual alignment with neuroscience principles, but experimental validation is absent.\n- **Clarity**: Limited by poor figure annotation and vague methodology.\n- **Significance**: Targets AGI limitations, but lacks practical impact without evidence."}, "weaknesses": {"value": "- **Methodological Flaws**: The SiFu graph (Definition 1) and signal tensor (Definition 2) lack validation against EEG data or cortical activation patterns.  The competitive activation formula ignores biological noise and synaptic delays.\n- **Experimental Gaps**: No performance metrics (e.g., perplexity, BLEU) compare BriLLM to GPT-1 or modern LLMs.  Scalability claims to 100–200B parameters are theoretical without training data or hardware details.\n- **Oversight**: Ignores computational cost trade-offs and energy efficiency compared to Transformers.  Potential biases in semantic mapping (e.g., cultural variability) are unaddressed.\n- **Validation**: Claims of multi-modal compatibility and interpretability lack demonstration with real datasets or tasks."}, "questions": {"value": "1.  Can the authors provide EEG or fMRI data (for example, brain encoding perfermance, or brain-like perfermance) comparisons to validate SiFu’s signal propagation against biological patterns?\n2.  What are the specific perplexity, BLEU, or other metrics for BriLLM’s 1–2B models versus GPT-1, and why were modern LLMs excluded?\n3.  How were the 100–200B parameter scalability estimates derived, and what training infrastructure supports this claim?\n4.  Can the authors quantify the impact of biological noise or synaptic delays on SiFu’s competitive activation mechanism?\n5.  Why were energy consumption and hardware requirements not compared to Transformer-based models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "06WfSexpym", "forum": "D4xSuGvLZA", "replyto": "D4xSuGvLZA", "signatures": ["ICLR.cc/2026/Conference/Submission15330/Reviewer_kCXf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15330/Reviewer_kCXf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761372455074, "cdate": 1761372455074, "tmdate": 1763283659290, "mdate": 1763283659290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BriLLM, a new brain-inspired LLM based on a paradigm called SiFu (Signal Fully-connected flowing) learning. Instead of using traditional representation learning like Transformers, SiFu creates a large graph where each node is a specific word or concept. The model processes language by dynamic signal propagation, where a signal flows through this graph, allowing it to handle sequences with linear $O(L)$ complexity, unlike the quadratic $O(L^2)$ complexity of Transformers. The authors trained 1-2B parameter models and claim they demonstrate stable learning and \"GPT-1-level\" generative abilities."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "**Ambitious conceptual goal:** The paper attempts to address fundamental, recognized limitations of the Transformer paradigm, such as quadratic complexity in sequence length and the \"black box\" nature of representations. The goal of creating a new ML paradigm grounded in neuroscientific principles is ambitious and valuable."}, "weaknesses": {"value": "This paper suffers from several fundamental weaknesses, ranging from incorrect characterizations of its own model to severe overstatements of its experimental results.\n1. **Mischaracterization of non-representation learning:** The model's signal tensor $r \\in \\mathbb{R}^{d_{node}}$ is, by definition, a learned, dense representation of the state. This signal is updated at each step, precisely like the hidden state of an RNN. The model is a graph-based representation learning model.\n2. **Superficial neuroscientific grounding:** The neurocognitive facts used for justification are superficial analogies. Static semantic mapping is simply a 1-to-1 mapping between a node and a token in a vocabulary. Dynamic signal propagation is an RNN-like state update. This makes it difficult to separate the genuine technical contribution from the complex and ultimately misleading terminology.\n3. **Very weak experimental validation:** The paper provides no meaningful quantitative evaluation. The authors' excuse that the model \"precludes direct comparisons to GPT-1's benchmarking\"  is unfounded. BriLLM is a generative language model. It can and should be evaluated using standard metrics, such as perplexity, on a held-out test set.\n4. **Overstated performance claims:** The claim to replicate \"GPT-1-level generative performance\"  is demonstrably false based on the paper's own provided samples. For example, in Table 6, the completion for \"The English biologist Thomas Henry Huxley\" is \"coined World C that ADE XaZul 30 Ars lead singular shipb more smaller im\". This output is incoherent gibberish. GPT-1 (2018) was capable of producing coherent, multi-sentence paragraphs. This discrepancy severely undermines the authors' credibility and the validity of their entire experimental section.\n5. **Architectural impracticality:** The model's $O(n^2)$ parameter scaling with vocabulary size $n$ is one of its weaknesses. While the authors use sparsity to reduce the parameter count, they acknowledge that a standard 40k-token vocabulary would still require a 100-200B parameter model. This is a massive, sparse, and inefficient architecture compared to modern Transformers, which achieve parameter efficiency through weight sharing.\n6. **Lack of clarity:** The description of the model's operation, particularly for prediction and training, is difficult to follow. Section 2.1 defines prediction as finding the node with the maximum signal energy via a complex formula, but Section 3.2 defines it as finding the node with the maximum L2 norm after signal aggregation."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UXDngoGkkF", "forum": "D4xSuGvLZA", "replyto": "D4xSuGvLZA", "signatures": ["ICLR.cc/2026/Conference/Submission15330/Reviewer_wo3a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15330/Reviewer_wo3a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898116078, "cdate": 1761898116078, "tmdate": 1762925623821, "mdate": 1762925623821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study proposes BriLLM, a novel brain-inspired large language model centered on a new learning paradigm called Signal Fully-connected flowing (SiFu). Unlike mainstream deep learning frameworks (such as Transformers) that rely on vector-based representations and transformations, SiFu draws from fundamental principles of cognitive neuroscience to emulate macro-scale brain information processing. Specifically, it incorporates two biologically grounded mechanisms: (1) static semantic mapping, where semantic units are consistently represented in dedicated cortical regions, and (2) dynamic signal propagation, through which cognition emerges via electrophysiological activity spreading across neural pathways. The authors formalize this mechanism as a fully connected graph, where nodes correspond to semantic units (e.g., words) and edges represent learnable signal transmission pathways. Within this framework, BriLLM performs generative language modeling and demonstrates several promising properties, including full node-level interpretability, inherent multimodal compatibility, and context-length-independent scalability. Overall, this is a highly original work and proposes a principled alternative pathway toward artificial general intelligence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The study introduces a genuinely novel, neuroscience-inspired learning paradigm with compelling potential advantages."}, "weaknesses": {"value": "1. Lacks direct performance comparisons with well-established LLMs (e.g., GPT-style models) on standard benchmarks, making it difficult to assess practical competitiveness.\n2. The model processes sequences strictly autoregressively in a fully recursive manner, precluding parallelization during training or inference, which may lead to slow computation for long sequences.\n3. The paper risks overstating its contributions. For instance, Table 1 characterizes traditional deep learning models as “Task-specific” while labeling BriLLM a “Generalist AGI system”. In reality, BriLLM is currently a prototype for language modeling and falls far short of AGI capabilities. Conversely, models like GPT-3 and beyond are widely recognized as foundational steps toward general-purpose intelligent systems."}, "questions": {"value": "What advantages does the model have in terms of training and inference speed compared to GPT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3AlD1c65nT", "forum": "D4xSuGvLZA", "replyto": "D4xSuGvLZA", "signatures": ["ICLR.cc/2026/Conference/Submission15330/Reviewer_fyDE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15330/Reviewer_fyDE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922628075, "cdate": 1761922628075, "tmdate": 1762925623448, "mdate": 1762925623448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}