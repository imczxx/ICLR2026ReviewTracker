{"id": "LtVTZu09B9", "number": 7892, "cdate": 1758041332764, "mdate": 1759897823917, "content": {"title": "Optimizing Diversity and Quality through Base–Aligned Model Collaboration", "abstract": "Alignment has greatly improved large language models (LLMs)’ generation quality at the cost of output diversity, yielding highly similar outputs across samplings. We propose BACo, an inference-time token-level model collaboration framework that dynamically combines a base LLM (for diversity) with its aligned counterpart (for quality), generalizing nudging (Fei et al., 2025) into a full bidirectional model collaboration to optimize and balance diversity and quality. BACo adopts routing strategies based on next-token prediction uncertainty and semantic role heuristics to determine which model to decode from at each step. Unlike prior approaches via retraining, prompt engineering, or multiple sampling passes, BACo can generate diverse yet high-quality outputs and offers strong controllability via a family of routing strategies. Across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art baselines. With our best router, BACo jointly enhances quality and diversity by 21.3% across all tasks and metrics. Human evaluations mirror these gains, highlighting that BACo reliably produces outputs that are both varied and high-quality.", "tldr": "", "keywords": ["Open-ended text generation", "LLM diversity", "Multi-model collaboration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be963c48b233307f4c65b43a45ed0a352b67b631.pdf", "supplementary_material": "/attachment/be2cc14fef785232e07aa56c2a995add7082d33c.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes the BACO framework to achieve balanced optimization between diversity and quality in output text during the inference phase. The essence of BACO is a router that selects either the base model (stronger diversity but weaker quality) or the aligned model (stronger quality but weaker diversity) when generating the next token. Selection rules can be logit-based (e.g., based on the model's maximum probability or entropy for the next token) or content-based (e.g., determining the type of the next top-ranked generated word). Different rules can be combined. Additionally, logit-based rules can be biased toward diversity or quality by adjusting thresholds. \n\nThe authors evaluate the method across multiple datasets, focusing on Flexibility (measured by the area of the Pareto curve achievable through adjustments) and Domination (evaluating how much of the Pareto frontier achieved by various methods is covered by the current method's Pareto curve). Results demonstrate that BACO exhibits superior Flexibility and Domination under optimal rule combinations. Manual evaluations further demonstrate that the text generated by the BACO framework surpasses aligned models in terms of quality, diversity, and creativity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The design philosophy of the BACO framework is concise and well-motivated. Its high degree of flexibility allows experimentation with various rules and corresponding combinations, while the model set awaiting routing can also be freely tested, thus demonstrating strong exploratory potential. User-desired trade-off preferences can be easily implemented through logit-based threshold rules.\n(2) A novel evaluation framework for two-dimensional trade-off problems is proposed, providing concrete quantitative metrics for the achievable flexibility of adjustments and the performance of methods within a set of trade-off approaches. I have not encountered similar quantification methods in previous work pursuing Pareto frontier thus deeming it novel.\n(3) Incorporating human evaluation enhances the confidence in the evaluation."}, "weaknesses": {"value": "My primary concern is that evaluating instruction following solely on NoveltyBench seems insufficient. Instruction following is a quality attribute where aligned models significantly outperform base models, as it represents a core objective in alignment. Judging from the prompt examples, NoveltyBench lacks the rigorous examination of instruction following seen in datasets like IFEval. I'm uncertain whether the current BACO framework rules would still perform well under strict instruction following evaluation, as content-based judgments may lean more toward coherence assessment, and it remains unclear whether logit-based judgments can accurately capture the critical tokens for instruction following and route them to the aligned model."}, "questions": {"value": "I suggest supplementing the IFEval dataset to evaluate the quality-versus-diversity trade-off in instruction following. IFEval requires models to generate text that meets specific constraints, and the compliance of text is verifiable by automated validation. Since compliant text is not unique, it can also be used for diversity evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qtyeVTynTW", "forum": "LtVTZu09B9", "replyto": "LtVTZu09B9", "signatures": ["ICLR.cc/2026/Conference/Submission7892/Reviewer_tZot"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7892/Reviewer_tZot"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760972068296, "cdate": 1760972068296, "tmdate": 1762919923453, "mdate": 1762919923453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BACO (Base-Aligned Collaboration), an inference-time framework that dynamically combines a base LLM with its aligned counterpart at the token level to optimize the diversity-quality trade-off. The method uses routing strategies based on prediction uncertainty and semantic heuristics to determine which model generates each token. Experiments across three tasks show 21.3% joint improvement in quality and diversity, with BACO covering 32.7% of the Pareto frontier."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a well-documented problem of alignment reducing output diversity. The inference-time approach requires no retraining and works with off-the-shelf models, making it immediately deployable. \n    \n2. BACO demonstrates strong and consistent empirical results across multiple settings. The improvements (21.3% overall, 32.7% frontier dominance) hold across different tasks, datasets, and model families (Llama-3, Olmo2). Human evaluations mirror automatic metrics, with annotators rating BACO outputs as more diverse and significantly more creative than aligned model baselines."}, "weaknesses": {"value": "1. The evaluation focuses heavily on diversity metrics without testing performance on standard task benchmarks. While Tables 2, 3, 7, 8, and 9 demonstrate improvements in diversity, it remains unclear how BACO affects core capabilities like mathematical reasoning, code generation, factual question answering, or instruction following. Routing to the base model could potentially degrade performance on tasks requiring precision and factuality. Without evaluation on benchmarks like GSM8K, HumanEval, TriviaQA, or MT-Bench, the practical impact of increased diversity on real task performance is unknown.\n\n2. While the paper claims BACO is \"lightweight\" and mentions optimizations like caching and speculative decoding in Appendix C, no actual runtime measurements are provided. There are no wall-clock time comparisons, computational overhead analysis, or memory usage statistics. The early stopping failure mode discussed in Section I.1, which requires resampling when the router is tuned aggressively, further undermines efficiency claims. The frequency of this failure and its practical impact remain unquantified.\n    \n3. The routing strategies appear ad-hoc and lack principled design. Multiple routers are presented without clear guidance on which to use for specific tasks."}, "questions": {"value": "1. Can you provide quantitative efficiency analysis including wall-clock time, and memory usage compared to single-model generation? How frequently does the early stopping failure occur across different routers and thresholds, and what is the overhead of resampling?\n    \n2. While main results in paper show improvements on diversity metrics across NoveltyBench, WildChat, and Narrative-Discourse datasets, but how does BACO affect performance on tasks like mathematical reasoning (GSM8K, MATH), code generation (HumanEval, Live Code Bench), instruction following (MT-Bench, AlpacaEval) or safety (Advbench, JailbreakBench)? Does increased diversity help or hurt these different capabilities, and are there task-specific trade-offs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XzeyMVFBDu", "forum": "LtVTZu09B9", "replyto": "LtVTZu09B9", "signatures": ["ICLR.cc/2026/Conference/Submission7892/Reviewer_Ywtf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7892/Reviewer_Ywtf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761275829121, "cdate": 1761275829121, "tmdate": 1762919923089, "mdate": 1762919923089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BACO, a token-level inference-time framework for combining a base and an aligned LLM to balance and optimize both diversity and quality of response. It follows prior “nudging” approaches, adding more flexible routing strategies based on token-level prediction uncertainty and linguistic roles. Experiments spanning three open-ended generation tasks show that BACO consistently surpasses existing baselines, providing fine-grained controllability of the diversity-quality trade-off."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow. It clearly introduces the core question and limitations of prior works. \n2. The evaluation protocol is comprehensive and well-designed, which would be meaningful for the research community."}, "weaknesses": {"value": "1. The proposed approach is highly related to model ensemble and collaboration approaches. Related literature should be included and discussed in the paper.\n2. The proposed routing approach is largely heuristic, with only a blur description of the philosophy. Adding experiments to show why using confidence/uncertainty or punctuation/function tokens would enhance the insight.\n3. The experiments are limited to open-ended generation tasks. It would be better to include QA/Math reasoning tasks, which could also benefit from diversity and have a ground truth label to directly reflect the response quality.\n4. It is unclear which router is the BACo Best? This may hurt the soundness of the main experiment."}, "questions": {"value": "1. Will it be possible to generalize the hard switch into a soft output probability mixture as in previous model ensemble works? (See EVA, DEEPEN, etc.)\n2. What would be the result if BACo is employed on reasoning tasks such as AIME/GPQA/LiveCodeBench? \n3. Will a learned router outperform an existing heuristic router? How to train such a router?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nLHHffEQTS", "forum": "LtVTZu09B9", "replyto": "LtVTZu09B9", "signatures": ["ICLR.cc/2026/Conference/Submission7892/Reviewer_AdKg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7892/Reviewer_AdKg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659392956, "cdate": 1761659392956, "tmdate": 1762919922645, "mdate": 1762919922645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}