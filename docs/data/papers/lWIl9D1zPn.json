{"id": "lWIl9D1zPn", "number": 17881, "cdate": 1758281622257, "mdate": 1759897147968, "content": {"title": "Hyperbolic Residual Quantization: Discrete Representations for Data with Latent Hierarchies", "abstract": "Hierarchical data arise in countless domains, from biological taxonomies and organizational charts to legal codes and knowledge graphs. Residual Quantization (RQ) is widely used to generate discrete, multitoken representations for such data by iteratively quantizing residuals in a multilevel codebook. However, its reliance on Euclidean geometry can introduce fundamental mismatches that hinder modeling of hierarchical branching, necessary for faithful representation of hierarchical data. In this work, we propose Hyperbolic Residual Quantization (HRQ), which embeds data natively in a hyperbolic manifold and performs residual quantization using hyperbolic operations and distance metrics. By adapting the embedding network, residual computation, and distance metric to hyperbolic geometry, HRQ imparts an inductive bias that aligns naturally with hierarchical branching. We claim that HRQ in comparison to RQ can generate more useful for downstream tasks discrete hierarchical representations for data with latent hierarchies. We evaluate HRQ on two tasks: supervised hierarchy modeling using WordNet hypernym trees, where the model is supervised to learn the latent hierarchy - and hierarchy discovery, where, while latent hierarchy exists in the data, the model is not directly trained or evaluated on a task related to the hierarchy. Across both scenarios, HRQ hierarchical tokens yield better performance on downstream tasks compared to Euclidean RQ with gains of up to 20% for the hierarchy modeling task. Our results demonstrate that integrating hyperbolic geometry into discrete representation learning substantially enhances the ability to capture latent hierarchies.", "tldr": "Residual Quantization in Hyperbolic space generates better discrete representations for data with latent hierarchies", "keywords": ["Residual quantization", "hyperbolic space", "semantic tokens", "latent hierarchies"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43611cbf5e4b0c1065c39c08b9f51adb3d0655ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Hyperbolic Residual Quantization (HRQ), a new method for creating discrete, multi-token representations of data that has a latent hierarchical structure. The authors argue that standard Residual Quantization (RQ) is suboptimal because its reliance on Euclidean geometry is a poor match for the branching, tree-like nature of hierarchies. The proposed HRQ method adapts this process to a hyperbolic manifold, performing residual calculations and distance measurements using hyperbolic operations. The method is evaluated in two settings: a \"Hierarchy Modeling\" task using WordNet hypernyms and a \"Hierarchy Discovery\" task where tokens are learned for a downstream recommender system. In both scenarios, the discrete tokens generated by HRQ are shown to be more effective and lead to better downstream performance than those generated by the standard Euclidean RQ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's core idea is a significant strength. The motivation to move from Euclidean to hyperbolic geometry for hierarchical data is well-founded, logical, and represents a novel application of these concepts to residual quantization.\n\nThe empirical results in the supervised \"Hierarchy Modeling\" task seems very strong. The reported performance gains over the baseline RQ method are substantial and provide clear evidence that the HRQ method is more effective at capturing the target hierarchical relationships.\n\nThe paper is also well-written and clear. It does a good job of introducing the necessary background concepts in both hyperbolic geometry and residual quantization, making the new method and its motivation accessible."}, "weaknesses": {"value": "A primary weakness is an apparent contradiction in the method's technical implementation. While the paper's premise is to use native hyperbolic operations, the loss function for training the codebook (l_cmt in Algorithm 1) appears to be a standard Euclidean L2 norm. This is not justified and seems to conflict with the core motivation of avoiding Euclidean metrics. I have not gone through all math to check if this holds for hyperbolic space (thus lowered my confidence). Still this seems like a major contradiction.\n\nA second major weakness is the omission of key, highly relevant baselines. The paper does not compare against other state-of-the-art models for hierarchical representation, such as the Hierarchy Transformer (HiT). This is a critical omission because HiT shares the same goal (encoding hierarchies), uses the same solution space (hyperbolic geometry), and is evaluated on the same key dataset (WordNet). The current paper only compares HRQ to Euclidean RQ, which merely proves that a hyperbolic approach is better than a Euclidean one for this specific task. It fails to demonstrate that this two-stage quantization-then-model approach is competitive with, or superior to, end-to-end continuous hyperbolic models.\n\nThe claims made in the \"Hierarchy Discovery\" experiment are also not fully substantiated. The experiment shows that HRQ tokens lead to better performance in a recommender system, which the authors attribute to the model's discovery of latent hierarchies. However, no direct evidence is provided that a meaningful hierarchy was actually found. The improved performance could just as easily be a result of the hyperbolic autoencoder acting as a more effective regularizer for that specific dataset."}, "questions": {"value": "Could you clarify the choice of using a Euclidean L2 norm for the codebook loss (l_cmt) instead of a loss function based on the hyperbolic distance metric? Did you experiment with a fully hyperbolic loss, and if so, how did it affect training stability and final performance?\n\nFor the \"Hierarchy Discovery\" experiment, do you have any way to qualitatively or quantitatively verify that the model is actually learning a semantically meaningful hierarchy, beyond the improved downstream task performance?\n\nHow do you think your two-stage quantization-then-translation approach would compare to end-to-end models like the Hierarchy Transformer (HiT), which also use hyperbolic geometry to learn continuous representations directly on the WordNet task?\n\nWhat is your hypothesis for why the standard RQ baseline performs so poorly as the hidden dimensionality increases in Figure 1, while HRQ's performance is so stable?\n\nCould you provide more geometric intuition on the hyperbolic residual calculation? In Euclidean space, the residual is a new vector. What is the geometric interpretation of this operation in the Poincaré ball model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pIvvwKlqYm", "forum": "lWIl9D1zPn", "replyto": "lWIl9D1zPn", "signatures": ["ICLR.cc/2026/Conference/Submission17881/Reviewer_6yTa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17881/Reviewer_6yTa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942615028, "cdate": 1761942615028, "tmdate": 1762927708623, "mdate": 1762927708623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "HRQ replaces Euclidean RQ with hyperbolic geometry—embedding, residuals, and distances—all aligned to hierarchical branching. On WordNet (supervised hierarchy modeling) and hierarchy discovery tasks, its discrete multitoken representations consistently outperform Euclidean RQ, with gains up to ~20% on the former."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Across hierarchy modeling (WordNet) and hierarchy discovery tasks, HRQ’s tokens improve performance—up to ~20% in the supervised setting compared to RQ."}, "weaknesses": {"value": "- Insufficient comparisons to strong hyperbolic baselines. Prior work on hyperbolic quantization (e.g., *HyperVQ*) is discussed but not empirically compared. As a result, the claimed advantages over stronger or more numerous hyperbolic baselines remain unsubstantiated; the study lacks both breadth in baselines and depth in experimental analysis.\n- Unclear motivation for hyperbolic space. The paper does not convincingly quantify *why* hyperbolic geometry is necessary here. The authors’ claim — *“If multitokens are structured semantically, they can share tokens across compositions, enabling information sharing and more efficient, robust representations than flat tokens.”* — is not backed by sufficient **theoretical** or **empirical** evidence.\n- Limited validation scope. The method is suggested to be evaluated on more datasets and more hierarchical tasks to establish generality and strengthen the paper’s empirical foundation."}, "questions": {"value": "1. How do the resulting discrete token representations perform when incorporated into Transformers and downstream tasks, compared to Euclidean ones?\n2. Are there any case studies that demonstrate the learned semantic hierarchy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rIGcaOhFT8", "forum": "lWIl9D1zPn", "replyto": "lWIl9D1zPn", "signatures": ["ICLR.cc/2026/Conference/Submission17881/Reviewer_rJFw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17881/Reviewer_rJFw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955706860, "cdate": 1761955706860, "tmdate": 1762927708082, "mdate": 1762927708082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HRQ, a method for learning discrete multitoken representations for hierarchical data structures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "It demonstrates empirical improvements, with HRQ tokens outperforming standard residual quantization on multiple benchmark tasks related to hierarchy modeling and discovery"}, "weaknesses": {"value": "Regarding the references, the authors have omitted numerous relevant citations, including HiHPQ and works on knowledge-graph representation and recommender systems, among others.\n\nFor the main loss formula, the authors failed to provide proper numbering, which represents poor writing practice. The authors employ Euclidean distance for RQ in Euclidean space; however, Euclidean RQ typically utilizes dot product. Furthermore, the contrastive loss formula is not an essential component of RQ and should only be employed when specific semantic hierarchical supervision modeling is required.\n\nHRQ primarily compares against the Euclidean version of RQ, lacking systematic analysis relative to other state-of-the-art structured representation learning approaches, and missing broader baseline comparisons.\n\nThe paper exhibits limited theoretical analysis and explanation, relying predominantly on experimental results while lacking theoretical discussion and proof regarding the limitations of hyperbolic space quantization mechanisms."}, "questions": {"value": "The manuscript does not link to code or a reproducible pipeline. Is a public implementation planned, and can you outline what main dependencies and practical steps are required to reproduce your experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8QUq2exUm6", "forum": "lWIl9D1zPn", "replyto": "lWIl9D1zPn", "signatures": ["ICLR.cc/2026/Conference/Submission17881/Reviewer_brqf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17881/Reviewer_brqf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984091765, "cdate": 1761984091765, "tmdate": 1762927707746, "mdate": 1762927707746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Hyperbolic Residual Quantization (HRQ), a novel discrete representation learning technique that performs residual quantization in hyperbolic space. They propose this method to better model hierarchical data structures, which are poorly captured by traditional Euclidean-based quantization methods. By leveraging the exponential growth and tree-like properties of hyperbolic geometry, HRQ produces multi-token representations that more accurately reflect latent hierarchies in data such as WordNet or product catalogs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a novel integration of hyperbolic geometry into residual quantization, enabling discrete representations that naturally capture hierarchical relationships.\n\nTheoretical motivation is strong and well-founded, aligning hyperbolic geometry’s exponential growth with hierarchical data’s inherent structure.\n\nExperiments show consistent and significant improvements over Euclidean baselines in both supervised and unsupervised settings, demonstrating the model’s robustness and practical relevance."}, "weaknesses": {"value": "The method assumes data follows a hierarchical structure, limiting its usefulness in domains without clear hierarchies.\nHow does the model behave when applied to data lacking an explicit or strong hierarchical organization?\n\n\nThe computational cost and stability issues of hyperbolic training are not explored, leaving efficiency and scalability uncertain.\nWhat are the practical training challenges or trade-offs when scaling HRQ to large datasets or deeper architectures?"}, "questions": {"value": "Same as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vVxpgHMBbH", "forum": "lWIl9D1zPn", "replyto": "lWIl9D1zPn", "signatures": ["ICLR.cc/2026/Conference/Submission17881/Reviewer_cMFE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17881/Reviewer_cMFE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154319093, "cdate": 1762154319093, "tmdate": 1762927707373, "mdate": 1762927707373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}