{"id": "ahWmeQG3K2", "number": 7063, "cdate": 1758006454123, "mdate": 1759897874555, "content": {"title": "EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models", "abstract": "Emotion understanding is a critical yet challenging task. \nRecent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities in this area. However, MLLMs often suffer from ``hallucinations'', generating irrelevant or nonsensical content.\nTo the best of our knowledge, and despite the importance of this issue, there has been no dedicated effort to evaluate emotion-related hallucinations in MLLMs.\nIn this work, we introduce \\textbf{EmotionHallucer}, the first benchmark for detecting and analyzing emotion hallucinations in MLLMs. \nUnlike humans, whose emotion understanding stems from the interplay of biology and social learning, MLLMs rely solely on data-driven learning and lack innate emotional instincts. \nFortunately, emotion psychology provides a solid foundation of knowledge about human emotions.\nBuilding on this knowledge, we assess emotion hallucinations from two perspectives: emotion psychology knowledge and realworld multimodal perception. \nTo support robust evaluation, we utilize an adversarial binary question–answer (QA) framework, which employs carefully crafted basic and hallucinated pairs to assess the emotion hallucination tendencies of MLLMs.\nBy evaluating 41 LLMs and MLLMs on EmotionHallucer, we find that:\n(1) most current models exhibit substantial issues with emotion hallucinations;\n(2) closed-source models outperform open-source models in detecting emotion hallucinations, and reasoning capability provides additional advantages;\nand (3) existing models perform better in emotion psychology knowledge than in multimodal emotion perception.\nAs a byproduct, these findings inspire us to propose the \\textbf{PEP-MEK} framework, which yields an average improvement of 9.90\\% in emotion hallucination detection across selected models.\nResources will be available on GitHub.", "tldr": "", "keywords": ["Emotion Hallucination", "Emotion Understanding", "Affective Computing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e365daa44953af3d2e0e1e0caf5e8a8555d14e3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces EmotionHallucer, the first benchmark designed to evaluate emotion hallucinations in MLLMs. The benchmark assesses two complementary aspects, emotion psychology knowledge and multimodal emotion perception. Evaluating 41 models using an adversarial binary QA framework, the paper reports three main findings and propose PEP-MEK, a framework that integrates modality-specific and emotional reasoning to mitigate emotion hallucination. PEP-MEK achieves an average 9.9% improvement across selected models. Therefore, EmotionHallucer provides a benchmark and valuable insights for advancing emotionally reliable MLLMs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.\tThe paper introduces the first study of emotion hallucinations in MLLMs, addressing a crucial yet previously unexplored aspect of emotion understanding.\n\n2.\tEmotionHallucer is designed based on emotion psychology and real-world emotion perception, spanning four modalities and multiple evaluation settings. The adversarial QA framework provides a controlled protocolto assess emotional reasoning errors.\n\n3.\tThe benchmark evaluates a large number of models, providing comprehensive insights into the current state of MLLMs. \n\n4.\tPEP-MEK is compatible with both open- and closed-source models through standard APIs, offering a plug-and-play approach for emotion hallucination mitigation.\n\n5.\tThe paper is well-structured, easy to follow."}, "weaknesses": {"value": "1.\tWhile the adversarial binary QA framework provides strong objectivity and control, and the authors have additionally performed consistency checks between binary and open-ended results, this setting may still not fully capture open-ended emotion hallucinations in real-world generative scenarios. It would be valuable to further discuss how this limitation could be addressed, and what directions the authors plan to explore in future work.\n\n2.\tThe benchmark primarily focuses on English-language. Although some data sources contain diverse cultural content, explicitly incorporating cross-cultural emotion understanding tasks would further enhance its generalizability.\n\n3.\tWhile this trade-off is reasonable given the improvement in reliability, a more detailed discussion on the efficiency–accuracy balance and potential optimization strategies would further strengthen the work."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SIcrT5EbZe", "forum": "ahWmeQG3K2", "replyto": "ahWmeQG3K2", "signatures": ["ICLR.cc/2026/Conference/Submission7063/Reviewer_jLHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7063/Reviewer_jLHy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760938037192, "cdate": 1760938037192, "tmdate": 1762919254702, "mdate": 1762919254702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EmotionHallucer, a novel benchmark designed to evaluate emotion-related hallucinations in multimodal large language models (MLLMs). The benchmark spans four modalities (text, image, audio, video) and is organized around two main dimensions: emotion psychology knowledge and multimodal emotion perception. The authors also propose PEP-MEK, a reasoning-enhanced framework aimed at mitigating emotion hallucinations. Extensive experiments on 41 LLMs and MLLMs reveal widespread emotion hallucination issues, particularly in multimodal perception tasks, and demonstrate the effectiveness of PEP-MEK in improving model robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Novel Benchmark: EmotionHallucer is the first comprehensive benchmark targeting emotion hallucinations, with a well-designed adversarial evaluation protocol.\n- Multimodal Coverage: The benchmark spans text, image, audio, and video, enabling a holistic assessment of MLLMs’ emotion understanding capabilities.\n- Large-Scale Evaluation: Experiments on 41 models offer broad insights into current limitations and trends.\n- Practical Mitigation Framework: PEP-MEK demonstrates consistent improvements across models and modalities, offering a simple yet effective approach to reducing emotion hallucinations."}, "weaknesses": {"value": "- Limited Cross-Lingual and Cultural Analysis: The benchmark is limited to English, and there is no discussion of how cultural or linguistic differences might affect emotion hallucination patterns. This limits the generalizability of the findings.\n- Superficial Error Analysis: While the paper reports performance drops in open-set and multimodal settings, it does not deeply investigate the root causes of failures (e.g., which types of cues are most often misinterpreted).\n- Benchmark Design Limitations: The binary QA format, while useful for controlled evaluation, may not fully capture the complexity of open-ended emotion understanding in real-world scenarios.\n- Lack of Human Baseline: The absence of human performance comparison makes it difficult to gauge the practical significance of the model results."}, "questions": {"value": "- Have the authors considered extending EmotionHallucer to include non-English or cross-cultural emotional expressions? If so, what challenges do they anticipate?\n- Could the authors provide more detailed analysis or examples of cases where PEP-MEK fails? Understanding its limitations could help guide future improvements.\n- How might the benchmark be adapted to support more open-ended emotion generation or reasoning tasks, beyond binary QA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "e9YhBIH2nP", "forum": "ahWmeQG3K2", "replyto": "ahWmeQG3K2", "signatures": ["ICLR.cc/2026/Conference/Submission7063/Reviewer_QF5h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7063/Reviewer_QF5h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787337535, "cdate": 1761787337535, "tmdate": 1762919254276, "mdate": 1762919254276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "EmotionHallucer introduces the first systematic benchmark for evaluating emotion-related hallucinations in multimodal large language models (MLLMs). The benchmark is grounded in emotion psychology and real-world multimodal perception and uses adversarial binary QA pairs (basic vs. hallucinated versions) across modalities (text, image, audio, video). The authors evaluate 41 MLLMs (open- and closed-source) on multiple subtests (e.g., perception-level, psychology/knowledge-level, reasoning-result), report quantitative metrics (including bias/FP measures and separate accuracy for basic vs. hallucinated items), and analyze model behaviors. They find that many models remain vulnerable to emotion hallucinations, that closed-source models often outperform open-source ones, and that models are typically better at explicit emotion-knowledge tasks than at grounded multimodal perception and inference. To improve detection, the paper proposes PEP-MEK, a multimodal+emotion-knowledge augmentation framework that boosts hallucination detection and is evaluated via ablations. The paper provides dataset construction, annotation procedures, limitations, and plans to release resources on GitHub."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "### A novel benchmark (EmotionHallucer) specifically targeting emotion hallucinations in MLLMs:\n- Covers multiple modalities and multiple diagnostic levels (perception, emotion knowledge, reasoning results).\n- Uses adversarially constructed basic vs. hallucinated QA pairs to probe hallucination propensity.\n### Large-scale empirical evaluation and analysis:\n- Systematic evaluation of 41 MLLMs (both open- and closed-source) with detailed metrics (Pct. Diff, FP Ratio, separate Basic vs. Hallucinated accuracy, overall scores).\n- Insights showing systematic weaknesses (e.g., multimodal perception and reasoning produce more hallucinations; closed-source models typically fare better).\n### Proposed mitigation/analysis method (PEP-MEK) and ablation studies:\n- PEP-MEK integrates psychology-grounded emotion knowledge and perceptual cues to improve hallucination detection.\n- Demonstrated consistent improvements across models and includes ablation studies showing the contribution of emotion knowledge and other components."}, "weaknesses": {"value": "### Annotation noise and scope limited to English and certain datasets:\n- The benchmark relies on human annotation (e.g., creating hallucinated variants), admitting annotation noise.\n- The dataset is English-only and does not address cross-lingual or cultural variability in emotional expression.\n### Partial exploration of root causes:\n- While the paper documents hallucination phenomena and correlates them with modality and model class, it does not deeply investigate underlying causes (e.g., pretraining biases, modality misalignment, lack of emotion-specific supervision) or provide mechanistic explanations.\n### Separation of evaluation axes and incomplete real-world integration:\n- Emotion understanding and hallucination detection are treated separately, whereas practical systems need integrated capabilities (joint perception, inference, and hallucination-awareness).\n- Temporal and long-form audio/video integration remain challenging and less explored; the benchmark and methods may not fully capture these complex real-world scenarios."}, "questions": {"value": "SEE WEAKNESS"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NO"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n2YpkBjYXG", "forum": "ahWmeQG3K2", "replyto": "ahWmeQG3K2", "signatures": ["ICLR.cc/2026/Conference/Submission7063/Reviewer_abcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7063/Reviewer_abcp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972725906, "cdate": 1761972725906, "tmdate": 1762919253853, "mdate": 1762919253853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EmotionHallucer, a benchmark to detect and analyze emotion-related hallucinations in multimodal LLMs. It evaluates two axes: \n- Emotion psychology knowledge (theory, definitions, empirical findings) \n- Real-world multimodal perception (category, intensity, reasoning cues/results) across text, image, audio, and video. \n\nThe benchmark uses adversarial binary QA pairs—a “basic” item and a matched “hallucinated” item—and counts a prediction as correct only if the model answers both correctly, reducing prompt/length confounds seen in captioning metrics and self-evaluation bias."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This is the first benchmark that is dedicated to emotion hallucinations, spanning both psychology knowledge and multimodal perception; prior hallucination suites are general-purpose. The seven subcategories (theory/definition/finding; category/intensity/reasoning cue/reasoning result) make the construct very concrete. And, the adversarial paired QA design (basic vs hallucinated) is what I call a neat, low-variance way to test detection of hallucination, beyond typical caption/LLM-judge setups. \n\nThe paper is well structured, with clear logic, a well-defined task taxonomy, examples, and an easy-to-follow pipeline; the appendices document the collection/annotation and PEP-MEK details; ethics and reproducibility statements are included. \n\nNon-trivial scale and coverage, with broad evaluation and clear metrics, which is very nice."}, "weaknesses": {"value": "1. Adversarial pair construction & QA artifacts. The process risks introducing superficial cues between the basic and “hallucinated” versions. Report inter-annotator agreement, pair-level quality controls, and checks against annotation artifacts (e.g., spurious lexical markers).\n\n2. Latency/compute overhead and failure cases are not quantified. A wall-clock and token-cost-wise comparison is needed here, along with ablations for each PEP-MEK component and per-subcategory gains."}, "questions": {"value": "1. Again, I would like to know a wall-clock and token-cost-wise comparison, along with ablations for each PEP-MEK component and per-subcategory gains.\n\n2. Bias balancing details. The authors stated that the yes/no is balanced. Can you share the exact balance per subcategory and modality, and how you prevented position or wording biases between paired items? \n\n3. Open-ended evaluation. Beyond the pilot LLM-judge setup, do you plan a human-rated open-ended benchmark slice to validate the binary proxy and reduce judge-model bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jVgumul8pt", "forum": "ahWmeQG3K2", "replyto": "ahWmeQG3K2", "signatures": ["ICLR.cc/2026/Conference/Submission7063/Reviewer_aDze"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7063/Reviewer_aDze"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991474648, "cdate": 1761991474648, "tmdate": 1762919253308, "mdate": 1762919253308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EmotionHallucer, the first benchmark designed to evaluate emotional hallucinations in multimodal large language models (MLLMs). The findings indicate that most current models exhibit significant issues with emotional understanding, particularly concerning hallucinations. By introducing the PEP-MEK framework, the authors demonstrate an average performance improvement of 9.90% in emotion hallucination detection. The study draws from emotion psychology knowledge and real-world multimodal perception, providing a comprehensive evaluation perspective. Overall, the paper contributes valuable tools and directions for future research in the field of emotional understanding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This study fills a critical gap in the evaluation of emotional hallucinations, offering the first benchmark tailored for MLLM emotional understanding. The introduction of the PEP-MEK framework shows significant effectiveness, enhancing model performance in hallucination detection. The authors provide robust experimental data and statistical evidence to support their conclusions, increasing the paper's credibility. The research methodology integrates insights from emotion psychology, ensuring the scientific validity of the assessments. Additionally, the use of diverse multimodal data sources adds practical relevance to the findings."}, "weaknesses": {"value": "Language Limitation: The study is restricted to English, failing to account for cross-linguistic and cross-cultural variations in emotional expression.\nComplex Definitions: The definitions and classifications of emotional hallucinations may be overly intricate, potentially leading to ambiguity in the evaluation process.\nSuboptimal Performance: Model performance in processing multimodal data, particularly in audio and video emotional understanding, remains inadequate.\nResult Stability: The stability and reliability of certain experimental results need further validation to ensure consistency.\nReal-World Reflection: The assessment of existing models may not accurately reflect their performance variations in practical applications."}, "questions": {"value": "In defining emotional hallucinations, how can researchers effectively balance scientific rigor with interpretability?\n\nIs the current benchmark adaptable to different types of multimodal models to ensure consistency in evaluation?\n\nCan the PEP-MEK framework be further optimized for additional emotional understanding tasks beyond those currently evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kUr1OMMSB0", "forum": "ahWmeQG3K2", "replyto": "ahWmeQG3K2", "signatures": ["ICLR.cc/2026/Conference/Submission7063/Reviewer_Fm9U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7063/Reviewer_Fm9U"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051352415, "cdate": 1762051352415, "tmdate": 1762919252909, "mdate": 1762919252909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}