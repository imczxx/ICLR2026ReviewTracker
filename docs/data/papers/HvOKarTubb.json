{"id": "HvOKarTubb", "number": 14554, "cdate": 1758238819416, "mdate": 1759897362848, "content": {"title": "Beyond Markovian Drifts: Action-Biased Geometric Walks with Memory for Personalized Summarization", "abstract": "Document summarization helps readers focus on the \"content-of-interest\", a *subjective* and *time-variant* quantity. Capturing this *dynamic subjectivity* requires modeling how user preferences evolve over time, thereby demanding *personalized summarization*. Recent news recommendation and summarization models often assume that preferences follow a *memoryless or short-memory random walk* on interaction graphs, i.e., a Markovian diffusion seeded at the latest interaction or compressed into a short hidden state or prompt. We ask whether such a hypothesis also holds for personalized summarization. To test this, we propose **Walk2Pers**, a lightweight encoder–decoder framework that extends the walk view with *action-conditioned geometric steps*, decomposed into (i) a *magnitude* controlling shift strength and (ii) an *orientation* capturing continuity vs. novelty. The process is mediated by dual memory lanes that reinforce consistent interests while suppressing disinterest, and is augmented with a drift term for summary requests. We show theoretically that such structured walks approximate first-order action-conditioned kernels, and empirically validate the hypothesis on PENS, OpenAI-Reddit, and PersonalSum. Using PerSEval, a personalization metric with strong human correlation, Walk2Pers outperforms specialized personalized summarizers by an average of $0.41 \\uparrow$, and strong LLM baselines (DeepSeek-R1-14B, LLaMA-2-13B, Mistral-7B, Zephyr-7B) by $0.22 \\uparrow$. Analyses further confirm cross-domain robustness ($0.19 \\uparrow$ over the best LLM) and stability on long histories. Together, these results support viewing personalized summarization as an *action-biased geometric walk with memory*, offering both interpretability and efficiency.", "tldr": "", "keywords": ["User Preference Modeling", "Personalized Recommendation", "Personalized Summarization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80e9b00251abbe4ca2ab02aeab25a34b9a7b53b2.pdf", "supplementary_material": "/attachment/47fa5877237a16a1e3d07e6d2197f95945467cda.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses personalized document summarization, where user preferences evolve dynamically over interaction histories (e.g., clicks, skips, summaries). It challenges the prevailing Markovian Drift Hypothesis (MDH)—assuming preferences follow memoryless or short-memory random walks (e.g., seeded at recent interactions or compressed states)—and proposes the Structured Walk Hypothesis (SWH) as an alternative.\n\nKey Contributions\n\nTheoretical Framework (SWH): Models preference evolution as an action-conditioned geometric walk on a User-Interaction Graph (UIG). Updates decompose into:\n- Magnitude ($ \\text{mag}(a(t)) $): Controls shift strength.\n- Orientation ($ \\theta(a(t)) $): Balances continuity (momentum) vs. novelty (orthogonal direction).\n- Dual memory lanes ($ h^+, h^- $): Reinforce interests (e.g., clicks) and suppress disinterests (e.g., skips).\n- Drift term ($ \\Delta $): Handles summary requests.\n- Proves SWH approximates first-order action-conditioned kernels, generalizing MDH.\n\nWalk2Pers Model: Lightweight encoder-decoder instantiation of SWH.\n- Encoder: Fuses actions into b-cells, applies dual memories and geometric steps; trained on next-behavior prediction and alignment losses.\n- Decoder (T5-based): Contextualizes query documents with user-aware gating (UCA variant) for personalized generation.\n- Efficient: $ O(pd) $ complexity for long horizons ($ p \\ll T $).\n\nEmpirical Validation:\n- Datasets: PENS (news, clicks/skips), PersonalSum (EN, summary requests), OpenAI-Reddit (multi-domain, long histories).\n- Next-Behavior Prediction (RQ1/2): Outperforms MDH baselines (e.g., NAML, NRMS, EBNR, SMD, AGD) on PENS: AUC 0.532, MRR 0.23, nDCG@10 0.249 (vs. AGD's 0.446/0.113/0.073).\n- Personalized Summarization (RQ3): On PENS, beats specialized MDH models (PENS variants, GTP, SP) by avg. 0.41 on PerSEval (PSE-JSD/SU4/METEOR); LLMs (DeepSeek-14B, Mistral-7B, etc., with 2-shot prompting) by 0.22 (e.g., 0.452/0.383/0.449 vs. DeepSeek's 0.248/0.094/0.097).\n- Cross-Domain & Robustness: On OpenAI-Reddit, +0.09/0.20/0.24 over best LLM; on PersonalSum, 0.31/0.28/0.3. Achieves human-judgment rating of 7/7 (RMSD 0.396). Stable on long histories; oracles (e.g., BigBird) lag by 0.20+.\n- Ablations confirm SWH components (memory, geometry) yield complementary gains.\n\nOverall, the work demonstrates SWH's superiority for capturing evolving preferences, offering interpretability and efficiency over MDH/LLM baselines, with potential for broader personalization systems."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Originality\n\nThe paper introduces a novel Structured Walk Hypothesis (SWH) for modeling user preference evolution in personalized summarization, extending beyond the prevalent Markovian Drift Hypothesis (MDH) by incorporating action-biased geometric walks with dual memory lanes for reinforcement and suppression. This formulation creatively combines ideas from graph diffusion (e.g., random walks with restart), trajectory embeddings (e.g., JODIE), and relational modeling (e.g., RotatE), while applying them to a new domain: dynamic, long-horizon user interactions in summarization. The decomposition of updates into magnitude (shift strength), orientation (continuity vs. novelty), and drift terms removes limitations of memoryless models, offering a fresh perspective on preference dynamics. The Walk2Pers model serves as an innovative instantiation, integrating these elements into a lightweight encoder-decoder framework.\n\nQuality\n\nThe work demonstrates high technical rigor, with a solid theoretical foundation showing that SWH approximates first-order action-conditioned kernels while generalizing MDH. Empirically, it is validated through comprehensive experiments on three diverse datasets (PENS, PersonalSum-EN, OpenAI-Reddit), addressing RQs via next-behavior prediction (Table 2: AUC 0.532, MRR 0.23 vs. MDH baselines like AGD's 0.446/0.113) and summarization tasks (Table 3: PSE-JSD/SU4/METEOR 0.452/0.383/0.449 on PENS, outperforming specialized models by 0.41 avg. and LLMs like DeepSeek-14B by 0.22). Cross-domain robustness (Table 4: +0.19 over best LLM on OpenAI-Reddit) and human-judgment alignment (Table 5: 7/7 rating, RMSD 0.396) further strengthen the evaluation. Ablations confirm complementary gains from SWH components (e.g., dual memory + geometry), and the model's efficiency (O(pd) for long horizons) is well-justified. Baselines are fairly chosen (e.g., NRMS, EBNR, 2-shot LLMs), with sound training details (e.g., joint losses, T5-base).\n\nClarity\n\nThe paper is exceptionally well-structured and readable, with clear progression from problem motivation (information overload in multi-aspect documents) to hypothesis testing (MDH vs. SWH), methodology (UIG graphs, equations like (3) for SWH), and evaluation (RQs explicitly addressed). Visual aids (e.g., Figure 1 illustrating Walk2Pers architecture) and tables (e.g., Table 1 contrasting SWH/MDH) enhance comprehension, while appendices provide derivations, notations (Table 11), and hyperparameters (Table 12). Technical terms are defined upfront (e.g., b-nodes, dual lanes), and the writing avoids jargon overload, making complex ideas like geometric decomposition accessible.\n\nSignificance\n\nBy outperforming SOTA summarizers and LLMs on personalization metrics (e.g., PerSEval with strong human correlation), the paper advances user-centric NLP, particularly for evolving preferences in news/recommendation systems. Its lightweight design offers practical efficiency over prompt-heavy LLMs, with implications for real-world applications like tailored updates in high-volume domains (e.g., Reddit, news feeds). The SWH framework could extend to broader personalization tasks (e.g., recommendation, dialogue), promoting interpretability via geometric/memory structures."}, "weaknesses": {"value": "Originality\n\nWhile the Structured Walk Hypothesis (SWH) innovatively applies geometric decompositions (magnitude and orientation via cos/sin terms) to user preference evolution in summarization, it draws heavily from existing temporal knowledge graph embedding (TKGE) techniques without sufficient differentiation. For instance, the action-conditioned geometric steps in Eq. (3) closely resemble rotational updates in models like RotatE (Sun et al., 2019, cited in the paper), which uses complex rotations to model relational patterns, and more recent TKGE works such as Time-Enhanced Compound Geometric Operations (TECGO; Zhao et al., 2025), which employs compound geometric operations (e.g., rotations and translations) on timestamps and entities for temporal fact prediction. Similarly, the dual memory lanes for reinforcement/suppression echo memory-augmented updates in temporal graph models like HGE (Sadeghian et al., 2023), which uses product spaces (Complex, Split-complex, Dual) to handle temporal heterogeneity. The application to personalized summarization is a novel domain shift, but the core mechanism—action-biased walks on interaction graphs—overlaps with heterogeneous random walk models like HeteEdgeWalk (Liu et al., 2023), which biases edges without meta-paths for recommendation.\n\nQuality\n\nThe empirical validation is comprehensive in scope but lacks depth in handling real-world variability and scalability testing, potentially undermining claims of robustness for long-horizon preferences. For RQ3, while Walk2Pers outperforms LLMs on PerSEval metrics (e.g., 0.452 PSE-JSD on PENS vs. DeepSeek's 0.248), the LLM baselines use only 2-shot prompting with history, ignoring advanced techniques like chain-of-thought or fine-tuning on personalization datasets, which could narrow the gap (as seen in recent TKGE surveys emphasizing hybrid LLM-graph models; Rossi et al., 2023). Cross-domain results on OpenAI-Reddit show gains (+0.19 over best LLM), but the dataset's avg. 39 d-nodes is modest; missing are stress tests on ultra-long trajectories (e.g., 500+ interactions, synthesizable from MIND extensions) to verify dual memory's suppression without catastrophic forgetting. Human-judgment alignment (7/7 rating, RMSD 0.396) is promising but based on limited samples (inferred from Table 5); a full user study with 50+ participants rating summaries for relevance/conciseness would better validate PerSEval's correlation. Additionally, no runtime or parameter efficiency comparisons are provided despite \"lightweight\" claims (O(pd) complexity); suggest adding FLOPs/ms per inference against NRMS/EBNR and LLMs on varying history lengths to quantify efficiency gains.\n\nSignificance\n\nThe work advances personalized summarization but overstates broader implications for \"personalization systems\" without demonstrating extensibility beyond news/Reddit domains, limiting its impact on diverse applications like e-commerce or social media feeds. For instance, while stability on long histories is claimed, evaluations omit noisy real-time scenarios (e.g., interleaved multi-user sessions or evolving topics like in Adressa extensions), where action biases might degrade. The focus on PerSEval (strong human correlation per Dasgupta et al., 2024) is apt, but ignoring standard summarization metrics like ROUGE-1/2/L (even as secondary) misses opportunities to show balanced personalization vs. generic quality trade-offs. Cross-domain robustness is shown (0.19↑ on OpenAI-Reddit), yet PersonalSum-EN relies on M2M-100 translations, potentially introducing artifacts—suggest validating with native Norwegian evaluators or alternative translators like NLLB. To enhance significance, extend to a fourth dataset (e.g., MIND for pure recommendation) and include a pilot on a downstream task like preference-based ranking, quantifying how SWH embeddings transfer to non-summarization personalization."}, "questions": {"value": "Enhancing LLM Baselines with Advanced Prompting or Fine-Tuning: In RQ3 evaluations (Tables 3 and 4), LLMs like DeepSeek-R1-14B are prompted with 2-shot + history, yielding lower PerSEval scores (e.g., 0.248 PSE-JSD on PENS vs. Walk2Pers' 0.452). However, this may not fully test LLM capabilities, as techniques like chain-of-thought prompting or fine-tuning on personalization datasets could improve performance, as noted in TKGE-LLM hybrids (e.g., Ji et al., 2023 survey on temporal KG completion). Could you re-run LLM baselines with CoT or LoRA fine-tuning on a subset of PENS/OpenAI-Reddit, and report updated metrics? This could clarify if the 0.22 average gap holds\n\nStress-Testing on Ultra-Long Trajectories and Noisy Scenarios: The datasets feature moderate history lengths (e.g., avg. 134 d-nodes in PENS, 39 in OpenAI-Reddit), but claims of long-horizon stability (e.g., dual memory suppression) aren't tested on ultra-long sequences (500+ interactions), where catastrophic forgetting might occur. Additionally, real-world noise like interleaved multi-user sessions or rapidly evolving topics (e.g., from Adressa/MIND extensions) is absent. Could you synthesize ultra-long trajectories from MIND and evaluate AUC/MRR (as in Table 2) or PerSEval under noise injection (e.g., 20% random skips)?\n\nDetailing Parameter Learning and Hyperparameter Sensitivity: Components like mag(a(t)), θ(a(t)), and ω(t) are mentioned but not fully explained in terms of optimization (e.g., are they learned via MLPs or fixed?). Appendix H.2 (inferred from text) covers training, but no sensitivity analysis is shown. Could you clarify learning mechanisms and add ablation on key hypers (e.g., α=0.6 in Lenc) via plots? This could clear up methodological ambiguities and strengthen quality if robustness is confirmed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eXIRhlVl0b", "forum": "HvOKarTubb", "replyto": "HvOKarTubb", "signatures": ["ICLR.cc/2026/Conference/Submission14554/Reviewer_HwNK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14554/Reviewer_HwNK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760843854193, "cdate": 1760843854193, "tmdate": 1762924943332, "mdate": 1762924943332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Walk2Pers model, aiming to break the limitation of the Markovian Drifts Hypothesis (MDH) regarding the short-term dependency of user interests.\nIts core innovation lies in viewing personalized summarization as an action-biased geometric walk with memory, which precisely captures long-term, complex preference evolution through geometric steps and dual memory lanes.\nThe main contribution is providing theoretical support and mechanism innovation, and significantly outperforming traditional summarization models and powerful LLM baselines in experiments."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper provides detailed theoretical proofs and discussion, which are exhaustive and rich.\n\n2. The proposed SWH (Structured Walk Hypothesis) in the paper is intriguing, as it challenges the widely prevalent Markovian Drifts Hypothesis in traditional recommendation systems and summarization models."}, "weaknesses": {"value": "1. Section 3.3.1 mentions that T5 is used for embedding, but the paper does not explain why T5 was chosen. It would be better to include an experiment using BERT or other more advanced embedding models.\n2.  When comparing with LLM baselines, the evaluation was performed using 2-shot+history prompting. In fact, due to the limited contextual understanding, it is common for these smaller-parameter models to perform poorly. If this evaluation method is adopted, the LLM baselines for comparison should include large-parameter closed-source models such as GPT-4 and Gemini 2.5 Flash to be more appropriate. If smaller-parameter models are to be compared, their fine-tuned versions should be compared instead."}, "questions": {"value": "As mentioned in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fb7x3hqJlW", "forum": "HvOKarTubb", "replyto": "HvOKarTubb", "signatures": ["ICLR.cc/2026/Conference/Submission14554/Reviewer_5Y5U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14554/Reviewer_5Y5U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726296171, "cdate": 1761726296171, "tmdate": 1762924942817, "mdate": 1762924942817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Walk2Pers, a lightweight encoder-decoder framework for personalized document summarization, which models user preference evolution as an action-conditioned geometric walk with memory. Unlike existing methods that assume short-term or memoryless (Markovian) user behavior, Walk2Pers explicitly represents preference dynamics through two components: magnitude, controlling the strength of preference shifts, and orientation, capturing the balance between continuity and novelty. The model incorporates dual memory lanes to reinforce consistent interests and suppress disinterest, and includes a drift term to handle user-specific summary requests. The authors provide a theoretical justification showing that the proposed structured walk approximates first-order action-conditioned kernels. Empirical results on three benchmark datasets (PENS, OpenAI-Reddit, and PersonalSum) demonstrate that Walk2Pers achieves consistent improvements over both specialized personalized summarizers and strong large language model baselines, as measured by the PerSEval metric. The framework also shows robustness across domains and stability over long interaction histories, suggesting that modeling personalization as a geometric walk with memory provides a more interpretable and efficient approach to personalized summarization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe problem of user preferences evolving over time is very interesting and practical.\n\n2.\tThe paper is overall well-written and easy to follow."}, "weaknesses": {"value": "1.\tI don’t see obvious weakness for this paper."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "u6PKCg2MnB", "forum": "HvOKarTubb", "replyto": "HvOKarTubb", "signatures": ["ICLR.cc/2026/Conference/Submission14554/Reviewer_RW71"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14554/Reviewer_RW71"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727675769, "cdate": 1761727675769, "tmdate": 1762924942255, "mdate": 1762924942255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that previous personalized summarization systems implicitly assume a Markovian Drift Hypothesis (MDH): user preference at step t+1 depends mostly on the latest interaction. However, authors of this work believe that this assumption is too weak for long, action-rich histories. It proposes the Structured Walk Hypothesis (SWH) and instantiates it as Walk2Pers, where user preference evolves as an action-biased geometric walk with memory: each interaction applies (i) an action-conditioned step with magnitude (how strong the shift is) and orientation (continuity vs. novelty), (ii) dual memory lanes to reinforce clicked interests and suppress skipped content, and (iii) a summary-specific drift for summarization actions. On three personalization datasets (PENS, OpenAI-Reddit, PersonalSum), Walk2Pers beats strong MDH-style encoders (NAML/NRMS/EBNR, SMD, AGD), specialized personalized summarizers, and mid-size LLMs prompted with user histories, with especially large gains on PerSEval metrics, supporting SWH over MDH in this task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Well-factorized update rule**: decomposing each step into magnitude, orientation, dual memories, and summary drift makes the dynamics interpretable and extensible. \n\n**Strong empirical gains**: Walk2Pers closes or reverses gaps against both domain models (PENS, GTP, SP) and prompted LLMs, which is nontrivial for personalization. \n\n**Task-aligned evaluation**: using PerSEval (human-correlated) plus next-behavior prediction gives both user-centric and model-centric evidence that Walk2Pers with Structured Walk Hypothesis (SWH) can indeed improve over MDH.\n\n**Meaningful Ablations**: removing geometric steps or dual memories degrades performance, indicating the gains aren’t from just a bigger encoder."}, "weaknesses": {"value": "-- Complexity vs. practicality: the full pipeline (UIG → b-layer → action-gated cells → dual memories → geometric steps → T5-UCA) is quite complex; deployment and latency costs aren’t quantified. \n\n-- Heavy dependence on good interaction logs: the method assumes rich click/skip/summarize traces; it’s unclear how it performs with sparse or noisy histories. \n\n-- Comparisons favor the hypothesis: many baselines are cast as MDH-style and naturally underuse long histories; stronger long-horizon sequence/retrieval baselines would make the claim tighter. \n\n-- Limited analysis of failure modes: the paper shows when SWH helps, but not when geometric novelty or negative memory might over-suppress relevant but infrequent topics."}, "questions": {"value": "-- Can the authors report training/inference cost per summary (tokens, FLOPs, wall-clock) vs. the best 2-shot LLM, to show the benefit is not purely from extra computation? \n\n-- How does Walk2Pers behave if the action distribution is highly imbalanced (many clicks, very few skips, rare summarize)? Does the dual-memory lane degenerate? \n\n-- Could the geometric step (magnitude/orientation) be learned conditionally on document facets (topic, source) to better handle cross-domain shifts in OpenAI-Reddit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nBk7FefRRC", "forum": "HvOKarTubb", "replyto": "HvOKarTubb", "signatures": ["ICLR.cc/2026/Conference/Submission14554/Reviewer_E9Qi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14554/Reviewer_E9Qi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762074364899, "cdate": 1762074364899, "tmdate": 1762924941429, "mdate": 1762924941429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}