{"id": "EQ67QGqorK", "number": 20320, "cdate": 1758304721201, "mdate": 1759896984031, "content": {"title": "Diff-ID: Identity-Consistent Facial Image Generation and Morphing via Diffusion Models", "abstract": "Generative diffusion models have revolutionized facial image synthesis, yet robust identity preservation in high-resolution outputs remains a critical challenge. This issue is especially vital for security systems, biometric authentication, and privacy-sensitive applications, where any drift in identity integrity can undermine trust and functionality. We introduce Diff-ID, a diffusion-based framework that enforces identity consistency while delivering photorealistic quality. Central to our approach is a custom 210K image dataset synthesized from CelebA-HQ, FFHQ, and LAION-Face and captioned via a fine-tuned BLIP model to bolster identity awareness during training. Diff-ID integrates ArcFace and CLIP embeddings through a dual cross-attention adapter within a fine-tuned Stable Diffusion U-Net. To further reinforce identity fidelity, we propose a pseudo-discriminator loss based on ArcFace cosine similarity with exponential timestep weighting. Experiments on held-out and unseen faces demonstrate that Diff-ID outperforms state-of-the-art methods in both identity retention and visual realism. Finally, we showcase a unified DDIM-based morphing pipeline that enables seamless facial interpolation without per-identity fine-tuning.", "tldr": "", "keywords": ["Diffusion Models", "Identity Preservation", "Adapters", "CLIP", "Arcface", "Generative Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7dda274ed3354b684d74ab9f296e49bc25e9d8a4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a Diff-ID that fuses ArcFace embedding and CLIP embeddings through a dual cross-attention adapter to fine-tune Stable Diffusion U-Net to generate ID-preserving and realism face images."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The goal of improving visual realism of face generation is interested to security-sensitive applications."}, "weaknesses": {"value": "1. Authors stated that the proposed method Diff-ID can\nretain fine-grained attribute control. However, there is no experiment to support this claim.\n\n2. There is no detailed description about what is evaluation dataset used to compute FS and FID scores.\n\n3. Visual realism is not properly evaluated. Authors should measure SSIM between generated and real images.\nOr using face anti-spoofing (FAS) network to evaluate if generated faces are realistic enough to pass the FAS.\n\n4. The presentation of this work should be improved."}, "questions": {"value": "1. What is Face Similarity that used to compute FIQ score?\n2. What types and how many images are used for evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L789baKHMN", "forum": "EQ67QGqorK", "replyto": "EQ67QGqorK", "signatures": ["ICLR.cc/2026/Conference/Submission20320/Reviewer_M4ke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20320/Reviewer_M4ke"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768467434, "cdate": 1761768467434, "tmdate": 1762933782275, "mdate": 1762933782275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes Diff-ID, a diffusion-based framework for high-fidelity face synthesis with enforced identity consistency. It assembles a custom 210K-image dataset from CelebA-HQ, FFHQ, and LAION-Face, captioned by a fine-tuned BLIP model to enhance identity supervision. The core design integrates ArcFace and CLIP embeddings via a dual cross-attention adapter within a fine-tuned Stable Diffusion 1.5 U‑Net, and applies a pseudo-discriminator loss built on ArcFace cosine similarity with exponential timestep weighting. Experiments on held-out and unseen identities report improvements in identity retention and visual realism over prior art, and the method introduces a unified DDIM-based morphing pipeline for seamless face interpolation without per-identity fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tLeverages a sizable and purpose-built captioned corpus (210K samples) with identity-aware supervision via BLIP fine-tuning, which can help disambiguate semantic and identity cues.\n2.\tBy extracting face information using both CLIP and ArcFace in parallel, the method attains complementary representations."}, "weaknesses": {"value": "1.\tThe manuscript would benefit from clearer exposition of its main contributions; stronger writing will help convey the novelty and impact.\n2.\tThe manuscript would benefit from a clearer articulation of its novel contributions, as both the feature fusion and ID loss follow established designs.\n3.\tThe experimental motivation is unclear, the writing is not well-articulated, and critical details are lacking. For instance, the paper fails to specify the concrete role of prompts in the experiments, leaving it impossible to understand the purpose of the experimental design. Take Figure 4’s hyperparameter selection experiment as an example: why does the face size vary? Moreover, in nearly all experiments, the background of the faces is not kept consistent—what is the reason for this? Is it related to the prompts? Additionally, when conducting qualitative experiments, did the authors consider explicitly defining the meaning of prompts to ensure clarity?\n4.\tGiven that both the face and the prompt reside in the CLIP shared embedding space, it is unclear why the authors do not project e_arc directly into this space, rather than performing multiple complex alignment and fusion operations that may incur additional information loss. In addition, the Adapter Module shown in the lower half of Figure 2 seems inconsistent with the main text’s description.\n5.\tThe FIQ Score is intended to be reported as a percentage, yet it can take values greater than 100%, making this design problematic.\n6.\tIn most experiments, the proposed method achieves SOTA on only two metrics. A closer examination reveals that FIQ was manually designed by the authors. Its superior performance over competing methods stems from adopting a diffusion model architecture, which lowers FID but inflates FS; this direction is clearly at odds with the paper’s stated research objective of IDENTITY-CONSISTENT evaluation.\n7.\tOn line 422, there is only a single closing parenthesis at the end.\n8.\tIn the ablation study, the DIFFID-Joint setup lacks clear exposition."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JxipUYopPj", "forum": "EQ67QGqorK", "replyto": "EQ67QGqorK", "signatures": ["ICLR.cc/2026/Conference/Submission20320/Reviewer_fTom"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20320/Reviewer_fTom"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832634049, "cdate": 1761832634049, "tmdate": 1762933781797, "mdate": 1762933781797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of identity drift in diffusion-based facial image synthesis—where existing methods often sacrifice fine-grained identity features (e.g., bone structure, eye spacing) when pursuing photorealism or attribute edits. The authors propose Diff-ID, a unified diffusion framework built on Stable Diffusion 1.5, designed to enforce identity consistency while maintaining visual realism. Diff-ID’s core components include a 210K identity-centric dataset with captions generated by a fine-tuned BLIP model to enhance identity awareness during training, a dual cross-attention adapter and a DDIM-based morphing pipeline. Extensive experiments on held-out and unseen datasets show Diff-ID outperforms SOTA methods (e.g., InstantID, Arc2Face) in the proposed FIQ Score (a combined metric of ArcFace similarity and FID), achieving 71.74 (validation) and 69.32 (unseen data)—demonstrating its ability to balance identity fidelity and photorealism."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Unlike prior work relying solely on CLIP (e.g., IPAdapter) or ArcFace (e.g., Arc2Face), Diff-ID’s dual cross-attention adapter leverages the complementary strengths of both embeddings:\n- The DDIM-based morphing approach avoids the limitations of GAN-based methods and other diffusion methods. By combining latent inversion with spherical interpolation, it produces smooth, photorealistic transitions.\n- The paper introduces the FIQ Score, a unified metric that integrates identity similarity and visual realism. Experiments cover both qualitative and quantitative evaluations, with comparisons to 4 SOTA baselines, ensuring results are rigorous and interpretable."}, "weaknesses": {"value": "- Diff-ID is optimized for frontal, unoccluded faces but lacks evaluation on challenging cases critical for real-world use:\n   - Extreme poses: Side profiles, tilted heads, or full-profile faces (where identity features like jawline or nose shape are partially obscured) may cause identity drift, as the model is not trained to disentangle pose from identity.\n   - Severe occlusions: Sunglasses, masks, or hair covering key facial regions (eyes, nose) could break ArcFace embedding extraction, leading to incorrect identity weighting in the adapter.\n\n- The paper’s core modifications to the diffusion pipeline risk being perceived as incremental rather than transformative. While Diff-ID addresses identity drift—a critical gap—its design builds heavily on existing diffusion paradigms (Stable Diffusion 1.5 backbone) with limited architectural novelty. The dual cross-attention adapter, though effective, is a \"fusion module\" added to the pre-trained U-Net rather than a redesign of the diffusion process itself. Prior work (e.g., InstantID’s embedding concatenation, IPAdapter’s decoupled attention) has already explored embedding integration in diffusion; Diff-ID’s innovation lies in combining ArcFace and CLIP via cross-attention, but this is a refinement rather than a paradigm shift.\n\n- Diff-ID is exclusively built on and validated for U-Net-based diffusion models (Stable Diffusion 1.5), with no exploration of DiT (Diffusion Transformer)-based architectures—a current mainstream in high-fidelity generative modeling (e.g., DiT-XL/2, Flux.1 variants). This limitation raises critical concerns. DiT’s token-wise attention and transformer backbone differ fundamentally from U-Net’s spatial-wise attention and encoder-decoder structure. Diff-ID’s dual cross-attention adapter (designed for U-Net’s spatial feature interaction) may not transfer to DiT’s token-based feature modeling. The paper does not discuss whether DiT’s inherent advantages (e.g., better long-range feature dependency, higher resolution support) could enhance identity consistency, nor does it explain why U-Net was chosen over DiT as the backbone. Given DiT’s growing adoption in industrial and academic settings, this gap limits Diff-ID’s applicability to cutting-edge diffusion models."}, "questions": {"value": "- How does Diff-ID’s design go beyond incremental improvements to existing diffusion-based identity preservation methods (e.g., InstantID’s embedding concatenation)? For example, is there a unique interaction between the dual cross-attention adapter and timestep-weighted loss that redefines how identity is modeled in diffusion, rather than just combining existing components?\n\n- Have you conducted preliminary experiments or theoretical analysis on adapting Diff-ID to DiT-based architectures? If so, what modifications were required, and how did performance compare to the U-Net backbone? If not, do you anticipate fundamental barriers to adapting Diff-ID to DiT, or could the framework be extended with minimal changes?\n\n- Have you tested Diff-ID on extreme poses, severe occlusions, or low-resolution inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3C84s4wpXE", "forum": "EQ67QGqorK", "replyto": "EQ67QGqorK", "signatures": ["ICLR.cc/2026/Conference/Submission20320/Reviewer_BX16"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20320/Reviewer_BX16"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903422287, "cdate": 1761903422287, "tmdate": 1762933781466, "mdate": 1762933781466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a diffusion-based framework for facial image generation and morphing that aims to maintain identity consistency in generated data. It integrates ArcFace embeddings (for identity cues) and CLIP embeddings (for semantic content) using a dual cross-attention adapter. The model also includes a pseudo-discriminator identity loss, weighted by diffusion timestep.\nthe methods is evaluated on multiple face datasets and compared to some of the recent identity-aware diffusion models such as InstantID, PhotoMaker, and Arc2Face. The authors claim improved trade-offs between identity retention and visual realism. Additionally, Diff-ID offers a morphing pipeline that blends two identities through DDIM inversion and embedding interpolation without per-subject fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive dataset curation and reasonable reproducibility commitment.\n- Clear architectural visualization and systematic ablation studies."}, "weaknesses": {"value": "- Incremental novelty as the dual cross-attention adapter is effectively a variant of IP-Adapter with an added ArcFace stream. No fundamental architectural or theoretical advance is presented.\n- Misplaced use of CLIP embeddings for morphing as Morphing only requires two embeddings. CLIP might add redundant semantic context unrelated to identity interpolation and complicates the pipeline without demonstrated benefit.\n- Lack of ablation proving CLIP’s necessity as no results show performance with ArcFace-only morphing, despite claiming that CLIP aids identity fidelity.\n- The DDIM-based morphing procedure (latent + embedding interpolation) is already standard in DiffMorpher and DreamBooth-based pipelines. among others.\n- the morphing success is not evaluated and compared accoridng to recent works.\n- Identity metrics (ArcFace similarity) may favor ArcFace-based training, inflating perceived improvements.\n- The argument for “identity consistency” lacks theoretical groundin as there is no definition of identity stability under diffusion noise or justification for exponential weighting beyond empirical tuning. needs clarification."}, "questions": {"value": "- Why is CLIP needed for morphing between two identities when ArcFace embeddings already provide identity representations?\n- How does Diff-ID differ technically from IP-Adapter or InstantID beyond adding ArcFace embeddings and a fusion MLP?\n- is there any quantitative evidence that the CLIP branch improves identity fidelity rather than introducing stylistic bias?\n- Did the authors test morphing using only ArcFace embeddings? What was the performance difference?\n-  Why is morphing not evaluated with standard FR vulnerability metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jGLltKV73M", "forum": "EQ67QGqorK", "replyto": "EQ67QGqorK", "signatures": ["ICLR.cc/2026/Conference/Submission20320/Reviewer_KGxo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20320/Reviewer_KGxo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179994061, "cdate": 1762179994061, "tmdate": 1762933781177, "mdate": 1762933781177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}