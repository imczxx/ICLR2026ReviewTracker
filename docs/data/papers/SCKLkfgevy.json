{"id": "SCKLkfgevy", "number": 793, "cdate": 1756818342036, "mdate": 1763732631490, "content": {"title": "BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models", "abstract": "This work investigates descriptive captions as an additional source of supervision for biological multimodal foundation models. Images and captions can be viewed as complementary samples from the latent morphospace of a species, each capturing certain biological traits. Incorporating captions during training encourages alignment with this shared latent structure, emphasizing potentially diagnostic characters while suppressing spurious correlations. The main challenge, however, lies in obtaining faithful, instance-specific captions at scale. This requirement has limited the utilization of natural language supervision in organismal biology compared with many other scientific domains. We complement this gap by generating synthetic captions with multimodal large language models (MLLMs), guided by Wikipedia-derived visual information and taxon-tailored format examples. These domain-specific contexts help reduce hallucination and yield accurate, instance-based descriptive captions. Using these captions, we train BioCAP (i.e., BioCLIP with Captions), a biological foundation model that captures rich semantics and achieves strong performance in species classification and text-image retrieval. These results demonstrate the value of descriptive captions beyond labels in bridging biological images with multimodal foundation models", "tldr": "", "keywords": ["AI for biology", "foundation models", "synthetic captions"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4269e029ea0410b33fa017fd2770fad15bfc5026.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes BioCAP, a biological multimodal model that leverages descriptive captions as an additional supervision signal alongside species labels. The authors argue that images and captions provide complementary views of a species’ latent morphospace, capturing biologically meaningful traits while suppressing spurious correlations. To overcome the scarcity of instance-level captions, they generate synthetic captions using a large multimodal language model (InternVL3 38B) guided by Wikipedia-derived visual information and taxon-tailored format examples. BioCAP is trained with a shared visual and text encoder, but uses dual visual projectors for taxonomic labels and captions. Experiments show BioCAP improves species classification and text–image retrieval with several baselines, while careful ablation studies demonstrate the benefits of their approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel approach**: The use of synthetic descriptive captions for biological multimodal models is innovative and addresses a clear bottleneck in organismal biology.\n- **Strong empirical performance**: BioCAP outperforms multiple baselines (CLIP, SigLIP, BioTrove-CLIP, BioCLIP, FG-CLIP) on ten species classification benchmarks, retrieval tasks (PlantID, Cornell Bird), and natural language understanding (INQUIRE-Rerank).\n- **Comprehensive evaluation**: The authors evaluate classification, retrieval, and language understanding, covering multiple organismal domains.\n- **Human evaluation and careful ablation studies**: Caption quality is assessed using four metrics (groundedness, specificity, completeness, and clarity) showing the effectiveness of synthetic captions and addressing hallucination concerns. Ablations confirm that dual projectors outperform a single projector, and that adding synthetic captions improves generalization, even for species without Wikipedia coverage."}, "weaknesses": {"value": "- **Choice of base model and baselines**: BioCAP is initialized from BIOCLIP, but the paper does not justify why **BIOCLIP 2**, the current state-of-the-art, was not used as the base model or included as a baseline, despite being cited in the experimentation section. Furthermore, there is no discussion of alternative multimodal models (e.g., LLaVA) or rationale for selecting BIOCLIP.\n- **Evaluation scope and claims**: Evaluation is limited to zero-shot species classification and a subset of retrieval benchmarks; prior work includes few-shot tasks and additional biological visual datasets (FishNet, NeWT, AwA2, Herb. 19, PlantDoc). \n- **Coverage limitations**: While ablation studies indicate improvements even for non-covered species, the paper does not explicitly analyze performance specifically on underrepresented species, leaving it unclear whether the approach benefits the full long-tail of species. \n- **Trait diversity in curated examples**: Only up to three examples per taxonomic class may bias the model toward common traits, potentially underrepresenting rare or atypical characteristics. An ablation study on different number of curated examples would be a nice addition.\n- **Propagation of synthetic caption errors**: While human evaluation supports caption quality, errors or omissions in synthetic captions could still affect model performance for species with limited or no reliable descriptions."}, "questions": {"value": "**Questions for the Authors**\n\n1. Why was BIOCLIP 2, the current state-of-the-art biological multimodal model, not used as the base model or included as a baseline?\n2. Have the authors evaluated performance specifically on underrepresented or rare species to assess generalization across the long tail of biodiversity?\n3. How sensitive is the model to the number and diversity of curated examples per taxonomic class? Would increasing this number improve generalization or trait coverage?\n4. Are there analyses on how synthetic caption errors propagate through the training process, especially for species with limited textual coverage?\n\n**Actionable Feedback**\n\n1. Compare BioCAP against BIOCLIP 2, the current state-of-the-art, and provide justification for choosing BIOCLIP as the base model. Discussion of alternative multimodal foundation models (e.g., LLaVA) would also strengthen the paper.\n2. Include few-shot classification, additional biological visual benchmarks (FishNet, Newt-AWA2, Herb-19, PlantDoc), and more diverse retrieval tasks. Explicit analysis of underrepresented or rare species would help assess generalization.  \n3. Assess whether limiting to three examples per taxonomic class biases the model, and consider ablations with more examples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bbGGmKTBRH", "forum": "SCKLkfgevy", "replyto": "SCKLkfgevy", "signatures": ["ICLR.cc/2026/Conference/Submission793/Reviewer_qs4b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission793/Reviewer_qs4b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663628680, "cdate": 1761663628680, "tmdate": 1762915606209, "mdate": 1762915606209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors create synthetic instance-level captions of images from the TreeOfLife-10M dataset and use them to train a version of BioCLIP with a caption encoder. The captions are morphological trait-based descriptions of the animals that correspond to subregions in the images. The authors articulate their synthetic caption generation strategy, report benchmarks on classification and retrieval, and perform a detailed ablation study. The results indicate that the trait captions improve classification and help constrain model attention to animal parts that help with classification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: the authors use an interesting approach to caption generation, prompting MLLMs with format specification on a per-class basis. The idea was to constrain output to focus on salient morphological description that can be challenging to pick out of raw text without guidance. The approach to leveraging the captions in BioCLIP is appealingly simple. \n- Quality: The overall qualtiy is quite good, lots of experiments executed with sufficient data. The ablation studies do a nice job of illustrating the benefits of adding these captions to the model. \n- Clarity: The paper is well-written and relatively easy to follow. There are a few sections (noted below) that could benefit from some rewriting. \n- Significance: This work provides more evidence that species classification is easier when you can direct the model to pay more attention to informative parts of the target. The use of captions, and the way in which they were generated, is a useful addition to the toolkit."}, "weaknesses": {"value": "The format example design discussion needs expansion. This seems to be a critical element of the work and it is treated very narrowly. It isn't clear what the 'classes' are that were used to query Gemini Deep Research, who did the winnowing of the results, or how consistent the examples were across the classes. This element may itself benefit from an exploration of how variable those exemplars are between model runs and how consistent the human overseers where in selecting appropriate descriptions."}, "questions": {"value": "- What is the taxonomic breakdown of the resulting descriptions? At line 224 you indicate the descriptions cover ~32% of the species in TreeOfLife-10M. Are there any biases in how the generated captions or is it uniformly distributed across organism groups?\n- What are the limitations of the caption generation strategy? Is the 32% coverage a fundamental limit of what is available on Wikipedia? \n- Are there any potential issues when mapping genus level descriptions onto species? Does that result in multiple species having the same descriptions?\n- Can you elaborate on the human evaluation task? Per the supplement, there was only one domain expert in the group of annotators and 15 computer science students. How well prepared are your human validators to assess the quality of the captions? What is the ecologist's area of expertise? \n- What is the overlap in the species coverage in the zero-shot datasets? How much of the performance improvement is related to prior knowledge of the organism groups being tested?\n- At line 230, it is noted that there are 347 taxonomic classes in TreeOfLife-10M. Is that a typo? Or are classes in this case the level of the taxonomic tree?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mn9g8FL7gV", "forum": "SCKLkfgevy", "replyto": "SCKLkfgevy", "signatures": ["ICLR.cc/2026/Conference/Submission793/Reviewer_uX5T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission793/Reviewer_uX5T"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927099843, "cdate": 1761927099843, "tmdate": 1762915606027, "mdate": 1762915606027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work present a biological multimodal foundation model that integrates synthetic, descriptive captions as additional supervision beyond taxonomic labels to improve species understanding. The authors propose to generate synthetic instance-level captions using multimodal LLMs guided by Wikipedia-derived visual information and taxon-specific format exam, then train a model with dual text views(species name + caption, )and dual visual projectors to decouple supervision. The results shown more than 20% improvements across 10 classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Multimodal alignment in biology is an under-explored and important task.\n- The paper propose an interesting idea that images and captions are treated as complementary projections of a species’ latent morphospace, so aligning them helps capture diagnostic traits while suppressing noise.\n- The paper introduced a dual-projector architecture elegantly separates taxonomy vs. caption supervision, and use context-guided caption generation (Wikipedia + format examples) effectively mitigates LLM hallucination.\n- The evaluation is comprehensive, including multiple benchmarks, both quantitative and qualitative analysis."}, "weaknesses": {"value": "- The caption reliance on Wikipedia-derived descriptors could reinforce taxonomic bias and exclude rare or poorly documented species. How does the framework handle species without any Wikipedia entry or minimal trait descriptions?\n- The repeated LLM re-generation could produce inconsistent style or attribute focus across species, causing the semantic drift.\n- Potential scalable issue since the caption generation and derail-view training are computational costly."}, "questions": {"value": "- Is there any caption noise? how did you filter those?\n- What effort is needed to extend the framework to video data?\n- The method is specifically designed for biology domain. How is the generalization capability to other scientific domains?\n- Is there any quantitative result for Grad-CAM?\n- Who are the human annotator? are they biology expert?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wwh29aVMsF", "forum": "SCKLkfgevy", "replyto": "SCKLkfgevy", "signatures": ["ICLR.cc/2026/Conference/Submission793/Reviewer_KyEW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission793/Reviewer_KyEW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960483764, "cdate": 1761960483764, "tmdate": 1762915605857, "mdate": 1762915605857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a BIOCLIP-style model that, during training, aligns images with taxonomic labels and instance-specific synthetic captions generated by an MLLM. The captions are constrained by Wikipedia-derived visual descriptions and taxon-tailored format examples to reduce hallucinations. The evaluation show strong performance for species classification and text-image retrieval by training on TreeOfLife-10: +8.8% average top-1 over BIOCLIP across 10 zero-shot species classification sets, and +21.3% average on natural-language tasks/retrieval. Ablations show that human eval favors the proposed model's captions over others. \nThe qualitative analysis with Grad-CAM images and t-SNE embeddings further highlight the improved embeddings with BioCAP."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes interesting tweaks like using images and captions as complementary views of a species’ latent trait vector and training with contrastive learning to emphasize diagnostic features. The dual projector cleanly separates heterogeneous supervision as compared to a single projector. \n\nThe paper will be of reasonable interest for the broader community interested in training VLMs for life sciences."}, "weaknesses": {"value": "Captions are biased toward the chosen InternVL3-38B; and there is no cross-MLLM comparison. \nBehavior labels for analysis are auto-assigned by GPT-4o that may cause label drift. \nLarge deltas are shown in experimental results, but statistical significance/confidence intervals aren’t reported for benchmarks."}, "questions": {"value": "can caption-aligned features also help zero-shot retrieval/classification?\nHow do results change with different captioners?\nWhat is the dedup protocol used? How do you ensure held-out test set?\nAre there any cases of observed hallucination in the final captions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YZRg6yYVRe", "forum": "SCKLkfgevy", "replyto": "SCKLkfgevy", "signatures": ["ICLR.cc/2026/Conference/Submission793/Reviewer_XfBu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission793/Reviewer_XfBu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036286432, "cdate": 1762036286432, "tmdate": 1762915605387, "mdate": 1762915605387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}