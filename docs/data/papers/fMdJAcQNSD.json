{"id": "fMdJAcQNSD", "number": 12420, "cdate": 1758207668296, "mdate": 1759897511013, "content": {"title": "Dirichlet-Prior Shaping: Guiding Expert Specialization in Upcycled MoEs", "abstract": "Upcycling pre-trained dense models into sparse Mixture-of-Experts (MoEs) efficiently increases model capacity but often suffers from poor expert specialization due to naive weight replication. Our analysis reveals that upcycled MoEs, even with conventional regularization, exhibit low-confidence, weakly differentiated routing, hindering performance. We introduce Dirichlet-Prior Shaping Loss (DPSL), a novel router regularization technique that directly shapes routing probability distributions by matching expert assignments to a target Dirichlet prior. DPSL offers fine-grained control over expert balance and specialization, and enables encoding of inductive biases such as encouraging experts to focus on specific modalities or tasks, without requiring manual intervention; notably, DPSL is a general tool applicable to any module that outputs categorical probability distributions, extending its utility beyond MoE training. Experiments on upcycled MoE vision-language models (with Qwen2, Phi3, Llama3.2 LLM backbones) show DPSL consistently outperforms upcycling strategies and regularization techniques across standard vision-language benchmarks, addressing the critical issue of poor specialization and fostering more adaptive, higher-performing models.", "tldr": "", "keywords": ["Mixture of Experts", "Upcycling", "MLLM", "VLM", "MoE training", "Specialization", "Prior"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f12ded1e117899a021616594f095f21373616bef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new regularization loss for Upcycled MoE training to address the issue of uniform routing weights. Specifically, the DPSL regularizes the distance between the empirical CDF and a target Beta CDF for each expert category."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear, and I believe that the distribution of routing probabilities is a critical issue in upcycled MoE models.  \n2. The proposed method aligns well with this motivation, and performance improvements are observed in several scenarios."}, "weaknesses": {"value": "1. It is unclear why shaping the routing probability distribution to follow a Dirichlet distribution is the optimal choice. Could the authors provide stronger theoretical or empirical justification for this design decision? Additionally, the reported performance gains appear marginal in most cases, which raises questions about the practical significance of the proposed approach.\n\n2. According to Table 8, DPSL achieves better load balancing than the standard load-balancing loss. Could the authors clarify the underlying reason for this improvement? Furthermore, would this advantage persist in Modality-Specific and Task-Specific settings? For a more comprehensive evaluation, it would also be helpful to report training and inference times.\n\n3. While the hyperparameter $\\alpha$ has a substantial impact on the logit distribution, Tables 4 and 5 suggest that identifying an optimal setting is non-trivial. This may limit the practical applicability of the method in real-world scenarios where extensive tuning is often infeasible."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Y8TbuHintb", "forum": "fMdJAcQNSD", "replyto": "fMdJAcQNSD", "signatures": ["ICLR.cc/2026/Conference/Submission12420/Reviewer_ZHna"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12420/Reviewer_ZHna"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739791012, "cdate": 1761739791012, "tmdate": 1762923311404, "mdate": 1762923311404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of poor expert specialization in Mixture-of-Experts (MoE) models created by upcycling pre-trained dense models. The authors argue that naive weight replication leads to homogeneous experts and low-confidence routing, which conventional regularization methods like load-balancing or z-loss fail to adequately resolve. To this end, they introduce Dirichlet-Prior Shaping Loss (DPSL), a novel regularization technique that directly shapes the router's output probability distribution. DPSL matches the empirical cumulative distribution function (CDF) of expert assignment probabilities to a target Beta distribution, which is the marginal of a chosen Dirichlet prior. This approach allows for fine-grained control over expert utilization and specialization. The authors demonstrate that symmetric priors can enforce confident and balanced routing, while asymmetric priors can be used to instill inductive biases, such as encouraging experts to specialize in specific modalities (e.g., vision vs. language) in Vision-Language Models (VLMs). Through extensive experiments on upcycled VLM MoEs with various backbones (Qwen2, Phi3, Llama3.2), the paper shows that DPSL consistently outperforms existing upcycling and regularization techniques on a suite of standard vision-language benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- DPSL is a clean approach that provides fine-grained control over router behavior. It moves beyond simple heuristics like load balancing and allows practitioners to instill complex and desirable statistical properties into the routing mechanism.\n- The authors have conducted an extensive set of experiments across three different modern LLM backbones, two MoE configurations, and six benchmarks.\n- The analysis of router output distributions (e.g., Figure 3 and Appendix C.5) provides valuable qualitative insight into why DPSL works better than alternatives, by encouraging more confident and diverse routing assignments."}, "weaknesses": {"value": "- The paper focuses exclusively on upcycled VLMs. While the authors claim DPSL is a general tool, they provide no evidence of its efficacy for training MoEs from scratch or in other domains like language-only models. Demonstrating its utility beyond the upcycling VLM setting would significantly strengthen the claims of generality.\n\n- The paper reports a 10-15% computational overhead during training, which is not negligible. The mitigation strategy of only applying it during a warm-up phase is reasonable, but a more detailed analysis of the impact on total training time would be welcome. Furthermore, DPSL introduces new hyperparameters (α and λ), and the ablation study shows that different model backbones prefer different settings (e.g., α=0.75 for Llama3.2 vs. α=1.0 for Qwen2). This suggests that some degree of architecture-specific tuning may be required, which could be a practical drawback."}, "questions": {"value": "- The lack of multiple seeds is a concern. Could you provide results for at least one model configuration (e.g., Llama3.2-1B 2in4) across 3-4 seeds for your method and the top-performing baselines (e.g., DeepSeek balancing, Drop-Upcycling)?\n- The results in Table 2 show that explicitly guiding specialization toward pre-defined task subsets (both with BTX and DPSL) underperforms the general-purpose symmetric DPSL. The paper speculates this is due to \"nonoptimal data subsets.\" An alternative hypothesis is that enforcing hard, fine-grained specialization is simply the wrong inductive bias for these multi-faceted VLM tasks. Could you comment on this alternative interpretation? Does this result suggest a potential limitation of highly specialized expert priors?\n- Could you discuss the expected behavior and potential challenges of applying DPSL to training MoEs from scratch? In that setting, the router and experts co-evolve from a random initialization. Would you expect DPSL to be more or less effective compared to its application in the upcycling setting where the router's main job is to break symmetry?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wVJA7z5Djt", "forum": "fMdJAcQNSD", "replyto": "fMdJAcQNSD", "signatures": ["ICLR.cc/2026/Conference/Submission12420/Reviewer_Tjkt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12420/Reviewer_Tjkt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837576766, "cdate": 1761837576766, "tmdate": 1762923311076, "mdate": 1762923311076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The article proposes a novel router regularization technique, Dirichlet-Prior Shaping Loss (DPSL), to address the issue of insufficient specialization among experts in sparse mixture-of-experts (MoE) models. The paper analyzes the routing problems of low confidence and weak discrimination in conventional upcycled MoEs. To improve this, the authors propose matching the routing probability distribution with the target Dirichlet prior, flexibly controlling the balance and specialization level of experts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method allows flexible control over the trade-off between expert balance and specialization by aligning router outputs with a tunable Dirichlet prior.\n\n- Experimental results demonstrate its strong performance.\n\n- The presentation is clear and easy to understand."}, "weaknesses": {"value": "- The DPSL method shows good performance on various benchmarks but relies on selecting the appropriate Dirichlet prior (e.g., α=0.75 being more effective for Llama3.2-1B). This dependence may increase the hyperparameter tuning effort during training. It is recommended to further analyze the parameter selection process to help readers understand the rationale behind the choices.\n\n- The authors mention in the abstract that DPSL is also a general-purpose tool. It is recommended to further elaborate on the potential applications of DPSL in other areas in the main text to demonstrate its broad applicability."}, "questions": {"value": "- Consistency of terminology, it is recommended to unify the use of either \"asymmetric priors\" or \"non-symmetric priors\" to ensure consistency and accuracy in terminology.\n\n- In Table 1, there is an inaccuracy in the labeling of the second-best data; please correct it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q9Vc3JrVZS", "forum": "fMdJAcQNSD", "replyto": "fMdJAcQNSD", "signatures": ["ICLR.cc/2026/Conference/Submission12420/Reviewer_6fqQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12420/Reviewer_6fqQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915206715, "cdate": 1761915206715, "tmdate": 1762923310759, "mdate": 1762923310759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of poor expert specialization with upcycled MoE models, where pre-trained dense models are converted to (sparse) MoEs. The authors propose the use of Dirichlet priors in order to regularize router probability distributions, matching expert assignments to target Dirichlet priors. The method shows good results in providing fine-grained control over expert balance/specialization, with experiments on popular vision language models that demonstrate improvements over baseline approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the challenge adressed is well defined, and explained intuitively through visualizations\n\n- the use of Dirichlet priors to shape routing distributions is well motivated \n\n- the framework can be more general than the presented application of MoE routing (e.g. in cases where modules output categorical probability distributions). Symmetric and asymmetric priors are both integrated in the framework\n\n- evaluation spans multiple backbones and MoE configurations, indicating broad applicability\n\n- ablation studies support the strategies employed, and provide insights into the method's behavior"}, "weaknesses": {"value": "- While the application to MoE routing is novel, the methodological innovation is incremental. The paper could benefit from discussing fundamental differences from related work.\n\n- while improvements are consistent, absolute improvements are often small (e.g., even less than 1% in some cases), which makes the improvement questionable at times given the additional computational overhead\n\n- although the framework can be more generally applicable, this is not evidenced in the paper\n\n- The paper could benefit from more in depth analysis; e.g. specialization evidence (Sec 3.6), it is not clearly demonstrated that experts actually learn different functions, and there is no analysis of what each expert specializes in. \n\n-In Table 2, modality-specific experiments are shown that demonstrate task-specific specialization that may sometimes underperform symmetric priors which could be seen as contradicting the specialization narrative."}, "questions": {"value": "please see above. Additionally,\n- more evidence for expert specialization - are experts really learning different functions? (E.g. via activations)\n- how does this approach compare to simpler approaches, e.g. entropy regularization\n- When should asymmetric priors be used? Given task-specific priors underperform e.g. in Table 2\n- How stable is the method with respect to parametrization? can some settings lead to collapse?\n- Scaling/computational overhead when increasing number of experts?\n- Make sure to describe well how Avg score in Table 1 is computed"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h1wfEQ74TN", "forum": "fMdJAcQNSD", "replyto": "fMdJAcQNSD", "signatures": ["ICLR.cc/2026/Conference/Submission12420/Reviewer_2RyL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12420/Reviewer_2RyL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189347270, "cdate": 1762189347270, "tmdate": 1762923310347, "mdate": 1762923310347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}