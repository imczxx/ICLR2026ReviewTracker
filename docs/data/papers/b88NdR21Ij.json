{"id": "b88NdR21Ij", "number": 2096, "cdate": 1756987762233, "mdate": 1759898169795, "content": {"title": "PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval", "abstract": "Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.", "tldr": "We introduce an embedding approach using all layers of an LLM, leading to an efficient and powerful framework for retrieval.", "keywords": ["multimodal", "retrieval", "VLM", "composed video retrieval"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1aac65af540d380a41b9edfb4abeaf4a03068d5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors study the problem of composed video retrieval (CoVR), claiming that the existing CoVR methods are limited in many ways: outdated architectures, computational limitations, slow caption generation. PREGEN is proposed to address the current limitations, where fine-tuning, caption generation is not required. More specifically, PREGEN relies on pre-trained VLM with lightweight encoding, both of which are kept frozen. One of the key points is fully utilizing each layer, where previous work has been using the final token in the last layer only. When training the model, hard negative mining strategies have been implemented for further improvements. The experimental results highlight the efficacy of PREGEN."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem formulation is clear. The limitations in existing studies have been well discussed, and PREGEN is proposed to address those limitations. \n- Experiments are well designed to support the claims. The authors show how using every layer boost the performances. Hard negative mining also brings additional improvements which has been explicitly shown in Table 3. \n- PREGEN achieves near 100% already in R@1."}, "weaknesses": {"value": "- The main idea of using every VLM layer is somewhat simple, which can be easily thought of/ or easily tried as the first attempt. Specifically, PREGEN with avg. over encoder outputs already achieves significant improvements. While this is a nice finding, it also shows that the proposed scheme does not require technical efforts (less technical challenge). It is also surprising that 1 layer PREGEN performs poorly compared to previous approaches with 1 layer, which raises many questions. \n- While the authors utilizes VLM, the scope of the study is limited only focusing on CoVR. When compared to UNITE which is the VLM-based model, the contribution of PREGEN is limited. Besides, UNITE is not specifically designed for CoVR, yet it performs impressively utilizing only the final layer. The authors can provide further discussions on this. \n- The authors never mentioned the computational limitations that might be cased by the full-ayer computations in VLM, which is layer connected to Transformer Encoder. Does this still beat previous models with *slow caption generation* methods in speed ?"}, "questions": {"value": "Q1. How efficient is PREGEN (full) when compared to PREGEN (1 layer) ? \n\nQ2. Overall, is there any trade-off between the performance and computational complexity from using the full layers in PREGEN? \n\nQ3. Is there any reason why PREGEN underperforms when using only 1 layer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f2lRSOQGLX", "forum": "b88NdR21Ij", "replyto": "b88NdR21Ij", "signatures": ["ICLR.cc/2026/Conference/Submission2096/Reviewer_7LY5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2096/Reviewer_7LY5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757182914, "cdate": 1761757182914, "tmdate": 1762916021178, "mdate": 1762916021178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focus on the task of composed video retrieval. PREGEN extracts and aggregates hidden states from all layers of a frozen vision-language model (VLM) to produce semantically rich video–text embeddings. Experiments on WebVid-CoVR, FineCVR, and Dense WebVid-CoVR show that PREGEN achieves strong performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, The motivation behind their design is well explained.\n2, Demonstrates strong performance.\n3, Solution is easy and makes sense to work."}, "weaknesses": {"value": "1, The multi-layer pooling to bring rich feature is intuitive, thus brings less novelty.\n2, Overclaim of no-training or finetuning. l85 is misleading.The aggregated feature is then projected by training.\n3, The generalisation to ood setup is not clear. Section 4.4 carry experiment on WebVid dataset. seems not a OOD scenario.\n4, Written is not smooth, for example,  the latent thoughts in the title is not defined or discussed."}, "questions": {"value": "1, How does PREGEN behave on unseen domains, is the light transformer encoder is able to handle this situation?\n2, Could aggregating hidden states helps as well when finetuing the LLM like what other methods do? \n3, What does the latent thoughts mean."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wwawUOyqbj", "forum": "b88NdR21Ij", "replyto": "b88NdR21Ij", "signatures": ["ICLR.cc/2026/Conference/Submission2096/Reviewer_5zJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2096/Reviewer_5zJ8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911552477, "cdate": 1761911552477, "tmdate": 1762916020841, "mdate": 1762916020841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PREGEN, an efficient composed video retrieval framework that leverages multi-layer representations from a frozen Vision-Language Model (VLM). \n\nInstead of using only the final-layer features, PREGEN aggregates hidden states across multiple layers to form a semantically richer embedding. \n\nThe approach avoids VLM fine-tuning and introduces a lightweight encoder for retrieval. Experimental results demonstrate **remarkable improvements**, with Recall@1 reaching up from 72% to 98% on standard CoVR benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and clearly presented, making it easy to follow the motivation and design choices.\n\n2. The **retrieval performance is impressive**, significantly outperforming prior methods on several CoVR benchmarks.  Improve previous results from  26.79 to 96.38 on FineCVR. \n\n3. Using frozen VLMs with a lightweight encoder is an efficient design philosophy, aligning with current trends in leveraging pre-trained multimodal models. The work provides some evidence of robustness across backbones and textual modifications."}, "weaknesses": {"value": "The Recall@1 = 98% result **seems unusually high**. There is no analysis on potential overfitting or data leakage, which undermines the credibility of the claim. __Can you check again if the evaluation code is correct?__ For example, test a subset from MSRVTT/MSVD.\n\nThe methodological novelty is limited: the core idea mainly extends existing MLLM-based retrieval pipelines by aggregating multi-layer features rather than relying solely on the last layer.\n\nThe paper lacks an efficiency analysis — inference and computational cost remain unclear, especially since modern MLLMs are heavy for retrieval tasks.\n\nThe comparison only contrasts single-layer vs. multi-layer aggregation; there is no systematic exploration (e.g., curve showing performance vs. number of layers used). The paper could benefit from more diagnostic experiments (e.g., cross-domain tests or noise robustness) to justify the claimed generalization ability."}, "questions": {"value": "Could the authors provide a computation cost comparison (e.g., FLOPs, inference latency) between PREGEN and other retrieval baselines?\n\nHow does performance evolve as more layers are aggregated? A trend curve (number of layers vs. Recall@1) would clarify the contribution of multi-layer fusion.\n\nHave the authors verified data splits to rule out potential leakage or overlap between training and test sets, given the unusually high Recall@1 results?\n\nCan PREGEN generalize to retrieval tasks with domain shift (e.g., from activity datasets to instructional videos)? Have you consider the common used benchmarks like MSRVTT/MSVD?\n\nWould incorporating intermediate fusion strategies (e.g., attention-weighted layer aggregation) further improve the representation efficiency?\n\nI **may reconsider my voting if all my concern are addressed during the rebuttal section**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OWn4Ch0rOy", "forum": "b88NdR21Ij", "replyto": "b88NdR21Ij", "signatures": ["ICLR.cc/2026/Conference/Submission2096/Reviewer_U79f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2096/Reviewer_U79f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136379999, "cdate": 1762136379999, "tmdate": 1762916019265, "mdate": 1762916019265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an efficient and powerful framework for Composed Video Retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper introduces an efficient and well-designed framework for composed video retrieval."}, "weaknesses": {"value": "1. **Writing issues** – The long sentence in line 95 (“This approach results in embeddings that fail…”) is confusing and should be rewritten for clarity.\n2. **Figures** – Figures 1 and 2 lack proper legends. For instance, the meanings of the blue, yellow, and pink circles are unclear.\n3. **Architecture confusion** – In line 231, Figure 2 shows that hidden states are encoded by a *transformer encoder*, but the text mentions another *VLM* used for processing. This inconsistency should be clarified.\n4. **Limited novelty** – The method mainly adopts a standard contrastive loss, which is commonly used in retrieval-related works. The novelty of the training objective appears limited.\n5. **Missing inference details** – The paper does not clearly explain how inference is conducted. During training, video and text embeddings are fused, but the target dataset only provides video embeddings. Please clarify how retrieval works at test time.\n6. **Dataset and metrics** – The paper lacks a clear and separate description of the datasets used and the evaluation metrics applied."}, "questions": {"value": "Please refer to the points listed under *Weaknesses*."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b0b4eF6PVO", "forum": "b88NdR21Ij", "replyto": "b88NdR21Ij", "signatures": ["ICLR.cc/2026/Conference/Submission2096/Reviewer_it1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2096/Reviewer_it1Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762165693486, "cdate": 1762165693486, "tmdate": 1762916019006, "mdate": 1762916019006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}