{"id": "Q4Nk6PlZYH", "number": 20133, "cdate": 1758302900360, "mdate": 1759896999887, "content": {"title": "DEPART: Hierarchical Multi-Agent System for Multi-Turn Interaction", "abstract": "Large Language Models (LLMs) excel at short-horizon tasks but struggle in complex, long-horizon scenarios involving multi-turn interactions, multi-step reasoning, and selective multi-modal perception. Two core challenges in these settings are effective long-term planning and mitigating cross-modal distraction. Our empirical analysis shows that single LLM agent exhibits steep performance drops as interaction steps increase, underscoring the limitations of monolithic approaches. To overcome these challenges, we propose $\\textbf{DEPART}$, a hierarchical multi-agent framework that decomposes planning, action execution, and visual understanding into specialized agents. Through its $\\textbf{D}$ivide, $\\textbf{E}$valuate, $\\textbf{P}$lan, $\\textbf{A}$ct, $\\textbf{R}$eflect, and $\\textbf{T}$rack cycle, DEPART supports dynamic task decomposition, feedback-driven adaptation, and selective vision grounding to reduce cost and improve robustness. Building on this architecture, we introduce Hierarchical Interactive Multi-turn Policy Optimization (HIMPO), a two-round post-training strategy that alternately optimizes planner and executor with dense role-specific and sparse task-level rewards to encourage specialization and coordinated long-horizon reasoning. Across WebArena-Lite and VisualWebArena benchmarks, DEPART with HIMPO consistently outperforms strong single-agent and post-trained baselines.", "tldr": "", "keywords": ["large language model", "multi-agent", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4a2da48f91c6bdad9a6e8913e52b064d6574e54.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DEPART, a hierarchical multi-agent system designed for long-horizon, multi-turn web-based tasks using LLMs. DEPART modularizes agent roles into planning, action execution, and visual perception, leveraging a structured Divide-Evaluate-Plan-Act-Reflect-Track cycle for dynamic task decomposition and adaptive coordination. The authors also present Hierarchical Interactive Multi-turn Policy Optimization (HIMPO), a post-training RL algorithm employing role-specific dense and sparse rewards to enhance specialization and coordinated reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- DEPART’s hierarchical decomposition (Figure 2) exhibits a well-considered separation between planning, visual processing, and action execution. This is grounded both in implementation and empirical ablation, highlighting modularity as a central system benefit.\n- The HIMPO method introduces clear curriculum learning—first using dense, role-specific rewards, then transitioning to sparse, task-level optimization. This approach is well-argued and ties theoretical foundations to practical implementation."}, "weaknesses": {"value": "- Pipeline modularity substitutes for model competence rather than demonstrating insight: The paper argues that modular decomposition is necessary, but offers little evidence that modularity is inherently superior rather than compensatory for limited backbone capability. If stronger models can learn to fuse planning, perception, and action end-to-end, the purported “need” for role separation becomes an artifact of current model weakness, not a principled design insight. The 3.2 section is not convincing enough because the assumption in eq  2 is vanilla.\n- Why this pipeline is not so convincing: It slices an already unstable LLM decision process into more interfaces that are hard to verify, amplifying error propagation and protocol dependence, while obscuring who learns what and how. Without demonstrating that modularity confers a principled advantage over a capacity- and training-driven end-to-end policy (e.g., via more insightful mechanistic analyses), the work risks “works on these benchmarks” without “why anyone should care.” \n- My insight to improve a general multi-modal agent (not only the performance but also capacity) is : if the backbone is strong enough and trained end-to-end with appropriate curricula and constraints, fusion—not segmentation—should be the default. The paper does not convince with this question, which is the crux."}, "questions": {"value": "See weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "er1wx5q9Zt", "forum": "Q4Nk6PlZYH", "replyto": "Q4Nk6PlZYH", "signatures": ["ICLR.cc/2026/Conference/Submission20133/Reviewer_7dB7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20133/Reviewer_7dB7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760513928641, "cdate": 1760513928641, "tmdate": 1762999996369, "mdate": 1762999996369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DEPART, a hierarchical multi-agent framework designed to improve large language models’ performance on long-horizon, multi-turn, and multimodal interaction tasks. The system separates planning, execution, and vision into specialized agents coordinated through a structured communication loop. In addition, the authors introduce HIMPO, a two-stage reinforcement learning post-training method that alternately optimizes planner and executor roles using dense and sparse rewards. Experiments on WebArena-Lite and VisualWebArena show consistent improvements over strong single-agent and RL-trained baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and well-organized, with comprehensive experiments on realistic web-based benchmarks (WebArena-Lite and VisualWebArena) that demonstrate consistent performance gains.\n\n2. The hierarchical decomposition of planning, execution, and vision helps improve modularity and interpretability compared to standard single-agent baselines."}, "weaknesses": {"value": "1. The overall novelty is limited — the hierarchical multi-agent setup and role division (Planner / Executor / Vision) have been explored extensively in prior works such as AutoGen [1], CAMEL [2], and MetaGPT [3]. The paper mainly reuses this idea with modest extensions rather than introducing a fundamentally new concept.\n\n2. The authors propose a new post-training RL method, HIMPO, which uses two stages with dense role-specific and sparse task-level rewards. The planner and executor share the same base model, distinguished only by role prompts. However, it is unclear whether this parameter-sharing actually works well — sharing parameters may limit role specialization or cause interference. Some evidence or ablation on this design would be helpful.\n\n3. The evaluation benchmarks (WebArena-Lite, VisualWebArena) mainly test web navigation and interaction, which involve limited reasoning depth. Therefore, the claimed improvements in “long-horizon reasoning” are not strongly supported by the experiments.\n\nOverall, while the system is functional, the work feels somewhat engineering-oriented and coarse in design, with incremental contributions over prior multi-agent LLM frameworks.\n\n[1] AutoGen: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation (Wu et al., 2023)\n\n[2] CAMEL: CAMEL: Communicative Agents for “Mind” Exploration of Large Scale Language Model Society (Li et al., 2023)\n\n[3] MetaGPT: MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework (2023)"}, "questions": {"value": "1. How does DEPART fundamentally differ from prior hierarchical or multi-agent frameworks such as AutoGen, CAMEL, or MetaGPT? The current design appears conceptually similar—could the authors clarify what new insights or mechanisms are introduced beyond architectural modularization?\n\n2. The paper frequently mentions improvements in “long-horizon reasoning.” However, the benchmarks (WebArena-Lite, VisualWebArena) mainly focus on web navigation and UI interactions. How do the authors justify that these results reflect true reasoning improvements rather than better interface control? Could the authors provide additional experiments on reasoning-heavy or cross-domain multi-turn datasets to demonstrate the generality of DEPART beyond web-based environments?\n\n3. Given that DEPART adds multiple agents and coordination overhead, how significant is the computational cost relative to the performance gain? Would simpler prompting strategies or single-agent fine-tuning achieve similar results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UI4nnqZeVC", "forum": "Q4Nk6PlZYH", "replyto": "Q4Nk6PlZYH", "signatures": ["ICLR.cc/2026/Conference/Submission20133/Reviewer_1Jwb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20133/Reviewer_1Jwb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815952520, "cdate": 1761815952520, "tmdate": 1762999997108, "mdate": 1762999997108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the failure of LLMs in complex, long-horizon tasks that require multi-turn interaction and multi-modal perception. The authors identify two primary challenges: (1) the difficulty of long-term planning in monolithic agent architectures and (2) \"cross-modal distraction,\" where irrelevant visual information degrades reasoning. An empirical study confirms that single-agent performance drops steeply as the number of required interaction steps increases.\n\nTo solve this, the authors propose DEPART, a hierarchical multi-agent framework that decomposes the problem into three specialized agents: a Planner Agent for high-level strategy, an Action Executor for text-based environment interaction, and a Vision Executor for multi-modal perception . The system operates on a \"Divide, Evaluate, Plan, Act, Reflect, and Track\" (DEPART) cycle, which supports dynamic replanning and, crucially, selective vision grounding to reduce costs and distraction.\n\nFurthermore, the paper introduces HIMPO (Hierarchical Interactive Multi-turn Policy Optimization), a two-round post-training strategy to train the planner and executor agents. Round 1 uses dense, role-specific rewards to encourage specialization and exploration. Round 2 uses sparse, task-level rewards to align the agents with the overall task success and mitigate reward hacking. Experiments on WebArena-Lite and VisualWebArena show that the DEPART architecture with HIMPO post-training significantly outperforms single-agent baselines"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The 3-agent hierarchical framework is a robust and well-justified design. By separating the high-level Planner from the low-level Action Executor, the system avoids the bottleneck of a single model managing both strategy and execution. This modularity is shown to be effective, as the 2-agent DEPART (prompting-only) outperforms strong single-agent models (Table 1).\n\nA key contribution is the identification and mitigation of \"cross-modal distraction\". The paper argues that selective visual processing is superior, as VLMs are computationally expensive and unnecessary visual context can interfere with reasoning . This is supported by case studies where a text-only agent succeeds on a task that a vision-enabled agent fails . The 3-agent DEPART design, which empowers the planner to selectively invoke the Vision Executor, is an elegant solution.\n\nProgressive Training Curriculum (HIMPO): The HIMPO training strategy is a sophisticated contribution. Instead of using a single reward, it employs a two-round curriculum that first builds specialized skills (using dense, role-specific rewards) and then aligns those skills with the final objective (using sparse, task-level rewards). This progressive approach is shown to be highly effective, with the 2-round HIMPO (M7+M8) achieving the best performance in ablations (Figure 4)."}, "weaknesses": {"value": "The paper's training strategy, HIMPO, explicitly excludes the Vision Executor from the optimization loop. The rationale is that static image interpretation is \"less dependent on long-horizon context\". While this simplifies training, it seems to be a significant limitation. The core problem of cross-modal distraction stems from the content of the visual analysis. If the (untrained) Vision Executor provides verbose or irrelevant information, the Planner's performance will suffer, even if it's called selectively. The framework's performance is thus dependent on the zero-shot capabilities of a pre-trained VLM, which is not optimized for the task.\n\nThe paper states that the Planner and Action Executor \"share the same model weights\" and are only differentiated by role-specific system prompts. This is presented as an efficiency benefit , but it complicates the core claim of \"specialized agents\". If both agents are the same model, it's unclear if the architecture is truly decomposing the task between specialized agents or just using a highly structured, role-based prompting scheme on a single model. This seems to partially re-introduce the monolithic bottleneck the paper aims to solve.\n\nThe paper states that the Planner and Action Executor \"share the same model weights\" and are only differentiated by role-specific system prompts. This is presented as an efficiency benefit , but it complicates the core claim of \"specialized agents\". If both agents are the same model, it's unclear if the architecture is truly decomposing the task between specialized agents or just using a highly structured, role-based prompting scheme on a single model. This seems to partially re-introduce the monolithic bottleneck the paper aims to solve.\n\nThe planner's dense reward in Round 1 is an \"intrinsic reward\" based on the KL divergence from a uniform distribution (Equation 9) . This rewards high-confidence, low-entropy outputs. However, the paper claims this first round is meant to \"foster strategy exploration\". Rewarding confidence seems antithetical to exploration, as it would likely encourage the agent to stick to a single, high-confidence plan rather than explore alternatives. This connection is not well-explained."}, "questions": {"value": "1. Since the Planner and Action Executor share parameters, how does the system truly achieve \"specialization\"? Does this not re-introduce the single-model bottleneck you identified, where one model must still manage both high-level planning and low-level execution, just prompted differently at each turn?\n\n2. Given that \"cross-modal distraction\" is a key problem , why was the Vision Executor excluded from HIMPO? Wouldn't training the Vision Executor with a reward for providing concise and task-relevant analysis be as important as training the Planner to call it selectively?\n\n3. How does rewarding the planner for \"confidence\" (low-entropy output via KL divergence) align with the stated goal of \"foster[ing] strategy exploration\" in Round 1? It seems this would penalize exploration and favor exploitation of the first plan it finds."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "77xX652ubV", "forum": "Q4Nk6PlZYH", "replyto": "Q4Nk6PlZYH", "signatures": ["ICLR.cc/2026/Conference/Submission20133/Reviewer_3zv4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20133/Reviewer_3zv4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959907596, "cdate": 1761959907596, "tmdate": 1762999996475, "mdate": 1762999996475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, \"DEPART,\" aims to solve the well-known problem of LLM agents failing in complex, long-horizon tasks that require multi-turn interaction and multi-modal understanding. We all know that monolithic models struggle as tasks get longer, which the authors show well in Figure 1.\n\nThe authors propose two main contributions:\n\n1. The DEPART Architecture: This is a hierarchical, multi-agent framework that splits the task into three specialized roles: a Planner, an Action Executor, and a Vision Executor. The core motivation for this 3-agent design is to mitigate what the authors call \"cross-modal distraction\" by allowing the Planner to selectively decide when to call the Vision Executor, rather than feeding it images at every step.\n2. The HIMPO Algorithm: This is a two-stage reinforcement learning (RL) post-training strategy. The first stage uses dense, role-specific rewards to get the agents up to speed (including an intrinsic \"confidence\" reward for the planner). The second stage switches to sparse, task-level rewards to align the agents with the final goal.\n\nThe authors evaluate this DEPART + HIMPO system on the WebArena-Lite and VisualWebArena benchmarks, showing that their method outperforms several baselines.\n\nOverall, this is a very strong empirical paper. It is well-written, the experiments are thorough, and the practical result of making a small open-source model outperform large proprietary ones is significant. However, I have major concerns about the conceptual novelty of both the architecture and the algorithm, which seem to be very closely based on several recent, existing works. My review will focus heavily on this point."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clarity & Significance: First, the paper is exceptionally well-written. It's clear, well-structured, and a pleasure to read. The problem it addresses—long-horizon, multi-modal agents—is without a doubt highly significant and of great interest to the ICLR community. The authors do a fantastic job motivating their work. Figure 1 is the perfect setup, clearly showing how a powerful model (Claude 3.7) just falls off a cliff as the number of interaction steps increases. This really sells the need for a non-monolithic approach.\n2. Empirical Quality & Rigor: The paper's greatest strength is its empirical rigor. The experimental quality is very high. They use challenging and appropriate benchmarks (WebArena-Lite and VisualWebArena). The ablation study in Section 5.3 and Figure 4 is excellent. I mean, the authors really did a thorough job breaking down the components of HIMPO (M1-M8) to demonstrate precisely why their two-stage curriculum (M7+M8 in the chart) is superior. I really appreciate this level of detailed ablation. The failure mode analysis in Figure 6 is also a very nice touch. Showing that the 2-agent setup significantly reduces \"Gets Stuck Midway\" and \"Fails to Strategize/Adapt\" errors provides strong evidence for the planner-executor split.\n3. Significant Practical Result: For me, the most important practical finding is in Table 2. The authors took a small, 4B-parameter open-source model (Qwen3-4B) and, by applying their HIMPO training, boosted its performance to 51.5%. This absolutely crushed the baseline MT-GRPO (37.6%) using the same model. What's more, this small, trained model also beat much larger, prompt-only proprietary models like Claude 3.7 (35.8% in Table 1). This is a really big deal. Demonstrating that a smaller, accessible model can be trained to be this effective is a wonderful, reproducible contribution to the field.\n4. Originality (in part): While I will spend a lot of time on novelty in the next section, I will concede one point here. Even if the components aren't new, the specific integration of a dynamically scheduled, separate vision executor, with the explicit goal of mitigating \"cross-modal distraction\", is a clever and effective piece of engineering."}, "weaknesses": {"value": "1. DEPART Architecture Lacks Conceptual Novelty: My main issue with this paper is the overstatement of its novelty. The core architectural contribution—separating a \"Planner\" from an \"Executor\"—is simply not new. This exact concept is the entire point of several recent papers. Specifically, PLAN-AND-ACT is built on separating \"high-level planning (what to do and why)\" from \"low-level execution (how to do it)\". Similarly, GoalAct proposed a framework combining \"global planning\" with \"hierarchical execution\". Both of these were publicly available well before this paper's submission. The authors even cite Erdogan et al., but they continue to frame the planner-executor split as a novel component of DEPART. To be blunt, it is not. The paper's contribution is not inventing the 2-agent hierarchy; it is adding a 3rd agent (vision) to this already-established 2-agent pattern. The paper must be revised to position its contribution correctly in light of this prior work.\n2. HIMPO Algorithm Also Lacks Algorithmic Novelty: I have the exact same problem with the \"novel\" HIMPO algorithm. It appears to be a very smart integration of several existing techniques, not a new algorithm in itself. Let's look at the pieces:\n   - Core Algorithm: The authors state HIMPO is a \"multi-turn variant of DAPO\". This means the foundational policy optimization algorithm is DAPO, which was already published.\n   - Planner Reward: This is the most glaring issue for me. The \"intrinsic reward\" for the planner, defined in Equation 9 (the KL divergence from a uniform distribution), appears to be mathematically identical to the \"self-certainty\" reward proposed in INTUITOR. The authors cite Zhao et al. in the appendix but not in the main body as the source of this reward function. This is a serious attribution issue. This is not their novel reward function; they are using a reward function from another paper.\n   - Curriculum: The two-stage curriculum itself (starting with dense, role-specific rewards and moving to sparse, task-level rewards) is a known pattern for tackling sparse-reward RL problems. For instance, WebRL also used a curriculum-based approach to train its web agent.\n   - Conclusion: So, to be clear, HIMPO is not a new algorithm. It is the application of the DAPO algorithm, using the intrinsic reward from INTUITOR, implemented with a known curriculum strategy. This is a great empirical paper showing this combination works, but it is not an algorithmic paper. The claims of novelty must be toned down significantly.\n3. Insufficient Analysis of the 3-Agent System's Failure: This is a huge miss. The paper's primary architectural argument is that the 3-agent system is better than a 2-agent system because it solves \"cross-modal distraction\". But the authors' own data in the appendix (Table 5) directly contradicts this claim.\n   - On VisualWebArena, the benchmark where vision is required, the 2-agent system (with vision) achieves a 38.6% success rate.\n   - Their flagship 3-agent system (the main DEPART proposal) achieves only 35.1%.\n   - This means adding the specialized vision executor hurt performance. This critical negative result is hidden in Appendix E.3 and dismissed in a single line as \"slight coordination challenges\". This is completely inadequate. This is a classic example of \"coordination overhead\", where the cost of managing and communicating between three agents (Planner, Action, Vision) exceeds the benefit of specialization. The fact that the authors did not dig into this—when it seems to undermine their entire 3-agent premise—is a major weakness."}, "questions": {"value": "- Q1 (On Novelty): This is my most important question. Given that the planner/executor split is functionally identical to prior work like PLAN-AND-ACT and GoalAct, and that the intrinsic reward in Equation 9 appears identical to the \"self-certainty\" reward from INTUITOR, can the authors please precisely restate their claims of conceptual novelty? What exactly is novel here beyond the (very successful) integration and empirical validation of these existing components?\n- Q2 (On the 3-agent vs. 2-agent failure): I am very confused by the result in Table 5 where the 3-agent system (35.1%) performs worse than the 2-agent system (38.6%) on VisualWebArena. This seems to invalidate the core motivation for the 3-agent design. Can you please provide a detailed analysis of the trade-off between \"coordination overhead\" (which you call \"coordination challenges\") and \"cross-modal distraction\"? Why did your main architecture fail on the very benchmark it was designed for?\n- Q3 (HIMPO Ablation): In Algorithm 1, you describe HIMPO as an alternating training process (train planner, then train executor, repeat). However, the ablations in Figure 4 seem to test the curriculum (M7 vs M8) and multi-agent setup (M3 vs M6), not the alternating part. Could you provide an ablation that compares this alternating optimization against a simpler joint optimization (i.e., updating both agents on the same batch)? I'm curious if this alternating design is actually a necessary part of its success.\n- Q4 (Static Vision Executor): You explicitly exclude the vision executor from the HIMPO training loop, keeping it static. What was the reasoning for this? Did you experiment with fine-tuning it as well? I wonder if the planner is learning to make requests that the static vision model isn't good at, and if co-training all three agents (even with a small learning rate for the vision model) would have improved the 3-agent system's performance and overcome the issues seen in Table 5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sa4XZwTukC", "forum": "Q4Nk6PlZYH", "replyto": "Q4Nk6PlZYH", "signatures": ["ICLR.cc/2026/Conference/Submission20133/Reviewer_aJwS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20133/Reviewer_aJwS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965573806, "cdate": 1761965573806, "tmdate": 1762999996580, "mdate": 1762999996580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}