{"id": "4K2NNr9W5C", "number": 1174, "cdate": 1756858931414, "mdate": 1759898223658, "content": {"title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models", "abstract": "To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53\\% ASR drop with only 0.2/0.3/0.6\\% performance drops on the 3 tested models on retrieval, and a 90\\% ASR drop with a 0.3\\% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly available.", "tldr": "", "keywords": ["Vision-Language Model", "Concentrated Attention", "Adversarial Robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e126bbf08481add1d9d2057baf92c5a52bbd93f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Function-word De-Attention (FDA), a lightweight and novel method to improve the robustness of vision-language models (VLMs) without compromising their clean performance. The authors observe that function words (e.g., “the,” “is,” “and”) can make VLMs more vulnerable to cross-modal adversarial attacks because these words are frequent but semantically uninformative. FDA operates by computing and subtracting the cross-attention between function words and image tokens from the original attention maps. While the underlying mechanism behind FDA’s effectiveness is not yet fully understood, the authors provide extensive experiments demonstrating consistent improvements across multiple models, datasets, and attack settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Identifying function words as a source of vulnerability in VLMs is an original and well-motivated idea. Good observation.\n\n2. FDA is conceptually straightforward and computationally efficient. It requires only a differential subtraction step within existing attention computations. It does not require additional parameters or retraining complexity.\n\n3. The authors evaluate FDA across multiple models, datasets, and both targeted and untargeted attacks. The improvement over adversarial training baselines (TeCoA, FARE) is consistent and substantial."}, "weaknesses": {"value": "1. It's an interesting observation that function words can lead as a source of vulnerability. However, the evidence provided are far from satisfying. Namely, a single sentence (“80.3% of images show higher similarity scores toward function words than content words after attacks”) and one visualization of distracted attention. It is insufficient to establish that this is a systematic rather than a coincidental effect. Additional qualitative and quantitative analyses would strengthen the claim. I also recommend including a baseline where function words are simply removed from the inputs, to better isolate and validate the proposed phenomenon.\n\n2. Lack of clarity regarding experimental setup. FDA introduces a hyperparameter $\\lambda$ to control the strength of “de-attention,” where $\\lambda = 0$ reduces the method to standard attention. However, I find it confusing that the authors did not specify the $\\lambda$ values used in the reported results. In particular, for the clean (non-attacked) setting, it is unclear whether the authors used a very small or even zero $\\lambda$, which would make the claim of minimal performance drop less convincing. Moreover, the paper does not analyze how varying $\\lambda$ affects performance and robustness, which is essential for understanding the method’s sensitivity and general applicability.\n\n3. Several presentation issues. 1) Figure 1 seems to have noticeably lower resolution than other figures, even for texts in the figure. 2) Typo in Equation (4), $S_{T_f}$ instead of $S_{t_f}$. 3) Mistake in line 197-198, \" Specifically, **TCL** shares the same backbones as **TCL** ...\""}, "questions": {"value": "I have made my suggestions along with weaknesses. Here are several open questions for the authors:\n\n1. Have the authors examined whether the vulnerability arises specifically from function words, or more generally from irrelevant or low-information words in the inputs?\n\n2. Could there be an adaptive mechanism to identify which words to down-weight or exclude, rather than relying on a fixed dictionary?\n\n3. Are there cases where function words contribute meaningfully to visual grounding or alignment, and if so, how does FDA handle those instances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hc5C7l24OX", "forum": "4K2NNr9W5C", "replyto": "4K2NNr9W5C", "signatures": ["ICLR.cc/2026/Conference/Submission1174/Reviewer_QdtA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1174/Reviewer_QdtA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761193984856, "cdate": 1761193984856, "tmdate": 1762915697690, "mdate": 1762915697690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Function-word De-Attention (FDA) to improve the robustness and performance of VLMs by reducing attention to function words. FDA works by subtracting the cross-attention between function words and images from the original attention. Experimental results demonstrate that FDA reduces ASR with minimal or even improved performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation and introduction of this paper is clear while use visualization to help readers quickly understand the task.\n\n2. The experiments are thorough, and the performance is verified on three models, two tasks, and three datasets."}, "weaknesses": {"value": "1. The authors' proposed operation, \"proper removal of function words could potentially defend VLMs against such attacks,\" appears to have some limitations on VLM's use cases: the paper does not demonstrate its ability to defend against the fundamental task of vision-language models, such as classification. Furthermore, defense methods like FARE [1], which were originally designed to enhance the robustness of the LVLM's vision encoder, seems also inapplicable to this scenario.\n\n2. In contribution summarization, the proposed “pioneer the theory” may be replaced with observation, motivation or other words. The paper does not use theory to prove that the difference in similarity after paying less attention can increase or decrease, or that the difference is bounded.\n\n3. In Figure 2, how the conclusion “models can learn a more aligned cross-modal embedding” get? The observational evidence provided in the introduction doesn't suggest that this phenomenon is achieved by learning a more aligned cross-modal embedding. On the contrary, if the function word is removed, intuitively, the effect of alignment should be weakened.\n\n4. Considering the trade-off between robustness and performance, it is recommended to compare with the TRADES [2] method.\n\n[1] Schlarmann C, Singh N D, Croce F, et al. Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models[J]. arXiv preprint arXiv:2402.12336, 2024.\n\n[2] Zhang H, Yu Y, Jiao J, et al. Theoretically principled trade-off between robustness and accuracy[C]//International conference on machine learning. 2019: 7472-7482."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nNtGkI5Nrs", "forum": "4K2NNr9W5C", "replyto": "4K2NNr9W5C", "signatures": ["ICLR.cc/2026/Conference/Submission1174/Reviewer_N7H5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1174/Reviewer_N7H5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761398730594, "cdate": 1761398730594, "tmdate": 1762915697543, "mdate": 1762915697543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper hypothesizes that function words (e.g., am, is, are)—which are semantically vague and non-specific—make Vision-Language Models (VLMs) vulnerable to cross-modal adversarial attacks.\nTo address this, the authors propose Function-word De-Attention (FDA), which introduces a parallel attention path that computes the cross-attention between function words and images, and then subtracts this “distraction” from the original attention through a differential subtraction mechanism.\n\nOn retrieval tasks, FDA reduces the Attack Success Rate (ASR) by 18/13/53% on ALBEF/TCL/BLIP, with only 0.2–0.6% clean performance degradation.\nOn visual grounding, it achieves a 90% ASR reduction with a 0.3% accuracy improvement.\nThe robustness gain scales with model size (e.g., +54% ΔASR for BLIP) and even improves zero-shot performance by +0.4% without fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Conceptually novel to focus on function words as a source of VLM vulnerability.\n- Simple and efficient method. No additional parameters or adversarial training required.\n- Consistent and strong improvement across multiple models, datasets, and tasks."}, "weaknesses": {"value": "- The paper does not perform a true white-box attack against FDA.\nWhile baselines are evaluated under full white-box settings (with gradient access), FDA’s differential subtraction module is hidden from the attacker.\nThe proposed Masked APGD (MAPGD) does not actually backpropagate through the FDA operation, so the comparison is not fair.\n\n- The “observation” section is weak.\nThe reported 80.3% statistic lacks detail—what dataset, model, or attack setting?\nShowing only one qualitative example in Fig. 1 is not convincing as empirical evidence.\n\n- The paper does not include any ablation or control experiment on other word classes (e.g., content words, nouns, verbs, adjectives).\n\n- Table 1 presentation is confusing: R@1 (↑, higher is better) and ASR (↓, lower is better) are shown in the same rows without clear separation.\n\n- The comparison is too limited: only TeCoA/FARE trained with ε = 1/255 are considered. Robustness should be compared against baselines trained under the same threat levels (ε = 2/255 or 4/255), as standard adversarial training does."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8IFucUMs1c", "forum": "4K2NNr9W5C", "replyto": "4K2NNr9W5C", "signatures": ["ICLR.cc/2026/Conference/Submission1174/Reviewer_nHus"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1174/Reviewer_nHus"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661872173, "cdate": 1761661872173, "tmdate": 1762915697317, "mdate": 1762915697317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Function-word De-Attention (FDA), a training-free defense that subtracts attention from function words in VLMs (e.g., ALBEF, BLIP).\nIt shows robustness gains under image-space attacks without hurting clean performance. However, the idea is limited to cross-attention models, and lacks ablation evidence to support its claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The function-word “de-attention mechanism” is straightforward and easy to integrate into VLMs.\n2. Zero training cost and minimal inference overhead.\n3. Works stably across three representative fusion-based VLMs (ALBEF, BLIP, TCL)."}, "weaknesses": {"value": "1. Limited scope\n    * FDA fundamentally requires cross-attention between text and image; thus, it cannot be applied to CLIP-style encoders or LLM-based multimodal models (LLaVA, InternVL).\n    * Therefore, I think the paper’s title (“robustness of VLMs”) overstates generality.\n2. Kind of trivial core observation\n    * Figure 1’s finding — that attacks disrupt function-word attention but not content-word grounding — is unsurprising and follows directly from semantic anchoring.\n3. Missing ablation\n    * The simplest verification — masking function words entirely and measuring retrieval performance — is absent.\n    * Without it, it’s unclear which is more important; “attention subtraction” or just “ignoring function words.”\n4. Over-claiming “training-free”\n    * Some sections mention optional fine-tuning for λ-scaling and per-layer adaptation, which contradicts the strict “zero-training” claim."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iAHCX1XdcL", "forum": "4K2NNr9W5C", "replyto": "4K2NNr9W5C", "signatures": ["ICLR.cc/2026/Conference/Submission1174/Reviewer_h995"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1174/Reviewer_h995"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908050522, "cdate": 1761908050522, "tmdate": 1762915697217, "mdate": 1762915697217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Function-word De-Attention (FDA), a parameter-free modification to fusion-based vision–language models (ALBEF, TCL, BLIP). The authors observe that adversarial image attacks increase cross-attention between images and function words, while disrupting content-word grounding. FDA computes a parallel attention path using only function words and subtracts this “distraction” from the original attention. Experiments across three datasets, three models, and six attacks show consistent robustness improvements with minimal changes in clean accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is simple, modular, and easy to integrate into fusion-encoder VLMs.  \n2. FDA shows substantial robustness gains across PGD, APGD, and MAPGD attacks on retrieval and grounding tasks.  \n3. Clean performance is largely preserved, and in some cases improved.  \n4. The experimental section is thorough, covering multiple datasets, attacks, and ablations."}, "weaknesses": {"value": "1. Limited applicability. FDA fundamentally relies on cross-attention between textual and visual tokens. It cannot be applied to CLIP-style dual encoders or multimodal LLMs such as LLaVA or InternVL. The title may therefore imply a broader generality than supported.\n\n2. Insufficient empirical evidence for the core hypothesis. The claim that adversarial attacks increase similarity toward function words is supported by one statistic and one visualization. A simple yet crucial ablation is missing: removing or masking function words entirely and measuring robustness and performance. Without this, it is difficult to isolate whether the gains come from FDA’s subtraction mechanism.\n\n3. Lack of clarity regarding the λ hyperparameter.  Equation (6) introduces λ as a learnable scalar controlling the strength of de-attention (line 173–174). However, the paper does not report its final values or variation across layers. Since λ determines how aggressively function-word attention is removed, this should be documented.\n\n4. Baseline fairness concerns. TeCoA and FARE are compared at ε = 1/255 (line 204: “Specifically, we use ϵ = 1 for TeCoA and FARE.”), which is lower than standard adversarial training budgets such as 2/255 or 4/255. This may underestimate their achievable robustness. A fairer comparison would match the threat level across defenses.\n\n5. Unclear effect of FDA without finetuning. Since the hypothesis stresses function-word vulnerability, it is important to test whether FDA alone (without downstream finetuning) improves robustness. The zero-shot experiments measure clean performance but not adversarial robustness."}, "questions": {"value": "1. What happens when all function words are simply removed from the input during inference?  \n   If such a baseline already shows improvements, it would further clarify the value and necessity of your proposed method.\n\n2. What are the learned λ values after finetuning, and how sensitive is the method to λ?  \n   I am also curious whether λ exhibits any systematic trend across layers (e.g., whether early and later layers produce similar λ values).\n\n3. In line 204 you state that ϵ = 1 is used for TeCoA and FARE. In Table 1, are the 2/255 and 4/255 results being compared against TeCoA/FARE trained with 1/255? If not, please clarify the setup."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iAHCX1XdcL", "forum": "4K2NNr9W5C", "replyto": "4K2NNr9W5C", "signatures": ["ICLR.cc/2026/Conference/Submission1174/Reviewer_h995"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1174/Reviewer_h995"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908050522, "cdate": 1761908050522, "tmdate": 1763638024546, "mdate": 1763638024546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "We thank all reviewers for their effort and advices."}, "comment": {"value": "We appreciate all reviewers for their invaluable inputs and effortness in perfecting our paper. Overall, **all reviewers** recognize the **simplicity and novelty** of our FDA. Specifically, **reviewer** **QdtA** regards FDA as *an original and well-motivated idea*, while **reviewer nHus** thinks the idea *conceptually novel*. Besides, **reviewer h995, N7H5 and QdtA** regard FDA as a *simple, straightforward* and *efficient* method. Lastly, **all reviewers** recognize the **thoroughness of our experiments** and the **effectiveness of FDA** across models/datasets/tasks/attacks. Moreover, **Reviewer nHus** and **QdtA** agree that FDA has *consistent and strong improvement*. On the other hand, **reviewer h995, nHus,** and **QdtA** advise using direct removal of function words as comparison to further validate the effectiveness of FDA, which will be addressed as follows"}}, "id": "82g5Bh5ZOj", "forum": "4K2NNr9W5C", "replyto": "4K2NNr9W5C", "signatures": ["ICLR.cc/2026/Conference/Submission1174/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1174/Authors"], "number": 20, "invitations": ["ICLR.cc/2026/Conference/Submission1174/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763738797126, "cdate": 1763738797126, "tmdate": 1763738797126, "mdate": 1763738797126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}