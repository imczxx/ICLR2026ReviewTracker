{"id": "uLwuPbVegY", "number": 16843, "cdate": 1758269314788, "mdate": 1759897216186, "content": {"title": "VoCap: video object captioning and segmentation from any prompt", "abstract": "Understanding objects in videos in terms of fine-grained localization masks and\ndetailed semantic properties is a fundamental task in video understanding. In this\npaper, we propose VoCap, a flexible video model that consumes a video and a\nprompt of various modalities (text, box or mask), and produces a spatio-temporal\nmasklet with a corresponding object-centric caption. As such our model addresses\nsimultaneously the tasks of promptable video object segmentation, referring expression segmentation, and object captioning. Since obtaining data for this task is tedious and expensive, we propose to annotate an existing large-scale segmentation\ndataset (SAV) with pseudo object captions. We do so by preprocessing videos with\ntheir ground-truth masks to highlight the object of interest and feed this to a large\nVision Language Model (VLM). For an unbiased evaluation, we collect manual\nannotations on the validation set. We call the resulting dataset SAV-Caption. We\ntrain our VoCap model at scale on a SAV-Caption together with a mix of other\nimage and video datasets. Our model yields state-of-the-art results on referring\nexpression video object segmentation, is competitive on semi-supervised video\nobject segmentation, and establishes a benchmark for video object captioning. Our\ndataset will be made available.", "tldr": "Video segmentation with location", "keywords": ["video segmentation; VOS; referring expression segmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe82b3884d3a8d29461e043146cb4a7060c2e2c9.pdf", "supplementary_material": "/attachment/0f72cf865474d633a757c245846dfdb44e1cdedb.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes VoCap, a flexible video model that can receive video and multimodal prompts, output spatiotemporal masks and target guidance, and simultaneously solve promptable video target segmentation, referential expression segmentation, and target captioning tasks. To address data scarcity, the SAV-Caption dataset is constructed from the existing large-scale segmentation dataset SAV, and the model is trained at scale using additional image/video datasets. Experiments show that VoCap establishes a benchmark in video target captioning and image localization captioning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. VoCap breaks through the limitations of existing video understanding models that are \"single input/single output\" and achieves deep unification of multimodal inputs and outputs. It is the first model that can simultaneously cover three key tasks (video object segmentation, representation expression segmentation, and object captioning).\n2. To address the industry challenge of \"high cost and small scale of video mask + caption annotation,\" the paper constructs the SAV-Caption dataset, which surpasses existing datasets in scale, quality, and correlation strength, providing crucial data support for video object understanding tasks."}, "weaknesses": {"value": "1. The SAV-Caption training set relies on pseudo-labels generated by Gemini 1.5 Pro. While visual cues (red outlines, blurred backgrounds) improve quality, these pseudo-labels introduce significant noise that may affect the model's generalization.\n2. SAV-Caption is built on the SAV dataset, but SAV's scene coverage does not explicitly mention \"extreme environments\" (e.g., low light, heavy rain) or \"fine-grained categories\" (e.g., different animal species, complex mechanical parts). Compared with existing datasets (such as MeVIS with motion expression scenes), SAV-Caption's scene complexity remains limited, which may lead to lower model performance in unconventional scenes.\n3. During model inference, the final caption is taken from the last frame of the video. Although the features of historical frames are fused through the memory module, key dynamic information of intermediate frames may still be lost. For example, when the target has multiple stages of action, such as \"standing → walking → jumping\", relying solely on the output of the last frame may simplify or omit the preceding actions."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2zksjkBpA4", "forum": "uLwuPbVegY", "replyto": "uLwuPbVegY", "signatures": ["ICLR.cc/2026/Conference/Submission16843/Reviewer_18P3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16843/Reviewer_18P3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723486965, "cdate": 1761723486965, "tmdate": 1762926865974, "mdate": 1762926865974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VoCap, a strong general model that, given a video and an arbitrary prompt (text/box/mask), simultaneously outputs the target’s spatio-temporal segmentation (masklet) and an object-centric caption. To support training and evaluation, the authors build SAV-Caption, a data pipeline that generates video object-centric captions using a VLM from SAV, with an additional human-annotated validation set. Across both image and video settings, VoCap achieves or surpasses state-of-the-art results on object captioning and referring-expression video object segmentation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper unifies prompting→segmentation and prompting→captioning within one framework, covering RefVOS, promptable segmentation, and location-conditioned captioning. Experiments show that training the location-conditioned captioning task benefits video object segmentation.\n2. The SAV-Caption training set contains ~50k videos and ~170k objects (avg. 11.8 words per caption). Pseudo-labels are mask-grounded rather than box/track-grounded, offering a solid basis for future video segmentation + captioning research.\n3. The method leads on RefVOS-DAVIS/MeViS/UVO-VLN, and reaches new SOTA on YTVOS when combined with FindTrack. On Visual Genome (box-conditioned captioning), it also outperforms recent methods.\n4. The paper specifies a two-stage training schedule (SAM2 reproduction → multi-task joint training), dataset mixing ratios, and full hyperparameters, and states an intention to release the dataset. Releasing training code and data-processing scripts would further strengthen the impact."}, "weaknesses": {"value": "No major flaws. My concerns and suggestions are listed below under Questions."}, "questions": {"value": "The training data quality heavily relies on Gemini-generated captions. Although the pipeline uses constrained prompts plus two visual prompting techniques and includes small-scale human checks, systematic biases (e.g., small-object omissions, “actor bias”) may affect style and value alignment. Suggestions:\nAdd a more systematic human audit with error-type distributions, and compare/aggregate pseudo-labels from multiple VLMs.\nConsider using a small set of human-labeled in-context examples to better constrain Gemini’s generations.\nBeyond reporting average caption length, analyze linguistic diversity (e.g., lemma distributions, voice—active/passive, temporal connectives) and compare with the human validation set.\n\nThe architecture is SAM2-like and supports online inference, but the paper lacks throughput/latency/memory comparisons. FindTrack converts inference to an offline mode and achieves SOTA; the paper notes FindTrack adds <10% overhead, but please provide end-to-end throughput, latency, memory, and power metrics—especially on long videos.\n\nThe current model and dataset are primarily single-object. Please report failure modes and possible multi-object extensions. For example, when one description refers to multiple instances, what captions/masks are produced? Could multiple object tokens or coordinated prompts help?\n\nOnly CIDEr is used for captioning. Since video object captioning stresses attributes and temporal dynamics, please add SPICE / RefCLIPScore / GPT-judge or human evaluations (consistency, readability, temporal coverage). Also, VoCap’s video caption is taken from the last frame, whereas pipeline baselines vote across per-frame captions; please include a last-frame vs. voting ablation to align evaluation protocols and rule out methodological bias."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mGKfLw0RO3", "forum": "uLwuPbVegY", "replyto": "uLwuPbVegY", "signatures": ["ICLR.cc/2026/Conference/Submission16843/Reviewer_p7Gk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16843/Reviewer_p7Gk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789642795, "cdate": 1761789642795, "tmdate": 1762926864784, "mdate": 1762926864784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary:\nVoCap proposes a unified model for video object description and segmentation, which can output both spatio-temporal segmentation masks (masklets) and object-level textual descriptions based on any input prompt (text, box, mask). It achieves state-of-the-art performance on referring expression video object segmentation (VOS)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Advantages:\n1. Efficient data utilization: By using pseudo-labels, the model significantly expands the training data and reduces manual labor costs.\n2. The paper is clear and easy to follow.\n3. Subtitle training improves the understanding of referring expressions, showcasing the advantages of language-vision collaboration.\n4. Qualitative examples demonstrate the effectiveness of the method."}, "weaknesses": {"value": "Disadvantages:\n1. What is the key difference in terms of spatio-temporal reasoning (mask-level) between the proposed method and [1]?\n[1] \"VISA: Reasoning Video Object Segmentation via Large Language Models\"\n2. The argument that \"there is yet no existing computer vision system that is capable of both spatio-temporal localization via segmentation masks, as well as a semantic understanding of objects via natural language\" might be an over-claim.\n3. The masklet (three separate temporal masks) could be presented in the prediction of Figure 2."}, "questions": {"value": "1. In the VOS field, it is common to use the first-frame mask as the reference for mask-guided segmentation, while some other works use a point as the base. Which of these two methods is more natural in the VLM (Vision-Language Models) field? Can the framework proposed by the authors support a point as an additional input in the future?\n\n2. It would be beneficial to add a \"Broader Impact\" section to discuss both the positive and negative impacts on the community. I recognize the importance of this field, but one question is that the paper already supports multiple modalities (text, box, mask) as inputs and dual outputs (mask + caption), offering considerable flexibility. Is this direction still the main focus of research in the field? Would further developing a more unified framework that supports more types of inputs have significant value? Additionally, is researching which types of user input could better adapt to existing technologies a potentially promising direction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oKD8lG8skQ", "forum": "uLwuPbVegY", "replyto": "uLwuPbVegY", "signatures": ["ICLR.cc/2026/Conference/Submission16843/Reviewer_UGYr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16843/Reviewer_UGYr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918674169, "cdate": 1761918674169, "tmdate": 1762926863820, "mdate": 1762926863820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a new dataset called SAV-Caption. Then they use the datasets to train the model VoCap. The authors evaluate both the models and datasets. However, based on the current results, I am not fully convinced with the performance of both the datasets and model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper builds an auto-mated pipeline to label the data. It reduces the cost of human-labeling.\n\nThe authors evaluate the performance of both the model and the datasets."}, "weaknesses": {"value": "1. The paper does not mention \"Sa2VA\" or the \"Ref-SAV\" dataset. Consequently, it lacks a direct comparison between its pseudo-labeling pipeline (which creates SAV-Caption) and the one used by Sa2VA. Given that the data labeling pipelines appear similar, the novelty of VoCap's contribution seems limited.\n\n2. The paper omits \"InstructSeg\" and \"Sa2VA\" from its comparison tables. In Section 5.2 and Table 4, the authors compare VoCap's performance on Referring Video Object Segmentation (RefVOS) against models like UniRef++ and SAMWISE. Based on this limited comparison, the paper claims to \"outperform the state-of-the-art on RefVOS for all datasets.\" However, the exclusion of known models like \"InstructSeg\" and \"Sa2VA\" makes this claim questionable.\n\n3. To properly verify the effectiveness of the proposed datasets, the authors should have used more advanced models for co-training or finetuning. Currently, verification of the dataset's effectiveness is limited to the results in Table 5. Given that the model used for this verification does not achieve state-of-the-art performance, it is difficult to evaluate the true utility of the proposed datasets.\n\n4. There are potentially alternative, and perhaps simpler, ways to label the SAV dataset. While the authors detail their chosen pipeline in Section 3 (using Gemini 1.5 Pro, Visual Prompting, and Background Obscuring), it would also be possible to use a model like the Describe Anything Model (DAM) to obtain object-level descriptions, which could form a dataset analogous to SAV-Caption.\n\nConclusion: Based on the current results, I am not fully convinced of the usefulness of the proposed datasets and model. Furthermore, the paper's state-of-the-art claim is questionable.\n\n\n\nDAM: https://describe-anything.github.io/\nInstructSeg: https://arxiv.org/abs/2412.14006\nSa2VA: https://github.com/bytedance/Sa2VA"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K9duVtwWkN", "forum": "uLwuPbVegY", "replyto": "uLwuPbVegY", "signatures": ["ICLR.cc/2026/Conference/Submission16843/Reviewer_Gtv6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16843/Reviewer_Gtv6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950063473, "cdate": 1761950063473, "tmdate": 1762926863342, "mdate": 1762926863342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **VoCap**, a unified framework for video object understanding that can generate spatio-temporal masks and natural language descriptions from any prompt.\nThe writing is clear and well-structured, with a coherent and novel task formulation supported by a logically developed methodology.\nExperimental results demonstrate state-of-the-art performance, confirming the effectiveness and potential of the proposed approach for video segmentation and captioning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper explores an interesting direction by attempting to unify video object segmentation and captioning within a single framework.\n\nThe idea of leveraging different input modalities (text, box, mask) is conceptually appealing and potentially useful for future multimodal understanding tasks.\n\nThe paper is clearly written and easy to follow, with well-organized structure and visual illustrations."}, "weaknesses": {"value": "The proposed VoCap framework mainly stacks existing techniques (SAM2 for segmentation and BLIP2-style text decoding) with minimal methodological innovation.\n\nThe model design lacks substantial novelty or clear insight into how segmentation and captioning are effectively integrated beyond simple module combination.\n\nThe experimental validation is insufficient and somewhat superficial; it primarily reports improvements on internal benchmarks without solid comparisons to recent or stronger baselines, like DAM[1]\n\nThe work does not provide a convincing analysis or explanation to justify the claimed synergy between segmentation and captioning modules.\n\n[1] Lian L, Ding Y, Ge Y, et al. Describe anything: Detailed localized image and video captioning[J]. arXiv preprint arXiv:2504.16072, 2025."}, "questions": {"value": "Missing comparisons with DAM, a relevant and competitive method in multimodal video understanding, undermines the fairness and completeness of the evaluation.\n\nLacks visual comparisons with other methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XgRhChWQ9O", "forum": "uLwuPbVegY", "replyto": "uLwuPbVegY", "signatures": ["ICLR.cc/2026/Conference/Submission16843/Reviewer_MS3b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16843/Reviewer_MS3b"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16843/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017103299, "cdate": 1762017103299, "tmdate": 1762926862803, "mdate": 1762926862803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}