{"id": "NTLZkcWSv5", "number": 491, "cdate": 1756742397734, "mdate": 1759898257619, "content": {"title": "BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities", "abstract": "Embodied capabilities refer to a suite of fundamental abilities for an agent to perceive, comprehend, and interact with the physical world.\nWhile multimodal large language models (MLLMs) show promise as embodied agents, a thorough and systematic evaluation of their embodied capabilities remains underexplored, as existing benchmarks primarily focus on specific domains such as planning or spatial understanding. To bridge this gap, we introduce BEAR, a comprehensive and fine-grained benchmark that evaluates MLLMs on atomic embodied capabilities. BEAR comprises 4,469 interleaved image–video-text entries across 14 domains in 6 categories, including tasks from low-level pointing, trajectory understanding, spatial reasoning, to high-level planning. Extensive evaluation results of 20 representative MLLMs reveal their persistent limitations across all domains of embodied capabilities. To tackle the shortfall, we propose BEAR-Agent, a multimodal conversable agent that integrates pretrained vision models to strengthen MLLM's perception, 3D understanding, and planning capabilities. It substantially enhances MLLMs’ performance across diverse embodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative improvement of 17.5% on GPT-5. Furthermore, our experiments indicate that enhancing MLLM's embodied capabilities can benefit embodied tasks in simulation environment.", "tldr": "a multimodal language model benchmark on embodied capabilities and a multimodal conservable embodied agent", "keywords": ["MLLM benchmark and evaluation", "Multi-modal Large Language Models", "Embodied AI", "Vision Language Models", "Embodied agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11cac8e8627c837dfd25716f630edda1201200ef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BEAR, a comprehensive benchmark designed to evaluate MLLMs on atomic embodied capabilities, which are essential for interacting with the physical world. BEAR comprises 4,469 interleaved image-video-text entries across 6 categories and 14 domains, and tests skills ranging from low-level pointing and spatial reasoning to high-level planning. Extensive evaluations of 20 MLLMs revealed persistent limitations across all domains, with proprietary models significantly outperforming open-source counterparts. To address these shortcomings, the authors propose BEAR-Agent, a multimodal conversational agent that integrates pretrained vision models and tools, substantially enhancing MLLM performance on BEAR and demonstrating benefits for embodied tasks in simulation environments."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Introduces a comprehensive and unified benchmark (BEAR) structuring embodied capabilities into 6 categories and 14 atomic skills, featuring 4,469 diverse image-video-text samples curated from 13 sources.\n+ Employs clear, rule-based evaluation metrics (SR for most tasks, IoU for Bounding Box, full episode success for Long-horizon) and follows standard, reproducible protocols like VLMEvalKit with specified input settings.\n+ Features well-designed multiple-choice questions with systematic distractor control, including options like 'None of the above', balanced answer key distribution, and category-specific difficulty calibration.\n+ Provides robust baselines by reporting performance for 20 representative MLLMs, includes a human performance baseline on a subset, and rigorously investigates the impact of TTS strategies."}, "weaknesses": {"value": "+ The paper's definition of \"embodied capabilities\" may be an overclaim. By structuring these capabilities exclusively around the 14 proposed skills (as stated in the Abstract and the end of the Introduction), the benchmark overlooks other critical aspects of embodied agents. For example, it does not cover an agent's understanding of physical properties (e.g., texture, friction), its own embodiment awareness (e.g., embodiedment limitation), or the dynamics of the environment.\n\n+ While the engineering effort in creating a comprehensive benchmark is valuable, the insights derived from the evaluation (Section 3.2 and 3.3) seem to offer limited novelty. The main findings are largely confirmations of well-established observations in prior work: (1) MLLMs have limited embodied skills [1]; (2) proprietary models outperform open-source ones [1]; (3) TTS methods are not always helpful [1, 2, 3] ; (4) failures stem from poor visual grounding and spatial confusion [4, 5]. Therefore, in terms of novelty, the work is more accurately positioned as \"a comprehensive, engineeringly solid diagnostic benchmark\", rather than offering fundamentally new insights to the field.\n\n+ The inclusion of BEAR-Agent (Section 4) is questionable. The agent's ability to improve performance mainly reiterates the known effectiveness of such methods  (e.g., AutoGen, MM-ReAct, and Voyager, which are cited by the authors) without yielding new insights. Its inclusion feels disconnected from the primary contribution (the benchmark), making the paper less cohesive. The proposed agent framework would have served the paper better if it were framed as a case study demonstrating how the fine-grained diagnostics from the BEAR benchmark can directly guide targeted agent improvement.\n\n+ A typo: \"action undertanding abilities\" -> \"action understanding abilities\" (line 354)\n\n\n[1] Yang, Rui, et al. \"Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents.\" arXiv preprint arXiv:2502.09560 (2025).\n\n[2] Zawalski, Michał, et al. \"Robotic control via embodied chain-of-thought reasoning.\" arXiv preprint arXiv:2407.08693 (2024).\n\n[3] Yang, Jihan, et al. \"Thinking in space: How multimodal large language models see, remember, and recall spaces.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[4] Zhao, Xinxin, et al. \"Imaginenav: Prompting vision-language models as embodied navigator through scene imagination.\" arXiv preprint arXiv:2410.09874 (2024).\n\n[5] Huang, Chenguang, et al. \"Visual language maps for robot navigation.\" arXiv preprint arXiv:2210.05714 (2022)."}, "questions": {"value": "1. What construct-validity evidence supports this taxonomy? Do you plan to cover other core abilities, such as physics, embodiment, and dynamics?\n\n2. Beyond systematizing known observations, what new findings or mechanisms does BEAR reveal?\n\n3. Is BEAR-Agent a methodological contribution or a reference demo to showcase BEAR’s diagnostic value? If the former one, do you have per-tool ablations and cross-dataset results if it’s intended as a standalone contribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I4JHuPstva", "forum": "NTLZkcWSv5", "replyto": "NTLZkcWSv5", "signatures": ["ICLR.cc/2026/Conference/Submission491/Reviewer_RzPp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission491/Reviewer_RzPp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760837427992, "cdate": 1760837427992, "tmdate": 1762915530144, "mdate": 1762915530144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a comprehensive benchmark of 4,469 interleaved image–video–text samples that categorizes embodied capabilities into 6 categories and 14 atomic skills, enabling fine-grained evaluation of MLLMs. Based on the BEAR benchmark, the authors evaluated popular MLLMs and identify their limitations. Moreover, they also propose BEAR-Agent, a conversable tool-augmented pipeline that can boost the MLLM performance for embodied tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The benchmark is well-structured and comprehensive, enabling fine-grained assessment of embodied capabilities.\n\n* The evaluation presents insightful test-time/model-size scaling studies that yield clear takeaways.\n\n* The proposed agent framework substantially improves MLLM agents on embodied tasks, particularly in perception and knowledge integration."}, "weaknesses": {"value": "* Though VQA is easy for evaluating MLLMs, embodied agents are inherently designed for interaction, which differs from answering questions or multiple choice; an additional experiment validating the correlation between BEAR performance and execution-based benchmarks (e.g., EmbodiedBench) would be helpful.\n\n* The long-horizon tasks currently include only 35 trajectories, which is small and may not cover sufficient task diversity and used skills. The authors can consider increasing this subset, for example by modifying trajectory datasets from prior work such as ALFRED.\n\n* For the model-size scaling results, I don’t agree with the conclusion that “embodied capabilities do not scale with model size”; the results appear to show an improving trend with an outlier (InternVL2), and the current points are limited. Adding more model families and sizes would make the conclusion more convincing.\n\n* For the “number of frames” in Figure 5, does this indicate you downsampling a video before sending it to the model while keeping the same first and the last frames? Please make this clearer.\n\n* For BEAR-Agent, can the authors show how much of the gain comes from each tool for different types of tasks (e.g., perception, spatial reasoning, knowledge-related questions)?"}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "onFIOWXLRe", "forum": "NTLZkcWSv5", "replyto": "NTLZkcWSv5", "signatures": ["ICLR.cc/2026/Conference/Submission491/Reviewer_ZQgK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission491/Reviewer_ZQgK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761464221462, "cdate": 1761464221462, "tmdate": 1762915529949, "mdate": 1762915529949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BEAR, a fine-grained VQA-style benchmark for embodied capabilities, and BEAR-Agent, a tool-augmented multimodal agent that addresses perception and spatial reasoning gaps. The benchmark decomposes long-horizon tasks into six categories and fourteen step-wise skills for diagnostic evaluation. Experiments across diverse models show persistent limitations and demonstrate that BEAR-Agent delivers notable improvements on BEAR with transfer to simulated manipulation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a comprehensive, fine-grained benchmark, conducts rigorous cross-model evaluations, and introduces a well-engineered agent that delivers transferable gains.\n2. The paper is well-written, with clear figures and detailed explanations that make the work easy to follow.\n3. The evaluation clarifies current MLLM limitations in global perception and 3D spatial reasoning, helping steer research directions."}, "weaknesses": {"value": "1. Although I appreciate the author's considerable efforts in this work, the motivation behind the benchmark design remains unclear to me. The author has not explained what additional benefits the introduction of step-wise skills provides. Furthermore, in Fig. 1, it is unclear what the fundamental difference is between the skills required by an embodied agent for 'spatial relationship pointing' versus 'relative direction' in spatial reasoning.\n\n2. The paper claims that its improvement over EmbodiedBench lies in decomposing each task into step-wise skills. However, the experiments do not provide a separate ablation for each skill. The experimental findings are very similar to those of EmbodiedBench and its related works, failing to reveal any novel insights or problems discovered by this benchmark.\n\n3. BEAR-Agent relies on category-specific prompt routing, extensive notebooks and knowledge bases, making it tightly coupled to task templates and raising concerns about generalization beyond BEAR-style tasks."}, "questions": {"value": "1. Regarding the performance with CoT, the authors claim that CoT offers limited and sometimes even negative improvements in performance. This finding is highly counterintuitive. Which prompting did the authors use? Could this conclusion be attributed to the use of overly long or inappropriate prompting?\n\n2. It would be better if the authors provide ablations on some key step-wise skills and reveal some more critical findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "58Kedyl2KY", "forum": "NTLZkcWSv5", "replyto": "NTLZkcWSv5", "signatures": ["ICLR.cc/2026/Conference/Submission491/Reviewer_EyWH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission491/Reviewer_EyWH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902968843, "cdate": 1761902968843, "tmdate": 1762915529823, "mdate": 1762915529823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an embodied benchmark that fine-grainedly evaluates MLLMs’ capabilities in Pointing, Bounding Box, Trajectory Reasoning, Spatial Reasoning, and Task Planning. Through these tasks, it analyzes how different factors such as cot, model size, and test-time scaling affect model performance. Based on this analysis, it introduces BEAR-Agent, a multimodal conversable agent that provides MLLMs with a set of visual and spatial tools to enhance their embodied reasoning abilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well structured and easy to follow. The benchmark design, experiments, and analysis are presented in a clear and logical way.\n2. The work introduces a fine-grained benchmark that provides a systematic way to study embodied reasoning ability of MLLMs.\n3. Based on the performance analysis, the work builds a conversable agent that helps MLLMs in embodied reasoning task with different visual tools, which is insightful."}, "weaknesses": {"value": "1. The paper’s main contribution (bear and bear-agent) appears to integrate existing concepts rather than introduce fundamentally new methods. The benchmark consolidates prior embodied evaluation efforts (e.g., EmbodiedBench, PhysBench, MotionBench) under a unified taxonomy, while bear-agent mainly combines established tools like GroundingDINO and DepthAnything through conversational prompting. The resulting work feels more like a well-executed synthesis than a conceptual or technical breakthrough.\n\n2. The analysis remains largely empirical and does not deeply examine why current MLLMs lack embodied capabilities. The discussion attributes failures to “omni-visual bottlenecks” or “spatial misalignment,” but there is no investigation from model-level perspectives such as attention visualization, architectural analysis, or representational probing. A deeper understanding of these mechanisms would strengthen the paper’s explanatory power.\n\n3. Bear-Agent relies heavily on prompting and external tool invocation rather than any learning-based or adaptive process. While this design demonstrates practical improvements, it limits generalization and scientific insight. Introducing learning mechanisms such as fine-tuning, feature alignment, or reinforcement-based adaptation could make the approach more principled and extend its utility beyond handcrafted prompting workflows."}, "questions": {"value": "1. The paper proposes BEAR-Agent based on the failure modes of MLLMs in BEAR and proves its success in ManiSkill tasks, which is good. Then, can we directly use BEAR to fine-tune MLLMs and improve their performance in generalized settings like ManiSkill or real robots? I would welcome new experiments exploring this.\n\n2. It’s a little bit counterintuitive that MLLMs’ performance in BEAR doesn’t improve with model size. Most of the time, I would think this is due to unclear or suboptimal prompts. Can the authors test more prompt variants and report the results (mean, std, etc)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NdvZBsAzAE", "forum": "NTLZkcWSv5", "replyto": "NTLZkcWSv5", "signatures": ["ICLR.cc/2026/Conference/Submission491/Reviewer_BnCS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission491/Reviewer_BnCS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950077878, "cdate": 1761950077878, "tmdate": 1762915529724, "mdate": 1762915529724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}