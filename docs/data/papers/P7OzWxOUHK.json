{"id": "P7OzWxOUHK", "number": 585, "cdate": 1756750492595, "mdate": 1759898252073, "content": {"title": "OneFlowSeq: Achieving One-Step Generation for Diffusion Language Models via Lightweight Distillation", "abstract": "Autoregressive models dominate Seq2Seq generation but suffer from slow, error-prone token-by-token decoding. Diffusion language models (DLMs) enable parallel refinement and global coherence, yet their iterative denoising requires hundreds of steps, limiting practicality. We propose **OneFlowSeq**, a novel framework that distills a powerful multi-step diffusion teacher (LLaDA-8B-Instruct) into a one-step generator via MeanFlow-based supervision and parameter-efficient prompt tuning. Our OneFlowSeq introduces a Jacobian-vector product signal that provides richer guidance than conventional distillation, allowing the student to not only match the 128-step teacher in terms of one-step generation quality. Experiments on paraphrasing, text simplification, and question generation benchmarks show that OneFlowSeq achieves state-of-the-art performance, while reducing trainable parameters by 1600$\\times$ and delivering inference speeds orders of magnitude faster than both autoregressive and multi-step diffusion baselines. This work establishes one-step diffusion as a practical and scalable paradigm for Seq2Seq generation.", "tldr": "OneFlowSeq distills a multi-step diffusion teacher into a one-step generator via MeanFlow and Jacobian signals, achieving SOTA Seq2Seq quality with fewer params, lower cost, and orders faster inference.", "keywords": ["Diffusion Language Models; Seq2Seq"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/158b864fd6a01b7e15d6a4eed66eb6e8e9fbb945.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "TThis paper proposes OneFlowSeq, a novel distillation framework for diffusion language models that resolves the speed-quality trade-off. It distills a multi-step teacher model into a lightweight, parameter-efficient student by using MeanFlow theory and a Jacobian-vector product signal for superior guidance. This enables the student to match the teacher's quality in a single generation step, achieving state-of-the-art results with a 1600x reduction in trainable parameters and a 160x inference speedup, facilitating scalable deployment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method achieves state-of-the-art performance in benchmark datasets such as QQP, Wiki-Auto, and Quasar-T for tasks like text simplification, question generation, and paraphrase detection. \n\n2. It significantly reduces the number of parameters required compared to previous methods while maintaining high inference speed."}, "weaknesses": {"value": "1.  Novelty: The paper's idea is very straightforward, amounting to a simple application of MeanFlow, which does not meet the standard for ICLR.\n\n2.  Experimental Results: The paper only evaluates a few simple text generation tasks and fails to include datasets like MMLU, which the base LLaDA model was evaluated on and are necessary to demonstrate the capabilities of large language models.\n\n3.  Experimental Baselines: Since the paper chose a soft-prompting approach, other parameter-efficient fine-tuning (PEFT) methods, such as LoRA, should have been evaluated as baselines."}, "questions": {"value": "NA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WmMuBcUvr8", "forum": "P7OzWxOUHK", "replyto": "P7OzWxOUHK", "signatures": ["ICLR.cc/2026/Conference/Submission585/Reviewer_cC1A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission585/Reviewer_cC1A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989505684, "cdate": 1761989505684, "tmdate": 1762915553615, "mdate": 1762915553615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OneFlowSeq, a distillation framework for turning a multi-step diffusion language model (LLaDA-8B-Instruct) into a one-step generator.\nThe method combines MeanFlow-based distillation with an additional Jacobian-vector product (JVP) supervision term to approximate the teacher’s flow direction, while keeping the backbone frozen and training a lightweight soft prompt (~5M parameters).\nExperiments on paraphrasing (QQP), text simplification (WikiAuto), and question generation (Quasar-T) claim comparable quality to 128-step diffusion generation and large inference speedups."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Tackles an important issue: making diffusion-based LMs practical by reducing multi-step inference to one step.\nElegant integration of MeanFlow and JVP supervision; clearly described training process."}, "weaknesses": {"value": "* Unfair and inconsistent baselines:\n- Teacher: LLaDA-8B-Instruct (8B parameters, pretrained on trillions of tokens).\n- Baselines: GPT-2 (1.5B), DiffuSeq (trained from scratch on small datasets), and DLM-One (re-implemented).\n- These baselines are not comparable in capacity and not pretrained on the similar size datasets, giving OneFlowSeq an unfair advantage.\n\n\n* Weak tasks and evaluation design:\n- Evaluations (QQP, WikiAuto, Quasar-T) involve short and easy sequences (1–2 sentences).\n- Missing evaluations on complex, long-form tasks like summarization (CNN/DailyMail, XSum) or reasoning datasets.\n- Results cannot demonstrate scalability or compositional generalization of the proposed method.\n\n* Missing strong baselines and ablations:\n- No comparison with Fast-DLLM, Block Diffusion, Consistency Models, or Rectified Flow Transformers.\n\n* Questionable efficiency and generality:\n- Reported 160× speedup is achieved with batch size 256 for OneFlowSeq vs. batch size 1 for AR models - not a fair per-sample measure.\n- Inference cost remains dominated by the 8B teacher backbone.\n- No results for real-time latency, FLOPs, or wall-clock performance on similar setting\n\n* Limited insight into JVP supervision:\n- The JVP term is presented as novel but is widely used in flow-matching and consistency distillation.\n- Its role in improving one-step alignment is not clearly isolated; improvements might stem from regularization rather than Jacobian matching."}, "questions": {"value": "How are the baselines pretrained as well? Do they have the same scale as LLaDA-8B?\nCan you provide results on longer, more complex tasks such as summarization or story generation?\nWhat is the true per-sample latency under equal hardware and batch size settings?\nWhy were recent consistency or flow-based distillation models not included as baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4vdYYMphR4", "forum": "P7OzWxOUHK", "replyto": "P7OzWxOUHK", "signatures": ["ICLR.cc/2026/Conference/Submission585/Reviewer_LZQF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission585/Reviewer_LZQF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027950241, "cdate": 1762027950241, "tmdate": 1762915553462, "mdate": 1762915553462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for distilling a text diffusion model into a model capable of predicting all tokens in parallel, using a second-order training objective.  The student model learns a parameter-efficient soft prompt model which adapts a copy of the teacher model for this objective.  Empirical results show that this method is effective for low-entropy conditional text generation, with much improved generation speed."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a surprisingly effective way to learn a model which does efficient non-autoregressive text generation.  Relevant ablations are also done."}, "weaknesses": {"value": "The model is based largely on the MeanFlow model, so the novel contribution is mostly in how to exploit this insight in an effective distilled model, and in the empirical results.  I found it impossible to understand the technical details without already understanding the MeanFlow model.\n\nThe model is only evaluated in low-entropy conditional generation tasks.  This makes sense, since one-shot text generation is presumably impossible in high-entropy tasks because of the multi-modal nature of the output distribution.  But they never evaluate or discuss this limitation.  Some design choices, such as lines 259-260 \"This intermediate decoding can be simplified by directly feeding the continuous embeddings corresponding to z_t_i into the model\", only make sense for low-entropy tasks.\n\nThe presentation of the model could be better.  The student model is a PEFT version of the teacher model (if I understand correctly), but they talk about the student model as if it consists only of the adaptation parameters without hardly mentioning that the student also includes a huge model with frozen parameters.  This is especially confusing because the teacher is also the same model also with frozen parameters."}, "questions": {"value": "For the student model, why use prompt tuning?  Have you tried more effective/efficient PEFT methods like LoRA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kjPvCTNSd9", "forum": "P7OzWxOUHK", "replyto": "P7OzWxOUHK", "signatures": ["ICLR.cc/2026/Conference/Submission585/Reviewer_mrfh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission585/Reviewer_mrfh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185265857, "cdate": 1762185265857, "tmdate": 1762915553296, "mdate": 1762915553296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}