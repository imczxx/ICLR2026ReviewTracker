{"id": "ewdqbKskUL", "number": 21668, "cdate": 1758320316463, "mdate": 1759896909652, "content": {"title": "Answer-Set Consistency of LLMs for Question Answering", "abstract": "Large Language Models (LLMs) sometimes contradict themselves when answering factual questions, especially when asked to enumerate all entities that satisfy the question. We formalize such self-contradiction as answer-set inconsistency: given two enumeration questions whose answers satisfy a set-theoretic relation (equivalence, disjointness, containment, etc.), the LLM generates responses violating the relation. To diagnose this phenomenon, we create a benchmark dataset comprising tuples of enumeration questions over which a variety of set-theoretic relations hold, and propose related metrics to quantify answer-set inconsistency. Our evaluation of several state-of-the-art LLMs reveal pervasive inconsistency across models, even in cases where the LLM can identify the correct relation. This leads us to further analyze potential causes and propose mitigation strategies wherein the LLM is prompted to reason about such relations before answering, which lead to improved answer-set consistency. This work thus provides both a benchmark and a systematic approach for evaluating, explaining, and addressing answer set inconsistency in LLM question answering, towards deriving practical insights to improve the reliability of LLMs.", "tldr": "We identify answer-set inconsistency in LLMs, provide a benchmark to evaluate this phenomenon, and propose mitigation strategies.", "keywords": ["LLMs", "Question Answering", "Consistency"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74b87d20cf629792c038695e6104fad2e1d64405.pdf", "supplementary_material": "/attachment/19f708bff09b4dd1dc1df792dcd808fe0b378e2e.zip"}, "replies": [{"content": {"summary": {"value": "This paper formalizes the problem of answer-set inconsistency in large language models (LLMs) for factual enumeration questions,\nwhere LLMs generate responses that violate set-theoretic relations (e.g., equivalence, containment, disjoint-ness) between questions. The paper proposes a benchmark dataset (ASCB) with 600 handcrafted question quadruples (2,400 questions) and evaluate 18 state-of-the-art LLMs. Key contributions include a novel theoretical framework, comprehensive metrics, and mitigation strategies that significantly improve consistency. Experiments demonstrate pervasive inconsistency across models, even when they correctly recognize relations, with mitigation\nstrategies achieving statistical significance (p < 0.001)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper introduces a rigorous theoretical foundation for answer-set consistency, formalizing it using set-theoretic relations\n(Section 3.1). This includes definitions for consistency and contradictions, enabling a sampling-based evaluation approach\nwith error guarantees (e.g., control relation R* for stochasticity analysis in Section 3.4). The framework bridges LLM behavior with\ndatabase-like query containment principles, advancing beyond prior work focused on single-answer consistency.\n\nS2. The empirical analysis is extensive, covering 18 LLMs (e.g., DeepSeek, Grok, Mistral, GPT, Gemini, Llama families) across\nmultiple tasks (Base evaluation, CtE, Oracle). Evaluation metrics (Consistency rates, Jaccard similarity) and statistical testing\n(McNemar test) reveal pervasive inconsistency.\n\nS3. The mitigation strategies (e.g., CtE) of inconsistency are practical and only require minimal prompt engineering, making\nthem accessible for real-world applications."}, "weaknesses": {"value": "W1. The proposed ASCB dataset is limited to 600 quadruples (2,400 questions) in English, focusing on static, factual domains.\nThis scale may not generalize to dynamic environments or high stakes applications requiring larger scales datasets. The manual\ncuration, while ensuring quality, restricts diversity and scalability.\n\nW2. Based on KGQA datasets, the experiments are conducted in isolated, single-turn contexts, ignoring real-world scenarios like\nmulti-turn dialogues. This limits the validity of consistency claims for interactive systems, where temporal dynamics might\nexacerbate inconsistencies.\n\nW3. While the paper highlights stochasticity as a cause of inconsistency in Section 3.4 and 4.2, it lacks accurate error reason analysis (e.g., entity-level semantic misunderstanding or knowledge gaps). For example, contradictions in Appendix F (Table 7) are not decomposed into specific error types, lacking of guidance to extend datasets and hindering targeted model improvements based on prompt strategy.\n\nW4. The paper describes different relations (including primary and implied relations) in Table 2. However, there is an unclear\nexplanation about modeling 12 pairs of questions in Section 3.2 and even why paper chooses five primary relations (R1-R5) in\nSection 3.3."}, "questions": {"value": "Limitations in dataset diversity, dynamic scenario evaluation, and error analysis reduce its generalizability. See the weaknesses (W1-W4) above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lIKhttb4YU", "forum": "ewdqbKskUL", "replyto": "ewdqbKskUL", "signatures": ["ICLR.cc/2026/Conference/Submission21668/Reviewer_ykMU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21668/Reviewer_ykMU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685305407, "cdate": 1761685305407, "tmdate": 1762941882988, "mdate": 1762941882988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is concerned with answer-set consistency of LLMs during question answering. When asked an *enumerative* question, the LLMs answers with a set. LLMs may contradict themselves when enumerating all entities satisfying the question. To this end, the authors consider consistency evaluation from a set-theoretic perspective, by checking equivalence, disjointness, containment of set operations, etc.\n\n\nThe paper proposes a benchmark, and reveal inconsistencies of existing LLMs on answer-set setting."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The setup is well motivated, since LLMs may be inconsistent when asked to enumerate all entities in a set-focused question. The related work is explained well, with prior studies' definitions of inconsistencies. Empirical results are extensive, covering many models, and showing that they are not yet fully consistent during question answering. Several parts of the paper can be improved, as explained below."}, "weaknesses": {"value": "- Section 3 requires rewriting. Mathematical notations could be simplified (e.g., table 2 should be as simple as possible). Some notations do not look standard, or not explained well. Since the authors consider a finite number of set operators, it is better to use in a *case by case* what it means to achieve consistency under equivalence, containment, disjointness or overlap. The notations for disjointness and overlap are mixed with Boolean True or False -- I have never seen these notations before. In summary, the consistency criteria should be explained in an intuitive manner, with illustrative examples.\n\n- There is no discussion on what prompts are given to the LLM and how answers are parsed -- they have major influence on the evaluation of inconsistency.\n\n- The mitigation strategy has a *conversational* aspect, which may improve consistency in a surface level, i.e., in-context, and not to the parametric knowledge of the LLM. What happens when the LLM does not know the enumerative answer?\n\n- The construction of the dataset is explained in a complicated manner. Why there are quadruples in line 202, and onwards?\n\n- Several sentences are repeated multiple times, but demonstrative examples are missing. Examples are line 182 to 185. Put differently, since the objective is to improve consistency, it is better to show examples of consistency/inconsistency without being too abstract in notations (refer to Ghosh et at (2025) for inspiration)."}, "questions": {"value": "Please address the points mentioned in the weakness.\n\nI am willing to increase score after rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f5Jl5Ox7B2", "forum": "ewdqbKskUL", "replyto": "ewdqbKskUL", "signatures": ["ICLR.cc/2026/Conference/Submission21668/Reviewer_Cckv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21668/Reviewer_Cckv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877642942, "cdate": 1761877642942, "tmdate": 1762941882627, "mdate": 1762941882627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce answer-set inconsistency, in which large language models (LLMs) can produce contradictory answers to enumeration questions that should obey set-theoretic relations.\nIn this paper, the authors have focused on the set relationship such as equality, containment, and disjointness between question-answer sets.\nTo evaluate it, they constructed the Answer-Set Consistency Benchmark (ASCB) containing 600 handcrafted quadruples (a total of 2,400 questions) derived from knowledge-graph QA datasets.\nThree approaches are defined 1) Base, 2) Classify-then-Enumerate (CtE), and 3) Oracle and these approaches are used to assess both relation classification and consistency of generated answers.\nExperiments on 18 modern LLMs, including GPT-5, Gemini-2.5, and Llama-3, reveal pervasive inconsistency even under low temperature (greedy sampling) settings.\nContainment and multi-set relations are found to be the most difficult, while equivalence and disjointness are easier.\nThe CtE strategy, which prompts the model to reason about relations before answering, significantly improves consistency across all models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper clearly formalizes an important concept, LLM answer-set consistency. This concept is quite interesting and has potential to provide a principled framework for LLM to be more logical coherent.\n\nS2. The empirical analysis across 18 modern models and the 2400 questions dataset are valuable contributions to understand LLM behaviours."}, "weaknesses": {"value": "W1. The proposed mitigation strategies, such as Classify-then-Enumerate, are relatively straightforward and simple. It would be great if the authors can provide more fundamental algorithmic improvements or mitigation strategy. \n\nW2. While the paper identifies sources of inconsistency, it provides limited theoretical analysis or deeper causal explanation beyond empirical observation.\n\nW3. The symbols in this paper feels a bit overloaded. $Q_i$, $RQ_i$, $R_i$. Maybe create a symbol table for line 280-285 such that the readers can find the meaning of the different relationship easier. \n\nComment: The author may find the following papers related to set/list semantic equivalence to be interesting: https://arxiv.org/abs/2312.10321 and https://arxiv.org/abs/2502.12466."}, "questions": {"value": "Please see W1-W3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jv9PwIrVYk", "forum": "ewdqbKskUL", "replyto": "ewdqbKskUL", "signatures": ["ICLR.cc/2026/Conference/Submission21668/Reviewer_RNjR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21668/Reviewer_RNjR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887738831, "cdate": 1761887738831, "tmdate": 1762941882218, "mdate": 1762941882218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to study a novel concept of answer-set consistency in large language models (LLMs), which refers to whether an LLM's responses to related enumeration questions respect expected set-theoretic relations such as equality, containment, and disjointness. The authors introduced a new benchmark with 600 handcrafted quadruples of logically related questions and used it to evaluate 18 contemporary LLMs under several prompting strategies. As evaluating, explaining, and addressing answer-set inconsistency in LLMs' question-answering capability has immense practical value, the paper is quite valuable in this regard."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novelty:** The paper makes a novel contribution by formalizing the concept of answer-set inconsistency for enumeration questions. This establishes a new, well-defined dimension for evaluating LLM's reliability based on set-theoretic relations. \n\n2. **Thorogh empirical analysis**: I appreciate the paper's comprehensive and systematic empirical analysis, which helps understand the nature of answer-set inconsistencies across a wide variety of LLMs."}, "weaknesses": {"value": "1. **Objectivity of some questions in the benchmark**: Some of the questions in the dataset are not objective. \nFor instance, “On what video streaming services can I watch the Hunter x Hunter anime series?”-it depends on the region or country. \n\n2. **Incompatiblity of Problem formulation and benchmark**: In section 2.1, the authors assume E to be \"the universe of entities from an arbitrary domain\" from where both questions and answers originate.  This formulation implicitly assumes an open-world or extensible entity space -- that is, E could, in principle, include all possible entities that exist within a domain, even those not observed in the dataset. Under this assumption, answer-set consistency is defined in an idealized, domain-agnostic logical sense. But, in practice, the ASCB benchmark is constructed from knowledge graphs (KGs) such as Wikidata and DBpedia, which operate under a closed-world assumption. Meaning,\nthe benchmark enumerations are derived only from entities present in the KG; Missing facts or unobserved entities are treated as false rather than unknown; Consequently, the \"universe\"  E is finite and closed to the KG’s vocabulary. This means that while the paper’s definitions rely on the notion of “true set relations within an arbitrary domain,” the evaluations measure consistency only relative to KG-encoded truth -- not the broader, theoretically open universe.\n\n3. **Lacking discussion on Synthetic**: Please discuss in detail the construction process of SYNTHETIC, in particular, what the prompts were, and how you came up with the questions in the first place.\n\n4. **Deeper understanding of the results is missing:** Table 3, ~R4 column => Why CtE and Oracle are underperforming (lower score) compared to Base in many models? You identified this issue in Appendix E, but are there some inherent limitations to your mitigation strategies on disjoint set queries?  We also see this problem on GPT5 ~R5, CtE. In addition, we also see that on %R5, CtE strategy makes GPT-4.1-nano, and GPT-5-nano both perform worse. What are the reasons behind this?"}, "questions": {"value": "See W2 and W4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CMt8uIC4XE", "forum": "ewdqbKskUL", "replyto": "ewdqbKskUL", "signatures": ["ICLR.cc/2026/Conference/Submission21668/Reviewer_KUx5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21668/Reviewer_KUx5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762356461548, "cdate": 1762356461548, "tmdate": 1762941881783, "mdate": 1762941881783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}