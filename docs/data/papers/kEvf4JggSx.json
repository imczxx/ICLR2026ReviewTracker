{"id": "kEvf4JggSx", "number": 8649, "cdate": 1758093615773, "mdate": 1759897771795, "content": {"title": "UniRA: Unified Representation Alignment for Diffusion Models via Local, Structural, and Global Constraints", "abstract": "Diffusion models have achieved tremendous advancements in generative modeling generation, enabling appealing experiences in visual content generation. Yet, their conventional training objective focuses merely on predicting added noises, without any explicit consideration on the learning of intermediate features. This narrow focus might learn redundant representations that capture limited semantics and poor structural details, thus leading to suboptimal performance. To ameliorate this, this paper proposes a unified representation alignment (UniRA) paradigm that augments the diffusion objective with explicit constraints on enhancing intermediate features. Specifically, UniRA enforces three complementary forms of alignment: local semantic fidelity for discriminative patch-level features, structural consistency to preserve relational organization, and global coherence to match overall feature distributions with real data. Extensive results on the challenging ImageNet and text-to-image benchmarks show that UniRA consistently improves convergence speed and synthesis performance, gaining improved FID and precision/recall scores under the same compute budget with compared baselines. Moreover, ablative analysis demonstrate the efficacy of UniRA in reducing feature redundancy and strengthening semantic information, and improving structural organization, thereby promoting high-quality synthesis.", "tldr": "", "keywords": ["Diffusion models", "Representation alignment", "Generative modeling", "Semantic fidelity", "Structural consistency"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/550ae8b92d1508d9a5cfac689eabd12cf3700875.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an aligment method of diffusion internal representation to the represetntation space of an existing pre-trained vision encoder model. The method has three alignment losses - one is local (and is like an existing methods called REPA) where internal patch representations are made to be similar to their equivalent patches in the pre-trained vision model. The other is structural where the patch-wise similarity structure of the represtentation is made to be similar to the pre-trained encoder. Finally, an global loss is proposed by training a discriminator which tries to distinguish internal diffusion representations and the pre-trained represetnation, pooled globally across the image."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This well executed paper has several strengths.\n\n* The core idea, especially the structural loss, are very interesting and cleverly use the known strengths of pre-trained encoders to shape the internal representation of the diffusion model.\n* The paper is nicely written and well structured - an enjoyable read.\n* experimental validation is very good - including baselines, ablations and qualitative analysis."}, "weaknesses": {"value": "I think the main issue the paper suffers is its relative limited significance. As much as I enjoyed the paper, most of the improvement comes from the local loss which was already proposed in REPA. I actually think this is not a reason to reject the paper, but it does diminish the scope and impact of the paper.\n\nMinor points:\n\n* I would have loved to see more discussion why MAE and other models perform worse here than DINO v2. This is just visible in the supplementary but I think is an important question.\n* Would we see the same levels of improvements with other image datasets, considering DINO was trained very much in light of ImageNet."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "82fLcxhn2M", "forum": "kEvf4JggSx", "replyto": "kEvf4JggSx", "signatures": ["ICLR.cc/2026/Conference/Submission8649/Reviewer_cW3w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8649/Reviewer_cW3w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574717747, "cdate": 1761574717747, "tmdate": 1762920471883, "mdate": 1762920471883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UniRA, a unified representation alignment framework for diffusion models that enhances intermediate feature representations through three complementary constraints: 1) Local semantic alignment with pretrained visual encoders (e.g., DINOv2), 2) Structural consistency via relational similarity matching, and 3) Global distributional coherence using a lightweight adversarial discriminator.\nThe method encourages the denoiser’s internal features to be semantically rich, spatially coherent, and distributionally well-structured instead of focusing on output-space noise prediction.\n\nExperiments on ImageNet-256/512, MS-COCO text-to-image, and multiple DiT/SiT architectures show that UniRA:\n- Improves FID and IS over REPA and base diffusion transformers\n- Produces more expressive and less redundant representations"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- UniRA unifies three alignment levels (local, structural, global) into a simple, modular framework applicable to any diffusion transformer.\n- Consistent FID/IS improvements across resolutions (256 / 512) and architectures (DiT, SiT) with faster convergence\n- Includes text-to-image (MMDiT), ablations on alignment components (Table 4), weight sensitivity (Table 5), and encoder types/sizes (Table A2)\n- Correlates FID with probe accuracy, shows layer-wise semantic improvement, timestep robustness, and reduced feature redundancy (Fig. 6).\n- Fig. 5 clearly visualize restored spatial organization and semantic locality; generated samples (Figs. 3–4, Appendix) are high quality.\n- Improves both efficiency and fidelity while remaining architecture-agnostic"}, "weaknesses": {"value": "- Builds directly on REPA, extending from local to multi-level alignment rather than introducing a fundamentally new mechanism.\n- Performance relies on DINOv2-like teachers; the method is less self-contained and may struggle when domain shift breaks encoder semantics.\n- The adversarial (global) term is said to be \"optional\" yet quantitative analysis of its stability or cost is minimal.\n- While feature quality improves, how this affects bias or semantic controllability isn’t explored.\n- The paper motivates alignment intuitively but provides little formal connection between improved intermediate representations and diffusion ELBO."}, "questions": {"value": "1. How sensitive is UniRA to the choice of alignment depth (e.g., 4th vs 8th layer) beyond Table A2?\n2. Did you experiment with adaptive weighting for $\\lambda$, $\\beta$, $\\gamma$ during training (e.g., curriculum)?\n3. Could UniRA be combined with self-distilled encoders (no frozen teacher) to mitigate reliance on external pretrained models?\n4. For global alignment: how is discriminator stability ensured, and does adversarial collapse ever occur?\n5. Would alignment at multiple timesteps (not only t=0.5) further improve robustness?\n6. Please fix a small typo on line 721 on Table A1, lr row (010001).\n7. Please add runtime/compute analysis. Since efficiency is a major claim, show training wall-time or FLOPs comparison with REPA.\n8. Could you please clarify failure cases or visual artifacts from over-alignment (loss of diversity)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zQ7vfH0TSf", "forum": "kEvf4JggSx", "replyto": "kEvf4JggSx", "signatures": ["ICLR.cc/2026/Conference/Submission8649/Reviewer_b2RM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8649/Reviewer_b2RM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830199634, "cdate": 1761830199634, "tmdate": 1762920471472, "mdate": 1762920471472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UniRA, a unified representation alignment paradigm for diffusion models that augments the standard denoising objective with three explicit constraints on intermediate representations: local semantic alignment (patch-level matching with pretrained encoders), structural consistency (similarity matrix matching to capture relational organization among patches), and global distributional coherence (adversarially aligning the pooled intermediate features). Experiments on ImageNet and text-to-image generation benchmarks show consistent improvements in sample quality metrics (FID, IS, precision/recall), with additional ablations and analyses demonstrating reduced feature redundancy and improved semantic fidelity over strong baselines such as REPA and SiT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clarity & transparency. The paper is clearly written with transparent method details; the appendix enumerates objectives, hyperparameters, and ablations; comparisons to strong baselines (e.g., REPA) are careful.\n- Consistent empirical gains.Across challenging settings the method improves standard metrics; e.g., on ImageNet-256 with SiT-L/2, FID drops from 10.0 (REPA) to 8.5 (UniRA), with similar gains on image and text-to-image benchmarks.\n- Reproducibility. Implementation choices and hyperparameter ranges are documented (e.g., Table A1), facilitating fair evaluation and reproduction."}, "weaknesses": {"value": "- Lack of novelty--“unified” story is under-justified. Each sub-objective has prior art; the main contribution is a combination/recipe. The paper lacks a compelling argument for why these three must be unified and why these specific instantiations are preferable to plausible alternatives (e.g., replacing structural alignment with multi-scale contrastive losses, or global adversarial matching with MMD/SWD).\n- Weak theory for multi-objective trade-offs. The three losses may conflict; current tuning relies on grids/heuristics, without principled weighting, curriculum, or adaptive schemes.\n- Diversity–fidelity trade-off possibly obscured. Strong representation alignment can suppress diversity; the paper lacks systematic precision–recall curves or coverage metrics, and FID/IS alone can be misleading.\n- Teacher dependence and domain-mismatch risk. Reliance on a frozen external encoder (e.g., DINOv2) can propagate teacher biases; performance under teacher–data mismatch is unclear."}, "questions": {"value": "- If the global adversarial term is replaced by MMD/SWD/CLIP-score alignment, or the structural term by InfoNCE/multi-scale contrastive objectives, how do results change? What evidence shows this triad is not interchangeable?\n- Under noise/occlusion/style perturbations to the teacher—or when swapping to weaker or narrower-domain encoders—what are the degradation curves of the three alignment terms? Any observable signs of overfitting to teacher features?\n- Have you measured pairwise gradient angles/alignment between the three losses? Do you observe phases where one term dominates and others yield negative marginal returns late in training?\n- Under strict parity of training steps × FLOPs × memory, how do UniRA, REPA, SiT, and other distillation-style methods compare in final metrics and convergence speed? Please report wall-clock and GPU hours."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xrGBCkHBQx", "forum": "kEvf4JggSx", "replyto": "kEvf4JggSx", "signatures": ["ICLR.cc/2026/Conference/Submission8649/Reviewer_D6ZQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8649/Reviewer_D6ZQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977936911, "cdate": 1761977936911, "tmdate": 1762920471129, "mdate": 1762920471129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents UNIRA, a method that enforces representation alignment while training diffusion transformers. On contrast to the previous work REPA, that introducted representation alignment, UNIRA performs alignment in terms of local semantic fidelity, structural cohernace and global coherence leading to better representation alignment between features of the diffusion transformer and the pretrained vision encoder leading to better intermediate features in the diffusion models that can generalize to discriminative tasks. Experiments show that UNIRA outperforms REPA for generation on ImageNet 1M dataset as well as the features potaying better performance for discriminative tasks like classification on ImageNet"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper points out potential drawback in patch level similarity based alignment leading to loss of structural and distribution information loss\n2. Extensive experiments are performed for generative discriminative tasks and extensive ablation studies are performed to show that the performance boosts by utilizing UniRA.\n3. The PCA analysis shows the improvement in representation quality for semantic segmentation of object with respect to REPA\n4. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. Aligning the distributions strongly with a stronger distribution alignment function seems like a natural design choice for boosting performance. From the methodological perspective, what is the difference of the approach from REPA other than additional loss functions in the latent features for better distribution alignment? \n2. Are there better regularizations one could utilize to obtain better results? How is the proposed regularization, the optimal distribution alignment ?\n3. In Table 4, Can the authors provide the results in the present of cfg. Does the performance trend remain the same with the presence of cfg\n4. Additionally, the authors claim that[Ln 399-400], without the local coherency global coherency becomes unreliable, Could the authors provide visualizations of PCA similar to Figure 5,  for each of the loss components and show that this is the case."}, "questions": {"value": "Please refer weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DbGDsSWUj0", "forum": "kEvf4JggSx", "replyto": "kEvf4JggSx", "signatures": ["ICLR.cc/2026/Conference/Submission8649/Reviewer_DxQ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8649/Reviewer_DxQ3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762228524631, "cdate": 1762228524631, "tmdate": 1762920470650, "mdate": 1762920470650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}