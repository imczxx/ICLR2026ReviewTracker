{"id": "jjwW2zS0w2", "number": 9182, "cdate": 1758114366765, "mdate": 1759897739021, "content": {"title": "Soft Quantization Activation Functions For Deep Learning", "abstract": "Activation functions (AFs) are a cornerstone of deep learning, providing the crucial nonlinearity needed for network expressiveness. However, widely used AFs like ReLU and GELU are fixed and non-adaptive, offering limited nonlinearity and often necessitating larger, more complex architectures to capture intricate functions. This paper introduces a new family of trainable, architecture-agnostic AFs called Soft Quantization Activation Functions (SQUAFs). We show theoretically that SQUAFs can approximate any continuous nonlinear one-dimensional function with arbitrary precision. Our extensive experiments demonstrate that networks equipped with SQUAFs consistently outperform their counterparts using existing AFs across diverse tasks. Specifically, we achieve orders-of-magnitude error reduction in function fitting, up to 25.27 dB gain in image fitting, and significant accuracy improvements in image classification and large language model (LLM) fine-tuning. Moreover, SQUAFs (1) enable smaller models to surpass larger ones trained with conventional AFs, and (2) can reduce the inter-device communication cost in model-parallel settings by up to 39-fold while still improving accuracy. These results highlight SQUAFs as a simple yet powerful drop-in replacement for standard AFs, offering both theoretical expressiveness and practical performance gains.", "tldr": "We propose a highly flexible trainable activation function that works well with all kinds of DNNs and tasks.", "keywords": ["activation function", "quantization", "nonlinearity"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b31e0b13ffac59c28452dec18787ada9a3f46c94.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The proposed activation function (AF) is an impressive work highlighting the issues with simple AF such as ReLU which needs deeper models to capture non linearity and trainable AFs like PReLU, swish etc which can capture non linearity more effectively but by adding extra parameter makes distributed training more complicated (communication between different nodes to share updated parameters). Trainable, architecture-agnostic AF, Soft Quantization Activation Functions (SQUAFs) proposed by Authors can approximate any continuous nonlinear one-dimensional function with arbitrary precision. SQUAFs models are claimed to be better than counterpart bigger models trained using existing AFs. Additionally, it is shown to save more than 39x communication cost compared to models using trainable parameters in distributed computing setting. Results of four different tasks shows consistent improvement in performance relavie the the baseline models. \n\nThis paper targets an important issue and the formulation of the problem and solution is interesting but the improvement in accuracy is not significant (given the the impact of new AFs in training is also now known) and limitation of 1D approximation may limit model expressivity. Some of the good results are on very small datasets which does not convey the real impact of this method. This is a weak reject and the score can be changed based on Author’s explanation as indicated in the limitations section."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Authors have proposed a new Activation function which is trainable (but with partial derivatives),  adaptable to quantization and it is architecture agnostic to improve the performance of four different class of models\n- Mathematical formulation of SQUAF is interesting and intuitive and it focuses on overcoming the limitations of existing AF.\n- SQUAF models consistently have high accuracy compared to baseline models\n- Designed as a plug-and-play replacement for existing activations in any architecture.\n- Detailed results comparing with other activation functions such as GELU, PRELY, SWISH and few more."}, "weaknesses": {"value": "- Table 4 : ResNet18 baseline performance is using ReLU (69.76) but if you replace ReLU with slightly more complex AF (SiLU), it will improve accuracy to almost similar to what can be achieved by SQUAF. Authors are encouraged to check those results and discuss the merit of using SQUAF in this context.\n- Authors are also encouraged to check the same for models trained on CIFAR as it can be trained much faster compared to ImageNet.\n- Table 7 results are impressive but CIFAR100 is too small and non complex dataset so showing a competitive results on larger datasets will make their claim stronger.\n- Overall the improvement in accuracy does not seem to be significantly high.\n- Trainable activation parameters could introduce extra computational or tuning cost, especially in large models. it will also be interesting to see the impact of these AFs on training complexity/training time etc.\n- It’s not clear how much gain comes purely from adaptability versus other design factors.\n- The approximation proof is restricted to one-dimensional functions and generalization to high-dimensional activations may be non-trivial. This can result in expressivity gap. Without extending the theory, it’s unclear if SQUAF’s adaptivity at the neuron level translates into provably higher expressivity at the layer or network level."}, "questions": {"value": "- Do Authors think that for resnet18/imagenet results achieved by SQUAF will be better compared to just replacing ReLU with SiLU \n- What are the training overheads? can it be quantified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QWmj89WiOR", "forum": "jjwW2zS0w2", "replyto": "jjwW2zS0w2", "signatures": ["ICLR.cc/2026/Conference/Submission9182/Reviewer_qi4Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9182/Reviewer_qi4Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761493179789, "cdate": 1761493179789, "tmdate": 1762920857458, "mdate": 1762920857458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper sets out to study more sophisticated activation functions than those currently employed in DNNs.   The paper proposes a family of activation functions, SQUAFs, that are based upon probability distributions combined over intervals (soft quantization).  There is some analysis of the representation ability of these in the appendices.  Empirical evaluation shows the activation functions can represent some test functions better.  Finally, the activation functions are applied to more standard DNN settings and improvements are claimed."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper highlights an interesting problem, how the design of activation functions can be considered an alternative to wider or deeper networks."}, "weaknesses": {"value": "Making the activation function more complex increases computational costs.  There is some evaluation of the impact on inference latency in the appendix but the main paper did not seem to discuss the cost of the new activation functions and I didn't see any analysis on the potential impact on training times.  Similarly, the increased complexity of the activation function seems to come at the cost of increasing the number of parameters (the $y_i$'s and $L$) of the model and it is unclear if the performance comparisons are fair comparisons in terms of number of (modifiable) parameters.\n\nThe motivation on the first page seems to be trading off activation function complexity versus increased number of weights, but this tradeoff does not appear to be quantified in the results section.  \n\nIt was unclear what the motivation was for evaluating how closely a function can be approximated.  \n\nIt was also unclear what the particular motivation was for using quantization (Section 2.2 and 2.3) for the activation function.\n\nThe claim that KD using SQUAFs can increase accuracy above the teacher seems highly suspect and as if some form of overfitting is occurring."}, "questions": {"value": "What is the motivation for introducing (soft) quantization in this paper? \n\nAre the parameters introduced by SQUAF (Equation 5 to 7) meant to be trained or are they meant to be fixed?  If they are fixed how were they chosen in your experiments?   If they are trainable, what is the increase in trainable parameters in the overall model?\n\nWhat is the relationship of i and j in P2 on page 5?\n\nLine 316: \"We consider fitting two sinusoidal functions of increasing complexity\" -- Why do this experiment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CRo6AWPMEA", "forum": "jjwW2zS0w2", "replyto": "jjwW2zS0w2", "signatures": ["ICLR.cc/2026/Conference/Submission9182/Reviewer_TiQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9182/Reviewer_TiQm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681593062, "cdate": 1761681593062, "tmdate": 1762920857194, "mdate": 1762920857194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new activation function called Soft Quantization Activation Functions (SQUAFs). The activation function is specific to a layer, and its shape is controlled by three parameters. The authors show that SQUAF can approximate any 1-dimension function to an arbitrary precision. Experimental results are promising and show the approach outperforming multiple existing AF on varied benchmarks ranging from regression to LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experimental results are convincing. The authors create models for regression, CIFAR-100 classification, LLM-finetuning and show that SQUAF outperforms other popular AF, including RELU, GELU, and SiLU. \n2. They provide a proof for approximating a 1-D function using SQUAF."}, "weaknesses": {"value": "1. Figure 1 results look impressive, but are not very convincing; the model might be memorizing things and might not generalize better. This concern is removed later with training and test results on other datasets. \n2. The 1-D approximation proof is limited and not extended to the general multidimensional case. \n3. I am not convinced about reducing communication costs with SQUAF-P. The other results are sufficiently impressive, so I do not see a need to include this here. \n4. The proposed AF does lead to some slowdown, as shown in Table 11."}, "questions": {"value": "It would be great if the authors could remove the half-baked claims from the paper. Otherwise, I do not see any major issues that need fixing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "csqElLNcaE", "forum": "jjwW2zS0w2", "replyto": "jjwW2zS0w2", "signatures": ["ICLR.cc/2026/Conference/Submission9182/Reviewer_2caT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9182/Reviewer_2caT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936045538, "cdate": 1761936045538, "tmdate": 1762920856636, "mdate": 1762920856636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Soft Quantization Activation Functions that are family of trainable functions that can approximate any continous 1D function with any arbitrary precision. The activation functions can replace fixed activation functions like ReLU and GELU and the paper demonstrates accuracy improvements across MLPs, CNNs and transformers. The proposed activation functions help reduce communication costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Proposed activation functions have theoretically proven universal approximation and differentiability.\n2. Consistent performance gains across diverse tasks and architectures.\n3. Reduces inter-device communication cost while improving accuracy.\n4. Paper demonstrates improvements in fine-tuning, classification etc."}, "weaknesses": {"value": "1. The proposed activation functions add additional trainable params which might hinder the optimzation process.\n2. Generalization of this method across larger models is unknown."}, "questions": {"value": "1. How sensitive are results to initialization of the quantization parameters (y, z, α)?\n2. Do you have more results on more realistic LLM workloads (large dataset, models (1B-8B scale)) on different benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RQXPlhdBF6", "forum": "jjwW2zS0w2", "replyto": "jjwW2zS0w2", "signatures": ["ICLR.cc/2026/Conference/Submission9182/Reviewer_1Xpj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9182/Reviewer_1Xpj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123558800, "cdate": 1762123558800, "tmdate": 1762920856237, "mdate": 1762920856237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}