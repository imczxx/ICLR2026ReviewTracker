{"id": "PXZ3V2CxHr", "number": 11708, "cdate": 1758203248704, "mdate": 1759897559525, "content": {"title": "Identifying Good and Bad Neurons for Task-Level Controllable LLMs", "abstract": "Despite their remarkable capabilities, the complex mechanisms by which neurons influence Large Language Models (LLMs) remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons accounting for target performance while neglecting neurons with other\nroles, e.g., inhibitive roles, resulting in an incomplete view of LLMs in task execution. Also, they are often customized for specific data structures, lacking flexibility for diverse tasks with varying input-output formats. To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification, with the key insight that task performance is jointly determined by neurons with two opposing roles: “good” neurons that facilitate task completion and “bad” neurons that inhibit it. NeuronLLM is instantiated by two main modules: Question-Answering-based Task Transformation (QATT) and Contrastive Neuron Identification (CNI). QATT transforms diverse tasks into unified question-answering format, enabling NeuronLLM to understand LLMs under different tasks; CNI identifies good and bad neurons via a new cross-entropy-based con-\ntrastive scoring method, featuring a holistic view of neuron analysis. Comprehensive experiments on LLMs of different sizes and families show that NeuronLLM substantially outperforms existing methods in identifying task-relevant neurons across four NLP tasks, providing new insights into LLM functional organization.", "tldr": "", "keywords": ["Neuron Localization", "LLM Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f70f3e9c54cb822e7c1e81240d060adec82df0a.pdf", "supplementary_material": "/attachment/9cc3172d5259b2ea61a19fae1df791dc12ac4109.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces NeuronLLM, a framework designed to improve task-level understanding and control of large language models (LLMs) by identifying neurons with supportive (\"good\") and inhibitory (\"bad\") roles for downstream NLP tasks. NeuronLLM employs a Question-Answering-based Task Transformation (QATT) to unify diverse task formats into a multiple-choice QA protocol, and a Contrastive Neuron Identification (CNI) module leveraging a cross-entropy-based additive scoring approach to attribute neuron importance. Extensive results are reported for three LLMs (LLaMA 2-7B, Baichuan 2-7B, LLaMA 2-13B) across four NLP tasks, with control-based interventions used to validate neuron function. The paper emphasizes the biological analogy of \"functional antagonism\" and claims superior empirical performance over recent SOTA neuron attribution methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tGood writing and motivation illustration: This paper highlights the reason why we need for not only activating the supporting neurons but also removing the neurons which declines the performance from both biological and mathematical view."}, "weaknesses": {"value": "1.\tLack of innovation: The main method of the paper is based on a biological concept and existing methods for enhancing and reducing activation values, but more often than not, biological concepts are applied to the shell of existing theories, lacking unique innovation.\n2.\tThe comparison of baselines lacks fairness: The main models you compared are Llama2-7B, Llama2-13B, and Baichuan2-7B. Why not use some newer and better open-source models, such as Qwen, Llama3, and other series of models. And some performance indicators have increased from 7% of the baseline to 63%, but the authenticity of the results needs to be measured.\n3.\tThe representativeness of dataset selection needs to be considered: you only selected one dataset for each task, how can you ensure that the selected dataset represents that task?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hJtBchmIAv", "forum": "PXZ3V2CxHr", "replyto": "PXZ3V2CxHr", "signatures": ["ICLR.cc/2026/Conference/Submission11708/Reviewer_GAoB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11708/Reviewer_GAoB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553224981, "cdate": 1761553224981, "tmdate": 1762922754086, "mdate": 1762922754086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem that the influence of neurons on task execution in large language models (LLMs) is opaque, and existing methods focus only on supportive neurons while lacking adaptability to multi-task scenarios. It proposes the NeuronLLM framework, which draws on the biological principle of functional antagonism, viewing task performance as the combined effect of “good” neurons that facilitate and “bad” neurons that inhibit task completion. The framework employs the Question-Answering-based Task Transformation (QATT) module to unify various tasks into a question-answering format, and the Contrastive Neuron Identification (CNI) module to identify “good” and “bad” neurons using a cross-entropy-based contrastive scoring method. Experiments show that NeuronLLM outperforms existing methods across multiple LLMs and four NLP tasks, providing a more comprehensive identification of task-relevant neurons and revealing the functional organization of the models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper proposes the NeuronLLM framework, which leverages two opposing types of neurons: task-supporting “good” neurons and task-inhibiting “bad” neurons, to achieve overall task-level control of LLMs.\n2.The paper introduces a Question-Answering-based Task Transformation module that unifies various tasks into a question-answering format, enabling NeuronLLM to interpret LLMs under different tasks.\n3.The paper presents a contrastive neuron identification module, which uses a novel cross-entropy-based contrastive scoring method to identify “good” and “bad” neurons, providing a holistic perspective on neuron analysis.\n4.The paper conducts comprehensive experiments across LLMs of different scales and families, demonstrating that NeuronLLM significantly outperforms existing methods in identifying task-relevant neurons."}, "weaknesses": {"value": "1.The paper uses performance change metrics RAC and RCC in the comparative experiments, which do not directly show the effects of Degrade and Enhance, i.e., whether the model’s performance decreases or improves. It is recommended to add metrics that can visually indicate performance increases and decreases.\n2.The paper proposes a Question-Answering-based Task Transformation module, which unifies diverse tasks into a QA format to ensure that the neuron attribution method can model a consistent output objective. However, the ablation study does not validate the effectiveness of this module. It is recommended to add relevant experiments to evaluate the impact of this module on the overall framework.\n3.In the ablation study regarding joint modeling of good and bad neurons, it is not specified whether the control of “Good” or “Bad” neurons is Exciting or Silencing. Similarly, in the “Both” strategy, it is unclear whether it acts as an enhancer or a degrader.\n4.Equation (1) in the paper explains that the contribution of a neuron to the objective function is approximated via integrated gradients, but the parameter kkk is not explained. It is recommended to provide an explanation to make it easier for readers to understand.\n5.Figure 1 shows the framework of the proposed NeuronLLM method, including the QA-based Task Transformation module, the Contrastive Neuron Identification module, and the Neuron Intervention and Evaluation module. However, the methods of the Contrastive Neuron Identification module and the Neuron Intervention and Evaluation module are not illustrated in the figure. Additionally, the font size of the QA-based Task Transformation module is too small; increasing the font size is recommended.\n6.The paper does not provide the code for the NeuronLLM framework, resulting in low reproducibility. It is recommended that the authors release the implementation for verification and wider use."}, "questions": {"value": "1.The paper proposes a Question-Answering-based Task Transformation module, which aims to unify diverse tasks into a QA format to ensure that the neuron attribution method can model a consistent output objective. How do the authors validate the effectiveness of this module?\n2.Table 2 shows the results of integrating existing neuron scoring methods into the NeuronLLM framework. In this experiment, the authors replaced the proposed ACE scoring method with the TN/QRNCA scoring methods. Can the TN/QRNCA scoring methods distinguish between good and bad neurons?\n3.Does the NeuronLLM framework proposed in the paper, which models 'good' and 'bad' neurons, include any visualization results? Has it attempted to fine-tune downstream tasks based on the identified 'good' and 'bad' neurons (such as using an enhancer strategy)? If so, what are the specific fine-tuning methods used and what are the resulting effects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7MGS53sIjx", "forum": "PXZ3V2CxHr", "replyto": "PXZ3V2CxHr", "signatures": ["ICLR.cc/2026/Conference/Submission11708/Reviewer_51Qj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11708/Reviewer_51Qj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707854577, "cdate": 1761707854577, "tmdate": 1762922753737, "mdate": 1762922753737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **NeuronLLM**, a novel framework for identifying task-relevant “good” and “bad” neurons in LLMs, inspired by the biological principle of *functional antagonism*. The authors argue that task performance is jointly determined by both supportive and inhibitory neurons, a perspective largely overlooked by prior work. NeuronLLM consists of two main components: (1) **QATT**, which transforms diverse tasks into a unified multiple-choice QA format, and (2) **CNI**, which employs a contrastive cross-entropy-based scoring method (ACE) to identify neurons with opposing roles. Extensive experiments across multiple LLMs and NLP tasks demonstrate that NeuronLLM significantly outperforms existing neuron attribution methods in both degrading and enhancing task performance through targeted neuron interventions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- *Novel Conceptual Insight*: The introduction of “bad” (inhibitory) neurons alongside “good” ones, inspired by functional antagonism, provides a more holistic and biologically plausible view of LLM internal mechanisms.\n- *Practical Framework Design**: The two-stage design (QATT + CNI) is well-motivated. QATT effectively standardizes diverse tasks, enabling consistent neuron analysis, while the ACE scoring in CNI naturally fits the QA format and captures both positive and negative contributions.\n- *Comprehensive Evaluation*: The paper provides thorough experiments across multiple model families (LLaMA 2, Baichuan) and sizes (7B, 13B), four distinct NLP tasks, and several strong baselines. The results convincingly demonstrate NeuronLLM’s superiority.\n- *Ablation and Analysis*: The paper includes valuable ablation studies (individual vs. joint intervention of good/bad neurons) and insightful analyses (common vs. task-specific neurons, enhancement vs. degradation asymmetry), which deepen the understanding of the framework and the phenomena it uncovers."}, "weaknesses": {"value": "- *Limited Scope of Tasks*: While the framework is proposed for task-level understanding, the evaluated tasks are all classification or multi-choice QA after transformation. It remains unclear how well NeuronLLM generalizes to true *generation* tasks where the output space is open-ended and the “distractor” choices in QATT are not naturally defined. Neuron-Level Knowledge Attribution in Large Language Models (Yu et al., 2024) claims that the active neurons are related to task domains and its form, I wonder does the identified neurons changed by the variant domains or forms? \n- *Justification of the “Bad” Neuron Concept*: The paper shows that silencing certain identified neurons degrades performance, which is a post-hoc validation. However, a more intrinsic characterization or explanation of *what these “bad” neurons are doing* is lacking. The concept is currently defined primarily by the intervention effect, lacking mechanism interpretation. In formula 3, integrated gradients are calculated on the gradient \n$$\\frac{\\partial F}{\\partial w_i^l} = \\frac{\\partial P(c^{*})}{\\partial w_i^l}$$ However, the \"good or bad\" rating of a neuron does not solely depend on how it affects the score of the correct option $s_c^{\\*}$, but on the difference between the gradient of $s_c^{\\*}$ and the weighted average of the gradients of all option scores.\n- *Computational Cost Discussion*: Although Figure 9 shows QATT improves efficiency compared to full-sequence generation, the overall cost of NeuronLLM (involving integrated gradients over multiple proxy questions and aggregation) is still non-trivial, especially for very large models. A discussion of the scalability and potential bottlenecks would be beneficial.\n- *Ablation on QATT's Role*: The paper ablated the effect of good/bad neurons but did not thoroughly ablate the contribution of QATT itself. For instance, how crucial is the multi-choice format with distractors compared to a simpler QA format?"}, "questions": {"value": "- *Generalization to Generation Tasks*: How would NeuronLLM handle free-form generation tasks? Would the distractor choices in QATT become arbitrary or hard to define, and how might this impact the contrastive scoring in CNI?\n - *Circuit Related to Identified Neurons*: How to determine whether silencing certain identified neurons degrade by destroying circuits in the model or by identifying neurons? The paper assumes a direct causal relationship between good/bad neurons and their functions. However, in the distributed representation of neural networks, there is a high degree of co adaptation and redundancy between neurons. How to rule out the possibility that the identified \"good\" neurons are actually only highly correlated with the unknown neuron X that truly plays a causal role?\n- *Intrinsic Nature of “Bad” Neurons*: Beyond their inhibitory effect on the target task, is there any intrinsic properties or patterns in the “bad” neurons? What’s the mechanism of good/bad neurons in LLMs?\n- *Sensitivity to Distractor Quality*: The quality of distractor choices seems crucial for the contrastive scoring. How sensitive is NeuronLLM to the method of distractor generation (e.g., random sampling vs. LLM-generated)? Is there any correlation between distractor quality and neuron identification accuracy?\n- *Comparison Broader Baselines*: The paper compares against recent neuron attribution methods (TN, QRNCA). Have you considered comparing with more general feature attribution methods (e.g., using integrated gradients directly on the original task format) to better isolate the gain from the good/bad neuron paradigm versus the QATT+ACE pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L1n8nec2VK", "forum": "PXZ3V2CxHr", "replyto": "PXZ3V2CxHr", "signatures": ["ICLR.cc/2026/Conference/Submission11708/Reviewer_JeJh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11708/Reviewer_JeJh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812969338, "cdate": 1761812969338, "tmdate": 1762922753462, "mdate": 1762922753462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes NeuronLLM, a novel framework that identifies both good and bad neurons in LLMs. NeuronLLM consists of two key modules: Answering-based Task Transformation (QATT) module and a Contrastive Neuron Identification (CNI) module. \n(1) QATT. This module offers an effective way to transform diverse tasks to a universal multi-choice QA form\n(2) CNI. This module adopts a cross-entropy-based contrastive neuron scoring method that is naturally suited for the QA format\nExtensive results on LLaMA 2-7B, Baichuan 2-7B, and LLaMA 2-13B show that NeuronLLM substantially outperforms state-of-the-art methods over multiple NLP tasks"}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* this work is easy to follow and the motivation is clearly stated\n* identifying both good and bad neurons is an interesting strategy"}, "weaknesses": {"value": "* this work is highly similar to an existing work, QR-Neuron [1], which severely undermines the novelty and contribution of this manuscript. (1) QR-Neuron is the first work that introduced multi-choice QA for neuron analyses, and this work claims that they propose this strategy and didn't give proper credit to prior work; (2) the proposed QATT also follows the core idea of the QR-Neuron work. I would suggest clarifying the distinction and novelty of QATT\n* the authors claim that *\"Cross-Entropy-based Contrastive Neuron Scoring can capture both the confidence of the LLM in the\ncorrect choice and its uncertainty about incorrect ones.\"*  *\"the probability of generating the correct choice over the whole vocabulary,\nleading to wrongly identified neurons that actually increase/decrease both probabilities of correct and incorrect answers\"*. I find this argument misleading. The probability is obtained by using softmax over the vocabulary, which also accounts for both correct and wrong answers.\n\n[1] Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts. AAAI 2025"}, "questions": {"value": "* I find that *NeuronLLM* is not an appropriate name since this work aims to identify neurons in LLMs rather than developing a new LLM\n* in line 261, does it mean that a proxy question set always consists of three questions? Why is the number fixed to three?\n* it is useful to give a definition about *good* and *bad* neurons"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9CkNgyFKie", "forum": "PXZ3V2CxHr", "replyto": "PXZ3V2CxHr", "signatures": ["ICLR.cc/2026/Conference/Submission11708/Reviewer_qwJC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11708/Reviewer_qwJC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762205704638, "cdate": 1762205704638, "tmdate": 1762922753153, "mdate": 1762922753153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}