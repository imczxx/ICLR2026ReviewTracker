{"id": "Ksgiup7ZNZ", "number": 20138, "cdate": 1758302944170, "mdate": 1759896999618, "content": {"title": "Union-of-Experts: Experts in Mixture-of-Experts are Secretly Routers", "abstract": "Mixture-of-Experts (MoE) is a foundational architecture in modern large language\nmodels (LLMs). However, a structural limitation has been overlooked: the router\nis external to the experts, rendering it unaware of their internal capabilities. This\ngap between routing decisions and expert capabilities limits model performance.\nIn this paper, we demonstrate that the activations of a small subset of “routing neurons” within each routed expert’s own parameters can faithfully capture the match\nbetween the expert’s capabilities and input tokens. Collectively, these distributed\nrouting neurons within each routed experts compose an implicit, capabilities-aware\n“router”, where the norm of the routing neurons’ activations suggests its corresponding expert’s weight. A straightforward implementation of this design requires\nactivating all experts to compute these routing signals, where the unselected experts’ routing neurons are abandoned. To avoid the computational waste from\nactivating unselected experts, we introduce another novel design: we unify the\nrouting neurons of all routed experts to form a virtual shared expert, replacing the\nstandard shared expert in MoE. In this virtual shared expert, activations are not\nwasted, as they serve not only for routing but also contribute to the final outputs of\nboth the shared expert and partial of routed experts. We name this new MoE variant\nUnion-of-Experts (UoE), drawing an analogy where the routing neuron acts as each\nexpert’s representative, and the virtual shared expert is their union, enabling the\nexperts’ autonomous selection and joint statement. We pre-train language models\nranging from 1B to 3B parameters, showing that UoE consistently outperforms\nstrong MoE baselines with comparable efficiency.", "tldr": "", "keywords": ["Mixture-of-Experts", "Language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/504806bef58d626fec5fcf6cdae456c7c88274d8.pdf", "supplementary_material": "/attachment/43c7790a7ff33410968a8f62bbe3a794659e975d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new dynamic architecture for LLMs, termed Union-of-Experts (UoE), designed to address the disconnect between the routing module and expert capabilities in traditional MoEs while overcoming the computational and memory efficiency bottlenecks of Autonomy-of-Experts (AoE). UoE use some neurons within each expert are designated as routing neurons, whose activation strength is used to determine the expert's suitability for processing the current input. The routing neurons of all experts collectively form a virtual shared expert, whose output is not only used for routing decisions but also directly contributes to the final prediction, thus avoiding computational waste. UoE consistently outperforms both MoE and AoE on multiple downstream tasks using pre-trained language models with 1B and 3B parameter sizes. It also achieves a training throughput comparable to MoE."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors combines routing neurons into \"virtual shared experts\" to reuse routing signals and avoid redundant computation in AoE.\n\n2. Detailed ablation experiments are provided to verify the effectiveness of the virtual shared expert and routing neuron design."}, "weaknesses": {"value": "1. The experiment lacks implementation details.\n\n2. The scale of the experiment is relatively limited, and both the data and model scale are relatively limited."}, "questions": {"value": "1. The inference efficiency of MoE is significantly affected by its implementation method. The author should explain how MoE is implemented.\n\n2. In light of the constrained experimental scale, the authors ought to illustrate how model capability evolves with increasing training-data size, thereby revealing the method’s potential at truly large scales.\n\n3. How efficient is the inference of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bYySJV4HFp", "forum": "Ksgiup7ZNZ", "replyto": "Ksgiup7ZNZ", "signatures": ["ICLR.cc/2026/Conference/Submission20138/Reviewer_AGhg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20138/Reviewer_AGhg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760793559944, "cdate": 1760793559944, "tmdate": 1762933168921, "mdate": 1762933168921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Union-of-Experts (UoE), a novel Mixture-of-Experts (MoE) architecture. UoE addresses two key limitations: the router in standard MoE is unaware of expert capabilities, and the Autonomy-of-Experts (AoE) incurs significant computational overhead. UoE replaces the standard router by utilizing a small subset of \"routing neurons\" within each expert's weight matrix to parameterize the routing function. This design enables capabilities-aware routing without the complexity or cost of AoE's low-rank factorization. Furthermore, these routing neurons are collectively unified to form a virtual shared expert, ensuring their activations contribute to the final output and preventing computational waste. Experiments show that UoE consistently outperforms MoE and AoE baselines while maintaining comparable efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The motivation is clear: achieve expert autonomy by reducing the overhead of AoE while eliminating the router in MoE. The idea is easy to grasp, requires minimal architectural changes, and incurs little overhead. By using a small subset of routing neurons and integrating their computation as a virtual shared expert, UoE ensures efficient resource utilization and incurs overhead nearly identical to standard MoE while delivering superior performance."}, "weaknesses": {"value": "The primary weaknesses revolve around a lack of detailed implementation specifics and insufficient empirical evidence to support key efficiency claims, as detailed in the questions below.\n\nMinor comments:\n\n* Fig. 2 is taken directly from the AoE paper, the authors may need to redraw the figure for the camera-ready version to prevent potential copyright issues."}, "questions": {"value": "1. Is the efficiency dilemma presented in Figure 3, which plots FLOPs and Peak Memory as a function of factorization rank $r$, a purely theoretical estimate based on the derived equations (Eq. 4 and Eq. 5), or is it based on empirical profiling numbers from real experiments? If it's a theoretical estimate, this should be clearly emphasized in Section 3.1 or in the figure caption.\n\n2. In Table 2, the achieved training TFLOPS for UoE ($86.51$) is notably lower than that of MoE ($90.40$). Could the authors elaborate on the implementation details in the Appendix to explain this difference? For example, during training, are the router neurons computed first, and then the final expert values re-computed? Will the router neurons' input-to-output path be calculated twice? Or, is it computed only once and then the output is efficiently merged with the remaining neurons' output at the code level? This isn't held against the authors, but some explanations would be highly preferred. It seems the router neurons' computation path might need to be utilized twice during training, potentially explaining the lower TFLOPS compared to MoE.\n\n3. Why do UoE models have a higher throughput but lower TFLOPS compared to MoE in Table 2?\n\n4. The claim that \"UoE incurs computational overhead that is nearly identical to MoE at inference time\"  lacks experimental validation. Could the authors provide an inference profiling analysis (e.g., inference TFLOPS, peak memory, and throughput) similar to Table 2 to quantitatively support this claim? In addition, a dedicated section in the Appendix detailing the implementation for efficient training/inference of UoE (e.g., regarding the computation of router neurons) would be highly beneficial.\n\n5. The title \"Union-of-Experts: Experts in Mixture-of-Experts are Secretly Routers\" is a bit misleading. The core mechanism involves explicitly constraining a small subset of neurons to represent routing information during training. The current title seems to suggest that a standard expert's full output could be used as a router directly, without training-time modifications. If possible, could the authors revise the title to more accurately reflect the main contribution, which is the use of specialized routing neurons and the virtual shared expert?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yPUFe0mgqq", "forum": "Ksgiup7ZNZ", "replyto": "Ksgiup7ZNZ", "signatures": ["ICLR.cc/2026/Conference/Submission20138/Reviewer_HmyS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20138/Reviewer_HmyS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629344307, "cdate": 1761629344307, "tmdate": 1762933167539, "mdate": 1762933167539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Union-of-Experts (UoE), a new Mixture-of-Experts (MoE) variant that unifies routing and expert computation to improve both efficiency and expert autonomy. The key insight is that a small subset of neurons—termed routing neurons—within each expert can represent the expert’s activation behavior and be used for routing decisions based on their activation norms. By consolidating these routing neurons from all experts into a virtual shared expert, UoE eliminates redundant computations present in Autonomy-of-Experts (AoE) while preserving expert autonomy.\n\nThe paper provides both theoretical analysis (showing how AoE’s low-rank factorization leads to inefficiency) and empirical validation, pretraining models up to 3B parameters on large-scale corpora. Experiments on multiple benchmarks (ARC, PIQA, HellaSwag, etc.) show that UoE outperforms both MoE and AoE in accuracy while maintaining computational efficiency comparable to MoE. UoE also achieves better expert load balancing and scalability as model size increases. Ablation studies confirm the importance of the virtual shared expert and the routing neuron mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated and clearly articulated problem: bridging the gap between routing decisions and expert capabilities.\n2. Elegant design that combines autonomy-based routing with MoE efficiency.\n3. Extensive experimental validation (1B–3B models) with consistent improvements.\n4. Solid analysis on computation–memory trade-offs and load balancing."}, "weaknesses": {"value": "1.The core claim—that external routers are unaware of expert capabilities—is not empirically validated, e.g., consistency analysis between router choices and optimal expert matches.\n\n2.The key assumption that a fixed subset of “routing neurons” can represent full expert behavior is supported mainly by empirical correlation, without rigorous theoretical or mechanistic explanation.\n\n3.The study only validates UoE on models up to 3B parameters, lacking comparison with larger-scale MoE architectures (e.g., DeepSeek-MoE) and raising concerns about scalability.\n\n4.Experiments are mostly conducted on commonsense reasoning and NLU tasks. Broader capabilities such as code generation and mathematical reasoning are remain unexamined.\n\n5.While improved balance is shown, the underlying reasons are not explored, nor is its potential trade-off with routing quality.\n\n6.It is unclear whether these neurons learn general-purpose functions or merely serve as routing proxies, weakening the design's motivation.\n\n7.The “virtual shared expert” is described as abstract during training and materialized at inference, complicating reproducibility and clear implementation."}, "questions": {"value": "These questions correspond to the weaknesses listed above:\n\n1.Have you empirically verified that external routers are misaligned with expert capabilities, e.g., through consistency analysis?\n\n2.What justifies that a fixed subset of neurons can represent full expert behavior beyond empirical correlation?\n\n3.How does UoE scale to larger MoE models (e.g., 10B–72B) and industrial training settings?\n\n4.Can the proposed routing mechanism generalize to other capabilities such as code or multimodal reasoning?\n\n5.Why does UoE achieve better load balance, and is there a trade-off with routing quality?\n\n6.Do routing neurons learn general-purpose representations or act purely as routing proxies?\n\n7.The description of the “virtual shared expert” is unclear — could the authors clarify its implementation and how it differs between training and inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tenHGehz0K", "forum": "Ksgiup7ZNZ", "replyto": "Ksgiup7ZNZ", "signatures": ["ICLR.cc/2026/Conference/Submission20138/Reviewer_HdQY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20138/Reviewer_HdQY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704130536, "cdate": 1761704130536, "tmdate": 1762933166860, "mdate": 1762933166860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel Mixture-of-Experts architecture that addresses the fundamental limitation where traditional routers operate externally to experts and remain unaware of their capabilities. The key innovation is demonstrating that a small subset of \"routing neurons\" ($N_s \\ll D$) within each expert can simultaneously perform two functions: their activation norms enable autonomous expert selection based on capability-input matching, and they collectively form a \"virtual shared expert\" that contributes to the final output, eliminating the computational waste of prior autonomous approaches. Experiments on language models up to 3B parameters show UoE consistently outperforms MoE and AoE baselines across 8 benchmarks while achieving 19.8% higher training throughput than AoE and maintaining computational costs identical to standard MoE, with additional benefits including improved load balancing without auxiliary losses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents an interesting architectural contribution where a small subset of neurons within each expert serves the dual function of routing decisions and output contribution, offering a novel perspective on integrating expert selection with expert computation that differs from conventional external router approaches.\n\n- The experimental evaluation provides thorough efficiency comparisons demonstrating that UoE achieves notable throughput improvements over AoE (19.8%) while maintaining computational costs comparable to standard MoE (Table 2), and the load balancing analysis (Figure 6, Table 3) reveals encouraging improvements across tested configurations.\n\n- The paper effectively communicates the core limitation of routers being decoupled from expert capabilities, employs helpful visual illustrations (Figures 1 and 4) that clarify the architectural distinctions, and maintains consistent mathematical formalism (Equations 1-6) throughout the exposition to support reader understanding."}, "weaknesses": {"value": "- The paper lacks rigorous theoretical justification for why the first Ns neurons can spontaneously learn to represent the entire expert's activation patterns. While the authors claim this correlation \"spontaneously emerges during training\" (Section 3.2), they provide no mathematical framework or analysis explaining why this should occur. The statement that \"the selection of routing neurons proves highly flexible\" is supported only by empirical observations relegated to Appendix B, without exploring what properties make certain neuron subsets more effective than others. A more principled approach would derive conditions under which routing neurons can provably approximate full expert activations, perhaps through analysis of gradient flow or information-theoretic bounds. Without this foundation, the core mechanism appears somewhat arbitrary and the generalizability to different architectures remains uncertain.\n\n- While the paper presents results up to 3B parameters, modern production MoE models operate at scales of hundreds of billions to trillions of parameters (e.g., GPT-4, Mixtral-8x7B). The largest model tested (3B) is too small to convincingly demonstrate that UoE's design principles will hold at scale. Specifically, it remains unclear whether: (a) the routing neuron correlation property persists as expert capacity grows, (b) the load balancing advantages maintain when N (number of experts) increases to 64 or 256 as in recent models, and (c) the memory savings remain significant when model width increases substantially. The authors acknowledge that \"wide models with large d and D\" face training instability (Section 3.1) but do not thoroughly investigate this limitation. Experiments on at least a 7B or 13B model would significantly strengthen the scalability claims.\n\n- The conceptual contribution of the \"virtual shared expert\" lacks depth in several aspects. First, the paper does not adequately explain why routing neurons should serve the dual purpose of routing and shared expert computation—this seems more like an engineering convenience than a principled design choice. Second, the ablation study (Table 4, configurations 1-3) shows that removing the shared expert causes performance degradation, but this is expected for any MoE with shared experts and does not validate that routing neurons specifically are the right choice for this role. Third, the paper claims the virtual shared expert \"consolidates common capabilities\" but provides no analysis of what capabilities it actually learns compared to routed experts or traditional shared experts. A more thorough investigation would include: visualization of what features the virtual shared expert captures, analysis of its activation patterns across different token types, and comparison of learned representations with those of conventional shared experts.\n\n- The evaluation is limited to 8 standard benchmarks that primarily test knowledge recall and basic reasoning (ARC, PIQA, HellaSwag, etc.), but lacks evaluation on: (a) generation quality metrics (perplexity on diverse corpora, human evaluation of generated text), (b) instruction-following capabilities, (c) domain-specific tasks where expert specialization matters most (e.g., code generation, mathematical reasoning, multilingual understanding), and (d) long-context understanding where expert selection patterns might differ significantly. Furthermore, all benchmarks are English-only, raising questions about whether UoE's routing mechanism generalizes across languages. The lack of diverse evaluation makes it difficult to assess whether UoE's improvements are robust across different use cases or merely artifacts of the specific benchmark selection.\n\n- Several key design decisions lack proper justification or ablation studies: (a) Why is Ns = round(D/K) the optimal choice? The paper only ablates doubling Ns but doesn't explore the full spectrum or provide sensitivity analysis. (b) Why are routing neurons placed at the \"first Ns neurons\" rather than being learned positions or distributed throughout the weight matrix? (c) The choice of L2-norm for measuring activation intensity is tested against only 3 alternatives (Table 4, configs 5-7), but other options like L1-norm, max activation, or learned weighted combinations are not explored. (d) The paper doesn't ablate the impact of the SiLU activation function specifically in the routing computation. (e) No analysis of how performance varies with different values of K (number of activated experts) beyond the tested configurations. A more comprehensive ablation study would systematically explore the design space and provide guidance for adapting UoE to different scenarios."}, "questions": {"value": "- Could you provide a more rigorous theoretical or mathematical explanation for why routing neurons (specifically the first $N_s$ neurons) can effectively capture the match between expert capabilities and input tokens? While Section 3.2 states this correlation \"spontaneously emerges during training,\" the mechanism remains unclear. Specifically: (a) What gradient dynamics or optimization properties lead to this emergent behavior? (b) Are there any information-theoretic bounds or approximation guarantees that justify using $N_s \\ll D$ neurons? (c) Under what conditions might this correlation fail to emerge? A theoretical framework would strengthen the core contribution and help practitioners understand when UoE might underperform.\n\n- Given that modern production MoE models operate at scales of 7B-100B+ parameters with 8-64+ experts, how confident are you that UoE's properties will hold at these scales? Could you discuss: (a) Have you conducted any preliminary experiments or analyses suggesting the routing neuron correlation persists as expert capacity grows significantly? (b) How does load balancing and routing quality change when $N$ (number of experts) increases to 16, 32, or 64? (c) What are the specific training instabilities you observed with \"wide models\" mentioned in Section 3.1, and what architectural modifications might address them? Understanding these scalability characteristics is crucial for assessing UoE's practical applicability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p8ETewJDCQ", "forum": "Ksgiup7ZNZ", "replyto": "Ksgiup7ZNZ", "signatures": ["ICLR.cc/2026/Conference/Submission20138/Reviewer_yzUJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20138/Reviewer_yzUJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976352817, "cdate": 1761976352817, "tmdate": 1762933166414, "mdate": 1762933166414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}