{"id": "JYke7CIuIa", "number": 377, "cdate": 1756736984697, "mdate": 1759898264406, "content": {"title": "SVL: Empowering Spiking Neural Networks for Efficient 3D Open-World Understanding", "abstract": "Spiking Neural Networks (SNNs) offer an energy--efficient route to 3D spatio--temporal perception, yet they lag behind Artificial Neural Networks (ANNs) due to weak pretraining and heavy inference stacks, limiting generalization and multimodal reasoning (e.g., zero--shot 3D classification and open--world QA). We present a universal \\textbf{S}pike--based \\textbf{V}ision--\\textbf{L}anguage pretraining framework (SVL) that equips SNNs with open--world 3D understanding while preserving end--to--end spike efficiency. SVL comprises two core components: (i) {Multi--scale Triple Alignment} (MTA), a label--free triplet contrastive objective aligning 3D, image, and text; and (ii) {Re--parameterizable Vision--Language Integration} (Rep--VLI), which converts offline text embeddings into lightweight weights for text--encoder--free inference. Moreover, we present the first fully spike--driven point Transformer, {Spike-driven PointFormer}, whose 3D spike--driven self--attention (3D-SDSA) reduces interactions to sparse additions, enabling faster, more efficient training. Extensive experiments show that SVL attains strong zero--shot 3D classification (85.4\\% top--1) and consistently outperforms prior SNNs on downstream tasks (e.g., +6.1\\% 3D cls, +2.1\\% DVS actions, +1.1\\% detection, +2.1\\% segmentation) while enabling open--world 3D question answering, sometimes outperforming ANNs. To the best of our knowledge, SVL represents the first scalable, generalizable, and hardware-friendly paradigm for 3D open-world understanding, effectively bridging the gap between SNNs and ANNs in complex open-world understanding tasks.", "tldr": "The first spike-based multimodal framework that empowers SNNs with open-world 3D perception while maintaining spike-driven efficiency.", "keywords": ["Spiking Neural Network+Spike-driven+Spike Point Transformer"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e0320d87df94e9706eacbaa11a5a98ab638f066.pdf", "supplementary_material": "/attachment/cb629c67b5b79519cd56f121ec60e762e61ae6c9.zip"}, "replies": [{"content": {"summary": {"value": "The authors first introduce a new pre-training framework (SVL) to enable spiking neural networks to be effectively trained on 3D data like point-clouds, with the main objective to offer high performance while keeping their inherent advantage of efficient spike-based processing (and hence low energy consumption). In addition, the authors also introduce a Transformer-style Spike-based architecture and evaluate its performance as well as the effect of their pre-training method across a number of 3D tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality & Significance:**  \n- The authors do a good job in outlining why the task matters, as well as how it is approached; all in all a clear motivation of the presented research\n- Convincing performance improvements through the SVL pretraining, especially for E-3DSNN variants\n- Elegant idea of compressing the required text-to-embedding encoder capabilities into small (and cheap) weight matrix for efficient inference\n\n **Quality:**  \n- Their work is placed well within related efforts, and the specific gap the authors tackle is clearly presented and justified\n- Appropriate extent of ablation studies to illustrate value/contribution of core components\n- The authors include a helpful analysis paragraph regarding time steps and firing bits, and provide recommendations for applying their findings in practice (which is often neglected and therefore much appreciated!)\n\n**Clarity:** \n- The work is mostly easy to follow, and Figure 1 provides a good illustration of core concept, and makes the components of the training process easy to grasp"}, "weaknesses": {"value": "- Proposed Transformer architecture still seems very expensive, both in parameter count and energy. Note this is true when compared to the conv-based E-3DSNN variants, but also even when compared to some of the ANN methods; especially when we assume one could replace the text-encoder in some of the ANN variants as well and therefore significantly reduce the energy consumption of their ‘text’ part! \n- Somewhat lacking explanation of details for architecture, see questions\n- Notation consistency should be improved, e.g.\n  - Bold vs non-bold for variable (e.g. y in eq 5, b in eq 6, ..)\n  - Variable typo in eq 6: upper limit of sum in denominator of the second term should be C (not B), I think?\n- Writing quality could benefit from some further refinement, e.g. \n  - Commas at the end of equations, although a new sentence starts in the next line (l.128, l.203, l.280, l.297, l.300, l.308, l.310, ... etc.)\n  - Grammar/word-order (e.g. l.169: comma after ‘constant’, represents -> represent;  l.195: at t time step -> at time step t; .. etc.)\n  - Non-bold paragraph heading: I’d recommend making l.305/306 ‘Spake-driven self-attention inside SDF’ to be bold given it’s the ‘heading’ for this paragraph; also typo: SDf -> SDF \n- Lack in clarity of some parts, see questions."}, "questions": {"value": "- The authors report the results for E-3DSNN variants pretrained with SVL in Table 1; Is there a possibility to also show results for these architectures without pretraining? Or do they require pretraining to solve the task?   \n  $\\rightarrow$ Results w/o pre-training are available for other tasks, e.g. Tables 3,4\n- Which operations/components in the authors’ Spike-driven Transformer architecture make it so expensive? I’d be curious to hear some insights/background on the underlying difficulties, and what could potentially be done in the future to further improve efficiency.\n- If I see this correctly, the NCE losses mainly aligns vector directions (given the pairwise norm. similarities), while the MSE loss operates on the raw vectors and emphasises direct matches (direction magnitude);    \n  $\\rightarrow$ Table 6 currently only shows all losses + MSE, but I would be quite interested to see how well only the MSE loss does on the Images, but if possible also on the text to get a better impression of the interacting influences and embedding relationships\n- Also: When is a normalisation by ‘T’ required/used (e.g. eq 5), and when not (e.g. eq 7)? (spike train vs spike firing rate)\n- Table 4: Why are there no results for your own Transformer architecture reported?\n- Sec 4.4 l.298: *‘A learnable add-only pointwise embedding …’*: The equation that follows should X being passed through and MLP; what exactly is the ‘add-only pointwise embedding here?  \n  $\\rightarrow$ If X is mapped via an MLP, this isn’t really a learnt embedding but rather a projection of the point, i.e. input dependent – whereas learnt embeddings are usually input-independent.   \n  $\\rightarrow$ Also, why is an MLP used here? (non-linearity required/beneficial to extract features?)\n- Sec 4.4 l.309: some more detail here would be helpful; The authors describe the three linear maps resulting in Q,K,V and then go on to show that these are processed by SDSA; However, is this the output of the SDF layer? Or is there, like in Transformers, another MLP or similar post-processing happening? \nSome refinement here would improve clarity. \n- I’d also like the authors to comment on the ‘value’ and main use cases for their architecture, especially given that their results show that the conv-based E-3DSNN variants perform on par but required less energy; Are there any limits and/or applications you see where the preference for E-3DSSN would change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HGZmWHbghx", "forum": "JYke7CIuIa", "replyto": "JYke7CIuIa", "signatures": ["ICLR.cc/2026/Conference/Submission377/Reviewer_Z5F1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission377/Reviewer_Z5F1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783540104, "cdate": 1761783540104, "tmdate": 1762915506954, "mdate": 1762915506954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SVL, a pretraining framework designed to bring Spiking Neural Networks (SNNs) into the domain of 3D open-world understanding, a field currently dominated by Artificial Neural Networks. The authors' goal is to leverage the famed energy efficiency of SNNs while equipping them with complex reasoning abilities, such as zero-shot classification and open-world question answering. The framework introduces several new components, including a Multi-scale Triple Alignment (MTA) strategy, a Re-parameterizable Vision-Language Integration (Rep-VLI) module to discard the text encoder at inference, and a new SNN backbone called the Spike-driven PointFormer"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel Research Direction:** The paper tackles an interesting and challenging problem by attempting to bridge the gap between the energy efficiency of SNNs and the high-level multimodal reasoning capabilities of ANNs in the 3D domain.\n2. **SNN-Specific Improvements:** The SVL pretraining framework appears to be effective *within the SNN domain*. As shown in Table 3 and Table 4, applying SVL consistently improves the performance of SNN backbones like Spike PointNet and E-3DSNN on various downstream tasks (e.g., +6.1% on ScanObjectNN, +2.1% on DVS Action).\n3. **New SNN Architecture:** The paper introduces the Spike-driven PointFormer, a new, fully spike-driven Transformer for point clouds, which seems to be a solid contribution to the SNN architecture landscape."}, "weaknesses": {"value": "1. **Unclear Efficiency Claims:** The primary motivation for using SNNs is energy efficiency. However, the paper only provides \"estimated\" energy consumption in Table 1 and does not include a direct comparison of computational costs (e.g., FLOPs or real-world inference latency) against the ANN baselines. The ablation study in Table 5 also suggests a trade-off between performance and efficiency (e.g., time steps), which is not fully explored against ANN counterparts.\n2. **Unfair Performance Scaling Comparisons:** The zero-shot classification results in Table 1 are difficult to interpret and seem to involve an unfair comparison. The base SNN models (e.g., E-3DSNN-T) show very poor performance. The results only become competitive after scaling the SNN model to a very large size (E-3DSNN-H). However, the paper fails to report the model sizes (parameter counts) for the ANN baselines (like ULIP, OpenShape, etc.), making it impossible to know if this is a fair comparison of similarly-sized models. It is highly likely that a large SNN is being compared to smaller, more efficient ANNs.\n3. **Confusing and Incomplete Comparisons in Captioning:** The 3D captioning results in Table 2 also raise concerns. The SNN-based vision encoder (SVL-13B) performs noticeably worse than the ANN-based PointBert when using the same LLM (Vicuna). While the performance is boosted by using SpikeLLM, this introduces another variable, and it's not clear how an ANN-based model would perform with this. Furthermore, the baseline SVL model is also tested with short caption prompts, but the SpikeLLM-enhanced version is not, making the comparison confusing.\n4. **Limited Scale of Pretraining Data:** The paper claims to achieve open-world understanding. However, the pretraining is conducted on Objaverse-LVIS, which contains only ~47K objects. This is an extremely small dataset for vision-language pretraining, especially compared to the web-scale datasets often used for ANNs. This limited scale casts doubt on the model's true generalizability and its open-world capabilities."}, "questions": {"value": "1. Could the authors provide the parameter counts for the ANN baselines (Point-Bert, OpenShape, ULIP, ULIP-2) in Table 1, so a fair comparison can be made against the SNN models (E-3DSNN-L, E-3DSNN-H)?\n2. In Table 2, why does the SNN-based vision encoder (Spike-driven PointFormer-L) underperform the ANN-based PointBert when both use the same Vicuna LLM?\n3. Given that the pretraining dataset (Objaverse-LVIS) is very small, how can the authors be confident in the model's \"open-world\" generalization? Have they tested how this framework performs if trained on larger-scale 3D datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8GtnkUojM6", "forum": "JYke7CIuIa", "replyto": "JYke7CIuIa", "signatures": ["ICLR.cc/2026/Conference/Submission377/Reviewer_N53d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission377/Reviewer_N53d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815348288, "cdate": 1761815348288, "tmdate": 1762915506856, "mdate": 1762915506856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**This paper makes three contributions:**\n1. A spiking neural net (SNN) archtecture: \"spike-driven transformer\" that handles 3D inputs\n2. Multiscale Triple Alignment (MTA): A pretraining strategy for spiking NNs with image / text / 3D modalities, that aligns each modality with the spike trains, using InfoNCE (with an additional MSE loss for image-spike).\n3. Rep-VLI: core innovation is to pre-compute and embed textual information directly into the weights of a lightweight classification layer, completely discarding the text encoder during inference.\n\n**The authors show comparisons to baselines on many tasks:**\nzero-shot 3D classification (ModelNet40, Objaverse-LVIS), supervised 3D classification (ModelNet40, ScanObjectNN), 3D detection and segmentation (KITTI), and video action recognition (DVS), and captioning (Objaverse)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Compared to the listed SNN approaches, the propose recipe outperforms existing SNN recipes by a percent or two on some 3D classification / segmentation / detection benchmarks.\n\nFigure 1 is very clear, and the benchmarks are appropriate for this task. \n\nThe energy analysis seemed compelling, though I am not an expert in SNNs or neuromorphic hardware."}, "weaknesses": {"value": "The current version paper feels a little bit like a collection of approaches thrown together, that yields gives a couple percent on various 3D classification tasks, compared to existing SNN recipes. My main concerns are about the rigor of the experimental results and novelty. \n\nPossibly these concerns below could be addressed in a rebuttal or updated version of the initial manuscript, but my feeling at this point is that it may be stronger as a resubmission.\n\n---\n### Novelty\nThe paper claims three novel contributions:\n\n**Multiscale Triple Alignment: (MTA):**\nInfoNCE losses are a great tool for non-generative multimodal alignment (as shown in CLIP). InfoNCE losses may not have been used for Spiking Neural Networks before, but this seems like a pretty direct application to me -- makes sense.\n\n**Rep-VLI:**\nThey authors state that \"Rep-VLI’s core innovation is to pre-compute and embed textual information directly into the weights of a lightweight classification layer, completely discarding the text encoder during inference\". I've seen this optimization used in several published works -- I'd personally consider it a standard trick for closed-vocabulary inference. E.g. https://github.com/OpenGVLab/PonderV2/blob/aad65d6954633d82141de15d1eb5fa9a23964ee6/ponder/models/ponder/ponder_indoor_base.py#L85\n\n**Architecture:**\nFor architecture contributions I usually look mainly at empirical results to determine the strength of the contribution. \n\n\n---\n### Experiments: \n**Architecture analysis:**\nWhen a paper introduces a new architecture as a main contribution, it's very hard to know whether the findings at one scale of data/compute will apply elsewhere. I would like to see some type of scaling analysis that varies both the architectures size and the amount of data. This type of anlaysis is doubly important when introducing new attention operations, like in this paper, since attention is the main driver of transformers' strong scaling performance.\n\nI didn't see any analysis like that in the submission. For reference -- DiT does a very nice job of scaling experiments: https://arxiv.org/pdf/2212.09748, I would an accept an architecture based on scaling experiments like that alone.\nLlighter-weight analysis like PointTransformerV3 can also be good https://arxiv.org/pdf/2312.10035\n\n\n**Baselines:**\nThe conclusion of the abstract states that the proposed approach \"bridges the gap between SNNs and ANNs in complex open-world understanding tasks.\". Each experiential table has separate sections that explicitly make the comparison between SNN approaches and ANN approaches. However, for the main results, the ANN results I'm familiar with are much stronger than what I see in the table.\n\nPossibly these could be improved enought\nFor 3D object classification, PointMamba (2024, >250 references) shows numbers on ModelNet40 and ScanObjectNN which are stronger than both the selected ANN baselines and the proposed method. \nPointMamba: https://github.com/LMD0311/PointMamba\n\nFor video action recognition, the only comparison is to PointNet (2017).\n\nCompared to the shown spiking NN baselines, the results are stronger, but not as strong as current ANN baselines. \nWith the pace of papers coming out, no one could be reasonably expected to follow all recent literature -- but updating the ANN section with more relevant/recent baselines seems reasonable."}, "questions": {"value": "Table 1: IIRC, ScanObject3D has several subsets. I may have missed where the authors indicated which subset they are evaluating on?\nTable 3: should Spike Point TransFormer be in the \"SNN\" section instead of \"ANN\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5TgUKD7cAR", "forum": "JYke7CIuIa", "replyto": "JYke7CIuIa", "signatures": ["ICLR.cc/2026/Conference/Submission377/Reviewer_axrE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission377/Reviewer_axrE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889271642, "cdate": 1761889271642, "tmdate": 1762915506722, "mdate": 1762915506722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}