{"id": "qYDObsHldZ", "number": 15226, "cdate": 1758249117164, "mdate": 1759897320403, "content": {"title": "Adaptive Moments are Surprisingly Effective for Plug-and-Play Diffusion Sampling", "abstract": "Guided diffusion sampling relies on approximating intractable likelihood scores, which introduces significant noise into the sampling dynamics. We propose using adaptive moment estimation to stabilize these noisy likelihood scores during sampling. Despite its simplicity, our approach achieves state-of-the-art results on image restoration and class-conditional generation tasks, outperforming more complicated methods, which are often computationally more expensive. We provide empirical analysis of our method on both synthetic and real data, demonstrating that mitigating gradient noise through adaptive moments offers an effective way to improve alignment.", "tldr": "Adaptive moments are surprisingly effective for plug-and-play diffusion sampling.", "keywords": ["Guided Diffusion Sampling", "Plug-and-Play Conditional Diffusion Sampling", "Adaptive Moment Estimation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/25af42c4a07f9f779f3a6a55fa45941df12381d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a very simple plug-in modification to plug-and-play diffusion guidance: keep exponential moving averages of the likelihood-gradient during sampling (Adam-style first/second moments) and use the stabilized gradient to guide updates. The authors instantiate this on DPS (\"AdamDPS\") and on classifier guidance (\"AdamCG\"), provide a 2-D GMM toy study, and report empirical gains on various inverse problems."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The change is minimal (Adam-style moments around the guidance gradient) and easy to graft onto existing DPS/CG code paths.\n\n- The toy GMM illustrates how DPS becomes unstable under noisy likelihood gradients whereas AdamDPS stabilizes trajectories; several restoration benchmarks also show improvements in LPIPS/FID vs. DPS and some training-free baselines."}, "weaknesses": {"value": "- Technical novelty is minimal. The paper just adds Adam-style gradient update to DPS/CG at inference time. There is no new objective, solver, or estimator; the contribution is only an optimizer wrapper around an existing guidance gradient (Algorithms 1–2). This is a straightforward, well-known idea from stochastic optimization brought over without new theory specific to diffusion guidance beyond intuition. This falls short of ICLR's bar for conceptual advance. The empirical results are useful but do not compensate for the lack of technical depth or new insight into diffusion guidance.\n\n- Potential overfitting in Fig. 5. The combination of highest accuracy with worst FID suggests the guidance may be overfitting to the evaluation classifier rather than improving image quality. Classification accuracy alone does not imply good samples if the images are classifier-friendly but perceptually poor. Please use a different (held-out) classifier for evaluation than the one used during guidance/training, and report cross-model accuracy (and, ideally, human or ImageReward/VQA scores) to confirm the gains are not due to classifier overfitting."}, "questions": {"value": "- Can you replicate gains on other plug-and-play frameworks (e.g., DDRM, PiGDM, MPGD-style data-space updates) and other backbones/samplers to support the \"surprisingly effective\" claim beyond DPS/CG?\n\n- Beyond demonstrating that AdamDPS smooths noisy guidance, is there substantive novelty beyond applying an optimizer wrapper to DPS/CG?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xsqQ2Lr8MW", "forum": "qYDObsHldZ", "replyto": "qYDObsHldZ", "signatures": ["ICLR.cc/2026/Conference/Submission15226/Reviewer_FmMn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15226/Reviewer_FmMn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742207221, "cdate": 1761742207221, "tmdate": 1762925522624, "mdate": 1762925522624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the classifier guidance and finds out that classifier guidance approximates the likelihood score which often includes a lot of noise. The paper proposes to regularize the guidance sampling process with ADAM, a popular method in neural network optimization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written\n2. the method is easy to understand"}, "weaknesses": {"value": "1. The novelty might be the concern. The Adam is not novel and the guidance is also not novel. The use of Adam during sampling process is also not new (this paper has similar idea https://www.ijcai.org/proceedings/2024/0157.pdf). The only difference is that Adam is applied to guidance term instead of denoising term as in the paper. \n2. The paper does not provide a new fundamental observations or scientific hypothesis.\n3. In terms of applications, the proposed method currently could not extend to classifier-free guidance which is a very popular guidance right now. \n4. The quantitative results are not provided, different metrics are not considered e.g FID, IS, Rec, Prec, CLIP, GenEval, T2I Bench."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qi1v0y7DqN", "forum": "qYDObsHldZ", "replyto": "qYDObsHldZ", "signatures": ["ICLR.cc/2026/Conference/Submission15226/Reviewer_WUrY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15226/Reviewer_WUrY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994661209, "cdate": 1761994661209, "tmdate": 1762925522187, "mdate": 1762925522187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the likelihood approximation noise in training-free plug-and-play diffusion models such as DPS. These models estimate the posterior score using $\\nabla_xt log⁡p(xt∣y)=\\nabla_xt log⁡p(y∣xt) + \\nabla_xt log⁡p(xt)$, where the likelihood term $\\nabla_{x_t}\\log p(y|x_t)$ is approximated by a differentiable surrogate $\\nabla_{x_t}L(f_\\phi(\\hat x_{0|t}),y)$. At high noise levels (early and mid diffusion steps), this estimate becomes highly unstable, leading to biased and noisy sampling.\n\nThe paper proposes maintaining exponential moving averages (EMAs) of the first and second moments of the likelihood gradient, similar to Adam. By tracking running averages of the gradient and its squared values across timesteps and adaptively rescaling updates, the approach stabilizes the guidance direction and reduces variance. Two variants are presented: AdamDPS for diffusion posterior sampling and AdamCG for classifier guidance.\n\nExperiments demonstrate consistent improvements: on inpainting, deblurring, and $16\\times$ super-resolution tasks (ImageNet and Cat datasets), AdamDPS achieves 1–2 dB PSNR gains and lower LPIPS/FID than DPS, UGD, and LGD. On CIFAR-10 and ImageNet 64×64 class-conditional generation, AdamCG improves top-1 and top-10 accuracy without significant degradation in FID. Synthetic 2-D Gaussian mixture experiments confirm that adaptive moments smooth the gradient variance peak occurring at intermediate diffusion steps."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Addresses a key limitation—noisy and biased likelihood gradients—with a simple, general, and low-cost modification. \n\nEmpirically robust across datasets, with clear ablations on noise scales and $(\\beta_1, \\beta_2)$.\n\nMinimal implementation effort with measurable gains in quality and stability."}, "weaknesses": {"value": "Limited to gradient-based plug-and-play samplers; not applicable to variational or optimization-based methods such as RED-Diff.\n\nNo theoretical analysis showing that adaptive moments preserve unbiasedness or convergence to the correct posterior $p(x_0)\\exp[-r(x_0,y)]$.\n\nMissing comparisons with RED-Diff, MPGD, and TMPD, which already incorporate momentum or adaptive scaling.\n\nNo examination of how the moving averages interact with the diffusion dynamics or affect the stationary distribution.\n\n\nRED-Diff — M. Mardani, J. Song, J. Kautz, A. Vahdat. A Variational Perspective on Solving Inverse Problems with Diffusion Models, arXiv:2305.04391 (2023).\n\nMPGD — Y. He, N. Murata, C.-H. Lai, Y. Takida, T. Uesaka, D. Kim, W.-H. Liao, Y. Mitsufuji, J. Z. Kolter, R. Salakhutdinov, S. Ermon. Manifold Preserving Guided Diffusion, ICLR 2024.\n\nTMPD — B. Boys, et al. Tweedie Moment Projected Diffusions for Inverse Problems, arXiv:2310.06721 (2023)."}, "questions": {"value": "How does AdamDPS fundamentally differ from optimization-based samplers such as RED-Diff that already employ Adam updates?\n\nDoes maintaining EMAs across timesteps bias the effective sampling trajectory?\n\nHow would AdamDPS perform against RED-Diff or TMPD under the same compute and step budgets?\n\nCould combining adaptive moments with Monte-Carlo smoothing (e.g., LGD) provide further gains?\n\nHow sensitive is the performance to the number of diffusion steps or when applied to ODE-based samplers such as DDIM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DaKJahzs9z", "forum": "qYDObsHldZ", "replyto": "qYDObsHldZ", "signatures": ["ICLR.cc/2026/Conference/Submission15226/Reviewer_sSbi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15226/Reviewer_sSbi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129243916, "cdate": 1762129243916, "tmdate": 1762925521406, "mdate": 1762925521406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AdamDPS, a new plug-and-play method for diffusion sampling. By noting that the guided sampling update is gradient ascent using the score of the likelihood distribution, the authors introduce Adam-style moment exponential moving averages to stabilize the overall update. The resulting method is tested on a toy GMM example, demonstrating improvements in sampling quality over the baseline diffusion posterior sampling (DPS). Experiments are then extended to reconstruction tasks on image datasets, where AdamDPS demonstrates the best combination of both FID and LPIPs. Ablations are performed on task-difficulty, Adam hyperparams, and wall-clock time."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper is complete and well-presented. Overall, the method is effective and experiments are thorough. I would recommend it for acceptance.\n\n**(S1)**: Simple and effective. The core idea of introducing Adam-style moment stabilization is simple and effective for diffusion guided plug-and-play sampling. The paper clearly motivates the problem with previous approaches, proposes a sound improvement, and demonstrates the improvement via experimental results.\n\n**(S2)**: Clear improvement over prior work on reconstruction tasks. Results in Fig 3 clearly demonstrate superior reconstruction quality and sample quality of AdamDPS. \n\n**(S3)**: Comprehensive ablations and analysis. The ablations on task difficulty is valuable and demonstrates the core motivation for the method-- as the task gets noisier, the stabilization introduced by Adam-style moments results in better performance. Other ablations on number of sampling steps and moment coefficient values are useful for any future users of this method. \n\n**(S4)**: Computational efficiency. The ablation on wall-clock time confirms that this method induces little to no overhead over baselines, which is valuable."}, "weaknesses": {"value": "**(W1)**: Slightly mixed results on class-conditional sampling. The FID on ImageNet and CIFAR-10 seems worse than baselines, even as the classification accuracy is much better. \n\n**(W2)**: Slight regressions over baselines on easy tasks. In Fig 6, for easier super-resolution or deblurring tasks, AdamDPS is slightly worse than TFG. This again slightly clouds the otherwise clear narrative of the paper. No explanation for this is given.\n\n**(W3)**: Details on the method are missing. A clearer explanation of the models used, experimental setup, metrics would be very valuable for clarity. For example, while an ablation on $\\beta_1$ and $\\beta_2$ were performed, it's not clear what the optimal values are for each task."}, "questions": {"value": "**(Q1)**: Is there a missing increment to $k$ in algorithms 1 and 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PbRitf5JtJ", "forum": "qYDObsHldZ", "replyto": "qYDObsHldZ", "signatures": ["ICLR.cc/2026/Conference/Submission15226/Reviewer_br5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15226/Reviewer_br5n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762568981760, "cdate": 1762568981760, "tmdate": 1762925520974, "mdate": 1762925520974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}