{"id": "OpFf3ZjiFa", "number": 22222, "cdate": 1758327958920, "mdate": 1759896879328, "content": {"title": "Selecting Out-of-Distribution Detector for Multiple Modalities", "abstract": "Out-of-distribution (OOD) robustness is a critical challenge for modern machine learning systems, particularly as they increasingly operate in multimodal settings involving inputs like video, audio, and sensor data. Currently, many OOD detectors have been proposed, each with different designs targeting various distribution shifts. A single OOD detector may not prevail across all the scenarios; therefore, how can we automatically select an ideal OOD detection model for different distribution shifts? Due to the inherent unsupervised nature of the OOD detection task, it is difficult to predict model performance and find a universally best model. Also, systematically comparing models on the new unseen data is costly or even impractical. To address this challenge, we introduce M3OOD, a meta-learning-based framework for OOD detector selection in multimodal settings. Meta learning offers a solution by learning from historical model behaviors, enabling rapid adaptation to new data distribution shifts with minimal supervision. Our approach combines multimodal embeddings with handcrafted meta-features that capture distributional and cross-modal characteristics to represent datasets. By leveraging historical performance across diverse multimodal benchmarks, M3OOD can recommend suitable detectors for a new data distribution shift. Experimental evaluation demonstrates that M3OOD consistently outperforms 10 competitive baselines across 12 test scenarios with minimal computational overhead.", "tldr": "", "keywords": ["Out-of-Distribution Detection", "Multimodal Learning", "Automatied Machine Learning", "Model Embeddings"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/edb4e7143aaa7c692e6c69e794e478cd62c3893a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents M3OOD, a meta-learning framework for zero-shot selection of Out-of-Distribution detectors in multimodal settings. The approach leverages historical performance data and combines multimodal embeddings with handcrafted meta-features to recommend suitable detectors for new datasets without requiring expensive evaluations. Experiments on video and optical flow data show M3OOD achieves competitive performance against multiple baselines with minimal overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the relevant challenge of automated OOD detector selection, extending it to multimodal contexts. The zero-shot selection paradigm is highly valuable for real-world deployment.\n2. The runtime analysis convincingly demonstrates that M3OOD's overhead is negligible compared to the cost of running the OOD detectors themselves, highlighting its practical utility."}, "weaknesses": {"value": "1. The Role of Table 3 is Unexplained and Misleading: Table 3, which presents the full performance matrix of all detectors on all dataset pairs, is the foundation of the meta-learning process and the primary evidence for the problem's existence. Yet, it is never mentioned or explained in the main text. This creates a critical disconnect for the reader.\n2. The claim of a general \"multimodal\" framework is significantly undermined by the exclusive use of video and optical flow—two highly correlated modalities. This limited scope makes the contribution feel more like an incremental adaptation of established meta-learning model selection to a specific data pair, rather than a broader breakthrough for heterogeneous multimodal data (e.g., vision-language, audio-text).\n3. Critical design choices lack thorough investigation. The justification for using XGBoost over more sophisticated neural meta-predictors is weak, and the simple concatenation for modality fusion ignores potentially more effective cross-modal interaction mechanisms."}, "questions": {"value": "1. If I understand correctly, Table 3 is central to understanding both the problem and your method's training data. Why is it not discussed in the main text?\n2. The performance of the Mahalanobis detector is catastrophic on some datasets (e.g., ~43% on EPIC). Does this indicate instability in the feature extraction or training process for certain detector-dataset pairs? Could such outliers negatively bias the meta-learner?\n3. How does M3OOD's performance degrade when the test dataset is highly dissimilar from all meta-training datasets? Please provide an analysis of its sensitivity to the meta-training distribution.\n4. Beyond concatenation, were more advanced fusion strategies for multimodal embeddings explored? Given the importance of cross-modal interactions, this seems a significant missed opportunity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FHTlPb36x5", "forum": "OpFf3ZjiFa", "replyto": "OpFf3ZjiFa", "signatures": ["ICLR.cc/2026/Conference/Submission22222/Reviewer_i7wL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22222/Reviewer_i7wL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479100046, "cdate": 1761479100046, "tmdate": 1762942121782, "mdate": 1762942121782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the unsupervised model selection problem for out-of-distribution (OOD) detection in multimodal settings. The authors argue that since no single OOD detector universally excels across all data distributions, an automated method for selecting the best detector for a new, unseen task is necessary. They propose M3OOD, a meta-learning framework that learns from the historical performance of a pool of OOD detectors across a variety of multimodal datasets. The framework represents new tasks using a combination of learned multimodal embeddings and handcrafted meta-features."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed problem is interesting\n2. The empirical evaluation is comprehensive"}, "weaknesses": {"value": "1. The study evaluates only post-hoc OOD detection methods. In scenarios requiring strong robustness, the importance of selecting a single OOD detector may be overstated, as multiple detectors could be used jointly to enhance reliability.\n2.  According to Table 3, the performance rankings of some OOD detectors vary only slightly across different datasets. This raises questions about the necessity of model selection when the performance differences are not substantial.\n3. Using a meta-learning approach for OOD detector selection introduces biases from the meta-training data. This may limit the generalizability of the selected detector to new and dissimilar distribution shifts.\n4. The runtime analysis in Figure 4 lacks clarity in its measurement setup. A more meaningful comparison should include total execution time, especially since M3OOD targets post-hoc methods. Inference time should also be compared to fully assess efficiency."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bVYGiRodNV", "forum": "OpFf3ZjiFa", "replyto": "OpFf3ZjiFa", "signatures": ["ICLR.cc/2026/Conference/Submission22222/Reviewer_kdbY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22222/Reviewer_kdbY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707251675, "cdate": 1761707251675, "tmdate": 1762942121507, "mdate": 1762942121507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes M3OOD, a meta-learning framework for selecting an OOD detector in multimodal settings. Experiments across Near-OOD and Far-OOD splits on several action-recognition datasets compare against 10 selection baselines"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Generally clear and readable; figures and tables are informative; Codes are publicly available."}, "weaknesses": {"value": "1. while this paper advertises multimodal selection across “video, audio, and sensor data,” the experiments fully focus on video. The applicability to text–image, audio, or sensor modalities is not demonstrated.\n2. All benchmarks are action-recognition datasets with related statistics and similar backbones, which may not reflect broader multimodal OOD realities (e.g., text–image tasks with VLMs on imagenet-1k).\n3. Common OOD metrics such as FPR@95% are absent.\n4. Baseline set includes trivial strategies (Random, Global Best, Mega-Ensemble) and out-of-date methods (no baseline published on/after 2024 is included in this paper.)\n5. Detectors are encoded via one-hot vectors, which restricts the practical usefulness since detector sets change frequently."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NSALkCgKgH", "forum": "OpFf3ZjiFa", "replyto": "OpFf3ZjiFa", "signatures": ["ICLR.cc/2026/Conference/Submission22222/Reviewer_PsuK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22222/Reviewer_PsuK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886727968, "cdate": 1761886727968, "tmdate": 1762942120992, "mdate": 1762942120992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes M3OOD, a meta-learning framework for automated selection of out-of-distribution (OOD) detectors in video understanding tasks. The core idea is to train a meta-predictor that, given characteristics of a dataset and a pool of candidate OOD detectors, recommends the most suitable detector without requiring retraining or access to ground-truth OOD labels.  Evaluation is performed by comparing the rank of the selected detector’s performance (measured via AUC-ROC) among all candidates across multiple dataset pairs. Results show that M3OOD outperforms several baselines, including random selection, MSP, Mahalanobis, and ensemble methods, achieving higher average ranks and demonstrating generalization to unseen datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The challenge of selecting an appropriate OOD detector without prior knowledge or extensive validation is highly relevant, especially in real-world deployment scenarios where manual tuning is infeasible. This work addresses a critical gap in practical OOD detection.\n\n2. The construction of multimodal meta-features—including video-specific, optical flow, and statistical features—is thoughtful and domain-informed. These features go beyond generic data statistics and are tailored to video data, enhancing the plausibility of cross-dataset generalization.\n\n3.  The paper employs a clean train-test split strategy across diverse datasets (HMDB, UCF, Kinetics, EPIC), ensuring that the meta-predictor is evaluated on unseen OOD conditions. The distinction between Near-OOD and Far-OOD settings adds depth to the evaluation."}, "weaknesses": {"value": "1. The method is evaluated exclusively on video datasets. While the meta-features are well-designed for video, the paper overreaches in suggesting broad applicability to other modalities without empirical validation. Claims about \"multimodal\" applicability are misleading—the input data is multimodal (RGB + flow), but the framework itself is not tested beyond video.\n\n2. There is no analysis of which meta-features or embedding components contribute most to performance. Without ablation studies, it is unclear whether the complexity of the complete feature set is justified or if simpler variants would suffice.\n\n3. While XGBoost is a reasonable choice, the paper does not compare it to alternative meta-learners (e.g., neural networks, random forests) that might better exploit the structured meta-features or embeddings.\n\n4. Relying solely on AUC-ROC rank as the evaluation metric obscures actual performance gaps. A method could be ranked highly but still underperform drastically in absolute terms. Reporting average AUC differences or failure rates would provide more insight."}, "questions": {"value": "Can you quantify the contribution of different meta-feature groups (e.g., video-specific vs. statistical vs. model embeddings) through ablation studies? Is the performance gain worth the added complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bojtxr9PqW", "forum": "OpFf3ZjiFa", "replyto": "OpFf3ZjiFa", "signatures": ["ICLR.cc/2026/Conference/Submission22222/Reviewer_zP9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22222/Reviewer_zP9D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016839928, "cdate": 1762016839928, "tmdate": 1762942120718, "mdate": 1762942120718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}