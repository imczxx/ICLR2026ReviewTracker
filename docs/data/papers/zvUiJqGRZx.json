{"id": "zvUiJqGRZx", "number": 206, "cdate": 1756730921545, "mdate": 1759898271677, "content": {"title": "CharacterShot: Controllable and Consistent 4D Character Animation", "abstract": "In this paper, we propose \\textbf{CharacterShot}, a controllable and consistent 4D character animation framework that enables any individual designer to create dynamic 3D characters (i.e., 4D character animation) from a single reference character image and a 2D pose sequence. We begin by pretraining a powerful 2D character animation model based on a cutting-edge DiT-based image-to-video model, which allows for any 2D pose sequnce as controllable signal. We then lift the animation model from 2D to 3D through introducing dual-attention module together with camera prior to generate multi-view videos with spatial-temporal and spatial-view consistency. Finally, we employ a novel neighbor-constrained 4D gaussian splatting optimization on these multi-view videos, resulting in continuous and stable 4D character representations. Moreover, to improve character-centric performance, we construct a large-scale dataset Character4D, containing 13,115 unique characters with diverse appearances and motions, rendered from multiple viewpoints. Extensive experiments on our newly constructed benchmark, CharacterBench, demonstrate that our approach outperforms current state-of-the-art methods. Code, models, and datasets will be publicly available.", "tldr": "CharacterShot supports diverse character designs and custom motion control (2D pose sequence), enabling 4D character animation in minutes and without specialized hardware.", "keywords": ["4d generation", "diffusion models", "pose-driven generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98f3c53e03a3367b4f03034c714b07dee2478ece.pdf", "supplementary_material": "/attachment/0c1ed477f28eb916fb50640488ca1c697083c5d7.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CharacterShot, a controllable 4D character animation framework that generates multi-view consistent 3D characters from a single reference image and a 2D pose sequence. Built upon the DiT-based image-to-video model CogVideoX, CharacterShot enables user-defined motion control and extends 2D animation to 3D by incorporating a dual-attention module and camera priors to ensure spatio-temporal and cross-view consistency. To create stable 4D outputs, the method applies a neighbor-constrained 4DGS optimization. The authors also present Character4D, a large-scale dataset and promised to open-source."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "CharacterShot introduces a novel framework that generates 4DGS from a single reference image and a 2D pose sequence. It combines a dual-attention module and camera priors to ensure both spatial-temporal and cross-view consistency. The proposed neighbor-constrained 4D Gaussian Splatting further improves geometric stability and reduces artifacts in motion. \n\nAdditionally, the introduction of the large-scale Character4D dataset significantly advances research resources for future 4D character animation. The effectiveness of constructed dataset is shown with the comparison with SV3D."}, "weaknesses": {"value": "CharacterShot adopted 4DGS rather than animatable 3DGS, which introduces practicality concerns.\n\nGenerating a separate 4DGS for every motion sequence is computationally expensive and time-consuming, which CharacterShot require 20+30 minutes in H800 GPU. In contrast, animatable 3DGS methods such as LHM[1] and AniGS[2] can model dynamic motion through deformation fields or skeletal animation, enabling efficient reuse of a single canonical representation across sequences. \n\nFurthermore, the paper lacks direct comparison with recent animatable 3DGS methods such as LHM and AniGS, which are more suitable baselines for evaluating motion-controllable representations. This omission limits the fairness and completeness of the evaluation, leaving open whether CharacterShot’s 4D generation truly offers superior efficiency or flexibility in real-world animation scenarios.\n\nIn summary, my major concern is (1) What is the benefit of generating in 4DGS rather than animatable 3DGS (2) and the lack of comparison with those methods.\n\n[minor]\nThere are few errors in citation. \n\nL69 :  Objverse Deitke et al. (2023) > Objverse (Deitke et al., 2023)\n\nL544 : missing year for Cameractrl\n\n\n[1] LHM: Large animatable human reconstruction model from a single image in seconds. arXiv preprint arXiv:2503.10625.\n\n[2] AniGS: Animatable gaussian avatar from a single image with inconsistent gaussian reconstruction. CVPR 2025"}, "questions": {"value": "L990–994: Is the H800 a consumer-grade GPU?\n\nHow long can CharacterShot generate? Is it fixed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F7BNemXhKs", "forum": "zvUiJqGRZx", "replyto": "zvUiJqGRZx", "signatures": ["ICLR.cc/2026/Conference/Submission206/Reviewer_RGZ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission206/Reviewer_RGZ3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760609895139, "cdate": 1760609895139, "tmdate": 1762915469575, "mdate": 1762915469575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CharacterShot, a novel 4D character animation framework aiming to \"democratize\" the CGI pipeline. It enables individual creators to generate controllable and consistent 4D (i.e., dynamic 3D) character animations from a single character reference image and a 2D pose sequence. The framework is a multi-stage pipeline: First, it pretrains a DiT-based 2D video model to accurately follow input 2D poses. Second, it lifts this 2D model to 3D by introducing a novel \"dual-attention module\" and camera priors to generate multi-view videos with spatio-temporal and cross-view consistency. Finally, it employs a \"neighbor-constrained 4D Gaussian Splatting (4DGS)\" optimization method to reconstruct a continuous and stable 4D character representation from these generated videos. To support this task, the authors also contribute a large-scale dataset, Character4D (with 13,115 characters), and a new benchmark, CharacterBench. Experiments demonstrate that the method outperforms existing SOTA approaches on the authors' self-constructed benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novel and Practical Problem Formulation: Generating 4D animation from a single image and 2D poses is a highly challenging yet valuable task. It significantly lowers the barrier to 4D content creation, as its input requirements are far less restrictive than methods requiring multi-view videos, 3D models, or even single-view videos. This has strong practical application potential.\n\nSolid Technical Contributions: The paper proposes a complete and technically sound pipeline to address this complex problem. The \"dual-attention module\" for simultaneously modeling spatial-temporal and spatial-view consistency is a key innovation for lifting 2D to multi-view 3D. Furthermore, the \"neighbor-constrained 4DGS\" directly tackles the stability issues inherent in 3D reconstruction from AI-generated videos, which may suffer from noise or inconsistencies."}, "weaknesses": {"value": "Self-Serving Evaluation on a Niche Benchmark: A major weakness is that all quantitative comparisons rely exclusively on the authors' newly created CharacterBench, which is built from their own Character4D dataset. While dataset contribution is noted, this creates a circular evaluation loop where the method is tested on the same data distribution it was trained on (or at least a very similar one, derived from VRoidHub). This benchmark, filled with 13k anime-style characters, may not be representative of broader 4D animation challenges (e.g., real humans, complex physics) and makes it impossible to fairly compare against SOTA methods, which were tuned for different data and tasks.\n\nQuestionable Generalization and Robustness: While the appendix shows generalization to \"Out-of-Character4D\" samples (real humans, other 3D models), the main evaluation is heavily focused on anime-style characters from VRoidHub. The method's performance on real-world humans, complex clothing dynamics (like capes or skirts), and its robustness to inaccurate pose estimations (which the authors admit as a limitation) are not thoroughly or quantitatively validated.\n\nSystem Complexity and Potential Error Propagation: The entire framework is a cascaded system (2D Animation -> Multi-view Video Gen -> 4DGS Optimization). This multi-stage pipeline is highly complex and relies on multiple fine-tuned models (e.g., CogVideoX and SV3D). This implies that errors can propagate and accumulate: for instance, if the multi-view video generation in Stage 2 is of low quality or lacks consistency, the 4DGS optimization in Stage 3 will likely fail to recover a high-quality 4D result, even with the neighbor-constraint."}, "questions": {"value": "Regarding Generalization: The Character4D dataset consists mainly of anime-style characters. How would CharacterShot perform if trained or tested on broader datasets focused on real humans (e.g., People-Snapshot, THHuman 2.0, or other dynamic human capture datasets)? This is crucial for understanding the model's ability to generalize to realistic human morphology and complex textures.\n\nRegarding the Dual-Attention Module: Could the authors elaborate on the design of the \"dual-attention module\"? How does it fundamentally differ, architecturally and computationally, from simply using separate (or sequential) spatial, temporal, and view-attention mechanisms (as critiqued in SV4D)? Why is this design more effective at capturing \"implicit visual transmission\"?\n\nRegarding Dependency on Intermediate Stages: The framework appears to rely on a fine-tuned SV3D as a \"View Generator\" to provide reference images for the multi-view video generation. To what extent is the final 4D animation quality sensitive to the fidelity of this initial static multi-view generation? If the view generator fails (e.g., produces an incorrect view or distorted details), can the system handle this robustly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iELKcCDTkz", "forum": "zvUiJqGRZx", "replyto": "zvUiJqGRZx", "signatures": ["ICLR.cc/2026/Conference/Submission206/Reviewer_7CK3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission206/Reviewer_7CK3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377573541, "cdate": 1761377573541, "tmdate": 1762915469437, "mdate": 1762915469437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presented a method, referred as CharacterShot, to create dynamic 3D characters from a single reference character image and a 2D pose sequence. Given a reference character image, CharacterShot employs the I2V model CogVideoX to generate its corresponding video sequence controlled by pose conditions. The paper proposed a dual-attention module to improve the spatio-temporal and cross-view consistency in the video. Afterwards, CharacterShot employs a coarse-to-fine 4D Gaussian Splatting to fix artifacts in the multi-view videos. The paper also presented a new Character4D animation dataset including 13,115 unique characters to fine-tune the SV3D model to generate multi-view images in the CharacterShot method. A separate CharacterBench is used to evaluate and compare with 4 other single-view video-driven 4D generation methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The visual quality of the multi-view character videos generated by CharacterShot appears clean and consistent, and very close to the ground truth. \n\nThe dual-attention module, which uses parallel 3D full attention blocks to enforce visual consistency across spatial-temporal multi-view images, is an interesting and novel approach. \n\nThe coarse-to-fine 3DGS, including the neighbor constraints in the fine stage, appears a reasonable post-processing to improve the character video."}, "weaknesses": {"value": "Since CharacterShot employs the I2V model CogVideoX that is DiT-based to generate the video given a character image, the paper claims this is the first DiT-based 4D character animation work. This claim is not a well-supported one to me. \n\nThe major experiments are only conducted on the new CharacterBench dataset introduced in this paper. The fairness of the comparison with 4 other methods on the this CharacterBench needs further justification, e.g., if other methods have been fine-tuned on the Character4D dataset as well."}, "questions": {"value": "What are the essential difference between generating multi-view videos from a character image or a human figure image? Which task is more challenging?\n\nThe proposed CharacterShot framework does not seem to be restricted to \\emph{character} video generation, except that the multi-view images are generated by the SV3D model fine-tuned on the Character4D dataset. So, if using a SV3D not finetuned on Character4D and employing a reference image of a real person, what the performance of the proposed method would be and compared to many latest relevant works? The technical advantages of the proposed method, rather than using dedicated character datasets, would be validated on other human figure video generation tasks.\n\nThe sample character images shown in the paper do not show that much diversity. Any more sophisticated characters in high-resolution, e.g., NPC in modern games, are tested?\n\nA minor suggestion: if some references have been published in previous ICLR, I would suggest to cite their ICLR version instead of the arXiv version."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r1NEmnEJBb", "forum": "zvUiJqGRZx", "replyto": "zvUiJqGRZx", "signatures": ["ICLR.cc/2026/Conference/Submission206/Reviewer_MbSK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission206/Reviewer_MbSK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761453127629, "cdate": 1761453127629, "tmdate": 1762915469249, "mdate": 1762915469249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CharacterShot, a novel framework that takes a 2D character image and a 2D pose sequence to generate a 4D animated character. This leverages the I2V model fine-tuned to take 2D pose sequences and camera parameters and output multi-view videos of the character, and then trains a 4DGS model to represent character animation in 4D. A dual 3D full-attention mechanism that applies to the view-spatial dimension and temporal-spatial dimension is used to ensure multi-view consistent video generation. Neighbor-based regularization in the multi-view video for 4DGS optimization enforces geometric consistency. Trained with the Character4D dataset, the method demonstrates superior generation results compared to prior methods using I2V in a single view and creating 4D from it."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* A novel framework that takes a 2D character image and a 2D pose sequence for 4D generation sounds promising, as the 2D input is more convenient than typical input while providing decent control over the generated motion.\n* The result demonstrates superior multi-view consistency for the generated 4D character animation.\n* A novel dataset built for the novel framework allows for further exploration of the idea."}, "weaknesses": {"value": "* Single-view video to 4D baselines, not just 2D to video part, could also be fine-tuned for fair comparison. Their quality degradation is more noticeable in novel views, which may be due to a lack of training data for generating multi-view character videos from single-view character videos.\n* Related works discuss prior works with too much focus on the general trend of generation methods. A better summary and greater emphasis on highly relevant works scattered across different sections on character-focused 3D/4D generation (with motion control) would be beneficial."}, "questions": {"value": "* Please clarify the test split used for CharacterBench and the train split used by the Character4D dataset: are motions or characters shared across splits, or are they from a similar source? This can impact the validity of the benchmark result.\n\n* L234: for a each -> for each\n* L297: The group seems to be neighbors, and how those neighbors are selected for each splat could be clarified here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E67AKIkCkn", "forum": "zvUiJqGRZx", "replyto": "zvUiJqGRZx", "signatures": ["ICLR.cc/2026/Conference/Submission206/Reviewer_peLs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission206/Reviewer_peLs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910713539, "cdate": 1761910713539, "tmdate": 1762915469057, "mdate": 1762915469057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}