{"id": "4zrxudTpzn", "number": 16984, "cdate": 1758270932095, "mdate": 1759897206245, "content": {"title": "Balancing Mixed Labels: Mixup meets Neural Collapse in Imbalanced Learning", "abstract": "*Minority collapse*, where minor classes become indistinguishable, is a key challenge in imbalanced learning, addressed by methods like Mixup with class-balanced sampling. \nIn parallel, a simplex equiangular tight frame from Neural Collapse (NC) has emerged as an effective frame in the classifier to mitigate minority collapse. \nWhile NC has been studied in both Mixup and imbalanced learning independently, its combination remains unexplored, particularly regarding the balance of mixed labels. \nWe investigate this overlooked factor and pose the question: *Is the mixed label balance important for alleviating minority collapse?* \nOur analysis reveals that (i) mixed labels should be balanced, and (ii) in this setting, interpreting mixed labels as singletons is beneficial.\nBuilding on the analysis, we propose a balanced mixed label sampler and a mixed-singleton classifier, which balance mixed labels and treat them as singleton labels.\nThrough theoretical analysis, visualization, and ablation studies, we demonstrate the effectiveness of our approach. \nExperiments on standard benchmarks further confirm consistent performance gains, highlighting the importance of balancing mixed labels in imbalanced learning.", "tldr": "We reveal issues of Mixup in imbalanced learning and propose a sampler (BMLS) and a classifier (MS), which balance mixed labels and treat them as singleton labels", "keywords": ["imbalanced learning", "mixup", "neural collapse"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b5ff1dd416a34ee0583bba2f7bfa71da6d914cba.pdf", "supplementary_material": "/attachment/07735c5e6ac169c11355a6be90dd26f56915ba6a.zip"}, "replies": [{"content": {"summary": {"value": "This work addresses minority collapse in imbalanced learning by proposing to balance mixed-label samples generated by Mixup. The authors introduce a Balanced Mixed Label Sampler (BMLS) and a Mixed-Singleton Classifier (MS), demonstrating that their approach yields performance gains, particularly for minority classes,"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a pertinent issue in imbalanced learning by investigating the often-overlooked factor of mixed-label balance in conjunction with Mixup.\n2. The theoretical analysis, grounded in the Layer-Peeled Model and Neural Collapse, provides a structured lens through which to analyze the problem."}, "weaknesses": {"value": "1. Insufficient and Outdated Empirical Comparisons:   \ni) The empirical evaluation primarily benchmarks against methods up to 2022 (e.g., LOM, CAS), omitting comparisons with more recent and potent state-of-the-art techniques. This selective baseline choice casts doubt on the claimed effectiveness and fails to situate the contribution within the current landscape.  \nii) Crucially, several highly relevant and strong baselines, such as Remix [a] (which explicitly manipulates mixed labels for balance) and other well-established methods (e.g., [b-c]), are either absent or relegated to a non-comparative grayed-out section in Table 7.\n2. Theoretical Analysis Lacks Rigor and Practical Alignment:   \ni)  Key components of the theoretical framework (e.g., Lemma 2, Proposition 1) are not self-contained but instead heavily rely on citations without verifying their direct applicability in the novel context of mixed labels. This undermines the theoretical contribution's solidity.  \nii) The convexity assumption of the loss function contradicts the non-convex nature of training deep networks with cross-entropy loss. The paper does not adequately justify how the semidefinite relaxation (Eq. 11) bridges this gap.    \niii) The analysis is based on the simplified Layer-Peeled Model, but the experiments employ standard deep networks. The paper does not provide evidence that the behaviors analyzed in LPM faithfully approximate those of the deep models used in practice.\n3. The core innovation of interpreting mixed labels as singletons is not conclusively validated. The performance gain of MS could stem merely from increasing the classifier's capacity to K^2 units. A controlled ablation, using a classifier with K^2 randomly initialized or fixed ETF units, is needed to isolate the benefit of the specific mixed-singleton construction.  \n4. The mixup coefficient λ is a critical hyperparameter that governs the interpolation between samples and labels. It is surprising that the paper contains no sensitivity analysis on λ or the parameter α of the Beta distribution it is sampled from. The performance and the proposed theoretical claims (e.g., independence from the mixup ratio) could be highly sensitive to this parameter. The robustness of BMLS and MS across different λ regimes remains an open and unverified question.\n\nReferences:  \n[a] Chou et al. Remix: rebalanced mixup. ECCV'2020.  \n[b] Cui et al. Parametric contrastive learning. ICCV'2021.  \n[c] Yang et al. Inducing neural collapse in imbalanced learning: Do we really need a learnable classifier at the end of deep neural network? NeurIPS'2022."}, "questions": {"value": "1. Why were more recent and directly relevant strong baselines (e.g., [a-c]) not included in the primary comparisons? Can the authors provide results against these methods?\n2. Could the authors provide more detailed proofs or intuitive justification for the applicability of Lemma 2 and Proposition 1 in the mixed-label setting, which is central to this work?\n3. How does the semidefinite relaxation (Eq. 11) handle the non-convexity inherent in training deep networks? Is there empirical evidence that the solutions of the LPM align with the features and classifiers learned by the deep models in the experiments?\n4. Can the authors conduct an ablation study using a classifier with K^2 singleton units not constructed via mixup? This would help decouple the effect of increased classifier capacity from the specific mixed-singleton interpretation.\n5. What is the sensitivity of the proposed methods to the mixup hyperparameters (λ or α)? Please provide a sensitivity analysis to demonstrate the robustness of your findings across different settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dAfJlUBcnI", "forum": "4zrxudTpzn", "replyto": "4zrxudTpzn", "signatures": ["ICLR.cc/2026/Conference/Submission16984/Reviewer_ucbY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16984/Reviewer_ucbY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538792000, "cdate": 1761538792000, "tmdate": 1762926997460, "mdate": 1762926997460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of minority class disappearance in imbalanced learning settings. The authors analyze Mixup from the perspective of neural collapse (NC) and propose BMLS and MS as methods to tackle this issue and enhance performance. They demonstrate through experiments that the proposed architecture achieves improved performance compared to existing approaches."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper analyzes Mixup*from the perspective of neural collapse (NC) and proposes a new architecture to address the problem of minority class disappearance. This approach leads to improved performance."}, "weaknesses": {"value": "A major concern lies in the clarity of the claims and the appropriateness of the theoretical justification presented in the paper.\n\n- It is difficult to clearly understand what the main claim of this paper is. The work appears to aim at improving Mixup from the perspective of neural collapse (NC). In this context, the proof presented in Section 5 seems to show that the proposed architecture satisfies the NC-related properties that the authors consider desirable. There appear to be two possible interpretations:  \n    (a) The claim that satisfying these NC properties leads to improved performance is already well established, and the authors are showing that their proposed architecture indeed satisfies them — in which case the proof can be seen as providing theoretical guarantees.  \n    (b) The authors themselves claim that these NC properties are beneficial and then show that their architecture satisfies them — in which case the argument functions more as motivation and background for the proposed architecture.\n    \n    These two interpretations differ significantly in terms of their scientific contribution, and it is important to make clear which one the paper intends. At present, I interpret the work as closer to (b). In that case, recognizing both the novelty of the proposed method and the value of theoretical guarantees simultaneously risks falling into circular reasoning, and therefore cannot be straightforwardly accepted.\n    \n- The intended effect of the proposed method is also unclear. While the disappearance of minority classes is identified as the main challenge, the experiments report only mean accuracy. It is not clear how this metric is defined, and more importantly, mean accuracy alone does not demonstrate improvements specifically for the minority classes. To substantiate the claimed contribution, the paper should report results that directly assess performance on the minority classes. As it stands, it is difficult to judge whether the proposed method truly achieves the effect it claims.\n    \n- Section 5 presents only a sketch of a proof, introducing new concepts without clearly stating what is actually being proven. As a result, it is very difficult to grasp the main claim or the logical structure of the argument."}, "questions": {"value": "Please clarify which of the two cases — **(a)** or **(b)** — represents the main claim of the paper.\n\nCan you also demonstrate through experiments how the proposed method affects the minority classes specifically?\n\nFinally, please explicitly state what is being proven in Section 5 — i.e., clearly formulate the statement or proposition that the proof is intended to establish."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3vvhFjrcbk", "forum": "4zrxudTpzn", "replyto": "4zrxudTpzn", "signatures": ["ICLR.cc/2026/Conference/Submission16984/Reviewer_a1Sm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16984/Reviewer_a1Sm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884415850, "cdate": 1761884415850, "tmdate": 1762926997159, "mdate": 1762926997159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses minority collapse in long-tailed learning under Mixup.\n It argues that collapse stems from imbalanced frequencies of mixed labels (pair-level imbalance) rather than class imbalance alone.\n To fix this, the authors propose: 1. Balanced Mixed Label Sampler (BMLS) – equalizes the occurrence of every label pair (a,b); 2. Mixed-Singleton Classifier (MS) – treats each mixed label as a new singleton class.\n They provide theoretical analysis via the Layer-Peeled Model (LPM) showing that pair-level balance mitigates collapse, and validate the claim empirically on multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: The paper introduces a new perspective — that pair-level imbalance (in mixed labels) is the true driver of collapse under Mixup. \n\n2. Quality: The theoretical analysis seems rigorous and well-motivated. Experiments align well with theory and are conducted carefully across multiple benchmarks.\n\n3. Clarity: Well-organized paper with clear motivation–theory–experiment flow."}, "weaknesses": {"value": "**1. Theoretical Limitations:**\n The claim that MS is “equivalent to an LPM with K^2 classes” is only approximate. Since mixed-class weights W^\\lambda_{(a,b)} = \\lambda w_a + (1-\\lambda) w_b are not independent, the coupling among weights might alter the geometric equilibrium. A formal treatment of this dependency would strengthen the theory.\n\n\n**2. Assumption Robustness:**  The main theoretical result assumes perfectly uniform pair sampling (P(a,b) constant). In practice, BMLS approximates this stochastically per mini-batch, which may not fully satisfy uniformity. A robustness analysis for imperfect balance or bounded variance would make the theoretical claims more convincing.\n\n\n**3. Scalability** :  While the authors note scalability as a theoretical limitation—due to the quadratic growth of label pairs with KKK—this also manifests empirically: the method performs strongly on smaller datasets (e.g., CIFAR-LT) but yields weaker gains on large-scale benchmarks such as ImageNet-LT, Places-LT, and iNaturalist2018.\nThis suggests that the pair-level balancing principle, though theoretically sound, may not fully scale in practice. Further analysis of this behavior or mechanisms to preserve balance under high-class regimes would strengthen the method’s generalization impact."}, "questions": {"value": "Please refer to the weaknesses above.\n\n1. Could the authors clarify whether the K^2-class equivalence of MS is purely theoretical or affects actual classifier behavior?\n\n\n2. How sensitive is the theoretical guarantee to deviations from perfectly uniform pair sampling? Have the authors observed performance degradation when the pair distribution is only approximately balanced?\n\n\n3. Could the authors elaborate on why the method’s gains diminish on large-scale datasets? Do they attribute this to computational constraints, representation saturation, or limitations of pair-level balancing itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VPF1Cs4MbI", "forum": "4zrxudTpzn", "replyto": "4zrxudTpzn", "signatures": ["ICLR.cc/2026/Conference/Submission16984/Reviewer_GkCW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16984/Reviewer_GkCW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949015020, "cdate": 1761949015020, "tmdate": 1762926996527, "mdate": 1762926996527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}