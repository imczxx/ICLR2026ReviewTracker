{"id": "QixNhagZ9t", "number": 1323, "cdate": 1756870632698, "mdate": 1759898215316, "content": {"title": "TS-Attn: Temporal-wise Separable Attention for Multi-Event Video Generation", "abstract": "Generating high-quality videos from complex temporal descriptions, which refer to prompts containing multiple sequential actions, remains a significant challenge. Existing methods are constrained by an inherent trade-off: using multiple short prompts fed sequentially into the model improves action fidelity but compromises temporal consistency, while a single complex prompt preserves consistency at the cost of prompt following capability. We attribute this problem to two primary causes: temporal misalignment between video content and the prompt, and conflicting attention coupling between motion-related visual objects and their associated text conditions. To address these challenges, we propose a novel, training-free attention mechanism, Temporal-wise Separable Attention (TS-Attn), which dynamically rearranges attention distribution to ensure temporal awareness and global coherence in multi-event scenarios. TS-Attn can be seamlessly integrated into various pre-trained text-to-video models, boosting StoryEval-Bench scores by 33.5% and 16.4% on Wan2.1-T2V-14B and Wan2.2-T2V-A14B with only a 2% increase in inference time. It also supports plug-and-play usage across models for multi-event image-to-video generation. The source code and video demos are available in the supplementary materials.", "tldr": "", "keywords": ["Video generation", "Diffusion model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da474ee3d1e56d5791617a144c29146e8c62d097.pdf", "supplementary_material": "/attachment/27d1df449ba08990a8296f8312bce552236e8905.zip"}, "replies": [{"content": {"summary": {"value": "This work presents advancements in a typical issue presented by video generation methods, which often produce sequences of events that present temporal anomalies like overlapping events and incorrect ordering. This problem is also discussed in detail, to better understand its causes and justify the approach.\n\nThe solution presented is TS-Attn, a method that requires a modification to the cross-attention layer such that it makes use of event ordering information presented in the prompt itself. This expanded attention layer  requires additional input of temporal segmentation information, which is generated using either an external API, human data, or a simpler segmentation method.\n\nFinally, the work does very complete experiments which (except for the caveats in the weaknesses section) appear to produce an important improvement to the provided baselines, while being a relatively simple to implement addition to the video generation systems."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper has substantial strengths:\n\n1) the problem is worth solving, and the root cause analysis is excellent\n2) the presented method is (to the extent of my knowledge) novel and interesting; event-aware attention modulation in particular seems like an interesting approach to me\n3) the benchmark is very complete and required the integration to 3 different models, which is impressive\n4) the benchmark results that are indicated in the paper are very strong, except for the points raised in the weaknesses section which will hopefully be easy to fix\n5) finally, the paper is well written, and the explanation is very clear"}, "weaknesses": {"value": "This work presents a very complete analysis of an original method.\n\nThe soundness and contribution scores of this paper are diminished by ambiguity on the impact of latency in the full system, as explained below.\n\nSpecifically, the work would benefit from a more detailed explanation of how the uniform segmentation method works. When explaining this method, the paper refers to a number of events in the prompt, but it's unclear how the events in the prompt are parsed themselves. To me this seems like a critical point because if the event segmentation in the prompt has to be provided by a model for example, the time to run such segmentation should be accounted for in the latency analysis.\n\nThis applies generally to claims about latency, for example in figure 1c, and in other tables in the paper. It's unclear to me which segmentation method was used for each figure and table, whether segmentation itself was included in the latency analysis or not, and whether TS-Attn required additional human input that other methods didn't require (e.g. if uniform segmentation required manual selection of prompt events).\n\nIn summary, the paper could benefit from:\n\n1) more clearly specifying how the uniform segmentation method works\n2) providing details on which segmentation method was used for each figure and table\n3) providing details on the impact of segmentation on latency\n\nMy understanding is that in most figures the LLM API approach is used, and it only adds about 2-3 seconds to latency (changing the +2% latency claim to 2.2% or 2.3% perhaps), but it would be good to be more explicit about this.\n\nMore minor points:\n\n4) including the prompts used in the segmentation methods in the appendix could be an interesting addition.\n5) that figure 2 makes use of 3 colors, 2 of which can be easily confused with each other."}, "questions": {"value": "My main questions are about the segmentation pipeline and full impact on latency, which are discussed in detail in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yjrApPHnl0", "forum": "QixNhagZ9t", "replyto": "QixNhagZ9t", "signatures": ["ICLR.cc/2026/Conference/Submission1323/Reviewer_oX7g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1323/Reviewer_oX7g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760723771476, "cdate": 1760723771476, "tmdate": 1762915736160, "mdate": 1762915736160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Temporal-wise Separable Attention, a training-free method to improve video generation with complex multi-event prompts. The proposed attention mechanism enables this by dynamically restructuring cross-attention distributions to ensure motion-related regions in each frame primarily attend to temporally aligned events, demonstrating superior results across various video generation models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem is well-defined and the solution is intuitively designed. The visualization in Figure 2 makes this more convincing.\n- The proposed Attention Rearrangement and Attention Reinforcement are carefully designed to better inject multi-prompts while maintaining pre-trained generative priors in a training-free manner.\n- The experimental results are extensive. The proposed method has been implemented on various pre-trained models to demonstrate its effectiveness and is also compared with other methods for multi-prompts."}, "weaknesses": {"value": "- There are some heuristics arising from the training-free design. For example, the erosion function and the accompanying spatial separation of subject tokens limit the applicable scenarios of this method to simple ones. For instance, cases where the subject is the style of the video or cannot be clearly distinguished in 2D spatial aspects fall outside this premise.\n- While the attached video results look good, there is an absence of video comparisons with other multi-prompt methods. More qualitative comparisons in the paper would also be beneficial."}, "questions": {"value": "Are there any issues with sudden scene changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zpPoRItams", "forum": "QixNhagZ9t", "replyto": "QixNhagZ9t", "signatures": ["ICLR.cc/2026/Conference/Submission1323/Reviewer_3mMY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1323/Reviewer_3mMY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966191216, "cdate": 1761966191216, "tmdate": 1762915735983, "mdate": 1762915735983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TS-Attn, a training-free attention mechanism that improves multi-event video generation by dynamically separating and modulating cross-attention between motion regions and multi-event textual conditions. By introducing motion region extraction and event-aware attention modulation, the method reduces temporal misalignment and cross-event coupling, achieving better temporal coherence and event accuracy. TS-Attn can be plugged into existing diffusion-based video models without retraining, yielding substantial performance gains on StoryEval-Bench with only ~2% extra inference cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated and intuitive idea that directly targets temporal attention entanglement in multi-event video generation.\n\n- Training-free and plug-and-play design makes it broadly applicable across existing diffusion models.\n\n- Extensive experiments and ablations demonstrate consistent improvements and robustness across architectures and benchmarks.\n\n- Clear presentation and visualizations that effectively explain both the mechanism and empirical benefits."}, "weaknesses": {"value": "**Lack of comparison with prior methods**\nThe paper omits several highly relevant works that also manipulate cross-attention maps to achieve fine-grained event grounding without retraining, such as DreamRunner [1], VideoTetris [2], and TALC [3].\nThese methods similarly align textual tokens with corresponding visual regions through attention reweighting, making them conceptually close to TS-Attn. However, the authors neither cite nor compare with them. Including these approaches as baselines or at least discussing their differences would strengthen the paperâ€™s positioning and contribution clarity.\n\n**Unclear generalization to multi-subject scenarios**\nThe proposed motion-region extraction appears to assume a single dominant subject, computing masks for the entire video latents.\nIn cases involving subject transitions (e.g., a person leaves and a cat enters), this design may fail to isolate subject-specific motion regions, leading to incorrect or conflicting event-to-visual grounding.\nClarifying how TS-Attn handles such cases, or showing examples involving multiple subjects, would improve the completeness of the work.\n\n---\n[1] DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation, 2024.\n\n[2] VideoTetris: Towards Compositional Text-to-Video Generation, 2024.\n\n[3] TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation, 2024."}, "questions": {"value": "See weakness. Overall, I lean towards borderline for the current version and am happy to update my rating if my questions are well answered."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KfMclvwXca", "forum": "QixNhagZ9t", "replyto": "QixNhagZ9t", "signatures": ["ICLR.cc/2026/Conference/Submission1323/Reviewer_Qohm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1323/Reviewer_Qohm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005184904, "cdate": 1762005184904, "tmdate": 1762915735586, "mdate": 1762915735586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper proposes a training-free temporal-wise separable attention for multi-event conditioned video generation"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the motivation of the work is clear and well-explained. the proposed motion region extraction and event-based attention modulation are intuitive and visualizations are reasonable\n- extensive experiments demonstrate the effectiveness of the proposed method. the visual results are convincing and clearly show the multi-event coherence\n- the method achieves reasonable performance improvements with negligible overhead\n- the paper provided source codes in the supplementary material"}, "weaknesses": {"value": "- the benchmark protocol and the evaluation metrics are not well-justified. while I understand there is no reasonable benchmark framework in the current field, the reliability of the vlm-based evaluation is still questionable, especially when evaluated against a commercial endpoint. in that case, it is hard to determine the actual performance gain based on the reported scores. a user study is highly recommended to validate the effectiveness of the proposed method considering the human evaluation is still the most reliable metric for video generation tasks\n- it is unclear what is the max possible number of events the proposed method can handle\n- what is the success rate of a given prompt? what are the typical failure cases?\n- while the proposed framework provides a solution for multi-subjects, it is unclear whether the proposed framework can faithfully handle multiple subjects with same/similar actions"}, "questions": {"value": "please refer to the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zraFI0YaWq", "forum": "QixNhagZ9t", "replyto": "QixNhagZ9t", "signatures": ["ICLR.cc/2026/Conference/Submission1323/Reviewer_HkQf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1323/Reviewer_HkQf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762252577658, "cdate": 1762252577658, "tmdate": 1762915735313, "mdate": 1762915735313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}