{"id": "s7oURFZTQD", "number": 14653, "cdate": 1758240868522, "mdate": 1759897357153, "content": {"title": "Why Multi-Grade Deep Learning Outperforms Single-Grade: Theory and Practice", "abstract": "Multi-grade deep learning (MGDL) has recently emerged as an alternative to standard end-to-end training, referred to here as single-grade deep learning (SGDL), showing strong empirical promise. This work provides both theoretical and experimental evidence of MGDL’s computational advantages. We establish convergence guarantees for gradient descent (GD) applied to MGDL, demonstrating greater robustness to learning-rate choices compared to SGDL. In the case of ReLU activations with single-layer grades, we further show that MGDL reduces to a sequence of convex optimization subproblems. For more general settings, we analyze the eigenvalue distributions of Jacobian matrices from GD iterations, revealing structural properties underlying MGDL’s enhanced stability. Practically, we benchmark MGDL against SGDL on image regression, denoising, and deblurring tasks, as well as on CIFAR-10 and CIFAR-100, covering fully connected networks, CNNs, and transformers. These results establish MGDL as a scalable framework that unites rigorous theoretical guarantees with broad empirical improvements.", "tldr": "This paper explores the reason why multi-grade deep learning outperforms single-grade deep learning.", "keywords": ["Multi-Grade Deep Learning", "theory", "practice"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a840266a4a187f6d83a8b4ff4729769dc524b815.pdf", "supplementary_material": "/attachment/4eb2f9400fb79e7e569b534317313497bddd7bfc.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces **Multi-Grade Deep Learning (MGDL)**, a training framework that decomposes end-to-end optimization into multiple shallow “grades,” each trained sequentially on the residual errors of previous stages. The authors claim this approach improves convergence stability and robustness to learning rates compared to standard **Single-Grade Deep Learning (SGDL)**. Theoretically, they prove that when each grade uses a single ReLU layer, the training reduces to a series of convex subproblems, and spectral analysis shows MGDL maintains Jacobian eigenvalues within (−1, 1), ensuring stable loss decay. Empirically, MGDL outperforms SGDL on image regression, denoising, CIFAR-10/100, and time-series prediction, showing smoother convergence and higher accuracy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an interesting and well-organized discussion of Multi-Grade Deep Learning (MGDL) as an alternative to standard end-to-end training. It combines both theoretical analysis and diverse experimental evidence, offering a clear connection between the proposed method’s mathematical properties, such as convergence guarantees and spectral stability, and its empirical performance across multiple domains including image regression, classification, and time-series prediction. This integration of theory and practice makes the study both insightful and convincing."}, "weaknesses": {"value": "1. **Unconvincing Empirical Validation**: The paper aims to explain **why** MGDL outperforms SGDL, but **whether** this claim holds is still debatable. MGDL is not yet a widely recognized training paradigm, and the current small-scale, toy-level experiments are insufficient to establish its general superiority across deep learning. A single comparative experiment on a canonical benchmark, such as training a ResNet-50 on ImageNet, would be far more persuasive than multiple synthetic or low-complexity tasks.\n2. **Limited and Disconnected Theoretical Contribution**: The main theoretical novelty is confined to Section 4, which shows that a single-layer ReLU network under MGDL yields a convex optimization problem. However, the connection between this convex proof and the empirical performance gains of MGDL remains unsubstantiated, leaving the theory detached from the observed advantages.\n3. **Overlap with Layer-Wise Training Methods**: MGDL closely resembles established **layer-wise training** approaches. The paper should explicitly discuss this relationship and clarify whether MGDL introduces any substantial difference beyond conceptual rebranding.\n4. **Lack of Standard Training Practices**: Section 6 omits common practices such as learning-rate warm-up and decay schedules, which are known to enhance stability and robustness. Their absence weakens the experimental validity of the claimed training stability improvements of MGDL.\n5. **Minor Presentation Issues**: Minor issues include small font sizes in some figures (e.g., Figures 4–5) and typographical errors:\n   - Line 408: duplicated “0.004”.\n   - Line 461: “Although oth models fit the training data affectively” → “Although both models fit the training data effectively.”"}, "questions": {"value": "1. **Effectiveness in Fine-Tuning Scenarios**: Is MGDL also effective when applied to fine-tuning a pre-trained network?\n2. **Loss of Layer Differentiation**: Previous studies such as [1] have shown that different layers in deep networks play distinct roles during end-to-end training. In contrast, MGDL freezes earlier grades and prevents such differentiation from emerging. Does this structural uniformity benefit generalization, or does it harm the representational hierarchy learned by deep models?\n3. **Guidelines for Grade Partitioning**: For a given architecture, how should one decide where to divide the network into grades? Is there a principled or empirical method for choosing the grade boundaries, or is it entirely task-dependent?\n\n[1] Do Vision Transformers See Like Convolutional Neural Networks? (NeurIPS 2021)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OnSwMSViLt", "forum": "s7oURFZTQD", "replyto": "s7oURFZTQD", "signatures": ["ICLR.cc/2026/Conference/Submission14653/Reviewer_zTvu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14653/Reviewer_zTvu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929012686, "cdate": 1761929012686, "tmdate": 1762925024939, "mdate": 1762925024939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies \"multi-grade\" deep learning. Which corresponds to training multiple models, each deeper then the previous, and each trained on the residue of the sums of models before. Only the last few layers, corresponding to the new added layers for the current model are updated while building each of these models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper harkens back to the classic layerwise training methods that pre-dated the end-to-end deep learning methods prevalent today. There is definitely potential value in revisting this framework."}, "weaknesses": {"value": "The main weakness is that theory sections 1 to 4 have minimal contribution. A layerwise model training on residual is interesting but not truly novel in the unheard of sense. This is acceptable if the theory or experimental results are strong. Unfortunately it seems neither is true in the current stage of the paper.\n\n1. Sections 1,2 and 3 are basic setup sections and do their job well (Figure 1 was particularly useful for me in understanding the setup). Theorem 1 and 2, while true don't really say anything interesting. e.g. some relation on $ L_l^* $ and Lstar would be required for at least some special cases. If $L_l^* $ is not lesser than $L^*$, why bother with MGDL? The line after Theorem 2, on $\\alpha_l$ and $\\alpha$ is also too hand-wavy. Why is $\\alpha_l << \\alpha$ ?\n\n2. Section 4, is arguably the main theoretical section and disappoints greatly. While equivalence of 7 and 8 is valid, it hides the fact that $P_l$ is exponential in the input-dimension. Computing the \"finite\" possibilites of the D_l matrix is also non-trivial. Hence minimising Equation 8 is not at all practical. Thus the value of Theorem 3 is very minimal. An equivalence in the other direction -- critical points of Eq. 7 are minimizers of Equation 8 would be a huge accomplishment. But it is likely not true. The non-convexity being bypassed is simply not true. At the risk of sounding vacuous, for *any* non-convex problem, one can find a convex problem whose minimizer would also be a minimiser of the non-convex problem. That is all has been stated in Theorem 3. \n\n3. All theoretical concerns would be moot in the face of undeniable empirical performance. I am not sure that is the case here. The original reference for MGDL seems to be in solving differential equations, I am not familiar with that area. Maybe MGDL really is a useful fit there, I am not sure.  But empirical support for MGDL in classic problems like classification and regression is far from established. e.g. in CIFAR10 classification the test accuracy is not even reported. I am not sure about the best/SOTA baseline umbers for the other tasks (Image regression/denoising) so I can't comment. But if the poposed method is really SOTA, significantly more baselines need to be reported across significantly more experiments.\n\n4. If the authors feel that MGDL is a valid approach in some concrete area, like say image denoising, I highly recommend pivoting the entire paper around such a domain. This also likely requires more experiments and exhaustive  baselines, but is likely the best way forward.\n\n5. If the authors want to stick to the general theory message of \"MGDL outperforms SGDL\". I recommend a significant reduction in scope, and one specific (maybe synthetic) setting where something meaningful and concrete can be shown for a modified version of Theorem 2 and 3.\n\nEither way the paper is fixed, the changes would be too big to be accepted as is."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "forlqlMe3u", "forum": "s7oURFZTQD", "replyto": "s7oURFZTQD", "signatures": ["ICLR.cc/2026/Conference/Submission14653/Reviewer_ybLx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14653/Reviewer_ybLx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076987832, "cdate": 1762076987832, "tmdate": 1762925024365, "mdate": 1762925024365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This major goal of this paper is to compare \"multi-grade deep learning\" (MGDL) and \"single-grade deep learning\" (SGDL, the standard end-to-end training of deep networks). In MGDL, a deep network is expressed as the composition of $L$ shallower networks, so that one can define $L$ grades ranging from the first network component to the entire deep network. The first grade is trained on the original data, and each subsequent grade is trained on the residual generated by previous grades. The main idea is to train networks in stages, where each shallow grade builds on the residuals of the previous one and propagates its output forward, incrementally approximating the target function and avoiding vanishing or exploding gradients.\n\nThe main contributions of the paper are as follows: (i) Theoretical guarantees for the convergence of SGDL (Theorem 1) and MGDL (Theorem 2). In particular, the authors prove that under certain conditions, MGDL allows for a much larger learning rate and thus mitigates the issue caused by exploding gradients. (ii) The paper presents extensive numerical experiments to show that MGDL consistently outperforms SGDL in several applications, demonstrating better robustness to large step size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The theory part of this paper is simple and neat, and provides insights to why MGDL should better mitigates vanishing/exploding gradients as compared to SGDL. The ReLU network example in Section 4 is also interesting since it gives an example of how MGDL can be used in practice to solve a convex optimization reformation of optimizing a deep network. The empirical validations are also thorough and convincing."}, "weaknesses": {"value": "My major concern is the following: It seems to me that both Theorem 1 and Theorem 2 should be trivial generalizations (if not exactly the same) of convergence theorems for GD that can be found in textbooks, and the upper bounds on the learning rate (i.e., $2 / \\alpha$ and $2 / \\alpha_l$) are also pretty standard. To make this result more convincing, maybe the authors can add some examples to compute $\\alpha$ and $\\alpha_l$ for deep neural networks? Also, the assumptions of Theorem 1 and Theorem 2 might not be satisfied in practice. For example, they assume that the activation function is $C^2$, which is not satisfied by the ReLU example in Section 4. They also assume that the iterates remain bounded throughout the entire GD path, which is rather strong. In fact, in many learning problems, a key step in proving convergence is to show that the iterates remains bounded."}, "questions": {"value": "Below are some minor comments to the authors: (i) For citing some of the references, e.g., citing two papers together, please use the command \\citep to put them in a parenthese; (ii) I am a bit confused by the notation of Section 3, shouldn't $g_{l+1}$ and $N_{D_{l+1}}$ be the same thing? If not, where is $N_{D_{l+1}}$ defined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ghWoAyix48", "forum": "s7oURFZTQD", "replyto": "s7oURFZTQD", "signatures": ["ICLR.cc/2026/Conference/Submission14653/Reviewer_qcqR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14653/Reviewer_qcqR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095861355, "cdate": 1762095861355, "tmdate": 1762925023880, "mdate": 1762925023880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a theoretical and empirical explanation for why Multi-Grade Deep Learning (MGDL) outperforms traditional end-to-end or Single-Grade Deep Learning (SGDL). MGDL divides deep optimization into multiple shallow subproblems (“grades”), each trained sequentially on residual errors. The authors prove convergence guarantees for gradient descent, show that when each grade has a single ReLU layer the overall nonconvex problem reduces to a sequence of convex subproblems, and analyze eigenvalue distributions to explain MGDL’s improved stability. Experiments across image regression, denoising, deblurring, CIFAR-10/100 classification, and transformer-based time series prediction demonstrate more robust and stable training than SGDL.\n\nThe paper’s combination of convergence analysis, convex reformulation, and broad empirical validation represents a meaningful theoretical and practical advance. Showing that MGDL keeps iteration eigenvalues within (-1,1) and decomposes ReLU networks into convex subproblems is novel and potentially impactful. The results generalize across architectures, including CNNs and Transformers, suggesting that MGDL provides a promising framework for stable deep optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear theoretical connection between MGDL’s modular training and improved convergence properties.\n\n- Convincing empirical validation across multiple domains with consistent PSNR and loss improvements.\n\n- Eigenvalue-spectrum analysis provides strong intuition for the oscillatory instability of SGDL.\n\n- Demonstrates generality by applying MGDL to Transformers.\n\n- Theoretical and empirical sections complement each other, offering both rigor and relevance."}, "weaknesses": {"value": "- The statement that MGDL “has recently emerged as an alternative to standard end-to-end training, showing strong empirical promise” appears overstated given its current level of adoption (roughly a dozen citations). The authors should temper this claim or clarify it as potential rather than established impact.\n\n- Does MGDL maintain its stability benefits when using SGD or Adam instead of full-batch GD as in practice SGD is used instead of GD ssepcially if dataset is very large ?\n\n- End-to-end gradients allow mutual adjustment of weights across layers; MGDL stops this once a grade is fixed. Probably where one expects it to fail and when not would be good idea to include. Similarly, in CCN feature dependencies across depth are cooperative and global — freezing submodules prevents global feature realignment.\n\n- Comparisons with other staged or layer-wise training paradigms (e.g., greedy pretraining, boosting-style methods) are absent and would strengthen the empirical context.\n\n-The paper would benefit from an explicit discussion of when MGDL is expected to work and when it might fail. In particular, MGDL’s advantages appear strongest for regression-style or low-level vision problems where layers act as additive refiners, while it may underperform in architectures or tasks that require deep inter-layer coordination (e.g., semantic CNNs or transformer attention models). Clarifying this boundary of applicability would make the contribution more practically useful and prevent overgeneralization."}, "questions": {"value": "-- Can MGDL be interpreted as a form of gradient boosting or residual fitting? How does it differ formally?\n\n-- Does MGDL maintain its stability benefits when using SGD or Adam instead of full-batch GD ( may be empirically) ?\n\n-- Would combining MGDL with normalization or skip connections provide additional stability, or would this be redundant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T0NvjdgeWo", "forum": "s7oURFZTQD", "replyto": "s7oURFZTQD", "signatures": ["ICLR.cc/2026/Conference/Submission14653/Reviewer_CXYz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14653/Reviewer_CXYz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762536201613, "cdate": 1762536201613, "tmdate": 1762925023201, "mdate": 1762925023201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}