{"id": "QuaJ6kJaBm", "number": 4375, "cdate": 1757668870493, "mdate": 1759898036171, "content": {"title": "Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks when equipped with external tools. However, current frameworks predominantly rely on sequential processing, leading to inefficient execution particularly for tasks requiring extensive tool interaction. This paper introduces Flash-Searcher, a novel parallel agent reasoning framework that fundamentally reimagines the execution paradigm from sequential chains to directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into subtasks with explicit dependencies, enabling concurrent execution of independent reasoning paths while maintaining logical constraints. Through dynamic workflow optimization, our framework continuously refines the execution graph based on intermediate results, effectively integrating summary module. Comprehensive evaluations across multiple benchmarks demonstrate that Flash-Searcher consistently outperforms existing approaches. Specifically, it achieves **67.7%** accuracy on BrowseComp and **83%** on xbench-DeepSearch, while reducing agent execution steps by up to **35%** compared to current frameworks. Furthermore, when distilling this parallel reasoning pipeline into single models, we observe substantial performance gains across diverse backbone architectures, underscoring the generalizability of our methodology. Our work thus represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks.", "tldr": "This paper proposes a DAG-based parallel LLM agent framework that resolves sequential inefficiency via concurrent subtasks, outperforms baselines, and shows generalizability on agent models to advance efficient complex reasoning.", "keywords": ["Workflow Optimization", "Agent Reasoning", "WebAgent", "Deep Research"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0cb56186bd9c9d6e725cad3a5b5b2a3795b83814.pdf", "supplementary_material": "/attachment/61555ba63089a64073a02b5a16fc08005d6efdb4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces FLASH-SEARCHE, a novel parallel agent reasoning framework. The core idea is to reconstruct the agent's execution paradigm from a sequential \"chain\" into a Directed Acyclic Graph (DAG). Specifically, it first decomposes a complex task into multiple subtasks with explicit dependencies (DAG-based Plan Construction); next, it concurrently executes independent reasoning paths and tool calls; finally, it uses an \"Adaptive Progress Tracking & Summarization\" module to dynamically optimize the workflow and continuously refine the execution graph based on intermediate results. Experimental results are good, demonstrate that the framework achieves SOTA performance while significantly boosting efficiency. For example, it achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch. Compared to baselines (like OAgents), it reduces agent execution steps by up to 35% and shortens overall execution time by 65%. Furthermore, the paper also demonstrates that this parallel reasoning strategy can be \"distilled\" and transferred to open-source models (like Qwen-2.5) via Supervised Fine-Tuning (SFT), achieving significant performance gains"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Significant Efficiency Gains and Cost Reduction**  \n    The framework's primary advantage is fundamentally solving the sequential execution bottleneck through parallelization. Experiments show it reduces agent execution steps by up to 35% and shortens overall execution time by 65%. This directly translates into lower end-to-end latency and computational costs, effectively addressing the practical viability issues of existing agents.\n\n2.  **Breaking the Efficiency-Effectiveness Trade-off with SOTA Performance**  \n    While many efficiency-boosting methods sacrifice performance, FLASH-SEARCHER achieves SOTA or highly competitive performance across four challenging benchmarks (BrowseComp, xbench-DeepSearch, GAIA, and HLE) while simultaneously delivering dramatic efficiency gains. For example, it achieves 67.7% on BrowseComp and 83.0% on xbench-DeepSearch, proving its parallel paradigm can complete complex reasoning tasks efficiently and with high quality.\n\n3.  **Validated Generalizability and Learnability of Parallel Reasoning**  \n    The paper's contribution is not just a framework, but also a generalizable methodology. The authors collected parallel reasoning trajectories and successfully \"distilled\" this capability into open-source models (the Qwen-2.5 series) using Supervised Fine-Tuning (SFT). This demonstrates that parallel reasoning is a learnable and scalable inductive bias, providing a new pathway for training more efficient, open-source agent models."}, "weaknesses": {"value": "### Disadvantages and Limitations\n\n* **Limited Task Spectrum and Capability Trade-offs:** The framework's impressive efficiency gains appear to be achieved by deliberately sacrificing key capabilities. The authors explicitly state that code interpreters were excluded as a \"deliberate design choice\" because managing their parallel execution and output volume would create \"substantial overhead\" and \"severely impact\" the framework's efficiency. This design choice avoids the risk of context explosion and management overhead but results in \"suboptimal performance on mathematical reasoning tasks.\" This limitation is compounded by the Crawl Tool, which truncates web content at 60,000 characters, introducing another source of information loss. Consequently, the SOTA performance is demonstrated on a limited spectrum of benchmarks (i.e., information-retrieval-heavy tasks), and its capability on mixed-tasks requiring complex computation remains a significant weakness.\n\n* **Un-quantified Overhead of DAG Maintenance:** The paper does not provide a quantitative analysis of the *additional computational cost* introduced by the framework's core mechanisms. Specifically, the \"DAG-based Plan Construction\" and the periodic \"Adaptive Progress Tracking & Summarization\" modules must incur their own LLM token costs for planning and summarizing. The paper acknowledges this \"computational overhead\" and suggests it can be tuned via the update interval $\\Delta$, but fails to provide any analysis on:\n    1.  The specific token cost of this maintenance.\n    2.  How this overhead scales with task complexity.\n    3.  A score-cost trade-off analysis by varying $\\Delta$. A thorough evaluation should present a curve showing how task success and *total cost* (execution cost + maintenance cost) change with different $\\Delta$ values, and whether this trade-off curve remains consistently superior to the baselines.\n\n* **Insufficient Baseline Comparison for Framework Evaluation:** The evaluation in Section 4.1 compares the FLASH-SEARCHER *framework* (using GPT-5) against other prompt-based frameworks. However, it lacks a comparison against the most advanced, proprietary \"deep research\" systems, which may themselves be *end-to-end trained* or highly optimized frameworks. The baselines used (e.g., \"OpenAI ChatGPT agent\") may not represent the true SOTA in *efficient* agentic research. While the *model* (Section 4.2) is compared against other trained models, the *framework's* SOTA claim is weakened by not being benchmarked against potentially stronger, specialized, and non-public industrial systems."}, "questions": {"value": "Same to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2bBFcbMrih", "forum": "QuaJ6kJaBm", "replyto": "QuaJ6kJaBm", "signatures": ["ICLR.cc/2026/Conference/Submission4375/Reviewer_aePY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4375/Reviewer_aePY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761225285290, "cdate": 1761225285290, "tmdate": 1762917324123, "mdate": 1762917324123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Flash-Searcher, a framework for tool-augmented LLM agents designed to overcome the latency and inefficiency of standard sequential execution. The core idea is to transform the agent's workflow from a linear chain (like ReAct) into a directed acyclic graph (DAG). This allows the framework to decompose a complex task into multiple subtasks with explicit dependencies, enabling the concurrent, parallel execution of independent reasoning paths. The authors claim this DAG-based approach, combined with dynamic workflow optimization, achieves good performance on several challenging web-agent benchmarks (e.g., BrowseComp, xbench-DeepSearch) while simultaneously reducing the number of agent execution steps by up to 35%. The paper also explores \"distilling\" these parallel reasoning trajectories by fine-tuning open-source models (like Qwen-2.5), claiming these models also achieve SOTA performance against other models of similar scale."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses a Good Problem: The paper tackles the high latency and inefficiency of sequential agent frameworks, which is a major bottleneck for their practical application.\n2. Good Empirical Performance: When using the powerful GPT-5 backbone, the Flash-Searcher framework achieves SOTA results on several difficult benchmarks, outperforming other frameworks (including another GPT-5 baseline, MiroFlow, as shown in Table 5).\n3. Good Efficiency Analysis: Section 5 provides an analysis of the framework's efficiency, demonstrating a concrete reduction in execution steps, which directly correlates to lower end-to-end latency."}, "weaknesses": {"value": "1. Misleading Experimental Details: In Figure 3, the authors did not clarify which models were used as the backbones for these baselines, making it unclear whether the performance gains come from the framework or the models themselves.\n\n2. Missing Critical Ablation Study: The paper must include a controlled study comparing a sequential ReAct-style agent against the parallel Flash-Searcher agent, using the exact same backbone model (e.g., GPT-4.1, or GPT-5 if possible), tools, and base prompts. This is the only way to scientifically validate the core claim that the DAG-based parallel execution is superior to sequential execution in terms of performance and/or efficiency.\n\n3. Unclear Conceptual Novelty* The paper fails to clearly differentiate its contribution from a crowded field of related work on parallel and graph-based agents (e.g., Plan-over-Graph, ParallelSearch, LATS). The descriptions in Section 2.2 and Appendix B are brief, and the stated differences (e.g., \"relaxed constraints\") are not empirically justified. A comparison table is needed to clarify the specific innovations of Flash-Searcher.\n\n4. Underwhelming Distillation Results: The results for the fine-tuned models (Table 1) are not compelling. The performance gains over existing SOTA fine-tuned models are incremental, not transformative. This weakens the paper's broader claim that it introduces a \"generalizable... parallel agent paradigm\" that can be effectively learned by smaller models. The modest gains suggest the parallel traces themselves may not be significantly more \"correct\" or \"efficient\" in a way that aids learning, and that the framework's main benefit is in runtime orchestration of powerful closed-source models."}, "questions": {"value": "Please solve the weakness provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z6PnuuJqUG", "forum": "QuaJ6kJaBm", "replyto": "QuaJ6kJaBm", "signatures": ["ICLR.cc/2026/Conference/Submission4375/Reviewer_URHx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4375/Reviewer_URHx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536666320, "cdate": 1761536666320, "tmdate": 1762917323600, "mdate": 1762917323600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel framework specifically designed to address the latency and computational inefficiency bottleneck found in existing sequential LLM-based frameworks for complex reasoning tasks. \n\nIts contributions include: \n1. It introduces a dynamic DAG-based framework for agent tool orchestration, supporting controlled aggressive parallelization that relaxes strict dependency constraints to maximize concurrent execution.\n2. It demonstrates state-of-the-art performance on multiple benchmarks (BrowseComp, xBench-DeepSearch, GAIA, HLE) while reducing execution steps by up to 35% compared to prior methods.\n3. It validates the parallel reasoning paradigm is scalable and transferable, through distilling FLASH-SEARCHER trajectories into open-source models (e.g., Qwen-2.5 series) via lightweight supervised fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel parallel framework: shifting LLM agent execution from sequential chains to a dynamic Directed Acyclic Graph (DAG) for parallel tool orchestration. This framework does strike a balance between theoretical soundness and practical efficiency.\n2. Clear methodology: The DAG construction, readiness predicate φ, and adaptive progress refinement (R) are formally defined and explained clearly.\n3. Comprehensive evaluation: Uses both framework-level and model-level experiments, including efficiency analysis, ablation on tool calls, and cross-benchmark validation."}, "weaknesses": {"value": "1. The central distinction and novelty of the framework do not lie in the DAG structure itself which is standard for task scheduling, but rather in the two specific optimization modules that enable parallel execution and dynamic refinement. These core innovations currently lack the technical detail necessary for reproduction:\n\n1.1 Ambiguity in Cross-Validation: The mechanism for \"controlled aggressive parallelization\" is technically underspecified. The paper introduces the readiness predicate ($\\varphi$) that allows starting a task based on \"auxiliary signals\" and \"heuristic consistency checks,\" but fails to detail the low-level implementation of this heuristic or what constitutes an auxiliary signal, hindering reproducibility. \n\n1.2 Ablation on Dynamic Update Interval ($\\Delta$): The system relies on a periodic update interval ($\\Delta$) to balance overhead and performance. However, the paper does not include an empirical ablation study to justify the chosen default of $\\Delta=7-9$ steps, leaving the claim of optimal balance unsubstantiated.\n\n2. Limited Evaluation Scope (Simple Tasks): The evaluation focuses exclusively on complex, multi-step tasks where parallelism is advantageous. The paper does not analyze whether the fixed overhead of DAG decomposition and parallel management introduces unnecessary latency compared to minimal-overhead sequential agents on simple, single-step queries.\n\n3. No latency or cost analysis beyond step counts: The claim of ~65% time reduction would be more convincing with wall-clock latency or cost breakdowns."}, "questions": {"value": "1. Could you clarify how cross-validation between parallel subtasks is implemented and whether it introduces additional computational overhead? Specifically, \n1.1 Auxiliary Signals: What specific information is captured as an \"auxiliary signal\" during partial execution\n1.2 Heuristic Check: What is the actual heuristic rule used for the \"consistency check\" that determines if the dependency is satisfied enough for aggressive parallelization (e.g., an LLM prompt asking for consistency, a deterministic keyword match, etc.)?\n1.3 Computational Overhead: Please quantify the cost of this feature. Does this cross-validation process introduce significant new LLM inference calls or state-management overhead, and how does this overhead factor into the claimed 65% overall execution time reduction?\n\n2. Could you clarify the Pass@1 score calculation: Specifically, does the automated judge explicitly penalize the answer for extraneous or hallucinated content, or is it only looking for the presence of the single correct fact?\n3. Do you have a quality metric for the LLM-as-Judge to validate that the automatic evaluation method is a reliable proxy for human assessment?\n4. Do you have any latency analysis beyond the reduced steps? \n5. Did you observe any failure patterns or instability when the system aggressively parallelizes subtasks (e.g., dependency violations or premature merges)?\n6. SFT usually will introduce the overfitting, can you elaborate on whether they retain explicit DAG reasoning capability or merely learn implicit parallelization patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns. \n\nThe paper does not involve human subjects, user data, or sensitive information. It focuses solely on algorithmic and architectural improvements for AI agents, and the methodology adheres to standard academic research practices."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "drAVBRQGVe", "forum": "QuaJ6kJaBm", "replyto": "QuaJ6kJaBm", "signatures": ["ICLR.cc/2026/Conference/Submission4375/Reviewer_Ridk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4375/Reviewer_Ridk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792811167, "cdate": 1761792811167, "tmdate": 1762917322934, "mdate": 1762917322934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Flash-Searcher, a DAG-based parallel agent reasoning framework that achieves impressive empirical results (67.7% on BrowseComp, 83% on xbench-DeepSearch, 35% reduction in execution steps) with solid methodological innovation. The core technical contributions, parallel execution architecture and adaptive workflow optimization, are valuable and well-executed. However, the writing quality exhibits AI-generation artifacts. These should be revised in the formal version to meet the requirements of ICLR."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong empirical results across multiple challenging benchmarks. The paper achieves state-of-the-art performance on four benchmarks (BrowseComp: 67.7%, xbench: 83%, GAIA: 82.5%, HLE: 44.0%) while reducing execution steps by 35% and time by ~65%. The cost-performance trade-off is compelling.\n\n2. Solid methodological innovation in parallel reasoning architecture. The DAG-based decomposition with goal-level parallelization and path-level sequential execution is elegant. The cross-validation mechanism (φ predicate in Eq. 4) cleverly balances aggressive parallelization with correctness guarantees.\n\n3. Comprehensive experimental validation.\nExperiments span multiple model backbones (GPT-5, GPT-4.1, DeepSeek-V3.1) and include successful distillation to open-source models (Qwen-2.5 family), demonstrating generalizability.\n\n4. Exceptional reproducibility effort.\nThe 27-page appendix with complete prompts, training data construction pipelines, and hyperparameter settings is commendable and rare in agent research."}, "weaknesses": {"value": "The paper exhibits heavy AI writing artifacts with excessive use of em dashes; recommend substantial revision."}, "questions": {"value": "For an agent framework, computational cost is critical. Please provide concrete costs in dollars in the appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "psoK3pQyhv", "forum": "QuaJ6kJaBm", "replyto": "QuaJ6kJaBm", "signatures": ["ICLR.cc/2026/Conference/Submission4375/Reviewer_uJW2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4375/Reviewer_uJW2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900873961, "cdate": 1761900873961, "tmdate": 1762917322751, "mdate": 1762917322751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a primary bottleneck in current tool-augmented Large Language Model (LLM) agents: inefficiency due to sequential processing. Existing frameworks, including MAS, typically execute tasks (e.g., plan, search, reflect, verify) in a linear chain. This approach leads to long execution times, high latency, and significant computational overhead, especially for complex web-based research tasks.\n\nTo address this, the authors propose FLASH-SEARCHER, a novel agent reasoning framework that fundamentally changes the execution paradigm from a sequential chain to a Directed Acyclic Graph (DAG).\n\nThe authors validate this framework on four challenging web agent benchmarks (BrowseComp, xbench-DeepSearch, GAIA, and HLE). They also demonstrate that the parallel reasoning trajectories generated by the FLASH-SEARCHER framework can be \"distilled\" via supervised fine-tuning (SFT) into standalone open-source models (like Qwen-2.5), significantly improving their performance without the need for the full, complex framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper tackles a clear and critical problem in agent research: latency and efficiency. The shift from a sequential to a parallel, DAG-based execution model is a novel and logical architectural contribution.\n\n* The evaluation is comprehensive, using four different and challenging benchmarks. The authors correctly measure not just final accuracy (effectiveness) but also the primary claim of efficiency (execution steps, tool calls per step).\n\n* A 35% reduction in execution steps is a substantial improvement. The paper provides clear evidence (Fig. 6) that this gain is real and is a direct result of the parallel architecture.\n\n* The success of the distilled models is a strong result. This demonstrates that the reasoning paradigm itself (i.e., how to structure parallel thought) is a learnable skill. It proves the contribution has value beyond just a complex, expensive-to-run framework, making the findings more accessible to the wider community."}, "weaknesses": {"value": "* The main results comparing against other agents (e.g., BrowseMaster) which use different backbones from the one used in the paper. For instance, BrowseMaster uses DeepSeek-R1-0528 backbone while the authors use GPT-5 backbone. It is not clear how much gains are from the proposed method vs the backbone model.\n\n* It is not clear why the authors chose to use just two tools: web_search and crawl_page. Is it because the framework is not flexible to generalize to more tools or some other limitation? The authors acknowledge this limitation (Section B), noting the poor performance of the distilled models on HLE (19.4%) is due to the lack of a code tool.\n\n* The paper mentions (Section 3.2) that subtasks can be scheduled without all prerequisites being met, based on some heuristic consistency. This is a pragmatic choice to maximize parallelism but can potentially be risky. The paper does not elaborate on what these heuristics are or how it handles cascading failures if a parallel step proceeds with incorrect, unverified information. This could potentially increase total steps if backtracking is required, a scenario that is not explored."}, "questions": {"value": "* Have the authors considered cross-validating the evaluation results with other models? Since model-based evaluation is used, it might be prone to reward hacking."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HuAREG9Qqe", "forum": "QuaJ6kJaBm", "replyto": "QuaJ6kJaBm", "signatures": ["ICLR.cc/2026/Conference/Submission4375/Reviewer_i6Xv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4375/Reviewer_i6Xv"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945142443, "cdate": 1761945142443, "tmdate": 1762917322347, "mdate": 1762917322347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}