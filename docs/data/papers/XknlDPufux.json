{"id": "XknlDPufux", "number": 6032, "cdate": 1757951111104, "mdate": 1763120361381, "content": {"title": "Zoom-In to Sort AI-Generated Images Out", "abstract": "na", "tldr": "We fine-tune VLMs to identify key regions on potentially AI-generated images that, upon closer observation, can yield a more grounded, explainable and accurate classification result..", "keywords": ["Image & Video Synthesis", "Multi-modal Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/70c089e327b588698947c09be8bbebad318ff928.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "For the task of detecting AI-generated images, this paper introduces ZoomIn, a two-stage forensic framework designed to emulate the process of a human expert. To support this approach, the research also involved the construction of a new dataset, MagniFake, which was generated via an automated pipeline leveraging Vision-Language Models (VLMs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Two-Stage Framework: Proposes ZoomIn, a novel two-stage forensic framework that emulates human visual inspection (scan-then-focus), enhancing both accuracy and interpretability. Delivers robust, human-intelligible explanations by grounding its final verdict in explicit visual evidence found within focused, magnified regions.\n2. Innovative Dataset: Introduces MagniFake, a large-scale dataset of 20,000 images annotated with bounding boxes and fine-grained forensic explanations, generated via an automated VLM pipeline.\n3. High Accuracy & Generalization: Achieves high detection accuracy (97.2% on its test set) and demonstrates strong generalization to external, out-of-distribution (OoD) datasets."}, "weaknesses": {"value": "1. Critical Dependency on Query 1: The framework's success is critically dependent on the initial \"Global Scan\" (Query 1) stage, which acts as a performance bottleneck. If this stage fails to identify decisive artifacts, the entire \"zoom-in\" mechanism is rendered ineffective.\n2. Insufficient Failure Mode Analysis: The ablation study on \"Random Cropping\" demonstrates the need for intelligent proposals but fails to analyze the robustness or failure modes (e.g., sub-optimal or incomplete proposals) of the proposal mechanism itself.\n3. Risk of Annotation Bias: The fully automated dataset creation process (using GPT-4o and Qwen-2.5-VL) risks a \"self-fulfilling prophecy,\" where the trained model learns the biases of the \"teacher\" VLMs rather than general, low-level forgery artifacts."}, "questions": {"value": "1. Could the authors provide an analysis of the conditions under which the Query 1 region proposals are likely to fail (e.g., with subtle or sparse artifacts)?\n2. If Query 1 proposes no bounding boxes, does the model simply default to its initial (and potentially incorrect) global verdict, bypassing refinement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YMUkiKcFm5", "forum": "XknlDPufux", "replyto": "XknlDPufux", "signatures": ["ICLR.cc/2026/Conference/Submission6032/Reviewer_HX3D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6032/Reviewer_HX3D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880496122, "cdate": 1761880496122, "tmdate": 1762918421202, "mdate": 1762918421202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "WVVoWfpHmO", "forum": "XknlDPufux", "replyto": "XknlDPufux", "signatures": ["ICLR.cc/2026/Conference/Submission6032/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6032/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763120358763, "cdate": 1763120358763, "tmdate": 1763120358763, "mdate": 1763120358763, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ZoomIn, a framework for AI-generated image (AIGC) detection, and introduces MagniFake, a new dataset for training and evaluation. ZoomIn uses a two-stage inference pipeline: it first localizes suspicious artifact regions via bounding boxes, then zooms in to deliver a final verdict. To support training, the authors curate MagniFake (10k images) with region-level annotations produced by GPT-4o and Qwen-VL. By a two-stage training pipeline, the final models exhibit superior performance agains both traditional and llm-based methods, showing the effectiveness of the approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow, with clear motivations.\n2. The experiment shows promising results and are extensive. It could potentially be a new baseline for this field.\n3. The problem is framed clearly and the pipeline is intuitive."}, "weaknesses": {"value": "1. Evaluation metrics: BLEU and ROUGE are both rule-based metrics. Are these appropriate metrics in the settings of open-ended reasoning? I believe incorporating llm as judges in this scenario is more suitable. Can you provide more insights on this?\n2. Lack of evaluation baselines. Since the data is generated by Qwen-VL and GPT-4o, to show the improvement of ZoomIn over its distilled teacher, the paper should also consider including GPT-4o and Qwen (equiped with the same inference pipeline) for comparison.\n3. The paper should also include LOKI[1] in the related work since this is also in the field of AIGC detection.\n\nReference:\n[1] LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models"}, "questions": {"value": "See above in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hEKsEcIpOj", "forum": "XknlDPufux", "replyto": "XknlDPufux", "signatures": ["ICLR.cc/2026/Conference/Submission6032/Reviewer_BCG8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6032/Reviewer_BCG8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911176145, "cdate": 1761911176145, "tmdate": 1762918420893, "mdate": 1762918420893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ZoomIn, a two-stage VLM-based forensic pipeline for AI-generated image detection. Stage 1 predicts a provisional label (real vs AI-generated), highlights “suspicious” regions via bounding boxes, and explains why. Stage 2 crops those regions and asks the model again for a refined final verdict plus a more grounded explanation. To train this, the authors introduce MagniFake dataset produced by an automated pipeline: GPT-4o writes forensic justifications, and Qwen-2.5-VL turns those justifications into spatial boxes. The model is then fine-tuned with SFT and a GRPO-style RL stage using customized rewards."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is tackling a genuinely important, high-impact problem: forensics on high-quality modern synthetic imagery, and especially the need for interpretable decisions instead of opaque “fake/real” classifiers.\n\n2. The two-stage pipeline from global hypothesis to zoom-in verification is clean and easy to understand.\n\n3. The method links textual rationales to bounding boxes and then uses those localized crops to revise the verdict. That at least tries to connect why I think it’s fake to where in the pixels I saw it."}, "weaknesses": {"value": "1. The paper repeatedly positions itself as a “paradigm shift” that turns VLMs from “passive analyzers into active visual investigators” and “encourages the model to think with images.” This is way too much of an oversell, since there have been a bunch of works adopting a similar pipeline, data construction, and training. The core mechanism, i.e., generate an initial answer, propose regions of interest, crop/zoom into them, and re-evaluate, is already very close to recent concepts in VLMs like grounded chain-of-thought or think-with-images. What ZoomIn adds is mainly: apply that template specifically to AI-generated image detection, and train it with a synthetic forensic dataset plus a custom RL reward. That is interesting, but it is nowhere near a conceptual paradigm shift for multimodal reasoning. There is no emerging recipe for a particular task.\n\n2. A more serious blocker is that MagniFake is created entirely by GPT-4o and Qwen-2.5-VL. The paper calls these “annotations” and then treats them as supervisory ground truth. But GPT-4o is not a forensic expert; it is known to hallucinate plausible-sounding forensic cues. The paper itself even admits that large VLMs tend to “guess” when details are hard to perceive and can produce “false reasoning with incorrect decisions.” Besides, when generating the bounding box, he only cleanup step is a heuristic filter that removes bounding boxes that are too big or degenerate object detections. There is no mention of any human verification pass, inter-annotator agreement, or audit of factual correctness in those explanations and boxes. Trained on the curated dataset, the model is rewarded for matching its own synthetic teacher signals, then evaluated on how well it matches those same synthetic teacher signals. This is self-referential and does not demonstrate true interpretability.\n\n3. In the main text, you say MagniFake has “10,000 real images and 10,000 AI-generated images,” sourced equally from ImageNet / COCO for real, and equally from GPT-Image-1 and Gemini 2.5 Flash Image for fake. But in Appendix A you describe MagniFake as “10,000 images (5,000 real and 5,000 AI-generated),” with AI-generated images created using only GPT-Image-1 (no Gemini), and you repeat that it is built from GPT-4o + Qwen-2.5-VL with automatic annotations.  What on earth is the setting? What on earth is the dataset used for training and testing, and what about the reported results?\n\n4. Generator diversity is very narrow. Even if we take the main test as the claim, all synthetic images in MagniFake come from GPT-Image-1 and Gemini 2.5 Flash Image. That’s not broad enough to claim robust generalization across novel, unseen models. The employed datasets for OOD testing are still largely diffusion-style imagery.\n\n5. Only accuracy is reported, no calibration or confusion breakdown. The model cannot output detection probability as confident scores. Besides, the employed performance measure is not convincing enough. Accuracy alone is insufficient in high-stakes forensic settings, where false positives and false negatives have very different consequences.\n\n6. In AI-generated images, the telltale area normally has vague boundaries. How can you guarantee the bbxs actually reflect those areas?\n\n7. “Reasoning quality” is measured by BLEU-1/2 and ROUGE-L comparing your generated explanations to the MagniFake reference explanations. This only shows stylistic and lexical similarity to a synthetic teacher, not factual grounding.\n\n8. Several training/inference details critical for reproduction are missing. The authors fine-tuned the 7B and 32B Qwen with SFT over all parameters of the vision encoder, projection, and language model, then applied GRPO. But for a 32B model, full fine-tuning + RL on only “8× A100 GPUs” at the stated learning rates is, frankly speaking, unrealistic. What's your batch size? What about other parameter settings?\n\n9. How do you decide to trigger Query 2 at inference time? Always? Only if confidence < threshold? This gating rule matters for runtime and deployment cost, but is not described concretely.\n\n10. Sometimes you call the model “ZoomIn,” sometimes “ZoomIn-7B / 32B,” sometimes “ZoomIn detector,” sometimes “zoom-in mechanism.” It's better be consistent.\n\n11. The evaluation measures, such as I-Acc., C-Acc., C-Cases, need explicit definitions in the main text.\n\n12. You repeatedly say the model “mimics a human forensic expert.” If that is really the case, you have to demonstrate this with human expert studies or soften the narration."}, "questions": {"value": "Please see the details above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7nNtNknZJW", "forum": "XknlDPufux", "replyto": "XknlDPufux", "signatures": ["ICLR.cc/2026/Conference/Submission6032/Reviewer_TJVw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6032/Reviewer_TJVw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989156389, "cdate": 1761989156389, "tmdate": 1762918420560, "mdate": 1762918420560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ZoomIn, a two-stage forensic framework to improve the accuracy and interpretability of detecting AI-generated images. Mimicking human inspection , the model first performs a global scan to identify suspicious regions, then \"zooms in\" on these cropped areas for a detailed local analysis to make a final, grounded verdict. To train this, the authors developed MagniFake, a new dataset of 20,000 real and synthetic images annotated with bounding boxes and forensic explanations. The method achieves 97.2% accuracy and provides clear, human-understandable explanations grounded in visual evidence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces \"ZoomIn,\" a two-stage framework that detects AI-generated images by first scanning for suspicious regions and then performing a focused analysis on those \"zoomed-in\" areas.\n\nTo train this model, the authors created \"MagniFake,\" a new dataset of 20,000 real and synthetic images annotated with bounding boxes and forensic explanations.\n\nThis method achieves high accuracy (97.2%) and provides human-understandable explanations that are grounded in specific visual evidence"}, "weaknesses": {"value": "1、The use of BLEU scores as a reward signal in the Reinforcement Learning (RL) phase is flawed, as it optimizes for superficial n-gram overlap rather than the logical correctness or causal soundness of the reasoning, risking that the model learns to mimic keywords rather than generate genuinely sound analysis.\n\n2、The MagniFake dataset's AI-generated images are sourced from only two models, raising concerns that the detector may be overfitting to the specific artifact \"signatures\" of those generators.\n\n3、 The analysis could be further strengthened by discussing and comparing against other relevant explainable MLLM-based detectors, for instance, FakeScope[1], So-Fake[2], and FakeVLM[3].  To provide a more comprehensive picture of the method's capabilities, it would be valuable to extend the evaluation to include additional testable benchmarks, such as LOKI[4] and Fakebench[5].\n\n[1] Fakescope: Large multimodal expert model for transparent ai-generated image forensics\n\n[2] So-Fake: Benchmarking and Explaining Social Media Image Forgery Detection.\n\n[3] Loki: A comprehensive synthetic data detection benchmark using large multimodal models\n\n[4] Spot the fake: Large multimodal model-based synthetic image detection with artifact explanation.\n\n[5] FakeBench: Probing Explainable Fake Image Detection via Large Multimodal Models"}, "questions": {"value": "1.Given that the BLEU reward only captures word similarity and not logical quality, did the authors consider using a more advanced reward model—to provide a more accurate training signal for the plausibility and logical soundness of the explanations?\n\n2.How confident are the authors that the Query 1 proposal network can generalize to entirely novel classes of artifacts from generative architectures unseen during training?\n\n3.How do the authors validate and mitigate the potential inherited biases introduced by the automated VLM-based dataset annotation pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gz7f2SQxGa", "forum": "XknlDPufux", "replyto": "XknlDPufux", "signatures": ["ICLR.cc/2026/Conference/Submission6032/Reviewer_AenC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6032/Reviewer_AenC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992310607, "cdate": 1761992310607, "tmdate": 1762918420221, "mdate": 1762918420221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}