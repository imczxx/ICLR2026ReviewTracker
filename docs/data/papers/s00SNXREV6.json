{"id": "s00SNXREV6", "number": 13600, "cdate": 1758219677444, "mdate": 1759897425644, "content": {"title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play", "abstract": "Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision–language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. \nTo address this challenge, we propose **Vision-Zero**, *a domain-agnostic self-play framework that generates visual deduction games from diverse images for scalable VLM training without human annotations.*\nSpecifically, Vision-Zero encompasses three main attributes:\n(1) **Strategic Self-Play Framework:**\nVision-Zero trains VLMs in \"Who Is the Spy\"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation.\n(2) **Gameplay from Arbitrary Images:** Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model’s reasoning ability across diverse domains and showing strong generalization to different tasks.\nWe demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images.\n(3) **Sustainable Performance Gain:** We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements.\nDespite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods.\nModels and code will be released upon acceptance.", "tldr": "", "keywords": ["Language Gamification", "Post-Training", "Vision–Language Models", "Self-Play Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54ea5d189cb064b037193f2378561d41dcb2c0e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Vision-Zero, a zero-human-in-the-loop post-training framework for VLMs built upon a visual “Who is the Spy?” self-play game. This paper further proposes iterative self-play optimization for this self-play game task, which consists of self-play optimization in clue stage and RLVR for the decision stage. Experiment demonstrate consistent gains over baseline methods for Qwen2.5-VL-7B base model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper demonstrates that self-play visual game can be an effective task for VLM post-training, which is an interesting finding.\n\n2.\tThe paper evaluates on a reasonably wide set of benchmarks to demonstrate the effectiveness of Vision-Zero."}, "weaknesses": {"value": "1.\tPrior work has already explored automatically crafted multi-image contrastive data for VLM post-training (e.g., MiCo). The paper should avoid claiming to be the first zero-human-in-the-loop paradigm and more carefully situate its contribution relative to [1].\n\n2.\tThe “Who is the Spy?” pipeline requires multiple forward passes per sample and a more complex training algorithm. To justify this complexity, the paper should include direct, apples-to-apples comparisons against simpler multi-image contrast baselines such as MiCo [1], using the same data sources (and ideally matched compute) to test whether the game mechanics yield additional gains beyond contrastive RLVR.\n\n3.\tPlease report training efficiency for Vision-Zero. The multi-turn interactions per sample could introduce substantial overhead; quantifying this is important for practical adoption.\n\n4.\tThe main text states 100 iterations while the appendix reports 150. Please reconcile this discrepancy and clarify which results correspond to which training steps.\n\n[1] MiCo: Multi-image Contrast for Reinforcement Visual Reasoning. NeurIPS 2025"}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bhdhLpwdck", "forum": "s00SNXREV6", "replyto": "s00SNXREV6", "signatures": ["ICLR.cc/2026/Conference/Submission13600/Reviewer_B6Um"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13600/Reviewer_B6Um"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13600/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760767228926, "cdate": 1760767228926, "tmdate": 1762924185825, "mdate": 1762924185825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Vision-Zero, a framework inspired by the “Spy vs. Player” game concept. The idea is creative and brings an interesting self-play strategy from LLMs into the vision-language domain (VLMs). The design is simple yet somewhat innovative. However, the overall performance improvement is modest and appears constrained by the capability of the underlying image editing models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper identifies two valuable and timely challenges in multimodal learning: data scarcity and knowledge ceiling.\n2.\tIt presents a clear motivation by extending the concept of self-play from large language models (LLMs) to vision-language models (VLMs).\n3.\tThe framework generates data using automated image editing tools or procedural rendering, which supports scalable dataset creation.\n4.\tThe Spy–Player game formulation is an interesting and original idea that provides a novel self-supervised interaction mechanism."}, "weaknesses": {"value": "1.\tQuestionable benefit over direct image generation\nAlthough the Spy–Player game design is intriguing, it is unclear what advantages it offers compared to directly training on images generated via image editing or procedural rendering. Since these methods can already produce labeled data, the benefit of framing the task as a game-based QA construction process remains unclear.\n2.\tStrong dependence on image editing quality\n\tThe approach heavily relies on the performance of image editing models (e.g., Gemini 2.5 or ImgEdit). If these models generate suboptimal or unrealistic edits, the quality of the resulting training data—and thus the model’s performance—may significantly degrade.\n3.\tInconsistent performance across benchmarks:\n\tThe proposed method is specifically trained on Chart/OCR-related tasks, yet its improvement in Table 2 (Chart/OCR) is smaller than in Table 1 (VLMEvalKit). The authors should explain why the task-specific fine-tuning yields less improvement than the general evaluation.\n4.\tUnclear fine-tuning strategy:\n\tThe paper does not specify whether the fine-tuning updates all model parameters or only certain components (e.g., adapter layers or LoRA modules). Clarifying this is essential for understanding the efficiency and scalability of the proposed approach.\n5.\tLimited generalization of image-editing-based augmentation:\n\tThe success of the method may mainly stem from the data augmentation effects of image editing. However, such augmentation relies heavily on model priors and may not generalize well to real-world or out-of-domain scenarios. The authors should discuss potential limitations in generalization and domain transfer."}, "questions": {"value": "1.\tHow fair is the comparison with prior works that do not assume access to camera poses? How does MultiViewPano perform when camera poses are inaccurate or unavailable?\n2.\tHow robust is the proposed method to poor-quality SEVA outputs? Can the authors provide visualizations of such cases?\n3.\tCould the authors expand the ablation study and visualize intermediate outputs to clarify which modules contribute to which improvements?\n4.\tHow are geometric distortions and artifacts handled during repeated enhancement steps?\n5.\tWhy does the proposed method not outperform CubeDiff, especially for indoor scenes?\n6.\tCould the authors ensure that all best-performing metrics in Table 1 are boldfaced for readability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ItIYyQY0yC", "forum": "s00SNXREV6", "replyto": "s00SNXREV6", "signatures": ["ICLR.cc/2026/Conference/Submission13600/Reviewer_K9Zp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13600/Reviewer_K9Zp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13600/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577499157, "cdate": 1761577499157, "tmdate": 1762936770066, "mdate": 1762936770066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a zero-human-in-the-loop training framework for VLMs using a gamified self-play mechanism inspired by social deduction games like \"Who Is the Spy?\", eliminating the need for expensive human-annotated datasets.\nThe game involves multiple agents (civilians and a spy) who observe slightly different images and must describe or deduce the differences through dialogue. The model improves via Iterative Self-Play Policy Optimization (Iterative-SPO), which alternates between self-play and reinforcement learning with verifiable rewards (RLVR) to avoid performance plateaus."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. First to apply self-play to VLMs in a gamified adversarial setting.\n2. Propose a novel training algorithm that avoids equilibrium stagnation by alternating between self-play and RLVR.\n3. The evaluated domains are quite comprehensive, including math, charts, vision-centric, and simulated images."}, "weaknesses": {"value": "- The improvement of the Chart/OCR is marginal, as shown in Tab.2.\n- The data preparation cost in the Tab.3 is ambiguous, where the statistics might be further explained.\n- The improvement over the InternVL series is marginal compared to the QwenVL series, as shown in Tab. 4, casting doubts on its generability.\n- The intuition/theory between the \"who is a spy\" proxy task and the general question-answering tasks remains unclear."}, "questions": {"value": "- Will the models cheat or find shortcuts via editing methods? Because the editing possibilities/categories are pre-defined. (e.g., swapping numerical attributes in chart data.)\n- Is this idea novel in the multi-agent community? I have noticed that Role-Advantage Estimation is based on the previous research. What's your differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zXyglaliL7", "forum": "s00SNXREV6", "replyto": "s00SNXREV6", "signatures": ["ICLR.cc/2026/Conference/Submission13600/Reviewer_CCNi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13600/Reviewer_CCNi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13600/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888592911, "cdate": 1761888592911, "tmdate": 1762924185176, "mdate": 1762924185176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents VISION-ZERO, a framework that enhances the reasoning capabilities of vision–language models (VLMs) through self-play and RLVR, thereby reducing reliance on costly human-annotated data. Specifically, the framework constructs a strategic environment inspired by the game “Who Is the Spy”, in which training data are naturally generated through self-play. The resulting data are then used to iteratively optimize VLMs using the Iterative-SPO algorithm. Extensive empirical results on multiple benchmarks demonstrate that VISION-ZERO surpasses baseline models, significantly enhancing the reasoning ability of VLMs while avoiding cross-capability negative transfer and substantially reducing dataset construction costs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-motivated. Existing RLVR methods rely heavily on carefully constructed datasets. By introducing self-play, this method enables reasoning improvement on label-free data, helping to extend RLVR to broader domains where manual annotation is difficult or impractical.\n2. The experiments are comprehensive. Evaluations on **Task Generalization Capability**, **Cross-Capability Negative Transfer Mitigation**, and **Low Dataset Construction Costs** strongly demonstrate the superiority of **VISION-ZERO**, providing solid evidence for its effectiveness in mitigating data scarcity and improving scalability.\n3. The framework proposed in this paper is novel. The *“Who Is the Spy”* game environment appears to be highly scalable and can be readily extended to diverse domains. This offers a new paradigm for enhancing the capabilities of vision–language models (VLMs)."}, "weaknesses": {"value": "1. The data curation pipeline is primarily focused on the **CLEVR** and **Chart** domains. As mentioned in *Appendix 2.2*, directly editing chart images using **NanoBanna** and **ChatGPT** can be extremely challenging. This raises concerns about the scalability of the proposed framework, particularly regarding how to generate image pairs with arbitrary data.\n2. The overall design of this framework appears to be sophisticated. The success of the framework relies on the effective coordination among **self-play in the game environment**, **Self-Play Optimization in the Clue Stage**, and **RLVR in the Decision Stage**. A deeper discussion on the stability of this framework would be valuable for understanding its robustness and generalization."}, "questions": {"value": "1. Ablation studies on critical components and hyperparameters (e.g., RAE and the number of civilians) would be helpful for further understanding the contribution of each module and the sensitivity of the framework.\n2. If **VISION-ZERO** is trained on static datasets, is there a possibility of **knowledge leakage** when the model encounters the same image pairs during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no concerns"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T30YO8Rths", "forum": "s00SNXREV6", "replyto": "s00SNXREV6", "signatures": ["ICLR.cc/2026/Conference/Submission13600/Reviewer_gbyd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13600/Reviewer_gbyd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13600/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990389166, "cdate": 1761990389166, "tmdate": 1762924184837, "mdate": 1762924184837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}