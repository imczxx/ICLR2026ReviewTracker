{"id": "dKyhgfe50H", "number": 24323, "cdate": 1758355450324, "mdate": 1759896771424, "content": {"title": "Discrete Diffusion for Bundle Construction", "abstract": "As a central task in product bundling, bundle construction aims to select a subset of items from huge item catalogs to complete a partial bundle. Existing methods often rely on the sequential construction paradigm that predicts items one at a time, nevertheless, this paradigm is fundamentally unsuitable for the essentially unordered bundles. In contrast, the non-sequential construction paradigm models bundle as a set, while it still faces two dimensionality curses: the combination complexity is exponential to the catalog size and bundle length. \nAccordingly, we identify two technical challenges: 1) how to effectively and efficiently model the higher-order intra-bundle relations with the growth of bundle length; and 2) how to learn item embeddings that are sufficiently discriminative while maintaining a relatively smaller search space other than the huge item set. \n\nTo address these challenges, we propose DDBC, a Discrete Diffusion model for Bundle Construction. DDBC leverages a masked denoising diffusion process to build bundles non-sequentially, capturing joint dependencies among items without relying on certain pre-defined order. To mitigate the curse of large catalog size, we integrate residual vector quantization (RVQ), which compresses item embeddings into discrete codes drawn from a globally shared codebook, enabling more efficient search while retaining semantic granularity. We evaluate our method on real-world bundle construction datasets of music playlist continuation and fashion outfit completion, and the experimental results show that DDBC can achieve more than 100\\% relative performance improvements compared with state-of-the-art baseline methods. Ablation and model analyses further confirm the effectiveness of both the diffusion backbone and RVQ tokenizer, where the performance gain is more significant for larger catalog size and longer bundle length.\nOur code is available at https://anonymous.4open.science/r/DDBC-44EE.", "tldr": "", "keywords": ["Bundle Construction", "Bundle Completion", "Recommendation System", "Discrete Diffusion Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/632e6ae048b1be1fe787aff3192f8469d5810ebe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Bundle construction suffers from a massive item pool and exponentially growing combinations. \nDDBC addresses these challenges by utilizing a diffusion model to generate an entire bundle. \nDDBC also quantizes item embeddings into discrete codes to reduce the search space.\nExtensive experiments show that DDBC outperforms current bundle generation methods and achieves the state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Tokenizing items through codebook quantization is effective for processing a massive item pool.\n* DDBC outperforms existing baselines by a huge margin."}, "weaknesses": {"value": "* The size of bundle is fixed and not adjustable, unlike other baselines.\n* Using the same item features for all baselines is doubtful. Each model may require its own embedding space depending on its architecture."}, "questions": {"value": "* How does a STE work for a codebook quantization?\n* How does the RVQ strategy illustrate an item in coarse-to-fine manner?\nSharing the same codebook for all positions without scaling seems to treat all tokens equally rather than hierarchically.\n* Even if each item is quantized into a unique code sequence, it seems permutations of the sequence would represent semantically same features since dequantizing the residual quantization is aggregating all codes of the sequence.\nHow does this affect the diffusion process?\nIs there a way to utilize this during the diffusion process?\n* How does the dedup code working? Does it included in the codebook? Should the diffusion model recover the dedup code by denoising?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "There is a name (subhamsahoo) that seems like an author in line 209 of ‘util.py’ in the code repository.\n```\nclass BinarySampler(GumbelSampler):\n\n  def sample(self, probs):\n    # TODO(subhamsahoo): use the temperature parameter.\n    pos_noise = self._sampling_noise().to(\n      dtype=probs.dtype, device=probs.device)\n    neg_noise = self._sampling_noise().to(\n      dtype=probs.dtype, device=probs.device)\n    del_noise_exp = (neg_noise - pos_noise).exp()\n    hard_sample = (probs * (1 + del_noise_exp)\n                   > 1).to(probs.dtype)\n    soft_sample = probs / (probs + (1 - probs) * del_noise_exp)\n    return soft_sample + (hard_sample - soft_sample).detach()\n```"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "51SK1d3rtd", "forum": "dKyhgfe50H", "replyto": "dKyhgfe50H", "signatures": ["ICLR.cc/2026/Conference/Submission24323/Reviewer_CnC2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24323/Reviewer_CnC2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732752907, "cdate": 1761732752907, "tmdate": 1762943041529, "mdate": 1762943041529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose DDBC (Discrete Diffusion for Bundle Construction), a novel framework that formulates bundle construction as a masked discrete diffusion process. Rather than following traditional step-by-step methods, DDBC generates bundles in an order-independent way by progressively filling in masked item tokens. The framework combines Residual Vector Quantization (RVQ), which represents items with discrete semantic codes to address the challenge of large item catalogs, with a Discrete Diffusion Model (DDM) designed to capture complex relationships within bundles without requiring a fixed item order. Comprehensive experiments on Spotify playlist and fashion outfit datasets show that DDBC achieves substantial performance improvements over both sequential and set-based baselines. Ablation studies further highlight the complementary strengths of RVQ and the discrete diffusion backbone."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper presents a well-motivated argument against the sequential construction paradigm, emphasizing that bundles are inherently unordered sets rather than sequences.\n2. The integration of discrete diffusion and residual vector quantization is technically coherent and aligns with recent advances in generative recommendation.\n3. The paper provides extensive experiments with clear ablations and sensitivity analyses, showing that DDBC scales better with longer bundles and larger catalogs."}, "weaknesses": {"value": "1. While the paper demonstrates robustness across different input-predict ratios, the framework fundamentally requires at least some partial bundle items at inference. The paper does not explore conditional generation from fully-masked states using only user embeddings or contextual features, which limits applicability for cold-start scenarios where no seed items are available.\n2. The model enforces a fixed number of items per bundle, whereas real-world bundles are inherently variable in size. Capping bundle lengths during training and testing introduces a methodological limitation that does not fully capture the characteristics of the bundle item collection as in original dataset.\n3. While the integration of RVQ with discrete diffusion is technically sound and well-executed, the core novelty lies primarily in combining existing techniques (RVQ from generative retrieval and masked diffusion from language models) rather than introducing fundamentally new algorithms. The ablation study demonstrates that both components are essential and complementary, but the contribution is more engineering-focused than conceptually novel. The paper would benefit from deeper theoretical analysis of why this combination works particularly well for bundle construction.\n\nMinor typos:\n-\tL122: Citation formatting error\n-\tL196: Use $z_{j,l}$ instead of $z_{jl}$\n-\tL244: The reverse process should be conditioned on current noisy tokens $Z^{(t)}$\n-\tL267: $\\hat{E}_j = \\sum_{l=1}^{L-1} e^{(l)}_{z^{(l)}(i)}$\n-\tL235, L287: The notation $\\alpha(t) != \\alpha_t$; $\\alpha(t)$ should be $1-t/T$ to yield deterministic at $t=T$"}, "questions": {"value": "1. At $t=T$, all tokens are fully masked, yet the model is trained to predict original tokens under Equation 7. While this teaches the model unconditional bundle priors, could the authors clarify: \n    - a) What proportion of training loss comes from high-noise timesteps ($t \\geq 0.8T$)\n    - b) Whether this supervision is necessary given that inference always starts from partially-observed bundles\n    - c) Whether alternative training strategies (e.g., restricting $t < T$) were explored?\n2. During inference, how are the “known” items in $b_x$ selected? Since bundle semantics could vary depending on which subset is revealed, does the model show robustness to different partial-item configurations?\n3. When bundles exceed the capped size $k$, how are items selected for truncation? Does this selection (e.g., first-k, random-k, most-popular-k) introduce bias toward specific bundle patterns? Have the authors analyzed whether truncation affects semantic coherence or diversity of the resulting training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t3Tjhqe3Lh", "forum": "dKyhgfe50H", "replyto": "dKyhgfe50H", "signatures": ["ICLR.cc/2026/Conference/Submission24323/Reviewer_ZPfg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24323/Reviewer_ZPfg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969505326, "cdate": 1761969505326, "tmdate": 1762943041339, "mdate": 1762943041339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DDBC, which is a discrete diffusion for bundle construction. The core motivation is that pervious bundle construction methods are based on sequential generation, where bundle length grows results in intra-bundle relational explosion and large catalog size makes item search space massive. DDBC treats bundle construction as masked discrete denoising over a compact discrete code space. DDBC quantizes item embeddings via multi-level RVQ, running masked discrete diffusion over the codes, and decoding the codes back to item IDs. The experiments show significant gains (about 100%) on some of the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The bundles are regarded as sets not sequences, which is . And the combination of the masked discrete diffusion and RVQ for bundle generation is novel in this domain.\n\nThe improvements are significant compared to the baselines. DDBC achieves >100% relative gains in Jaccard and F1 on Spotify with k = 60/90. DDBC scales well to have bigger relative improvements on the datasets with larger k.\n\nDDBC is tiny and cost less inference time compared to many baselines like BundleMLLM."}, "weaknesses": {"value": "The size of bundle seems to be fixed-length. If so, the application of DDBC could be limited. DDBC does not model set invariance. Although the bundles are not designed as serialized sequences and order is randomized, permutation invariance is not considered.\n\nThe evaluation includes only playlist and POG. I wonder if DDBC still performs well when bundles are from completely different domain, e.g., shopping sets from different categories.\n\nThe way of handling personalization is not explored. The generation is unconditional w.r.t user preferences, e.g., the RVQ codebooks of DDBC are global, potentially flattening user preference patterns. The current framework may hardly be deployed to scenarios where personalization is required."}, "questions": {"value": "Could the diffusion be modified to support variable-length bundle reconstruction?\n\nCould DDBC extended to personalization scenarios? e.g., change architecture to conditional diffusion and inject the user preference using the conditions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zARI2evOuz", "forum": "dKyhgfe50H", "replyto": "dKyhgfe50H", "signatures": ["ICLR.cc/2026/Conference/Submission24323/Reviewer_B6yG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24323/Reviewer_B6yG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762369406971, "cdate": 1762369406971, "tmdate": 1762943041116, "mdate": 1762943041116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a discrete diffusion based bundle construction method. To address the complexity challenge of bundle construction caused by the bundle size and large item corpus, it (1) uses RVQ to get the code representation of each item by training a shared codebook; and (2) trains a discrete diffusion model to gradually mask items in a given bundle in the forward process and then gradually reconstruct the masked items in the reverse process with the item token learned through RVQ module. Experiments on two real-world dataset partially prove the effectiveness of the proposed method (as from the result, the proposed method only works well on one of the adopted dataset with larger bundle size). Overall, this is a quite solid study on bundle construction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper is generally well written and organized, which makes it easy to follow. \n\nS2. The investigated research question is quite meaningful and practical in the real-world application. More importantly, the authors identified the limitations of the existing sequential-based bundle construction, as the order of the items within a bundle may not matter a lot when constructing the bundle (it could be a set).\n\nS3. The effectiveness of the model has been verified on the Spotify dataset with the large bundle size, while it does not work on the POG dataset with smaller bundle size. \n\nS4. Extensive ablation studies and important hyper-parameter analysis further show the efficacy of the important modules of the model\n\nS5. The source code is provided to ensure a better reproducibility."}, "weaknesses": {"value": "W1. The title does not really reflect the task that has been investigated in the paper. The paper actually investigates the bundle completion task instead of exact bundle construction task. \n\nW2. It would better if the authors can highlight their technical contribution. \n\nW3. The investigate the research question, though being practical and meaningful in real-world application, seems to lack generalizability. It relies on partial items in a bundle, so cannot create a bundle from scratch. However, in the real-world applications, there are many scenarios, where creating bundle from scratch is required. \n\nW4. Some related works are missing, for instance, Adaptive In-Context Learning with Large Language Models for Bundle Generation (SIGIR 2024). The authors are highly suggested to cover a comprehensive literature, especially LLM based method for bundle construction/completion. \n\nW5. The fixed-length setting may limit the flexibility of the proposed model. In the real-scenario, we may not be able to show too many items for the users in one page, so how to determine the subset of the items that should be displayed to the user?"}, "questions": {"value": "Q1. How to distinguish the tokens of unpopular items using RVQ? Existing study shows that RVQ in the Euclidean space may not be able to well distinguish the unpopular items, which has a large amount in the real-world scenarios. \n\nQ2. Does the step T affect the performance? If so, how?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F8vR61j4Mz", "forum": "dKyhgfe50H", "replyto": "dKyhgfe50H", "signatures": ["ICLR.cc/2026/Conference/Submission24323/Reviewer_UnGS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24323/Reviewer_UnGS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762648990936, "cdate": 1762648990936, "tmdate": 1762943040868, "mdate": 1762943040868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}