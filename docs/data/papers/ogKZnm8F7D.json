{"id": "ogKZnm8F7D", "number": 10881, "cdate": 1758184019948, "mdate": 1759897622793, "content": {"title": "Future Policy Aware Preference Learning for Mathematical Reasoning", "abstract": "Preference learning methods such as Direct Preference Optimization (DPO) have become standard for Large Language Model (LLM) post-training, yet they are often ineffective for mathematical reasoning. A key challenge is the large token overlap between preferred and dispreferred trajectories; lowering the probability of dispreferred trajectories also reduces the probability of shared useful tokens, leading to over-penalization and overall performance collapse. As a mitigation, existing algorithms include the probability of a trajectory under the current policy as a regularization term, which decreases the effect of the gradient when the probability is low. \nHowever, by the time this effect takes hold, useful tokens may have already been over-penalized as the model has begun to degrade.\nTo address this, we propose Future Policy Aware (FPA) preference learning, which replaces the current policy with a future policy in the regularization term. This future policy is estimated via lightweight, logit-space extrapolation from a reference model toward the current model. FPA enables safer training by preemptively regularizing potentially problematic gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH and GSM8K benchmarks. FPA yields consistent performance gains, with the largest improvements observed with SimPER, achieving gains of up to 5.75%. \nWe demonstrate that FPA provides proactive regularization while preserving the probability of shared, useful mathematical tokens, and enables longer, degradation-free training with negligible computational overhead. We will release our code publicly upon publication.", "tldr": "", "keywords": ["Preference Learning", "Large Language Models", "Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9575e870605e672f43dd394812174ca7e6a2cf25.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies why standard preference-learning methods degrade on mathematical reasoning due to gradient entanglement created by token overlap between preferred and rejected trajectories. It argues existing regularizers are reactive and kick in too late, over-penalizing shared useful tokens. The method introduces Future Policy Aware (FPA) preference learning: replace the current-policy probabilities used in the regularization term with a predicted future policy obtained by lightweight logit-space extrapolation from a reference toward the current model, applied via stop-gradient. Experiments apply FPA to DPO, RPO, and SimPER across GSM8K and MATH with several backbones, showing consistent gains, better stability, and negligible overhead, plus analyses of coefficients, angles, and forgetting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a simple, general method: a drop-in, reference-aware extrapolation that proactively regularizes gradients without extra forward passes; compatible with multiple preference objectives.\n2. The paper also provides a thorough empirical analysis: gradient angle/norm diagnostics, coefficient behavior, targeted ablations (positive-only/negative-only), $\\lambda$ sensitivity, and learning-rate controls that isolate the source of benefit beyond simple step-size changes."}, "weaknesses": {"value": "1. The main concern is that the paper emphasizes preference learning from the title through the body, yet does not evaluate on standard preference-learning tasks, i.e., RLHF-related benchmarks such as AlpacaEval 2 [1] and MT-Bench [2]. The work focuses on math reasoning datasets, but on-policy RL methods like RLVR currently perform better than off-policy RL methods on math reasoning (RLVR is at least the mainstream approach). This raises doubts about the necessity of using DPO for math reasoning. If the authors report FPA on RLHF-related tasks, the case would be much stronger.\n2. The main motivation is: “Regularization in preference learning currently scales gradients using a coefficient computed under the current policy, which is reactive and often engages only after shared, useful tokens have already been over-penalized.” It is unclear whether there is supporting experimental evidence or a theoretical derivation for this statement.\n3. If the focus is solely math reasoning, the experimental tables should add results using the same training data with RLVR. This is the mainstream approach for LLM math reasoning. Readers will be curious about the comparison between FPA and RLVR.\n4. In lines 1059–1060, the authors state that the evaluation metric is pass@1 averaged over 8 runs. Given that scores for math reasoning models often vary substantially across runs [3], it is recommended to increase the number of runs.\n\n[1] AlpacaEval: An automatic evaluator of instruction-following models.\n[2] Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.\n[3] A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility"}, "questions": {"value": "Please refer to my weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZmBvdZQpKp", "forum": "ogKZnm8F7D", "replyto": "ogKZnm8F7D", "signatures": ["ICLR.cc/2026/Conference/Submission10881/Reviewer_yZuy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10881/Reviewer_yZuy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826472066, "cdate": 1761826472066, "tmdate": 1762922092790, "mdate": 1762922092790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim to improve preference alignment of LLMs on mathematical reasoning tasks, by proposing a new algorithm named Future Policy Aware (FPA) preference learning. The authors pose that current existing regularization techniques are reactive (activates after the model has already started to degrade), while FPA is proactive by estimating a future policy. The future policy is estimated via logit space extrapolation from a reference model, and acts to stabilize preference alignment. This simulates a type of \"early brake\" method, and experiments demonstrate best improvement when combined with other algorithms on classical mathematical reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear and well-written paper that discusses the background of well-known preference alignment algorithms and known challenges. There is discussion of gradient entanglement, and Fig 2 shows that the dispreferred gradient norm is consistently larger than the preferred one (validating this as the source of instability). \n\n- Addresses catastrophic forgetting, which is indeed a problem in preference learning today. Preference learning has recently become ubiquitous in the fine-tuning pipeline for many applications, hence the topic of this paper is well-posed and relevant. Fig 5 in particular shows the catastrophic performance collapse of SimPER after 25,000 steps. Fig 7 summarizes ablation studies well.\n\n- The method is computationally cheap, making it simple and lightweight. Since FPA is a linear extrapolation, it adds negligible computational overhead. The method itself is a simple proactive regularizer, and the authors have identified the practical use-case of mathematical reasoning to demonstrate its efficacy."}, "weaknesses": {"value": "- The strongest results are derived from a combination of FPA and SimPER. However this critically negates the key advantage of SimPER: it's reference-free. FPA introduces the need for a reference model. The authors claim this is reasonable since logits can be cached during data generation, but this ultimately adds data complexity and negates SimPER's main selling point. \n\n\n- The proposed method is somewhat incremental. FPA is not a new algorithm for preference alignment, but is an additional step that can be added in the preference alignment pipeline. As the community pushes towards scaling large models feasibly, making the most use of compute resources, extracting maximum value from limited data: other stand-alone preference alignment algorithms (DPO, SimPO, ORPO) advertises being scalable and simple (reference-free). This is a somewhat opposite direction from what FPA introduces. Unless the addition of the extra FPA step (incurring reference models, additional hyperparameters, memory) yields dramatic improvements in general preference alignment. \n\n\n- The paper focuses on the topic of mathematical reasoning as its main motivation, and the gradient entanglement problem is the most severe due to high token overlap. It is unclear if FPA can provide any benefit in other domains, where a dispreferred trajectory might have very little token overlap with a preferred one. \n\n\n- The proposed future policy is heuristics driven. The mechanism is a first-order linear extrapolation in logit space, and not a model-based prediction of a future state. It is unclear if this extrapolation can capture the non-linear dynamics of the optimizer's future state. Since there is some lack of mathematical interpretability, a wider domain of experiments is likely needed to evaluate this heuristic. Especially since it introduces the additional hyperparameter $\\lambda$."}, "questions": {"value": "- Fig 7 shows that adding FPA only to the preferred gradient is not neutral, but causes the model to collapse just like the baseline. This is counter-intuitive. Why does proactively regularizing a \"good\" gradient result in failure?\n\n- How does FPA interact with the learning rate? With reference to Table 3: if using FPA's proactive regularization should you use a higher/lower learning rate than normal, and is it possible to perform ablation on this effect? Currently the paper uses the same learning rate for baselines and FPA variants, which might be sub-optimal. \n\n- Have the authors experimented with other wide domains in preference learning? Such as summarization, or other tasks. Since the future in FPA is not tied to a specific time step, it's unclear how this heuristic future interacts with actual future states being tracked by the optimizer (i.e. variance estimates, etc). Further experimentation in wider domains might yield more transparency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TMZZGb8IbR", "forum": "ogKZnm8F7D", "replyto": "ogKZnm8F7D", "signatures": ["ICLR.cc/2026/Conference/Submission10881/Reviewer_Vjxt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10881/Reviewer_Vjxt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933664856, "cdate": 1761933664856, "tmdate": 1762922092353, "mdate": 1762922092353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"Future Policy Aware\" (FPA) preference learning, a general approach to stabilize and improve preference optimization for large language models (LLMs) in the context of mathematical reasoning. Standard preference learning methods like Direct Preference Optimization (DPO) often degrade performance in mathematical domains due to gradient entanglement between preferred and dispreferred trajectories, which share numerous tokens. FPA addresses this by replacing regularization based on the current policy with an anticipatory version based on an extrapolated \"future\" policy estimated using logit-space interpolation. The approach is shown to provide proactive, adaptive regularization, leading to more stable and effective preference learning. The method is evaluated empirically on MATH and GSM8K benchmarks using three algorithms (DPO, RPO, SimPER) across multiple math-oriented and general LLMs, with FPA delivering consistent improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear Problem Motivation & Importance: The paper clearly articulates the problem of gradient entanglement—an under-addressed yet critical failure mode in mathematical preference learning for LLMs.\n2. Mathematical Detail and Theoretical Analysis: The formulation and justification for FPA are well-explained. The derivation of the extrapolated policy (Equation in section 2 and section 3) ensure the computational efficiency and soundness of the approach. Appendix B offers a careful mathematical rationale for why classical DPO may fail on deterministic preference data (as in math reasoning).\n3. Low Computational Overhead: Section C.2 states and justifies that FPA incurs negligible or only marginal extra cost, leveraging already computed model logits."}, "weaknesses": {"value": "1. No Exploration of Step-wise Preference Learning: While gradient entanglement is clearly diagnosed as a source of performance degradation, the work restricts itself to trajectory-level preference optimization. Recent research indicates that step-level feedback and optimization can more precisely target undesirable intermediate states, which is especially relevant in mathematical reasoning tasks with compositional structure. The absence of any empirical exploration or theoretical rationale for not considering step-level preference mechanisms leaves an opportunity unexplored\n2. Degree of Generalization Claimed Without Evidence: While the paper argues that FPA is broadly applicable to \"any preference learning algorithm relying on policy probabilities,\" the empirical evaluation is limited to three post-training alignment algorithms and two datasets. There is little evidence to support the generalization of FPA to, for example, RLHF variants or reinforcement learning outside mathematical tasks.\n\nReferences\n- Chen, Guoxin, et al. \"Step-level value preference optimization for mathematical reasoning.\" arXiv preprint arXiv:2406.10858 (2024). \n- Lai, Xin, et al. \"Step-dpo: Step-wise preference optimization for long-chain reasoning of llms.\" arXiv preprint arXiv:2406.18629 (2024)."}, "questions": {"value": "1. How does FPA interact with step-level preference learning: Have you compared FPA with recent advances in step-level or sub-trajectory-based optimization methods (e.g., SVPO, Step-DPO), which have also shown strong performance on mathematical reasoning tasks? Furthermore, can FPA be applied to both trajectory-level and step-level optimization, or are there any technical obstacles that would make such integration difficult? \n\n2. Generalization of FPA: How does FPA perform on LLM tasks that are characterized by low gradient entanglement—for example, generative tasks, open-domain dialogue, or summarization? Could you provide empirical evidence or at least qualitative analysis to confirm that FPA does not degrade performance in such settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SJPWMpop5M", "forum": "ogKZnm8F7D", "replyto": "ogKZnm8F7D", "signatures": ["ICLR.cc/2026/Conference/Submission10881/Reviewer_qnp4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10881/Reviewer_qnp4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956266368, "cdate": 1761956266368, "tmdate": 1762922091777, "mdate": 1762922091777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Future Policy Aware (FPA) preference learning, as a plug-in weighting strategy of the preferred and dispreferred samples in DPO-style algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This appears to be a fairly standard DPO-style paper: it introduces a modified loss function, validates results using ≤7B models on math datasets, and conducts typical analyses on metrics and hyperparameters, with an effort to provide more extensive experimental results."}, "weaknesses": {"value": "I find the paper’s motivation, logic, and claimed novelty unconvincing.\n\n1. The motivation for using “regularization” throughout the paper is unclear. For example, in the discussion following Equation (1), the coefficient labeled as a “regularization term” appears mischaracterized; it simply controls the relative strength of components rather than serving as a true regularizer.\n\n2. The relationship between Equation (1) and other loss formulations should be clarified. It would help to derive the gradient of each variant to explicitly show the linear combination between desired and undesired output logits, along with the corresponding coefficients.\n\n3. Equation (5) depends on the current policy $\\pi_\\theta$, the reference policy $\\pi_{\\text{ref}}$, and a hyperparameter $\\lambda$. The authors should provide its explicit form in terms of $\\pi_\\theta$ and $\\pi_{\\text{ref}}$, and explain precisely how it differs from the algorithms in Table 1 that use the hyperparameter $\\beta$. A detailed mathematical derivation is needed here.\n\n4. There is no clear justification for why extrapolating the coefficient term would improve performance over the original formulation, especially given the mathematical ambiguity noted in point 3. I also don't see empirical analysis of the coefficient itself, which should be the main focus of this paper.\n\n5. The novelty appears limited with just slight modification of the DPO-style loss functions.\n\n6. The theoretical analysis is unclear. Points 2 and 3 require mathematical derivations, but none are provided. Moreover, Appendix B.2 focuses entirely on SimPER, which appears unrelated to the main contributions of this work."}, "questions": {"value": "Why FPA is better? I cannot see any reason and benefit by replacing a term in the coefficient to the extrapolation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZGJoMFeT53", "forum": "ogKZnm8F7D", "replyto": "ogKZnm8F7D", "signatures": ["ICLR.cc/2026/Conference/Submission10881/Reviewer_TG7n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10881/Reviewer_TG7n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241791460, "cdate": 1762241791460, "tmdate": 1762922091288, "mdate": 1762922091288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}