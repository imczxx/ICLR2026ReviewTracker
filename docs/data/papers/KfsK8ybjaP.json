{"id": "KfsK8ybjaP", "number": 6700, "cdate": 1757992736802, "mdate": 1759897900181, "content": {"title": "Probabilistic Uncertain Reward Model", "abstract": "Reinforcement learning from human feedback (RLHF) is a critical technique for training large language models. However, conventional reward models based on the Bradley-Terry model (BTRM) often suffer from overconfidence when faced with inconsistent labels or out-of-distribution samples, leading to reward hacking, where the policy model blindly optimizes for proxy rewards while degrading true performance. This paper proposes the Probabilistic Uncertain Reward Model (PURM), which generalizes the Bradley-Terry model to learn the reward distributions that emerged from the preference data. We theoretically derive the loss function of PURM and introduce a novel method that uses the overlap between distributions to define and derive the quantify uncertainty. Empirical results show that PURM outperforms existing methods with more accurate reward and sound uncertainty estimations, and sustains effective learning for more optimization steps and obtain higher maximum win rate in RLHF. The data and code of this paper are released at https://anonymous.4open.science/r/Probabilistic-Uncertain-Reward-Model/", "tldr": "we propose a Probabilistic Uncertain Reward Model (PURM), a natural generalization of the classical Bradley-Terry reward model, that can directly learn the reward distribution emerged from the preference data.", "keywords": ["reward model", "reward hacking", "RLHF", "uncertainty"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96323ae9bc2d0816b831871feec980d87286e7fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Probabilistic Uncertain Reward Model (PURM), a generalization of the\nclassical Bradley–Terry Reward Model (BTRM) that represents rewards not as scalars but as Gaussian distributions.\nThe authors derive the corresponding loss function for PURM and propose a new metric to quantify uncertainty.\nEmpirically, they show that PURM matches existing reward models performance when predicting the reward, pro-\nvides a sometimes more reliable measure of uncertainty, and seems to help mitigate reward hacking when used to train an LLM policy\nwhile improving win rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The challenge of addressing both uncertainty and reward hacking in reward models is interesting, and this paper makes a step in this direction. Modeling the reward as a normal variable seems novel.\n2. Proofs are written step by step and are easy to follow."}, "weaknesses": {"value": "1. Conclusions about hyperparameter choice seems to be drawn from experiments by running a single seed, which\nis far from ideal given the relatively small differences shown in the figures. This makes the results far less convincing. In particular, there seems not to be a large (or any?) difference between using the overlap-measure of uncertainty or just simply the sigma (empirically, that is).\n2. Some metrics, such as length-controlled win rate, are not clearly defined or explained.\n3. Some inconsistencies between loss in the equation and the one coded up, see below in questions."}, "questions": {"value": "1. What is reason for changing the judge in section 3.3 between Figures 5(a) and 5(b) ?\n2. In your implementation of the PURM loss in the code and in Appendix B, you used the average of log sigmoid(z),\nwhereas the straightforward loss from equation 4 is using the log of average of sigmoid(z). This seems to be a discrepancy that you do not mention. The loss you seem to use is an upper bound via Jensen’s inequality, can you explain why not simply use the log of average of sigmoid(z) instead ?\n3. In section 3.2 you claim that your measure of uncertainty performs better, but figure 5(a) shows that using\njust sigma gives comparable performance (given the experiment uses only one seed). How does the average of sigma\nor the average of sigma divided by mu behave ? Can you make a convincing case that your measure performs better?\n4. In figure 10, why do the curves with w=1e5 and w=1e6 differ even at early optimization steps < 400 when\nthere isn’t supposed to be any difference as the list size still hasn’t reached 1e5 ? And if this difference comes\nfrom randomness during optimization then how and why did you conclude that 1e6 is the best hyperparameter\nchoice ?\n\n[Extra question out of curiosity] DPO rewrites the BT reward to arrive at a loss that gets rid of the reward model alltogether. Can you do something similar for your case, when you assume the rewards are Gaussian normals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wqaGkUsOwx", "forum": "KfsK8ybjaP", "replyto": "KfsK8ybjaP", "signatures": ["ICLR.cc/2026/Conference/Submission6700/Reviewer_y6mz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6700/Reviewer_y6mz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736837135, "cdate": 1761736837135, "tmdate": 1762918991808, "mdate": 1762918991808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Probabilistic Uncertain Reward Model (PURM) as an extension of the classic Bradley–Terry reward model. Instead of producing a deterministic scalar reward, PURM outputs the parameters of a Gaussian distribution (μ,σ). The Bhattacharyya coefficient is further employed to measure the overlap between reward distributions, which serves as an uncertainty quantification mechanism. The authors claim that this approach improves the stability of RLHF training and mitigates reward hacking."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation: The deterministic outputs of BTRM indeed lead to overconfidence and reward hacking risks. The paper addresses a practically relevant issue.\n\n2. Simple and intuitive method: Modeling rewards as distributions and incorporating overlap-based uncertainty is straightforward and low-cost to implement.\n3. Interesting uncertainty quantification: Using the Bhattacharyya coefficient, rather than variance alone, better aligns with the intuition of distributional separability.\n4. Reasonable experimental coverage: Includes tests on public preference datasets and RLHF settings, along with ablation studies.\n5. Practical applicability: Minimal code changes are required to integrate PURM into existing RLHF frameworks."}, "weaknesses": {"value": "1. Limited novelty: Distributional reward modeling and uncertainty quantification are not new ideas. For example, URM[1] already proposed modeling reward uncertainty via probabilistic distributions. PURM is conceptually similar but does not clearly articulate its unique theoretical or empirical contributions.\n2. Strong Gaussian assumption: Assuming reward distributions follow a Gaussian lacks justification. Real-world preference data may be skewed or multi-modal, raising concerns about robustness.\n3. High sensitivity to λ: The effectiveness of the method strongly depends on the penalty coefficient λ, yet the paper provides no principled guidance or adaptive mechanism for its selection, limiting practical usability.\n\n[1] Lou, Xingzhou, et al. \"Uncertainty-aware reward model: Teaching reward models to know what is unknown.\" arXiv preprint arXiv:2410.00847 (2024)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zWpXQPNGaH", "forum": "KfsK8ybjaP", "replyto": "KfsK8ybjaP", "signatures": ["ICLR.cc/2026/Conference/Submission6700/Reviewer_CaMJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6700/Reviewer_CaMJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897574204, "cdate": 1761897574204, "tmdate": 1762918991122, "mdate": 1762918991122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Probabilistic Uncertain Reward Model (PURM) as an extension of BTRM used in RLHF. Instead of modeling scalar rewards, PURM represents each reward as a Gaussian distribution parameterized by a mean $\\mu$ and a standard deviation $\\sigma$. This probabilistic formulation aims to capture uncertainty and mitigate reward hacking.\n\nParts of this review were discussed with a colleague to ensure clarity and accuracy.\n\n**Contributions:**\n1. Introduces a probabilistic variant of BTRM that outputs a Gaussian reward distribution rather than a scalar value.\n2. Derives a tractable Monte Carlo–based training objective for learning from preference data.\n3. Proposes a novel use of the Bhattacharyya Coefficient to quantify uncertainty in reward modeling.\n4. Integrates uncertainty into RLHF by penalizing uncertain rewards to mitigate reward hacking."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental results are strong;\n2. The idea of modeling reward uncertainty in RLHF is interesting and conceptually aligns with the intuition that reward confidence should guide policy learning."}, "weaknesses": {"value": "1. The method makes the reward modeling problem much more complicated than necessary. The probabilistic formulation and uncertainty estimation introduce large computational overhead. It would be helpful if the authors could justify whether such improvements are worth the added cost in real-world RLHF settings.\n2. The derivation in Eq. (7)–(9) treats the pairwise reward difference $r_1 - r_2$ as Gaussian, but the validity of this assumption is not discussed. Since both $r_1$ and $r_2$ are modeled as independent Gaussian variables, this implicitly assumes independence between responses, which is unrealistic in preference data (they are conditioned on the same prompt $x$).\n3. The paper approximates the intractable sigmoid–Gaussian integral using Monte Carlo sampling, but does not discuss the computational cost of this approximation during large-scale training.\n4. The proposed use of the Bhattacharyya coefficient as a global uncertainty measure (Eq. 14–15) is questionable. Averaging pairwise overlaps with a random subset of the dataset (Eq. 16) is computationally heavy and not theoretically grounded as an uncertainty estimator.\ntypographical issues:\n1. Inappropriate citation styles: all references are in `\\citep` form. Mixing `\\citet` and `\\citep` properly would improve readability.\n2. Differential notation `d` in `dz`, `dw` and `sigmoid` should be typeset as an operator (e.g.`\\mathrm{d}z`, `\\mathrm{d}w` and `\\sigma(z)`)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QXydpGzm7C", "forum": "KfsK8ybjaP", "replyto": "KfsK8ybjaP", "signatures": ["ICLR.cc/2026/Conference/Submission6700/Reviewer_CysS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6700/Reviewer_CysS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902423884, "cdate": 1761902423884, "tmdate": 1762918990262, "mdate": 1762918990262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the *Probabilistic Uncertain Reward Model (PURM)*, which extends the traditional Bradley–Terry Reward Model (BTRM) by introducing a probabilistic framework that models reward distributions rather than point estimates. PURM further quantifies uncertainty via the Bhattacharyya Coefficient, allowing uncertainty-aware reward penalties during RLHF to mitigate reward hacking. Empirical results show that PURM improves stability and achieves higher win rates over BTRM and other uncertain reward models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is simple, intuitive, and easy to implement. It appears effective in practice and introduces little additional latency compared to BTRM.\n2. The *Uncertainty Evaluation* section presents particularly interesting observations, especially that PURM can adjust its reward uncertainty when training data labels are randomly flipped, while other baselines cannot.\n3. The paper is well-written, with smooth logical flow and clear presentation of both the intuition and methodology."}, "weaknesses": {"value": "1. **Conceptual novelty and related work.**\n    The core idea, i.e. replacing a scalar reward with a Gaussian distribution and introducing an uncertainty-based penalty, is quite straightforward. I am surprised that such a distributional approach to reward modeling has not been explored before. I am not an expert in this subarea, but I found several potentially related works that are not discussed in the paper:\n\n    - *Bayesian Reward Models for LLM Alignment*, ICML 2024 (Workshop)\n    - *Active Preference-Based Gaussian Process Regression for Reward Learning*, RSS 2020\n    - *Know What You Don’t Know: Uncertainty Calibration of Process Reward Models*, arXiv:2506.09338\n    - *Aligning Crowd Feedback via Distributional Preference Reward Modelling*, ICLR 2025 (Workshop)\n\n    If these works are relevant, the authors should position PURM more clearly relative to them, clarify its distinctive contributions, and include comparative experiments. If they are not directly related, it would still be valuable to explain *why* distributional reward modeling has received little prior attention.\n\n2. **Empirical claims need stronger support.**\n\n    - The choice of the Bhattacharyya Coefficient (BC) as the uncertainty measure is insufficiently justified. In Appendix C.1, its performance is not substantially better than simply using standard deviation, and this difference could likely be offset by tuning the hyperparameter λ.\n    - Line 264 (“We attribute this to the fact that …”) asserts a causal interpretation that is not supported by explicit ablation or visualization.\n    - Section 3.3 shows improved RLHF performance, but it does not clearly demonstrate that PURM mitigates *reward hacking* per se; it could simply reflect better learned reward.\n\n3. **Limited exploration of downstream behavior.**\n    The empirical analysis in Section 3.2 is incomplete. More fine-grained studies would greatly strengthen the paper, examples are:\n\n    - Does PURM also improve policy model's robustness to noisy preference data or out-of-distribution (OOD) evaluation tasks?\n    - How does it perform on tasks with inherently low reward noise, such as code or math reasoning where unit-test-based rewards are nearly deterministic?\n\n    Exploring such aspects would definitely help the community understand the broader implications of probabilistic reward modeling.\n\n------\n\n### **Minor Issues**\n\n1. Line 382: *“GPT-4o Hurst et al. (2024) is used …”* should read *“In Hurst et al. (2024), GPT-4o is used …”*.\n2. The figures contain text that is too small to be readable after printing; font sizes should be increased for accessibility.\n\n------"}, "questions": {"value": "See the concerns noted in the *Weaknesses* section. My current rating is deliberately conservative, but I would be happy to engage in discussion and to raise my ratings accordingly it if the authors address these issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9oqnmdldXp", "forum": "KfsK8ybjaP", "replyto": "KfsK8ybjaP", "signatures": ["ICLR.cc/2026/Conference/Submission6700/Reviewer_TxYA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6700/Reviewer_TxYA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189013971, "cdate": 1762189013971, "tmdate": 1762918989697, "mdate": 1762918989697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}