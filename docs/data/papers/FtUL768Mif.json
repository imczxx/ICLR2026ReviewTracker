{"id": "FtUL768Mif", "number": 9258, "cdate": 1758116517570, "mdate": 1759897734831, "content": {"title": "Adapted-Language ViT: Empowering Self-Supervised Vision Transformers with LLMs", "abstract": "The integration of Large Language Model~(LLMs) blocks with Vision Transformers (ViTs) holds immense promise for vision-only tasks by leveraging the rich semantic knowledge and reasoning capabilities of LLMs. However, a fundamental challenge lies in the inherent modality mismatch between text-centric pretraining of LLMs and vision-centric training of ViTs. Direct fusion often fails to fully exploit the LLM's potential and suffers from unstable finetuning. As a result, LLM blocks are kept frozen while only the vision components are learned. As a remedy to these challenges, we introduce Adapted-Language Vision Transformers (ALViT), a novel approach that bridges this modality mismatch through a synergistic pre-training strategy. ALViT co-adapts a ViT backbone and an LLM fusion block by (1) employing Masked Auto-Encoding (MAE) to pre-train the ViT for richer visual representations, and (2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM block using the MAE objective. This joint optimization guides the ViT to produce LLM-aligned features and the LLM to effectively interpret visual information. We demonstrate through extensive experiments that ALViT significantly improves performance in various downstream vision tasks, showcasing an effective and efficient way to harness LLM knowledge for visual understanding.", "tldr": "The paper proposes a method that combines self-supervised learning with adapted large language model blocks to enhance vision transformers, demonstrating strong performance and robustness across multiple benchmarks.", "keywords": ["representation learning", "transformers", "vision-language modeling"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d876602113bf4fc72250125b6cb7b983c8bbd23.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AL-ViT (Adapted Language Vision Transformer), a framework designed to improve multimodal alignment between visual and linguistic representations in large-scale Vision Transformers. The key idea is to co-adapt a ViT and an LLM block through joint MAE-based self-supervision and LoRA-based LLM adaptation, achieving the alignment between modalities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Simple Solution:** The training pipeline (MAE pre-training + LoRA adaptation) is well-structured and straightforward.\n\n**Good presentation:** The paper is well-organized and easy to follow."}, "weaknesses": {"value": "**Incremental Technical Novelty:** The method primarily combines existing techniques — MAE for visual pretraining, LoRA for efficient adaptation, and LLM fusion from LM4Vision — into a joint optimization scheme. There is no fundamentally new algorithmic or theoretical contribution beyond this integration.\n\n**Unclear Mechanistic Insight:** The paper hypothesizes that joint MAE-LoRA training bridges modality mismatch, but provides no formal analysis or theoretical grounding. The attention-entropy study is qualitative and does not establish causality.\n\n**Cost and Gain:** The addition of an LLM block (even partially adapted) introduces large computational overhead and complexity for minimal gain. There is no clear analysis of training cost, FLOPs, or memory increase.\n\n**Weak Motivation for LLM Use:** Since the LLM is frozen and only LoRA-adapted via reconstruction loss (not text supervision), it is unclear whether the language knowledge actually benefits the visual features — especially without textual context or captions."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ysNFx5vuxY", "forum": "FtUL768Mif", "replyto": "FtUL768Mif", "signatures": ["ICLR.cc/2026/Conference/Submission9258/Reviewer_T4ti"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9258/Reviewer_T4ti"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761053262626, "cdate": 1761053262626, "tmdate": 1762920910804, "mdate": 1762920910804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This author introduces a novel framework called ALVIT, which aims to infuse the semantic knowledge of LLMs into self-supervised Vision Transformers to enhance performance on pure vision tasks. To address the modality mismatch between ViTs (vision-centric) and LLMs (text-centric), the authors propose a co-pretraining strategy. This strategy pre-trains the ViT backbone with a MAE objective, while simultaneously using the same MAE reconstruction loss to train Lora layers within the LLM fusion blocks. This joint optimization guides the ViT to produce LLM-friendly features and also enables the LLM to effectively interpret visual information. Experiments show that ALVIT significantly outperforms strong baselines (including MAE-ViT and LM4Vision) on ImageNet and its variants (such as IN-A, IN-C)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Co-Pretraining Strategy**: The core contribution of this paper is its co-pretraining framework. Using the MAE reconstruction loss to simultaneously guide the learning of the ViT backbone and the adaptation of LoRA layers in the LLM blocks is a novel and effective method to resolve the modality mismatch between visual and language representations.\n\n- **Promising Performance**: ALVIT not only achieves performance improvements on ImageNet-1K but, more importantly, demonstrates significant advantages on multiple robustness benchmarks. This strongly proves that the model successfully leverages the knowledge from the LLM to enhance its resilience to out-of-distribution samples.\n\n- **Solid Ablation Studies**: The authors validate the design choices of ALVIT through comprehensive ablation studies.\n\n- **In-depth Mechanism Analysis**: Through a background robustness analysis on ImageNet and visualization of attention entropy, the paper provides profound insights into ALVIT's working mechanism. The analysis indicates that ALVIT exhibits a stronger ability to distinguish between background and foreground, which explains the source of its robustness."}, "weaknesses": {"value": "- **Insufficient Discussion on MAE as an SSL Paradigm**: The authors chose MAE because its reconstruction loss is suitable for co-training. However, the authors' own analysis (Figure 3(a)) shows that the attention pattern of the MAE ViT/B baseline is \"indifferent\" to background and foreground regions. Given that other SSL paradigms (like DINO) are known for their strong foreground/background separation capabilities, the paper should further discuss why MAE is the sole or optimal choice for this task and whether other SSL objectives were considered.\n\n- **Lack of Explanation for \"Stronger LLMs Not Bringing Gains\"**: A surprising finding (Table 3) is that using more advanced LLMs (like Gemma 2 or LLaMA 3.1) did not bring any performance improvement over the older LLaMA 1. This contradicts the intuition that stronger LLMs should provide richer semantic knowledge. The paper reports this phenomenon but fails to provide an in-depth discussion or hypothesis. Does this imply that ALVIT primarily utilizes the general transformer architecture of the LLM blocks rather than their specific, more advanced semantic knowledge?\n\n- **Limited and Modest Gains on Downstream Tasks**: Although the paper includes object detection and instance segmentation results on MS COCO in Appendix B.1, these evaluations are not presented in the main text. Furthermore, while the results show consistent improvements, the gains are relatively modest (e.g., only a +0.5 increase in Bounding Box AP on COCO). To more comprehensively demonstrate that ALVIT is a superior representation learning method, the authors should present evaluations on a broader range of downstream tasks in the main paper."}, "questions": {"value": "Please see the weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Is3rOmfBjs", "forum": "FtUL768Mif", "replyto": "FtUL768Mif", "signatures": ["ICLR.cc/2026/Conference/Submission9258/Reviewer_4iid"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9258/Reviewer_4iid"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447992569, "cdate": 1761447992569, "tmdate": 1762920910252, "mdate": 1762920910252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Adapted-Language Vision Transformers (ALViT), which integrate frozen LLM blocks into ViTs via a MAE + LoRA self-supervised training scheme. By co-adapting both modules, ALViT achieves higher accuracy than previous LLM-fusion baselines and robustness on ImageNet benchmarks and shows improved background sensitivity via attention-entropy analyses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear and novel motivation: extend supervised only LM4Vision to SSL training.\n2. Efficient tuning: The author find the frozen LM layer is not suitable for SSL training, and uses LoRA for finetuning without increasing much trainable parameters.\n3. Solid empirical performance: although marginal, consistently outperforms MAE-ViT basleine and LM4Vision; robustness improvements are also convincing.  \n4. Thorough ablations: analyzes LoRA, parameter count, LLM layers, random initialization, and multiple seeds.  \n5. attention entropy visualizations support the hypothesis of improved information filtering and robustness."}, "weaknesses": {"value": "1. Lack of justification for the training objective:\nMasked Image Modeling is a well-established self-supervised learning objective, but it is no longer the most advanced one. Methods such as MoCo, DINO, and iBOT all outperform MAE by a large margin. Why do the authors choose MAE instead of adopting these stronger SSL objectives?\n\n2. Lack of metrics:\nThe paper mainly evaluates fine-tuning accuracy. However, for SSL models, other important metrics include linear probing accuracy and kNN accuracy. Can ALViT also outperform the MAE baselines on these metrics?\n\n3. Missing baseline:\nIn Table 3, the authors compare different variants with roughly the same number of trainable parameters. However, an important baseline seems missing: simply increasing the depth of the MAE ViT-B to match the parameter count (for example, using a 13-layer ViT-B). Additionally, the paper primarily focuses on ViT-B-sized models—can ALViT maintain its advantage across larger model scales?"}, "questions": {"value": "1. Why do the authors choose Masked Image Modeling as the training objective, given that more advanced SSL methods such as MoCo, DINO, and iBOT have been shown to outperform MAE by a large margin?\n\n2. Can the authors report additional SSL metrics such as linear probing accuracy and kNN accuracy to evaluate whether ALViT also outperforms MAE baselines on these measures?\n\n3. Can the author provide an additional baseline where the depth of MAE ViT-B is simply increased (e.g., using a 13-layer ViT-B) to match the trainable parameter count of ALViT? \n\n4. Can the authors verify whether ALViT maintains its performance advantages across different model scales beyond ViT-B?\n\nSince there is no borderline this year, I would still recommend borderline accept at this point for the clear motivation, solid results and experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T9WfYJFxLp", "forum": "FtUL768Mif", "replyto": "FtUL768Mif", "signatures": ["ICLR.cc/2026/Conference/Submission9258/Reviewer_Ajvx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9258/Reviewer_Ajvx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662531449, "cdate": 1761662531449, "tmdate": 1762920909615, "mdate": 1762920909615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, building on the foundation of LLM4Vision, introduces a new method for using LLMs to enhance vision-only capabilities. The main change is the addition of the Masked Auto-Encoding (MAE) pre-training task, during which the LLM is fine-tuned via LoRA. Overall, it has achieved a few improvements in image classification tasks. This work has inspirational significance (or heuristic value) for the study of vision-only pre-training paradigms that can simultaneously fine-tune both the visual encoder and the LLM."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper is well-written, clearly articulated, and the methodology is relatively easy to follow.\n2.  The exploration of applying LLMs within a vision-only pre-training paradigm is insightful.\n3.  The feature analysis within the ablation study is well-executed, providing an intuitive visualization of the differences at the feature representation level resulting from the proposed training strategy."}, "weaknesses": {"value": "1.  This work offers limited novelty built upon the LLM4Vision foundation. The finding that fine-tuning a few parameters on an adapted pre-training task can boost performance is rather straightforward and somewhat anticipated.\n2.  Furthermore, judging from the final results, the performance improvement appears to be quite marginal.\n3.  Regarding performance validation, the authors have primarily focused on classification tasks. As a general-purpose visual encoder, its effectiveness must be validated on a broader range of vision tasks (e.g., detection, segmentation, visual understanding).\n4.  Moreover, the paper lacks direct comparisons with LLM4Vision across this wider set of tasks, making it difficult to fully assess the benefits of the proposed modifications."}, "questions": {"value": "1.  What is the model's performance on a more diverse set of vision tasks (e.g., object detection, semantic segmentation)? \n2.  Are there more experimental results providing a direct comparison against LLM4Vision under identical settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KttX2o0u9p", "forum": "FtUL768Mif", "replyto": "FtUL768Mif", "signatures": ["ICLR.cc/2026/Conference/Submission9258/Reviewer_BRBi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9258/Reviewer_BRBi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967473925, "cdate": 1761967473925, "tmdate": 1762920909185, "mdate": 1762920909185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}