{"id": "YTonAPBqI9", "number": 3406, "cdate": 1757419262474, "mdate": 1759898091658, "content": {"title": "EduDial: Constructing a Large-scale Multi-turn Teacher–Student Dialogue Corpus", "abstract": "Recently, several multi-turn dialogue benchmarks have been proposed to evaluate the conversational abilities of large language models (LLMs). As LLMs are increasingly recognized as a key technology for advancing intelligent education, owing to their ability to deeply understand instructional contexts and provide personalized guidance, the construction of dedicated teacher-student dialogue benchmarks has become particularly important. To this end, we present EduDial, a comprehensive multi-turn teacher-student dialogue dataset. EduDial covers 345 core knowledge points and consists of 34,250 dialogue sessions generated through interactions between teacher and student agents. Its design is guided by Bloom’s taxonomy of educational objectives and incorporates ten questioning strategies—including situational questioning, zone of proximal development (ZPD) questioning, and metacognitive questioning—thus better capturing authentic classroom interactions. Furthermore, we design differentiated teaching strategies for students at different cognitive levels, thereby providing more targeted teaching guidance. Building on EduDial, we further develop EduDial-LLM 32B via training and propose an 11-dimensional evaluation framework that systematically measures the teaching abilities of LLMs, encompassing both overall teaching quality and content quality. Experiments on 17 mainstream LLMs reveal that most models struggle in student-centered teaching scenarios, whereas our EduDial-LLM achieves significant gains, consistently outperforming all baselines across all metrics.", "tldr": "", "keywords": ["Large Language Models; Multi-turn Teacher-Student Dialogue;Intelligent Education"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/17619cea33912107294e7ed6b75fea842286eb43.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This article presents a high-quality multi-round teacher-student dialogue dataset (EduDial) that covers 345 core K-12 mathematics knowledge points and contains 34,250 dialogues. The dataset is split into two parts:  \n(1) MTI (SFT) dataset – 13,700 “one-teacher-three-students” dialogues generated by GPT-o1 under five teaching principles;  \n(2) PDTS (DPO) dataset – 20,500 “one-teacher-one-student” preference pairs generated by an SFT model and further refined with differentiated-teaching prompts.  \nAll samples are validated through Expert-Machine Dual Verification. An 11-dimensional expert-designed evaluation framework is proposed and used to benchmark mainstream LLMs. After two-stage SFT+DPO training on EduDial, the 32B model surpasses current state-of-the-art LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) Practical educational value: the dataset is constructed under expert pedagogical guidance and emphasizes personalized instruction, emotional support, and teaching students according to their aptitude.  \n(2) Comprehensive evaluation: a multidimensional framework is established and applied to a wide range of open-source and proprietary LLMs, providing a solid foundation for follow-up research.  \n(3) Rigorous quality control: expert verification and machine filtering are adopted throughout data generation."}, "weaknesses": {"value": "(1) Evaluation protocol ambiguity: it is unclear whether the quality scores come from the 1v3 (MTI) setting, the 1v1 (PDTS) setting, or both; the score differences between Table 2 and Table 3 are not explained.  \n(2) Student-model issue: the paper mentions that the student side of the SFT dataset receives inputs from the teacher and other students, yet it does not clarify why an SFT-trained “student model” is used or how it participates in evaluation; if the evaluated teacher model interacts with the same student model it was trained with, the reported gains may reflect familiarity rather than generalizable teaching ability.  \n(3) OOD generalization concern: because student profiles are identically distributed in training and test sets, performance improvements might degrade for out-of-distribution students.  \n(4) Stage-split motivation: the paper does not adequately justify why dataset construction and training are split into two distinct stages (1v3 vs. 1v1); the relevance of the unused stage to the final evaluation scenario needs further explanation.  \n(5) Differentiated teaching location: differentiated instruction is emphasized in the DPO dataset but is absent from the SFT dataset; the rationale for this asymmetry is not provided.  \n(6) Dialogue length: the maximum sequence length in the DPO stage is set to 1024 tokens, which may be insufficient for longer multi-turn conversations."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "H4BpbnqEmr", "forum": "YTonAPBqI9", "replyto": "YTonAPBqI9", "signatures": ["ICLR.cc/2026/Conference/Submission3406/Reviewer_AVdn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3406/Reviewer_AVdn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552043332, "cdate": 1761552043332, "tmdate": 1762916708842, "mdate": 1762916708842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents EduDial, a comprehensive multi-turn teacher-student dialogue dataset. EduDial covers 345 core knowledge points and consists of 34,250 dialogue sessions generated through interactions between teacher and student agents. The dataset is designed to help large language models (LLMs) ask appropriate questions at the right time in educational settings. It follows five progressive teaching stages, introduction, concept exploration, deep understanding, knowledge application, and reflection, and incorporates ten questioning strategies aimed at students with varying cognitive levels. The authors also train EduDial-LLM 32B using this dataset and evaluate the model using an 11-dimensional evaluation framework, assessed by both humans and LLM-based evaluators."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Propose a multi-turn teacher-student dialogue dataset that covers 345 core knowledge points and comprises 34,250 dialogue sessions.\n2. Design teacher and student agents that follow progressive teaching stages and reflect students’ varying cognitive levels.\n3. Conduct manual reviews to assess both the quality of the dataset and the teaching effectiveness of the model."}, "weaknesses": {"value": "1. It is unclear whether the MTI and DPO datasets are intended to simulate classroom interactions involving multiple students and a single teacher, or one-on-one teacher-student dialogues. For example, line 344 suggests a one-on-one setting, while the dialogue examples in the appendix appear to reflect one-to-many interactions. In the latter case, it is also unclear whether the dialogues are specifically tailored to each student's cognitive level.\n2. It is not evident whether the quality of the student agent simulation has been evaluated.\n3. While the paper mentions a human evaluation, it lacks information about the qualifications or expertise of the evaluators, specifically, whether they are capable of assessing teacher-student educational dialogues."}, "questions": {"value": "Is there an ablation study or justification for choosing QwQ-32B-Preview as the backbone model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "32lETCuUns", "forum": "YTonAPBqI9", "replyto": "YTonAPBqI9", "signatures": ["ICLR.cc/2026/Conference/Submission3406/Reviewer_1cck"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3406/Reviewer_1cck"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963771540, "cdate": 1761963771540, "tmdate": 1762916708598, "mdate": 1762916708598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the issue that general dialogue benchmarks fail to evaluate the core capabilities of large language models (LLMs) in the education domain. It proposes EduDial, a multi-turn teacher-student dialogue dataset, which covers 345 core knowledge points and 34,250 dialogue sessions. Guided by Bloom’s taxonomy, the dataset is designed with a five-stage teaching process and ten questioning strategies, and differentiated teaching strategies are formulated for students at different cognitive levels. Based on EduDial, the researchers trained EduDial-LLM 32B and proposed an 11-dimensional evaluation framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a good approach. The proposed dialogue dataset may be beneficial for certain scenarios in the education domain. \nThe figures in this paper clearly illustrate the entire process, allowing for quick comprehension of the workflow."}, "weaknesses": {"value": "1. The paper mentions two major challenges: (1) Frequent questioning may disrupt students’ cognitive flow; (2) Adjusting questioning strategies according to different teaching stages. However, the paper does not adequately elaborate around these two challenges. I fail to understand how the work in this paper addresses the first challenge.\n\n2. In the validation section, the use of the SFT + DPO verification method to validate the effectiveness of the dataset is questionable. How can I tell whether the improved performance stems from well-trained models or the superiority of the dataset? This remains doubtful.\n\n3. I do not know what the base model used for training EduDial is—did I miss any relevant information?\n\n4. The discussion on the combined evaluation of human expert assessment and machine evaluation is far from sufficient. Although an experiment proves a certain degree of similarity between human and machine evaluations, it cannot rule out the bias in human expert assessment. \n\n5. Relying solely on the expert-machine dual verification (GPT-4o + expert review) and lacking data calibration involving real teachers and students, the paper fails to prove that the data can truly reflect classroom teaching logic."}, "questions": {"value": "Please see weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7Ae93wykY5", "forum": "YTonAPBqI9", "replyto": "YTonAPBqI9", "signatures": ["ICLR.cc/2026/Conference/Submission3406/Reviewer_A2ik"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3406/Reviewer_A2ik"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762091841167, "cdate": 1762091841167, "tmdate": 1762916708408, "mdate": 1762916708408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}