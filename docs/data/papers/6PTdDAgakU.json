{"id": "6PTdDAgakU", "number": 12310, "cdate": 1758207015799, "mdate": 1759897518621, "content": {"title": "Unsupervised Elicitation of Language Models", "abstract": "To steer pretrained language models for downstream tasks, today's post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, we introduce a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels,  without external supervision.  On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, our method matches the performance of training on golden labels and outperforms training on crowdsourced human supervision. On tasks where LMs' capabilities are strongly superhuman, our method can elicit those capabilities significantly better than training on human labels. Finally, we show that our method can improve the training of frontier LMs: we use our method to train an unsupervised reward model and use reinforcement learning to train a Claude 4 Sonnet-based assistant. The resulting assistant matches its counterpart trained on production-grade human labels on average, with higher scores on chat and safety yet lower scores on math and coding.", "tldr": "We propose an unsupervised method to post-train pre-trained language models, which can match or exceeds real-world human supervision even at production scale.", "keywords": ["language models", "post-training", "elicitation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d58cef3a45d66d2c2b39ad3106fa686cbb5c2ba.pdf", "supplementary_material": "/attachment/7199b4257fc4d6565653f78f9c2d5f239cef412e.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Internal Coherence Maximization (ICM), an unsupervised elicitation algorithm that fine-tunes pretrained LMs on self-generated labels without external supervision. It optimizes a score combining mutual predictability (labels are inferable from one another under the LM) and simple logical consistency constraints, using a simulated-annealing-style search over label assignments. On TruthfulQA, GSM8K-verification, and Alpaca RM, ICM reportedly matches golden-label training and outperforms crowdsourced human labels; further, it trains an unsupervised RM to drive RL for a Claude 4 Sonnet assistant with average performance comparable to a human-supervised RM, learning faster on chat and safety. The authors argue ICM can elicit superhuman capabilities when present in the pretrained model and highlight limits when concepts are not salient to the LM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear unsupervised objective: mutual predictability plus lightweight consistency constraints yields a simple, general scoring function that avoids explicit human labels.\n\nPractical search procedure: the simulated-annealing-like loop and inconsistency-fixing subroutine are straightforward to implement and explain.\n\nBroad evaluations: includes standard benchmarks and a production-style assistant setting with reward-model-driven RL; analyzes failure cases when concepts are not salient.\n\nAblations: examines initialization robustness, role of consistency, and compares to equally accurate random label perturbations, supporting the value of the learned labels."}, "weaknesses": {"value": "1. Benchmark currency and coverage: Several core evaluations are dated. For conversational ability and instruction-following, please include AlpacaEval 2.0 (length-controlled) and Arena-Hard 2.0; for verifiable reasoning, add recent math suites such as Math500 and AIME’24/’25 style evaluations to better substantiate claims on reasoning/generalization.\n\n2. Missing RLAIF and weak-supervision baselines: Comparisons should include popular RLAIF pipelines using AI-labeled feedback from stronger external judges/reward models, as well as modern weakly supervised/self-training methods. This helps isolate the advantage of ICM against established AI feedback and weak-labeling approaches.\n\n3. Self-bias concerns: The framework may amplify a model’s own biases or spurious correlations, especially since mutual predictability is computed under the same LM that will be fine-tuned. Please clarify how “self-bias” is diagnosed, monitored, and mitigated (e.g., cross-model agreement, disagreement-based sampling, calibration checks, or ensemble critics).\n\n4. Contribution/novelty positioning: Relative to recent self-improvement/self-training literature, the framing risks reading as a prompt/label search variant with limited conceptual novelty. A clearer theoretical positioning and empirical differentiation from modern self-improving pipelines (e.g., iterative self-consistency labeling, judge-as-teacher schemes, entropy minimization in reasoning) would strengthen the contribution.\n\n5.  External validity on frontier models: While the Claude-based assistant result is promising, please provide stronger head-to-head baselines (e.g., RL with high-quality human labels at different scales, AI-judge–driven RLAIF with modern judges) and report human eval or blinded pairwise comparisons to reduce circularity risks when using an in-family RM as evaluator."}, "questions": {"value": "See weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1944TnANrM", "forum": "6PTdDAgakU", "replyto": "6PTdDAgakU", "signatures": ["ICLR.cc/2026/Conference/Submission12310/Reviewer_fmdG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12310/Reviewer_fmdG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760575849100, "cdate": 1760575849100, "tmdate": 1762923238157, "mdate": 1762923238157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes internal coherence maximization (ICM), an unsupervised post-training method that iteratively generates labels with a language model and then finetunes the model with its self-generated labels. During the label generation & selection process, ICM uses mutual predictability and logical consistency criteria for scoring. ICM is evaluated comprehensively on several benchmarks as well as production level Claude post-training tasks. Results show that ICM is a promising unsupervised post-training approach and is comparable to human annotations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-motivated and important problem: studying how to improve language models without human supervision is an important topic in the field, especially for hard tasks such as math and scientific research.\n2. Clear idea and simple methodology that works well: the proposed ICM method is conceptually neat and seems easy to implement. Meanwhile the results are pretty strong given this intuitive approach.\n3. Broad evaluation: the authors conduct many experiments and ablations to study ICM and show its effectiveness on many task domains."}, "weaknesses": {"value": "**The \"superhuman\" framing and claim is problematic given the evaluation method**\n- Why are GSM8K, TruthfulQA, and Alpaca used as proxies for superhuman supervision tasks? Why not include harder/cleaner/less-contaminated \"superhuman\" benchmarks (e.g., MATH, GPQA, AIME, etc.) if the central claim is eliciting beyond human-quality capabilities?\n- The \"superhuman capability\" demonstration uses a gender prediction task. This seems more like a patten matching task instead of complex reasoning, and in this task human annotators very likely do not have enough knowledge about male vs. female writing. A more convincing evidence would be showcasing strong performance on latest benchmarks such as GPQA or SWE-bench that ICM-trained models can surpass RLHF-trained models.\n\n**Unrealistic assumption of zero ground truth**\n- In practice, we can always train LLMs on easy tasks where we have ground truth labels. How does ICM compare to easy-to-hard generalization (i.e., prompting/finetuning on easy tasks with ground truth and evaluate on hard tasks)? \n- I suspect that on hard tasks like GPQA and MATH, it's much harder for model to explore and filter good labels with the consistency scoring rule and add-one-label-per-iteration method. It would be very informative if the authors provide comparison between training/prompting with easy ground truths and ICM on hard tasks such as MATH and GPQA.\n- Relatedly, how does ICM compare to confidence-threshold pseudo-labeling?\n\n**Questionable results on Alpaca**\n- In Alpaca, it is surprising that prompting also beats training with human feedback (Figure 3 right). In high‑quality industrial pipelines this is rarely observed. Is this due to quality issues of the dataset used or unreliability in the test gold labels (majority vote of four crowd workers)?\n\n**Missing analysis of the method**\n- ICM relies on iteratively improving label quality. However, there's no label-accuracy-over-ICM-iterations discussion in the paper. Without this, it's unclear whether performance arises from accurate labels generated by ICM or from other properties of the pseudo-labels (e.g., selecting task *prompts* that are useful instead of labels).\n- In addition, the method's sensitivity to $\\alpha$ is not fully explored. It would be beneficial to see how different $\\alpha$ affects ICM's performance."}, "questions": {"value": "Please see questions discussed in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sqrfjDCnD0", "forum": "6PTdDAgakU", "replyto": "6PTdDAgakU", "signatures": ["ICLR.cc/2026/Conference/Submission12310/Reviewer_J2Nm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12310/Reviewer_J2Nm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761592052285, "cdate": 1761592052285, "tmdate": 1762923237910, "mdate": 1762923237910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (Part 1)"}, "comment": {"value": "**1. Evaluated tasks not hard enough / not really superhuman [Mvot,J2Nm,fmdG]**\n\nWe’d like to clarify that our experiments already include tasks that humans cannot reliably supervise. For example, humans only achieve 60% accuracy on gender prediction; human labels are noisy on preference modeling tasks like Alpaca [1].\n\nIn particular, given reviewers’ specific interests on hard reasoning tasks, we’d like to emphasize that the Claude dataset in Sec 4.4 already covers many hard-to-supervised tasks, such as judging correctness in MATH/GPQA-level hard reasoning tasks.\n\nWe further run additional experiment results on MATH (constructed similarly to GSM8k-verification), which is suggested by multiple reviewers (J2Nm,fmdG) as a hard task. Our method ICM nearly matches training on gold labels and significantly improves upon zero-shot baseline.\n\n| Method                          | Accuracy   |\n| ------------------------------- | ---------- |\n| Zero-shot                       | 57.5 ± 1.2 |\n| Golden Supervision              | 75.5 ± 0.3 |\n| ICM (Ours) | 74.7 ± 0.2 |\n\n**2. How was data contamination ruled out [FrSc,J2Nm]**\n\nWhile we cannot directly check data contamination since we don’t have access to llama pre-training corpus, there are several pieces of evidence that make data contamination less worrying.\n- As shown in figure 2, the zero-shot performance of llama base models are close to randomly guessing (e.g. 60% on TruthfulQA, 50% on Alpaca, 48% on GSM8K)\n- We reformat GSM8K and TruthfulQA into classification tasks. The GSM8K solutions are also newly sampled from diverse LMs. Take GSM8K for example, while GPT-4o achieves an accuracy of 89.8% in the original format (i.e. directly generating solutions), it only achieves an accuracy of 65% in judging solution correctness.\n- Most of our experiments are based on llama models. Recent papers ([2]) show that while Qwen models have serious data leakage issues that make even optimizing with random rewards increase their performance on math/coding benchmarks, llama models do not.\n- In Sec 4.4, we confirm that the production preference dataset is not involved in the pre-training corpus of Claude models.\n\n\n[1] AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. NeurIPS 2023.\n\n[2] Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination. arXiv 2025\n\n[3] The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning. NeurIPS 2025"}}, "id": "zO226phSGS", "forum": "6PTdDAgakU", "replyto": "6PTdDAgakU", "signatures": ["ICLR.cc/2026/Conference/Submission12310/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12310/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12310/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763634919186, "cdate": 1763634919186, "tmdate": 1763634919186, "mdate": 1763634919186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Internal Coherence Maximization (ICM), an unsupervised post-training method for language models that removes the need for human-labeled supervision. Instead of using external labels, ICM fine-tunes a pretrained model on labels it generates for itself, searching for a labeling scheme that is both mutually predictable (labels can be inferred from one another under the model) and logically consistent. Using tasks such as GSM8K-verification, TruthfulQA, and Alpaca, ICM achieves performance comparable to training with golden labels and surpasses crowdsourced human supervision. It also outperforms commercial chat models on these benchmarks. On a superhuman task (author-gender prediction), ICM elicits capabilities that humans cannot reliably label. Furthermore, the authors train a Claude 4 Sonnet assistant entirely without human labels, obtaining results on par with a human-supervised version. The work positions unsupervised elicitation as a viable alternative to RLHF for aligning frontier models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper stands out for its originality and surprisingly strong results. The idea of training LMs without any human labels—using Internal Coherence Maximization to find logically consistent, self-generated labels—is both simple and powerful. The experiments convincingly show that ICM can match or beat human-supervised baselines and even train a Claude 4 assistant competitively. The method feels timely and meaningful as models grow beyond human supervision, and the authors back it up with clear ablations and thoughtful analysis."}, "weaknesses": {"value": "The main limitation is that ICM’s success depends heavily on how well the underlying model already understands the target concept. When the concept isn’t salient, the method collapses to random guessing. The paper could also do more to explain why mutual predictability works so well—right now it feels more empirical than theoretical. In addition, using closed models like Claude limits reproducibility and makes it hard to verify the claimed parity with human-supervised training."}, "questions": {"value": "How was data contamination ruled out, given that the datasets are public and large models often see similar content in pretraining?\n\nWhat happens if ICM is applied to more open-ended generation tasks rather than classification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iLLrf1Solw", "forum": "6PTdDAgakU", "replyto": "6PTdDAgakU", "signatures": ["ICLR.cc/2026/Conference/Submission12310/Reviewer_FrSc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12310/Reviewer_FrSc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966405649, "cdate": 1761966405649, "tmdate": 1762923236884, "mdate": 1762923236884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an unsupervised procedure (ICM) to discover and train on a set of latent “consistent” labels that a pretrained LM can already internally predict. Concretely, it searches for label assignments that (i) are mutually predictable by the model and (ii) don’t violate simple logical constraints, then fine-tunes on those labels. Evaluations cover truthfulness, math-verification, preference data, a stylized “salience” stress test, and a larger RL setting with an unsupervised reward model."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "+ Clever, self-justifying idea: if the model already “knows” a concept, use that signal instead of noisy human labels. The objective is clean and intuitive.\n+ The framework is modular: predictability term + logical consistency + simple search/repair loop.\n+ Salience analysis is honest and useful (the method fails when the concept isn’t in the model).\n+ Early signs the approach can scale (reward modeling / RL) rather than being just a small-bench trick."}, "weaknesses": {"value": "- Scope/generalizability unclear. Most demonstrations look like binary or pairwise decisions (true/false, better/worse). It’s not clear how the objective behaves with non-binary targets. The paper reads a bit specialized to “logical-consistency-style” problems.\n- Missing self-rewarding/self-training baselines. For a claim of “unsupervised elicitation,” comparisons to modern self-rewarding / RLAIF-style methods (LM-as-judge or LM-derived rewards), and simple self-training with confidence filters are expected.\n- Weaker on harder/specialized tasks (e.g., chat hard/math). The overall eval on truly hard domains feels thin."}, "questions": {"value": "- Beyond binary/pairwise: how would you expect the method to adapt to multiclass labels (e.g., 4–5 categories) or even more general tasks?\n- How do you guard against spurious-but-consistent solutions when constraints are incomplete, especially on hard/nuanced tasks where these samples might be prevalent. Any diagnostics to detect / mitigate this?\n- Could you please also report/estimate the computational overhead of the searching process vs. reward modeling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fPp4R9iSiE", "forum": "6PTdDAgakU", "replyto": "6PTdDAgakU", "signatures": ["ICLR.cc/2026/Conference/Submission12310/Reviewer_Mvot"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12310/Reviewer_Mvot"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992152973, "cdate": 1761992152973, "tmdate": 1762923236593, "mdate": 1762923236593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}