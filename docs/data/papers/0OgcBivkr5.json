{"id": "0OgcBivkr5", "number": 17547, "cdate": 1758277410636, "mdate": 1759897167965, "content": {"title": "Lifelong-Learning Embeddings: Incremental and Continual Representation Learning for Dynamic E-Commerce Trends", "abstract": "E-commerce is a highly dynamic domain where products and consumer behaviors evolve rapidly. Embedding-based representations are central to deep learning–based personalization systems, yet conventional embeddings are static and therefore, they cannot easily incorporate new tokens (e.g., new products) without retraining, which is costly and often infeasible due to privacy or data retention constraints. To address this, we propose Lifelong-Learning Embeddings, a framework that (1) incrementally extends embeddings to integrate new tokens, (2) adapts embedding dimensionality to balance expressiveness and efficiency, and \n(3) employs continual learning to mitigate catastrophic forgetting. Experiments on a real-world dataset and two benchmark datasets show that our approach consistently outperforms static embeddings in accuracy while incurring only modest training-time overhead, demonstrating its effectiveness and adaptability in dynamic e-commerce environments.", "tldr": "Lifelong-Learning Embeddings (LLE) dynamically expand vocabularies, adapt embedding dimensionality, and apply continual learning to preserve past knowledge while integrating new data.", "keywords": ["Embeddings", "Representation learning", "Incremental Learning", "Continual Learning", "E-Commerce"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4aec4db5f56ed0d9e8e9487284ba7bc1b3c15e4f.pdf", "supplementary_material": "/attachment/4402a4b72e07157e82dae1b10d4c17b7647aac65.zip"}, "replies": [{"content": {"summary": {"value": "The paper focuses on the challenge of learning embeddings in dynamic e-commerce environments, where products and consumer behaviors change rapidly. Traditional static embeddings are inadequate if they fully retrain the model to incorporate new tokens, which is computationally expensive and often infeasible due to privacy or data retention constraints. To tackle this, the authors propose LLE, a framework that incrementally updates embeddings to include new tokens, adapts embedding dimensionality for efficiency, and applies CL to mitigate catastrophic forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper targets a practical and important challenge in e-commerce recommendation systems\n- Overall, the paper is well written and easy to follow."}, "weaknesses": {"value": "- The novelty of the paper is limited. The proposed embedding addition step and CL step in LLE are standard methods widely used in existing systems. The proposed idea of alignment is closely related to the dimensionality change in the second step, which is relatively uncommon in practical systems, but the proposed alignment loss is not new and the motivation of adaptive embedding sizes are not well justified.\n- While the authors claim that their method is a drop-in replacement for existing embedding layers, but this is not true if they have dynamic embedding sizes because most neural models don’t support this.\n- Experiments are conducted with simple baselines without considering many related works on cold-start recommendation and auto-regressive recommendation. In fact, there have been many studies in cold-start recommendation that can generate embeddings for new items (e.g., via LLMs) and train them together with the old ones."}, "questions": {"value": "How does LLE perform if incorporated into a fully end-to-end training of a SOTA model in recommendation tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m09yC22vgN", "forum": "0OgcBivkr5", "replyto": "0OgcBivkr5", "signatures": ["ICLR.cc/2026/Conference/Submission17547/Reviewer_yjCu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17547/Reviewer_yjCu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760696667094, "cdate": 1760696667094, "tmdate": 1762927419185, "mdate": 1762927419185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies a new embedding learning method for recommender systems called Lifelong-Learning Embeddings (LLE).\nThe key idea is that as new tokens (categorial featues) are introduced to the vocabulary at time step $t + 1$,\nLLE gives a way to:\n- retain previously learned embeddings for all tokens up to time $t$, and\n- automatically resize embedding dimensions based on validation loss while minimizing information loss via continual learning (CL).\n\nThe authors apply LLE to 1 proprietary industirial dataset and 2 public benchmarks (YooChoose and RetailRocket).\nThe time scale of vocabulary shifts considered is 1-week, so Algorithm 1 is run\non each week of data and presented in Section 6. Overall, the approach and experiments are interesting and compelling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work attackes a very important problem (i.e., good embedding learning methods for online recommender systems)\n- Their method adapts the dimension of the embeddings over time.\n- Self-contained layer designed to replace existing embedding layer without modifying downstream part of model too much\n- Good choice of datasets and interesting experiments"}, "weaknesses": {"value": "- How do we deal with large-vocabulary features (e.g., cardinality = 10^9 id features)?\n  * This work doesn't mention the hashing trick (\"Feature hashing for large scale multitask learning\" [Weinberger et al., ICML 2009])\n  * How can LLE afford to keep collionless embeddings for tokens in a growing vocabulary? As written, this can blow out RAM."}, "questions": {"value": "**Questions**\n- [063] Are the \"knowledge retention\" and \"compactness properties\" contradictory?\n  If we keep computational efficiency constant (e.g., the total bottom\n  embedding dimension) and increase one feature's embedding dimension,\n  then we necessarily reduce another feature's embedding dimension. How does\n  LLE reconcile this? Line 170 mentions that CL is used. How does this work? It seems that we're forced to degrade model quality.\n- If a new token arrives at each online example (e.g., a new search query),\n  how do you solve the CL step (line 201) fast enough? It's not clear to me\n  how this can scale up to an industry setting. It seems helpful to discuss how large a data iteration $D^(t)$ should be\n  (e.g., a batch of 2048 events in real time or O(hour) data).\n- Re Figure 2: What are the \"Unknown tokens\" in subplots (d--f)? Do\n  they correspond to the \"New Tokens\" in plots (a--c)? If so, why aren't the\n  first and second columns in plots (d--f) mostly orange? Is this due to the\n  removal of the 99.5th percentile of interactions?\n- In line 283, which transformer classifier / reference are you using?\n- In Algorithm 1 Line 2, how do you decide to update\n- How do you choose $\\Delta d$ when validation loss overfits/underfits (line 311)?\n- In Figure 6, the average embedding size of `Industrial` increases over time.\n  This increases model resource cost. If we have a fixed budget on resource cost,\n  what would happen in this case?\n- Given that the alignment loss function in line 200 is generic, it would be nice to\n  see experiments where everything is fixed but we sweep over different choices of $\\ell_{\\text{align}}$.\n\n**Misc**\n- [039] Suggestion: Consider adding the reference \"Unified Embedding: Battle-tested feature representations for web-scale ML systems\" [Coleman et al., NeurIPS 2023]\n- [055] Typo: \"?\" --> missing reference\n- [421] Suggestion: move the legend from figure 5(a) to a shared legend for all three figures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VrbShel6Vj", "forum": "0OgcBivkr5", "replyto": "0OgcBivkr5", "signatures": ["ICLR.cc/2026/Conference/Submission17547/Reviewer_sSRG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17547/Reviewer_sSRG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835405751, "cdate": 1761835405751, "tmdate": 1762927418545, "mdate": 1762927418545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge that traditional and static embeddings in e-commerce cannot handle the rapid introduction of new products without costly retraining. The authors propose a framework called Lifelong-Learning Embeddings, which is designed to (1) incrementally add new tokens to the embedding table, (2) adapt the embedding dimensionality to balance performance and efficiency, and (3) use continual learning strategies to prevent ``catastrophic forgetting'' of previously learned information. Experiments on a real-world industrial dataset and two public benchmarks show that LLE outperforms static embeddings in accuracy, while only incurring a modest training-time overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper studies the incremental learning in recommender systems, which is an interesting investigation with practical deployment consideration.\n2. The experimental results demonstrate the effectiveness of the proposed methods over several baselines."}, "weaknesses": {"value": "1. The primary concern with this paper is the insufficient motivation. Given the proposed setting of weekly data updates, this already constitutes a relatively low-frequency batch update strategy that is entirely feasible in industrial practice. More critically, the experimental results reveal a non-trivial gap between the proposed method and the upper bound. For AUC as the evaluation metric (the paper does not clearly specify whether this is a CTR prediction task), AUC is typically a ranking metric that tends to yield relatively high scores. Therefore, while the numerical differences may appear modest, they actually represent significant performance degradation in real-world e-commerce scenarios. Additionally, the proportion of new products introduced each week is inherently low, making full model retraining actually manageable for e-commerce platforms, which are extremely eager to maximize commercial profits and are willing to sacrifice computational costs (linear computational overhead is typically acceptable in such scenarios). Consequently, this incremental learning approach seems unnecessary for weekly update frequencies. In practice, asynchronous embedding table updates would be entirely viable at this cadence. In addition, the proposed method might be more compelling and well-motivated for daily update scenarios.\n2. The paper presentation requires further polishing. Several issues need attention, including but not limited to: Lines 55, 181, and 212."}, "questions": {"value": "1. Have the authors considered the model's performance under daily update frequencies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zhw1XC45HG", "forum": "0OgcBivkr5", "replyto": "0OgcBivkr5", "signatures": ["ICLR.cc/2026/Conference/Submission17547/Reviewer_a5yD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17547/Reviewer_a5yD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011894758, "cdate": 1762011894758, "tmdate": 1762927417926, "mdate": 1762927417926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method for incrementally and continually updating embeddings in a dynamically changing e-commerce environment. The proposed approach consists of three modules. The first module maps tokens either to existing embeddings or to a new latent space. The second module handles changes in embedding dimensions by assigning values that include copies of the existing embeddings. The third module computes the final embedding values by aligning the semantics of overlapping tokens through contrastive learning. The authors evaluated their method on three datasets, including one private dataset, and achieved superior AUC performance compared to the baselines. Additionally, an ablation study was conducted to analyze the performance contributions of each component in the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors proposed a very simple and intuitive approach. The proposed method can operate independently of both the training scheme and the embedding size, and offline experiments demonstrated superior performance compared to the baselines. The ablation study is also well-formulated and clearly structured."}, "weaknesses": {"value": "The comparison criteria are too simplistic. For instance, in practical scenarios, the reason for using LLE might be that Baseline 2 in Section 5 is infeasible — likely due to the large amount of training data and high computational cost. If that’s the case, how would the results change if the training data were sampled to match LLE’s data size? It would also be meaningful to compare the proposed method with various cold-start techniques.\n\nThe embeddings for new tokens were initialized with either average or random values. Since product metadata or textual information were not utilized, this approach has limitations in addressing real-world cold-start problems.\n\nEmbedding quality was evaluated solely based on purchase prediction (AUC). The generalization ability of the model has not been validated through other downstream tasks such as recommendation, CTR prediction, or user similarity estimation."}, "questions": {"value": "When lifelong learning continues, contrastive learning (CL) is applied to ensure that overlapping tokens retain consistent semantics in their embeddings.\nHowever, one might question whether this approach limits potential improvement in embedding quality. Since CL primarily enforces consistency rather than optimization, embeddings for overlapping tokens may become resistant to beneficial updates that could better capture new contexts or evolving semantics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "47LrPNX9UA", "forum": "0OgcBivkr5", "replyto": "0OgcBivkr5", "signatures": ["ICLR.cc/2026/Conference/Submission17547/Reviewer_SFmT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17547/Reviewer_SFmT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762342313311, "cdate": 1762342313311, "tmdate": 1762927417445, "mdate": 1762927417445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}