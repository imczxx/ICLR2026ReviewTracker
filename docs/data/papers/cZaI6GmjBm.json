{"id": "cZaI6GmjBm", "number": 9855, "cdate": 1758144101012, "mdate": 1759897691563, "content": {"title": "DiaBlo: Diagonal Blocks Are Sufficient For Finetuning", "abstract": "Fine-tuning is a critical step for adapting large language models (LLMs) to domain-specific downstream tasks. To mitigate the substantial computational and memory costs of full-model fine-tuning, Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed to update only a small subset of model parameters. However, performance gaps between PEFT approaches and full-model fine-tuning still exist. In this work, we present *DiaBlo*, a simple yet effective PEFT approach that updates only the diagonal blocks of selected model weight matrices. Unlike Low-Rank Adaptation (LoRA) and its variants, DiaBlo eliminates the need for low-rank matrix products, thereby avoiding the reliance on auxiliary initialization schemes or customized optimization strategies to improve convergence. This design leads to stable and robust convergence while maintaining comparable memory efficiency and training speed to LoRA. Through extensive experiments across a range of tasks—including commonsense reasoning, arithmetic reasoning, code generation, and safety alignment—we show that fine-tuning only diagonal blocks is sufficient for strong and consistent performance. DiaBlo not only achieves competitive accuracy but also preserves high memory efficiency and fast fine-tuning speed.", "tldr": "Fine-tuning only the diagonal blocks of weights yields superior performance", "keywords": ["Finetuning", "Parameter-Efficient", "LLM", "Diagonal Block"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/26a952324fbcba943d67bf8c49af78885be4c5a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- The paper proposes a new PEFT method called DiaBlo, which fine-tunes the weights as $\\hat{W} = W + D$, where $W$ are the pretrained weights and $D$ are the residual fine-tuning weights as a block-diagonal matrix.\n- $D$ is initialized with all zeros\n- The method is evaluated on commonsense reasoning, arithmetic reasoning, code generation tasks and safety alignment tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The use of block diagonal matrix for the residual fine-tuning weights is a experimentally motivated.\n- The method is easy to implement and computationally efficient."}, "weaknesses": {"value": "1. The experimental comparisons on the quantization experiments are not correct. \n    - The papers experiments use a base model with a different quantization method than the baselines, and the baseline results are taken directly from Table 7 in [1].\n    - [1] proposes a new quantization method called AiQ, fine-tunes it using LoRA, and then compares it with other quantization methods fine-tuned using LoRA.\n    - However, the experiments in the paper initialize the quantized base model from MagR [2], fine-tunes using DiaBlo, and then compares it with results that use multiple different quantization methods fine-tuned using LoRA.\n    - To elaborate, baseline results in Table 4 (taken from Table 7 in [1]) are of the nature \"Quantization Method x/y/z + LoRA\", ment to compare quantization methods. However, DiaBlo results are of the form \"Quantization Method MagR + DiaBlo\". Hence, there is no way to tell if the performance difference is due to the quantization method or the fine-tuning method.\n    - Hence, the quantization method is a confounder that makes the comparisons invalid.\n2. The comparison with baselines in all the tables are taken from multiple sources. Even if the high level settings like precision and hypeparameters are matched, the results are not directly comparable as subtle details in the implementation can lead to different results. For a proper comparison, the baseline methods should be run with the same setup as the proposed method.\n    - For example, the baseline results in Table x are taken from 3 sources.\n    - When the the baseline methods and DiaBlo are evaluated on the same benchmark but with bf16 precision in Table 6, DiaBlo shows minor improvements over LoRA. As the baseline results are taken from a different work, the improvements could be due to a different training setup.\n3. Given that LoRA has had immense practical impact, the contribution of a new PEFT method does not have much impact without a significant advantage other than performance alone. In that sense, the contribution of the paper is limited.\n\n---\n\n## References\n[1] \"ApiQ: Finetuning of 2-Bit Quantized Large Language Model\", Liao et al., EMNLP 2024\n\n[2] \"MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization\", Zhang et al., NeurIPS 2024"}, "questions": {"value": "1. What is the GPU memory consumed by DiaBlo compared to LoRA/DoRA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EVXx51y4gM", "forum": "cZaI6GmjBm", "replyto": "cZaI6GmjBm", "signatures": ["ICLR.cc/2026/Conference/Submission9855/Reviewer_yEpk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9855/Reviewer_yEpk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833375900, "cdate": 1761833375900, "tmdate": 1762921326785, "mdate": 1762921326785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DiaBlo, a param. efficient fine-tuning (PEFT) method for LLMs using the diagonal blocks of weight matrices. One of the key novelties in this work is that DiaBlo does not use extra low-rank matrices multiplied together (like LoRA’s A x B structure) to adapt model weights. Instead, it directly updates selected diagonal blocks within the model’s existing weight matrices, which eliminates the need for initialization or custom optimization strategies. The work covers tasks such as commonsense reasoning, arithmetic reasoning, and code generation, showing that DiaBlo can match or outperform LoRA with comparable memory/efficiency results. Furthermore, the method shows robustness under quantized architectures (4/2-bit)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. By removing the complexity of low-rank structures, this work presents a clear alternative to LoRA-style PEFT. The results show that DiaBlo attains comparable performance without the added overhead of extra trainable matrices, simplifying both tuning and optimization.\n\n2. The evaluation spans diverse supervised fine-tuning tasks -- including code generation, arithmetic reasoning, and commonsense reasoning -- covering a balanced range of short to moderate sequence lengths.\n\n3. The results in Table 5 are particularly convincing, showing that random sparse update patterns (and SMT) underperform compared to DiaBlo. This supports the claim that the structured diagonal-block design is the key driver of its performance advantage."}, "weaknesses": {"value": "1. Most evaluated benchmarks involve short output sequences, except for code generation. Testing DiaBlo on tasks with longer input–output contexts would better demonstrate its scalability and performance stability under extended sequence conditions (see q1).\n\n2. The discussion of sparsity-based PEFT methods misses some recent relevant work, such as S2FT (NeurIPS 2025)[1] and SparseLoRA (ICML 2025)[2]. Including these would strengthen the discussion on sparsity in the introduction and would provide a better picture of the current limitations in state-of-the-art PEFT methods.\n\nReferences:\n\n[1] Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen, \"S2FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity\", NeurIPS 2025\n\n[2] Samir Khaki, Xiuyu Li, Junxian Guo, Ligeng Zhu, Chenfeng Xu, Konstantinos N. Plataniotis, Amir Yazdanbakhsh, Kurt Keutzer, Song Han, Zhijian Liu, \"SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity\", ICML 2025"}, "questions": {"value": "1. Most evaluated tasks focus on short output sequences, such as commonsense and reasoning benchmarks. Including long-form, multi-turn dialogue datasets like MT-Bench would better demonstrate DiaBlo’s scalability and effectiveness in extended conversational contexts.\n\n2. The Appendix shows that trainable modules (QKVO/GUD) are fixed across methods, which is reasonable, but it remains unclear whether DiaBlo’s robustness comes from having a larger set of modules trainable on all methods (for example, for LLaMA3-8B, all the QKVOGUD modules have some trainable parameters). An ablation over different subsets of trainable components would clarify if the observed gains persist under more constrained settings. For example, see Figure 4.0 in S2FT on adding trainable parameters to only a subset of modules, such as QK, or GUD, etc.\n\n3. Adding results on the GLUE benchmark would help assess general language understanding and show how well DiaBlo transfers to broader NLP tasks.\n\n4. Reporting zero-shot baseline performance would contextualize fine-tuning improvements. Clarifying whether the LLaMA3-8B variant used is “instruct” or base would also make the evaluation setup more transparent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dMDAQP03Zo", "forum": "cZaI6GmjBm", "replyto": "cZaI6GmjBm", "signatures": ["ICLR.cc/2026/Conference/Submission9855/Reviewer_hx7w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9855/Reviewer_hx7w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956124895, "cdate": 1761956124895, "tmdate": 1762921326458, "mdate": 1762921326458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a Parameter-Efficient Fine-Tuning method called DiaBlo that updates only the diagonal blocks of selected model weight matrices. The work argues that this method enables axis-aligned, per-dimension scaling that LoRA cannot capture and verifies the method on a large variety of benchmarks with Llama 7B/8B/13B models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed work eliminates the inherent optimization difficulties associated with low-rank decomposition by avoiding the use of matrix products.\n\n- DiaBlo demonstrates higher stability in 4-bit and 2-bit arithmetic reasoning tasks."}, "weaknesses": {"value": "- Compared to strong baselines like SMT with similar trainable parameter amount, the proposed method does not show significantly better performance. In other words, the paper argues the memory and computation efficiency of the proposed method, but the model doesn’t achieve significant improved performance compared to baselines when they share the same amount of trainable parameters.\n\n- In table 1, it shows DiaBlo N =128 doesn’t get better performance compared to DiaBlo N =64 although doubled trainable parameters. This raises concerns of the scaling ability of the proposed Diablo."}, "questions": {"value": "- The reviewer suggests to add Full Finetuning results in Table 1. \n\n- The paper mentions when N is not a common factor of m1,m2, it needs to expand and pad the weight into a proper size and then select the corresponding diagonal blocks. This can be common case. But the paper doesn’t touch this point clearly in later experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IEJIbZ9iIf", "forum": "cZaI6GmjBm", "replyto": "cZaI6GmjBm", "signatures": ["ICLR.cc/2026/Conference/Submission9855/Reviewer_qumS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9855/Reviewer_qumS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980348885, "cdate": 1761980348885, "tmdate": 1762921326197, "mdate": 1762921326197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new parameter-efficient finetuning method which targets trains a block-diagonal sparse matrix on top of linear layers in model weights. This approach seems to outperform various other PEFTs with around the same number of parameters on a variety of tasks, as well as when compared to other baselines which involve training sparse weight updates."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The idea is quite elegant, relatively simple to implement and efficient to train -- there isn't much adaptation required to existing finetuning libraries to get this working.\n- The results are broad (covering standard PEFT benchmarks) and thus convincing.\n- Ablations cover the first questions I had regarding whether block-diagonal is better than other ways of selecting entries to tune in the weight matrix; it does seem like it is broadly a better strategy than other ideas."}, "weaknesses": {"value": "- Since we use a standard suite of benchmarks to evaluate PEFTs, it's possible that our literature is engaging in test-set overfitting (compare how the ImageNet challenge or LMSYS arena were overfit by organisations submitting many models repeatedly). It would thus be nice to show how the technique performs under varying learning rates and block sizes (e.g. as done for LoRA in [Schulman et al. (2025)](https://thinkingmachines.ai/blog/lora/)). It is nice though that there are not as many hyperparameters as other PEFTs!"}, "questions": {"value": "- In your experience how difficult was it to hyperparameter tune this method?\n- What model components seem most important for good performance when finetuning? Is there a layer-wise effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ohOoInPxUd", "forum": "cZaI6GmjBm", "replyto": "cZaI6GmjBm", "signatures": ["ICLR.cc/2026/Conference/Submission9855/Reviewer_uDXN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9855/Reviewer_uDXN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762246796032, "cdate": 1762246796032, "tmdate": 1762921325869, "mdate": 1762921325869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}