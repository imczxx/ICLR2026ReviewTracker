{"id": "TRwQND3xpt", "number": 8764, "cdate": 1758097483159, "mdate": 1759897765077, "content": {"title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI", "abstract": "Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection.\nDesktop environments---particularly gaming---offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning.\nWe present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks.\nUnlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains.\nOur framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152× compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation.\nUsing 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6\\% success rate on LIBERO manipulation and 83.3\\% on CANVAS navigation benchmarks.\nThis validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics.\nWe will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models. (Demo available at [link](https://www.notion.so/D2E-Scaling-Vision-Action-Pretraining-on-Desktop-Data-for-Transfer-to-Embodied-AI-279e81a6e92380b4a672d19c924494eb?source=copy_link))", "tldr": "Desktop gaming data effectively pretrains embodied AI: 152× compression via OWA Toolkit, YouTube pseudo-labeling with Generalist-IDM, achieving 96.6% on LIBERO manipulation and 83.3% on CANVAS navigation with 1.3K hours of data.", "keywords": ["embodied ai", "vision-language-action models", "inverse dynamics models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2698f5191e306fcd78cf981b130218b948263ebb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes D2E (Desktop-to-Embodied AI), a framework that leverages large-scale desktop interaction data (screen, keyboard, mouse) as an alternative to expensive real-world embodied trajectories for pretraining vision-action models.\nThe system consists of three main components: (1) OWA Toolkit – a high-performance desktop data collection and compression pipeline based on an extended MCAP format (OWAMcap), achieving up to 152× compression over prior datasets. (2)Generalist-IDM – a timestamp-aware inverse dynamics model (NEP-τ) that predicts human actions from videos and is used to pseudo-label over 1K hours of YouTube gameplay. (3)VAPT – a vision-action pretrained model that transfers desktop-learned representations to robot manipulation (LIBERO) and navigation (CANVAS), achieving 96.6% and 83.3% success rates respectively."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Solid engineering contribution**: The OWA Toolkit is an impressive system-level effort that enables synchronized, multimodal desktop data collection and efficient storage. The compression performance and open-source reproducibility are highly commendable.\n\n2. **Reproducibility and openness**: The authors provide thorough implementation details, training settings, and datasets, which greatly enhance the paper’s credibility and community value."}, "weaknesses": {"value": "1. **Lack of academic novelty**:\nThe overall contribution is primarily engineering-oriented rather than conceptual or algorithmic. While the proposed OWA Toolkit and data infrastructure are impressive from a systems perspective, the work introduces limited new ideas in terms of representation learning, model design, or theoretical insight.\n\n2. **Generalist-IDM design appears incremental**:\nThe core methodological component—Generalist Inverse Dynamics Model (Generalist-IDM)—is not sufficiently novel or deeply motivated. The NEP-τ formulation is essentially an incremental extension of standard next-event prediction, and its role in improving downstream performance remains unclear. It is uncertain whether the strong results of VAPT stem from the NEP-τ design itself or simply from the high-quality and diverse data collected by OWA Toolkit.\n\n3. **Lack of ablation and sensitivity analysis**:\nThe paper provides limited empirical evidence dissecting the effectiveness of individual components. In particular, the temporal offset parameter τ is introduced as a key idea, yet there is no systematic study on how τ is selected or how sensitive the model performance is to its value. Without such analysis, the robustness and generality of the proposed modeling choice remain uncertain.\n\n4. **Missing causal connection between design and outcome**:\nAlthough the experiments show that a model trained on limited human desktop data and pseudo-labeled gameplay videos can generalize across unseen domains, the paper does not clearly explain why this happens. It lacks causal analysis connecting the proposed components (e.g., NEP-τ, OWA data quality, pseudo-labeling) to the observed generalization behavior. As a result, it is difficult to attribute the performance gains to specific methodological factors rather than data scale or diversity alone."}, "questions": {"value": "1. Could the authors provide a deeper analysis of how the temporal offset mechanism (NEP-τ) contributes to the observed improvements? In particular, how do results change if τ is removed or varied?\n\n2. To what extent do the downstream results of VAPT originate from the model design (NEP-τ, Generalist-IDM) versus the quality and diversity of the collected OWA data? Some controlled comparisons would help isolate these factors.\n\n3. The paper shows strong zero-shot transfer to unseen games and robotic tasks. What is the hypothesized mechanism behind this generalization? Are certain components (e.g., timestamp-based tokenization) more critical than others?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "78zER026o5", "forum": "TRwQND3xpt", "replyto": "TRwQND3xpt", "signatures": ["ICLR.cc/2026/Conference/Submission8764/Reviewer_rEUi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8764/Reviewer_rEUi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761301174411, "cdate": 1761301174411, "tmdate": 1762920546914, "mdate": 1762920546914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces D2E, a framework that leverages large-scale desktop interactions collected via the OWA toolkit and generalized through a timestamp-based IDM for pretraining embodied AI. Using more than 1.3K hours of human and pseudo-labeled gameplay data, the approach demonstrates strong transfer to robotics tasks, achieving 96.6% on LIBERO manipulation and 83.3% on CANVAS navigation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Using game data for embodied pretraining is an interesting direction.\n\n2. The paper provides a detailed system design for data collection."}, "weaknesses": {"value": "1. The main contribution of the paper lies in how to collect action data and in the system-level design, while the algorithmic innovation is relatively limited. Currently, there is a large body of work that uses OOD data for pretraining, so my concern is that this paper may not be a very good fit for a learning-focused conference.\n\n2. Using game data for navigation tasks in CANVAS makes sense, but its effectiveness for manipulation tasks remains questionable. In addition, LIBERO can be easily hacked with certain tricks, which makes it difficult to serve as a fair or reliable benchmark for validating algorithms."}, "questions": {"value": "Did the pretraining stage also include robotics data, or was it purely based on desktop game data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9B0tMAyuyZ", "forum": "TRwQND3xpt", "replyto": "TRwQND3xpt", "signatures": ["ICLR.cc/2026/Conference/Submission8764/Reviewer_SNK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8764/Reviewer_SNK1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720688915, "cdate": 1761720688915, "tmdate": 1762920546446, "mdate": 1762920546446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents D2E, a framework that scales vision-action pretraining using large-scale desktop interaction data (screen, keyboard, mouse) instead of costly physical robot trajectories. It introduces the OWA Toolkit for efficient multimodal data capture, an IDM for pseudo-labeling internet gameplay videos, and a VAPT that transfers learned representations to robotics. Trained on about 1.3K hours of human and pseudo-labeled data, the model achieves 96.6% success on libero and 83.3% on canvas navigation, showing that desktop interactions can effectively serve as scalable pretraining for embodied AI."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. End-to-end pipeline and scale: The paper provides a complete framework covering data collection, pseudo-labeling, pretraining, and robotic transfer. It builds on 31 desktop games with 335 hours of human demonstrations and expands to over 1,000 hours of pseudo-labeled YouTube gameplay, achieving clear scalability.\n\n2. Solid engineering contribution: The OWA toolkit enables synchronized screen, keyboard, and mouse capture with nanosecond precision and 152× compression, significantly reducing random-access I/O cost and improving data-loading throughput during training.\n\n3. Openness and reproducibility: The authors commit to releasing the toolkit, datasets, and pretrained models with detailed documentation, ensuring transparency and easy reproducibility for future research."}, "weaknesses": {"value": "The experimental evidence supporting why and when desktop data benefits embodied tasks remains insufficient. While results show positive transfer, the paper lacks a deeper analysis of task suitability—for example, whether desktop-derived sensorimotor patterns genuinely align with the fine-grained control and contact dynamics required in manipulation tasks."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kXU6XMWRuV", "forum": "TRwQND3xpt", "replyto": "TRwQND3xpt", "signatures": ["ICLR.cc/2026/Conference/Submission8764/Reviewer_cyb3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8764/Reviewer_cyb3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898969297, "cdate": 1761898969297, "tmdate": 1762920546107, "mdate": 1762920546107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes D2E, a framework for pretraining models on desktop data (e.g., game videos paired with human actions). D2E offers a comprehensive toolbox that includes standardized data collection and storage, efficient data reading, and a scalable pseudo-label annotation strategy for desktop data, powered by a specifically designed and trained Inverse Dynamics Model (IDM). Extensive experiments are conducted to validate the framework across multiple aspects, including data collection, processing, and reading efficiency, as well as the effectiveness of using D2E for pretraining embodied models. The results show that adapting the pretrained models to embodied and navigation tasks leads to notable performance improvements, revealing a promising direction for leveraging scalable desktop data in embodied model pretraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Given the high cost of embodied data collection, exploring new data sources that capture human intention knowledge and can contribute to embodied learning is a highly meaningful research direction. While desktop data has been explored in previous works for training embodied models, those methods primarily focus on developing and validating models within in-domain game environments. This paper extends beyond that scope, aiming to make desktop data more broadly useful for embodied learning.\n\n2. The high-quality datasets collected and the well-designed data collection pipeline represent valuable contributions to the research community. They provide essential infrastructure for future work on large-scale embodied pretraining, facilitating both reproducibility and scalability.\n\n3. The experiments and analyses conducted on each component of the proposed framework are comprehensive. Beyond demonstrating the feasibility of pretraining on desktop data, the paper offers a practical reference and a solid foundation for developing future learning systems capable of utilizing any video-based data for embodied intelligence.\n\n4. The significant performance gains observed on downstream tasks following D2E pretraining clearly demonstrate the framework’s effectiveness in extracting generalizable knowledge from desktop data, highlighting its potential as a scalable and efficient pretraining paradigm."}, "weaknesses": {"value": "1. The utilization of data with machine-generated pseudo labels leads to inconsistent performance changes across the Manipulation and Navigation benchmarks. This inconsistency suggests that the quality and reliability of pseudo labels may vary significantly depending on the task type, and further analysis is needed to understand their impact on downstream performance.\n\n2. Conducting experiments solely on the Libero benchmark is insufficient to support the claim that pretraining on desktop data contributes to learning generalized embodied knowledge. Validation on real-world manipulation tasks would substantially strengthen the paper’s argument. \n\n## Minor Issue\n\n1. The font style used in the paper is inconsistent with the official ICLR template, and should be adjusted to comply with the formatting standards."}, "questions": {"value": "1. Could the authors provide more details about the baseline implementations reported in Table 1? It would be helpful to clarify whether these baselines were reimplemented under the same settings or adopted from existing works.\n\n2. There remains a considerable gap between game data and embodied manipulation data due to the coarse-grained differences in action spaces and the varying requirements for 3D physical understanding. A more comprehensive analysis is needed to quantify how much embodied manipulation tasks actually benefit from desktop data pretraining, and under what conditions such transfer is most effective."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "There are several potential concerns regarding responsible research practices during human data collection and the legal compliance of using publicly sourced data. While the authors have provided an ethics statement in the Appendix, which addresses some of these issues, additional clarification may still be needed. In particular, the data privacy of the collected game videos should not be overlooked, as many gameplay recordings may inadvertently contain personal information, such as usernames, chat histories, or identifiable content. A clearer explanation of how such sensitive information is detected, filtered, or anonymized would strengthen the paper’s ethical rigor."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lDlrx5YaRw", "forum": "TRwQND3xpt", "replyto": "TRwQND3xpt", "signatures": ["ICLR.cc/2026/Conference/Submission8764/Reviewer_5KYr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8764/Reviewer_5KYr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914064303, "cdate": 1761914064303, "tmdate": 1762920545461, "mdate": 1762920545461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}