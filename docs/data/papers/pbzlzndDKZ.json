{"id": "pbzlzndDKZ", "number": 4986, "cdate": 1757827960794, "mdate": 1759898001549, "content": {"title": "SyMerge: From Non-Interference to Synergistic Merging via Single-Layer Adaptation", "abstract": "Model merging offers an efficient alternative to multi-task learning by combining independently fine-tuned models, but most prior approaches focus mainly on avoiding task interference. We argue instead that the real potential of merging lies in achieving synergy, where tasks enhance one another. Our intuition comes from a pilot study showing that when a classifier trained on one task is paired with the encoder of another, the resulting cross-task performance strongly predicts merge quality. Moreover, adapting even a single task-specific layer can substantially improve this compatibility, suggesting a simple yet powerful lever for synergy. Building on this insight, we introduce SyMerge, a lightweight framework that jointly optimizes one task-specific layer and merges coefficients. To ensure stability without labels, SyMerge employs a robust self-labeling strategy guided by expert model predictions, avoiding the pitfalls of entropy-based adaptation. This minimalist yet principled design achieves state-of-the-art results across vision, dense prediction, and NLP benchmarks, while also producing adapted layers that transfer effectively to other merging methods.", "tldr": "", "keywords": ["Model merging", "Multi-task learning", "Task conflicts", "Task vectors", "Task-specific layers"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58905e777f48011b3187abbd4e151f9e9c346f3e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors' motivation stems from an empirical finding that a model's cross-task generalization capability is a strong predictor of its merging performance.\n\nSyMerge is a lightweight, test-time adaptation method that works with unlabeled data. It jointly optimizes two components: the merging coefficients for the shared encoder and a single task-specific layer (e.g., the classifier). To provide a stable training signal without ground-truth labels, it employs a robust self-labeling strategy, using the predictions from the individual fine-tuned \"expert\" models as targets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clarity: The motivation is clearly laid out in Section 2.2, with intuitive figures that build a strong case for the authors' approach. The methodology is described precisely, and the connection between the motivating pilot study and the final SyMerge design is logical and clear.\n\nSignificance: Model merging is an efficient and increasingly important alternative to full multi-task training. By providing a method that is not only effective but also lightweight and scalable, the paper offers a practical solution to a relevant problem. SyMerge consistently outperforms other methods, often by a large margin, especially as the number of tasks increases (Table 1). Furthermore, the insight that the adapted layers are transferable (Table 4) opens up interesting possibilities for improving existing model merging techniques."}, "weaknesses": {"value": "SyMerge's approach involves jointly training the merging coefficients and a task-specific layer. However, for the comparison to be entirely fair, it is crucial to understand if the baseline methods are afforded a similar adaptation step. A significant portion of its performance improvement could be attributed to this classifier fine-tuning, rather than purely to the superiority of the merged encoder's representations.\n\nThe theoretical justification in Section 3.2 hinges on the assumption of \"cross-task linearity\". While this is a reasonable starting point for analysis, it is a strong assumption that may not fully capture the complex, non-linear interactions within deep neural networks. The paper would be strengthened if the authors could include a brief discussion on the limitations of this assumption or provide some empirical validation suggesting it holds approximately in their experimental settings.\n\nConfidence-based filtering mechanism seems to be used for the vision tasks. A small ablation in the main paper showing the performance impact of this filtering mechanism would be helpful to clarify whether it is a minor tweak or a critical component for achieving the reported results."}, "questions": {"value": "In lines 144-149, you define two performance metrics: (1) \"cross-task performance\" (Encoder A + Classifier B) and (2) \"merging performance\" (merged A&B encoder + Classifier B). Could you clarify why the latter is considered \"merging performance\" when it's evaluated only on Task B? Is it simply an average over all possible B's for a given set of models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rgtW1h26Bv", "forum": "pbzlzndDKZ", "replyto": "pbzlzndDKZ", "signatures": ["ICLR.cc/2026/Conference/Submission4986/Reviewer_KP85"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4986/Reviewer_KP85"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760704694067, "cdate": 1760704694067, "tmdate": 1762917808843, "mdate": 1762917808843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose SyMerge, a lightweight model merging framework that jointly optimizes task-specific layers and merging coefficients at test time, shifting the goal from \"avoiding interference\" to actively seeking synergy between tasks. The core idea stems from a set of pilot experiments: cross-task performance of encoders from different tasks strongly predicts merging quality; and fine-tuning only a single task layer significantly improves this compatibility. To ensure stable unsupervised adaptation, the authors abandon unstable entropy minimization and instead use self-labeling guided by predictions from various experts. The paper demonstrates strong quantitative and ablation results on multiple benchmarks in vision, dense prediction, and NLP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper offers a novel objective and perspective, shifting from non-interference to synergy, providing a clear conceptual framework and explaining why some merging methods have upper bounds.\n2. The method is concise and efficient—it jointly optimizes a single layer and coefficients only during testing, without introducing additional large models or modules, resulting in low engineering implementation costs.\n3. The writing is quite easy to read and it was well-written"}, "weaknesses": {"value": "1. The method essentially uses the predictions of individual models as supervision signals. When some experts are systematically inaccurate in the target domain, spurious labels may guide the merged model towards incorrect solutions. This point is mentioned in the limitation section of the paper, but it lacks quantitative sensitivity analysis for low-quality experts.\n2. Although the paper provides default learning rate, iteration count, and initialization, it lacks hyperparameter sensitivity curves and computational costs for different task numbers or model scales. This will affect the usability evaluation of the method in real-world large-scale scenarios.\n3. The theoretical conditions are relatively strong; Proposition 1 is based on the assumptions of \"cross-task linearity\" and convex output loss. However, the nonlinearity of actual depth models may render these assumptions incomplete. The paper provides a proof, but lacks empirical testing of the approximation of these assumptions on the used benchmark.\n4. The method requires a small amount of unlabeled target domain data. However, target domain data is difficult to obtain for many tasks."}, "questions": {"value": "1. The method requires a small amount of unlabeled target domain data. However, target domain data is difficult to obtain for many tasks. How sensitive is the method to this target domain data? Can it be extended to scenarios with no data or very little data?\n2. How computational resources and efficiency does the method offer for every stage?\n3. Is the improvement in the method due to the synergy brought about by merging or the result of additional data alignment? If we don't use task vectors and only use the base model, can we also improve the model performance through this additional target domain data? Introducing target domain data is unfair to comparing with other methods; can other methods also achieve better results by introducing this target domain data? The authors need to conduct further research.\n4. How does the method perform on larger models? How can target domain data be used for collaboration on some NLP generation tasks?\n\nIf the author can solve the question and the weakness well, i will raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WXNqxECnO7", "forum": "pbzlzndDKZ", "replyto": "pbzlzndDKZ", "signatures": ["ICLR.cc/2026/Conference/Submission4986/Reviewer_m1Pr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4986/Reviewer_m1Pr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736220776, "cdate": 1761736220776, "tmdate": 1762917808383, "mdate": 1762917808383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SyMerge, a model merging framework that pursues not just the avoidance of task interference but active task synergy when merging independently fine-tuned models. The method achieves this by jointly adapting a single task-specific layer together with merging coefficients at test time, guided by expert model self-labels, rather than unstable entropy minimization. The authors provide both theoretical and empirical evidence for why enhancing cross-task compatibility is key for successful model merging and demonstrate SyMerge’s effectiveness across vision, dense prediction, and NLP tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work advances a shift in objectives for model merging—arguing for positive synergy rather than mere non-interference. This reconceptualization is original in the landscape of model merging.\n2. Theoretical justification is provided showing that improved cross-task performance tightens loss bounds for merged models, supporting the focus on functional alignment.\n3.  SyMerge outperforms a strong suite of prior model merging baselines in multi-task classification, dense prediction, and NLP, with results approaching those of individually fine-tuned models."}, "weaknesses": {"value": "1. While the pursuit of task synergy is motivated well, the core adaptation step (jointly tuning a single layer and coefficients with self-labeling) is a fairly incremental extension over test-time adaptive methods such as AdaMerging. The framework design—minimizing cross-entropy or L1 to match expert predictions—can be considered a straightforward application of self-labeling in existing frameworks (Representation Surgery and WUDI-Merging).\n2. The proposed method’s reliance on the predictions from the individual expert models as supervision means that improvements are ultimately bounded by the expert’s limitations.\n3. The theoretical analysis relies on known assumptions such as cross-task linearity and convexity, but practical models do not strictly satisfy these. The proof glosses over how close “approximate” linearity is achieved in practice, and no bounds or ablations connect the assumption to observed efficacy."}, "questions": {"value": "1. Can the authors provide empirical analysis on the impact of the cross-task linearity assumption underpinning Proposition 1? Are there cases where nonlinear interactions or loss non-convexity cause SyMerge to underperform?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eYCzMatUD1", "forum": "pbzlzndDKZ", "replyto": "pbzlzndDKZ", "signatures": ["ICLR.cc/2026/Conference/Submission4986/Reviewer_3JoF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4986/Reviewer_3JoF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900439544, "cdate": 1761900439544, "tmdate": 1762917807723, "mdate": 1762917807723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a test-time adaptive model merging method, SyMerge. The key idea is to fine-tune the merging coefficients and task-specific weights through distillation between the merged model and all task experts on an unlabeled test set. Extensive experiments across vision, dense prediction, and NLP benchmarks demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work uncovers an interesting phenomenon that stronger cross-task performance leads to better merging performance, and provides theoretical support for this observation.\n\n2. The paper is well-organized and easy to follow.\n\n3. The experiments demonstrate that the proposed method achieves promising results."}, "weaknesses": {"value": "1. **Robustness to the size and quality of the unlabeled test set.** The proposed method relies on using an unlabeled test set to perform distillation between the merged model and all task experts. However, it is unclear how well this approach would work in more practical scenarios such as few-shot, long-tail, noisy, or OOD test sets. These settings naturally arise in real-world applications where users may input any query data.\n\n2. **Potentially misleading distillation.** Each task expert is distilled on the entire test set, including samples from other tasks. For instance, when the task expert for Task A is distilled on data from Task B, its outputs may be meaningless or even incorrect. Such cross-task distillation could mislead the merged model and result in the learning of spurious or erroneous knowledge.\n\n3. **Unclear connection between the proposed method and the core motivation (i.e., enhancing cross-task alignment).** Although the paper empirically and theoretically shows that merging performance is correlated with cross-task alignment, it remains unclear why the proposed method improves such alignment, thereby enhancing merge performance. From my perspective, the core formulation (i.e., the cross-entropy loss function) merely enforces the merged model to match the performance of each task expert, thus improving task-specific accuracy rather than directly addressing cross-task alignment. I would encourage the authors to provide more formal technical insights explaining how the proposed method and its formulation support this motivation.\n\n4. **Limited technical novelty.** The core formulation, specifically the distillation loss, is well studied in model merging and other areas of machine learning. The self-labeling strategy appears to be a standard knowledge distillation procedure that matches the outputs between teacher and student models."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4nXs1sJpyE", "forum": "pbzlzndDKZ", "replyto": "pbzlzndDKZ", "signatures": ["ICLR.cc/2026/Conference/Submission4986/Reviewer_z7KY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4986/Reviewer_z7KY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930649195, "cdate": 1761930649195, "tmdate": 1762917807480, "mdate": 1762917807480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for model merging, where one layer is adapted on top of doing the merge. The layer is adapted in a test-time adaptation fashion with the goal to exploit the synergies across tasks. To train the layer and find the merging coefficients they use the cross entropy loss between the output of the merged model and the corresponding expert model (single task finetuned one)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The test of concept experiments add empirical value on the choices made on the framework\n- There is a section that covers the choice of the objective function, which not only justify the selected loss but shows there was a careful experimental design process\n- The experiment cover different models, showing the method works across a range of common model choices."}, "weaknesses": {"value": "I only have minor comments, some of the figures and tables that occupy half page on pages 8 and 9 could be arranged so they do not cut the text so much like in the current version."}, "questions": {"value": "The exploration of synergy in tasks is very interesting, however, have you consider what happens when the task being added does not play nice with the others? how well the method could minimize this interference and still get an decently performing model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jEBut7QOT8", "forum": "pbzlzndDKZ", "replyto": "pbzlzndDKZ", "signatures": ["ICLR.cc/2026/Conference/Submission4986/Reviewer_BQ5a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4986/Reviewer_BQ5a"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762268742621, "cdate": 1762268742621, "tmdate": 1762917807084, "mdate": 1762917807084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}