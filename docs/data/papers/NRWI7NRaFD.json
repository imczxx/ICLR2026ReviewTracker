{"id": "NRWI7NRaFD", "number": 22696, "cdate": 1758334544010, "mdate": 1759896851859, "content": {"title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters", "abstract": "We address the task of video style transfer with diffusion models, where the goal is to preserve the context of an input video while rendering it in a target style specified by a text prompt. A major challenge is the lack of paired video data for supervision. We propose PickStyle, a video-to-video style transfer framework that augments pretrained video diffusion backbones with style adapters and benefits from paired still image data with source–style correspondences for training. PickStyle inserts low-rank adapters into the self-attention layers of conditioning modules, enabling efficient specialization for motion–style transfer while maintaining strong alignment between video content and style. To bridge the gap between static image supervision and dynamic video, we construct synthetic training clips from paired images by applying shared augmentations that simulate camera motion, ensuring temporal priors are preserved. In addition, we introduce Context–Style Classifier-Free Guidance (CS–CFG), a novel factorization of classifier-free guidance into independent text (style) and video (context) directions. CS–CFG ensures that context is preserved in generated video while the style is effectively transferred. Experiments across benchmarks show that our approach achieves temporally coherent, style-faithful, and content-preserving video translations, outperforming existing baselines both qualitatively and quantitatively.", "tldr": "", "keywords": ["Video style transfer", "video diffusion model", "video-to-video translation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2cc7a7aa6a74ac7bfc3d3a4c90f38bc278e90859.pdf", "supplementary_material": "/attachment/162b27ba572c9a4a02c33c945a7f82d34af98557.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces PICKSTYLE, a video style transfer framework that efficiently adapts pre-trained video diffusion models via low-rank style adapters and a novel guidance strategy, achieving superior results by leveraging paired image data and simulating temporal coherence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The Core Innovations are as follows:\n\n1. Efficient Low-Rank Adaptation: The integration of specialized style adapters into self-attention layers enables effective motion-style transfer while maintaining computational efficiency and strong content-style alignment.\n\n2. Bridging Image-Video Domain Gap: A novel synthetic clip construction strategy using shared augmentations that simulate camera motion, effectively leveraging static image supervision for dynamic video stylization.\n\n3. Factorized Classifier-Free Guidance: The proposed Context-Style Classifier-Free Guidance (CS-CFG) innovatively disentangles style and context control, ensuring precise style application without compromising video content integrity."}, "weaknesses": {"value": "1. The proposed 2D motion simulation may lack generalizability for complex real-world videos involving significant 3D perspective changes. The current experiments, as shown in Figure 7, are primarily validated on relatively simple motions, leaving its performance on more complex scenarios unverified.\n\n2. The decision to keep the text-video cross-attention module frozen is a potential limitation. If the base model was not exposed to certain style descriptions during its pre-training, the framework might struggle to establish a correct correspondence between novel textual style prompts and their visual manifestations."}, "questions": {"value": "Could you clarify the design choice for C_null? Specifically, why was shuffling frame orders adopted instead of using a null context (empty set)? Furthermore, what is the rationale behind the specific form of the context direction, why don't use this formulation as the context direction: ϵ_cond - ϵ_theta(x_t, t; T, C_null), and were other alternative formulations explored or ablated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "18LvYkbOsh", "forum": "NRWI7NRaFD", "replyto": "NRWI7NRaFD", "signatures": ["ICLR.cc/2026/Conference/Submission22696/Reviewer_zJFo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22696/Reviewer_zJFo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640681084, "cdate": 1761640681084, "tmdate": 1762942341235, "mdate": 1762942341235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PickStyle, a video style transfer framework using diffusion models to render videos in a text-specified style while preserving content, overcoming the lack of paired video data. It achieves this by augmenting video diffusion backbones with low-rank style adapters and training on synthetic video clips created from paired images using simulated camera motion. Context–Style Classifier-Free Guidance (CS–CFG) is also introduced, which independently guides the style (text) and context (video) directions, resulting in superior, temporally coherent, and content-preserving style translations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents end-to-end, feedforward video style transfer network.\nThe dataset construction strategy would be useful to the research community."}, "weaknesses": {"value": "- All baseline methods are built on far inferior backbones (some are based on t2i backbone) and it’s not a surprise that PickStyle-based on VACE (WAN) beats the selected baselines. More recent baselines or any methods thats applied on the same WAN backbone is needed.\n- The base model, VACE inherently cannot perform video style transfer? If so, can we see how PickStyle is improved compared to the original VACE backbone?\n- Compared to the normal CFG, how much more computation overhead does CS-CFG incur in terms of both memory and time?\n- My biggest concern is the lack of technical contribution. The adapter module is ControlNet-style network, also frequently adapted and used in recent DiT-based generation methods. The CFG with additional condition term (triangular CFG) is not new either. For example, SV3D or VideoJAM uses similar approaches."}, "questions": {"value": "Please see the weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3eEvS5Ewt4", "forum": "NRWI7NRaFD", "replyto": "NRWI7NRaFD", "signatures": ["ICLR.cc/2026/Conference/Submission22696/Reviewer_PThQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22696/Reviewer_PThQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943482984, "cdate": 1761943482984, "tmdate": 1762942340945, "mdate": 1762942340945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PickStyle introduces a video-to-video style transfer framework that preserves motion and context while rendering stylized frames from one of nine trained styles. The method’s primary innovation seems to be a context-style classifier-free guidance mechanism, allowing explicit control over content and style conditioning during diffusion. Additionally, a tunable noise initialization strategy enables improved temporal coherence and perceptual fidelity. The paper includes reasonable experiments demonstrate improvements over prior methods in both qualitative and quantitative metrics, including a standard battery of metrics across content, video quality, etc."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1 The paper builds on the Wan2.1 generation backbone to include style adaptation and seems to be able capture nine different styles in a way that generalizes from image-pairs to video.  The paper makes reasonable technical innovations to accomplish this that seem original and relevant, such as CS-CFG which creates a tunable trade-off between fidelty and stylization (although this trade-off seems not analyzed in the paper).\n\nS2 The paper includes a motion augmentation strategy that enables the use of image pairing as training data.\n\nS3 Results seem compelling and meaningful analyses are included.  It seems clear that PickStyle is best at being able to match the style prompt, at least according R Precision score (although it is curious that this score only uses one frame from the video).  It also seems that the video quality aspects are strong."}, "weaknesses": {"value": "W1 The key aspects of the proposed method seem to the LoRA adapters that modulate the attention to capture style, the CS-CFG method and the noise initialization.  Yet, none of these are really thoroughly analyzed in any way.  For example although Fig. 8 captures one example of where CS-CFG helps, the interplay between $t_\\text{guide}$ and $c_\\text{guide}$ is not studied.  Similarly, we have no evidence about to what degree the context-based initialized is necessary.  Hence it is impossible to actually assess whether the technical innovations align with the observed results improvements, or if it is from other sources (e.g., the different data, the augmentation approach, etc.)\n\nW2 It is not clear whether the comparisons are fair.  Considering this paper creates a composite dataset with nine styles, have the other methods to which the paper compares been retrained on this dataset?  The paper does not sufficiently describe this critical point.\n\nW3 The approach to augment the paired image samples with some motion to generating pair training videos is not well described in the text and therefor hard to analyze.  It would seem, for example, that the types of augmentations used are not able to capture realistic motions in video resulting from 3D content and perspective effects.  This implies that perhaps the datasets used and results shown, however compelling they may be, may not be indicative of utility on more general video. \n\nW4 It seems that PickStyle is the most computationally expensive of the methods evaluated.\n\n\nMinor things\n- The manner in which the references are typically cited, e.g., \"VACE Jiang et al. (2025)\" is not proper, at least not for this style of including the author name.  These should be in parenthesis or better incorporated directly into the text.  VACE by Jiang et al. (2025) or VACE (Jiang et al. 2025)."}, "questions": {"value": "Q1 What would happen if multiple style prompts were given as input?  What would happen if an out of set style prompt were given?\n\nQ2 What are the limitations of applying this to video?  Is there any reason to expect degradation for longer videos, for example?\n\nQ3 Is the dataset created here publicly available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CcQNFcKIdi", "forum": "NRWI7NRaFD", "replyto": "NRWI7NRaFD", "signatures": ["ICLR.cc/2026/Conference/Submission22696/Reviewer_eovg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22696/Reviewer_eovg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009879946, "cdate": 1762009879946, "tmdate": 1762942340700, "mdate": 1762942340700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PICKSTYLE, a video-to-video style transfer framework leveraging diffusion models with context-style adapters. The method aims to preserve motion and context while translating videos into diverse styles, using paired still image data and synthetic motion augmentation for training. A novel Context–Style Classifier-Free Guidance (CS–CFG) mechanism is proposed to independently control style and context during generation. The approach is evaluated against several baselines, showing improvements in temporal coherence, style fidelity, and perceptual quality across multiple metrics and styles."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1) The qualitative and quantitative results demonstrate that PICKSTYLE achieves superior style transfer, temporal stability, and perceptual quality compared to existing baselines. \n2) The paper provides extensive quantitative and qualitative comparisons with multiple baselines, covering a wide range of styles and metrics."}, "weaknesses": {"value": "1) The paper is difficult to follow in several key sections. The training procedure, especially how style and content consistency are achieved, is not clearly explained. The technical details of the model architecture and training pipeline are scattered and could benefit from a more structured presentation. \n2) The manuscript does not sufficiently highlight the core technical differences that make PICKSTYLE outperform baseline methods. The related work section is shallow, mostly listing existing approaches without deep analysis or positioning of the proposed method’s unique contributions. \n3) The training dataset is selectively curated, focusing on a limited set of styles (e.g., Anime, Pixar, Clay, LEGO, etc.) and synthetic Unity3D renderings. There is little discussion or evidence regarding the model’s ability to generalize to styles not covered in the training data, raising concerns about robustness and applicability.\n4) It is unclear whether baseline methods were trained or fine-tuned on the same dataset as PICKSTYLE. Without this information, the fairness of the comparisons and the claimed superiority of the proposed method are questionable."}, "questions": {"value": "Refer to the weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gWR3KciB7y", "forum": "NRWI7NRaFD", "replyto": "NRWI7NRaFD", "signatures": ["ICLR.cc/2026/Conference/Submission22696/Reviewer_tRTK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22696/Reviewer_tRTK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034982613, "cdate": 1762034982613, "tmdate": 1762942340490, "mdate": 1762942340490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}