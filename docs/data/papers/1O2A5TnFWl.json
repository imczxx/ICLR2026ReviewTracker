{"id": "1O2A5TnFWl", "number": 18630, "cdate": 1758289624768, "mdate": 1759897090845, "content": {"title": "Filter before Plug: One-for-All Framework for Covariate-Aware Forecasting with Time Series Foundation Models", "abstract": "Time series forecasting plays a critical role in numerous real-world applications. Recent advances in Time Series Foundation Models (TSFMs) have achieved strong performance by modeling historical dependencies; however, they frequently neglect the impact of exogenous covariates. Existing methods either train from scratch, losing the advantages of TSFMs, or design plugin modules that are tightly coupled with specific architectures. To address these limitations, we propose FLUG, a One-for-All framework where independently trained modules complement TSFMs. We design an Endogenous Series Filter (EFit) module guided by the Hurst Exponent to separate exogenous components from the time series, thereby enabling TSFMs to focus on modeling and forecasting endogenous patterns. In parallel, we introduce a Covariate Plugin (CPin) module that employs Multi-Scale Patchify fusion and a Causal-Aware Masking strategy based on Gradient Reversal Layer to capture the exogenous information of the target variable. By decomposing endogenous and exogenous dependencies, FLUG enables integration of covariate information across a variety of TSFMs. To supplement existing publicly available covariate time series data, we curate and release four additional datasets. Extensive experiments on real-world business and supplementary data demonstrate the framework’s effectiveness and scalability.", "tldr": "We propose a  a One-for-All framework where independently trained modules complement TSFMs.", "keywords": ["Time series forecasting"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bb2d999fa402739d91e20ec08b7446c4ca1f59cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes FLUG, a \"One-for-All\" framework designed to enhance Time Series Foundation Models (TSFMs) by incorporating exogenous covariate information in forecasting tasks. FLUG encompasses two independently trained modules: (1) an Endogenous Series Filter (EFit), guided by the Hurst exponent, that extracts the endogenous (self-dependent) components of a time series; and (2) a Covariate Plugin (CPin) using multi-scale patchification and causal-aware masking to capture exogenous influences from covariates. The paper demonstrates how these modules can be plugged into any TSFM without requiring end-to-end fine-tuning. Evaluation across standard and newly curated datasets suggests the modular approach consistently improves forecasting performance, especially in data-rich power generation environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe independently trainable Filter and Plugin modules for TSFMs present a flexible approach to incorporating covariate information. This “One-for-All” design improves applicability over tightly coupled plugin approaches and end-to-end architectures.\n2.\tThe CPin module includes a Gradient Reversal Layer for dynamic, data-adaptive causal masking, a nontrivial step beyond simple correlation-based selection. The effectiveness of this element is demonstrated empirically and visually."}, "weaknesses": {"value": "1.\tThe paper repeatedly asserts that end-to-end integration of covariates undermines the intrinsic forecasting capability of pretrained TSFMs. However, no theoretical justification or controlled ablation supports this claim, leaving the argument largely anecdotal.\n2.\tThe notion of “future-known” covariates is central to the proposed CPin module, yet its operational definition is unclear. For different datasets, the boundary between exogenous variables that are truly known ahead of time versus those only predictable via auxiliary models remains ambiguous.\n3.\tThe experiments heavily rely on pretrained TSFMs such as TimesFM and Chronos, but the paper omits crucial details on how these models were obtained, what datasets they were trained on, and whether they were retrained or adapted. This lack of transparency hinders fair comparison and reproducibility.\n4.\tFLUG is only evaluated when coupled with pretrained TSFMs. It remains unclear whether the proposed “filter-before-plug” paradigm is compatible with or beneficial to fully end-to-end forecasting models such as Timer-XL, TimerXer, PatchTST, or iTransformer.\n5.\tThe paper states that large-scale industrial datasets from Ningxia and Jilin are “processed and released,” but no download links, licenses, preprocessing pipelines, or privacy-handling protocols are provided. This lack of disclosure raises concerns about data accessibility and reproducibility.\n6.\tThe additional EFit and CPin modules likely increase computational cost, yet the paper provides no quantitative analysis (e.g., FLOPs, inference latency, or parameter counts). It remains unclear whether the plugin overhead could outweigh performance gains, especially on smaller datasets or real-time applications.\n7.\tAlthough the paper claims that FLUG is a “One-for-All” paradigm, most datasets and case studies focus on energy forecasting. The absence of evidence in heterogeneous domains such as finance, healthcare, or commodity markets limits the generalizability of the claimed framework."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gEENJwcfLB", "forum": "1O2A5TnFWl", "replyto": "1O2A5TnFWl", "signatures": ["ICLR.cc/2026/Conference/Submission18630/Reviewer_SeXV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18630/Reviewer_SeXV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799014309, "cdate": 1761799014309, "tmdate": 1762928341416, "mdate": 1762928341416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FLUG, a One-for-All framework for incorporating exogenous covariates into Time Series Foundation Models (TSFMs). It introduces two independently trained modules: EFit, which uses the Hurst Exponent to extract endogenous dependencies, and CPin, which applies multi-scale fusion and causal-aware masking to capture exogenous effects. By disentangling endogenous and exogenous components, FLUG can adapt to various TSFMs without fine-tuning. Experiments show that FLUG consistently improves forecasting performance, demonstrating its versatility and effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed One-for-All paradigm—where covariate and endogenous modules are trained independently of any foundation model—is conceptually elegant. It enables true plug-and-play adaptation across diverse TSFMs, significantly enhancing flexibility and reusability without the need for costly retraining or architectural coupling.\n\n2. The release of large-scale photovoltaic and wind power datasets containing 1.3 billion samples with rich, well-documented covariate structures represents a valuable contribution to the time-series forecasting community. These datasets not only provide a realistic benchmark for covariate-aware forecasting but also offer an important resource for advancing research on scalable and generalizable TSFMs in real-world industrial settings."}, "weaknesses": {"value": "1. The EFit and CPin modules are trained independently and later integrated into frozen TSFMs for inference. However, the paper does not clearly demonstrate how well these modules generalize when the TSFM representations differ from those seen during their own training. In practice, the assumption of full independence between the modules and the foundation model may not hold, potentially limiting robustness and adaptability across architectures.\n\n2. Although ChronosX is briefly discussed, the paper lacks direct quantitative or ablation comparisons with other plugin-based adaptation methods under equivalent training budgets or data settings. Such comparisons would be essential to fairly evaluate the claimed “One-for-All” advantage of FLUG over existing plugin paradigms.\n\n3. The paper provides no clear analysis of the computational or latency overhead introduced by integrating both EFit and CPin into existing TSFM pipelines. A discussion of inference speed, additional memory usage would help assess the framework’s practicality in real-world forecasting systems.\n\n4. The writing quality, particularly in the methodology section, is notably poor. The descriptions often omit key details such as module input–output specifications, mathematical formulations, tensor dimensions, and intermediate representations. As a result, the method is unnecessarily difficult to follow, making it challenging for readers to fully reproduce or verify the proposed approach."}, "questions": {"value": "1. Covariates from other modalities, such as images or text, can also have a significant impact on time series prediction. Can the method proposed in this paper generalize to these types of data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wrrb86YGjF", "forum": "1O2A5TnFWl", "replyto": "1O2A5TnFWl", "signatures": ["ICLR.cc/2026/Conference/Submission18630/Reviewer_82Hr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18630/Reviewer_82Hr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805609602, "cdate": 1761805609602, "tmdate": 1762928341074, "mdate": 1762928341074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FLUG, a One-for-All framework designed to integrate exogenous covariates into Time Series Foundation Models (TSFMs) without retraining or fine-tuning foundation models. It separates time series into endogenous and exogenous components via two independently trained modules, EFit for long-term dependency extraction, and CPin for modeling causal, multi-scale covariate effects"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Innovative modular design**. FLUG introduces independently trainable filter and plugin modules that can be seamlessly attached to various pretrained TSFMs, establishing a general “one-for-all” adaptation paradigm.\n- **Theoretically grounded endogenous filtering.** The use of the Hurst Exponent and Fractional Brownian Motion-based loss provides a statistically meaningful way to isolate temporally dependent (endogenous) components from noisy or covariate-influenced signals.\n- **Extensive and rigorous evaluation.** Experiments cover both public benchmarks (e.g., GFC14, PDB) and newly released industrial datasets, with comprehensive ablations confirming the contributions of each module and loss component."}, "weaknesses": {"value": "1. **Questionable motivation of the Hurst exponent.** In line 213, the paper assumes that a Hurst exponent greater than 0.5 implies strong trends and periodicity. However, the interpretation of the Hurst exponent is conceptually inaccurate. As acknowledged by the authors in Appendix C.1, a Hurst value greater than 0.5 indicates increases are likely to be followed by further increases. This property is not related to periodicity at all. Therefore, using the Hurst exponent as an indicator to extract periodic components in the endogenous series lacks theoretical rigor, and given the importance of periodicity in time series, I have doubts about the representative learning ability of this method.\n\n2. **Lack of discussion on zero-shot forecasting capability.** One of the central motivations for TSFMs is their strong zero-shot generalization, yet the paper provides no analysis of whether FLUG preserves or diminishes this property. Since both the EFit and CPin modules introduce additional learned parameters trained on downstream datasets, a dedicated study or ablation on zero-shot transferability is crucial, especially when the paper mainly focuses on ''one for all''.\n\n3. **FBM-based generator may be overly idealized.** FBM assumes Gaussianity and stationary increments. Many real-world series violate these assumptions. The paper should justify why FBM is a valid anchor for endogenous structure and does not harm forecasting ability.\n\n4. **Inconsistent trend in multi-scale patchify analysis.** In Figure 4, model performance exhibits a clear trend of deterioration when extending the scales from the second scale ([16, 32]) to the fourth scale ([4, 8, 16, 32]). The paper does not discuss this anomaly. A more thorough explanation would strengthen the empirical analysis.\n\n5. **Unclear implementation details of mask computation.**\nThe description of the causal-aware masking mechanism lacks clarity, particularly regarding whether Pearson and Spearman coefficients are computed across the entire time series or within local temporal windows. Without a rigorous experimental protocol, it is difficult to assess whether the reported correlations truly reflect dynamic causal relationships between covariates and targets."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eK4f5QACWJ", "forum": "1O2A5TnFWl", "replyto": "1O2A5TnFWl", "signatures": ["ICLR.cc/2026/Conference/Submission18630/Reviewer_qMhU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18630/Reviewer_qMhU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889915970, "cdate": 1761889915970, "tmdate": 1762928340557, "mdate": 1762928340557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a framework to enable covariate-aware forecasting using TSFMs.\n\nThis adaptation is achieved through two modules\n\n1. Endogenous Series Filter:- this module captures stable patterns and dependencies from the history. This module is a TS encoder which takes as input the original series and constructs a new series which is further filtered using a discriminator to only retain the endogenous information\n2. Covariate Plugin:- this module divides covariates into patches at different scales and retains the covariates with the largest influence on the target time series. \n\nOutput from both modules are added together to obtain the final output. These modules are trained using a combination of loss functions. \n\nMost TSFMs are pretrained to handle univariate forecasting. The main contribution of the work is the framework to perform covariate-aware forecasting with TSFMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Authors have performed a detailed ablation studies.\n2. The proposed framework is novel and ablation studies clearly show the significance of each component."}, "weaknesses": {"value": "1. Evaluation seems incomplete. EPF datasets contain 5 time series (EPF-PJM, EPF-BE, EPF-DE, EPF-FR, EPF-NP). Not sure why evaluation is restricted to only one time series (PJM)\n2. Proposed framework is not tested for long-term forecasting.\n3. Computational overhead of the framework is not discussed.\n4. Training details are not included. Particularly the size of the encoders which can introduce the largest computation overhead.\n5. The solution is great, but it takes away the ability of foundation models to forecast zero-shot, as the framework requires training for each dataset.\n6. Evaluation with ChronosX (Arango et al.) is missing."}, "questions": {"value": "1. What is the size and the architecture of the encoders used in EFit and CPin?\n2. What is the training and computation overhead of this framework?\n3. Why are other EPF datasets excluded?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ieDuwHoCMQ", "forum": "1O2A5TnFWl", "replyto": "1O2A5TnFWl", "signatures": ["ICLR.cc/2026/Conference/Submission18630/Reviewer_gwAV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18630/Reviewer_gwAV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998077627, "cdate": 1761998077627, "tmdate": 1762928339817, "mdate": 1762928339817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}