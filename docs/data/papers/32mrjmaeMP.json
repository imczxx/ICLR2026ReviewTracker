{"id": "32mrjmaeMP", "number": 21000, "cdate": 1758312605810, "mdate": 1763723932864, "content": {"title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature", "abstract": "Task Arithmetic (TA) provides a modular and scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors, but existing approaches typically require external task data, which conflicts with TA’s modularity and availability constraints like privacy concerns. We propose a data-free approach by framing representation drift regularization as a curvature matrix approximation problem. This allows us leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature (KFAC) to obtain practical regularizers. Our method is data-free, has constant complexity with respect to the number of tasks, and improves performance on TA benchmarks.", "tldr": "We tackle cross-task interference in Task Arithmetic with a data-free approach. By viewing drift as curvature approximation and applying K-FAC, we achieve scalable task disentanglement and improved benchmark performance.", "keywords": ["task arithmetic", "model editing", "task vector"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95934888053c614ad92d5f52b7719a9a7f045d09.pdf", "supplementary_material": "/attachment/b037cf8268e6ad171c2bbc174ad26b78d8a43fbf.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method for weight disentanglement in task arithmetic. To address the limitations of prior constraints on cross-task representation drift—which often require access to auxiliary data during fine-tuning and incur high computational cost—the authors approximate the penalty via Kronecker-factored decomposition. This lets fine-tuning proceed without explicit data, needing only each layer’s input covariance and output-gradient covariance, while substantially reducing compute. Experiments across both vision and NLP tasks show that the approach matches or surpasses the strong baseline $\\tau$Jp."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear presentation: The paper is well structured, easy to follow, and provides accessible explanations of the theoretical derivations.\n\n- Novelty of bringing KFAC into task arithmetic: It reframes the traditional representation-drift penalty as a curvature (GGN) approximation under squared loss, then applies a KFAC approximation to make the method more data-efficient during fine-tuning and significantly cheaper computationally.\n\n- Comprehensive empirical evaluation: Beyond merge performance versus recent state-of-the-art methods, the experiments also cover cost analysis, comparisons with linearization-based approximations, and ablations over different KFAC estimation choices."}, "weaknesses": {"value": "- Dependence on other-task statistics (A, B): While the method mitigates the need for raw auxiliary data required by τJp, it still relies on access to precomputed input/gradient covariance factors from the other tasks to be disentangled. This dependence may limit applicability in settings with privacy or access constraints and introduces coupling to external task-specific artifacts.\n\n- Missing non-linear regime comparisons to SOTA: The paper compares against recent methods such as TSV and ISO only in the linear regime, not in the non-linear regime where those methods are primarily positioned. If performance is comparable to—or worse than—those SOTA approaches, the practical advantage of the proposed method remains unclear, especially since TSV/ISO can merge using parameters alone whereas the proposed approach requires auxiliary task statistics.\n\n- Underspecified regularization strength selection: The KFAC regularizer’s weight is fixed per task/dataset, but the paper does not clearly describe how these values were chosen nor provide sensitivity or robustness analyses. This gap raises concerns about reproducibility and about how performance depends on the strength of the regularization."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6OhZGFkr70", "forum": "32mrjmaeMP", "replyto": "32mrjmaeMP", "signatures": ["ICLR.cc/2026/Conference/Submission21000/Reviewer_K5Dr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21000/Reviewer_K5Dr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760689930861, "cdate": 1760689930861, "tmdate": 1763000004755, "mdate": 1763000004755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a data-free approach to improving task arithmetic grounded in Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimization approximation that captures local curvature information in parameter space."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Quality: Good. The paper presents a well-motivated and technically sound contribution, with strong theoretical grounding in second-order optimization and empirical evidence supporting its claims. Experimental evaluation is relatively comprehensive, covering diverse architectures and tasks, with consistent metrics and clear ablation studies isolating the contribution of curvature-based disentanglement.\n2. Clarity: The paper is very well written and easy to read. \n3. Significance: The proposed solution is efficient and effective (on par with the best provided TA method for vision datasets, yet slightly lower performance on language datasets). Its performance on both standard finetuning and linear finetuning shows its flexibility."}, "weaknesses": {"value": "1. Minor Novelty concern because this paper seems relatively incremental as it’s built upon the key ideas of Ortiz-Jimenez et al., 2023 and Yoshida et al. (2025)."}, "questions": {"value": "1. I got confused by the naming of “Dataless” because even linear finetuning requires data, and gradients/Jacobian require data.\n2. In Eq(5), is $R(\\theta)$ also removed in nonlinear finetuning?\n3. Can you discuss the relationship of GGN and other Fisher or Hessian matrix based methods like “Efficient Model Editing with Task-Localized Sparse Fine-tuning.” (Iurada, Leonardo, et al. 2025) and “Merging Models with Fisher-Weighted Averaging” (Matena, Raffel, 2022)?\n4. Notation: In line 179, what does the coefficient matrix refer to here? In line 218, what is $s_{n,m}$?\n5. Eq 8: What’s the memory and runtime cost of this approximation and can you explain any intuition of this approximation?\n6. In Figure 5, wonder why some of the 8 standard vision tasks like SVHN are missing.\n7. In Table 1 & 2, include the results of Porrello et al. (2025)’s numbers for a more straightforward comparison.\n8. Line 369 “naive” typo."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NKvq2e1gxR", "forum": "32mrjmaeMP", "replyto": "32mrjmaeMP", "signatures": ["ICLR.cc/2026/Conference/Submission21000/Reviewer_RdRs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21000/Reviewer_RdRs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886020196, "cdate": 1761886020196, "tmdate": 1763000004616, "mdate": 1763000004616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a data-free regularizer for Task Arithmetic (TA) that penalizes cross-task representation drift using a curvature surrogate. Starting from the linearized network around the pre-trained weights, the drift penalty becomes a quadratic form in the Jacobian Gram / GGN matrix; the authors approximate it with KFAC, yielding a per-layer Kronecker structure that is cheap to evaluate during fine-tuning (Eqs. (3), (6)). They further introduce a merged regularizer that aggregates per-task KFAC factors into a single surrogate to keep cost constant in the number of tasks (Eq. (8)). Experiments on CLIP (8-Vision) and T5 show strong gains for TA addition/negation and competitiveness with τ-JP—without needing other tasks’ data. Notably, ViT-L/14 reaches 91.6 abs / 99.3 norm with α=1, and their method excels at task negation while preserving control accuracy. The paper also claims minimal training overhead and code release."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tConceptual clarity: The drift penalty is cleanly derived from the linearization, tying representation drift to the GGN and enabling reuse of mature curvature approximations (Sec. 3.1–3.3). \n\n•\tScalability: The merged KFAC surrogate (Eq. 8) achieves O(1) cost in tasks and empirically matches the naïve O(T) sum. Table 3 shows near-parity on ViT-B/16 and T5. \n\n•\tStrong empirical results: On 8-Vision, KFAC-regularized TA outperforms linear and non-linear FT, matches or beats τ-JP in several settings without external task data, and yields robust α=1 behavior (Table 1). It also improves task negation (Table 2). \n\n•\tTask-localization signal: The paper links its regularizer to a practical normalcy score (|J_\\theta f(x,\\theta_0)\\tau_t|_2^2) that separates in- vs out-of-task inputs (Fig. 5). \n\n•\tPracticality: Overhead analysis and ablations (Fig. 6–7) are useful; MC=1 with ~128–256 examples often suffices for KFAC estimation."}, "weaknesses": {"value": "•\tMissing baseline: No comparison to Task-Localized Sparse FT; given the shared goal (localized updates with low interference), this absence limits claims of state-of-the-art effectiveness on task-local editing.\n\n•\tAttention-only baseline not fully aligned: While “Non-linear (Attn.)” appears, the paper doesn’t replicate the full protocol and metrics of Fine-Tuning Attention Modules Only, so the reader can’t conclude whether KFAC-regularized FT beats that specific method under its strengths (e.g., disentanglement maps/metrics across α-sweeps, stability under merging). \n\n•\tHeuristic merge lacks theory: The Kronecker aggregation (Eq. 8) is a heuristic; no approximation error bounds or conditions are provided, though it works empirically. \n\n•\tMemory footprint: The Limitations section concedes KFAC’s quadratic-in-units per layer memory cost, which could bite for very large LMs; compression or structured factors are left to future work. \n\n•\tLanguage results trail τ-JP: On T5, the approach narrows the gap but τ-JP still tops the leaderboard in places (Fig. 3a), hinting the curvature surrogate could be further improved for text. \n\n•\tPrivacy/deployment details: Since the method proposes sharing curvature stats instead of data, the paper would benefit from concrete guidance on what to share (sizes, precisions, any leakage risks), especially beyond CLIP/T5 scales. (Code release is stated but details aren’t in the main text.)"}, "questions": {"value": "1.\tTLSF baseline: Can you add results against Efficient Model Editing with Task-Localized Sparse Fine-tuning on the same 8-Vision and T5 setups, plus an editing-localization metric (e.g., Δ outside-task vs in-task) to demonstrate task localization superiority?\n\n2.\tAttention-only FT: Could you replicate the Fine-Tuning Attention Modules Only protocol (datasets, α-grids, disentanglement error maps) to enable a direct SOTA comparison under its claims? \n\n3.\tTheory for Eq. (8): Any conditions (e.g., approximate independence of factors across tasks) under which the Kronecker factor summation is a principled approximation to (\\sum_t B_t\\otimes A_t)? \n\n4.\tMemory/precision trade-offs: How do FP16/8-bit factor storage and block-sparsity affect accuracy and the normalcy score? Could you move factors off-GPU during training without bottlenecks? \n\n5.\tScale to LLMs: What are the empirical costs when applying the method to 7B–70B-parameter LMs (rough factor sizes, setup time), and do your MC/num-examples findings (Fig. 7) still hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "StHTqN4xCZ", "forum": "32mrjmaeMP", "replyto": "32mrjmaeMP", "signatures": ["ICLR.cc/2026/Conference/Submission21000/Reviewer_1ihd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21000/Reviewer_1ihd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966918075, "cdate": 1761966918075, "tmdate": 1763000004595, "mdate": 1763000004595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses cross-task interference in model merging/task arithmetic by proposing a method for enhancing weight disentanglement. The authors demonstrate that *representation drift regularization*, a data-dependent solution, can be reformulated in a linearized setting as a quadratic form involving the Jacobian's Gramian matrix (G). They then use a tractable approximation (KFAC) of G to propose a new, data-free regularizer. The authors also propose a heuristic to merge KFAC matrices with O(1) complexity. The resulting KFAC-regularized task vectors are shown to be more disentangled, leading to better performance when merging models that have been fine-tuned linearly."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The theoretical derivation is elegant, connecting representation drift in a linearized model to a practical, data-free regularizer via the GGN and KFAC.\n- In the linearized fine-tuning setting, the proposed method performs on-par with or better than the data-dependent $\\tau$Jp baseline.\n- KFAC regularization at training time allows simple weight averaging (task arithmetic) to outperform more complex, SOTA post-hoc merging methods (like TSV) that are applied to unregularized vectors if training was in the linearized regime.\n- The method is shown to be computationally fast."}, "weaknesses": {"value": "- The entire experimental validation is confined to the linearized finetuning framework (see, e.g., Ortiz-Jimenez et al., 2023). The authors don't test their KFAC regularizer on full, non-linear finetuning. Is the regularizer only effective in a linearized regime?\n- The paper's validation against SOTA merging methods (TIES, TSV, etc.) is limited to this niche linearized FT setting (Figure 4). This avoids the most practical question: how do these methods perform on task vectors from standard, non-linear fine-tuning? These non-linear task vectors are the default, readily available artifacts in most real-world scenarios. If SOTA post-hoc methods applied to these standard vectors already outperform the entire KFAC/linearized-FT framework, the practical motivation for this work is unclear. The paper fails to demonstrate that its regularizer is necessary or effective in the standard, non-linear setting.\n- (Minor) The performance gains on T5 are less impressive, and the data-dependent alternative ($\\tau$Jp) is better."}, "questions": {"value": "- What are the performance results of SOTA post-hoc methods (TIES, TSV, etc.) when applied to task vectors from standard non-linear fine-tuning? How does the KFAC-regularized framework compare in that more practical scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8UVeYV1T6N", "forum": "32mrjmaeMP", "replyto": "32mrjmaeMP", "signatures": ["ICLR.cc/2026/Conference/Submission21000/Reviewer_KRUA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21000/Reviewer_KRUA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003618826, "cdate": 1762003618826, "tmdate": 1763000004393, "mdate": 1763000004393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}