{"id": "0Olva8f90o", "number": 24376, "cdate": 1758356240207, "mdate": 1759896769069, "content": {"title": "Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models", "abstract": "Pretrained diffusion models have demonstrated strong capabilities in zero-shot inverse problem solving by incorporating observation information into the generation process of the diffusion models. However, this inevitably presents a dilemma: excessive integration can disrupt the generative process, while insufficient integration fails to emphasize the constraints imposed by the inverse problem. To address this, we propose $\\textit{Noise Combination Sampling}$, a novel method that synthesizes an optimal noise vector from a noise subspace to approximate the measurement score function, replacing the noise term in the standard Denoising Diffusion Probabilistic Models process. This enables conditional information to be naturally embedded into the generation process without reliance on step-wise hyperparameter tuning. Our method can be applied to a wide range of inverse problem solvers, including image compression, and, in most scenarios, especially when the number of generation steps $T$ is small, achieves superior performance with negligible computational overhead, significantly improving robustness and stability.", "tldr": "We address the stability issue in solving general linear inverse problems based on diffusion models by approximating the noise term in DDPM with the projection of its measurement score onto the noise subspace.", "keywords": ["Diffusion Model", "Inverse Problem", "Generative Model", "Optimization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/721a09898ec25a28c3f25783c6b5e04c940d860b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the¬†Noise Combination Sampling (NCS) framework¬†to address the key challenge of balancing observation constraint integration and diffusion process consistency in diffusion-based inverse problem solving. NCS constructs an optimal noise vector¬†(via linear combination of base Gaussian noise from a codebook) to implicitly embed observation information, ensuring the noise remains standard normal and the sampling trajectory stays on the data manifold. It provides a closed-form solution for optimal weights (via Cauchy‚ÄìSchwarz inequality), unifies existing methods (DPS, MPGD, DDCM) as special cases, and achieves both better performance (higher PSNR, lower LPIPS) and efficiency across tasks like inpainting, super-resolution, and deblurring."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The closed-form solution for optimal weights (Theorem 2) is strictly derived using inner products and the Cauchy‚ÄìSchwarz inequality, eliminating the need for heuristic iterative optimization. This ensures reproducibility‚Äîresearchers can directly compute¬†\\gamma^*¬†without tuning empirical parameters.\n2. Unlike baselines (e.g., DPS) that suffer severe quality degradation when diffusion steps¬†T¬†are reduced (to cut costs), NCS maintains high robustness. Experiments show it achieves high-quality results even with small¬†T¬†(e.g., 100 steps vs. 1000 steps), as the optimal noise combination preserves manifold consistency without relying on excessive iterations.\n3. This method replaces DDCM‚Äôs exponential-complexity noise selection, where¬†C¬†is quantization bins) with NCS‚Äôs linear-complexity combination. For example, combining 3 noise vectors achieves inner-product magnitudes comparable to DDCM‚Äôs search over 1024 vectors, slashing storage (smaller codebooks) and computation (fewer inner-product calculations) costs."}, "weaknesses": {"value": "1. This method lacks clear guidelines for selecting codebook size¬†K¬†and base noise distribution. A small¬†K¬†restricts the noise combination space (failing to match rare observation directions), while a large¬†K¬†increases memory usage and inner-product computation time. No adaptive mechanism (e.g., dynamic¬†K¬†based on task complexity) is proposed.\n2. The authors state that the NCS method approximates the measurement score through the linear combination of Gaussian noise vectors. Can similar effects be achieved via approximation methods such as Hermite polynomials? After all, Hermite polynomials are the optimal basis functions for Gaussian-related processes.\n3. The authors state that the NCS method can be integrated into sampling strategies such as DDPM. Is NCS incompatible with DDIM out of inverse problems, a more commonly used deterministic sampling method? Can it be extended to Rectified flow in video generation models?\n4. In the description of Figure 1, the authors state that NCS embeds the measurement score into the optimal noise within an ellipsoidal subspace, which is defined by the span of the noise codebook. I confuse that is this definition related to the one in Equation (10)? Why is the noise obtained from a simple weighted sum explained using the concept of an ellipsoidal subspace?\n5. When the authors explain the advantages of NCS, they highlight a key feature: it adheres to conditional constraints while ensuring generation stability. However, most of the experimental results focus on image quality metrics such as PSNR. Although these metrics are suitable for evaluating inverse problems, I am also curious about NCS‚Äôs performance in terms of conditional compliance‚Äîfor example, metrics like image-text alignment."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3a3uPO7Frt", "forum": "0Olva8f90o", "replyto": "0Olva8f90o", "signatures": ["ICLR.cc/2026/Conference/Submission24376/Reviewer_2DcC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24376/Reviewer_2DcC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807705370, "cdate": 1761807705370, "tmdate": 1762943062079, "mdate": 1762943062079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Noise Combination Sampling (NCS), a method for solving diffusion-based linear inverse problems by replacing the stochastic noise term in the diffusion update rule with a constructed linear combination of Gaussian noise vectors. The combination weights are chosen to align with the estimated measurement score direction, derived via a closed-form expression using the Cauchy‚ÄìSchwarz inequality. Experiments on standard inverse problems such as inpainting, deblurring, super-resolution, and compression are presented, showing moderate performance gains and faster convergence in some settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well-written and clearly structured, making the proposed method easy to follow.\n2. The idea of modifying the diffusion noise term instead of using explicit gradient guidance is interesting and practically implementable.\n3. The authors conduct some ablations such as varying codebook size K and report quantitative metrics across multiple datasets, showing some empirical rigor."}, "weaknesses": {"value": "1. The review template seems to be missing some elements, such as line numbers and the header stating ‚ÄúUnder review as a conference paper at ICLR 2026.‚Äù The former, in particular, would have made it easier to reference specific parts of the text when providing comments.\n2. NCS primarily reparameterizes the noise term in existing diffusion posterior sampling methods, offering little conceptual or theoretical advancement. The ‚Äúclosed-form‚Äù derivation is a straightforward application of the Cauchy‚ÄìSchwarz inequality, and the claimed improvements over prior approaches are minimal. Overall, the contribution feels incremental rather than a substantive methodological innovation.\n3. The paper claims that the constructed noise combination remains Gaussian if the combination weights are independent of the noise codebook. However, in the proposed method, the weights $ùõæ_i$ are computed from the inner products between the codebook vectors and the measurement score, creating explicit statistical dependence between $ùõæ_i$ and $Œµ_i$. This invalidates the independence assumption underlying the ‚ÄúGaussianity lemma‚Äù and breaks the formal justification that the composed noise $Œµ^*_t$ follows $\\mathcal{N\\mathrm{(\\mathbf{0},\\mathbf{I})}}$.\n4. Methods like DPS and MPGD are reasonable baselines but outdated, as many faster samplers now exist (e.g., Œ†GDM, DDNM, DiffPIR, MGPS) that achieve strong performance within 50-100 NFEs. It remains unclear whether NCS-DPS and NCS-MPGD can outperform these state-of-the-art methods, and Table 2 does not clearly demonstrate whether NCS provides any real improvement over DAPS.\n5. In Section 3, the authors derive all formulas under the assumption of linear inverse problems, yet later report results for the nonlinear phase retrieval task without providing its explicit formulation or explaining how NCS applies in this setting.\n6. The authors claim that NCS unifies most existing gradient-based approaches, but this appears largely superficial, consisting mainly of algebraic reformulations where the guidance term is replaced by a projected noise. There is no shared probabilistic interpretation or derivation showing these methods as genuine special cases of a single framework.\n7. The authors report that they used $œÉ_y$ = 0.05, but the qualitative figures suggest $œÉ_y$ ~= 0.0 (e.g., Figure 2, box inpainting)."}, "questions": {"value": "1. *Regarding weakness 3:* Can the authors clarify how the Gaussianity of $Œµ^*_t$ is preserved in practice given that $ùõæ$ depends on {$Œµ_i$}? If the independence assumption is violated, what is the actual distribution of the constructed noise, and how does this affect the validity of the diffusion process?\n2. *Regarding weakness 4:* Can the authors explain why stronger and more recent zero-shot inverse solvers such as were omitted from comparison? Additionally, can they clarify whether NCS actually improves DAPS, given the quantitative results reported in Tables 2,3 and 4?\n3. *Regarding weakness 5:* How does NCS behave when the forward operator is ill-conditioned or nonlinear? Does the Gaussianity assumption still hold in such cases, and how exactly was phase retrieval implemented or adapted within the proposed framework?\n4. Can the authors report the additional runtime and GPU memory consumption introduced by NCS for typical values of K?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bAmsfpQSRh", "forum": "0Olva8f90o", "replyto": "0Olva8f90o", "signatures": ["ICLR.cc/2026/Conference/Submission24376/Reviewer_bF26"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24376/Reviewer_bF26"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937410857, "cdate": 1761937410857, "tmdate": 1762943061859, "mdate": 1762943061859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Noise Combination Sampling (NCS) framework to address the stability dilemma in solving linear inverse problems with diffusion models. Instead of directly modifying the sampling trajectory, which risks disrupting data manifolds and generative consistency, NCS embeds conditional information into the noise term of Denoising Diffusion Probabilistic Models (DDPM). It synthesizes an optimal noise vector from a predefined noise codebook to approximate the measurement score, with closed-form optimal weights derived via the Cauchy‚ÄìSchwarz inequality to ensure that the synthesized noise remains standard normal. NCS addresses the core dilemma of diffusion-based inverse problems that excessive integration disrupts generation and insufficient integration ignores constraints by synthesizing optimal noise vectors to embed conditional information, rather than directly modifying the sampling trajectory."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. NCS embeds conditional constraints into the DDPM noise term instead of directly modifying the sampling trajectory to avoid pushing generated samples off the data manifold and breaking diffusion generation consistency. This design aligns with the intrinsic stochasticity of diffusion models and has solid theoretical motivation.\n\n2. NCS does not redesign the approximation of conditional score and is expected to expand the performance of many existing methods that apply data consistency through conditional gradient score approximation."}, "weaknesses": {"value": "1. The paper acknowledges unstable results on nonlinear tasks (e.g., phase retrieval) but provides no theoretical analysis on the reason why NCS fails here. For example, whether the noise subspace can approximate non-linear conditional gradient score or if the closed-form weight solution breaks down under non-linear constraints.\n\n2. The choice of noise codebook size K (e.g., 512 for 4√ó super-resolution and 64 for 8√ó super-resolution) is purely empirical. In the paper, although K is noted to work \"across a broad range\", there does not exist a quantifiable relationship between K, data dimensionality, or task complexity, nor does it define the boundary where increasing K reduces noise independence (mentioned in Section 3.2) and yields degraded performance. Ablation studies are also missing for key parameters related to noise combination such as the number of combined noise vectors K on performance. This makes it hard to validate the robustness of NCS to parameter changes."}, "questions": {"value": "1. Can NCS be adapted to nonlinear inverse problems? If not, what theoretical modifications would be required?\n\n2. Is there any theoretical guideline for K? For example, how should K scale with data dimensionality (d) or task complexity to balance noise expressiveness and independence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oNyD8BbFSF", "forum": "0Olva8f90o", "replyto": "0Olva8f90o", "signatures": ["ICLR.cc/2026/Conference/Submission24376/Reviewer_qukq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24376/Reviewer_qukq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986972934, "cdate": 1761986972934, "tmdate": 1762943061633, "mdate": 1762943061633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Noise Combination Sampling (NCS), a new approach to solving linear inverse problems using pretrained diffusion models without retraining. The main idea is to replace the standard Gaussian noise term in the denoising process with a synthesized noise vector, formed as an optimal linear combination of Gaussian samples drawn from a codebook. The combination weights are derived via a simple closed-form expression based on the Cauchy‚ÄìSchwarz inequality, which aligns the noise with the conditional measurement score.\n\nThe authors claim that this procedure naturally embeds conditional information into the generation process while avoiding the instability and heavy hyperparameter tuning required by prior guidance-based inverse solvers such as Diffusion Posterior Sampling (DPS) and Manifold-Preserving Gradient Descent (MPGD). They further argue that existing diffusion-based inverse problem solvers can be interpreted as special cases of NCS, including the recently proposed Denoising Diffusion Codebook Models (DDCM). Empirical results on FFHQ and ImageNet show moderate improvements in PSNR and LPIPS, particularly for low sampling step counts (T = 20‚Äì100)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written, with logical organization and careful presentation of derivations, figures, and tables. The notation is consistent, and the main idea is easy to follow even for readers without deep expertise in diffusion models.\n2. The derivation of the optimal noise combination through the Cauchy‚ÄìSchwarz inequality is elegant. It provides a compact, closed-form solution that is computationally lightweight and easy to implement.\n3. The authors successfully demonstrate that several well-known inverse problem solvers (DPS, MPGD, DDCM) can be interpreted as instances of the same general principle. This unifying viewpoint could be valuable for researchers seeking a more cohesive understanding of guidance mechanisms in diffusion models. Also, across multiple datasets and problem types, the NCS variants perform as well as or slightly better than their corresponding baselines, especially when the number of diffusion steps is small. This consistency suggests that the approach is robust and stable in practice. More importantly, the method requires no extra training and introduces negligible additional computational cost. The linear complexity with respect to the codebook size makes it practical and accessible for a wide range of applications. By relating the approach to DDCM, the paper opens the possibility of extending diffusion-based generative compression with simpler noise quantization schemes."}, "weaknesses": {"value": "While the paper is neat and clearly executed, the conceptual advance feels incremental and its underlying mechanism insufficiently explored. The proposed idea of aligning a noise combination to a measurement gradient is mathematically simple and closely related to existing guidance schemes. The derivation relies on a direct application of Cauchy‚ÄìSchwarz, and it is unclear why this reformulation should lead to qualitatively better sampling.\n\nMoreover, the claimed advantages (manifold preservation, stability, robustness to step size) remain intuitive hypotheses rather than demonstrated phenomena. No quantitative measure or theoretical justification is provided for why embedding the conditional information into the noise term, as opposed to the mean term, should improve results. The experimental section reports moderate numerical gains but does not investigate what aspects of NCS drive these improvements‚Äîwhether due to the noise combination itself, implicit regularization effects, or differences in implementation.\n\nOverall, the work reads as a well-presented reformulation of known methods rather than a fundamentally new contribution. It could be strengthened by deeper theoretical analysis and more targeted experiments probing why the approach works. If the authors are able to answer the following questions in the next section, it would be a big help for us."}, "questions": {"value": "1. Could the authors provide a theoretical or empirical explanation for why replacing the noise term with an optimally combined version improves stability or reconstruction quality? Is there any evidence (e.g., manifold distance, variance analysis, or effective step size) showing that the NCS trajectory stays closer to the learned data manifold?\n\n2. In practice, NCS appears mathematically similar to taking a guided noise step proportional to $\n\\nabla_x \\log p(y\\mid x)$. Could the authors clarify how NCS differs in effect from existing gradient-based corrections? Are there cases where the two produce substantially different trajectories?\n\n3. How sensitive is performance to the codebook size K and the number of combined noise vectors m? Does increasing K always help, or does it introduce variance and instability due to correlation among noise samples? The method is restricted to linear inverse problems. Are there conceptual or mathematical obstacles to extending NCS to nonlinear or learned degradation operators (e.g., differentiable renderers, neural forward models)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6zKiz4Tq5E", "forum": "0Olva8f90o", "replyto": "0Olva8f90o", "signatures": ["ICLR.cc/2026/Conference/Submission24376/Reviewer_KBnG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24376/Reviewer_KBnG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762419272130, "cdate": 1762419272130, "tmdate": 1762943061263, "mdate": 1762943061263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}