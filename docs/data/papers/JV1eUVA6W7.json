{"id": "JV1eUVA6W7", "number": 6900, "cdate": 1758000919201, "mdate": 1763641309016, "content": {"title": "SEED: Towards More Accurate Semantic Evaluation for Visual Brain Decoding", "abstract": "We present SEED ($\\textbf{Se}$mantic $\\textbf{E}$valuation for Visual Brain $\\textbf{D}$ecoding), a novel metric for evaluating the semantic decoding performance of visual brain decoding models. It integrates three complementary metrics, each capturing a different aspect of semantic similarity between images inspired by neuroscientific findings. Using carefully crowd-sourced human evaluation data, we demonstrate that SEED achieves the highest alignment with human evaluation, outperforming other widely used metrics.\nThrough the evaluation of existing visual brain decoding models with SEED, we further reveal that crucial information is often lost in translation, even in the state-of-the-art models that achieve near-perfect scores on existing metrics. This finding highlights the limitations of current evaluation practices and provides guidance for future improvements in decoding models.\nFinally, to facilitate further research, we open-source the human evaluation data, encouraging the development of more advanced evaluation methods for brain decoding.", "tldr": "", "keywords": ["Brain Decoding", "fMRI", "Evaluation Metric"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/359bb10ba3711a6418de42296ed00229629846a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In “SEED: Towards More Accurate Semantic Evaluation for Visual Brain Decoding” authors proposes a new metric, SEED, to assess the semantic accuracy of images reconstructed from brain activity. They argue that existing metrics (e.g., SSIM, CLIP, Inception) fail to align with human perception of semantic similarity, often overestimating model performance. SEED combines three complementary measures: (1) Object F1, which captures object-level overlap using open-vocabulary grounding models; (2) Cap-Sim, which compares captions of ground-truth and reconstructed images using text embeddings; and (3) EffNet, a feature-based similarity measure. Through medium-scale human evaluations on 1,000 pairs from NSD and additional datasets, SEED metric demonstrates stronger correlation with human judgments than existing metrics and reveals that state-of-the-art decoding models still struggle to reconstruct fine-grained semantics."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a very timely contribution: Evaluation misalignment between metrics and human perception is a major issue in brain-to-image decoding. In the last 2-3 years I have worked in the field and seen a significant amount of brain decoding works, always competing for few fractions of percentage points of saturated metrics that not always reflect real performance.\nHere, SEED addresses a crucial gap that could significantly impact future research.\n\nThe metric is well-motivated, interpretable, and partially grounded in both neuroscience and computer vision (object-based attention and semantic binding).\n\nThe authors conduct meta-evaluations, robustness tests across models/datasets, and qualitative analyses (worst-case examples, semantic near-miss, and failure modes).\n\nA very good point is the commitment to release the human evaluation dataset adds substantial reproducibility and community value. I'd also suggest the authors to make this metric a very easy to use (ideally a one-liner python function) to encourage reproducibility."}, "weaknesses": {"value": "Introducing the metric is an important point, but I felt something is missing in this work:\n\nSome literature is absent, the number of work and approaches has become very big. Many are missing. To position in the field and constructively criticize the practices I think extended literature review is needed. \n\nThis metric is better aligned with human judgement, but still has some bias from the model and limitations. Could the author discuss more in detail these limitations and future work to be improved? Other metrics, ecc.\n\nOverall, this metric is a linear combination of a bunch of others, all relying on some models. In appreciate the point of view even if the technical contribution is somehow limited."}, "questions": {"value": "1) Will this metric/code be released in a easy to access and no-brainer way to encourage wide usage? I think this is really the keypoint here, otherwise the whole thing loses a lot of traction.\n\n2) In the field everyone is competing to have his own line in bold. Your new metric was evaluated on some famous approach, but the best-in class and the order didn't change much. Since the ranking was mostly unaffected, could you highlight better the cases where this metric is telling something more? I liked the failure cases ecc, but I really would love to see more example and use cases where errors or successes were hided by other metrics and SEED is able to point them out\n\n3) The sample size for human evaluation is kind of limited but I understand this is difficult to solve. There is a lot of work on THINGs dataset with million of annotations and human preferences between categories and images ecc, as well as many models trained on them, Does it make sense to compare SEED with these models?\n\n4) One thing that could blow my mind is whether you think SEED could became an objective loss function to be optimized. Right now, as far as I understood it involves several non-differentiable operation that limit the use as a metric alone, but do you think this could be extended as a training objective?\n\n5) Discussion for a position paper like this is a bit short and dry. What are the implication? What's the significance? Why it's needed? Plus general reflection on the field would improve the maturity of the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DqeKihkjaB", "forum": "JV1eUVA6W7", "replyto": "JV1eUVA6W7", "signatures": ["ICLR.cc/2026/Conference/Submission6900/Reviewer_cg7v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6900/Reviewer_cg7v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760456861704, "cdate": 1760456861704, "tmdate": 1762919142323, "mdate": 1762919142323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "First of all, we deeply appreciate the valuable criticism and insights from the three reviewers. We are happy to improve our work based on the reviewers’ suggestions.\n\nWe also thank the reviewers for acknowledging the strengths and significance of our contribution. As noted, proposing a new evaluation metric for visual brain decoding is both timely and well-motivated. Our metric is grounded in neuroscientific considerations and demonstrates robust alignment with human judgments across different models and datasets. Furthermore, we provide detailed failure-mode analyses of current decoding models and release our human-evaluation dataset to support future research in this area.\n\nAt the same time, we take seriously the constructive concerns raised in the reviews. These include the potential failures introduced by relying on existing models, concerns regarding reproducibility and usability of the open-sourced evaluation pipeline, and several additional points raised by individual reviewers.\n\nWe have done our best to understand and address each reviewer’s feedback. We also recognize that some concerns may not be fully resolved within the scope of the rebuttal. If that is the case, we would be more than happy to clarify further during the discussion phase.\n\n**The feedback from reviewers and our response to the questions are reflected in the manuscript or the appendix, with additional experiments, figures, and discussions. Any new changes are written in blue text, so feel free to check it out if you are interested.** Any new feedback or questions regarding these changes are also welcome!\n\nOnce again, we sincerely thank the reviewers for their thoughtful comments and look forward to a constructive and engaging discussion to further improve our work.\n\n### **Regarding Comments on the Limitations of SEED.**\nAs all three reviewers noted, SEED relies on off-the-shelf models, which means it may inherit their biases or errors. Consequently, there can be cases where SEED’s semantic similarity assessments deviate from human evaluation.\n\nWe agree with the reviewers' sentiment, however we want to stress it is not a problem unique to SEED as most existing evaluation metrics (AlexNet, Inception, CLIP, EffNet, SwAV) also rely on AI models to extract image features. \n\nAs discussed in Sec. 5.3, each component of SEED may demonstrate undesirable biases for some specific cases. As shown in those examples, we believe that these limitations can be partially diluted by combining the three metrics, complementing each other.\n\nNevertheless, we admit that there can be cases where SEED still deviates from human evaluation. We showcased these failure cases in the revised manuscript, and also added a discussion about this in the Sec. D.2, while discussing a possible direction for advancing the evaluation metric."}}, "id": "nu07xXoyMF", "forum": "JV1eUVA6W7", "replyto": "JV1eUVA6W7", "signatures": ["ICLR.cc/2026/Conference/Submission6900/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6900/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6900/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763641417478, "cdate": 1763641417478, "tmdate": 1763643241211, "mdate": 1763643241211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SEED (Semantic Evaluation for Visual Brain Decoding) — a new metric designed to better evaluate visual brain decoding models in terms of semantic similarity. The authors integrate three components — Object F1, Cap-Sim, and EffNet — to capture complementary aspects of human-like visual perception. Extensive human evaluations demonstrate that SEED aligns more closely with human judgments than existing metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* A more human-like evaluation framework is essential for the brain decoding community.\t\n\n* The use of large-scale human evaluations is impressive."}, "weaknesses": {"value": "* A key limitation of SEED lies in its reliance on off-the-shelf captioning and detection models (e.g., GIT, Grounding-DINO). These components were not trained to reflect human semantic judgments but to optimize task-specific objectives (caption likelihood or object detection accuracy). As a result, SEED may inherit their systematic errors, leading to misleading evaluations in certain cases.\n\n* The authors argue that in some existing metrics such as n-way identification, decoding models have reached near-ceiling performance, but this is largely due to the small candidate pool typically used in evaluation. If the number of candidate images is substantially increased (e.g., 100-way or 1,000-way identification), most current decoding models exhibit significant performance degradation, revealing substantial room for improvement.\n\n* Moreover, recent progress in visual decoding is moving toward multimodal decoding frameworks (text and image). In such settings, the semantic quality of reconstructed images can already be assessed through predicted text. This trend raises questions about the necessity of using GIT-based caption generation in SEED. Since GIT introduces additional linguistic biases and noise unrelated to the decoding model itself.\n\n* The complexity of SEED may to some extent hinder its widespread adoption. For example, its reliance on multiple models introduces issues such as version differences, model updates, and parameter inconsistencies, which reduce the reproducibility of results and the comparability within the community.\n\nMinor: Several recent works such as [1-2] in brain decoding are not cited or discussed.\n\n[1] Bridging the Gap between Brain and Machine in Interpreting Visual Semantics: Towards Self-adaptive Brain-to-Text Decoding, ICCV 2025.\n\n[2] Mindgpt: Interpreting what you see with non-invasive brain recordings, IEEE TIP 2025."}, "questions": {"value": "* Could SEED be adapted to multimodal decoding (e.g., brain-to-text or cross-modal tasks)?\n\n* Could evaluating the semantic fidelity of reconstructions using fine-grained category labels rather than object detection models provide a more efficient and reliable assessment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iIPT3iHJ8M", "forum": "JV1eUVA6W7", "replyto": "JV1eUVA6W7", "signatures": ["ICLR.cc/2026/Conference/Submission6900/Reviewer_hyma"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6900/Reviewer_hyma"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761407133468, "cdate": 1761407133468, "tmdate": 1762919141880, "mdate": 1762919141880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical problem: current evaluation metrics are poorly aligned with human judgment of semantic similarity in visual brain information reconstruction. Even if results achieve near-perfect scores on existing metrics, they may still be semantically flawed. To address this, they propose SEED, a new evaluation metric that integrates three complementary components—Object F1, Cap-Sim, and EffNet. Through extensive \"meta-evaluation\" against a new human judgment dataset, they demonstrate that SEED aligns significantly better with human evaluation than all existing metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of the article is very good. The current evaluation of decoding models indeed has such a problem. Especially, many tasks utilize contrastive learning in the CLIP space and then proceed with image generation. Naturally, this leads to excellent generation scores.\n\n2. The design of SEED is thoughtful. The two new proposed metrics, Object F1 (object-level attention) and Cap-Sim (feature binding into a scene description), offer novel and complementary perspectives. It is indeed something worth noting during the recovery process.\n\n3. The article conducted numerous human alignment experiments and evaluated the results of many models, proving that the proposed indicators are indeed closer to human understanding of image restoration."}, "weaknesses": {"value": "1. The proposed indicators mainly focus on assessing semantics and may be insensitive to information like color and texture. This is also a significant factor influencing restoration and human perception.\n\n2. The article does not provide a clear description of how human evaluations are conducted. For example, in Figure 1, why is that image ranked 846th out of 1000? How was this ranking determined? The human evaluators rated both \"semantic and perceptual similarity\". For the meta-evaluation, which rating was used?\n\n3. The method used in the article is reasonable but somewhat complex, which is composed of many current models. I'm not sure if there might be any hidden biases here. If one of the models has a problem, will it lead to the failure of the evaluation? This merely presents a feasible solution, but does not discuss whether there are better evaluation methods or directions for improvement."}, "questions": {"value": "1. In terms of semantic understanding, this is a problem that all generative models encounter. Does this method and insight also work for current generative models? Or is there any new semantic metric in the field of image generation now? How are they being used?\n\n2. Given that multiple large models were used, what was the speed of the inference? What is the total amount of computing resources consumed for the operation? If it is extremely large, it may affect the use of the indicators. Besides, how and why are the weights of multiple models determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PnKUnG0qB4", "forum": "JV1eUVA6W7", "replyto": "JV1eUVA6W7", "signatures": ["ICLR.cc/2026/Conference/Submission6900/Reviewer_UN34"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6900/Reviewer_UN34"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836027584, "cdate": 1761836027584, "tmdate": 1762919141302, "mdate": 1762919141302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}