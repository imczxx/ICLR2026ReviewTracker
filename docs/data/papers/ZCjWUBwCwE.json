{"id": "ZCjWUBwCwE", "number": 12791, "cdate": 1758210332052, "mdate": 1759897486188, "content": {"title": "AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval", "abstract": "Accurate recall from large-scale memories remains a core challenge for memory-augmented AI assistants performing question answering (QA), especially in similarity-dense scenarios where existing methods mainly rely on semantic distance to the query for retrieval. Inspired by how humans link information associatively, we propose AssoMem, a novel framework constructing an associative memory graph that anchors dialogue utterances to automatically extracted clues. This structure provides a rich organizational view of the conversational context and facilitates importance-aware ranking. Further, AssoMem integrates multi-dimensional retrieval signals—relevance, importance, and temporal alignment—using an adaptive mutual information (MI)-driven fusion strategy. Extensive experiments across three benchmarks and a newly introduced dataset, MeetingQA, demonstrate that AssoMem consistently outperforms state-of-the-art baselines, verifying its superiority in context-aware memory recall.", "tldr": "We propose AssoMem, a novel framework that solves the challenge of accurate, scalable memory QA by forming an associative memory graph and adaptively fusing multi-dimensional retrieval signals, resulting in state-of-the-art performances.", "keywords": ["memory-augmented LLM", "scalable retrieval", "memory question answering"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73b928dc8f55f74360662e6f8ea22a63d2b69a4e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes AssoMem, a memory question answering framework for large, similarity-dense conversational histories. It constructs an associative memory graph that links LLM-generated “clues” to raw utterances, retrieves top clues, and then ranks candidate utterances. The ranking component, RITRanker, fuses relevance, importance estimated by query-personalized PageRank, and temporal alignment, with mutual-information guided weights that vary by question type. A multi-task denoising fine-tuning step is used to improve small models’ answer generation ability from noisy retrieved context. Experiments on LongMemEval and the new MeetingQA benchmark show consistent gains over utterance, session, and hybrid baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper designs novel memory structure and ranking metrics. The method bridges the relevance-only limitation of existing memory and retrieval methods.  \n1. The empirical retrieval performance outperforms the baselines by a large margin. Multiple models and baselines are studied. \n1. This paper contributes a new MeetingQA dataset that can be used for testing long-term memory systems in different scenarios."}, "weaknesses": {"value": "1. HippoRAG [1,2] designs a graph-structured memory as well and also uses PPR. I would like to see some comparisons as this paper is also a graph-based method.  \n1. Several designs (clue generation, temporal token) are bound to chat-based tasks. By comparison, general memory architectures can be applied to and are actually tested on long-context and RAG tasks. [2, 3]\n1. More details regarding the fine-tuning needs to be discussed. It seems that the fine-tuning is an optional component and does not fit with the entire memory architecture. However, it indeed affects the memory read performance a lot, according to Table 2. It is rare to specially design a fine-tuning procedure just to improve the narrow task of “reading chat memory retrieved from a graph index”. Does the fine-tuning also improve the model’s general long-context ability? Conversely, are existing long-context training recipes/models (e.g., [4] [5]) applicable to this paper?\n1. I would like to see more cost analysis both in terms of latency and in terms of tokens consumed. \n1. I would like to see more details and further analyses to show the value of the new dataset. \n\n[1] Hipporag: Neurobiologically inspired long-term memory for large language models. Gutiérrez et al., 2024.\n\n[2] From RAG to Memory: Non-Parametric Continual Learning for Large Language Models. Gutiérrez et al., 2025. \n\n[3] M+: Extending memoryllm with scalable long-term memory. Wang et al., 2025.\n\n[4] Make your llm fully utilize the context. An et al., 2024. \n\n[5]  Wildlong: Synthesizing realistic long-context instruction data at scale. Li et al., 2025.\n\n\nI am willing to raise my ratings if these concerns are sufficiently addressed."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rVTXA3CtQ8", "forum": "ZCjWUBwCwE", "replyto": "ZCjWUBwCwE", "signatures": ["ICLR.cc/2026/Conference/Submission12791/Reviewer_xRKF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12791/Reviewer_xRKF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760748940578, "cdate": 1760748940578, "tmdate": 1762923598717, "mdate": 1762923598717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AssoMem, a memory-augmented question answering framework that constructs an associative memory graph to link dialogue utterances with automatically extracted clues. By combining multi-dimensional retrieval signals—relevance, importance, and temporal alignment—using a mutual information-driven fusion strategy, AssoMem achieves importance-aware and context-sensitive memory recall. Extensive experiments across three benchmarks, including the newly proposed MeetingQA dataset, demonstrate its superiority."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. AssoMem integrates relevance, importance, and temporal alignment effectively, enabling context-rich and adaptive memory recall for QA tasks.\n\n2. The use of an associative memory graph is a contribution, facilitating semantic connections and improving memory organization.\n\n3. Introduction of the MeetingQA benchmark and extensive experiments validate AssoMem's superiority, with clear performance gains over prior methods."}, "weaknesses": {"value": "1. The innovation of the paper appears to be limited, as the idea of capturing relationships between memories using graph representations has already been explored by several existing methods, such as HippoRAG [1] and A-Mem [2].\n\n2. The experimental comparisons in the paper seem insufficient. In addition to the aforementioned methods, the authors have not compared their approach with other well-known memory-based methods, such as mem0 [3].\n\n3. I find the model fine-tuning process described in Section 3.3 confusing. For QA tasks, wouldn't prior fine-tuning risk leaking information about subsequent queries? Furthermore, such a fine-tuning process seems to lack practical application value in real-world scenarios.\n\n4.  The construction details of the MeetingQA dataset are insufficiently described, and the dataset has not been made publicly available. This raises concerns about the fairness and reliability of the dataset.\n\n5. Some formulas in the paper lack clear explanations of their symbols. For instance, the meaning of $y_m^\\lambda$ in Eq. (3) is not adequately clarified.\n\n[1] Jimenez Gutierrez, Bernal, et al. \"Hipporag: Neurobiologically inspired long-term memory for large language models.\" Advances in Neural Information Processing Systems 37 (2024): 59532-59569.\n\n[2] Xu, Wujiang, et al. \"A-mem: Agentic memory for llm agents.\" arXiv preprint arXiv:2502.12110 (2025).\n\n[3] Chhikara, Prateek, et al. \"Mem0: Building production-ready ai agents with scalable long-term memory.\" arXiv preprint arXiv:2504.19413 (2025)."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zx0aArDXvj", "forum": "ZCjWUBwCwE", "replyto": "ZCjWUBwCwE", "signatures": ["ICLR.cc/2026/Conference/Submission12791/Reviewer_P9dL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12791/Reviewer_P9dL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760974696256, "cdate": 1760974696256, "tmdate": 1762923598259, "mdate": 1762923598259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AssoMem, a memory question-answering framework designed to improve large-scale memory retrieval for LLM-based assistants. The key idea is to model associative human-like recall through a multi-signal retrieval mechanism that integrates relevance, importance, and temporal dimensions. It constructs an associative memory graph linking utterances to automatically extracted ”clues“，employs Personalized PageRank for importance scoring, and fuses multi-dimensional scores using a mutual information–based weighting strategy, with a specific fine-tuning strategy for the LLM. In addition, the paper also introduce new benchmark, MeetingQA, to simulate real-world meeting scenarios where multi-turn dialogues form the memory base paired with diverse QA examples. Experiments on LongMemEval and a new synthetic benchmark, MeetingQA, show consistent improvements over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark is valueable.\n2. The proposed method is effective as valied by comprehensive experiments."}, "weaknesses": {"value": "1. The method is overly complex yet underexplained. There are many components in the proposed method, i.e., associative memory graph,  multi-dimensional scores fusion and specific fine-tuning strategy. Desipte putting them together leads to better performance, it is unclear current design is the best way or it is worthy. For example, what if combine MemGAS with later multi-dimensional scores fusion? What if do not finetune the model?\n2. The associative memory graph construction and page rank part is quite similiar with MemGAS, which weaken the contribution in the method side. \n3. Some details or experiments are not clear. for example, how to make sure the quality and diversity of introduced benchmark? the detailed latency comparison with other baselines.\n4. The paper can benefit with improved writing and presentation. The text is technically overloaded, using jargon and formulaic notations without sufficient narrative clarity. Figures and examples provide limited intuition."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qEeVRK8Uje", "forum": "ZCjWUBwCwE", "replyto": "ZCjWUBwCwE", "signatures": ["ICLR.cc/2026/Conference/Submission12791/Reviewer_xUZR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12791/Reviewer_xUZR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767507521, "cdate": 1761767507521, "tmdate": 1762923597895, "mdate": 1762923597895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel memory QA framework, AssoMem, designed to address the challenge of accurate recall for AI assistants operating on large-scale, similarity-dense memory repositories. Existing methods, which rely primarily on semantic relevance between the query and memory, perform poorly in these \"similarity-dense\" scenarios. Inspired by human associative memory, AssoMem constructs an \"associative memory graph\" that anchors dialogue utterances to automatically extracted \"clues.\" This graph structure enables \"importance-aware\" ranking of memories. The core of the framework is a retriever called RITRanker, which innovatively integrates three distinct signals: Relevance, Importance, and Temporal alignment. These signals are weighted using an adaptive fusion strategy driven by CMI to dynamically adjust each signal's contribution based on the query's intent. The paper also introduces a new benchmark, MeetingQA. Experiments demonstrate that AssoMem outperforms existing baselines across several datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper correctly identifies a core weakness in existing RAG and memory systems: relevance-only retrieval is ineffective in \"similarity-dense\" scenarios.\n2. The introduction of the MeetingQA dataset is a useful contribution to the community, addressing the need for benchmarks that simulate real-world, multi-turn dialogue scenarios."}, "weaknesses": {"value": "1. The paper claims scalability, but the offline graph construction (which includes LLM-based tagging for every session) takes ~1950 seconds (Table 12). This is for a small dataset of 2,500 sessions. When the memory scales to the 100k or 1M entries expected of a \"second brain,\" this offline cost seems prohibitively expensive, challenging the \"scalable\" claim.\n2. The \"importance\" score relies on running Personalized PageRank at query time (Section 3.2.3, \"the utterance cells in t are set to the similarity between query and utterance\"). This requires a graph computation for every query, which can be slow on large graphs. The paper does not analyze this specific query latency, only the overall \"RIT Scoring\" (0.39s).\n3. The paper bundles AssoMem with a generator LLM fine-tuning strategy (Section 3.3). This conflates the contributions. The main results in Table 1 represent the full system (including the fine-tuned LLM). This comparison is potentially unfair, as the baseline methods (e.g., \"Topic grouping\") do not appear to receive this specialized denoising and multi-task fine-tuning. The performance gain attributed to AssoMem might stem not just from the superior retriever but also from a specially-tuned generator.\n4. A key innovation is the adaptive CMI fusion. However, the ablation study (Table 10) comparing it to other fusion strategies (LR, RF, SVM) is relegated to the appendix. More importantly, it is missing the most crucial baseline: a simple, non-adaptive weighted sum."}, "questions": {"value": "1. The CMI fusion strategy appears to require (query, utterance, usefulness label) triples for training. How is this model trained? Does this imply AssoMem must be deployed only after a significant data annotation effort for a specific user, or is it trained once on the benchmark datasets and then generalized?\n2. The paper mentions using query-personalized PPR. What is the actual latency of this PPR computation on the large graph (LongMemEval_l), and how does this latency scale as the graph grows from 2.5k to 100k sessions?\n\n*Typo:\nLine 82: Associative memory graph -> Associative memory graph:"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YLn8ax6Lyl", "forum": "ZCjWUBwCwE", "replyto": "ZCjWUBwCwE", "signatures": ["ICLR.cc/2026/Conference/Submission12791/Reviewer_1BTS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12791/Reviewer_1BTS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011433591, "cdate": 1762011433591, "tmdate": 1762923597563, "mdate": 1762923597563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}