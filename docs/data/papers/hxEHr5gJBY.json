{"id": "hxEHr5gJBY", "number": 7610, "cdate": 1758029287550, "mdate": 1763710619219, "content": {"title": "TQA-Bench: Evaluating LLMs for Multi-Table Question Answering", "abstract": "The advance of large language models (LLMs) has unlocked great opportunities in complex data management tasks, particularly in question answering (QA) over complicated multi-table relational data. Despite significant progress, systematically evaluating LLMs on multi-table QA remains a critical challenge due to the inherent complexity of analyzing heterogeneous table structures and the potentially large scale of serialized tabular data. Existing benchmarks primarily focus on single-table QA, failing to capture the intricacies of connections across multiple relational tables, as required in real-world domains such as finance, healthcare, and e-commerce. To bridge this gap, we present TQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities of LLMs in tackling complex QA tasks over complicated relational data. Our benchmark incorporates diverse relational database instances sourced from real-world public datasets and introduces a flexible sampling mechanism to create tasks with varying multi-table context lengths, ranging from 8K to 64K tokens. To further ensure robustness and reliability, we integrate symbolic extensions into the evaluation framework, enabling the assessment of LLM reasoning capabilities beyond simple data retrieval or probabilistic pattern matching. We systematically evaluate a range of LLMs, both closed-source and open-source, spanning model scales from 2 billion to 671 billion parameters. Our extensive experiments reveal critical insights into the performance of LLMs in multi-table QA, highlighting both challenges and opportunities for advancing their application in complex, data-driven environments.", "tldr": "", "keywords": ["Table Question Answering", "Large Language Model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/df21d8451d5fe51e6cfab2a5047d564cfc216641.pdf", "supplementary_material": "/attachment/6ecfb0aef4fade7bf47222404d65ee412399e83b.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces TQA-Bench, a new benchmark for evaluating LLMs on multi-table question answering (QA) tasks. The authors argue that existing table QA benchmarks largely focus on single-table contexts and fail to capture the relational complexity of real-world datasets.\n\nTQA-Bench is built from three data sources (World Bank, Data.gov, and BIRD) and supports variable context lengths (8K–64K tokens). The queries are generated following the common approach of generating some templates and filling them (referred to as symbolic extension). The authors evaluate a wide range of open- and closed-source LLMs (2B–671B parameters), exploring the effects of table serialization formats (Markdown, CSV, JSON, HTML), table size, and comparison between direct prompting and Text2SQL approaches. Results show that:\n \n- Multi-table QA is significantly more challenging than single-table QA.\n\n - Markdown serialization performs best across formats.\n\n - Model performance degrades with longer contexts.\n\n - Text-to-SQL approaches scale better with context length, though they struggle on analytical queries."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Filling a gap in table reasoning**: Aims to addresses a gap in benchmarks for multi-table QA, a domain of increasing relevance for LLM-based data management.\n\n* **Comprehensive evaluation**: Includes an extensive set of LLMs, formats, and context sizes, producing useful empirical insights.\n\n* **Symbolic extension in question generation**: The idea of augmenting quetion templates with symbolic augmentations allows automatic creation of benchmark questions and and assessing reasoning depth.\n  \n* **Scalable dataset design**: The sampling mechanism for controlling serialized length (8K–64K) is well-motivated for long-context model evaluation."}, "weaknesses": {"value": "* **Missing related work on multi-table QA**:\n  The paper overlooks directly relevant and recent multi-table QA datasets such as\n  \n  * Wu et al., Evaluating LLMs with Multi-Table Multi-Hop Complex Questions. ICLR 2025.\n\n  * Singh et al., MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space. arXiv preprint arXiv:2506.11684 (2025)\n\n  Both address similar goals and should be discussed and compared experimentally or at least conceptually. Without this, the novelty claim is weakened.\n\n* **Dataset scope and representativeness**:\n  The included databases (especially those from BIRD) are highly structured and relational, which may not reflect the semi-structured nature of many real-world tables commonly used in reasoning tasks. This limits the generality of conclusions and may bias the benchmark toward text-to-SQL-style reasoning. A clearer discussion contrasting structured (relational) vs. semi-structured table QA would be valuable.\n\n* **Lack of dataset analysis**:\n  The paper spends much of its space on model results (Tables 3–5) but omits essential dataset diagnostics, such as the number of databases, query types per complexity class, or distribution of join depth. Such analysis is necessary to assess the diversity and difficulty of TQA-Bench.\n  \n* **Formatting and presentation**:\n  While generally clear, several sections are overly detailed (e.g., exhaustive model tables) at the expense of key conceptual comparisons. The narrative could be improved by focusing more on dataset design choices and benchmark positioning rather than numerical results alone.\n  \n* **Unclear size compression explanation**:\n  The token count for Markdown, CSV, and JSON in Table 3 decreases from 8K to smaller values (5.4k, 3.7k, 5.75k tokens), which seems counterintuitive. Clarification or an example of the serialization process would help ensure reproducibility."}, "questions": {"value": "1. How does TQA-Bench compare quantitatively to MMQA and MTabVQA in size, complexity, and domain coverage?\n  \n2. Are any of the databases semi-structured (e.g., with missing relations, nested tables)? If yes, could you provide statistics or examples?\n  \n3. How are symbolic extensions implemented?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uDmUA9M4xD", "forum": "hxEHr5gJBY", "replyto": "hxEHr5gJBY", "signatures": ["ICLR.cc/2026/Conference/Submission7610/Reviewer_sY5W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7610/Reviewer_sY5W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503389057, "cdate": 1761503389057, "tmdate": 1762919694433, "mdate": 1762919694433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TQA-Bench proposes a multi-table QA benchmark that (1) collects relational databases from WorldBank / Data.gov / BIRD, (2) samples those databases to produce serialized multi-table contexts at multiple lengths (8K → 64K tokens), (3) generates templated questions augmented with “symbolic extensions” and Python answer generation, and (4) evaluates a wide range of LLMs (open + closed source, ~2B–671B params) under different serialization formats and with both direct prompting and Text2SQL approaches. Key reported findings include Markdown being the best serialization, instruct models outperforming chat models, large performance drops as context grows, and Text2SQL being more stable but struggling on complex analytics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Important problem & scale — it addresses an under-explored but high-impact task (multi-table QA across large relational contexts) and explicitly targets large serialized contexts (8K–64K tokens), which is timely as long-context LLMs become common\n\nDiverse, public data sources — the benchmark draws from WorldBank, Data.gov, and BIRD (not Wikipedia), which increases variety and reduces simple contamination from wiki tables. The curated selection and reasoning about referential integrity are sensible.\n\nthe paper evaluates many models (28) spanning architectures, sizes, and closed/open source; runs format (Markdown/CSV/JSON/HTML) comparisons; contrasts single- vs multi-table; and compares direct prompting vs Text2SQL. That breadth yields many actionable observations."}, "weaknesses": {"value": "W1: The paper reports many percent accuracies and heatmaps, and it mentions generating “multiple instances to reduce variance,” but I found no paired statistical tests, confidence intervals, or significance testing for model comparisons (e.g., better vs baseline). It’s unclear how stable reported differences (e.g., Markdown vs CSV) are\n\n\nW2: The authors intentionally excluded Wikipedia but still use WorldBank / Data.gov / BIRD — public sources that may be present in LLM pretraining corpora. The paper claims sampling reduces leak, but gives no contamination analysis (e.g., checking if test templates/queries or answer strings appear in model pretraining corpora or online).\n\n\nW3: The evaluation uses a single-choice format (A/B/C/D) with prompts that insist on a single letter answer. This punishes partially correct outputs or correct numeric answers that were phrased differently; at the same time, the Text2SQL evaluation requests the chain-of-thought and SQL code. For complex analytical tasks (correlation, averages), multiple formats for correct answers exist.\n\n\nW4: The paper gives overall accuracy and heatmaps but lacks a deep qualitative error analysis on why top models fail on correlation and composite tasks. There are mentions (e.g., SQL composition difficulties) but not many concrete failure examples analyzed in detail. \n\n\nW5: Metric clarity: Clarify how average scores are computed across subcategories (micro vs macro averages). Provide a full metric table with counts per subcategory."}, "questions": {"value": "same as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GkMeaVu11h", "forum": "hxEHr5gJBY", "replyto": "hxEHr5gJBY", "signatures": ["ICLR.cc/2026/Conference/Submission7610/Reviewer_rikH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7610/Reviewer_rikH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710340261, "cdate": 1761710340261, "tmdate": 1762919694011, "mdate": 1762919694011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new benchmark for evaluating large language models over TableQA tasks, focusing on multi-table question answering settings. Detailed dataset construction procedures are presented, where a sampling strategy is proposed to preserve the foreign key dependencies between tables in a database, and a template-based question generation strategy is proposed to generate questions together with ground-truth answers. Experimental results with 28 large language models are presented, with a few interesting findings such as markdown being the most effective input format when feeding tables into large language models and that direct table prompting consistently outperforms Text2SQL-based solutions in accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents a new multi-table TableQA dataset of varying scales which could be useful for follow-up studies.\n\n2. A large number (28) of large language models are tested over the proposed benchmark dataset.\n\n3. There are some interesting findings in the reported results, e.g., markdown is the most effective input format for feeding tables into large language models; and direct table prompting consistently outperforms Text2SQL-based solutions in accuracy.\n\n4. Source code and datasets are included in the submission."}, "weaknesses": {"value": "1. Novelty:\n\n- The proposed new TableQA benchmark is somewhat incremental. While its scale is larger in terms of the table/database sizes (which is relatively easy to achieve), there is limited variety in the questions (14 template questions) which does not make a particularly interesting dataset. \n\n- Some of the findings are not particularly interesting, e.g., LLMs perform better when the question related tables are precisely identified and given to the LLMs (instead of all tables in a database), although the paper may still serve as empirical evidence for future studies. \n\n2. Technical details: \n\n- Table 7 suggests that the constructed dataset has 8K to 64K tokens per **table** on average, while Table 10 suggests that these are the average number of tokens per **database**. Please clarify which is intended. \n\n- It is not quite sure what is the intended calculation target of the correlation queries.\n\n- In Table 11, it is not quite sure why there are 14,000 question instances under each context length setup. There are 100 database instances (= 10 different databases x 10 sampled instances per database), and there are 14 questions per database (= 2 template questions per question subcategories x 7 subcategories). It seems 100 x 14 = 1,400 question instances to me. \n\n- While there are 28 LLMs involved in the experiments in total. Each set of the experiments uses a different subset of LLMs. The rationale behind the choice of LLMs for each experiment is unclear. The separation of experimental setup from the results of each experiment makes the paper unnecessarily difficult to follow. \n\n3. Minor presentation issues: \n\n- \"The rise of large language models (LLMs) Jin et al. (2022)\" => \"The rise of large language models (LLMs) (Jin et al. 2022)\"\n\n- \"Average Columns\" => \"Average #Columns\", \"sampled row number\" => \"sampled number of rows\"\n\n- \"greater or equal than\" => \"greater than or equal to\"\n\n- \"department delay\" => \"departure delay\"\n\n- What does it mean by \"average total delay\"?\n\n- Published version of Tablebench should be cited instead of the arxiv version."}, "questions": {"value": "See Weaknesses 1 & 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7QepZGwWvf", "forum": "hxEHr5gJBY", "replyto": "hxEHr5gJBY", "signatures": ["ICLR.cc/2026/Conference/Submission7610/Reviewer_6s7s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7610/Reviewer_6s7s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965487102, "cdate": 1761965487102, "tmdate": 1762919693563, "mdate": 1762919693563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TQA-Bench, a new benchmark for multi-table question answering. The benchmark is constructed from real-world databases (BIRD, Data.gov, and WorldBank) and features serialized table data ranging from 8K to 64K tokens, specifically targeting long-context capabilities. A key feature is the use of \"symbolic extensions\" to programmatically generate a diverse set of questions from templates. The authors present a comprehensive evaluation of 28 LLMs on this benchmark, analyzing performance across different model types, serialization formats, and prompting methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ A primary strength is the extensive benchmarking of 28 different models, encompassing a wide variety of open-source instruct, chat, and reasoning-focused LLMs (e.g., Qwen, Llama, DeepSeek).\n\n+ The emphasis on long-context performance (8K-64K tokens) is highly relevant. It addresses a notable gap in many existing table QA benchmarks, which often feature much smaller data volumes. This provides valuable insights into how model performance scales or degrades, with increasing context length.\n\n+ The paper provides a thorough analysis of model performance, examining different serialization formats, and various prompting methods (e.g., direct QA vs. text-to-SQL)."}, "weaknesses": {"value": "While the benchmark is well-constructed, **a major concern is its long-term utility for the community.** The benchmark may already be approaching saturation or being \"readily solved\" by current models. The paper acknowledges existing multi-table benchmarks, positioning long-context as the key differentiator. However, the performance of the best-evaluated model (DeepSeek-R1) is already very high: 94.38% at 8K, 89.56% at 32K, and 81.50% at 64K. These scores suggest the benchmark may be too easy for top-tier models. This concern is amplified by the absence of current frontier models (e.g., GPT 5, Gemini 2.5 Pro, Claude 4.1), making it difficult to gauge the benchmark's true \"headroom\" or difficulty.\n\nThe paper identifies the Correlation (COR) task as particularly difficult. **However, it is questionable whether direct prompting or text-to-SQL are the most practical or powerful approaches for such tasks**. A more pragmatic solution, which the benchmark overlooks, would be to use coding agent (an LLM generated and executed code iteratively given observations from the environment). It would be valuable to evaluate models within an agentic framework (e.g., open-source SWE-agent, Qwen-Code, or close but strong Claude Code). Such an agent could write and execute Python code, a trivial way to solve correlation problems, based on feedback from a code interpreter. It might reflect a more realistic and capable problem-solving paradigm."}, "questions": {"value": "Could the authors elaborate more on the benchmark's positioning?\n\n+ How does TQA-Bench's focus on long-context serialized data differ from Spider 2.0, a industry-level complex text-to-SQL benchmark which also target complex database schemas and multi-step reasoning.\n\n+ The distinction between this multi-table QA setup and a more general task, where a coding agent interacts with several local .csv or .db files using Python, is also a bit unclear to me. The latter seems to be a superset of the task proposed in this benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MGVJDYC1wS", "forum": "hxEHr5gJBY", "replyto": "hxEHr5gJBY", "signatures": ["ICLR.cc/2026/Conference/Submission7610/Reviewer_aAd1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7610/Reviewer_aAd1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762074080877, "cdate": 1762074080877, "tmdate": 1762919692871, "mdate": 1762919692871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}