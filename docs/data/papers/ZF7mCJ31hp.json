{"id": "ZF7mCJ31hp", "number": 23314, "cdate": 1758342039323, "mdate": 1759896821526, "content": {"title": "Contrastive Conditional–Unconditional Alignment for Long-tailed Diffusion Model", "abstract": "Training data for class-conditional image synthesis often exhibit a long-tailed distribution with limited images for tail classes. Such an imbalance causes mode collapse and reduces the diversity of synthesized images for tail classes. For class-conditional diffusion models trained on imbalanced data, we aim to improve the diversity and fidelity of tail class images without compromising the quality of head class images. We achieve this by introducing two simple but highly effective loss functions. Firstly, we employ an Unsupervised Contrastive Loss (UCL) utilizing negative samples to increase the distance/dissimilarity among synthetic images. Such regularization is coupled with a standard trick of batch resampling to further diversify tail-class images. Our second loss is an Alignment\nLoss (AL) that aligns class-conditional generation with unconditional generation at large timesteps. This second loss makes the denoising process insensitive to class conditions for the initial steps, which enriches tail classes through knowledge sharing from head classes. We successfully leverage contrastive learning and conditional-unconditional alignment for class-imbalanced diffusion models. Our framework is easy to implement as demonstrated on both U-Net based architecture and Diffusion Transformer. Our method outperforms vanilla denoising diffusion probabilistic models, score-based diffusion model, and alternative methods for class-imbalanced image generation across various datasets, in particular ImageNet-LT with 256×256 resolution.", "tldr": "Leverage contrastive learning to improve class-imbalanced diffusion model for image generation in particular for tail classes", "keywords": ["Diffusion Model", "Long-tailed distribution", "Contrastive Learning", "Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed72e581a66815a31a9d5594909ce9a4e91f6f8e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new framework named CCUA (Contrastive Conditional–Unconditional Alignment) to address the challenges of mode collapse and insufficient diversity or fidelity in conditional diffusion models (e.g., DDPM, SBDM) trained on long-tailed datasets. CCUA introduces two key loss components: (1) an Unsupervised Contrastive Loss (UCL) that operates in the unconditional latent space using only negative samples to enhance intra-class diversity by increasing the distance among generated samples; and (2) an Alignment Loss (AL) that aligns conditional and unconditional noise estimations during early denoising steps, facilitating knowledge transfer from head classes to tail classes. The proposed method is compatible with both U-Net and Transformer-based diffusion architectures, and demonstrates superior performance on multiple long-tailed benchmarks, including ImageNet-LT, Tiny ImageNet-LT, and Places-LT, particularly in improving diversity (Recall) and fidelity (FID) for tail classes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an innovative approach to applying contrastive learning in diffusion models by using only negative samples, rather than both positive and negative pairs. Moreover, it creatively adapts the conditional–unconditional alignment concept from GANs to diffusion models, enabling knowledge sharing during early denoising stages — a notable conceptual transfer.\n\n2. The proposed method achieves significant quantitative improvements across multiple long-tailed datasets, particularly in Recall (diversity) metrics, demonstrating its effectiveness in alleviating mode collapse and enhancing sample diversity.\n\n3. The CCUA framework is clearly structured with two distinct loss components and is easy to integrate into existing DDPM/SBDM training pipelines. It is compatible with both U-Net and DiT architectures, and the paper provides a clear explanation of how UCL mitigates mode collapse.\n\n4. The work provides a general and effective regularization strategy for addressing class imbalance in conditional diffusion models. It represents a meaningful advancement in improving diffusion models for real-world, long-tailed data distributions."}, "weaknesses": {"value": "1. The paper claims to improve the generation quality of tail classes while maintaining the fidelity of head classes. However, key quantitative tables (e.g., Table 1) only report overall FID, IS, Precision/Recall, and $FID_{tail}$. To fully substantiate this claim, it would be important to include metrics directly reflecting the quality of head-class generations, such as $FID_{head}$.\n\n2. In Section 2, the paper states that “our unsupervised contrastive loss is formulated in the unconditional latent space and is implicitly extended to conditional training through our conditional-unconditional alignment loss.” This statement is somewhat unclear, as Equation (7) in Section 3.3 defines $\\mathcal{L}\\_{ucl}$ and $\\mathcal{L}\\_{al}$ as two independent weighted losses. The mechanism by which $\\mathcal{L}\\_{al}$​ implicitly extends or interacts with $\\mathcal{L}\\_{ucl}$​ is not sufficiently explained.\n\n3. As mentioned in Section 2, _Dispersive Loss_ was originally designed for balanced datasets rather than long-tailed ones. A stronger and more convincing comparison would adapt _Dispersive Loss_ to the long-tailed setting to better isolate the specific contribution of the proposed alignment loss $\\mathcal{L}\\_{al}$​.\n\n4. To apply $\\mathcal{L}\\_{ucl}$, the paper divides the noise estimation network $\\epsilon\\_{\\theta}$​ into an encoder and a decoder. While this separation is natural for the U-Net architecture, the definition becomes less clear for the Transformer-based SiT, where all SiT/DiT blocks are treated as the encoder and only the final linear layer as the decoder. The rationale for treating all blocks’ outputs as the contrastive latent space, rather than using a bottleneck or intermediate representation, is not well justified or empirically validated."}, "questions": {"value": "1. The paper computes the UCL in the unconditional latent space $e(\\*;\\emptyset)$ and transfers its effect to the conditional model via $\\mathcal{L}\\_{al}$​. Could the authors clarify why UCL is not directly applied in the conditional latent space $e(\\*;c)$? A theoretical justification or comparison experiment would help explain whether applying UCL directly in the conditional space would indeed degrade performance.\n\n3. In the experimental setup (Section 4.1), the compared methods for U-Net architectures using contrastive learning or explicit diversity enhancement appear to be limited to works published in or before 2024. Given the rapid progress in this area, it would be helpful to discuss or compare with more recent (2025 or latest) SOTA diffusion models that share similar motivations.\n\n4. The proposed UCL computes the contrastive objective using only negative samples, aiming to enhance diversity by maximizing inter-sample distances. However, standard contrastive learning typically includes positive-pair attraction terms. It would be useful to provide further justification or an ablation study to clarify whether including positive pairs (same-class samples) would negatively affect long-tailed generation, and whether relying solely on repulsive forces might lead to overly dispersed latent representations (i.e., excessive intra-class variance)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8o6MYTcqpj", "forum": "ZF7mCJ31hp", "replyto": "ZF7mCJ31hp", "signatures": ["ICLR.cc/2026/Conference/Submission23314/Reviewer_jyaR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23314/Reviewer_jyaR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479769609, "cdate": 1761479769609, "tmdate": 1762942600591, "mdate": 1762942600591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of long-tailed image generation with diffusion models. The authors observe that diffusion models trained on long-tailed datasets (e.g., ImageNet-LT) tend to produce low-quality and less diverse images for tail classes. To address this, they propose a new regularization framework called CCUA (Contrastive Conditional–Unconditional Alignment), which combines (1) an Unsupervised Contrastive Loss (UCL) to promote feature dispersion and prevent latent collapse, and (2) an Alignment Loss (AL) to align conditional and unconditional noise predictions in the early timesteps of diffusion, enabling structural knowledge transfer from head to tail classes. Extensive experiments demonstrate that the proposed framework improves the generation quality of diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper effectively integrates multiple techniques to address the long-tailed generation problem. The approach is well motivated, drawing inspiration from prior research on class-imbalance GANs, and demonstrates empirical effectiveness.\n- The method consistently performs well across both UNet-based architectures and Diffusion Transformers (DiT), highlighting its generalization capability.\n- The paper is well-structured, with intuitive visualizations illustrating improved tail-class diversity and fidelity."}, "weaknesses": {"value": "- As acknowledged in the paper, the training process requires a longer duration.\n- Loss-function ablation is evaluated only with FID, lacking complementary metrics (IS, Precision/Recall, FID-tail) to substantiate the claimed diversity–fidelity balance.\n- While the results are promising, it remains somewhat unclear how much of the improvement stems from the proposed CCUA loss compared to the batch resample strategy. Since resampling already provides gains with minimal cost, a controlled comparison isolating CCUA’s effect would further clarify its independent contribution and better justify the added training overhead."}, "questions": {"value": "- In the ablation study on the loss function, it would be helpful to include additional evaluation metrics such as Inception Score (IS) and Precision/Recall to provide a more comprehensive analysis.\n- Would it be possible to include an experiment similar to Table 5 on the TinyImageNet-LT dataset to further validate the method’s generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FfLqRC68lU", "forum": "ZF7mCJ31hp", "replyto": "ZF7mCJ31hp", "signatures": ["ICLR.cc/2026/Conference/Submission23314/Reviewer_2xLz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23314/Reviewer_2xLz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542622266, "cdate": 1761542622266, "tmdate": 1762942600377, "mdate": 1762942600377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CCUA, a framework designed to improve the performance of class-conditional diffusion models trained on long-tailed datasets. CCUA combines unsupervised contrastive loss and alignment loss together to address the problem that models trained on imbalanced data tend to suffer from mode collapse and reduced fidelity for tail classes. The method shows empirical enhancement on ImageNet-LT,Places-LT and CIFAR-LT variants."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The two proposed losses are intuitive and well-motivated and the method is also simple to implement, plus easy to integrate into existing diffusion pipelines.\n\n2. The empirical evidence is extensive: results span multiple strong baselines, architectures (U-Net, DiT), and datasets. CCUA consistently outperforms prior work, especially in terms of FID for tail classes and diversity (Recall), as seen in Table 1, Table 2, and Table 3."}, "weaknesses": {"value": "The primary weakness is that the paper's contribution appears to be an assemblage of existing or concurrent ideas rather than a fundamental innovation. The first component, UCL, is a dispersive loss. The authors themselves state it is \"similar to\" concurrent work on dispersive losses. The second component, AL, is admittedly a direct adaptation from the long-tailed GAN literature. The paper's insight is to translate the \"low-resolution alignment\" from GANs to \"large timestep alignment\" in diffusion, which is a reasonable but straightforward adaptation. Therefore, the core contribution is limited to combining these two specific losses into an integrated loss, which feels more incremental than foundational.\n\nThe paper provides insufficient justification and no deep mathematical analysis for why this specific combination of UCL and AL is optimal. The two losses are simply added together with weighting hyperparameters. There is no deeper analysis or theoretical argument for how these two losses interact or why they form a principled, unified framework, beyond the high-level intuition.\n\nThe method introduces a major practical drawback. The paper's \"Limitation\" section (Appendix, page 15) explicitly states the method increases training time significantly (1.6x for DDPM, 1.48x for SiT) due to the two forward passes required for the Alignment Loss.\nFurthermore, UCL (a contrastive-style loss) typically benefits from large batch sizes to see a rich set of negatives. However, the 1.5-1.6x increase in computational cost from the AL component would likely force a reduction in batch size. The paper fails to analyze or discuss this practical tension.\n\nThe paper's empirical claims are not sufficiently stress-tested, which is a significant flaw for a method that relies on a weighted-loss combination. The paper completely lacks a sensitivity analysis for its two most critical hyperparameters, $\\alpha$ and $\\gamma$. It is impossible to know if the strong results are due to extensive, unreported tuning or if the method is robust. The robustness analysis is very limited. It only tests robustness to random seeds (Appendix Table 10) and fails to analyze performance across different imbalance factors to see how it performs as the long-tail problem becomes more or less severe.The evaluation metrics, while good, are limited. For generative models, metrics like CLIP Score are often vital for judging image-concept alignment, especially for diverse tail classes, but this is missing from the evaluation.\n\nThe paper fails to adequately discuss or compare against highly relevant prior work on contrastive diffusion (e.g., Yan et al., 2022) and other conditional diffusion models (Wang et al., 2023)."}, "questions": {"value": "See weekness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mM8rBIkylS", "forum": "ZF7mCJ31hp", "replyto": "ZF7mCJ31hp", "signatures": ["ICLR.cc/2026/Conference/Submission23314/Reviewer_yVZe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23314/Reviewer_yVZe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728085358, "cdate": 1761728085358, "tmdate": 1762942600172, "mdate": 1762942600172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of training class-conditional diffusion models on long-tailed datasets, where tail classes suffer from poor sample quality and diversity (mode collapse) due to data scarcity. The authors propose a framework named Contrastive Conditional–Unconditional Alignment (CCUA), which introduces two novel loss functions to the standard diffusion model training pipeline. The authors demonstrate the effectiveness of CCUA by integrating it into both U-Net and Diffusion Transformer (DiT/SiT) architectures. Extensive experiments on long-tailed versions of CIFAR, TinyImageNet, Places, and ImageNet show that CCUA significantly improves generation quality (FID) and diversity (Recall) for tail classes, often without compromising (and sometimes improving) the quality of head classes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles the practical problem of generative model performance on imbalanced, real-world data. The proposed method is simple and intuitive.\n- The method is validated on a wide range of standard long-tailed datasets (ImageNet-LT, Places-LT, TinyImageNet-LT, etc.) at various resolutions, including challenging 256x256 generation. \n- The paper is well-written, clearly structured, and easy to follow."}, "weaknesses": {"value": "1. Training Cost: The alignment loss (Lal) requires a forward pass for both the conditional and unconditional predictions for each sample during training. The appendix (line 802) states this results in a ~1.6x increase in training time per epoch compared to a standard DDPM. This is a significant computational overhead that should be mentioned and discussed as a limitation in the main paper, not just in the appendix.\n\n2. Justification for Latent Space h: The definition of the encoder e(*) that produces the latent h for the contrastive loss feels somewhat arbitrary, especially for the Diffusion Transformer. Some justification for why this specific feature level is optimal for applying the repulsive force would strengthen the paper. It is unclear if other layers were considered.\n\n3. Unclear Rationale for Hyperparameter Choices: \n- The loss weighting parameters $\\alpha$ and $\\lambda$ are introduced but their values, selection process, and sensitivity are not discussed.\n- The appendix (line 723) mentions that for TinyImageNet-LT and Places-LT, the UCL was also weighted by t/T, similar to the AL. This seems like a critical implementation detail. The rationale for applying this timestep weighting to UCL for some datasets but not others is not provided, which raises questions about the method's generality and the tuning required for new datasets."}, "questions": {"value": "1. Regarding the training overhead, you mention a 1.6x increase in training time in the appendix. Could you please discuss this limitation in the main paper? Is it possible that the improved convergence speed (e.g., reaching a better FID in fewer epochs, as seen in Table 1) partially amortizes this per-epoch cost? What's the performance gap between baselines and CCUA given the same training budget measured in GPU$\\times$Day?\n\n2. Could you elaborate on the design choice for the latent representation h used in the UCL? Specifically for the Diffusion Transformer, why was the output of the final transformer block chosen? Did you experiment with applying the contrastive loss at different depths of the network, and how did that affect performance? Moreover, could you explain why \n\n3. In Table 3 (TinyImageNet-LT), CCUA shows a slightly higher FID on 'Head' classes compared to some baselines. Could you comment on this small performance degradation and whether there is a fundamental trade-off between improving tail classes and maintaining head class performance?\n\n4. The alignment loss L_al is linearly weighted by t/T. Have you experimented with other weighting schedules (e.g., quadratic, or a step function that applies the loss only for t > T_threshold)? How sensitive is the model's performance to this specific linear schedule?  \nIn the appendix (line 723), you note that the UCL was also weighted by t/T for the TinyImageNet-LT and Places-LT datasets. This detail is not mentioned in the main method description. Could you clarify why this was necessary for these specific datasets and not for ImageNet-LT? This seems to suggest the method's components might require dataset-specific tuning.\n\n5. How's the performance of CCUA w.r.t. the imbalanced factor (e.g., 0.001 as in OCLT)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NQmISyjGrS", "forum": "ZF7mCJ31hp", "replyto": "ZF7mCJ31hp", "signatures": ["ICLR.cc/2026/Conference/Submission23314/Reviewer_KAa1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23314/Reviewer_KAa1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911116660, "cdate": 1761911116660, "tmdate": 1762942599904, "mdate": 1762942599904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}