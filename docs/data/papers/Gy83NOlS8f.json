{"id": "Gy83NOlS8f", "number": 11202, "cdate": 1758193223070, "mdate": 1763738907417, "content": {"title": "Analysis of approximate linear programming solution to Markov decision problem with log barrier function", "abstract": "There are two primary approaches to solving Markov decision problems (MDPs): dynamic programming based on the Bellman equation and linear programming (LP). Dynamic programming methods are the most widely used and form the foundation of both classical and modern reinforcement learning (RL). By contrast, LP-based methods have been less commonly employed, although they have recently gained attention in contexts such as offline RL. The relative underuse of the LP-based methods stems from the fact that it leads to an inequality-constrained optimization problem, which is generally more challenging to solve effectively compared with Bellman-equation-based methods. The purpose of this paper is to establish a theoretical foundation for solving LP-based MDPs in a more effective and practical manner. Our key idea is to leverage the log-barrier function, widely used in inequality-constrained optimization, to transform the LP formulation of the MDP into an unconstrained optimization problem. This reformulation enables approximate solutions to be obtained easily via gradient descent. While the method may appear naive, to the best of our knowledge, a thorough theoretical interpretation of this approach has not yet been developed. This paper aims to bridge this gap.", "tldr": "", "keywords": ["Markov decision programming", "reinforcement learning", "linear programming", "dynamic programming"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c044305558f53e00304153731ed8a15f85dec90c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel framework for solving Markov Decision Problems (MDPs) through a linear programming (LP) formulation augmented with a log-barrier function.\nThe authors show that this reformulation converts the constrained LP into an unconstrained optimization problem, which can then be efficiently solved using gradient descent.\nTheoretical results include upper and lower error bounds on the approximate Q-function and policy performance, both shown to scale linearly with the barrier parameter.\nThe framework is extended to deep reinforcement learning, yielding new variants of DQN and DDPG that replace the standard Bellman loss with a log-barrier–based objective.\nExperimental results demonstrate competitive or superior performance in several OpenAI Gym and MuJoCo tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The application of log-barrier methods to the LP formulation of MDPs is original and fills a gap in reinforcement learning literature, connecting classical optimization theory to RL in a principled way.\n\nExtending the framework to DQN and DDPG variants demonstrates the practical potential of the theoretical findings.\n\nThe empirical results show tangible benefits in stability and performance, particularly in complex continuous-control tasks."}, "weaknesses": {"value": "The experiments are confined to a few benchmark environments.\nBroader testing (e.g., across more complex or stochastic settings) would strengthen the empirical claims.\n\nThere is little analysis isolating the contribution of the log-barrier term relative to standard LP or Bellman-based objectives."}, "questions": {"value": "How sensitive is the convergence behavior to the choice of the barrier parameter $\\eta$ and step-size in practice?\nCould adaptive or annealing strategies improve stability?\n\nCan the proposed approach be generalized to constrained RL or safe RL problems where inequality constraints are inherent to the objective?\n\nHave the authors considered how the proposed loss compares, in practice, to other convex formulations such as convex Q-learning or logistic Q-learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x9UblSdK07", "forum": "Gy83NOlS8f", "replyto": "Gy83NOlS8f", "signatures": ["ICLR.cc/2026/Conference/Submission11202/Reviewer_fCTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11202/Reviewer_fCTj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760711234044, "cdate": 1760711234044, "tmdate": 1762922351800, "mdate": 1762922351800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a log-barrier-based reformulation of the LP formulation for MDPs, replacing the inequality constraints with a single-objective function fη. The authors provide a thorough theoretical analysis of the approximation error, convergence and policy performance, and extend the framework to deep reinforcement learning (RL) via novel DQN and DDPG variants with empirical validation on Gymnasium and MuJoCo tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of log-barrier functions to transform the LP formulation of MDPs into an unconstrained problem is innovative and a comprehensive theoretical interpretation has not been widely explored in the RL literature.\n\n2. The paper provides a comprehensive theoretical foundation, including error bounds, convergence analysis and policy performance guarantees. The proofs are detailed and well-structured in the appendix.\n\n3. The paper is well-organized with a clear statement of formulation and a clear presentation of analysis.\n\n4. The authors evaluate the proposed method on both discrete and continuous control real-world tasks, demonstrating the competitive or improved performance over standard DQN and DDPG."}, "weaknesses": {"value": "In general, broader evaluation on more real-world datasets is needed to strengthen the claims."}, "questions": {"value": "1. How does the computational cost of the log-barrier approach scale with the size of the state and action spaces? Are there any extreme settings where the method becomes computational costly?\n\n2. What is the performance of log-barrier function method compared to other LP-based or primal-dual RL methods in terms of sample efficiency and computation time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pP881yMSPn", "forum": "Gy83NOlS8f", "replyto": "Gy83NOlS8f", "signatures": ["ICLR.cc/2026/Conference/Submission11202/Reviewer_csmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11202/Reviewer_csmF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847653369, "cdate": 1761847653369, "tmdate": 1762922351294, "mdate": 1762922351294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new approach to solving the Bellman optimality equation in tabular Markov decision processes.\nInstead of addressing the problem iteratively via dynamic programming, the authors formulate it as an LP.\nThey then reformulate the LP—with its inequality constraints—into an unconstrained optimization problem using a log-barrier, and obtain a solution by gradient descent.\nLeveraging the convexity of the log-barrier function, the paper shows that the resulting objective is strongly convex (and hence strictly convex), which in turn justifies the effectiveness of gradient-descent–based optimization.\nMoreover, by establishing the relationship between the approximation error of the proposed solution and the barrier parameter, the paper demonstrates convergence toward the optimal solution.\nEmpirically, the authors validate the approach by modifying the DQN objective with a log-barrier term and evaluating performance across several environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. In terms of originality, this work investigates an LP-based route that has been relatively underexplored for solving the Bellman optimality equation. In particular, it introduces a new method that applies log-barrier techniques from convex optimization and develops a corresponding theoretical foundation.\n\n2. While I have not thoroughly verified every proof in the appendix, the paper provides convergence guarantees for the proposed method. This foundation appears promising for subsequent research in the RL community on LP-based planning and related topics.\n\n3. The manuscript is written clearly and is relatively easy to follow. The experimental section supports the claimed advantages of the method."}, "weaknesses": {"value": "1. Motivation of the proposed approach should be supplemented. In one sentence, the proposed approach can be viewed as: cast the Bellman optimality equation as a constrained LP, convert it to an unconstrained problem with a log-barrier, and solve it by gradient descent. Although the theoretical foundation is established, the motivation for preferring this route over classical DP is not fully developed. A discussion comparing convergence speed and computational complexity to DP-based methods would strengthen the case for the approach.\n\n2. More direct empirical validation should be added. Additional experiments could more directly isolate the contribution of the log-barrier loss. As the authors note, the log-barrier DQN differs from standard DQN not only in its loss but also, for example, in not using a target network. Hence, it is not entirely clear that the observed results are driven by the barrier term itself. A tabular setting directly comparing the proposed method to DP would provide more definitive evidence of its utility."}, "questions": {"value": "1. On line 128, $\\pi^\\ast$ is defined as the greedy policy with respect to the optimal value, whereas on line 182 it is described via a probabilistic policy induced by the dual optimal variables. If $\\pi^\\ast$ is deterministic, can it still be represented in the form given on line 182 through $\\lambda^*$?\n\n2. As I understand it, $w$ functions like a hyperparameter, similar to $\\eta$. Beyond the uniform choice (e.g., $w = 1/(|S||A|^2)$), what alternatives are reasonable, and what implementation differences would they entail? How is this parameter chosen in the actual experiments?\n\n3. I do not fully understand the claim that standard DQN struggles with error propagation at sharp decision boundaries, while the log-barrier LP approach avoids this issue (line 396-401). Could the authors provide experimental evidence that directly supports this explanation?\n\n4. Intuitively, the log-barrier imposes a strong penalty for violating Bellman constraints, controlled by $\\eta$. Would using other penalties such as hinge or softplus lead to different empirical behavior or performance?\n\n5. Why does the proposed method underperform (or perform less favorably) on the Hopper task? What characteristics of Hopper differentiate it from the other tasks in ways that affect the method?\n\n6. Figure 3 suggests that smaller $\\eta$ yields tighter bounds. In practice, should $\\eta$ therefore be set as small as possible? What criteria were used to select the barrier parameter in Tables 1 and 2?\n\n[Minor]\n\n1. On line 209 the log-barrier function is denoted by $\\phi$, whereas in Eq. (4) it is written as $\\varphi$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ccQvobHzUR", "forum": "Gy83NOlS8f", "replyto": "Gy83NOlS8f", "signatures": ["ICLR.cc/2026/Conference/Submission11202/Reviewer_Khvy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11202/Reviewer_Khvy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879374223, "cdate": 1761879374223, "tmdate": 1762922350142, "mdate": 1762922350142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reformulates the Q-LP for discounted MDPs using a log-barrier to combine all Bellman inequality constraints into a single unconstrained objective. For this unconstrained optimization problem, the paper studies its theoretical results and proves that both the upper and lower bounds scale linearly with the barrier parameter. They also propose barrier-style DQN/DDPG variants and report empirical gains on several MuJoCo tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Theoretical soundness: Provides a clean log-barrier reformulation of the Q-LP, with proofs of convexity/strong-convexity on sublevel sets, Lipschitz gradients, exponential gradient-descent convergence, and linear-in-$\\eta$ upper/lower bounds.\n\n2. Conceptual clarity: The problem is well specified and the pipeline—Q-LP → barrier objective → first-order conditions → approximate duals/policies—is presented coherently and is easy to follow.\n\n3. Practical algorithmic instantiation: The barrier idea is instantiated with minimal changes to standard DQN/DDPG, making it straightforward to implement."}, "weaknesses": {"value": "At present, I do not find the contributions sufficient for ICLR. As a theory-focused paper, the motivation and insights feel limited. That said, I may be missing something, and I am happy to adjust my score if the concerns below are addressed.\n\nIn particular, my main concerns are as follows:\n\n1. Motivation and positioning. The motivation for introducing a log-barrier is not fully convincing. I appreciate that the LP formulation of MDPs is less explored, but it remains unclear what concrete advantages the log-barrier brings over standard alternatives. In Section 4, the paper asserts that Lagrangian or primal–dual methods can be slow or lack convergence guarantees, but no references are provided and there are no experimental comparisons. I suggest to expand this discussion with specific contrasts in performance metrics such as convergence speed, required assumptions, dependence on $|\\mathcal{S}|$, $|\\mathcal{A}|$, computational cost, and practical stability.\n\n2. Empirical support and ablations. The advantages over standard DQN/DDPG in Section 7 are stated but not theoretically grounded, and some claims read as conjectural. The paper also lacks ablation studies. It would help to include controlled synthetic simulations/experiments where the barrier variants have a clear, interpretable edge, compared to standard benchmark methods. \n\n3. The theoretical results do not appear to extend to the deep RL setting in Section 6, as the Q function is not guaranteed to be convex as a function of $\\theta$. I suggest to add some discussions for the neural settings."}, "questions": {"value": "1. The main theoretical results show that the upper and lower bounds scale linearly with $\\eta$, but the proposed algorithm only uses a fixed $\\eta$. Are there any potential concerns on using a decreasing sequence of $\\eta$ in the practical algorithm?\n\n2. If possible, would you list the technical contributions of the paper, such as new frameworks or proof techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GqWT64Ush8", "forum": "Gy83NOlS8f", "replyto": "Gy83NOlS8f", "signatures": ["ICLR.cc/2026/Conference/Submission11202/Reviewer_UmFB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11202/Reviewer_UmFB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908773300, "cdate": 1761908773300, "tmdate": 1762922349600, "mdate": 1762922349600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}