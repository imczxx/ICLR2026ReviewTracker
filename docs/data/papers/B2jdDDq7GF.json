{"id": "B2jdDDq7GF", "number": 22592, "cdate": 1758333204040, "mdate": 1759896857785, "content": {"title": "Noise-Aware System Identification for High-Dimensional Stochastic Dynamics", "abstract": "Stochastic dynamical systems are ubiquitous in physics, biology, and engineering, where both deterministic drifts and random fluctuations govern system behavior. Learning these dynamics from data is particularly challenging in high-dimensional settings with complex, correlated, or state-dependent noise. We introduce a noise-aware system identification framework that jointly recovers the deterministic drift and full noise structure directly from the trajectory data, without requiring prior assumptions on the noise model. Our method accommodates a broad class of stochastic dynamics, including colored and multiplicative noise, that scales efficiently to high-dimensional systems, and accurately reconstructs the underlying dynamics. Numerical experiments on diverse systems validate the approach and highlight its potential for data-driven modeling in complex stochastic environments.", "tldr": "", "keywords": ["stochastic differential equation", "system identification", "joint inference"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee3ac5434f34a7bee6979901d7c90df39259e650.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a mathematical framework for identifying both the drift and diffusion terms of stochastic differential equations directly from trajectory data. The authors derive a likelihood-based loss for the drift and a quadratic-variation loss for the diffusion, leading to a consistent and asymptotically normal estimator. They extend the approach to high-dimensional settings by parameterizing these functions with neural networks and demonstrate its performance on synthetic SDEs, interacting particle systems, and stochastic PDEs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "•\tSolid theoretical foundation grounded in stochastic analysis.\n\t•\tClear derivations linking the loss to likelihood and Radon–Nikodym principles.\n\t•\tImplementation details are transparent and reproducible.\n\t•\tNumerical tests validate correctness and convergence rates."}, "weaknesses": {"value": "•\tThe work is primarily a statistical estimation study, not a machine-learning or representation-learning contribution.\n\t•\t“Deep learning” (Sec. 3.5) is overstated: networks act only as generic function approximators, not as part of a new ML method.\n\t•\tExperiments are fully synthetic and demonstrate mathematical correctness rather than generalization or learning capability.\n\t•\tLacks any real or learned high-dimensional data relevant to ICLR (e.g., diffusion generative models, Neural SDEs).\n\t•\tNo discussion of computational scaling, sample complexity, or robustness to discrete/noisy observations.\n\t•\tThe connection to modern diffusion modeling or score-based generative learning—the key SDE context within ML—is missing."}, "questions": {"value": "1.\tHow does this framework relate to Neural SDEs or score-based diffusion models used in generative learning?\n\t2.\tHow robust is the approach when data are available only at discrete and noisy time points?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BEjTISO0zH", "forum": "B2jdDDq7GF", "replyto": "B2jdDDq7GF", "signatures": ["ICLR.cc/2026/Conference/Submission22592/Reviewer_LZm2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22592/Reviewer_LZm2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760873944559, "cdate": 1760873944559, "tmdate": 1762942294068, "mdate": 1762942294068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a noise-aware framework for identifying both the deterministic drift and the stochastic diffusion terms in high-dimensional stochastic dynamical systems directly from trajectory data. Unlike methods that treat noise as a nuisance, it jointly learns the full state-dependent and correlated noise structure alongside the drift, using a two-stage approach based on quadratic variation for the diffusion and a likelihood-based loss derived from the Girsanov theorem for the drift. The method is validated on examples like interacting particle systems and stochastic PDEs, demonstrating its ability to handle complex noise and scale to high dimensions using deep learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The general idea is easy to follow\n* The proposed method is theoretically grounded and is novel"}, "weaknesses": {"value": "* The \"Related Works\" section (1.1) should be moved to a later part of the paper. Currently, it discusses specific methods and loss functions before the core model and notation have been introduced in Section 2. This disrupts the logical flow and may confuse readers. Positioning it as an independent section after the methodology would provide the necessary context for the comparisons made.\n\n* The theoretical derivation assumes continuous-time observation. In practice, data is discrete, and the method relies on fine time discretization (Δt is small, e.g., 0.001 in examples). Its performance with sparse, irregular, or low-frequency data is not explored and would likely degrade significantly, as approximations for dx_t and quadratic variation become poor.\n\n* The two-stage process is elegant but creates a pipeline error. Any inaccuracies in estimating $\\Sigma$ will propagate into and bias the subsequent drift estimation $f$, as the drift loss function depends on $\\Sigma^{-1}$. The paper does not analyze the sensitivity of the final result to errors in the first stage.\n\n* All experiments use synthetic data with known ground truth. There is no evaluation on empirical datasets."}, "questions": {"value": "* The authors claim superior performance, but the paper lacks comparisons against established baselines. How does the method quantitatively compare against, for instance, a well-tuned Neural SDE or a recent variant of SINDy for SDEs on your own benchmarks?\n\n* You highlight handling \"correlated and state-dependent noise,\" yet key examples use diagonal (IPS) or additive (SPDE) noise. Can you demonstrate the method's performance on a system with a full, non-diagonal, state-dependent diffusion matrix?\n\n* The loss function for the drift is derived from the Girsanov theorem, which is a known concept in stochastic processes. What is the specific algorithmic novelty here? Is it the joint framework, the specific decoupling of the estimation, or the application to high-dimensional learning with NNs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mxNGwM9Bj5", "forum": "B2jdDDq7GF", "replyto": "B2jdDDq7GF", "signatures": ["ICLR.cc/2026/Conference/Submission22592/Reviewer_XqkQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22592/Reviewer_XqkQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734365962, "cdate": 1761734365962, "tmdate": 1762942293765, "mdate": 1762942293765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a noise-aware framework for identifying both the drift and diffusion terms in high-dimensional stochastic dynamical systems from trajectory data. The method is derived from the Girsanov theorem and the Radon–Nikodym derivative, leading to a likelihood-based loss that allows simultaneous estimation of the deterministic and stochastic components without assuming the specific form of the noise model. The authors validate their approach on several examples, including interacting particle systems and stochastic PDEs, and provide theoretical convergence guarantees."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper provides a solid derivation of the drift loss based on stochastic process theory, which is different from the existing methods.\n\n2. Demonstrations on both finite-dimensional and PDE-type stochastic systems show good performance of the proposed method."}, "weaknesses": {"value": "1. The paper does not compare with established SDE inference methods such as [R1],[R2],[R3] . Without such benchmarks, it is difficult to assess how much improvement the proposed method offers beyond the existing works.\n\n2. Although the introduction mentions physics, biology, and finance, the experiments are purely toy models. It would be useful to explore whether the method could be applied to financial time series or stock dynamics, where stochastic modeling is central, or to other real data domains such as EEG brain signals.\n\n3. Since the method claims to be noise-aware, it would be important to analyze its behavior under different noise magnitudes or correlated noise.\n\n__References__\n[R1] Course, K., & Nair, P. B. (2023). State estimation of a physical system with unknown governing equations. Nature, 622(7982), 261-267.\n\n[R2] Oh, Y., Lim, D. Y., & Kim, S. (2024). Stable neural stochastic differential equations in analyzing irregular time series data. arXiv preprint arXiv:2402.14989.\n\n[R3] Li, X., Wong, T. K. L., Chen, R. T., & Duvenaud, D. (2020, June). Scalable gradients for stochastic differential equations. In International Conference on Artificial Intelligence and Statistics (pp. 3870-3882). PMLR."}, "questions": {"value": "Q1: How does the proposed method compare quantitatively to recent SDE inference frameworks such as the references [R1],[R2],[R3] mentioned in weaknesses.?\n\nQ2: Can the authors demonstrate or discuss whether the learned model generalizes to real-world stochastic processes, for example, financial or biophysical data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xnPGozeQMw", "forum": "B2jdDDq7GF", "replyto": "B2jdDDq7GF", "signatures": ["ICLR.cc/2026/Conference/Submission22592/Reviewer_Qb5c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22592/Reviewer_Qb5c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744934805, "cdate": 1761744934805, "tmdate": 1762942293541, "mdate": 1762942293541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new framework for the estimation of SDEs via a novel loss function, which is derived from a suitable negative log-likelihood. The authors derive a convergence result in the case where the estimator belongs to a hypothesis class with finite dimension. They evaluate their approach on two synthetic examples: a system of interacting particles and the stochastic heat equation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clearly written and motivated paper\n- Theoretical guarantees: convergence results"}, "weaknesses": {"value": "- Lack of scalability\n- Validation only on synthetic data\n- No numerical comparison against the state-of-the-art in learning SDEs\n- Numerical evaluation limited to relatively small dimension not matching the claim of scalability\n- No evaluation on real-world data"}, "questions": {"value": "1/ Could you elaborate on the claim of existence and uniqueness of estimator under your formulation (line 98) ?\n\n\n2/ Can the theoretical results be maintained if one assumes trajectories that are discrete in time as is the case in practice ?\n\n\n3/ Since your method requires computing matrix square-root which is cubic in complexity, can you justify your claim of scalability to high-dimensions ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5oblceclYX", "forum": "B2jdDDq7GF", "replyto": "B2jdDDq7GF", "signatures": ["ICLR.cc/2026/Conference/Submission22592/Reviewer_4XYQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22592/Reviewer_4XYQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830701847, "cdate": 1761830701847, "tmdate": 1762942293302, "mdate": 1762942293302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}