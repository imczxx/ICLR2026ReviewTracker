{"id": "5gUMhTUDi0", "number": 24111, "cdate": 1758352848737, "mdate": 1759896781162, "content": {"title": "Sampling Complexity of TD and PPO in RKHS", "abstract": "We revisit Proximal Policy Optimization (PPO) from a function-space perspective. \nOur analysis decouples policy evaluation and improvement in a reproducing kernel Hilbert space (RKHS): \n(i)  A kernelized temporal-difference (TD) critic performs efficient RKHS-gradient updates using only one-step state–action transition samples.\n(ii) a KL-regularized, natural-gradient policy step exponentiates the evaluated action-value, recovering a PPO/TRPO-style proximal update in continuous state-action spaces. \nWe provide non-asymptotic, instance-adaptive guarantees whose rates depend on RKHS entropy, unifying tabular, linear, Sobolev, Gaussian, and Neural Tangent Kernel (NTK) regimes, and we derive a sampling rule for the proximal update that ensures the optimal $k^{-1/2}$  convergence rate for stochastic optimization.\nEmpirically, the theory-aligned schedule improves stability and sample efficiency on common control tasks (e.g., CartPole, Acrobot), while our TD-based critic attains favorable throughput versus a GAE baseline. \nAltogether, our results place PPO on a firmer theoretical footing beyond finite-dimensional assumptions and clarify when RKHS-proximal updates with kernel-TD critics yield global policy improvement with practical efficiency.", "tldr": "By framing PPO in an RKHS (kernel) setting, we provide a new analytical perspective that both deepens understanding and delivers global convergence guarantees.", "keywords": ["Kernel method", "Kernel gradient descent", "PPO", "Temporal difference"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43de4d08c44054d186d4095bc4aa6528cc88c981.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper revisits the theory of temporal difference value estimation and natural policy gradient from the perspective of function spaces. First, it sets up a value estimation setting in which the value function lies in a RKHS. Under some assumptions, the paper provides a result on the dynamic of the estimation error of the value function with a kernelized temporal difference method. Then, the kernelized TD is incorporated into an actor-critic, natural policy gradient method, for which convergence analysis is derived. Finally, the paper reports a few experiments in simple continuous control domains to validate the theory and to compare the resulting algorithmic implementation with PPO."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Establishing the sample complexity of popular algorithms PPO in general function spaces is an interesting proposition. I am not familiar with prior work on this matter and with the function-space analysis reported in the paper, so it is hard for me to evaluate the strengths of the submission beyond noting that the problem is relevant."}, "weaknesses": {"value": "- The paper may not be accessible for a general RL audience\n- Some assumptions may be unreasonable in most settings (e.g., Asm 1 and 6)\n- Practical implications of the reported results are not stated clearly\n- The empirical comparison between PPO and the proposed algorithm may be flawed or comparing the baselines in mismatching settings"}, "questions": {"value": "**Disclaimer**: My current evaluation is essentially an educated guess of someone working in (theoretical) RL but not familiar with functional analysis. As I could not understand most of the submission, I am mostly focusing my review on the presentation of the results and how they could be made more accessible for a broader audience, while I am leaving a technical evaluation of the work to reviewer that are more familiar with the employed mathematical tools and prior works.\n\n----\n\n**Evaluation**: Assuming a solid technical contribution, I think the paper shall improve the presentation in a few aspects to clear the bar for acceptance. It shall discuss the practical implication of the reported theoretical results, it shall clarify how the analysis advance over prior works more directly (e.g., comparison of convergence rates in the same function space), it shall clarify why the reported results are important and valuable going forward.\n\nIn terms of motivation, I am not sure the paper aims at providing a theoretical support for new algorithmic solutions that advance over PPO and others or more of a theoretical understanding on when (in which domains/settings) is PPO expected to work/be efficient.\n\n----\n\n**Comments**\n\n1. I am not sure why the title and abstract suggest that the paper provides an analysis of PPO, when the actual algorithm that is being analyzed looks quite different than PPO. Perhaps natural policy gradient would be more appropriate.\n\n2. Assumption 1 and 6 look quite strong to me. Everything, evaluation/optimization, is easier when good data are available, but a crucial challenge of RL is to collect them from interaction with the MDP.\n\n3. What does assumption 2 mean in more intuitive terms?\n\n4. Not easy to understand what Theorem 9 and its corollary imply. I understand that the \"training loss\" decreases with a certain function of $n$, although the actual rate is not trivial to grasp. Is $\\beta$ capturing a property of the function space or a parameter of the approach?\n\n5. How do the results in Corollary 10 relate with previous works?\n\n6. Why in eq. 21 the return of a policy is computed in expectation over the state distribution of an optimal policy, instead of the state distribution induced by the policy itself?\n\n7. Experiments: If I understand it correctly, the comparison between PPO and the proposed algorithm is spurious. It looks like PPO is running with trajectories collected on-policy, while the proposed algorithm gets data from some sort of reset distribution. Learning from a \"good\" reset distribution is way easier than learning from actual rollouts. Can the authors clarify how the experiments are indeed comparing apples to apples?\n\ntypos and minor:\n- l139: \"and it is equipped this space with the bilinear form\" something is off with the sentence construction\n- l151: \"We first impose the following assumption on the Markov chain\". Which Markov chain?\n- l235: \"Derivethe\"\n- l260: \"Unclear whether C_1 and C_2 are universal constants or coming from Asm.1\"\n- l372: misplaced dot\n- l406: missing bracket\n- l454: \"learning Q\" -> \"to learn Q\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PKy3YCmd2v", "forum": "5gUMhTUDi0", "replyto": "5gUMhTUDi0", "signatures": ["ICLR.cc/2026/Conference/Submission24111/Reviewer_4UFk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24111/Reviewer_4UFk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668294103, "cdate": 1761668294103, "tmdate": 1762942942475, "mdate": 1762942942475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies an actor-critic method akin to TRPO/PPO in an RKHS, which integrates TD learning in a kernelized way as critic with a KL-regularized natural policy gradient update. It is related to a kernelized version of natural actor-critic method. On the technical side, the paper establishes sample complexity bounds need per policy optimization step to achieve a target error."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The per-step sample complexity bounds in the paper are definitely interesting and important.\n- The analysis of kernelized TD critic is clean and rigorous. The connection to the metric entropy in the RKHS provides a unified perspective, which is valuable.\n- The approach to connect TD learning with NPG-PPO-style policy improvement updates is classic yet elegant."}, "weaknesses": {"value": "- Assumption 6 is a little unusual as it controls the state transitions per step. The situation worsens for weakly-ergodic MDPs with poor mixing times. It can be worth discussing further justifications, and benign and pathological extremal cases.\n- The dependency on the effective time-horizon $1-\\gamma$ is a little obscure. In the bounds, does effective time-horizon appear in terms of $1-c\\gamma$, or are there any hidden multiplicative factors of $(1-\\gamma)^{-1}$?"}, "questions": {"value": "- Does the analysis extend to multi-step TD or TD($\\lambda$) for $\\lambda \\in (0,1]$ with eligibility traces?\n- A key component of (natural) policy gradient algorithms is exploration. This is usually established by assuming finite concentrability coefficients, or using entropic regularization with a fixed regularization coefficient. In this paper, how is sufficient exploration guaranteed to avoid strictly suboptimal and near-deterministic policies? The role of KL-regularization from an exploration perspective could be discussed further.\n- In conjunction with the previous point, can the analysis here be extended to entropic regularization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fttytWYOiG", "forum": "5gUMhTUDi0", "replyto": "5gUMhTUDi0", "signatures": ["ICLR.cc/2026/Conference/Submission24111/Reviewer_L5Ur"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24111/Reviewer_L5Ur"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818032664, "cdate": 1761818032664, "tmdate": 1762942942239, "mdate": 1762942942239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits Proximal Policy Optimization (PPO) from a function-space perspective by introducing a kernelized temporal-difference (TD) method for policy evaluation and a KL-regularized natural-gradient method for policy improvement, both within a Reproducing Kernel Hilbert Space (RKHS). The authors provide non-asymptotic error bounds and sample complexity analysis, showing that the proposed framework improves stability and sample efficiency in standard control tasks (such as CartPole and Acrobot). The results suggest that this method places PPO on a more solid theoretical foundation, extending it beyond finite-dimensional assumptions and clarifying when kernelized updates with TD critics yield global policy improvement with practical efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* *Theoretical Innovation*: The paper makes a significant contribution by combining kernel methods with reinforcement learning, particularly through the use of kernelized TD for policy evaluation and KL-regularized natural gradient updates for policy improvement. This approach successfully addresses theoretical gaps in PPO and similar algorithms when dealing with complex, non-linear function approximators.\n* *Comprehensive Analysis*: The paper offers a detailed analysis of sample complexity and convergence rates, providing a unified framework for understanding PPO's behavior in RKHS. The analysis is not limited to simple models but extends to Sobolev spaces, NTK, and deep neural networks, offering theoretical guarantees for various function spaces.\n* *Empirical Validation*: The experimental results strongly support the theoretical analysis, demonstrating that the proposed method outperforms the standard PPO algorithm in terms of sample efficiency and computational efficiency. The experiments on benchmark control tasks such as CartPole and Acrobot show that the theoretical predictions align well with empirical performance.\n* *Practical Relevance*: The paper provides not only theoretical insights but also practical guidance on how to improve PPO and similar reinforcement learning algorithms. The proposed methods are directly applicable to large-scale reinforcement learning tasks that require efficient sample usage and computational performance."}, "weaknesses": {"value": "* *Assumption of Bounded Distributions*: One of the key assumptions in the paper is that the distribution $\\sigma(s,a)$ is bounded. While this assumption simplifies the theoretical analysis, it may not hold in all real-world scenarios, especially when dealing with highly variable or concentrated distributions. Further discussion on how the algorithm behaves under weaker or less restrictive assumptions would be valuable.\n* *Computational Complexity Analysis*: While the paper provides a thorough theoretical analysis, it lacks an in-depth discussion on the computational complexity of the proposed method, especially for high-dimensional problems. As the state and action spaces grow, it would be useful to analyze how the algorithm scales in terms of computation and memory.\n* *Limited Experimental Environment*: The experiments are conducted on relatively simple control tasks (CartPole and Acrobot). While these tasks are useful for validating the method, it would be helpful to see how the proposed method performs on more complex, high-dimensional environments to better assess its scalability and robustness."}, "questions": {"value": "* *Relaxing the Assumption on Bounded*: The assumption that $\\sigma$ is bounded might not hold in all real-world applications. Have you considered how the algorithm performs if $\\sigma$ is not bounded or exhibits more variability? Are there methods to ensure the algorithm's stability and convergence even when this assumption is relaxed?\n* *Computational Efficiency and Scalability*: In the experiments, you demonstrated superior performance on standard tasks. How does the algorithm scale with respect to both computation and memory as the size of the state and action spaces increases? Are there strategies to further optimize the method for large-scale reinforcement learning problems?\n* *Nonlinear Approximators and Deep Networks*: The paper discusses the applicability of kernelized TD and RKHS methods for non-linear function approximators. Do you think this approach can be extended to more complex architectures such as deep neural networks? What are the practical challenges of applying this method to deep reinforcement learning algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f0s8Xc4Mdj", "forum": "5gUMhTUDi0", "replyto": "5gUMhTUDi0", "signatures": ["ICLR.cc/2026/Conference/Submission24111/Reviewer_eHMp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24111/Reviewer_eHMp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843969832, "cdate": 1761843969832, "tmdate": 1762942942005, "mdate": 1762942942005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical and algorithmic study of Temporal-Difference (TD) learning and Natural Policy Gradient (NPG)/PPO methods in Reproducing Kernel Hilbert Spaces (RKHS). The work unifies prior results across tabular, linear, Sobolev, Gaussian, and Neural Tangent Kernel (NTK) regimes, with empirical validation on standard control benchmarks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Provides the first comprehensive analysis of TD and PPO/NPG in infinite-dimensional RKHS, bridging classical tabular/linear theory with modern kernel and neural tangent kernel regimes. \n\n2. Introduces an efficient kernel TD evaluator that acts as an implicit preconditioner, achieving geometric convergence without cubic matrix inversion, thereby improving scalability in continuous spaces.   \n\n3.  The results elegantly connect function-space policy optimization to kernelized and neural-network representations, offering a unified view of convergence and sample complexity across different model classes.   \n\n4.  Experiments on control benchmarks (CartPole, Acrobot) support the theoretical predictions and show improved stability and efficiency compared with PPO baselines."}, "weaknesses": {"value": "1) Limited Experimental Scope: The empirical evaluation is restricted to low-dimensional toy environments, leaving scalability to high-dimensional or real-world continuous control tasks (e.g., MuJoCo, DMControl) untested.  \n\n2. Strong Theoretical Assumptions: The analysis relies on idealized assumptions such as bounded RKHS norms, smooth reward functions, and known transition kernels—conditions rarely satisfied in practical RL settings.    \n\n3. Sampling Distribution Dependence: The framework presumes access to controllable sampling distributions and well-behaved state–action densities (Assumptions 1 & 6), which may not hold for off-policy or non-stationary data.    \n\n4. Implicit Domain Knowledge: While the paper claims generality, the theoretical derivation implicitly assumes that the underlying domain shift or policy complexity can be quantified via RKHS norms, which may not be observable in real applications.    \n\n5. Lack of Comparative Baselines: No direct comparison with recent kernel-based RL methods or strong deep-RL algorithms (e.g., SAC, TRPO, DDPG) limits the practical interpretability of the reported advantages.    \n\n6. Algorithmic Practicality: Although the authors claim computational efficiency, the per-iteration kernel operations could still be costly for large datasets, and no analysis of memory or runtime scaling is provided."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uDiViqC91u", "forum": "5gUMhTUDi0", "replyto": "5gUMhTUDi0", "signatures": ["ICLR.cc/2026/Conference/Submission24111/Reviewer_4sex"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24111/Reviewer_4sex"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762830334312, "cdate": 1762830334312, "tmdate": 1762942941845, "mdate": 1762942941845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}