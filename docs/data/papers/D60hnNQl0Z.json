{"id": "D60hnNQl0Z", "number": 12649, "cdate": 1758209259620, "mdate": 1759897496136, "content": {"title": "Panorama: Fast-Track Nearest Neighbors", "abstract": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose embeddings are close to that of a given query in a high-dimensional space, aiming to balance accuracy with speed. Used in recommendation systems, image and video retrieval, natural language processing, and retrieval-augmented generation (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT utilize graph, tree, clustering, and quantization techniques to navigate large vector spaces. Despite this progress, ANNS systems spend up to 99% of query time to compute distances in their final refinement phase. In this paper, we present PANORAMA, a machine learning-driven approach that tackles the ANNS verification bottleneck through data-adaptive learned orthogonal transforms that facilitate the accretive refinement of distance bounds. Such transforms compact over 90% of signal energy into the first half of dimensions, enabling early candidate pruning with partial distance computations. We integrate PANORAMA into SotA ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and Annoy, with out index modification, using level-major memory layouts, SIMD-vectorized partial distance computations, and cache-aware access patterns. Experiments across diverse datasets—from image-based CIFAR-10 and GIST to modern embedding spaces including OpenAI’s Ada 2 and Large 3—demonstrate that PANORAMA affords a 2-30x end-to-end speedup with no recall loss.", "tldr": "We accelerate the refinement step in approximate nearest-neighbor search using learned orthogonal transforms.", "keywords": ["approximate nearest-neighbor search", "orthogonal transform", "energy compaction"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b6dd787210f320b6d130ce025643f96394948f6.pdf", "supplementary_material": "/attachment/c0257bc257d3862c90885ea8a8f8e2baa9238fff.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes PANORAMA, a framework to accelerate the refinement phase in Approximate Nearest Neighbor Search (ANNS). The key idea is to use a learned orthogonal transformation that concentrates the norm of the data in the first few dimensions, allowing for early candidate pruning via tight lower bounds (LB) on the L2 distance. The authors integrate PANORAMA into existing ANNS systems (IVFPQ, HNSW, MRPT, and Annoy), demonstrating empirical speedups of 2-30× while maintaining recall.\n\nThe method builds upon the idea that a well-chosen orthogonal transform T can compact signal energy, thereby yielding tighter cumulative distance bounds (via Cauchy–Schwarz inequalities). A Cayley parameterization is used to learn T, and partial distances are computed incrementally to prune candidates when the lower bound exceeds the top-k distance threshold."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is relevant: reducing the verification cost in ANNS is practically important.\n\n- The integration into multiple ANNS backends is technically solid and shows strong engineering effort.\n\n- Experimental results are extensive, and speedup claims are clearly reported."}, "weaknesses": {"value": "**W1. Incorrect runtime accounting for the learned transformation**\n\nThe learned transform T is applied both to database vectors and queries (Section 4), but in experiments, the query time does not include the transformation cost—as visible from the released code (I checked simple_benchmark.py and see transformed queries are stored separately from original queries) and described pipelines (eq (1)). Since T(q) must be computed for every query, omitting this step underestimates query latency. For large d, the transformation cost (a dense matrix–vector multiply) can dominate the supposed savings from partial distance pruning.\n\n**W2. Theoretical–empirical mismatch in the use of the lower bound**\n\nThe theoretical lower bound relies on decomposing ∥q−x∥ = ∥T(q)∥ + ∥T(x)∥ − 2 ⟨T(q),T(x)⟩ (Eq. 1). However, even if T compacts the energy of\nx and q individually, it does not guarantee that the energy of their difference ∥q−x∥ is similarly front-loaded. Thus, the learned transform may tighten bounds on norms of x, but not necessarily on distances, breaking the link between the “energy compaction” assumption (A1) and the pruning efficacy. The experiments show speedups mainly from implementation optimizations rather than LB tightness. Fig 6 seems to reflect the concentration of the norm of data points only, not the distance. I checked evaluate_all_transformed_datasets.py in the released code, which seems to confirm my findings.\n\n**W3. Novelty is limited, and the lack of relevant competitors**\n\nThe idea of leveraging partial or bounded L2 distances has been explored in several recent works, including Gao & Long (2023) and Yang et al. (2025), both cited by the authors. Those methods also used partial distance estimation or orthogonal projections to accelerate refinement. The proposed contribution proposes a new learning transformation that potentially gives tight lower-bound formulations. Hence, the improvement is primarily an engineering optimization (cache layout, batching, SIMD) rather than a new algorithmic insight. Also, the competitors are ANNS solvers without the use of lower bounds. They do not include relevant competitors (e.g. Gao & Long (2023) or standard methods: ANNS with DCT/FFT transformation) that use LB for speeding up the verification.\n\n**W4. Empirical gains not well-attributed**\n\nIt is unclear how much of the reported 2–30× speedup originates from the theoretical contribution (learned transformation and bound) versus from memory layout changes (level-major batching) or engineering effort. The lack of ablation on the cost of the learned transform further blurs this distinction.\n\n**W5. Unrealistic distributional assumptions**\n\nAssumption A3 in Section 3 states that the squared distances ∥q−x∥ follow a Gaussian distribution. This is not true for high-dimensional candidate sets where all candidates are close to q. In practice, kNN candidates are not independent random samples—they cluster around\nq. This invalidates parts of the theoretical derivation and weakens claims of Theorem 2.\n\n**Minor:**\n- Notations need to be consistent in the whole paper, i.e. candidate size (N' in Problem 1, N in Theorem 2), C in Theorem 1 and Theorem 2 and 3; so(d) vs SO(d)\n- Theorem 1 is trivial.\n- I think Subsections 4.1 and 4.2 should be significantly improved. I could not see the link between the learning objective in (6) and 4.1 subsection.\n- Text font in Algorithm is smaller than the main text."}, "questions": {"value": "Please address the raised weaknesses above, and some further questions\n\n**Q1. Query-time transformation overhead.**\n\nPANORAMA applies the learned orthogonal transformation T(q) to each query (Appendix B.5). What is the average per-query overhead when this transformation is included in the full pipeline timing (not isolated)? How significant is this cost relative to the total query latency, especially on high-dimensional datasets such as GIST or CIFAR-10?\n\n**Q2. Tightness of the lower bound.**\n\nHow tight is the proposed PANORAMA lower bound on **distance** compared to other dimension-wise partial-distance bounds such as those obtained via FFT or DCT transforms. Could you quantify the actual distance-ratio gap (i.e., LB/∥x−q∥) using the same number of reduced dimensions to better interpret the bound’s tightness beyond speedup metrics?\n\n**Q3. Threading and cache sensitivity.**\n\nAre all query experiments executed in single-threaded or multi-threaded mode? Since the proposed lower-bound (LB) pruning mechanism relies on early termination within distance accumulation, concurrent threads may interfere due to cache sharing. How sensitive is the observed speedup to the degree of threading and CPU cache hierarchy?\n\n**Q4. Training cost and data scale.**\n\nCould you specify the runtime to learn the transformation compared to the cost of building indexes? Appendix B.4 mentions training times under 20 minutes (≈ 1 hour for SIFT and Large/CIFAR-10), which seem to be too large compared to building indexes used by Faiss."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VFRXhOiI73", "forum": "D60hnNQl0Z", "replyto": "D60hnNQl0Z", "signatures": ["ICLR.cc/2026/Conference/Submission12649/Reviewer_6PQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12649/Reviewer_6PQd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761184099908, "cdate": 1761184099908, "tmdate": 1762923490618, "mdate": 1762923490618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ANNS is a highly practical research direction with extensive applications in industry. The authors of this paper propose a data-adaptive transformation method that compresses feature discriminablity (energy) into the leading dimensions without losing structural relationships. Combined with pruning, this approach reduces the computational cost of feature similarity calculations. The method can be jointly applied with all four existing classic ANNS algorithms to further enhance retrieval efficiency."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1）The research problem has very significant practical application value.\n2）The theoretical proofs are comprehensive.\n3）The proposed method is compatible with all four existing classic approaches, further improving retrieval efficiency.\n4）It not only introduces an algorithm but also provides optimization solutions at the system level."}, "weaknesses": {"value": "1）Lack of large-scale experiments. The largest dataset used in the paper is only 1 million in scale, which is relatively small for practical industrial applications. For instance, the datasets in previous NeurIPS competitions have already reached the billion scale.\n2）Generalizability of the data-driven transformation matrix. The quality of this transformation matrix is likely highly dependent on the training dataset, raising concerns about its generalization capability.\n3）Substantial reconstruction overhead when integrated with methods like IVFFlat. The underlying reconstruction effort required for integration is non-trivial."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zZgwvUIJGQ", "forum": "D60hnNQl0Z", "replyto": "D60hnNQl0Z", "signatures": ["ICLR.cc/2026/Conference/Submission12649/Reviewer_HxV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12649/Reviewer_HxV7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657585728, "cdate": 1761657585728, "tmdate": 1762923490233, "mdate": 1762923490233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of optimizing the *refinement* phrase of approximate near neighbor (ANN) search, which the authors define as identifying the top $k$ elements from an initial match set of $|\\mathcal{C}| > k$ items. The authors introduce a novel framework called Panorama to solve the refinement problem by leveraging orthogonal transforms. In particular, the authors introduce data-adaptive learned orthogonal transforms based on the Cayley transform over the Stiefel manifold that aims to concentrate vector distances in the initial dimensions and thus reduce the computational complexity of distance calculations during the refinement phase. In addition to theoretical guarantees, the authors also implement their approach by carefully considering the memory layout of the underlying ANN algorithm and report substantial speedups on the order of 2-30x for the refinement phase."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method introduced in the paper of energy-based learned orthogonal transforms is very creative and a novel application within this particular problem domain. \n\n2. The authors work in combining rigorous theoretical guarantees with high-performance implementations that consider low-level memory layout details is a strong effort in bridging theory and systems work, which is a rare combination in the retrieval literature.\n\n3. The ideas in the paper have the potential to inspire further work along this direction. \n\n4. The authors proposed approach is very general and can be applied to virtually any ANN algorithm regardless of its inner workings."}, "weaknesses": {"value": "In my opinion, the biggest weakness in the current version of the paper is an insufficient discussion of related work on this topic. The notion of refinement in retrieval and similarity search is very well studied and goes by various names such as \"reranking\" and \"approximate distance computations.\" I think it would be very helpful if the authors included a dedicated related work section that discussed prior approaches to refinement. Moreover, I believe it is critical for the authors to compare against previously published techniques in their experimental evaluation and thereby consider more rigorous baselines than naive refinement. In particular, prior works that I think are very relevant to this paper and should be discussed include: (1) [Finger](https://arxiv.org/pdf/2206.11408), (2) [Probabilistic Kernel Function for Angle Testing](https://arxiv.org/pdf/2505.20274), and (3) [A Bi-metric Framework for Fast Similarity Search](https://arxiv.org/pdf/2406.02891) (plus the broader literature on reranking techniques). I believe that addressing and experimentally evaluating against this prior literature is critical for positioning this new work appropriately. \n\nIn addition, I think it would be very helpful if the authors considered additional large-scale benchmark datasets at the 100M or 1B vector scale, such as those from Big ANN Benchmarks."}, "questions": {"value": "1. Can the authors provide a more thorough discussion of prior published work in the refinement literature, including perhaps the papers listed above (if they are in fact relevant)? I think it is critical to include this discussion in the paper in a standalone section. \n\n2. Can the authors also provide an experimental comparison with previously published refinement algorithms that go beyond naive refinement? Additional experiments on large-scale datasets, such as those from Big ANN benchmarks, might also be very helpful in supporting the claims made in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N3gFPY6KTs", "forum": "D60hnNQl0Z", "replyto": "D60hnNQl0Z", "signatures": ["ICLR.cc/2026/Conference/Submission12649/Reviewer_6394"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12649/Reviewer_6394"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988564908, "cdate": 1761988564908, "tmdate": 1762923489814, "mdate": 1762923489814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studied a method to improve the efficiency of approximate nearest neighbor search with no performance loss. The approximate nearest neighbor search algorithms are fast, but this work combines with naive kNN L2Flat (Douze et al., 2024) which performs\na brute-force kNN search over the entire dataset. The computation cost of kNN grows fast when the feature dimension of dataset goes large. The authors propose a multiple level, multiple batches method to index the dataset, the level 1 (leaf level) has M dimension which equals the number of features of the dataset.  \n\nAccording to the Algorithm 1, that the computation is linear with parameters. However, it is not sure if the memory cost is same as Problem 1 2^|D|, $|D|$ is the number of samples. For retrieval question, the challenge also comes from large number of dataset pool. If memory cost is $2^|D|$, it is not desirable. \n\nThe lower bound and upper bound as shown in Equation 3 and 4 are not informative enough. In a sense, the bound do not contribute a lot to the theoretical guarantee. \n\nAlso the main contribution is the speedup without recall loss. If the number of retrieved samples is too big, the retrieved results are not useful. The number of returned samples is not specified in the experimental results. If authors could help solve the concerns, that would be helpful."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem is interesting and with practical use. The introduction introduce the most recent works that this work is most related. That is helpful to understand the main contribution of the work.\n\nThere are multiple dimensions of experiments to validate the speedup contribution of the work."}, "weaknesses": {"value": "The lower bound and upper bound are loose, they do not show contributions to the theoretical guarantee. \n\nThe memory cost is 2^|D|, $|D|$ is the number of samples which is huge in retrieval question. The multiple-level indexing method as shown in Figure 3 seems computational expensive to me, since the retrieval requires inner product computation from tree root to leaf. It does not make use of any correlation between samples in batches, batches.\n\nThe recall loss claim does not specify the number of returned samples, if the recall loss has the trade of the number of retrieved samples, the retrieval results are not informative enough."}, "questions": {"value": "1. According to the Algorithm 1, that the computation is linear with parameters. However, it is not sure if the memory cost is same as Problem 1 2^|D|, $|D|$ is the number of samples.\n\n2. How does the lower bound and upper bound as shown in Equation 3 and 4 contribute to the theoretical guarantee?\n\n3. What is the average number of retrieval for each query for different datasets in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wiCifUONwB", "forum": "D60hnNQl0Z", "replyto": "D60hnNQl0Z", "signatures": ["ICLR.cc/2026/Conference/Submission12649/Reviewer_YBgs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12649/Reviewer_YBgs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182961778, "cdate": 1762182961778, "tmdate": 1762923489251, "mdate": 1762923489251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}