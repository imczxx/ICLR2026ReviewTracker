{"id": "ywPINvcN95", "number": 2266, "cdate": 1757044017516, "mdate": 1759898159513, "content": {"title": "Knowledge-Augmented Long-CoT Generation for Complex Biomolecular Reasoning", "abstract": "Understanding complex biomolecular mechanisms requires multi-step reasoning across molecular interactions, signaling cascades, and metabolic pathways. While large language models (LLMs) show promise in such tasks, their application to biomolecular problems is hindered by logical inconsistencies and the lack of grounding in domain knowledge. Existing approaches often exacerbate these issues: reasoning steps may deviate from biological facts or fail to capture long mechanistic dependencies. To address these challenges, we propose a Knowledge-Augmented Long-CoT Reasoning framework that integrates LLMs with knowledge graph–based multi-hop reasoning chains. The framework constructs mechanistic chains via guided multi-hop traversal and pruning on the knowledge graph; these chains are then incorporated into supervised fine-tuning to improve factual grounding and further refined with reinforcement learning to enhance reasoning reliability and consistency. Furthermore, to overcome the shortcomings of existing benchmarks, which are often restricted in scale and scope and lack annotations for deep reasoning chains, we introduce PrimeKGQA, a comprehensive benchmark for biomolecular question answering. Experimental results on both PrimeKGQA and existing datasets demonstrate that although larger closed-source models still perform well on relatively simple tasks, our method demonstrates clear advantages as reasoning depth increases, achieving state-of-the-art performance on multi-hop tasks that demand traversal of structured biological knowledge. These findings highlight the effectiveness of combining structured knowledge with advanced reasoning strategies for reliable and interpretable biomolecular reasoning.", "tldr": "", "keywords": ["Knowledge-augmented reasoning; Long chain-of-thought (Long-CoT); Knowledge graphs; Biomolecular reasoning; Interpretability in biological discovery"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48be5ea92122e1271148d648bff95a5b1fbe63d3.pdf", "supplementary_material": "/attachment/0f00823629b473b872c327612cbe11f70011bc19.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes **Bio-KCoT**, a knowledge-augmented long chain-of-thought framework that integrates knowledge graphs with LLM reasoning for complex biomolecular problems. It also introduces **PrimeKGQA**, a new benchmark designed to evaluate multi-hop biomedical reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear problem motivation: LLMs struggle with biologically grounded multi-step reasoning; the paper argues for structured KG guidance instead of pure retrieval completeness.\n\n2. Clear and coherent methodology: The pipeline is well-structured, integrating KG-guided path retrieval, CoT generation, and SFT + RL training in a logical and effective way.\n\n3. Meaningful benchmark contribution: The PrimeKGQA dataset is thoughtfully designed, with multi-level tasks that capture biologically relevant reasoning depth and enable systematic evaluation."}, "weaknesses": {"value": "1. Limited novelty and conceptual contribution: The work essentially fine-tunes a small open model (Qwen) on a domain-specific KG reasoning task using SFT and GRPO. While technically sound, the novelty beyond applying existing RLVF-style training to this setting is not strongly articulated.\n\n2. Main comparison lacks stronger baselines: The key experiment (Figure 3a) mainly compares Bio-KCoT with its own base model Qwen, which naturally benefits from 'in-benchmark' training. More direct comparisons with other reasoning-oriented or biomedical LLMs would better establish the model’s advantage and fairness.\n\n3. Insufficient ablation analysis: The ablations are limited and leave key questions unanswered: (a) there is no analysis on how SFT and RL respectively contribute to performance or how their data ratios affect results; and (b) the comparison between Bio-KCoT and distilled CoT is only shown on one benchmark (Figure 3b), without testing across datasets. These gaps make it hard to assess the necessity and effectiveness of each pipeline component."}, "questions": {"value": "The paper notes that QA pairs are generated from “head–relation pairs, with the corresponding tail entity serving as the answer.” It remains unclear how these head–relation pairs are selected from the knowledge graph and how question quality is ensured. Could the authors clarify the selection and filtering process, and whether any human validation was used to confirm that the generated questions are meaningful rather than trivial or noisy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zWV9sNQl5N", "forum": "ywPINvcN95", "replyto": "ywPINvcN95", "signatures": ["ICLR.cc/2026/Conference/Submission2266/Reviewer_A8FZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2266/Reviewer_A8FZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660320524, "cdate": 1761660320524, "tmdate": 1762916168364, "mdate": 1762916168364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Bio-KCoT, a framework that integrates knowledge graph-guided reasoning with large language models for biomolecular question answering. The approach constructs reasoning paths from knowledge graphs, generates long chain-of-thought responses, and uses a two-stage training process (SFT + GRPO). The authors also introduce PrimeKGQA, a benchmark derived from the PrimeKG knowledge graph."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Important Problem Domain: Biomolecular reasoning is a critical application area where factual accuracy and structured knowledge integration are essential. The motivation to address hallucinations and logical inconsistencies in this high-stakes domain is well-founded.\n\n2. Comprehensive Framework Design: The three-stage approach (KG path extraction → CoT generation → pruning) is systematic and well-motivated. The integration of structured knowledge with long-form reasoning addresses real limitations of current LLMs.\n\n3. Comprehensive results: show improvement not only on in domain benchmark, but on domains requiring generalization."}, "weaknesses": {"value": "1. My main concern is about benchmark construction: \n\nPrimeKGQA is constructed from PrimeKG which integrates 20 high-quality biomedical resources to describe 17,080 diseases with 4,050,249 relationships, but the benchmark creation is template-based where questions are automatically generated from head-relation-tail triples. This raises several concerns:\n\n- Artificial Task Design: The problems may not truly require complex KG reasoning or long CoT. Many could potentially be answered with parameterized knowledge without reasoning.\n- Limited Reasoning Depth: The \"difficulty\" categorization based on path length (d ≤ 5 for basic, 6-7 for medium, ≥8 for hard) may not reflect genuine reasoning complexity.\n- Lack of novelty in terms of tasks: these are all basic drug discovery tasks common in bio and molecular domains. It would be more interesting that these questions are beyond simple knowledge-based QA and have more authentic scenarios, such as research questions.\n\n\n2. Knowledge Graph Dependency: The approach is fundamentally limited by KG coverage and quality. Many biomedical questions may require reasoning beyond what's explicitly encoded in structured KGs. Can this method be extended into general retrieval of related knowledge, including online search and textual databases. Also, it lacks comparison with DeepResearch style models."}, "questions": {"value": "1. Path Quality: How do you ensure that the extracted KG paths are actually relevant for reasoning rather than just providing factual connections? e.g. for drug-drug interaction, most of the connections after a few hops may be weak and not logical.\n\n2. Reasoning vs. Retrieval: Many of the tasks shown (Table 1) appear to be factual lookups (e.g., \"Which disease can be treated with Fluvastatin?\") rather than complex multi-step reasoning. What's the average number of reasoning steps used to finish these reported results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Va5FJc2U5", "forum": "ywPINvcN95", "replyto": "ywPINvcN95", "signatures": ["ICLR.cc/2026/Conference/Submission2266/Reviewer_8tFo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2266/Reviewer_8tFo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982368348, "cdate": 1761982368348, "tmdate": 1762916168209, "mdate": 1762916168209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Bio-KCoT, a knowledge-augmented long chain-of-thought (CoT) framework for complex biomolecular reasoning. It integrates knowledge graph (KG)-guided multi-hop path retrieval and pruning with supervised fine-tuning (SFT) and reinforcement learning via Group Relative Policy Optimization (GRPO).\nThe method first extracts entities from a question and its correct answer, instantiates diverse path templates (linear, divergent, convergent) in a biomolecular KG to obtain reasoning paths, and uses these paths to guide CoT generation and subsequent pruning for conciseness and fidelity.\nThe curated CoTs supervise SFT to mitigate hallucinations and improve factual grounding; GRPO with a composite reward (format + answer correctness) further aligns reasoning and outputs.\nThe authors introduce PrimeKGQA, a benchmark derived from PrimeKG, covering basic, medium, and hard multi-hop tasks with explicit reasoning-path supervision.\nExperiments show Bio-KCoT (4B/8B) matches or surpasses strong closed-source and reasoning-focused models as task complexity increases, achieving SOTA on medium/hard categories (e.g., off-label use, side effects, contraindications). The approach generalizes to BioASQ, BiomixQA, and MEDDDX without extra training, and ablations confirm the benefit over distilled CoT without KG guidance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear, well-motivated integration of structured knowledge with long-CoT: KG-guided path instantiation and pruning directly address factual grounding and logical consistency issues common in biomolecular LLM reasoning.\n\nMethodological novelty in combining: (i) multi-structure path templates (linear/divergent/convergent) capturing non-local evidence, (ii) prompt-based CoT construction aligned to KG paths, (iii) pruning to remove spurious steps, and (iv) GRPO with a simple, effective composite reward that enforces both structure and correctness.\n\nStrong empirical results where it matters: consistent gains at higher hop depths and on hard tasks (contraindications, DDIs), demonstrating that the framework scales with reasoning depth rather than memorization.\n\nNew benchmark (PrimeKGQA): provides multi-hop, KG-grounded QA with explicit reasoning chains and difficulty stratification; thoughtful train/test split by head entities to reduce leakage."}, "weaknesses": {"value": "Insufficient description of dataset construction. The paper does not detail how question and answer entities in PrimeKGQA are extracted and disambiguated, which templates and constraints are used to instantiate paths from the knowledge graph, or the concrete criteria for subsequent filtering and cleaning. Fallback mechanisms and quality-control standards to ensure the validity and correctness of constructed questions are also unspecified.\n\nDataset quality and potential leakage are not systematically evaluated. The difficulty and discriminability of the option sets (correct answers vs. distractors) have not been rigorously measured. It remains unclear whether distractors are structurally “hard negatives” (e.g., graph neighbors, same type, same path depth) or primarily surface-level confounders. The study should report the option generation and screening pipeline, the structural separability between correct answers and distractors within the KG, and analyze the risk that models achieve high scores via template or pattern memorization, complemented by adversarial or shuffle-based evaluations to validate robustness.\n\nInadequate definition and evidence for question types and difficulty stratification. While hop depth and branching width d are used as difficulty indicators, the mapping between Levels and Task Categories in Table 1 lacks statistical support; no distributions, variance, or representativeness of categories within each difficulty tier are provided. Moreover, the illustrations in Table 1 are not clarified as real subgraph patterns for those question types versus conceptual schematics. The paper should add visualizations and summary statistics of real examples and report correlations between the difficulty metrics and external validity signals (e.g., human solving time, error rates) to substantiate the effectiveness of the stratification.\n\nPotential performance saturation and concerns about real-world extrapolation. Bio-KCoT’s accuracy on medium- and hard-level tasks in PrimeKGQA approaches a ceiling, which may reflect dataset limits rather than methodological limits, risking optimistic estimates. This saturation suggests a substantial gap between the benchmark and real biomolecular reasoning challenges.\n\nComparative imbalance introduced by teacher distillation. The method relies on chain-of-thought traces generated or pruned by stronger LLMs as supervision during SFT, creating an imbalance in training signal strength relative to baselines that do not use teacher signals. This makes it difficult to disentangle gains from the framework versus those inherited from the teacher. Additional teacher-distilled baselines are needed to isolate and quantify the teacher’s contribution and ensure robust attribution of conclusions.\n\nInsufficient baselines and comparison scope. The absence of strong baselines tailored to knowledge-graph and multi-hop reasoning constrains an objective assessment of novelty. The study should incorporate representative KG-QA pipelines and reinforcement learning baselines designed for these settings, and report rigorous comparisons under matched resource budgets and evaluation protocols."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2W259V0kF4", "forum": "ywPINvcN95", "replyto": "ywPINvcN95", "signatures": ["ICLR.cc/2026/Conference/Submission2266/Reviewer_g4bQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2266/Reviewer_g4bQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992008099, "cdate": 1761992008099, "tmdate": 1762916168022, "mdate": 1762916168022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Bio-KCoT, a chain-of-thought reasoning framework designed for biomolecular question answering. The approach integrates knowledge graph reasoning into LLMs by retrieving structured reasoning paths from a biomolecular knowledge graph (PrimeKG) to yield grounded reasoning chains. The curated CoTs are then used to fine-tune Qwen3 models (4B and 8B) via supervised fine-tuning and GRPO. Additionally, the authors introduce PrimeKGQA, a new benchmark built from PrimeKG that supports biomedical QA for diverse task types (e.g., indications, side effects, drug–drug interactions). Experiments show that Bio-KCoT achieves competitive or superior accuracy on complex tasks compared to both open- and closed-source LLMs, and demonstrates transfer to out-of-distribution datasets"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Problem formulation and motivation are strong - The authors clearly identify the gap in existing CoT reasoning methods for biomedical QA tasks, that is, the lack of factual grounding and demonstrate how Bio-KCoT can function as a bridge. \n2. The paper presents a thorough and well-structured pipeline - entity extraction, KG-based path retrieval, CoT generation, pruning, and RL fine-tuning. \n3. Novel dataset contribution -  The introduction of PrimeKGQA is a meaningful resource for the community, as there are few large-scale, CoT-annotated biomedical reasoning datasets.\n4. Empirical rigor - The improvements over Qwen base models are consistent and substantial, especially in medium and hard reasoning tasks."}, "weaknesses": {"value": "1. Possible data leakage concerns -  The dataset is derived from publicly available biomedical sources ( PrimeKG). It remains unclear whether this data maybe used in the base model pretraining and might bias evaluation. \n2. Limited empirical improvement over large models -  Despite the novel training pipeline, Bio-KCoT’s absolute performance remains very limited compared with much larger closed-source models (e.g., GPT-4o) on average accuracy. \n3. Underexplored multimodal integration - The framework does not explore integration with other biological modalities such as structure, or with captioning datasets such as PubMed-300k."}, "questions": {"value": "1. Given that PrimeKGQA are derived from public biomedical knowledge bases, what steps were taken to ensure no overlap with model pretraining data? Is it possible to conduct a temporal split experiment (e.g., excluding data added after Qwen3 training) to quantify leakage?\n2. Could the authors reason on why the Bio-KCoT models (even at 8B) underperform larger models such as GPT-4o or Gemini on some basic tasks despite being domain-tuned? \n3. Can Figure 3a be expanded to include additional open-source biomedical LLMs for more robust out-of-dataset evaluation?\n4. What are the plans for dataset hosting and licensing ? Will there be a public dataset availability with a permanent DOI ? \n5. Are there any experiments to show generalization beyond the BioMedical datasets to general molecular datasets ? For eg, MolInstructions [1] and MolTextQA [2]. I understand that the rebuttal period is limited, and I would like to clarify that the absence of these additional experiments will not negatively affect my evaluation. \n\n[1] Fang, Yin, et al. \"Mol-instructions: A large-scale biomolecular instruction dataset for large language models.\" arXiv preprint arXiv:2306.08018 (2023).\n[2] Laghuvarapu, Siddhartha, et al. \"MolTextQA: A Question-Answering Dataset and Benchmark for Evaluating Multimodal Architectures and LLMs on Molecular Structure–Text Understanding.\" Journal of Data-centric Machine Learning Research (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I8jz3o048w", "forum": "ywPINvcN95", "replyto": "ywPINvcN95", "signatures": ["ICLR.cc/2026/Conference/Submission2266/Reviewer_6BKe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2266/Reviewer_6BKe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102611815, "cdate": 1762102611815, "tmdate": 1762916167883, "mdate": 1762916167883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}