{"id": "WWnCWCzQcS", "number": 5417, "cdate": 1757908109714, "mdate": 1763013202222, "content": {"title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning", "abstract": "Graphical User Interface (GUI) agents have demonstrated remarkable progress in automating complex user interface interactions through reinforcement learning (RL). However, current approaches face a fundamental dilemma: offline RL enables stable training on pre-collected trajectories, but struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards and prohibitive deployment costs. To address it, we present $\\textbf{Semi-online Reinforcement Learning}$, a novel paradigm that simulates online RL on offline trajectories. During each rollout process, we preserve the original model output within the multi-turn dialogue, where a Patch Module adaptively recovers the divergence between rollout and expert trajectories. To capture long-term training signals, Semi-online RL introduces discounted future returns into the reward computation and optimizes the policy with weighted step-level and episode-level advantages. We further introduce Semi-Online Performance ($\\textbf{SOP}$), a metric that aligns better with true online performance, serving as a practical and effective proxy for real-world evaluation. Experiments show that ours $\\textbf{UI-S1-7B}$ achieves SOTA performance among 7B models across four dynamic benchmarks, with significant gains over the base model (e.g., +12.0\\% on AndroidWorld, +23.8\\% on AITW), demonstrating significant progress in bridging the gap between offline training efficiency and online multi-turn reasoning.", "tldr": "We present Semi-online Reinforcement Learning, a novel paradigm that simulates online RL on offline trajectories.", "keywords": ["GUI Agent", "Reinforcement learning", "Large Language Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ec055d90fa4224a5e204e78fe94cadc3daac0e89.pdf", "supplementary_material": "/attachment/cb4da439bc339f1b72c90a66dddcf2d2bfbe901f.zip"}, "replies": [{"content": {"summary": {"value": "The manuscript deals with reinforcement learning for GUI development. An approach called semi-online RL is introduced."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The use of LLMs to support RL is an interesting research topic."}, "weaknesses": {"value": "* The presentation is unclear.\n* Terms are used that have not been defined.\n* The integration into existing RL literature is insufficient. In particular, model-based RL and rollouts in model-based RL are not discussed.\n* The entire method remains unclear.\n* In my opinion, it is completely unacceptable to claim the term “semi-online RL” without going into detail about existing RL techniques.\n* No justification is given as to why the results are considered statistically significant.\n\nDetails and further comments:\n\nThe sentence “offline RL [...] struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards” does not seem to make sense to me.\n\nThe sentence “Semi-online RL introduces discounted future returns into the reward computation” does not seem to make sense to me.\n\nThe terms policy, policy model, and agent are used without it being clear what differences are meant by them, or whether they are just different terms for the same thing."}, "questions": {"value": "Is it intended that the presented method will also bring advantages in classic RL benchmarks, such as Halfcheetah?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ybDD1jjvgL", "forum": "WWnCWCzQcS", "replyto": "WWnCWCzQcS", "signatures": ["ICLR.cc/2026/Conference/Submission5417/Reviewer_1YPk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5417/Reviewer_1YPk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761057635666, "cdate": 1761057635666, "tmdate": 1762918049467, "mdate": 1762918049467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "YdLa11xlXv", "forum": "WWnCWCzQcS", "replyto": "WWnCWCzQcS", "signatures": ["ICLR.cc/2026/Conference/Submission5417/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5417/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763013201542, "cdate": 1763013201542, "tmdate": 1763013201542, "mdate": 1763013201542, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **UI-S1**, a framework that bridges the gap between offline and online reinforcement learning for GUI automation. Traditional offline RL is stable but lacks multi-turn reasoning, while online RL captures long-horizon dependencies but requires costly real-time interactions. UI-S1 proposes a **Semi-Online RL** approach that simulates online dynamics entirely on **offline expert trajectories**, achieving both data efficiency and contextual consistency.\n\nUI-S1’s core innovation is the **Patch Module**, which “repairs” rollouts when the model’s predicted actions deviate from expert trajectories—allowing continued training without terminating early. Combined with a **dual-level policy optimization** (step-level for immediate accuracy, episode-level for overall success), this enables long-horizon learning without environment interaction. The authors also introduce the SOP metric, an offline measure that strongly correlates (R² = 0.93) with real-world task success, providing a reliable proxy for evaluating GUI agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a Semi-Online Reinforcement Learning framework that bridges offline and online RL without requiring live environment interaction. This hybrid formulation is an original contribution that removes major practical barriers of online GUI agents while retaining long-horizon reasoning capabilities. The approach substantially improves the scalability of GUI automation models by eliminating the need for interactive training. This has high significance for deploying intelligent agents in real-world software automation."}, "weaknesses": {"value": "1. The so-called Semi-Online approach does not actually utilize any data from real online interactions. It remains fundamentally an offline training method that relies on pre-collected expert trajectories, with more fine-grained optimization at the step level. Therefore, it would be more convincing if the paper compared UI-S1 directly against other expert-data-based training methods on the same datasets, such as SFT and the following related works:  \n   - *DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning* (https://arxiv.org/abs/2406.11896)  \n   - *UI-R1: Enhancing Efficient Action Prediction of GUI Agents by Reinforcement Learning* (https://arxiv.org/abs/2503.21620)\n\n2. The paper’s key contribution—Semi-Online training—heavily depends on expert trajectories for the Patch Module, yet the source and composition of these expert data are not described in sufficient detail.  \n   Specific questions remain unanswered:  \n   - How were the expert trajectories collected or constructed?  \n   - What is the proportion of data from different sources within the reported 2,000 samples?  \n   - If these trajectories are verified as “perfect rollouts,” were they also used in the SFT stage? If so, what are the results of training solely with these verified trajectories for SFT?\n\n3. Section 3.4 assigns different weights to various reward components, but the paper does not provide a theoretical or empirical justification for these specific weights.  \n   Moreover, the SOP method (Semi-Online Performance) is not explicitly defined—its mathematical formulation is unclear, and it is ambiguous whether SOP represents the same quantity as the weighted reward function described in Section 3.4.\n\n4. The results show that Off-Policy Thought Patch consistently underperforms both Thought-Free and On-Policy Patch methods. However, the paper does not explain the positive motivation or utility of this variant—its conceptual necessity and practical benefit remain unclear."}, "questions": {"value": "The questions are already included within the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dFLWxRHGbu", "forum": "WWnCWCzQcS", "replyto": "WWnCWCzQcS", "signatures": ["ICLR.cc/2026/Conference/Submission5417/Reviewer_rNwR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5417/Reviewer_rNwR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761111669944, "cdate": 1761111669944, "tmdate": 1762918049162, "mdate": 1762918049162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Semi-online Reinforcement Learning (RL) for GUI agents, a framework that trains on offline trajectories while simulating online rollouts through a Patch Module. The method optimizes performance using dual-level advantages combined with discounted returns, achieving strong results on AndroidWorld, AITW, and MiniWob++ benchmarks.\nAdditionally, the paper proposes the SOP metric, designed to effectively measure performance in semi-online settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is very well written and organized.\n- The experiments demonstrate the effectiveness of Semi-Online RL. Across benchmarks like AndroidWorld, AITW-Gen, and MiniWob++, the proposed UI-S1 model consistently outperforms baselines such as SFT and Offline RL.\n- The paper provides thorough ablation studies covering discount factors, training paradigm combinations (SFT + RL), the role of episode-level advantages, and patch threshold sensitivity."}, "weaknesses": {"value": "- Reliance on expert trajectories and \"oracle\" dynamics. The rollout process depends on expert next states when the actions align, and on patched expert actions when they don’t. This introduces an negative bias in return estimation and restricts the model’s exposure to true on-policy states, potentially limiting robustness and generalization.\n\n- Limited methodological novelty. Compared with offline GRPO or \"step-level GRPO\", the main improvements of this work lie in the advantage computation and the organization of history. These changes do not represent substantial algorithmic innovation."}, "questions": {"value": "- Insufficient analysis of offline GRPO improvements. It is especially important to analyze why offline GRPO underperforms SFT in multi-turn tasks but outperforms in single-turn tasks, and how the authors’ modifications specifically address the shortcomings of offline GRPO in multi-turn interactions.\n\n- Unclear rationale behind the success of the Thought-Free Patch. The results show that the Thought-Free Patch performs comparably to, or even better than the On-Policy Thought Patch. However, since the Thought-Free Patch operates entirely out of distribution, this outcome seems counterintuitive and requires further explanation or theoretical justification.\n\n- Lack of clarity on the effect of $\\epsilon$. A larger $\\epsilon$ corresponds to more training data, and according to the definition of advantage, $\\epsilon$ should not affect advantage estimation. Therefore, it is unclear why increasing $\\epsilon$ leads to worse performance, this phenomenon needs more analysis and discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TmpI1q9RYn", "forum": "WWnCWCzQcS", "replyto": "WWnCWCzQcS", "signatures": ["ICLR.cc/2026/Conference/Submission5417/Reviewer_e69T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5417/Reviewer_e69T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761214255988, "cdate": 1761214255988, "tmdate": 1762918048750, "mdate": 1762918048750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents UI-S1, a framework for GUI automation using a method termed Semi-Online Reinforcement Learning that bridges the gap between single-turn offline RL and multi-turn online RL.\nIt simulates online interaction using offline data through a Patch Module that corrects action mismatches and maintains training continuity.\nA dual-level reward system captures both step-wise and long-term advantages, improving multi-step reasoning.\nThe authors also propose a new evaluation metric, Semi-Online Performance (SOP), which correlates strongly with real-world performance.\nUI-S1-7B achieves state-of-the-art results among 7B models, outperforming prior methods on multiple GUI benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Bridges offline single-turn and online multi-turn RL: Introduces \"Semi-Online RL\", effectively combining the merits of both worlds.\n- Efficient learning from static data: The Patch Module enables continued training despite action mismatches, improving data utilization and multi-turn learning.\n- More Reliable evaluation metric: Proposes Semi-Online Performance (SOP), a fast and accurate proxy for real-world online performance (R² = 0.934).\n- State-of-the-art results in multi-turn performance: The UI-S1-7B model achieves top performance across multiple GUI automation benchmarks among models of the same size"}, "weaknesses": {"value": "I believe the paper is proposing a reasonably novel method, shows good performance on benchmarks, and overall has a demonstrated merit over prior works, however I am utterly confused by the terminology. To highlight just the most extreme cases:\n\n- \"Traditional offline RL trains on static trajectories where each step conditions on expert demonstrations\"\n- Similarly: \"Traditional offline RL optimizes only for immediate step-wise accuracy, resulting in multi-turn planning failure. \"\n\n--> To me, these statements appear to be inaccurate or, at minimum, a rebranding of well-established terminology. There are hundreds of papers on offline reinforcement learning that explicitly address long-horizon (i.e., multi-turn) decision-making by maximizing discounted cumulative reward—arguably the standard definition of RL. The setting you describe (offline + expert data + single-step optimization at training time) seems to correspond closely to what is typically referred to as behavior cloning (e.g., [1–3]). Could you please clarify what makes this rebranding necessary / what I am missing here?\n\n[1] Foster, Dylan J., Adam Block, and Dipendra Misra. \"Is behavior cloning all you need? understanding horizon in imitation learning.\" Advances in Neural Information Processing Systems 37 (2024): 120602-120666.\n\n[2] Kumar, Aviral, et al. \"Should i run offline reinforcement learning or behavioral cloning?.\" International conference on learning representations. 2021.\n\n[3] Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. \"Diffusion policies as an expressive policy class for offline reinforcement learning.\" arXiv preprint arXiv:2208.06193 (2022).\n\nIn addition, the term semi-online RL seems potentially misleading, as—if I understand correctly—no online interactions are involved in your setup. Overall, it appears that the manuscript does not sufficiently engage with a substantial body of prior work in this area.\n\nFinally, I read some of the claims as a bit overstated, e.g. \"[...] Semi-Online RL successfully bridges both capabilities rather than trading one for the other\", \"[...] validating that Semi-online RL doesn’t sacrifice single-turn capabilities.\", etc.\nWhen I compare single-turn performance of the proposed model with e.g. UI-TARS-7B, averaging over the 8 columns I get: 67.5 vs 79.5, which appears a quite significant drop compared to this model.\n\nI'm happy to raise the score if the above can be addressed."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8saY9mukxU", "forum": "WWnCWCzQcS", "replyto": "WWnCWCzQcS", "signatures": ["ICLR.cc/2026/Conference/Submission5417/Reviewer_6v2V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5417/Reviewer_6v2V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935212490, "cdate": 1761935212490, "tmdate": 1762918048363, "mdate": 1762918048363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}