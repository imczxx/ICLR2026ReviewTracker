{"id": "DQ68Sqk0Js", "number": 7045, "cdate": 1758005935197, "mdate": 1762924924011, "content": {"title": "LIQT: Bridging Liquid Neural Dynamics and Human Perceptual Mechanisms for Blind Image Quality Assessment", "abstract": "Blind Image Quality Assessment (BIQA) seeks to predict perceptual quality in reference-free scenarios, yet conventional methods often hard to capture the human visual system's adaptive spatio-temporal integration of degradation patterns. Inspired by the adaptive temporal dynamics of biological neural circuits, we propose Liquid Image Quality Transformer (LIQT), a novel BIQA framework that integrates Liquid Neural Networks (LNNs) with Transformer-based architectures. LIQT incorporates Liquid Self-Attention (LSA) equipped with Closed-Form Continuous-Time Module (CFCTM), which reformulates liquid time-constant neurons into stable closed-form solutions through learnable decay rates and Padé approximation, thus enabling LIQT to dynamically modulates feature extraction based on local image features. To emulate multi-scale perceptual evaluation, a Multi-Scale Image Quality-Aware Decoder (MIQAD) aggregates multi-scale features from LIQT for comprehensive quality regression. This work pioneers the integration of biomimetic neural mechanisms into BIQA and experiments in six benchmark datasets that span various types of distortion and image content demonstrate the superior performance of LIQT over state-of-the-art methods.", "tldr": "", "keywords": ["Image Quality Assessment", "Liquid Neural Networks", "Transformer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/5f49ff5746af605e997fdf7761751e22ccf98c6f.pdf", "supplementary_material": "/attachment/349fe7d9485b38320c7875a89dd75930d25e7c7d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Liquid Image Quality Transformer (LIQT), a framework for BIQA that mimics the human visual system. LIQT integrates Liquid Neural Networks (LNNs) into a Transformer architecture to better model the continuous-time neural dynamics of human perception. The framework uses a Liquid Time-Constant Transformer (LTCFormer) with a novel Closed-Form Continuous-Time Module (CFCTM) to adaptively process image features based on their local characteristics. Additionally, a Multi-Scale Image Quality-Aware Decoder (MIQAD) is used to aggregate features at different scales for a more comprehensive quality prediction. Experimental results show that this lightweight model outperforms state-of-the-art methods on six benchmark BIQA datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript is well-written, with a clear logical flow and a well-organized structure.\n\n2. The primary strength lies in the novelty of the LIQT framework. It’s a lightweight IQA model and demonstrates good performance."}, "weaknesses": {"value": "1. The central weakness lies in the paper's core motivation. The authors introduce the \"Spatio-Temporal Representation Disentanglement (STRD)\" challenge to justify the use of LNNs. However, the definition and applicability of this \"spatio-temporal\" problem to the static, non-sequential domain of image quality assessment are insufficiently justified and questionable.\n\n2. The paper fails to provide a clear and compelling justification for its method of transforming a spatial problem into a temporal one. The model processes image patches sequentially, but the mapping from spatial patch location to a \"temporal\" order is not adequately explained or defended. It is unclear how this specific sequential processing is analogous to HVS dynamics for a static image.\n\n3. The cross-dataset evaluation in Table 2 could be strengthened. Many recent VLM-based IQA models (e.g., DeQA-Score, Q-Insight) demonstrate robust generalization by training only on the KonIQ-10k dataset and testing on all others. Including this specific experimental setting would provide a more direct and rigorous comparison of LIQT's generalization capabilities against contemporary methods.\n\n4. The benchmark comparison in Tables 1 and 2 should include results on newer and more challenging datasets, such as PIPAL and AGIQA."}, "questions": {"value": "If the \"historical data\" and their temporal information are essential, as the LNN-based motivation implies, the model's output should be sensitive to the patch processing order. \nHow sensitive is the model's performance to the patch processing order? For instance, have the authors conducted an ablation study comparing the standard sequential order to a randomized patch order?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n49WCfyu6B", "forum": "DQ68Sqk0Js", "replyto": "DQ68Sqk0Js", "signatures": ["ICLR.cc/2026/Conference/Submission7045/Reviewer_GrXp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7045/Reviewer_GrXp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761178257003, "cdate": 1761178257003, "tmdate": 1762919242152, "mdate": 1762919242152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We believe that there are some areas in the manuscript that require further improvement, we have decided to withdraw our paper after careful deliberation and discussion. We wish to express our appreciation to the reviewers, ACs, and SACs for their valuable time and insightful comments on this submission."}}, "id": "QYZXaIkRmt", "forum": "DQ68Sqk0Js", "replyto": "DQ68Sqk0Js", "signatures": ["ICLR.cc/2026/Conference/Submission7045/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7045/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762924922323, "cdate": 1762924922323, "tmdate": 1762924922323, "mdate": 1762924922323, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses Blind Image Quality Assessment (BIQA), where no reference image is available and the model must infer perceptual quality solely from the degraded image. The authors propose Liquid Image Quality Transformer (LIQT), which integrates Liquid Self-Attention (LSA) and Multi-Scale Image Quality-Aware Decoder (MIQAD)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. An IQA model using liquid attention is proposed.\n2. The proposed method achieves state-of-the-art performance."}, "weaknesses": {"value": "1. The motivation is weak. As shown in Fig.1, it is hard to connect liquid attention closer to HVS. Why CNN and transformer do not have memory. They have memorized the historical data in their parameters. Can LNN memorize new data without training?\n2. The generalization ability is limited to some extent."}, "questions": {"value": "1. Can LNN memorize new data without training?\n2. What is “train param.”. Is it same with the “param.”? Can you report MAC/GFlops comparison or test time comparison with other models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jdu6sjOzML", "forum": "DQ68Sqk0Js", "replyto": "DQ68Sqk0Js", "signatures": ["ICLR.cc/2026/Conference/Submission7045/Reviewer_vvcc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7045/Reviewer_vvcc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836297196, "cdate": 1761836297196, "tmdate": 1762919241639, "mdate": 1762919241639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes LIQT, a blind image quality assessment (BIQA) model that blends Liquid Neural Network (LNN) dynamics with a Transformer backbone. The core technical pieces are a Closed Form Continuous Time Module (CFCTM)—intended to reparameterize liquid time constant (LTC) neurons via a closed form with a Padé (1,1) approximation. Results on six IQA datasets show strong correlations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Conceptual novelty: A clear, biomimetic rationale for using continuous‑time dynamics on spatial tokens.\n\n2. Compact, competitive model: 7 M parameters with strong PLCC/SRCC on several datasets"}, "weaknesses": {"value": "1. While the introduction emphasizes modeling the human visual system’s ability to integrate historical perceptual experiences through spatio-temporal memory consolidation—suggesting an intention to capture long-term or cross-image temporal dependencies (see Fig. 1(a))—the actual implementation of LIQT does not realize this form of memory. In the methods section, the so-called “temporal” dynamics are created by converting spatial adjacency within each image window into a pseudo-temporal sequence, which is then processed by the Closed-Form Continuous-Time Module (CFCTM). The model resets its state for every new image and maintains no persistent or cross-sample memory. Consequently, the claimed alignment with human temporal memory or experience integration is conceptually overstated. The method effectively performs intra-image sequential integration rather than inter-image temporal learning, weakening the coherence between the biological motivation and the engineered mechanism.\n\n2. Equations (7)–(8) introduce time-continuous reset and update gates but do not explain how the gating variables are obtained. \n\n3. The MLP responsible for mapping relative positions to temporal parameters plays a key role in addressing the STRD problem. However, the paper does not clarify important implementation details—such as whether the generated temporal sequence is monotonic or normalized, how the number of temporal \nsegments is determined, or whether the total duration of these segments is constrained.\n\n4. The paper states LIQT “outperforms … across all datasets,” but Table 1 shows stronger results for LQMamba‑B on KonIQ‑10k and TID2013.\n\n5. The paper emphasizes parameter count but does not report FLOPs or training time vs. baselines."}, "questions": {"value": "1. The qualitative analysis requires deeper interpretation and clearer justification. The current evidence does not sufficiently substantiate the claim that the LIQT model effectively captures or emphasizes spatiotemporal feature aggregation within images\n\n2. The overall presentation of the paper lacks clarity and precision. The writing is occasionally verbose and conceptually inconsistent, which obscures the main ideas and technical contributions. Several figures are not professionally formatted or adequately annotated, limiting their explanatory value. Furthermore, the conclusions and claimed contributions are not clearly delineated from the methodological description and tend to be overstated relative to the actual empirical evidence provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Rbm6PxsAgM", "forum": "DQ68Sqk0Js", "replyto": "DQ68Sqk0Js", "signatures": ["ICLR.cc/2026/Conference/Submission7045/Reviewer_CkYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7045/Reviewer_CkYg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881150329, "cdate": 1761881150329, "tmdate": 1762919241304, "mdate": 1762919241304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LIQT, a novel Blind Image Quality Assessment (BIQA) framework that bridges liquid neural dynamics and human perceptual mechanisms. LIQT integrates Liquid Neural Networks (LNNs) with Transformer-based architectures to capture the adaptive spatio-temporal integration of degradation patterns in the human visual system. The framework incorporates Liquid Self-Attention (LSA) equipped with a Closed-Form Continuous-Time Module (CFCTM) to dynamically modulate feature extraction based on local image features. A Multi-Scale Image Quality-Aware Decoder (MIQAD) aggregates multi-scale features for comprehensive quality regression. The paper demonstrates the superior performance of LIQT over state-of-the-art methods on six benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The LIQT framework is well-designed and technically sound. The CFCTM and MIQAD components are innovative and effective.\n2) The experimental results demonstrate the superior performance of LIQT over state-of-the-art methods.\n3) The paper is generally well-written and easy to follow."}, "weaknesses": {"value": "1) While the paper mentions the advantages of LNNs, it could more explicitly articulate why they are particularly well-suited for BIQA compared to other approaches.\n2) The technical details of the CFCTM and LSA mechanisms could be more accessible to a broader audience. Provide more intuitive explanations and visualizations.\n3) The ablation studies in Table 3 are helpful, but they could be more comprehensive. Consider ablating different components of the CFCTM and MIQAD modules.\n4) While the paper claims computational efficiency, a more detailed analysis of the computational complexity of LIQT compared to other methods would be beneficial.\n5) The motivation for the window tokenization approach could be clearer. Why is this necessary for modeling spatio-temporal relationships?"}, "questions": {"value": "6) Expand the cross-dataset evaluation in Table 2 to include more comprehensive testing scenarios. Specifically, evaluate the performance of models trained on authentic distortion datasets (e.g., LIVEC, KonIQ-10k, LIVEFB) and tested on synthetic distortion datasets (e.g., LIVE, CSIQ, TID2013), and vice versa. This will provide valuable insights into the generalizability of LIQT across different types of distortions.\n\nExpand the cross-dataset evaluation in Table 2 to include more comprehensive testing scenarios. Specifically, evaluate the performance of models trained on authentic distortion datasets (e.g., LIVEC, KonIQ-10k, LIVEFB) and tested on synthetic distortion datasets (e.g., LIVE, CSIQ, TID2013), and vice versa. This will provide valuable insights into the generalizability of LIQT across different types of distortions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sxGSxdhIXV", "forum": "DQ68Sqk0Js", "replyto": "DQ68Sqk0Js", "signatures": ["ICLR.cc/2026/Conference/Submission7045/Reviewer_wFos"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7045/Reviewer_wFos"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922375136, "cdate": 1761922375136, "tmdate": 1762919241027, "mdate": 1762919241027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}