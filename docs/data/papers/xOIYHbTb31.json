{"id": "xOIYHbTb31", "number": 2604, "cdate": 1757162374604, "mdate": 1759898138070, "content": {"title": "ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns", "abstract": "Mixture-of-Experts (MoE) effectively scales model capacity while preserving computational efficiency through sparse expert activation.\nHowever, training high-quality MoEs from scratch is prohibitively expensive. A promising alternative is to convert pretrained dense models into sparse MoEs. Existing dense-to-MoE methods fall into two categories: \\textbf{dynamic structural pruning} that converts dense models into MoE architectures with moderate sparsity to balance performance and inference efficiency, and \\textbf{downcycling} approaches that use pretrained dense models to initialize highly sparse MoE architectures. However, existing methods break the intrinsic activation patterns within dense models, leading to suboptimal expert construction. In this work, we argue that the Gated Linear Unit (GLU) mechanism provides a natural blueprint for dense-to-MoE conversion. We show that the fine-grained neural-wise activation patterns of GLU reveal a coarse-grained structure, uncovering an inherent MoE architecture composed of consistently activated universal neurons and dynamically activated specialized neurons.\nLeveraging this discovery, we introduce ExpertWeaver, a training-free framework that partitions neurons according to their activation patterns and constructs shared experts and specialized routed experts with layer-adaptive configurations. Our experiments demonstrate that ExpertWeaver significantly outperforms existing methods, both as a training-free dynamic structural pruning technique and as a downcycling strategy for superior MoE initialization.", "tldr": "", "keywords": ["Mixture-of-Experts; Gated Linear Unit; Dense-to-MoE"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f7cbfc960e75ab097bda8093abc004333089a46b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ExpertWeaver, a training-free approach for converting dense LLMs into Mixture-of-Experts (MoE) architectures by leveraging GLU activation patterns.\nThe method collects neuron-wise gate activations across multiple tasks, measures each neuron’s specialization using the coefficient of variation (CV), and clusters them into shared and routed experts via balanced K-Means.\nRouters are constructed analytically, without any additional training.\nExpertWeaver operates in two modes: (1) training-free structural pruning for efficient inference, and (2) model downcycling, in which the converted MoE is further refined with limited continued pretraining (~200B tokens)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea is simple leveraging existing GLU structures in dense LLMs yet it proves to be effective, outperforming other existing methods.\n\n2. Demonstrates consistent improvements over LLM-Pruner, FLAP, and CMoE across Qwen2.5-7B and LLaMA3-8B baselines.\n\n3. The paper is clearly written and well-organized, with convincing empirical results supporting its claims."}, "weaknesses": {"value": "1. The proposed method can be interpreted as a dynamic, data dependent filtering mechanism.\nWhile the paper includes an ablation on diversity, it would be useful to also analyze how data quantity and diversity affect performance sensitivity and stability.\n\n2. Tables 1–2 exclude reasoning and generative benchmarks (e.g., GSM8K, HumanEval).\nThe authors should clarify why these tasks were omitted, as including them would better demonstrate the method’s generality.\n\n3. Details about the training libraries and distributed setup are missing.\nClarifying whether the implementation and code will be publicly released would improve reproducibility.\n\n4. The selected baselines appear relatively weak, raising concerns about the fairness of comparison.\nConsidering that Qwen2.5 [1] was trained on approximately 18T tokens, comparing it with OLMoE trained from scratch on 500B tokens may not be meaningful. Including dense variants such as Qwen2.5-1.5B or 3B would yield a more balanced evaluation.\n\n5. The claim that ExpertWeaver achieves lower training loss than both random and LLaMA-MoE initialization should be discussed in greater depth. Prior works (e.g., [2], [3]) suggest that random or partially re-initialized experts can sometimes achieve better long-term convergence.\nIt would help to clarify under which specific conditions ExpertWeaver’s initialization provides an advantage.\n\n[1] https://arxiv.org/abs/2412.15115\n\n[2] https://arxiv.org/abs/2406.16554\n\n[3] https://arxiv.org/abs/2502.19261"}, "questions": {"value": "1. The approach assumes GLU based gating. How would it perform with squared ReLU [1] or the unconventional SwiGLU variant [2]?\n\n2. Could the method start from an instruction-tuned checkpoint instead of a base pretrained model?\nWhether ExpertWeaver could be applied to reasoning-oriented models such as Qwen3 [3], which are already RL-based instruction models.\n\n[1] https://arxiv.org/abs/2406.11704\n\n[2] https://arxiv.org/abs/2508.10925\n\n[3] https://arxiv.org/abs/2505.09388"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qIYLouIsW5", "forum": "xOIYHbTb31", "replyto": "xOIYHbTb31", "signatures": ["ICLR.cc/2026/Conference/Submission2604/Reviewer_tQX7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2604/Reviewer_tQX7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761324667426, "cdate": 1761324667426, "tmdate": 1762916299805, "mdate": 1762916299805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ExpertWeaver proposes a training-free method to convert pretrained dense LLMs with GLU-based FFNs into sparse Mixture-of-Experts (MoE) architectures. The core idea is to treat GLU gating signals as a natural blueprint for expert construction: (1) record multi-task neuron activation patterns, (2) use Coefficient of Variation (CV) to quantify layer-wise specialization and adaptively allocate shared vs. routed experts, (3) cluster specialized neurons via balanced K-Means and build a training-free router from gating centroids. The method is evaluated in two settings: dynamic structural pruning (moderate sparsity, no further training) and downcycling (high-sparsity MoE initialization followed by limited continued pretraining)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovative observation that GLU gating encodes neuron-level specialization patterns.\n2. Elegant training-free conversion procedure; no additional routing training needed.\n3. Consistent performance gains over previous training-free pruning baselines (FLAP, CMoE, LLM-Pruner)."}, "weaknesses": {"value": "1. Lack of thorough inference-throughput analysis despite claiming efficiency as a key motivation.\n2. Missing inference throughput experimental setup details (input length, TP/EP config, number of GPUs, software library information).\n3. No comparison with Drop Upcycling or other recent upcycling methods.\n4. The baselines are partly outdated and omit dense models released in the same generation as the used base model (Qwen2.5-7B)—for example, Qwen2.5-3B or Llama-3.2-3B—which would provide fairer comparisons.\n5. Comparing ExpertWeaver—initialized from the high-performance pretrained model Qwen2.5-7B—with an open-project MoE(OLMoE) at an intermediate checkpoint, does not constitute a fair evaluation of efficiency."}, "questions": {"value": "1. Why did you choose Qwen2.5-7B as the base model for ExpertWeaver? If the goal is to demonstrate the general usefulness of the proposed method, using a model such as OLMo-7B, which shares a similar data distribution and architecture with the comparison target OLMoE-1B-7B, would allow for a fairer evaluation.\n2. Given the strong performance of Qwen2.5-7B, how do you rule out the possibility that the superiority of ExpertWeaver (from Qwen2.5-7B) over OLMoE-1B-7B is mainly due to the higher base model quality rather than the effectiveness of your conversion method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gHaPFS50p7", "forum": "xOIYHbTb31", "replyto": "xOIYHbTb31", "signatures": ["ICLR.cc/2026/Conference/Submission2604/Reviewer_AAY3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2604/Reviewer_AAY3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700426457, "cdate": 1761700426457, "tmdate": 1762916299448, "mdate": 1762916299448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a method to create MoE models from a large dense model by the “downcycling” manner: partitioning the original FFN weights to construct the MoE layer by utilizing the underlying activation pattern on the intermediate neurons. Neurons are categorized as “shared” or “routed” according to its CV of absolute values, and “routed” neurons are further partitioned by K-means on the activation pattern to construct non-shared experts. Router weights for non-shared experts are directly obtained from the original gate parameter of the FFN layer. Experiments show its effectiveness through comparing downstream performance and efficiency on further training (SFT and CPT)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The MoE construction algorithm itself is very intuitive. In particular, the method of constructing router weights directly from existing gate weights is simple and rational. However, the act of obtaining neuron activation information from calibration data is a form of machine learning, so calling it \"training-free\" is likely an overstatement.\n* Reproducing the method seems easy, and the information necessary for reproduction is fully covered in the paper. Since the proposed method works on models of any size, it should be easy for others to verify the method's effectiveness."}, "weaknesses": {"value": "* The use of the term \"pruning\" is arbitrary. Pruning in conventional research refers to methods that reduce the parameters themselves, whereas AbsTopK-GLU and the final goal of a downcycled MoE preserve all parameters and operate adaptively on the input. The objectives of these methods do not align.\n* Related to the above, the comparisons in Table 1 and Table 2 mix methods that reduce parameters with adaptive methods, making it difficult to find meaning in the comparisons themselves.\n* It is unclear why the CV can separate shared experts from routed experts. This value merely indicates the magnitude of variation in a neuron's value and does not seem to be an indicator that can distinguish between neurons that are active only in specific patterns and those that simply have large fluctuations. For example, wouldn't observing the distribution of neurons that pass through AbsTopK-GLU be better at capturing the desired characteristics?\n* Table 3: there is no necessary comparison of the base model, Qwen2.5-7B. Since the performance of LLMs varies according to its construction process, highlighting actual contribution from the base model is more important than comparing with unrelated models."}, "questions": {"value": "* Figure 1 a) is not clearly interpretable. At least some clustering of neurons to highlight difference of their behaviors if required.\n* Depending on the settings, Eq. (6) could potentially result in zero shared experts or all neurons being assigned to shared experts. Is this acceptable for the method?\n* Continue pretraining -> continual pretraining or continued pretraining\n* Table 2: ExperWeaver -> ExpertWeaver\n* Overall, the text in the figures is too small, making them difficult to read when printed. I understand there is limited space, but please provide more visually clear figures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8UIRyUXOYV", "forum": "xOIYHbTb31", "replyto": "xOIYHbTb31", "signatures": ["ICLR.cc/2026/Conference/Submission2604/Reviewer_keBY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2604/Reviewer_keBY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989387928, "cdate": 1761989387928, "tmdate": 1762916298967, "mdate": 1762916298967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}