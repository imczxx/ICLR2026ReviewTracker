{"id": "lGfnvZsE2F", "number": 17969, "cdate": 1758282508601, "mdate": 1763722445524, "content": {"title": "DocRobust: Enhancing Robustness of Multi-modal LLMs in Low-Quality Document Image Scenarios", "abstract": "Document images are primary carriers of knowledge and information, yet their effective understanding is often hindered by degradations such as noise, blur, and low resolution. In this paper, we address the challenge of robust document understanding under such low-quality conditions by proposing the DocRobust-Module (DRM)—an efficient feature restoration module that, when integrated with a multimodal large language model, enables the recovery of lost visual and semantic information with minimal parameter modifications. Our method is supported by a novel two-stage training strategy that incrementally guides the model to restore critical information from both visual and semantic perspectives. To support the fine-tuning of MLLMs with DRM, we construct DocRobust-VQA, a large-scale visual question answering dataset containing extensive low-quality document images along with high-quality counterparts and QA annotations. With over 189K clear-blurry images pairs annotated by 417K QA pairs, DocRobust-VQA provides sufficient finetuning data for enhancing the robustness of MLLMs under real-world degradations. Extensive experiments demonstrate that our method consistently improves performance on low-quality document images, offering new insights and a scalable solution for robust document understanding.", "tldr": "A novel feature-level restoration method for MLLM in low-quality document image scenarios, with a large-scale VQA dataset for restoration training.", "keywords": ["Multimodal Large Language model", "Visual Question Answering", "Robustness"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7d215ea6ae5d1e2423e86cbb61cb44377179dee8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the practical problem of performance degradation in MLLMs when processing low-quality document images. First, the authors introduce the DocRobust-Module (DRM), a lightweight, plug-and-play module designed for feature-level restoration. This module is inserted into the MLLM architecture to recover corrupted visual and semantic information with low parameter overhead. Second, to support the training and evaluation of their approach, the authors construct DocRobust-VQA, a dataset containing over 189K paired high-quality and synthetically degraded document images. Through evaluation, the authors demonstrate that their method improves the performance of MLLMs on the new benchmark as well as on real-world document datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The DRM is a lightweight, plug-and-play solution that operates efficiently at the feature level. This is a more elegant approach than computationally expensive pixel-level restoration methods.\n\n2. The DRM is trained using a feasible two-stage training strategy (Visual Alignment followed by Semantic/Overall Alignment) that effectively guides the model to restore low-level visual features.\n\n3. The creation of the large-scale DocRobust-VQA dataset provides a much-needed public resource for training and benchmarking model robustness in this domain."}, "weaknesses": {"value": "1. The necessity of the specific two-stage training approach is not fully justified through ablation studies. I observe that in Table 1, the performance of DocRobust-visual and DocRobust-semantic remains bad across most metrics. A crucial baseline is missing: a single-stage, end-to-end fine-tuning of the MLLM with the DRM module (i.e., only the \"Overall Alignment\" stage).  Consequently, it is unclear whether the Visual Alignment stage and the Semantic Alignment stage are indispensable components for success.\n\n2. The authors do not provide detailed descriptions of the specific procedures for each of the five corruption methods used in generating corrupted images, nor visual examples.\n\n3. Since VQA pairs are derived from clean images, severe corruption may render the ground-truth answers visually unverifiable, thereby introducing noisy signals during training.\n\n4. The following is more of a suggestion than a weakness: The font size of the text in Figure 1 is too small and should be enlarged for better readability."}, "questions": {"value": "1. How would the removal of the Visual Alignment stage and the Semantic Alignment stage impact the performance of DRM?\n\n2. What are the specific implementation details of the five corruption methods used in generating corrupted images?\n\n3. Could excessive corruption occur, resulting in images that cannot be aligned with the Ground Truth?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cdYKoA7Yzi", "forum": "lGfnvZsE2F", "replyto": "lGfnvZsE2F", "signatures": ["ICLR.cc/2026/Conference/Submission17969/Reviewer_PYGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17969/Reviewer_PYGY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380686336, "cdate": 1761380686336, "tmdate": 1762927763507, "mdate": 1762927763507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "hEKobtPFjp", "forum": "lGfnvZsE2F", "replyto": "lGfnvZsE2F", "signatures": ["ICLR.cc/2026/Conference/Submission17969/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17969/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763722444932, "cdate": 1763722444932, "tmdate": 1763722444932, "mdate": 1763722444932, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses robustness of Multimodal Large Language Models (MLLMs) on low-quality document images. The authors propose DocRobust-Module (DRM), a lightweight feature restoration module inserted between the vision encoder and projector MLP, trained with a two-stage strategy (Visual Alignment followed by Semantic/Overall Alignment). To support training, they construct DocRobust-VQA, a dataset with 189K clear-corrupted image pairs and 417K QA annotations. Experiments on InternVL-2.5 models show consistent improvements on synthetic degradations, real-world images, and even adversarial examples."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Robustness to low-quality documents is practically important.\nDocRobust-VQA provides a substantial resource (189K pairs, 417K QA)\nConsistent improvements across benchmarks."}, "weaknesses": {"value": "Limited diversity in corruption types (only 5 categories).\nNo analysis of distribution shift between synthetic and real degradations.\nOnly evaluated on InternVL-2.5 family - generalization to other MLLM architectures unclear.\nMissing comparisons with other robustness approaches[1][2][3][4].\n[1] UReader [2] DocKylin [3] DocOwl [4] TokenVL [5] Vary"}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2RuaQD9tGc", "forum": "lGfnvZsE2F", "replyto": "lGfnvZsE2F", "signatures": ["ICLR.cc/2026/Conference/Submission17969/Reviewer_RJRD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17969/Reviewer_RJRD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761490635585, "cdate": 1761490635585, "tmdate": 1762927762975, "mdate": 1762927762975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the DocRobust-Module (DRM) to tackle degraded document understanding: DRM integrates with MLLMs for visual/semantic information recovery, uses a two-stage training strategy, constructs the large-scale DocRobust-VQA dataset for fine-tuning, and experiments confirm its consistent performance improvement on low-quality documents."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper has a good starting point, and the problem it aims to address—the understanding of low-quality document images—is highly meaningful. The proposed method, which focuses on reconstructing high-quality features from low-quality ones, is also logically sound."}, "weaknesses": {"value": "1. The paper focuses on the task of document understanding. However, no document images are presented throughout the entire manuscript—this includes images from both the synthetic dataset and those associated with the DocVQA task.\n2. (1) Luminance, (2) Distortion, (3) Blurriness, (4) Noise, and (5) Compression—how is each of these five types of degradation specifically implemented? For document images, Distortion can be introduced to generate camera-captured-like images. However, for scene text images, how should Distortion be incorporated?\n3. In Table 4, DocRes is designed for document rectification. What is the significance of applying it to scene text images?"}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H3QDB1G0MF", "forum": "lGfnvZsE2F", "replyto": "lGfnvZsE2F", "signatures": ["ICLR.cc/2026/Conference/Submission17969/Reviewer_YUNo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17969/Reviewer_YUNo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839361063, "cdate": 1761839361063, "tmdate": 1762927762490, "mdate": 1762927762490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DocRobust-Module (DRM), a lightweight feature restoration module designed to enhance multimodal large language models (MLLMs) for robust document understanding under degradations such as noise, blur, and low resolution. A two-stage training strategy enables DRM to recover lost visual and semantic information efficiently. To support fine-tuning, the authors build DocRobust-VQA, a large-scale dataset with 189K clear-blurry document image pairs and 417K QA annotations. Experiments show that DRM significantly improves MLLM performance on low-quality document images, providing a scalable approach to robust document analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose to focus on the problem of understanding real-world documents with quality degradation, which is of practical requirement. \n- A large-scale dataset is proposed, which consists of 189,771 images paired with 417,502 question-answer pairs, providing sufficient scale and diversity to support the training of multimodal large language models to gain robustness under degraded visual conditions.\n- The paper introduces an adapter to improve the quality of visual tokens, which is learned through a three-stage setting."}, "weaknesses": {"value": "- In this paper, only InternVL2.5-1B is used as a baseline for constructing the model, which may constrain the generalization of the proposed method.\n- In Table 1, the comparison is somewhat unfair; compared with the InternVL2.5-1B baseline, the better performance may mainly benefit from further data training.\n- The technical contribution is relatively not new; the way provided for constructing the model follows a typical way. Besides, how much each stage contributes to the final model is not quite clear."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D5PMaWl2sc", "forum": "lGfnvZsE2F", "replyto": "lGfnvZsE2F", "signatures": ["ICLR.cc/2026/Conference/Submission17969/Reviewer_wJxe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17969/Reviewer_wJxe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915130867, "cdate": 1761915130867, "tmdate": 1762927762111, "mdate": 1762927762111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}