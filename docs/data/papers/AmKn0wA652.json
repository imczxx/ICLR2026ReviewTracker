{"id": "AmKn0wA652", "number": 21040, "cdate": 1758313133891, "mdate": 1763704154525, "content": {"title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge", "abstract": "Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. \nTo address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. \nVisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks.  \nAdditionally, reasoning enhancements such as scaling up inference compute (with ''thinking'' modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.", "tldr": "", "keywords": ["MLLM", "benchmark", "reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5489e74a95b43dc2faf5c7c271a858f3de405a86.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new benchmark designed to evaluate the pure reasoning capabilities of Multimodal Large Language Models, intentionally decoupling reasoning from domain-specific knowledge. The authors argue that current benchmarks, like MMMU, are \"knowledge-intensive,\" making it impossible to know if a model is genuinely reasoning or just recalling facts from its training data. It consists of 1,168 puzzle-like questions (many sourced from the Chinese Civil Service Examination) spanning five categories: algorithmic, analogical, deductive, inductive, and spatial. These questions require minimal external knowledge to solve."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's main strength is its clear and successful isolation of reasoning ability. It provides compelling evidence that current models are poor at pure logic when they cannot rely on their vast stores of memorized knowledge.\n\n2. The paper delivers a critical insight: the current strategies for improving models (scaling size, more verbose \"thinking\") are not effectively improving general reasoning. This challenges a dominant trend in AI development.\n\n3. The authors don't just report scores. They analyze why models fail, showing that Reasoning Error (56%) is the dominant problem, not perception. They also analyze how \"thinking\" modes fail (e.g., verbosity without substance) and how models adopt different strategies for these tasks (\"answer-first\" vs. \"option-first\")."}, "weaknesses": {"value": "1. Limited Scope of \"Puzzles\": The paper admits this limitation. The benchmark consists of abstract, \"puzzle-like questions.\" It's uncertain if a model's (poor) performance on these logic puzzles translates to poor performance on all other forms of knowledge-light, real-world reasoning.\n\n2. Multiple-Choice Format: The evaluation is restricted to multiple-choice questions. This format can be gamed and does not test a model's ability to generate an open-ended reasoned argument from scratch."}, "questions": {"value": "1. How effectively does the benchmark truly decouple reasoning from knowledge? Couldn't \"common-sense\" knowledge (which models have) or implicit biases from pre-training still be significant confounding variables?\n\n2. The benchmark heavily features puzzles from the Chinese Civil Service Examination. To what extent do these questions test a universal, abstract reasoning ability versus a specific, culturally-influenced type of logic puzzle that humans are trained to solve?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I5aACi4OTW", "forum": "AmKn0wA652", "replyto": "AmKn0wA652", "signatures": ["ICLR.cc/2026/Conference/Submission21040/Reviewer_HLRk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21040/Reviewer_HLRk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451409702, "cdate": 1761451409702, "tmdate": 1762940619250, "mdate": 1762940619250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VISUALPUZZLES, a benchmark designed to evaluate multimodal reasoning ability (especially visual reasoning) while minimizing dependence on domain-specific knowledge.\nUnlike existing benchmarks like MMMU, which mix factual and reasoning challenges, VISUALPUZZLES focuses on pure reasoning through puzzle-like tasks that only require common sense and information present in the image and text."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper makes an important attempt to categorize model errors into distinct cognitive stages, namely perceptual, spatial, and reasoning errors.\n2.\tThis classification highlights the multi-stage nature of multimodal reasoning and helps identify where models fail—whether in perception or logic."}, "weaknesses": {"value": "1.\tAmbiguity in borderline cases: Errors involving spatial relations or visual analogies could fall into both perception and reasoning categories. The paper briefly lists “spatial/orientation error” as a separate type (17%) but does not clarify how annotators decided when a spatial issue counts as perception vs. reasoning.\n2.\tNot automatic, labor-intensive, lack of technology. The manual annotation process (100 sampled items) lacks methodological transparency—e.g., how many annotators participated, whether they reached consensus, or if any examples were double-coded.\n3.  The coupling of perception and reasoning has become quite common (like MME-COT)."}, "questions": {"value": "Please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A2D7IE5H3U", "forum": "AmKn0wA652", "replyto": "AmKn0wA652", "signatures": ["ICLR.cc/2026/Conference/Submission21040/Reviewer_LzRj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21040/Reviewer_LzRj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971910671, "cdate": 1761971910671, "tmdate": 1762940618458, "mdate": 1762940618458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VISUALPUZZLES, a 1,168-item multimodal benchmark explicitly designed to evaluate reasoning while minimizing domain-specific knowledge. The authors argue that many current MLLM benchmarks (e.g., MMMU) confound these two factors, so gains are hard to attribute. \nVISUALPUZZLES is built from puzzle-like questions (notably including translated Chinese Civil Service logical questions), normalized to 4-choice multiple choice, and annotated along five reasoning types (algorithmic, analogical, deductive, inductive, spatial) and three difficulty levels. The key empirical findings are: 1) models perform far below even the 5th-percentile human accuracy, 2) models appear to “know enough” to solve the tasks but still fail, 3) scaling model size and adding “thinking/CoT” modes do not reliably help, and 4) performance on knowledge-heavy benchmarks does not transfer to this knowledge-light setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This benchmark decouples “reasoning ability” from “amount of domain knowledge” by using low-knowledge visual puzzles, so it can expose reasoning weaknesses more cleanly. It also compares against MMMU with a knowledge-concept checklist to show it requires significantly less background knowledge. That means scores here are more likely to reflect “can’t reason” rather than “hasn’t memorized enough.” \n\n2. The dataset covers five major reasoning types—algorithmic, analogical, deductive, inductive, and spatial—and balances difficulty (easy/medium/hard) plus text/image options, so the structure is very clear. This design can test general reasoning across different patterns instead of overfitting to just one or two. It also makes later fine-grained error analysis and modular training/evaluation much easier. \n\n3. The authors ran large human and multi-model evaluations and quantified findings like “all models are below the human 5th percentile” and “thinking/long CoT is not consistently helpful,” which shows the benchmark has strong discriminative power. It also reveals phenomena like “strong on MMMU ≠ strong on VISUALPUZZLES” and “bigger models don’t automatically reason better,” giving clear guidance for future model design."}, "weaknesses": {"value": "1. The paper mentions that “Larger models tend to have higher knowledge accuracy, and this often translates into higher overall benchmark performance.”  \nHowever, another fact is that larger models often exhibit planning, reflection, and self-correction abilities that smaller models lack, and the gains brought by model size scaling in these abilities were ignored by the authors in their comparison.  \nThe authors attempt to argue that benchmarks do not primarily test domain knowledge by showing that larger-scale models have no significant performance improvement, but such insignificant improvement may also be due to the narrowness of the data domain, which prevents the aforementioned reasoning capabilities of larger models from being effectively activated.\n\n2. The analysis between the reasoning models and their corresponding baselines is not rigorous enough: GPT-4o and o1, QvQ and Qwen2.5-VL cannot be regarded as strict counterparts, and the quantitative results involve only three groups of model comparisons, which undermines the credibility of the paper’s conclusions.\n\n3. The baselines used in the paper are mostly from before February 2025, which makes them somewhat outdated for evaluating state-of-the-art VLMs. In particular, the paper lacks evaluation of VLMs that have benefited from reinforcement learning techniques since their rise. This makes the experimental conclusions of the paper less instructive for the latest models.\n\n4. The authors emphasize that “One major source of questions is the Chinese Civil Service Examination.”  \nHowever, this type of question is often **non-obvious, rule-based questions with a hidden pattern: if you spot the pattern, they’re easy; if you don’t, they’re almost impossible.**  \nTherefore, they may not represent general logical ability, but rather resemble specialized tasks in a specific domain, whose main difficulty arises from their particularity."}, "questions": {"value": "1. Right now the 82.1% vs 71.5% result is heavily LLM-mediated. Can you provide a small, human-annotated subset (say 100 problems × 2 models) with inter-rater agreement to show that VISUALPUZZLES genuinely induces longer chains of logical steps?\nDid you look at failed chains separately? It could be that VISUALPUZZLES mostly induces longer but wrong chains, which is not quite the same as “higher reasoning complexity.” \n\n2. You show that “thinking” variants do not always help. What happens if you add simple self-consistency (k=5) or MCTS? A negative result there would make your conclusion much stronger."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iAN9w6F8WL", "forum": "AmKn0wA652", "replyto": "AmKn0wA652", "signatures": ["ICLR.cc/2026/Conference/Submission21040/Reviewer_SXNo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21040/Reviewer_SXNo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986875524, "cdate": 1761986875524, "tmdate": 1762940617974, "mdate": 1762940617974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VisualPuzzles, a benchmark designed to evaluate the reasoning capability of multimodal models by separating them from domain knowledge. The dataset contains 1168 multiple-choice questions, divided into five reasoning categories: algorithm, analogy, deduction, induction, and spatial reasoning. Experiments show that there is a large gap between the model's reasoning ability and that of humans."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Many existing benchmark tests confuse knowledge and reasoning, and this paper explicitly proposes to decouple the two, which is an important and valuable research direction."}, "weaknesses": {"value": "1. The Skill-Based Nature of Civil Service Exam Questions: Many logic questions in the Chinese civil service exam (Administrative Aptitude Test), especially graphic reasoning, are highly skill-based. Once these specific patterns are mastered, problem-solving becomes pattern matching rather than genuine general reasoning.\n\n2. Lack of a Human Baseline in Reasoning Category Correlation Analysis: Section 5.2 of the paper points out that the model's performance across different reasoning categories is highly correlated, inferring that this is because the model relies on shared \"surface patterns\" rather than using different cognitive processes like humans. This conclusion lacks a crucial control group: a category correlation matrix of humans on this dataset. Differences in difficulty between question categories and similarities in inherent logic (e.g., algorithms and deductive reasoning themselves may be highly correlated) could all lead to skewed correlations. A human baseline dataset would make this argument more convincing.\n\n3. The paper's extensive qualitative analysis, such as answering strategies (Section 5.1), reasoning patterns (Section 4.4), and error analysis (Section 5.3), focuses primarily on the Claude-3.7-Sonnet-Thinking model. \n  - The paper does not provide sufficient evidence to suggest that phenomena observed in Claude-3.7 (such as the \"Answer-First\" strategy) can be generalized to all other models, especially families of models with potentially drastically different structures and training methods. \n  - Although more experimental results with o4-mini are shown in the appendix, the experimental results (e.g., Answer-First vs. Option-First) seem to differ significantly from those of Claude 3.7, which may indicate that Claude 3.7 cannot be considered a representative model."}, "questions": {"value": "The paper divides reasoning into five categories. However, in practice, many puzzles may involve multiple types of reasoning simultaneously. For example, a complex graphical induction problem may also require spatial reasoning ability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LbnpyULXP6", "forum": "AmKn0wA652", "replyto": "AmKn0wA652", "signatures": ["ICLR.cc/2026/Conference/Submission21040/Reviewer_c6sn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21040/Reviewer_c6sn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991318714, "cdate": 1761991318714, "tmdate": 1762940617279, "mdate": 1762940617279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}