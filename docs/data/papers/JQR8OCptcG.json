{"id": "JQR8OCptcG", "number": 3243, "cdate": 1757384862666, "mdate": 1763752380357, "content": {"title": "Improving Set Function Approximation with Quasi-Arithmetic Neural Networks", "abstract": "Sets represent a fundamental abstraction across many types of data. To handle the unordered nature of set-structured data, models such as DeepSets and PointNet rely on fixed, non-learnable pooling operations (e.g., sum or max) -- a design choice that can hinder the transferability of learned embeddings and limits model expressivity. More recently, learnable aggregation functions have been proposed as more expressive alternatives. In this work, we advance this line of research by introducing the Neuralized Kolmogorov Mean (NKM) -- a novel, trainable framework for learning a generalized measure of central tendency through an invertible neural function. We further propose quasi-arithmetic neural networks (QUANNs), which incorporate the NKM as a learnable aggregation function. We provide a theoretical analysis showing that, QUANNs are universal approximators for a broad class of common set-function decompositions and, thanks to their invertible neural components, learn more structured latent representations. Empirically, QUANNs outperform state-of-the-art baselines across diverse benchmarks, while learning embeddings that transfer effectively even to tasks that do not involve sets.", "tldr": "We introduce quasi-arithmetic neural networks (QUANNs), which use Neuralized Kolmogorov Mean---newly introduced trainable pooling mechanism---to improve set functions approximation.", "keywords": ["representation learning", "set function learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b56d38455680383afb47fec50e40fad281fd010a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposed a novel trainable aggregation function for sets, called Neuralized Kolmogorov Mean, and a quasi-arithmetic neural networks which includes the Neuralized Kolmogorov Mean in it.  The authors explore both unary and binary (similar to set attention) QANNs. The main contributions of the paper are:\n1. A novel aggregation function for sets - the Neuralized Kolmogorov Mean.\n2. Theoretical justification of its soundness, including proofs of permutation invariance and universal approximation properties.\n3. The results show some consistent although sometimes slight benefit over other methods, with the results about transfer learning to and from classification tasks helping to support the author’s claims of better structured latent spaces."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths of the paper include:\n- the idea is quite simple and easily explained, and the paper is clear with its intentions, methods and outcomes.\n- a novel learnable aggregations function\n- theoretical justifications"}, "weaknesses": {"value": "The mean weakness of the paper are:\n- poor comparison against other learnable aggregation functions\n- the majority of the experiments relies on semi-synthetic datasets and very simple datasets such as Omniglot and MNIST, so there is no evidence that the benefits observed would translate into a more meaningful task.\n- the accuracy on ModelNet40 for DeepSets is quite low compared to the original paper (82% for 100 points, and 90% for 5,000 points), but the authors report ~65% (see Table 5).\n\nAll the weakness are expanded and further explained in the question section below."}, "questions": {"value": "The methodological and novelty part of the paper is convincing, but unfortunately the experimental sections is somewhat weak. I would like to mention that there are many methods in the literature, and it would be unrealistic to expect any paper to include all possible competitors. However, it is needed to compare against the most relevant and closely related methods, such as [1] and [2].\n\n**Methodology**.\n1. As it stands, the paper seems to claim (see for e.g. the abstract) that the learnable function is entirely novel. This is inaccurate, as other methods such as PNA[1] and LAF[2] (among others) have already introduced learnable aggregation functions to address the problem of having fixed aggregation functions. Why did the authors not position their proposed aggregation function in relation to [1] and [2]?\n\n**Experiments**. \n1. Comparisons with closely related works (e.g. [1], and [2]) are missing. Is there a justification as of why those methods were not included in the experimental evaluation?\n2. The reported accuracies for DeepSets[3] are much lower compared to the original work. The paper reports ~65% (see Table 5), while the original paper achieved 82% for 100 points and 90% for 5,000 points. Why is there such a discrepancy? Also comparisons agains other methods are missing as previously mentioned. What is the rationale for their exclusion?\n3. The authors mention a potential application for aggregations is in graph learning tasks. There is quite some literature on neural aggregation for graph level tasks (in the context of combing node representations after message passing) and use of e.g. set transformers in that context can give performance uplift. I see no reason why this method would not work in that context, and I would encourage the authors to investigate this aspect, at least briefly. The large number of practical tasks that have graph representations would help increase the potential impact of this work.\n\n**Results**.\n1. The MNIST-Sets experiments are quite related to those presented in [2]. Why was this comparison not included?\n2. The results for the MNIST-Sets experiments (Table 5, \"sum\" task) are difficult to interpret relative to DeepSets, which reported an accuracy (not MSE) of over 60% for sets containing 50 images. It is unclear how MSE is as a meaningful metric for this task, since the aggregation output (a sum) is an integer. What would the accuracy be for QUANN-1 and QUANN-2 on the sum task?\n3. It is not clear why ablation 2 performs so much worse than Ablation 1, when it is the same as Ablation1 but includes more parameters? Is there any explanation?\n\n**Minor issues**.\n1. Table 5 is too small and it should be broken perhaps in two parts. Experiments from the different subsections are summarized in the same Table which makes the reading a bit confusing.\n2. Research questions section: does the proposed approach [learn] a.\n3. Missing closing parenthesis at the end of the Sum decomposition section\n\n[1] Corso, Gabriele, et al. \"Principal neighbourhood aggregation for graph nets.\" Advances in neural information processing systems 33 (2020): 13260-13271.\n\n[2] Pellegrini, Giovanni, et al. \"Learning Aggregation Functions.\" IJCAI. International Joint Conferences on Artificial Intelligence Organization, 2021.\n\n[3] Zaheer, Manzil, et al. \"Deep sets.\" Advances in neural information processing systems 30 (2017)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5YZvFt68nl", "forum": "JQR8OCptcG", "replyto": "JQR8OCptcG", "signatures": ["ICLR.cc/2026/Conference/Submission3243/Reviewer_3i4p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3243/Reviewer_3i4p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760532042685, "cdate": 1760532042685, "tmdate": 1762916624099, "mdate": 1762916624099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents QUANNs, a learnable aggregation function, for (deep) set models. The learned aggregation is favorable over fixed aggregators in terms of performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The formulation of the learnable aggregation function via Neural Kolmogorov Mean is theoretically interesting. Combined with the improved performance and the insights of set function approximation (derivation of derivative and invertability) and theoretical analyses and discussion of the benefits, makes the paper very interesting."}, "weaknesses": {"value": "The ablation is not clear, a discussion of various (left out / alternative) components and their impact on the performance is lacking.\n\nIn Table 3, it is not clear whether the numbers show that the method is capable of the task. The numbers vary greatly across tasks, which makes it hard to assess what the meaning of 'good' performance is here.\n\nIn related work, it is not clear to me how the methods from 3.2 are different from 3.1 - I know they are different, but a short explanation would make this section stronger. Regarding slot attention, a reference is missing, Unlocking Slot Attention by Changing Optimal Transport Costs [1]. Because they also address set prediction tasks, it's interesting to see how the proposed method compares to this reference.\n\nIn the experiments, it does not say whether there were tasks with sets of varying sizes within one task. Dealing with variable-sized sets would make the experiments more interesting.\n\nThere is a discussion but it lacks a critical view of the learnable aggregator, when it may not be beneficial, etc."}, "questions": {"value": "Can you discuss the various contributions of components that are reported in the ablation, why they matter, and their impact on the performance?\n\nCan you explain the difference between the methods from 3.1 and 3.2?\n\nDo some of the experiments contain a task with variable-sized sets?\n\nWhat do the numbers in Table 3 tell us, given that they are so broad in their range?\n\nAre there cases where it is not beneficial to learn the aggregator, but to fix it with a well-chosen function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gCeJeDsDkS", "forum": "JQR8OCptcG", "replyto": "JQR8OCptcG", "signatures": ["ICLR.cc/2026/Conference/Submission3243/Reviewer_ZtCS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3243/Reviewer_ZtCS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761150718784, "cdate": 1761150718784, "tmdate": 1763014965383, "mdate": 1763014965383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comment."}, "comment": {"value": "We would like to thank all reviewers for their thorough evaluation of our work and for the insightful comments and suggestions. In response to the feedback, we have extended our experimental evaluation by adding an additional baseline, LAF (Pellegrini et al. 2021), and by incorporating experiments on the real-world dataset - QM9 (Ramakrishnan et al. 2014}. \n\nBelow, we address each reviewer’s comments and questions in detail."}}, "id": "yDNKK3YXEA", "forum": "JQR8OCptcG", "replyto": "JQR8OCptcG", "signatures": ["ICLR.cc/2026/Conference/Submission3243/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3243/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3243/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763752451100, "cdate": 1763752451100, "tmdate": 1763752451100, "mdate": 1763752451100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel approach for learning the pooling operator in set function approximation. By leveraging a neuralized version of the Kolmogorov mean, it manages to approximate various measures of central tendency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and clear. The choice of Kolmogorov mean and its neutralized version is well-motivated. There is a sound discussion of the theoretical advantage of the proposed solution over alternatives, complemented with promising experimental results."}, "weaknesses": {"value": "There were few works that explicitly addressed the problem of learning pooling operators in the past:\n\n- Euan Ong, Petar Veličković, Learnable Commutative Monoids for Graph Neural Networks, LOG 2022.\n- P. Zuidberg Dos Martires, Neural Semirings, NeSy 2021.\n- G Pellegrini, A Tibo, P Frasconi, A Passerini, M Jaeger, Learning aggregation functions, IJCAI 2021.\n\nWhile none of them directly suggests using the Kolmogorov mean, they all attempt to go beyond predefined aggregators, and at least one (neural semiring, arguably a not very popular paper) explicitly mentions the use of invertible neural networks. Positioning the contribution with respect to these works would better clarify its novelty.\n\nCountability assumption. This is not really a weakness (most existing results, starting from deep sets, assume countable sets), but given its relevance (sets of real vectors are uncountable) it would be worth discussing the assumption in the limitations.\n\nMinor: \n- please check your citation reference fornatting and use citep when appropriate. \n- Pk(X) -> Sk(X) [or viceversa] in the description of Eq. 2"}, "questions": {"value": "Can you clarify the main differences wrt alternative approaches for learning pooling operators?\n\nCan you discuss more clearly the implications of the countability assumption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v9oxvhHPo0", "forum": "JQR8OCptcG", "replyto": "JQR8OCptcG", "signatures": ["ICLR.cc/2026/Conference/Submission3243/Reviewer_71eq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3243/Reviewer_71eq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992059020, "cdate": 1761992059020, "tmdate": 1762916623629, "mdate": 1762916623629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces quasi-arithmetic neural networks (QUANNs) for learning set functions. By parameterizing the Kolmogorov mean function (also called the quasi-arithmetic mean, and hence the name of the proposed architecture), QUANNs can be seen as an extension of the Normalized DeepSets architecture, where the standard mean function in Normalized DeepSets is replaced with a learnable Kolmogorov mean. More generally, QUANNs further allow aggregation over subsets of set elements akin to the Janossy pooling. The authors provide explanation for the theoretical benefits QUANNs, as well as hypotheses for the practical advantage of QUANNs. The author verify their hypotheses by carrying out a set well designed experiments using both synthetic and real-world data. Empirically, QUANNs are shown to outperform state-of-the-art set neural network baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea to parameterize the Kolmogorov mean function and replace the standard mean pooling in existing set neural network architectures is intuitive and interesting. Empirically, the resulting QUANN architecture outperforms all baselines. It is good to see that a simple and principled modification leads to effective performance gain.\n\nThe experiments are well designed to empirically verify the authors' hypotheses on the practical advantages of QUANNs."}, "weaknesses": {"value": "Writing can be improved. \n- Section 5.2 has a lot of references to the theorems in the appendix. It is difficult to follow the discussion in this section without having to read the theorems in the appendix. I think it would be helpful to at least state some informal and short versions of the theorems here. \n- Please use citet{…} and citep{…} appropriately, e.g., use citep{…} for references that appear in lines 28-32. There are many misuses of citet{…} or cite{…} throughout the entire paper, please fix these. In Line 116, should define S_k(X) here, not P_k(X). Be consistent in the use of notations in Equation (2) and (4).\n\n\nI am not sure how practical the new architecture is. The authors should discuss the practical relevancy of set representation learning in the current state of machine learning. Can't LLMs/vLLMs solve the tasks considered in the experiments?"}, "questions": {"value": "For the synthetic data experiment, Ablation 2 should in theory be strictly better than Ablation 1. Why do the results in Table 3 show the opposite? Was it because of bad training, bad tuning/regulations, or else?\n\nAlso for the synthetic data experiment, many of the functions involve computing some average in one way or another. This aligns with the normalization used in QUANN. What happens if you want to learn functions that are not averages, but sums (e.g. vector norms)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qNlws6PVIC", "forum": "JQR8OCptcG", "replyto": "JQR8OCptcG", "signatures": ["ICLR.cc/2026/Conference/Submission3243/Reviewer_oz2P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3243/Reviewer_oz2P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762726147670, "cdate": 1762726147670, "tmdate": 1762916623397, "mdate": 1762916623397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}