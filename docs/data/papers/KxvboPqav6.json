{"id": "KxvboPqav6", "number": 18635, "cdate": 1758289642050, "mdate": 1763709707059, "content": {"title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields", "abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce **Neural Gaussian Force Field (NGFF)**, an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present **GSCollision**, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (∼4 TB). Evaluations on synthetic and real 3D scenarios show NGFF’s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.", "tldr": "", "keywords": ["Physical reasoning", "video prediction"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0018eec4a202839461e4485336f7ba44a1f34bbc.pdf", "supplementary_material": "/attachment/90ac022886fc42c632a139045ebfc3349691daec.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces NGFF, a framework capable of generating videos with physics-grounded dynamics for scenes containing multi-body interaction. For foreground objects represented by 3D Gaussians, a DeepONet, which consumes the geometric and dynamic attribute of pairwise objects, is employed to predict the received forces and torques in a feed-forward manner. Upon this, local stress fields are predicted to model local deformations. The learned force field is then integrated with an ODE solver to predict object’s motion. In conjunction with the model, they also constructed a dataset containing dynamic Gaussians simulated by MPM for training and benchmark the proposed method. Experimental results show that the proposed feed-forward method can achieve accurate dynamic prediction and demonstrates clear generalization in some aspects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper is well-written and easy to follow. \n2.\tThis work shows that force fields in multi-object interactions can be efficiently and accurately predicted by NGFF, with strong positional, temporal and compositional generalization."}, "weaknesses": {"value": "1.\tL192 mentions that they leverage 3D generation model to ”address occlusions and invisible parts” from multi-view input, but how to ensure consistency between assets obtained via single (front) view-to-3D generation and the input multi view content in regions invisible in the front view?\n2.\tIn Equation (3), why can the local force field be predicted only from the net force/net torque, contact region and location/velocity of its own each point, which seems under-determined.\n3.\tThe framework does not consider lighting effects induced by motion, which are crucial for realistic video synthesis. This omission weakens its claimed competence as a video generation method.\n4.\tFitting the dynamics of only 10 objects already requires a 4T dataset, which appears to suggest a poor data efficiency."}, "questions": {"value": "1.\tDoes “10 objects” refer to 10 individual instances or 10 categories? If it means categories, how many objects collected for each category, and why not assess generalization to novel instance? \n2.\tShould generative modeling be considered when scaling to more diverse categories?\n3.\tL965 says that state descriptors are embedded via an MLP, while L208 mentioned that the state $s(t)$ and $\\dot{s}(t)$ containing point cloud and its velocity in $M\\times3$. Are these the same “state”? If so, how are the point clouds processed in this MLP?\n4.\tSome notations in Equation (1) lack clear definition. E.g., $I_{t,k}$ and $\\hat{G}$ are not defined in the context. Does this loss aim to enforce consistency of parameters and rendered image between simulated and predicted Gaussian?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ln1xeRUS1Q", "forum": "KxvboPqav6", "replyto": "KxvboPqav6", "signatures": ["ICLR.cc/2026/Conference/Submission18635/Reviewer_QtAo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18635/Reviewer_QtAo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761582538654, "cdate": 1761582538654, "tmdate": 1762928346553, "mdate": 1762928346553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Neural Gaussian Force Field (NGFF) to address the challenge of predicting physical dynamics from raw visual data. It learns an implicit force field to drive the temporal evolution of 3D Gaussian Splatting, thereby modeling 4D dynamics. Experimental results demonstrate that NGFF achieves state-of-the-art performance in the 4D generation of collision motion in both simulation and real-world cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using an implicit force field, rather than an explicit physics engine, to govern 3D GS evolution is highly innovative. It effectively tackles the critical bottlenecks of large computational overhead and insufficient robustness inherent in tightly coupling 3D GS with physical simulation.\n2. The paper provides extensive quantitative and qualitative results across diverse and challenging evaluations, which demonstrate NGFF's effectiveness in both 4D dynamic prediction and the more challenging real-world simulation."}, "weaknesses": {"value": "1. Limited Performance: Since the model is primarily trained on data generated by the Material Point Method (MPM) simulation, its learned dynamics are inherently limited by the accuracy, fidelity, and approximations of the underlying MPM solver. This raises a concern that the performance of the trained NGFF model is limited to the quality of the synthetic ground truth.\n2. Generalization to Unseen Physics: While NGFF performs excellently on the provided datasets, its ability to generalize to unseen material properties (e.g., how a model trained only on rigid and soft bodies handles sand or fluids) or unseen complex constraints (e.g., complex joints or hinges) remains unclear. This is a crucial factor for real-world applicability.\n3. Missing Implementation Details: Modeling and training details are not clear enough, making this paper hard to follow. For example, what attributes of Gaussian kernels need to be supervised, and how is L' calculated in Eq.1? What is $s$ in Eq.6, and how to convert it to the state of Gaussian kernels?"}, "questions": {"value": "My questions are mainly based on the above weaknesses:\n1. Do the predictions from NGFF perform better compared to the MPM simulation results? Is it possible to train the model with real captured data?\n2. The predicted simulation in the paper is limited to the collision of rigid or soft body objects. Can this method be extended to more complex physical motions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cyHRqoqAi8", "forum": "KxvboPqav6", "replyto": "KxvboPqav6", "signatures": ["ICLR.cc/2026/Conference/Submission18635/Reviewer_sf96"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18635/Reviewer_sf96"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740239899, "cdate": 1761740239899, "tmdate": 1762928346225, "mdate": 1762928346225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The NGFF framework integrates 3D Gaussian perception with Neural Gaussian Force Fields to generate interactive, physically realistic 4D videos from multi-view inputs. It models dynamics by learning an explicit force field integrated via an ODE solver, achieving two orders of magnitude faster simulation than previous Gaussian methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The NGFF framework integrates 3D Gaussian perception and Neural Gaussian Force Fields to generate interactive, physically realistic 4D videos. It models dynamics by learning an explicit force field, achieving two orders of magnitude faster simulation. It also introduces the GSCollision dataset."}, "weaknesses": {"value": "The NGFF framework currently relies on multi-view inputs for reliable 3D Gaussian reconstruction, needing extension to monocular or partial observations to match human ability. The benchmark covers only 10 representative objects, necessitating scaling to far more diverse materials and articulated structures. Additionally, visual quality is sometimes slightly lower than SOTA video generation models due to 3D reconstruction error."}, "questions": {"value": "How will you extend NGFF to robustly predict 4D dynamics from monocular or partial RGB inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MFMiAl8hi8", "forum": "KxvboPqav6", "replyto": "KxvboPqav6", "signatures": ["ICLR.cc/2026/Conference/Submission18635/Reviewer_jh1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18635/Reviewer_jh1Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046257811, "cdate": 1762046257811, "tmdate": 1762928345503, "mdate": 1762928345503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents the Neural Gaussian Force Field (NGFF) for animating 3D Gaussians. In summary, subjects are reconstructed from multi-view visual observations, and a continuous force field animates these reconstructed objects. The force field (NGFF) is predicted using a neural network. To train this force-prediction network, the authors simulate a large-scale dataset using the Moving Particle Method (MPM). The key contributions of this work are the development of the force-prediction network and the creation of the proposed dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work introduces a new approach to generating physically plausible 4D Gaussians in a feed-forward manner. The force-field prediction network is innovative.\n\nThe motivation and method are clearly presented, making them easy to understand.\n\nThe proposed dataset is valuable for studying 4D physics-plausible Gaussians."}, "weaknesses": {"value": "I have two concerns on this work.\n\nI have two concerns regarding this work:\n\n1. The quality of the proposed dataset:\n   (1) Is the MPM simulator truly representative of real-world physics? I worry that a simple MPM may not produce high-quality physics data. If that’s the case, the dataset's value may be diminished.\n   (2) The simulations are conducted in a simple box environment, which lacks complexity and reduces diversity.\n   (3) Although 3DGS offers high fidelity as a representation, there is still a noticeable gap between the rendered video and real-world captured video, as illustrated in Fig. 3.\n\n2. The soundness of the force field prediction network:\n   (1) The predicted target is highly dimensional and ambiguous. I doubt whether the model can effectively learn PDE dynamics from the data."}, "questions": {"value": "- The author should provide clearer information about the dataset: (1) How many particles are included? (2) How are collisions handled? (3) Which constitutive model is used in the Material Point Method (MPM)? (4) How many simulation steps were performed?\n\n- How can you demonstrate that the predicted results align with PDE knowledge rather than relying solely on mass dynamics? (This is an open question.)\n\n- Have you considered objects with uneven quality distribution?\n\n- If the surface of the environment is more complex, such as featuring uneven slopes, can your model accommodate this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yCsViUVrYu", "forum": "KxvboPqav6", "replyto": "KxvboPqav6", "signatures": ["ICLR.cc/2026/Conference/Submission18635/Reviewer_T5cC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18635/Reviewer_T5cC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184699705, "cdate": 1762184699705, "tmdate": 1762928344174, "mdate": 1762928344174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}