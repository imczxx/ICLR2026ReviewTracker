{"id": "N32MEJqbPu", "number": 9165, "cdate": 1758113631739, "mdate": 1763695221764, "content": {"title": "LogicEvolve: Advancing Logical Reasoning Toward Self-Evolution", "abstract": "The rapid progress of large language models (LLMs) highlights the urgent need for continuously evolving benchmarks that keep pace with advancing model capabilities. Yet existing benchmarks often rely on one-off curation or fixed scripts, lacking scalability and long-term adaptability. To this end, we present LogicEvolve, a highly automated multi-agent framework that enables dynamic control of deterministic symbolic tasks' structure, difficulty distribution, and scale with minimal human intervention.  Building on LogicEvolve, we introduce CLUB (Complex Logical Unified Benchmark), spanning diverse task types—including string puzzles, grid reasoning, and card games—for systematic evaluation of logical reasoning. Experiments show that even state-of-the-art models such as Grok4 and GPT-5 reach only ~55–56\\% accuracy across multiple independent evaluations, far below desirable levels, with clear weaknesses in certain subcategories. These findings underscore logical reasoning as a persistent and unsolved core challenge for LLMs. All code, data, and an interactive evaluation platform will be publicly released after the review period, ensuring reproducibility and fostering further research.", "tldr": "We present LogicEvolve, the first multi-agent framework for autonomously generating and evolving logical reasoning tasks, and CLUB, a unified benchmark for systematically evaluating models’ reasoning capabilities.", "keywords": ["logical reasoning", "large language models", "benchmark", "automatic task generation", "multi-agent framework", "self-evolution"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/17f6340094f523f98fe2f67d4bf47a8461776856.pdf", "supplementary_material": "/attachment/e84379389626c3170264f81896736ba57157109d.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a multi-agent framework for developing benchmark tasks for logical reasoning. They generate examples from a wide range of tasks including many traditional types of logic puzzles as well as a few new ones. The benchmark a range of top models and find that performance is inconsistent across tasks, and reasoning depth remains an issue."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The primary original contribution of this paper is the agentic system for generating new task items. This system is useful in being able to generate new benchmark tasks with controls for both problem difficulty and type. The results are comprehensive, assessing a wide range of models and breaking down the scores across several dimensions. These results further support the understanding that LLMs are brittle reasoning agents with challenges in long context settings."}, "weaknesses": {"value": "I am concerned about the soundness of the results in this paper.\n\nSpecifically:\n\n- The \"extensible_zebra_logic\" task appears both as \"extensible\" and \"extended\" (pg 18) but is never actually defined, even in the lengthy appendices, despite being a task which is specifically called out as an example of a challenging novel task.\n- The framework clearly requires human intervention in various places, but the specifics are not described. Appendices C 7-8 only make it clear that interventions are necessary, not how often they occur. This also contradicts the claims about being a \"level 4\" automation.\n- The human puzzle creation baseline is extremely vague, described in four lines with no details about the number of humans, their expertise in creating puzzles, or whether they used any tools.\n- Humans seem like the wrong baseline for puzzle creation anyways, given that just about every puzzle in this benchmark is a well-known puzzle with clear procedural methods for creating new puzzles. I strongly suspect that code for creating these puzzles already exists online in most cases.\n- This task creation system should probably be described as adversarial. Since tasks are removed if \"certain models\" (not specified) are scoring > 90%, the benchmark is selecting for examples that the models do poorly on ex post.\n\nCrucially:\n- I need to see evidence that this framework would generalize to new puzzle types outside of those which are already very common on the internet. The \"evolve\" claims of the framework are not substantiated by showing ability to create common puzzles.\n- Moreover, if there are persistent issues in correctly generating puzzles/metadata (as shown in the need for human intervention) then I would also want evidence that the puzzles would continue to be correct if the system were asked for superhuman level puzzles.\n\nFinally, I dislike the claim of being \"inspired by the process of human evolution\". It is vague and the iterative code generation approach with a verifier is not actually related to evolutionary algorithms or fitness-based selection approaches generally. Using unsubstantiated biological metaphors can mislead readers about the actual results of the paper."}, "questions": {"value": "1. What exactly was the scope of human intervention necessary for the agents to complete the task?\n2. What would be your expectations of a human score on this benchmark?\n3. Why make comparisons to computational complexity? Many NP-complete or higher problems (e.g. set cover) are effectively approximated by a greedy algorithm, which is not logically intensive to use.\n4. What happens if you ask the system to invent a new, more difficult, logic problem type?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NNlKB7XqD7", "forum": "N32MEJqbPu", "replyto": "N32MEJqbPu", "signatures": ["ICLR.cc/2026/Conference/Submission9165/Reviewer_wJvd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9165/Reviewer_wJvd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761302300037, "cdate": 1761302300037, "tmdate": 1762920844529, "mdate": 1762920844529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LogicEvolve, a highly automated multi-agent framework for generating, parsing, and evaluating complex logical reasoning tasks. Building upon this framework, the authors propose CLUB (Complex Logical Unified Benchmark), a dynamic benchmark comprising 10 categories of logical puzzles, with 100 dynamically generated instances per category. CLUB is designed to comprehensively assess large language models' capabilities across diverse reasoning paradigms, including deduction, induction, abduction, and hybrid reasoning. The framework features a novel benchmark maintenance mechanism to prevent data leakage and ensure long-term validity by generating new tasks when models achieve high performance. Extensive experiments reveal significant limitations in state-of-the-art models, highlighting the persistent challenges in long-range consistency and generalization under complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**High Automation and Scalability**: LogicEvolve achieves a high level of automation in both task generation and evaluation pipeline construction, significantly reducing human effort. Its design allows for dynamic expansion in task categories, quantity, and difficulty, making it highly scalable and sustainable.\n\n**Comprehensive Benchmark Design**: CLUB covers a wide range of logical reasoning types (deductive, inductive, abductive, hybrid), reasoning monotonicity (monotonic vs. non-monotonic), and spatial dimensions (1D, 2D), providing a systematic and multi-faceted evaluation of reasoning capabilities.\n\n**Innovative Benchmark Maintenance Mechanism**: The framework introduces a unique ID and timestamp system to prevent data leakage, and automatically generates new tasks when models exceed a performance threshold, ensuring the benchmark remains challenging and relevant over time.\n\n**Empirically Insightful Findings**: The evaluation results offer valuable insights into current models' limitations, such as the \"curse of complexity\" and poor generalization on novel tasks, revealing critical bottlenecks in logical reasoning."}, "weaknesses": {"value": "**Single-dimensional Evaluation**: The benchmark relies solely on final answer accuracy for evaluation, lacking deeper analysis of the reasoning process (e.g., step-by-step correctness, logical consistency). This limits its ability to distinguish between models that arrive at the correct answer through valid reasoning versus those that do so by chance or flawed logic."}, "questions": {"value": "Since LogicEvolve uses LLM agents to generate the CLUB benchmark without extensive human annotation, how can the authors ensure the correctness and reliability of the generated tasks? Would incorporating systematic human evaluation to verify a subset of the benchmark improve its credibility and trustworthiness?\n\nGiven that LogicEvolve can be executed multiple times to generate different benchmarks, how diverse are these benchmarks in terms of logical structure and reasoning depth? Is the diversity merely superficial (e.g., surface-level variations), or does it reflect meaningful differences in underlying logical complexity and reasoning patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l7C1RRQNXP", "forum": "N32MEJqbPu", "replyto": "N32MEJqbPu", "signatures": ["ICLR.cc/2026/Conference/Submission9165/Reviewer_EYHj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9165/Reviewer_EYHj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451111660, "cdate": 1761451111660, "tmdate": 1762920844184, "mdate": 1762920844184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LogicEvolve, a multi-agent framework designed to automatically evolve logical reasoning benchmarks.\n\nThe framework is composed of four key agents:\n- Metadata Agent which defines task structures and logical rules  \n  - input : base **assumptions (B)** or **instance set (I)**  or **both**;\n  - output: **task meta** like the puzzle_rule、parameters、examples ;\n- Generator Agent  which synthesizes new problem instances  \n  - input: **task meta** \n  - output: **new_instance** contain <puzzle, question, answer>\n- Solver Agent which verifies solvability and ensures unique solutions  \n  - input: **new_instance**\n  - output: **response** containing <answer, metric, cot>\n- Evaluator Agent which automates the entire pipeline  \n  - input: new_instance + answer + eval config \n  - output: **evaluation_report** with model metrics & feedback\n\nBuilding upon this framework, the authors release CLUB (Complex Logical Unified Benchmark)—a dynamically extensible suite of ten logic-reasoning task types such as Sudoku, Zebra puzzles, string transformations, and card games.\nExperiments over 30+ LLMs show that even frontier systems like Grok-4 and GPT-5 achieve only ~55% accuracy, demonstrating that complex logical reasoning remains unsolved."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a meaningful and timely problem — the limitations of static reasoning benchmarks that can easily lead to model overfitting or data leakage. The idea of using a multi-agent system to automatically generate and evolve new logical reasoning tasks is interesting and has clear potential for maintaining benchmark freshness and scalability over time.\n\nThe overall scope and amount of work are substantial. The paper presents not only a conceptual pipeline but also an implemented system that includes task definition, automatic generation, solver verification, and evaluation. The proposed CLUB benchmark is large and diverse, and the experiments cover more than thirty state-of-the-art language models with detailed analyses, showing a solid amount of engineering effort."}, "weaknesses": {"value": "1. Lack of evidence that CLUB is better or more discriminative than prior benchmarks.\n\n  While CLUB is positioned as a “next-generation” benchmark, the paper provides no direct comparison between CLUB and existing logical reasoning datasets (e.g., LogiQA2.0, BIG-Bench Hard, SynLogic).\n  - Lack of validation for the “evolution” itself. The paper does not compare model results before and after the benchmark evolves\n  - Without showing that new tasks change model rankings (instead of scores) or reveal different weaknesses, the claim of benchmark evolution remains unconvincing.\n  If the model ranking order remains identical to that on older benchmarks, CLUB’s incremental value is limited.\n\n2. Unclear implementation details and insufficient justification for the agent-based design\n\n  - The paper presents LogicEvolve as a multi-agent framework, but the design remains mostly conceptual. The paper does not specify which framework or environment is used to build the agents, how they communicate, or whether they share contextual information across multiple rounds of interaction. It is also unclear how the system maintains memory or state during iterative “self-evolution,” which is central to the claimed autonomy of the framework.\n  - The motivation for adopting an agent-based formulation is not fully explained. Could the same workflow be realized with a sequence of coordinated prompts or scripts? What concrete benefits does the “agent” setup bring compared with simpler prompting pipelines? Beyond comparing against human task curation, are there alternative baselines or studies showing that an agentic setup leads to better task quality or reasoning diversity? It would also help to discuss whether recent advances (e.g., better context engineering, communication strategies, or shared-memory mechanisms) could make such agents more effective in this setting.\n\n3.Overclaiming generality of the framework and benchmark.\n\n  - Although the paper claims to “advance logical reasoning as a whole,” the proposed CLUB benchmark only covers a narrow range of symbolic and puzzle-style reasoning tasks (e.g., Sudoku, Zebra puzzles, Maze).\n  - These are well-defined, parameterized problems that differ substantially from more open and dynamic reasoning settings explored in recent works—such as commonsense and causal reasoning (CommonsenseQA) or agent-based and interactive reasoning (SWE、 GDPeval). These categories of reasoning cannot be easily “evolved” by the LogicEvolve framework, since they rely on open-world knowledge, dynamic context, or environment interaction rather than rule-based constraint solving. The paper’s claims should therefore be re-scoped to emphasize automated generation of structured logic puzzles, rather than general logical reasoning capabilities."}, "questions": {"value": "1. Regarding the benchmark evolution and validation (related to Weakness 1):\n  - Could the authors provide quantitative evidence that the “evolved” CLUB benchmark differs from its earlier or static versions?\n  - How do model rankings or performance correlations change before and after the benchmark evolves?\n  - Have the authors analyzed whether the newly generated tasks introduce new reasoning challenges or failure modes not captured by previous datasets?\n  - If the rankings remain largely consistent, how should we interpret the notion of “evolution” — as diversity, difficulty scaling, or conceptual novelty? \n\n2. Regarding the agent-based framework (related to Weakness 2):\n- What framework or implementation environment is used for the agents? Are they built on an existing toolkit (e.g., LangChain, AutoGen, CAMEL), or implemented via custom prompting pipelines?\n- How do the agents communicate and share context across iterations? Is there any persistent memory mechanism or state tracking to support “self-evolution”?\n- Could the same workflow be implemented using a simpler sequence of prompts instead of an explicit multi-agent setup?\n- Have the authors compared the agent-based version with such baselines, or explored design choices (e.g., better context engineering or communication strategies) that make the agentic formulation more effective?\n\n3. Regarding the generality of the framework and reasoning coverage (related to Weakness 3):\n- How do the authors position LogicEvolve relative to other reasoning paradigms such as commonsense reasoning (e.g., CommonsenseQA, WinoGrande), textual deductive reasoning (ProofWriter), or interactive agent reasoning (SWE, GdpEval)?\n- Do the authors envision LogicEvolve being extended to these open-domain or dynamic reasoning settings, or is it intended primarily for structured symbolic puzzles?\n- What limitations would prevent LogicEvolve from handling tasks that require external knowledge, causal inference, or environment interaction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tlj3ibwC1g", "forum": "N32MEJqbPu", "replyto": "N32MEJqbPu", "signatures": ["ICLR.cc/2026/Conference/Submission9165/Reviewer_6SZY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9165/Reviewer_6SZY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882420564, "cdate": 1761882420564, "tmdate": 1762920843726, "mdate": 1762920843726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces LogicEvolve, a novel, highly automated multi-agent framework designed to autonomously generate and evolve dynamic logical reasoning tasks, and presents CLUB (Complex Logical Unified Benchmark), a challenging new benchmark built using this framework. The experiment underscores that that current LLMs are far from achieving satisfactory performance on higher-order and complex reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper describes the methods clearly. The CLUB benchmark is challenging and effective to evaluate the reasoning capabilities of LLMs. The experiment on a number of LLMs provide insights on the limitations of current LLMs. The  code and data to be released will also be helpful to the community."}, "weaknesses": {"value": "1. Even though the agent workflow are described in detail, the model information is missing. For each agent, what model do you use and why?\n2. The evaluation only covers LLMs without tools. Since the benchmarks are generated with code, it is likely that LLMs can perform well with tools like code interpreter. Including this setting will make the experiment more comprehensive."}, "questions": {"value": "1. Since all the data are automatically generated by the agents, which may have hallucination.  Can you guarantee that they are solvable and the gold answers are correct?\n2. What model do you use to generate the benchmark and what is the cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tb9sE2LWzr", "forum": "N32MEJqbPu", "replyto": "N32MEJqbPu", "signatures": ["ICLR.cc/2026/Conference/Submission9165/Reviewer_K1t9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9165/Reviewer_K1t9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990439274, "cdate": 1761990439274, "tmdate": 1762920843363, "mdate": 1762920843363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}