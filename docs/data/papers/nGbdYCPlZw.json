{"id": "nGbdYCPlZw", "number": 8026, "cdate": 1758052489383, "mdate": 1759897813360, "content": {"title": "Class-Conditional Domain Alignment via Kernel Cauchy-Schwarz Mutual Information", "abstract": "Domain Generalization (DG) seeks to learn models that are robust to unseen distribution shifts, a critical challenge for real-world machine learning applications. A dominant paradigm is to enforce domain invariance by aligning feature distributions from multiple source domains. However, aligning marginal feature distributions indiscriminately can discard critical class-discriminative information, especially when class priors vary across domains. We address this limitation with Domain Alignment via Kernel Cauchy-Schwarz Mutual Information (DAS-MI), a novel framework that advances principled class-conditional alignment. The core principle is to maximize the statistical dependence between same-class features across different domains. We operationalize this using the Cauchy-Schwarz Quadratic Mutual Information (CS-QMI), a powerful information-theoretic measure. Critically, and in contrast to prior work relying on complex approximations or adversarial training, our approach yields a closed-form, non-parametric alignment objective derived from kernel density estimates. This results in a stable loss that integrates seamlessly into deep learning pipelines. Extensive experiments across five benchmark datasets demonstrate performance comparable to state-of-the-art methods. DAS-MI offers a theoretically-grounded and practically efficient solution to domain generalization that robustly preserves discriminative information.", "tldr": "We derive a closed-form alignment objective from kernel mutual information that enforces class-conditional invariance, ensuring robust generalization to unseen domains.", "keywords": ["Information Theory", "Domain Generalization", "Mutual Information", "out-of-distribution generalization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d4b6b05064f83d5cc48ca1b293b4db52aeeafa0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes DAS-MI, a domain-generalization method that replaces traditional marginal alignment with class-conditional alignment using the Cauchy-Schwarz Mutual Information. The authors derive a high-probability generalization bound showing that OOD risk depends on the mutual information $I(Z;D|Y)$, which leads to a trainable objective that can be approximated via kernel methods with finite samples. They evaluate on several conventional OOD benchmarks, showing higher accuracy compared to the current SOTA method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method is built upon an intuitive adaptation of a rigorously derived high-probability generalization bound. The additional conditioning on $Y$ may bring extra improvement when the label distribution differs across domains. Empirical results show significant improvements on several OOD benchmarks."}, "weaknesses": {"value": "* I don't agree that the new bound based on $I(Z;D|Y)$ (eq. 11) is tighter than the original one using $I(Z;D)$. It's easy to verify that $I(Z;D|Y) = I(Z,Y;D) - I(Y;D) \\ge I(Z;D) - I(Y;D)$. When the label distribution $P_{Y|D}$ is identical across domains, we have $I(Y;D) = 0$ and thus this new bound is strictly worse than the original one. This condition is unfortunately met in the selected 5 OOD benchmarks, which means that the proposed class-conditional method is actually worse than the marginal matching methods (e.g., CORAL) under the current experimental settings.\n* The methodological contribution is somewhat limited. The theoretical framework and the Cauchy-Schwarz divergence are both borrowed from existing works. The proposed method thus looks like a direct application of the CS divergence on distribution-matching-based domain generalization algorithms.\n* The current experimental settings may lead to unfair comparison. The standard DomainBed benchmark requires conducting a hyperparameter sweep in the given range to enforce a fair comparison, simulating the costs of finding the optimal hyperparameter combination. However, the hyperparameters are directly fixed to manually selected ones for the proposed method (Table 2), which is unfair for the other baseline methods."}, "questions": {"value": "* I would suggest that the authors consider more benchmarks where the label distribution varies across domains, as this is the only case where the new bound based on $I(Z;D|Y)$ may be tighter than the original one. Also, please adapt to the standard hyperparameter sweep for a fair evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "u6A0zcK975", "forum": "nGbdYCPlZw", "replyto": "nGbdYCPlZw", "signatures": ["ICLR.cc/2026/Conference/Submission8026/Reviewer_NGsE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8026/Reviewer_NGsE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761000867707, "cdate": 1761000867707, "tmdate": 1762920025985, "mdate": 1762920025985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DAS-MI, a novel domain generalization (DG) framework that addresses the limitations of marginal feature alignment by enforcing class-conditional domain alignment through kernel Cauchy-Schwarz Quadratic Mutual Information (CS-QMI). The authors establish a theoretical foundation by deriving a high-probability generalization bound linking class-conditional mutual information minimization to target-domain risk."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper exhibits several significant strengths. First, it makes a substantial theoretical contribution by rigorously connecting class-conditional CS-QMI minimization to DG generalization bounds using kernel methods, avoiding restrictive assumptions common in adversarial or variational approaches. The algorithmic design is elegant: the closed-form CS-QMI estimator (Eq. 18) enables efficient optimization without auxiliary networks, while kernel Gram matrix operations ensure numerical stability and easy integration into deep learning pipelines."}, "weaknesses": {"value": "Despite its strengths, the paper has notable limitations in algorithmic and experimental components. The work lacks convergence guarantees for the proposed loss (Eq. 20). The non-convexity of neural networks and kernel-based objectives is not addressed, leaving uncertainty about optimization behavior (Section 3.1–3.2). The evaluation is confined to standard DG datasets (max 0.6M images), with no testing on high-dimensional, large-scale data (e.g., ImageNet-21K).\n\n\nThe theoretical framework contains several limitations requiring attention. I've carefully checked the proof, with some confusing parts below. Please correct me if I was wrong. The bounded density ratio assumption (used in Eq. 13) is unjustified for deep features, which may exhibit complex class boundaries violating this condition (Section 2.4 and Appendix C). Additionally, the paper assumes feature distributions are smooth for kernel density estimation but does not reconcile this with the high-dimensional, sparse nature of deep features, creating a contradiction (Section 3.1 vs. Section 2.1). In Appendix C, the inequality $I(Z;D|Y) \\lesssim \\sum_{i \\neq j} D_{CS}(P_i^c, P_j^c)$ (Eq. C.6–C.7) overlooks the convexity gap; CS divergence is convex, but the average pairwise divergence does not directly bound JS divergence without multiplicative constants. The constant $M'$ in Eq. 13 is also undefined, obscuring its dependence on loss boundedness and density ratios (Section 2.4). \n\nI would be willing to reconsider the ratings if the above concerns are well discussed and solved."}, "questions": {"value": "Please refer to the above weakness.\n\nA good job. And I am mainly interested in the theory counterpart."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RJXIrc2OwD", "forum": "nGbdYCPlZw", "replyto": "nGbdYCPlZw", "signatures": ["ICLR.cc/2026/Conference/Submission8026/Reviewer_KebG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8026/Reviewer_KebG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557470246, "cdate": 1761557470246, "tmdate": 1762920025603, "mdate": 1762920025603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a class-conditional alignment framework via Kernel Cauchy-Schwarz Mutual Information, which captures same-class features across different domains. The authors operationalize this using the Cauchy-Schwarz Quadratic Mutual Information (CS-QMI), yielding a closed-form, non-parametric, and stable alignment objective. Extensive experiments across different datasets demonstrate comparable performance over SOTA methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper derives a tighter class-conditional MI-based domain generalization bound. Driven by their theoretical results, they propose the algorithm’s objective, which connects the derived bound via the empirical estimator of CS divergence (CS-QMI).\n* They further propose a computationally efficient, closed-form estimator for the CS-QMI, yielding a closed-form, non-parametric, and stable optimization objective.\n* Experimental results demonstrate their comparable and even better performance over SOTA methods."}, "weaknesses": {"value": "* The bounded density ratios is the strict assumption. Are the proposed methods and theoretical results effective when the domain shift is large? Additionally, this paper implicitly assumes that concept shift does not exist. If indeed, the authors should discuss the impact and limitations of the theoretical results and proposed methods when the concept shift exists.\n* The authors claim the proposed method is ‘computationally efficient’. The experiments should include a comparison of computation time and memory overhead.\n* Experiments should include ablation studies about the performance improvements stemming from the novel CS-QMI estimator itself or from the class-conditional alignment mechanism (compared to aligning the entire domain feature distribution without conditioning on the class label)."}, "questions": {"value": "refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NOQGRJtSjC", "forum": "nGbdYCPlZw", "replyto": "nGbdYCPlZw", "signatures": ["ICLR.cc/2026/Conference/Submission8026/Reviewer_oVSG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8026/Reviewer_oVSG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836247609, "cdate": 1761836247609, "tmdate": 1762920024883, "mdate": 1762920024883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use Cauchy-Schwarz Mutual Information (CS-QMI) for the domain generalization task. Based on the previous theoretical analysis on the generalization bound (Dong et al., 2025), the authors show that CS-QMI leads to a tighter bound. The paper performs experimental comparisons on 5 different datasets, with 2 outperforming the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using CS-QMI for the domain generalization task is interesting. \n\nThe paper tries to derive a tighter generalization bound."}, "weaknesses": {"value": "1. ``Restrictive assumptions, such as the requirement for extensive domain annotations, or incur prohibitive computational overheads ...''. Can you specify this? From this sentence, I cannot follow what the motivation is for using Mutual information. This should be clarified. \n\n2. I would also suggest moving the key Lemma to the main manuscript to highlight the central insight and smooth the connection to the proposed objective. \n\n3. For Contribution 2, please clearly specify how your estimator differs from Yu et al.\\ (2024b), which also uses KDE with kernel Gram matrices. The formulations of the two are similar. \n\n4. ``Under mild assumptions of bounded density ratios'', please state the assumptions precisely (bounds, supports, regularity) and show how they are used to derive Eq. 12. \n\n5. Equation numbering in the Appendix is inconsistent (some equations are numbered, others are not). Please make numbering consistent throughout.\n\n6. Lemma 2 equates $I(Z;D\\mid Y{=}c)$ with the uniform-weight JS divergence over domains. Real DG rarely has $P(D\\mid Y{=}c)$ uniform; please extend the identity/bounds to non-uniform weights or discuss limitations. When $P(D\\mid Y{=}c)$ is uniform, the class-conditional MMD is also equivalent to the conditional MMD, both tractable via KDE. This should be discussed and compared with. \n\n7.  Assumption 2 requires $a_c \\\\le \\\\frac{P_{i}^{c}(z)}{\\\\bar P^{c}(z)} \\\\le b_{c} \\\\quad \\\\text{a.e.},$\nwhich implicitly enforces shared support across domains and a positive lower bound on the barycenter density, $\\\\inf_{z} \\\\bar P^{c}(z) \\\\ge \\\\kappa_c > 0.$ This assumption is too strong and can fail precisely under domain shift.\n\n8. The experimental results are not convincing: only two out of five datasets reach SOTA. This raises concerns about generalizability across datasets. \n\n9.  Line 400 states ``perform ablation studies to understand the key components of our approach,'' but no ablations are shown in that section. This is misleading; comprehensive ablations, especially for key components, are needed.\n\n10. The paper uses a CS-divergence-based estimator for DG. Please compare against other divergences, especially KL and JSD (theory references Eq.~12), as well as MMD, class-conditional MMD, and conditional MMD. Discuss theoretical advantages (assumptions, tightness, estimation properties) and provide empirical comparisons."}, "questions": {"value": "See my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BMA3PNdmmR", "forum": "nGbdYCPlZw", "replyto": "nGbdYCPlZw", "signatures": ["ICLR.cc/2026/Conference/Submission8026/Reviewer_RrMY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8026/Reviewer_RrMY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890367282, "cdate": 1761890367282, "tmdate": 1762920024460, "mdate": 1762920024460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}