{"id": "gbxhesw4UH", "number": 15108, "cdate": 1758247865353, "mdate": 1759897328175, "content": {"title": "Rescue: Retrieval Augmented Secure Code Generation", "abstract": "Despite recent advances, Large Language Models (LLMs) still generate vulnerable code. \nRetrieval-Augmented Generation (RAG) has the potential to enhance LLMs for secure code generation by incorporating external security knowledge.  \nHowever, the conventional RAG design struggles with the noise of raw security-related documents, and existing retrieval methods overlook the significant security semantics implicitly embedded in task descriptions.\nTo address these issues, we propose \\textsc{Rescue}, a new RAG framework for secure code generation with two key innovations. \nFirst, we propose a hybrid knowledge base construction method that combines LLM-assisted cluster-then-summarize distillation with program slicing, producing both high-level security guidelines and concise, security-focused code examples. Second, we design a hierarchical multi-faceted retrieval to traverse the constructed knowledge base from top to bottom and integrates multiple security-critical facts at each hierarchical level, ensuring comprehensive and accurate retrieval. \nWe evaluated \\textsc{Rescue} on four benchmarks and compared it with five state-of-the-art secure code generation methods on six LLMs. The results demonstrate that \\textsc{Rescue} improves the SecurePass@1 metric by an average of 4.8 points, establishing a new state-of-the-art performance for security. Furthermore, we performed in-depth analysis and ablation studies to rigorously validate the effectiveness of individual components in \\textsc{Rescue}. Our code is available at \\url{https://anonymous.4open.science/r/RESCUE}.", "tldr": "", "keywords": ["secure code generation", "retrieval augmented generation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cccbf74a4880b0ca8a741260ee09c5664ceb710f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces RESCUE, a retrieval-augmented framework designed to enhance the security of code generated by LLMs. The framework constructs a hybrid knowledge base by combining LLM-summarized security guidelines with statically extracted code slices and employs a hierarchical retrieval process across three facets. Experiments across six LLMs and four benchmarks demonstrate consistent improvements in both functionality and security, including a 4.8% gain in SecurePass@1 on CodeGuard+. The work effectively integrates static analysis with retrieval to guide more secure code generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a timely and important problem concerning security vulnerabilities in code generated by large language models.\n- Demonstrates strong empirical results through extensive experiments across multiple models and benchmarks.\n- The paper is clearly written and well-organized."}, "weaknesses": {"value": "- Incremental contribution relative to prior work.\nWhile RESCUE is a well-engineered and empirically validated framework, its core ideas appear incremental relative to existing work in secure code generation. Several prior studies have already applied retrieval-augmented generation (RAG) to integrate security knowledge into large language models. For instance, CodeGuarder [1] employs security-enhanced retrieval to inject vulnerability knowledge and code examples, while Tony et al. [2] dynamically retrieve CWE-based secure coding guidelines to mitigate unsafe generations. Furthermore, hierarchical RAG structures similar to RESCUE’s have been explored in HiRAG [3]. As a result, the primary novelty of RESCUE seems to lie in method integration and empirical comprehensiveness, rather than methodological innovation. The authors should more clearly articulate the conceptual distinctions between RESCUE and these prior approaches and include them as baselines to substantiate the claimed advantages.\n\n>[1] Lin, Bo, et al. “Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection.” arXiv preprint arXiv:2504.16429 (2025).\n\n>[2] Tony, Catherine, Emanuele Iannone, and Riccardo Scandariato. “Retrieve, Refine, or Both? Using Task-Specific Guidelines for Secure Python Code Generation.” ICSME 2025.\n\n>[3] Huang, Haoyu, et al. “Retrieval-Augmented Generation with Hierarchical Knowledge.” arXiv preprint arXiv:2503.10150 (2025).\n\n- Limited scale and coverage of the security knowledge base.\nThe security knowledge base used by RESCUE is derived from SafeCoder, containing only 372 Python and 332 C/C++ instances. This limited scale raises concerns regarding coverage, representativeness, and generalization, especially given the diversity and complexity of real-world software ecosystems. The authors should discuss the cross-domain applicability of RESCUE, its robustness to unseen frameworks or programming languages, and how the restricted dataset size may constrain retrieval effectiveness.\n\n- Unclear design of recursive summarization pipeline.\nThe recursive summarization pipeline groups CWE clusters into batches of ≤10 instances, but the paper does not specify whether batching is guided by semantic similarity, class balance, or representativeness. If batches are formed randomly, distinct vulnerability–fix pairs may be merged, yielding over-generalized or contradictory summaries. Fixed batch sizes may also introduce information imbalance, causing rare yet critical vulnerabilities to be underrepresented. Such an uncontrolled summarization process risks semantic drift and bias in the resulting knowledge base.\n\n- Insufficient justification of rank fusion method.\nAlthough the paper identifies rank fusion as a key innovation, its mathematical formulation and empirical validation are insufficiently detailed. The “modified RRF” method is described only through a formula involving thresholding and rank filtering, with hyperparameters stated to follow prior work. However, no ablation study isolates its contribution or demonstrates how it improves retrieval quality compared to standard RRF. As presented, the approach reads as a heuristic adjustment rather than a rigorously justified advancement."}, "questions": {"value": "1. How does RESCUE fundamentally differ from prior RAG-based systems such as CodeGuarder or HiRAG?\n2. Can RESCUE generalize effectively to diverse real-world vulnerabilities beyond its limited training set?\n3. What is the computational cost of RESCUE in realistic code-generation scenarios, and how feasible is its deployment at scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K00tn1O3B2", "forum": "gbxhesw4UH", "replyto": "gbxhesw4UH", "signatures": ["ICLR.cc/2026/Conference/Submission15108/Reviewer_M2w4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15108/Reviewer_M2w4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761695744152, "cdate": 1761695744152, "tmdate": 1762925430423, "mdate": 1762925430423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes \\textsc{Rescue}, a hybrid knowledge base construction method that combines LLM-assisted cluster-then-summarize distillation with program slicing, producing both high-level security guidelines and concise, security-focused code examples. It designs a hierarchical multi-faceted retrieval to traverse the constructed knowledge base from top to bottom and integrates multiple security-critical facts at each hierarchical level.\nThis paper evaluates \\textsc{Rescue} on four benchmarks and compared it with five state-of-the-art secure code generation methods on six LLMs. \nIts evaluation results demonstrate that \\textsc{Rescue} improves the SecurePass@1 metric by an average of 4.8 points."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper focuses on addressing an important question\n- This paper's results has shown substantially improvement compared with baselines\n- This paper design a comprehensive retrieval approach for related vulnerabilities"}, "weaknesses": {"value": "Generally, the paper tries to address an important security problem while some concerns about the evaluation setting exist.\n\n1.1 Evaluation Benchmark. The main concern of this paper is that the evaluation benchmarks are programming-contest benchmarks (HE, BCB, LCB). These benchmarks are mainly self-contained, and mostly function-level. Avoiding vulnerabilities in these benchmarks are not convincing and this paper can be substantially improved after including real-world code-generation/-completion benchmarks. \n\n1.2 Additionally, the retrieved dataset consists only real-world (repo-level) vulnerabilities and their patches. Thus, this paper needs to add necessary motivation examples to illustrate how these retrieved examples help address potential vulnerabilities. \nPlease note that Figure 2 is not convincing. In a HumanEval/Livecodebench task, it is usually meaningless to replace `yaml.load()` with `yaml.safe_load()`.\n\n1.3 Vulnerability Selection. The SafeCoder dataset consists both C/C++ and Python vulnerabilities while the evaluation is conducted on only Python (HE, BCB, LCB).\n\n2. Note that vulnerabilities can be divided into functionality-specific (e.g., SQL injection) and non-functionality-specific (i.e., memory-related ones) [1], this paper is suggested to add a deep analysis about how various CWE types are used in retrieval and their contribution to the end-to-end improvement.\n\n[1] Chen T, Wang Z, Li L, et al. Detecting Functionality-Specific Vulnerabilities via Retrieving Individual Functionality-Equivalent APIs in Open-Source Repositories[C]//39th European Conference on Object-Oriented Programming (ECOOP 2025). Schloss Dagstuhl–Leibniz-Zentrum für Informatik, 2025: 6: 1-6: 27."}, "questions": {"value": "Please refer to the preceding questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aNTda5orr4", "forum": "gbxhesw4UH", "replyto": "gbxhesw4UH", "signatures": ["ICLR.cc/2026/Conference/Submission15108/Reviewer_WRYK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15108/Reviewer_WRYK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748437777, "cdate": 1761748437777, "tmdate": 1762925430032, "mdate": 1762925430032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a framework called RESCUE, which aims to enhance LLMs to generate more secure code. The core innovations include a hybrid knowledge base construction method that uses LLM-assisted summarisation and static program slicing to distil raw security data into clear guidelines and concise code examples. Additionally, RESCUE employs a hierarchical multi-faceted retrieval system that proactively analyses coding tasks based on vulnerability causes, API patterns, and code similarity to accurately retrieve relevant security knowledge. Experiments across multiple benchmarks and six different LLMs demonstrate that RESCUE significantly improves both security and functional correctness. Ablation studies confirm the necessity of the specialised knowledge base construction and the effectiveness of the multi-faceted retrieval strategy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- RESCUE consistently outperforms all existing methods across multiple benchmarks in terms of SecurePass@1, a comprehensive metric that jointly evaluates both functionality and security. This demonstrates the framework's ability to generate code that is not only correct but also resistant to common vulnerabilities, representing a significant advancement over prior approaches that often sacrifice one dimension for the other.\n- The paper proposes a novel and systematic method for building a refined, hierarchical knowledge base from raw security data, effectively addressing the noise and redundancy present in conventional RAG designs. By combining LLM-assisted summarization with static program slicing techniques, RESCUE creates concise, actionable security guidelines and distilled code examples that are directly applicable to code generation tasks."}, "weaknesses": {"value": "- The evaluation framework used in the paper, which relies on static security analysis tools, presents a key limitation: The evaluation framework employs static security analysis tools, which are known to potentially generate false positives and negatives\n- RESCUE introduces some additional time cost to achieve its security improvements. Although the paper suggest the overhead is acceptable and can be further reduced through engineering optimizations, the additional costs are mostly unclear"}, "questions": {"value": "- What is the computational overhead?\n- Have the authors checked for potential false positives in their static analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tdTx73f0QF", "forum": "gbxhesw4UH", "replyto": "gbxhesw4UH", "signatures": ["ICLR.cc/2026/Conference/Submission15108/Reviewer_zcNL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15108/Reviewer_zcNL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821674799, "cdate": 1761821674799, "tmdate": 1762925429268, "mdate": 1762925429268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RESCUE, a RAG-based framework for secure code generation with two main innovations: (1) a hybrid knowledge base construction combining LLM-assisted cluster-then-summarize distillation with program slicing, and (2) hierarchical multi-faceted retrieval that analyzes API patterns, vulnerability causes, and code similarity. The method achieves 4.8% average improvement in SecurePass@1 across 4 benchmarks and 6 LLMs. The authors also performs ablations to find that the security knowledge base construction is helpful over directly security data and the hierarchical retrieval consistently outperforms non-hierarchical baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Core approach is technically sound with clear motivation\n- Comprehensive experiments (4 benchmarks, 6 LLMs, 5 baselines)\n- Ablation studies validating the main components\n- The paper is generally well-organized structurally and is easy to understand"}, "weaknesses": {"value": "- The novelty of the contribution seems to be in applying the different components like cluster-and-summarize knowledge base with heirarchical retrieval. The method involves too many moving components like API pattern and vulnerability cause analysis. The gains based on the complexity of the system seems not much significant, which would limit the adoption of such an approach for secure code generation.\n- The method involves many different hyperparameters like hop limit, thresholds for api and vulnerability analysis, batch size for cluster-and-summarize, etc. No justification is provided for how the value are chosen for most such hyperparameters.\n- Given the complexity of the approach, it's possible that some of the gains in the paper come from overfitting"}, "questions": {"value": "- How were the threshold values (τ_API=4.0, τ_VA=0.75, τ_C=0.65) selected, and how sensitive are your results to these choices?\n- Your training data contains only 9 CWE types for Python and 12 for C/C++, but the benchmark has 23 and 17 respectively (Table 8). Can you provide a breakdown of performance on seen vs. unseen CWE types, and explain how the method generalizes when no similar examples exist in the knowledge base?\n- Given that RESCUE seems to adds 3-4x computational overhead compared to zero-shot generation, can you provide a cost-benefit analysis showing when this overhead is justified, and propose a lightweight filtering mechanism to apply RESCUE only to security-critical code? This could be big enough to be part of the future work.\n-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8a3kNlq1sH", "forum": "gbxhesw4UH", "replyto": "gbxhesw4UH", "signatures": ["ICLR.cc/2026/Conference/Submission15108/Reviewer_YZm1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15108/Reviewer_YZm1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762443296554, "cdate": 1762443296554, "tmdate": 1762925428725, "mdate": 1762925428725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}