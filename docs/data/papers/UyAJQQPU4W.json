{"id": "UyAJQQPU4W", "number": 6023, "cdate": 1757950796507, "mdate": 1759897939200, "content": {"title": "VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding", "abstract": "Multimodal large language models (MLLMs) often struggle to ground reasoning in perceptual evidence. We present a systematic study of perception strategies—explicit, implicit, visual, and textual—across four multimodal benchmarks and two MLLMs. Our findings show that explicit perception, especially when paired with textual cues, consistently yields the best improvements, particularly for smaller models. Based on this insight, we propose VTPerception-R1, a unified two-stage framework that decouples perception from reasoning. Stage I introduces perception-augmented fine-tuning, and Stage II applies perception-aware reinforcement learning with novel visual, textual, and consistency rewards. Experiments demonstrate that VTPerception-R1 significantly improves reasoning accuracy and robustness across diverse tasks, offering a scalable and auditable solution\nfor perception-grounded multimodal reasoning.", "tldr": "We enhance multimodal reasoning by explicitly grounding visual and textual perceptions before reasoning, resulting in consistently improved performance across diverse benchmarks.", "keywords": ["Reasoning model", "multimodal large model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b360146db17f4a5b1139785800e0042528d2fe1.pdf", "supplementary_material": "/attachment/e79e380b19086da39c221746fe1ae3b59b2e754e.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose VTPerception-R1, a two-stage framework that decouples perception from reasoning in multimodal large language models (MLLMs). Through a systematic analysis of explicit, implicit, visual, and textual perception strategies across four multimodal benchmarks and two MLLMs, the study reveals that explicit perception—especially when combined with textual cues—consistently leads to the largest performance gains. Building on these findings, VTPerception-R1 introduces a two-stage framework with novel visual, textual, and consistency rewards. Experimental results demonstrate improvements in accuracy across diverse tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors provide insightful analyses of how different visual and textual perception configurations influence model performance on Qwen2.5-VL-32B and Qwen2.5-VL-7B, offering valuable understanding of perception strategies in multimodal reasoning.\n\n2. They further propose a reinforcement learning–based approach that demonstrates clear performance improvements over the baseline models, highlighting the effectiveness of perception-aware training."}, "weaknesses": {"value": "1. The reward design appears overly complex, potentially leading to substantial increases in training cost and implementation difficulty.\n\n2. The reported performance gains are relatively modest. Among the five evaluated benchmarks, MathVista and MMMU even show declines in accuracy, while the remaining three benchmarks exhibit only marginal improvements of 0.5%, 1.5%, and 0.6%, respectively.\n\n3. The effectiveness of the reinforcement learning strategy remains unclear. A baseline comparison with a naive RL setup—using only format and answer rewards on the same dataset—is missing. Moreover, ablation results indicate that removing certain rewards does not cause notable degradation and, in some cases, even improves performance."}, "questions": {"value": "How are the rewards assigned—particularly the Visual Key-Info Reward, Textual Key-Info Reward, and Description–Reasoning Consistency Reward? Are these rewards automatically generated by large language models (LLMs), or are they computed through predefined rules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "69W3hGmnXX", "forum": "UyAJQQPU4W", "replyto": "UyAJQQPU4W", "signatures": ["ICLR.cc/2026/Conference/Submission6023/Reviewer_3Y1B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6023/Reviewer_3Y1B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761211780599, "cdate": 1761211780599, "tmdate": 1762918418871, "mdate": 1762918418871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how to better ground multimodal AI reasoning in perceptual evidence, finding that explicit perception guided by textual cues is most effective. The authors then propose VTPerception-R1, a two-stage framework that decouples perception from reasoning using specialized fine-tuning and reinforcement learning. Their method significantly improves reasoning accuracy and robustness across various tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a structured, empirical study comparing different perception strategies.\n\n- It introduces VTPerception-R1, a two-stage framework that decouples perception from reasoning and combines fine-tuning with perception-aware reinforcement learning rewards.\n\n- The proposed method is empirically validated to boost performance across diverse tasks."}, "weaknesses": {"value": "While this paper conducts an empirical analysis, the resulting conclusion appears to be somewhat straightforward and unsurprising. Moreover, the differences in the experimental results presented in Table 1 are not particularly significant (approximately 1%).\n\nThe method proposed in this paper is simple and straightforward, lacking sufficient technical novelty or contribution."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iCiWEGfdNT", "forum": "UyAJQQPU4W", "replyto": "UyAJQQPU4W", "signatures": ["ICLR.cc/2026/Conference/Submission6023/Reviewer_1tEU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6023/Reviewer_1tEU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761398974367, "cdate": 1761398974367, "tmdate": 1762918418559, "mdate": 1762918418559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VTPerception-R1, a unified two-stage training framework designed to enhance multimodal reasoning in large vision–language models. In Stage I, the model undergoes SFT, where each sample is extended with a structured < description> section that summarizes key visual and textual evidence before reasoning and answering. In Stage II, the model is further optimized using reinforcement learning based on the DAPO algorithm, which integrates novel rewards targeting visual perception, textual perception, and reasoning–evidence consistency. Experiments across multiple multimodal benchmarks show that VTPerception-R1 significantly improves reasoning accuracy and robustness, particularly for smaller models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper reinforces the consensus that explicitly incorporating visual cues is crucial for improving multimodal reasoning."}, "weaknesses": {"value": "**Poor writing and unclear presentation.** The paper is not easy to follow, and several important concepts are insufficiently explained. For example (but not limited to), the “visual notes” mentioned in Table 1 are not described in the text, and key terms such as structured visual grounding and structured visual–text grounding lack clear definitions or illustrative examples. The authors are encouraged to clarify these concepts and improve overall readability.\n\n**Questionable conclusion**:  The claim that “incorporating additional textual perception yields only marginal performance gains” may not be universally valid. The current experiments lack benchmarks that specifically require reasoning over textual information, making the conclusion insufficiently supported by evidence.\n \n**Limited novelty:** The benefit of visual perception for reasoning has been observed in prior studies, and the proposed method mainly reuses existing SFT and RL procedures with small design tweaks (e.g., the < description> tag). The paper would benefit from a clearer explanation of what is conceptually new compared to prior visual CoT methods. \n\n**Unsatisfactory experimental results and ablations.** The paper does not clearly demonstrate the benefits of using the < description> tag during SFT. Moreover, in Table 3, removing certain reward components unexpectedly improves performance on some benchmarks (e.g., removing Visual key-info improves MathVista, and removing Textual key-info improves MMMU), suggesting that the contribution of these components is not consistently positive. The paper lacks comparison with representative RL baselines such as GRPO, DAPO, and PAPO. Although the method builds upon a DAPO-style optimization framework, no quantitative ablations are provided to demonstrate its advantage over prior RL algorithms."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9cizb3C05q", "forum": "UyAJQQPU4W", "replyto": "UyAJQQPU4W", "signatures": ["ICLR.cc/2026/Conference/Submission6023/Reviewer_bMgG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6023/Reviewer_bMgG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484333222, "cdate": 1761484333222, "tmdate": 1762918418245, "mdate": 1762918418245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VTPerception-R1, a two-stage framework designed to enhance perceptual grounding in MLLMs. The work begins with a systematic study of perception strategies across four MM reasoning benchmarks (MMMU, MathVista, EMMA, OlympiaBench) and two MLLMs Qwen2.5-VL models (32B and 7B versions). Findings indicate that explicit perception, especially when paired with textual cues, yields consistent improvements, particularly for smaller models.\nFollowing these, the proposed VTPerception-R1is a two-stage framework that decouples perception from reasoning. Stage I involves perception-augmented fine-tuning, and Stage II uses perception-aware reinforcement learning with verifiable visual, textual, and consistency rewards focusing on different required aspects during perception and reasoning. Experiments show that VTPerception-R1 significantly improves reasoning accuracy and robustness across six benchmarks. Analysis on the rewards are also provided."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality:\n  1. Novel RL signals: The introduction of visual, textual, and consistency rewards in stage II is a novel contribution. This sophisticated reward mechanism directly optimizes for perceptual grounding and reasoning alignment, which is a key requirement for auditable and robust MLLMs.\n  2. The core idea of explicitly decoupling perception from reasoning and addressing them in a unified two-stage framework (VTPerception-R1) is valuable and clearly motivated. This modularity offers an good solution to perception-grounded reasoning.\n\n- Quality: The paper begins with a systematic comparative study of various perception strategies (explicit vs. implicit, visual vs. textual) across multiple benchmarks and models. This grounding empirical analysis clearly justifies the design choices of the proposed framework and provides valuable insights for the field.\n\n- Clarity: This paper is very well-organized and easy to follow. This is also contributed by the systematic study in section 3, which clearly state the motivation and thereby introduce the design for following two-stage approach. \n\n- Significance: By promising a scalable and auditable solution that significantly improves reasoning accuracy and robustness, the paper addresses a critical limitation of current MLLMs, which could be helpful to diverse research tasks and also practical scenarios."}, "weaknesses": {"value": "1. Limited diversity in model families: Experiments rely mainly on Qwen2.5-VL models; results on other architectures (e.g., InternVL-style models) would enhance generality claims.\n2. Moderate novelty in algorithmic form: While the perception-aware RL design is novel, the framework builds upon existing SFT+RL training pipelines, R1-style reasoning pipelines and DAPO-style training methods, the technical contributions are primarily integrative from this perspective. \n3. Ablation depth: Although reward ablations are strong, further analysis on the impact of the perception-first weighting schedule or the <description> length constraint would better demonstrate the underlying mechanism.\n4. Limited qualitative interpretation: While performance metrics are comprehensive, additional qualitative reasoning traces could illustrate the interpretability gains more vividly."}, "questions": {"value": "1. In line 270 (or 269), what is the format of visual evidence between \\<description\\>\\</description\\>, are they textual discriptions of visual information or encoded visual representations? In my understanding, you use textual discriptions for visual evidence (not sure if I miss anything?). If it is correct, have you tried (or why not) directly using visual representations or combining it with visual representations? For example, the visual representation could be encoded form a cropped key area for reasoning.\n\n2. How do you sample the training data from existing dataset? Please provide detailed statistics.\n\n3. Following the above question, the data used to train Vision-R1 and Vision-SR1 are also employed in this paper. What's reasoning for preventing VTPerception-R1 to outperform these models regarding MathVista Benchmark?\n\n4. Can you provide further analysis on the three rewards (visual, textual and consistency) regarding task types? I suppose MathVista and MMMU present different features compared to the other employed benchmarks, as they prefer either visual or textual key-info reward instead of the full model. \n\n5. Since the framework aims for an auditable solution, please include visualizations of the model's intermediate steps. Can the system clearly pinpoint why a decision was made (e.g., the exact visual crop and textual description used from the decoupled perception stage)? Showing a failure case where a baseline hallucinates but VTPerception-R1 corrects itself would be highly impactful.\n\nSuggestion:\nThere is a typo in line 255: \"¡description¿ fields\", which might be caused by Latex package conflict."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uGBMfcmmkx", "forum": "UyAJQQPU4W", "replyto": "UyAJQQPU4W", "signatures": ["ICLR.cc/2026/Conference/Submission6023/Reviewer_EWYv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6023/Reviewer_EWYv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900440804, "cdate": 1761900440804, "tmdate": 1762919141457, "mdate": 1762919141457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}