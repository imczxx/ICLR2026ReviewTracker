{"id": "RYHzkIqHI4", "number": 679, "cdate": 1756768214190, "mdate": 1763677730359, "content": {"title": "Image Tokenizer Needs Post-Training", "abstract": "Recent image generative models typically capture the image distribution in a pre-constructed latent space, relying on a frozen image tokenizer. However, there exists a significant discrepancy between the reconstruction and generation distribution, where current tokenizers only prioritize the reconstruction task that happens before generative training without considering the generation errors during sampling. In this paper, we comprehensively analyze the reason for this discrepancy in a discrete latent space, and, from which, we propose a novel tokenizer training scheme including both main-training and post-training, focusing on improving latent space construction and decoding respectively. During the main training, a latent perturbation strategy is proposed to simulate sampling noises, i.e., the unexpected tokens generated in generative inference. Specifically, we propose a plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer, thus boosting the generation quality and convergence speed, and a novel tokenizer evaluation metric, i.e., pFID, which successfully correlates the tokenizer performance to generation quality. During post-training, we further optimize the tokenizer decoder regarding a well-trained generative model to mitigate the distribution difference between generated and reconstructed tokens. With a $\\sim$400M generator, a discrete tokenizer trained with our proposed main training achieves a notable 1.60 gFID and further obtains 1.36 gFID with the additional post-training. Further experiments are conducted to broadly validate the effectiveness of our post-training strategy on off-the-shelf discrete and continuous tokenizers, coupled with autoregressive and diffusion-based generators.", "tldr": "", "keywords": ["Image Generation", "Tokenizer", "Post-training"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8aee04933e4bf5000120718783bacf3ea63aab10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the causes of performance discrepancies in discrete latent tokens and introduces a new tokenizer training framework composed of main-training and post-training stages. The main-training phase incorporates a latent perturbation strategy to mimic sampling noise and improve the robustness and efficiency of tokenizers. A training method and a new evaluation metric, namely pFID, are also proposed to better align tokenizer performance with generation quality. In the post-training phase, the tokenizer decoder is further optimized with respect to a trained generative model to reduce the gap between generated and reconstructed tokens."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "[1]. The core idea is simple yet effective, and the authors provide solid experimental evidence to demonstrate its validity.\nThe comparisons with existing methods are fairly comprehensive, which helps position the contribution clearly within the current literature.\n\n[2]. The paper presents extensive experiments and analyses that enhance the reader’s understanding of the proposed approach.\nThese results and discussions are valuable for future research in this area and may inspire subsequent work on improving image tokenizers and post-training strategies."}, "weaknesses": {"value": "[1]. The design of the loss function appears overly complex, and there is no verification of parameter sensitivity for each hyperparameter. I suggest providing a table summarizing the optimal values of these parameters and explaining how those optimal values were determined.\n\n[2]. It would be helpful to include experiments on the text-to-image task using JourneyDB. Without T2I results, it is difficult to assess the generalization ability of your method. Considering the complexity of your loss function, it may also be challenging for future researchers to effectively apply your approach in practice.\n\n1. Sun K, Pan J, Ge Y, et al. Journeydb: A benchmark for generative image understanding[J]. Advances in neural information processing systems, 2023, 36: 49659-49678.\n\n[3]. There is a typo in Table 4 (c): **Perservation ratio** should be corrected to **Preservation ratio.**\n\n[4]. There are several issues in both the references and the OpenReview abstract, which suggest that this work may have been completed under significant time constraints.\n4.1. The abstract on OpenReview contains the character sequence \\ie, which appears to be an unprocessed LaTeX command.\n4.2. The reference section includes numerous duplicated entries and formatting inconsistencies, indicating that the manuscript was not thoroughly proofread before submission. This raises concerns about the overall accuracy and attention to detail in the paper’s presentation.\n\n*Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\n Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\n image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\n arXiv:2010.11929, 2020.*\n\n*Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\n Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\n scale, 2021. URL https://arxiv.org/abs/2010.11929.*\n\n*Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with\n vq-vae-2. Advances in neural information processing systems, 32, 2019a.*\n\n*Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with\n vq-vae-2, 2019b. URL https://arxiv.org/abs/1906.00446.*\n\n*J Ning, C Li, Z Zhang, Z Geng, Q Dai, K He, and H Hu. All in tokens: Unifying output space of\n visual tasks via soft token. arxiv 2023. arXiv preprint arXiv:2301.02229.*\n\n*X Zhu, W Su, L Lu, B Li, X Wang, and J Dai. Deformable detr: Deformable transformers for\n end-to-end object detection. arxiv 2020. arXiv preprint arXiv:2010.04159, 2010.*\n\n4.3. Some of the cited works appear to have low relevance to your study and could be removed.\n\n[5] Figure 16 still contains many noticeable artifacts. For example, the text on the clock in the first row, first column is distorted; the bird’s tail in the first row, third column appears blurry; the human figure in the second row, first column fails to reconstruct properly; and the dog in the second row, second column is unclear. These examples demonstrate that, while your method shows certain improvements compared to the pre–post-training baseline, the overall visual quality remains subpar. This suggests that the effectiveness of your improvements is somewhat limited and raises questions about the actual contribution of the proposed pFID metric."}, "questions": {"value": "[1]. Why not use adversarial noise? It would be interesting to explore how the model behaves when adversarial noise is applied. Would the proposed tokenizer or post-training strategy still maintain robustness under such perturbations?\n\n[2]. How many GPU hours were required for training, and what kind of GPUs were used for inference? Please provide detailed computational requirements, including total GPU hours and the type or number of GPUs used. This information is important for assessing the reproducibility and scalability of your method.\n\n[3]. What happens when using codes that are weakly correlated with the latent code? An analysis or ablation using latent codes that are less correlated with the original ones would help clarify the stability and robustness of the learned representation.\n\n[4]. How does your method perform on datasets other than ImageNet? Evaluating on additional datasets would strengthen the claim of generalization and demonstrate that the proposed post-training method is not limited to ImageNet-specific settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "q2oeVJzVmv", "forum": "RYHzkIqHI4", "replyto": "RYHzkIqHI4", "signatures": ["ICLR.cc/2026/Conference/Submission679/Reviewer_3GFk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission679/Reviewer_3GFk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761291618281, "cdate": 1761291618281, "tmdate": 1762915579867, "mdate": 1762915579867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the gap between reconstruction and generation in image tokenizers and introduces a two‑stage scheme called RobusTok. In the main training stage, a latent perturbation strategy is applied in discrete latent space to simulate sampling noise, yielding a tokenizer that is more robust to generation errors. A new metric, pFID, is proposed to better correlate tokenizer quality with downstream generation performance. In the post‑training stage, the tokenizer decoder is further optimized with respect to a well‑trained generator to mitigate the remaining distribution gap between reconstructed and generated tokens. With empirical experiments, it indicates effectiveness across both autoregressive and diffusion‑based generators and for off‑the‑shelf discrete and continuous tokenizers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear analysis on the reconstruction–generation gap.\n- In this paper, it analyzes the cause of the discrepancy in discrete latent space and introduces pFID, a metric designed to align tokenizer evaluation with downstream generative quality. The empirical study provides detailed evidence supporting the conjecture that robustness in latent space improves generation.\n\n2. Method aligns the conjecture with practice.\n- The proposed two-stage design, consisting of latent perturbation for robust latent construction and post-training for decoder alignment, closely follows the stated hypothesis and is validated through comprehensive experiments on both autoregressive and diffusion generators."}, "weaknesses": {"value": "1. Loss design complexity and attribution.\n- The loss formulation in Appendix A.2 makes tokenizer training appear somewhat complex. While the central conjecture is that simulating sampling error via latent perturbation reduces the reconstruction–generation gap, the perturbation is combined with several other loss terms, making the specific contribution of the perturbation harder to isolate. \n- It remains unclear whether the observed gains persist without semantic regularization from an external vision encoder (e.g., DINO). A clear ablation that removes semantic regularization and isolates the effect of each loss component would strengthen the claims.\n\n\n2. Unclear criteria for when post-training is beneficial.\n- The paper motivates post-training as a means to address the residual gap between latents produced by a trained generator and those seen during tokenizer training. However, it remains ambiguous under what circumstances post-training should be applied. Since pFID is introduced as a diagnostic measure correlated with generation quality, clarifying whether a specific pFID threshold is used to trigger or terminate post-training would improve the claim.\n\n\n3. Limited scope of experiments.\n- The experiments are conducted primarily on the ImageNet 256×256 benchmark. It would strengthen the empirical validation to include additional experiments on ImageNet 512×512 or MS-COCO, demonstrating the generalization of the proposed method to higher resolutions and more diverse datasets.\n\n\n4. Additional baselines for completeness.\n- In Table 2 (class-conditional ImageNet 256×256), it would be beneficial to include references to FlexTok [1] and ResGen [2] within the related work discussion. Including these recent methods would strengthen the comparative context and highlight where the proposed approach stands relative to concurrent advances in tokenizer design.\n\n\nReferences\n- [1] FlexTok: Resampling Images into 1D Token Sequences of Flexible Length\n- [2] Efficient Generative Modeling with Residual Vector Quantization-Based Tokens"}, "questions": {"value": "1. Figure 7 (t‑SNE) controls.\n- In the t-SNE visualization comparing latent spaces with and without latent perturbation, were all other losses and regularizers, including semantic regularization from any external encoder, kept identical across conditions? Providing an explicit description of the controls or including an ablation without semantic regularization would make the attribution to latent perturbation more convincing.\n\n2. Post‑training scheduling and stopping criteria.\n- What metric and threshold determine when to start and when to stop post-training in practice? Is pFID used as a signal to initiate post-training, and is gFID or pFID employed as the stopping criterion? If pFID is intended as a practical diagnostic, it would be useful to know whether it consistently converges toward a target threshold during post-training across different models and datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KziR2yxnx9", "forum": "RYHzkIqHI4", "replyto": "RYHzkIqHI4", "signatures": ["ICLR.cc/2026/Conference/Submission679/Reviewer_ugEE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission679/Reviewer_ugEE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980727285, "cdate": 1761980727285, "tmdate": 1762915579669, "mdate": 1762915579669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors argue that conventional tokenizers, optimized solely for reconstruction fidelity, lack the necessary robustness to handle the noisy and out-of-distribution latent codes produced during the generative inference stage. To solve this, they propose a two-stage training scheme called \"RobusTok\". The first stage introduces a latent perturbation strategy to build a robust latent space by simulating sampling noises. The second stage, post-training, further refines the tokenizer's decoder to align it with the specific distribution of latents generated by a pre-trained generative model. The paper presents a new evaluation metric, pFID, to correlate tokenizer performance with generation quality. Through extensive experiments on autoregressive models, the authors demonstrate that their method significantly improves generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a novel analysis of the discrepancy between reconstruction and generation performance in token-based generative models. It argues that tokenizer robustness, not just reconstruction fidelity, is a critical factor for high-quality image synthesis.\n- The proposed two-stage training scheme presents a practical and effective solution."}, "weaknesses": {"value": "- The proposed method of injecting noise into the latent space to achieve robustness bears a strong resemblance to VAE. To properly contextualize this contribution, the authors should provide a direct comparison with VAVAE [1], another relevant work that also leverages VAE and DINO features.\n\n- The paper's ablation studies should be expanded to include experiments conducted *without* the DINO distillation loss. First, to disentangle the performance gains attributable to the proposed latent perturbation from those provided by DINO; and second, to validate whether the noise injection strategy remains effective in the absence of DINO semantic guidance. Furthermore, the motivation for incorporating DINO distillation is not explicitly justified and appears to be treated as a default choice, whereas its role and necessity should be clearly articulated.\n\n- As shown in Tables 5 and 7, the proposed latent perturbation strategy, while boosting generative performance, consistently degrades reconstruction quality. This introduces a critical trade-off between generation and reconstruction that the authors should discuss in detail. A thorough analysis of this trade-off is needed to understand its implications and to clarify whether this degradation is a fundamental compromise or a side effect that could be mitigated.\n\n[1] Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "O9c8wxiXs0", "forum": "RYHzkIqHI4", "replyto": "RYHzkIqHI4", "signatures": ["ICLR.cc/2026/Conference/Submission679/Reviewer_3zmd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission679/Reviewer_3zmd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984629367, "cdate": 1761984629367, "tmdate": 1762915579532, "mdate": 1762915579532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the mismatch between clean encoder latents and generated tokens produced by a pretrained generator.\nTo mitigate this discrepancy, they first train the tokenizer with random latent perturbations, improving decoder robustness to noise.\nIn a second stage, they fine-tune the decoder on the generator’s actual reconstruction errors to further align it with the generator’s distribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Achieves strong FID despite using a relatively small model.\n- Clearly demonstrates the weakness of training tokenizers independently from generators.\n- Introduces a simple yet effective strategy to simulate generator errors through post-training."}, "weaknesses": {"value": "- Requires an additional ~50 epochs of training — could this process be done jointly with generator training?\n- The generator’s parameter count is small, but encoder–decoder size and efficiency are not discussed.\n- While rFID improves, comparisons on PSNR or SSIM would strengthen the evaluation."}, "questions": {"value": "- Is each tokenizer specific to a given generator, or can it generalize across generators?\n- FID is strong, but how does it perform on the validation set? Could the model be overfitting? What about visualization on more challenging classes where other generator usually fails?\n- Could the authors elaborate on the role and impact of the DINO contrastive loss?\n- pFID correlates more closely with gFID than rFID. why does this relationship hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vXyOBfC5dD", "forum": "RYHzkIqHI4", "replyto": "RYHzkIqHI4", "signatures": ["ICLR.cc/2026/Conference/Submission679/Reviewer_Vx6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission679/Reviewer_Vx6s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991163001, "cdate": 1761991163001, "tmdate": 1762915579369, "mdate": 1762915579369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}