{"id": "4kz4586euw", "number": 14176, "cdate": 1758229746035, "mdate": 1759897385990, "content": {"title": "Simulation-Free Structure Learning for Stochastic Dynamics", "abstract": "Modeling dynamical systems and unraveling their underlying causal relationships is central to many domains in the natural sciences. Various physical systems, such as those arising in cell biology, are inherently high-dimensional and stochastic in nature, and admit only partial, noisy state measurements. This poses a significant challenge for addressing the problems of modeling the underlying dynamics and inferring the network structure of these systems. Existing methods are typically tailored either for structure learning or modeling dynamics at the population level, but are limited in their ability to address both problems together. In this work, we address both problems simultaneously: we present StructureFlow, a novel and principled simulation-free approach for jointly learning the structure and stochastic population dynamics of physical systems. We showcase the utility of StructureFlow for the tasks of structure learning from interventions and dynamical (trajectory) inference of conditional population dynamics. We empirically evaluate our approach on high-dimensional synthetic systems, a set of biologically plausible simulated systems, and an experimental single-cell dataset. We show that StructureFlow can learn the structure of underlying systems while simultaneously modeling their conditional population dynamics --- a key step toward the mechanistic understanding of systems behavior.", "tldr": "We introduce a principled approach for jointly recovering the underlying network structure and dynamic response of a physical system using flow- and score-matching.", "keywords": ["Structure Learning", "Trajectory Inference", "Single-cell", "Flow Matching", "Schrödinger Bridge"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a59db2525fbfe20c30ce0ab10ae41aec6137e304.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes STRUCTUREFLOW, a simulation-free framework to jointly learn (i) the network structure of a high-dimensional stochastic dynamical system and (ii) its conditional population dynamics from snapshot data (observational and CRISPR-style interventions). Technically, the method formulates the joint task as a multi-marginal Schrödinger Bridge and trains a pair of neural fields with [SF]²M. Interventions are modeled as ideal knockouts by masking outgoing influences in the graph layer. Empirically, on (a) linear SDE synthetic systems up to d=500, (b) six BoolODE “biological” systems, and (c) a real single-cell CRISPR time-series, the method shows higher AP/AUROC for structure recovery than baselines such as RF, SCODE, dynGENIE3, OTVelo, TIGON, and competitive to [SF]²M/TIGON for dynamic inference; on RENGE left-out timepoints, average W2 5.634 vs 9.718 (RF); on left-out interventions, average W2 5.683 (best among joint methods). Training uses Sinkhorn couplings between adjacent snapshots and Group Lasso for sparsity. Results are reported typically over five seeds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work formulates joint structure+dynamics learning as a multi-marginal SB with [SF]²M, avoiding inner-loop simulation.\n2. On BoolODE systems, AP/AUROC for structure recovery are consistently top-tier across six systems.\n3. Method assumptions and architecture are well delineated. Besides, results are separated for tasks that baselines can vs cannot handle.\n4. Simulation-free training enables scaling as shown in Fig. 2, and the intervention generalization is practically valuable in biology."}, "weaknesses": {"value": "1. The introduction could come with more references in the motivation stage. \n2. A line of work is missing in the related works: relational inference / structural inference. These works also work on structure learning and have much to do with the single-cell data.\n3. This work comes with strong assumptions. An ideal knockout (zeroing all outgoing edges) may not reflect realistic CRISPR effects (partial/pleiotropic impacts). I would like to suggest testing robustness to imperfect masks and off-target edges.\n4. Moreover, the stationary structure between marginals is strong. Please consider adding experiments with time-varying graphs or change-point detection to assess failure modes."}, "questions": {"value": "1. Provide an experiment where you freeze $s_\\phi$ after pretraining (or fix $\\sigma$) and show that the learned structure in $v_\\theta$ remains stable. Conversely, clamp sparsity and let $s_\\phi$ adapt, and check how dynamics/structure metrics trade off?  \n2. It would be good to see more ablations. Please consider sweeping $\\alpha$ (flow vs score loss), Group-Lasso $\\lambda$, graph hidden width, and knockout mask schedule. If possible, please include CIs over ≥3 seeds."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7YqfYRpTZ8", "forum": "4kz4586euw", "replyto": "4kz4586euw", "signatures": ["ICLR.cc/2026/Conference/Submission14176/Reviewer_6hGC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14176/Reviewer_6hGC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760942656487, "cdate": 1760942656487, "tmdate": 1762924632876, "mdate": 1762924632876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces STRUCTUREFLOW, a simulation-free framework that jointly performs dynamical inference and structure learning for stochastic systems from snapshot data (including interventional conditions). The method parameterizes an autonomous neural graphical vector field (NGM) for structure and a time-dependent score; training adopts [SF]^2M losses with entropic OT pairings between adjacent timepoints, and structure is read off via group-lasso on the first NGM layer. Experiments span synthetic systems and a single-cell CRISPR dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Clear, modular parameterization: autonomous NGM for stationary structure + time-varying score; neat and practically convenient. \n- Includes interventional modeling via explicit knockout masks; joint training is end-to-end."}, "weaknesses": {"value": "- The core building blocks—[SF]^2M training and NGM structure parameterization—are established; the main additions seem to be the autonomous/score split and intervention masks. This feels incremental.\n- The graph is learned purely from data; no biological priors or constraints are incorporated. This limited the identifiability and interpretability.\n- Although the paper refers to its formulation as a “multi-marginal Schrödinger Bridge problem” (Intro), the derivations and training procedure operate only on adjacent timepoint couplings.  There is no global multi-marginal coupling or joint entropy minimization across all timepoints, as in formal multi-marginal SB theory (e.g., Chen et al., NeurIPS 2023, Theodoropoulos et al., NeurIPS 2025).  This may suggest a conceptual misunderstanding: the proposed approach is not a true multi-marginal SB formulation in my view.\n- Structure is read from the NGM’s first layer via group-lasso norms, but the paper provides no identifiability/consistency guarantees for recovering directed structure. Please formalize the conditions under which edges are recoverable, or position results as a heuristic.  \n- The model assumes isotropic diffusion and explicitly notes it does not consider unbalanced settings (birth/death/mass change), which are central in single-cell population dynamics. This limits biological fidelity, especially relative unbalanced style approaches that model growth/death. \n- Training repeatedly computes Sinkhorn EOT couplings between snapshots; the paper highlights “simulation-free” benefits but does not report wall-clock, memory, or scaling curves in sample size and dimension. Please add complexity analysis and runtime/memory tables for EOT and NGM components.\n- The manuscript lists RF, OTVelo, TIGON, and [SF]^2M as baselines, but provides fewer implementation details. Without such detail, it is difficult to assess the fairness of the comparison. Moreover, while the paper emphasizes “simulation-free efficiency,” no runtime or memory analysis is presented relative to baseline methods. Including detailed baseline descriptions and efficiency metrics would strengthen empirical validity.\n- I do not find the LLM usage statement, which is required according to the ICLR submission guidelines."}, "questions": {"value": "1.\t**Clarification of the “multi-marginal Schrödinger Bridge” formulation**\nCould you clearly define your optimization problem? Does it jointly couple all timepoint distributions in a single entropy-regularized objective (as in Chen et al., NeurIPS 2023; Theodoropoulos et al., NeurIPS 2025)?\nIf not, please justify the terminology “multi-marginal” and explain how your pairwise couplings maintain temporal consistency across all marginals.\n2.\t**Identifiability and structure recovery**\nUnder what mathematical or statistical conditions can your readout from the NGM reliably recover directed edges?\n3.\t**Fairness and transparency of baseline comparisons**\nWere the baselines (RF, OTVelo, TIGON, [SF]^2M) retrained in your pipeline using identical preprocessing, data splits, and comparable hyperparameter search budgets (e.g., Optuna sweeps)? Could the authors provide more details on the implementation of the baseline?\n4.\t**Computational efficiency and scalability**\nHow does STRUCTUREFLOW’s runtime and GPU memory footprint scale with the number of samples and timepoints compared to other methods?\nA wall-clock or asymptotic scaling analysis could support your “simulation-free efficiency” claim.\n5.\t**Extension to unbalanced dynamics**\nSince many biological systems exhibit cell proliferation and death, could your framework be extended to an unbalanced Schrödinger Bridge?\n6.\t**Use of biological priors** Have you explored incorporating known biological priors to guide sparsity or edge weighting?\nIf not, do you expect such priors would improve identifiability or stability in the real dataset experiments?\n7.\t**LLM usage disclosure** Did the team use large language models in code generation, writing, or data preparation? If yes, please include this information as required by the ICLR.\n\n\nI hope I have not misunderstood some aspects of the work. I am open to revising my score should the authors provide a convincing and well-supported rebuttal.\n\nReferences\n- Tong et al., Simulation-Free Schrödinger Bridges via Score and Flow Matching, AISTATS 2024. \n- Chen, T., Liu, G.-H., Tao, M., & Theodorou, E. A. (2023). Deep Momentum Multi-Marginal Schrödinger Bridge. NeurIPS 2023.\n- Theodoropoulos, P., Saravanos, A. D., Theodorou, E. A., & Liu, G. H. (2025). Momentum Multi-Marginal Schr\\\" odinger Bridge Matching. NeurIPS 2025\n\nThe reviewer wrote this report independently, with an LLM used only for improving clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UFlsc2gw0G", "forum": "4kz4586euw", "replyto": "4kz4586euw", "signatures": ["ICLR.cc/2026/Conference/Submission14176/Reviewer_CZaQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14176/Reviewer_CZaQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447904725, "cdate": 1761447904725, "tmdate": 1762924632348, "mdate": 1762924632348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new approach to learn stochastic dynamical systems with a network structure. It considers a setting where empirical distributions are observed at given time points, and the goal is to construct a SDE that models the underlying data-generating process. The authors formulate this goal as a multi-marginal Schrödinger Bridge problem with Wiener process as a prior. The modelling is based on the estimation of the vector field defining the SDE by a sparse neural network. Extensive numerical experiments are conducted on synthetic and real-world genomics data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an import problem in dynamical system learning\n- It is clearly written and motivated\n- It evaluates the proposed method extensively in various settings"}, "weaknesses": {"value": "- It lacks a principled justification for learning the underlying network of the dynamical system (See questions below)\n- It would benefit from a more elaborate description of the novelties compared to [SF]2M. Is it a slight modification in the learning objective ?\n- Overstated claim on learning causal dependencies"}, "questions": {"value": "1/ Could you elaborate on the claim of deducing properties such as bifurcations or attractors from a neural dynamical system such as the one learned here ?\n\nNeural dynamical systems significantly contrast with learning closed-form dynamical systems like in [1, 2, 3] and analysing qualitative properties such as bifurcations and attractors for neural dynamical systems is very challenging and largely an open question. The paper might benefit from clarifying this contrast.  \n\n2/ Given that the vector field is modelled with a highly non-linear transformation (a neural net), why would you expect the first (sparse) weight matrix to encode the right network structure ? \n\n3/ How sensitive is the learned structure to the regularisation strength in the Group Lasso term?\n\n4/ Is the method stable when the time marginals are unevenly spaced or noisy?\n\n\n[1] Brunton, Steven L., Joshua L. Proctor, and J. Nathan Kutz. \"Discovering governing equations from data by sparse identification of nonlinear dynamical systems.\" Proceedings of the national academy of sciences 113.15 (2016): 3932-3937.\n\n[2] Dakhmouche, Ramzi, Ivan Lunati, and Hossein Gorji. \"Robust Symbolic Regression for Dynamical System Identification.\" Transactions on Machine Learning Research. 2025.\n\n[3] Sun, Fangzheng, et al. \"Symbolic Physics Learner: Discovering governing equations via Monte Carlo tree search.\" The Eleventh International Conference on Learning Representations. 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The authors refer to one of the cited papers (in line 481) as their own, which seems to reveal their identity, and hence violate ICLR policy."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "031eDTtSs7", "forum": "4kz4586euw", "replyto": "4kz4586euw", "signatures": ["ICLR.cc/2026/Conference/Submission14176/Reviewer_MmjV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14176/Reviewer_MmjV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685459125, "cdate": 1761685459125, "tmdate": 1762924631896, "mdate": 1762924631896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents StructureFlow, a novel simulation-free flow matching method for performing structure learning and modelling dynamics of dynamical systems simultaneously. The method is nicely designed and is based on interesting ideas: using neural graphical model to capture the system structure and a time-dependent score function to capture evolving stochastic dynamics. The method is clearly presented and comprehensive experimental evaluation shows that StructureFlow achieves its objetives."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper addresses a very important problem of joint dynamical inference and structure learning.\nS2: It introduces an elegant design of StructureFlow, and the novelty is very high.\nS3: The paper has a comprehensive evaluation of StructureFlow through synthetic systems, biologically simulated systems, and a real-life single-cell dataset.\nS4: The paper is clearly written and easy to follow."}, "weaknesses": {"value": "W1: The scalability of the StructureFlow is not fully addressed.\nW2: The evaluation of StructureFlow for structure learning is not fully satisfactory.\n\nMinor comments:\n1. Page 3, lines 148-150 have the same information as lines 143-146.\n2. Page 4, line 165: I am confused about q(z), which does not appear in Eq. 8 or before."}, "questions": {"value": "Q1: Can you please discuss about the scalability of StructureFlow? I understand simulation-free flow match itself is quite scalable, but I am curious about StructureFlow.\nQ2: For the evaluation of structure learning, only limited methods are considered and compared with StructureFlow. There is a benchmark for structural inference of dynamical systems with synthetic data, covering more SOTA methods such as NRI-based ones. It will be interesting to see whether StructureFlow is still competitive with those methods.\nQ3: I am curious about the details of how dynGENIES is applied and evaluated on the BoolODE data. Do you use the simulated trajectories directly?\nQ4: I am also curious about the details of how TIGON is applied and evaluated. According to previous experience, TIGON does not scale well and needs to perform dimensionality reduction first to reduce the number of variables. It is a bit surprising to me that it works for the real biological dataset to infer a 103*103 network."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wcXI5TLFe9", "forum": "4kz4586euw", "replyto": "4kz4586euw", "signatures": ["ICLR.cc/2026/Conference/Submission14176/Reviewer_HiCq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14176/Reviewer_HiCq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727083745, "cdate": 1761727083745, "tmdate": 1762924631424, "mdate": 1762924631424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}