{"id": "AB0GxLOAn9", "number": 8252, "cdate": 1758076323374, "mdate": 1759897796257, "content": {"title": "Fin-Herding: Comprehensive Evaluation for LLM Herding Behavior in Finance Domain", "abstract": "Large language models (LLMs) are increasingly deployed in financial contexts, raising critical concerns about reliability, alignment, and susceptibility to adversarial manipulation. While prior finance-related benchmarks assess LLMs' capabilities in sentiment analysis (SA), question answering (QA), and named entity recognition (NER), they are often restricted to short context and therefore fail to demonstrate LLM capacities in weighing positives vs negatives and making decisions under a long financial context, which mimics the actual investment decision situation. We introduce Fin-Herding (financial herding under long and uncertain financial context), a benchmark for evaluating LLM investment decision-making when faced with uncertainty and possible human-biased opinions. Fin-Herding includes 8868 long firm-specific analyst reports, including both negative and positive aspects of firms analyzed by sophisticated analysts with investment ratings (Bullish/Neutral/Bearish) spanning from various industries. We present large language models with firm analyst reports with/without analyst investment ratings, respectively, to get investment ratings generated by LLMs. We compare LLM investment rating with analyst rating as well as quasi-true-label based on real-time stock return. Our experimental results reveal that there is a significant increase in herding score(captures the extent to which LLMs follow analyst ratings) across models when presenting analyst ratings in context, ranging from 5\\% to 10\\%. Masking human opinions can encourage LLMs to think independently, regardless of right or wrong. We believe that Fin-Herding can advance future research in the area of automatic investment trading using an LLM agent.", "tldr": "", "keywords": ["LLM Fairness", "LLM Alignment", "finance", "uncertainty"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d54630607a8d7b2e5e6db2e84a432b13ef73cda.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Fin-Herding, a benchmark designed to evaluate herding behaviour of large language models (LLMs) in financial decision-making. The dataset consists of 8,868 long analyst reports across multiple industries, each labelled with investment ratings (bullish, neutral, bearish). The authors test 18 open-source and private models under two conditions: with and without the analyst’s rating included in the input. They define a simple Herding Score to measure how often model predictions align with analysts’ ratings and also compare performance using one-month stock returns as quasi-true labels. Results show that models exhibit stronger alignment when analyst ratings are visible, suggesting susceptibility to human bias. The paper concludes that masking human opinions promotes more independent reasoning in LLMs."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The topic itself is interesting and important.\n\n- The dataset covers multiple industries and long financial context.\n\n- The inclusion of both private and open-source models provides a broad comparative perspective."}, "weaknesses": {"value": "- Generally, the novelty and contribution are limited. The paper mainly constructs a dataset and reports descriptive comparisons of herding behaviour with and without analyst ratings, but there is no in-depth analysis or exploration of why this happens. There’s no attempt to interpret the underlying mechanism using any interpretability tools, attention-score analysis, or other ablation studies (e.g., varying the position of analyst opinions). The study ends with surface-level statistics rather than any analytical or causal understanding.\n\n- The results themselves are inconsistent. In most cases, including the analyst’s rating improves the model’s predictive ability, while in others the performance actually drops. Very few models outperform the human analyst without herding, yet some (such as internlm2-chat-7b) even surpass the analyst’s accuracy after herding. This inconsistency makes it hard to draw general conclusions, and the paper does not attempt to explain these contradictions or explore any cross-model patterns.\n\n- The definition of “bullish” and “bearish” seems self-adapted and lacks evidence or reference support. The authors use a ±1% monthly threshold without justification, whereas it is typically defined around ±20% annually in financial literature. This self-defined scheme looks arbitrary and weakens the validity of the categorisation.\n\n- The scoring scheme is also unclear. When the authors say the model’s rating is “the same as the analyst’s rating”, it’s not explained what the rating range actually is. If the ratings are on a scale like 1–100 or 1–10, it’s difficult to understand how such high herding scores (up to 90%) are possible even when the analyst ratings are masked. If the rating simply refers to \"bullish\", \"bearish\" and \"neutral\" (should refer to sideways I guess), it's not fine-grained enough. And as mentioned above the definitions are not well justified. \n\n- The writing and presentation need substantial improvement. There are many missing justifications (for example, the definition of bullish and bearish mentioned above). In several places the paper uses the word “significant” without any statistical testing or confidence evidence. These are just minor examples, and there are many similar writing issues that contribute to an unprofessional style. The result presentation also needs to be improved. Currently, it requires frequent switching between pages to cross-check tables and compare models, which disrupts readability and understanding.\n\nMinors:\n\n- Typos on the quotation marks."}, "questions": {"value": "- Can the authors justify the ±1% threshold and explain why it captures meaningful bullish/bearish distinctions?\n\n- Have the authors considered using interpretability tools (e.g., attention analysis, gradient attribution) to investigate why models herd?\n\n- Were the experiments conducted in a temporal setup?\n\n- Is there a reproducibility statement, even though it is not mandatory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IvSVZgUkrD", "forum": "AB0GxLOAn9", "replyto": "AB0GxLOAn9", "signatures": ["ICLR.cc/2026/Conference/Submission8252/Reviewer_GmPJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8252/Reviewer_GmPJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760533707587, "cdate": 1760533707587, "tmdate": 1762920194086, "mdate": 1762920194086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his paper introduces the Fin-Herding benchmark to evaluate if llms exhibit herding in financial analyst reports. The core problem that assessing llm independence in the presence of human bias in finance is highly relevant and timely. The data are substantial, and the experiments are extensive, encompassing a wide range of models. However, the paper suffers from several fundamental flaws that undermine the reliability of its conclusions and the value of its contributions. The primary issues are: 1) The metric for \"herding\" is overly simplistic and lacks connection to established financial theory; 2) There is no discussion or control for the risk of data contamination ,i.e., train-test leakage; 3) The baseline prediction accuracy of the models is so low that the discussion on mitigating herding becomes questionable from a practical utility standpoint. While the problem is important, the current methodology and narrative are not yet mature enough for acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Dataset construction: The creation of a dataset with 8,868 long-form analyst reports across multiple sectors, coupled with the clever analyst rating experimental design, is a solid contribution.\n2. Experimental scale: The evaluation of 18 open-source and closed-source models provides comprehensive empirical evidence."}, "weaknesses": {"value": "### Weakness\n1. Oversimplified Metric for \"Herding\" Lacks Financial Rigor\nThe paper's definition of \"Herding Score\" is merely the alignment between the llm's output and the analyst's rating. This is insufficient from a behavioral finance perspective. True Herding research focuses on whether a group of analysts systematically disregards their own signals to follow the consensus in response to new information. The setup here is closer to measuring opinion anchoring or conformity bias. Using a simple agreement metric cannot distinguish between an llm rationally agreeing with the analyst after reasoning and one mindlessly conforming.\n2. Inadequate Consideration of Data Contamination Risk\nThe analyst reports sourced from Yahoo Finance are public and have a very high probability of being included in the training data of many llms, especially general-purpose models. If the models have already \"seen\" these reports and their subsequent stock performance during training, the experiment evaluates \"memorization\" or \"alignment to a known label\" rather than \"reasoning capability.\" This invalidates all conclusions about Herding and predictive accuracy.\n3. Low Baseline Model Performance Undermines Practical Relevance\nAs shown in Tables 5 and 6, the prediction accuracy of most models is around 40%, only marginally better than random guessing. The F1 scores are also consistently low. This raises a critical question: these models, in their current state, simply do not possess reliable financial forecasting capability. Therefore, the practical utility of studying how to make an inaccurate model \"avoid following another inaccurate analyst\" is highly questionable. It resembles a \"blind leading the blind\" problem."}, "questions": {"value": "Suggestions:\n1. Incorporate Established Financial Metrics: The authors should integrate classic metrics from the analyst herding literature, such as Lakonishok, which measures herding based on the distribution of bullish/bearish opinions within a group of analysts.\n2. Analyze llm rating changes in relation to analyst rating revisions or earnings announcements. A more robust experiment would be to present two reports with conflicting opinions and observe if the llm herds towards the first opinion while ignoring contradictory evidence in the second. \n3. It is imperative for the authors to use established tools to check the overlap between their test set and major training corpora.\n4. The paper must explicitly report the methodology and results of the contamination check. If significant contamination is found, the authors should create a new, clean test set, e.g., using reports published after the knowledge cutoff dates of the models, and re-run the core experiments.\n5. The contribution should be more explicitly framed as revealing a socio-cognitive bias in llms, e.g., conformity to authority, rather than providing a practical solution for financial decision-making. The conclusion should temper any implication of \"improving investment performance\" and instead emphasize the implications for llm safety, alignment, and reliability evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aDSi3RWpiq", "forum": "AB0GxLOAn9", "replyto": "AB0GxLOAn9", "signatures": ["ICLR.cc/2026/Conference/Submission8252/Reviewer_X6Ue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8252/Reviewer_X6Ue"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801237754, "cdate": 1761801237754, "tmdate": 1762920193725, "mdate": 1762920193725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Fin-Herding, a benchmark designed to evaluate whether large language models exhibit herding behavior when exposed to human-biased financial opinions. The authors compare LLM-generated investment ratings with analyst ratings and actual stock returns. They find that when analyst ratings are visible within the reports, LLMs’ alignment with analysts increases significantly. However, masking analyst ratings reduces this effect, and some models even outperform analysts in predicting future stock performance under these conditions."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper documents a new phenomenon, “LLM herding,” in the financial context. The authors perform extensive empirical analyses using a host of different LLMs, confirming the existence of herding across different model classes. They also provide a benchmark dataset that could be potentially useful for future research."}, "weaknesses": {"value": "The authors’ claim that LLMs are susceptible to herding when presented with analysts’ ratings is not particularly surprising. Furthermore, there are several critical issues with the research design.\n\n1.\tLLM prompts do not change the model weights but change the model’s answering patterns. Analysts’ ratings are arguably the most important information that effectively summarizes the overall document. Even though it constitutes a very small part of the entire text, the rating itself will arguably receive significant attention when the models process the text. Given this, it is not very surprising that the models place an emphasis on the summarized rating and follow the information. To empirically assess whether the ratings of analyst reports attract disproportionate model attention, the authors could implement an open-source experiment using models. Specifically, they can compute each layer–head’s attention share directed toward the rating tokens, identified via phrases like “Buy,” “Hold,” or “Sell,” relative to all other tokens in the text. Comparing this “attention share to rating” with control spans of equivalent length located elsewhere in the report would reveal whether attention mass systematically concentrates on the rating. Furthermore, I am confused about what the readers can learn from this seemingly evident observation. The authors should clarify how this research can contribute both to academics and practitioners. The authors claim that they “propose a potential way to avoid herding.” Are they referring to the “masking” that they performed?\n\n2.\tIf the authors wanted to effectively test whether LLMs really do herd, they could have presented the model with “manipulated” ratings. That is, they could have changed the rating from bearish to bullish but retained the remaining content. If the models ignore the content and simply follow the overall ratings, the authors could argue that the models are heavily affected by the ratings themselves. In any case, I believe that this counterfactual experiment is required.\n\n3.\tAnother interesting experiment aligning more closely with the economic definition of “herding” would be to present multiple LLM agents with a company-related newspaper article and ask each to independently form stock return expectations. In a follow-up experiment, allowing the agents to “discuss” or exchange outputs before re-estimating their expectations would reveal whether their forecasts converge. A decrease in disagreement among the agents after the discussion would provide clear evidence of herding behavior among LLMs.\n\n4.\tThe authors’ research design does not necessarily reflect how investors make use of LLMs when making investment decisions. Investors seldom use analyst reports to form price expectations. As the authors note, analyst reports are the most biased financial documents. MD&As in 10-Ks are biased but are arguably less biased than analyst reports. Investors, including quant firms, make use of LLM agents to analyze lengthy financial documents and extract key information. They then make decisions based on the LLM outputs (either using professional judgment or quantitative tools on the generated outputs). I recommend that the authors explore other “less biased” financial texts than analyst reports. It is not very surprising that the model becomes more biased when the text itself is heavily biased with opinions. Several alternatives that I could suggest are: (i) MD&A texts from 10-Ks (less biased than analyst reports), and (ii) financial statements and footnotes from 10-Ks (largely boilerplate yet neutral).\n\n5.\tAnother interesting qualitative exercise would be to investigate where LLMs generate different ratings from analysts’ ratings when ratings are masked. It is important to understand why LLMs made such deviations. Do the reports themselves contain more negative information than positive information but end up providing a buy recommendation (or vice versa)?\n\n6.\tAdditionally, the stock prediction results reported in Tables 5 and 6 are concerning. The accuracies are below 50%, implying that one could potentially earn positive alpha by taking the opposite position of the models’ predictions. This contradicts prior research showing that LLMs can achieve above-random accuracy in predicting returns. The discrepancy likely arises because the analyst reports used as input are already biased, even when explicit analyst ratings are masked. Hence, the authors should tone down claims that LLMs “outperform analysts” and interpret the 40–50% predictive accuracy more cautiously, clarifying that it may reflect systematic bias in the source documents rather than true model capabilities."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "szVMq8pmwO", "forum": "AB0GxLOAn9", "replyto": "AB0GxLOAn9", "signatures": ["ICLR.cc/2026/Conference/Submission8252/Reviewer_PycM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8252/Reviewer_PycM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895699646, "cdate": 1761895699646, "tmdate": 1762920193304, "mdate": 1762920193304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Fin-Herding, a benchmark designed to evaluate whether large language models (LLMs) exhibit herding behavior, defined here as alignment with human (analyst) opinions, when making financial investment judgments. Using 8,868 analyst reports sourced from Yahoo Finance, the authors construct two versions of each report (with and without analyst rating) and prompt 18 open- and closed-source LLMs to output their own bullish/neutral/bearish investment ratings. The proposed herding score measures the degree of agreement between model and analyst ratings. Empirically, when analyst opinions are included, all models’ herding scores rise by roughly 5–10%, suggesting that the presence of human ratings drives greater conformity. The authors interpret this as evidence that LLMs “follow human bias” and lose independent reasoning capacity."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Cross-model and cross-sector evaluation: The experiments span 18 LLMs (including GPT-5, Claude-4, Llama-3, Gemma, Qwen2, etc.) and multiple industries, offering broad empirical coverage.\n2. Interesting motivation:  Exploring how LLMs integrate, or over-rely on, human judgments in finance touches an important area related to model alignment, trust, and bias propagation."}, "weaknesses": {"value": "1. Conceptual Ambiguity: “Herding” is defined purely as label agreement between LLM and analyst, without distinguishing between rational information uptake and irrational conformity. In behavioral finance, herding implies unjustified convergence of opinions despite private signals. Here, however, concordance could simply reflect legitimate reasoning based on shared evidence. Without counterfactual or adversarial setups (e.g., feeding incorrect analyst ratings), the observed effect cannot be interpreted as herding in the financial sense.\n\n2. Misinterpretation of Context Sensitivity: The finding that LLMs’ outputs shift when given additional contextual information is unsurprising. It merely shows standard context conditioning or sycophancy, a well-known phenomenon already analyzed in general-domain studies such as Sharma et al. (ICLR 2024) and Xie et al. (ICLR 2024). The claim that this behavior reveals a unique financial herding bias therefore overstates novelty.\n\n3. Experimental Design and Statistical Validity: \n- Severe label imbalance. Only 0.29% of analyst samples are “bearish” while over 72% are bullish, yet all evaluations rely on plain accuracy. No class weighting, resampling, or calibration metrics are applied, making comparisons statistically unreliable.\n- Arbitrary “true label.” The “ground truth” based on one-month return ±1% is unjustified. Analyst reports typically express 6–12-month horizons, and monthly stock returns are dominated by market noise. This weakens any inference about predictive accuracy or “analyst vs. model” performance.\n- Incomplete control for implicit sentiment cues. Only the first sentence (containing the rating) is masked; stylistic or lexical cues throughout the report can still leak sentiment, making the masked/unmasked distinction unreliable.\n- No statistical testing. Differences of 5–10% in accuracy or herding score are reported without significance testing or confidence intervals."}, "questions": {"value": "1. What is the conceptual distinction between LLM alignment and herding, and how should they be properly differentiated?\n\n2. From an experimental perspective, several questions remain — in particular, what is the rationale for defining the ground truth based on a one-month return with a ±1% threshold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5OmiQLHXAH", "forum": "AB0GxLOAn9", "replyto": "AB0GxLOAn9", "signatures": ["ICLR.cc/2026/Conference/Submission8252/Reviewer_qnuF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8252/Reviewer_qnuF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098125495, "cdate": 1762098125495, "tmdate": 1762920192859, "mdate": 1762920192859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}