{"id": "CxLaZWbUjc", "number": 3074, "cdate": 1757327647850, "mdate": 1759898110545, "content": {"title": "Adaptive Data-Knowledge Alignment in Genetic Perturbation Prediction", "abstract": "The transcriptional response to genetic perturbation reveals fundamental insights into complex cellular systems. While current approaches have made progress in predicting genetic perturbation responses, they provide limited biological understanding and cannot systematically refine existing knowledge. Overcoming these limitations requires an end-to-end integration of data-driven learning and existing knowledge. However, this integration is challenging due to inconsistencies between data and knowledge bases, such as noise, misannotation, and incompleteness. To address this challenge, we propose ALIGNED (Adaptive aLignment for Inconsistent Genetic kNowledgE and Data), a neuro-symbolic framework based on the Abductive Learning (ABL) paradigm. This end-to-end framework aligns neural and symbolic components and performs systematic knowledge refinement. We introduce a balanced consistency metric to evaluate the predictions' consistency against both data and knowledge. Our results show that ALIGNED outperforms state-of-the-art methods by achieving the highest balanced consistency, while also re-discovering biologically meaningful knowledge. Our work advances beyond existing methods to enable both the transparency and the evolution of mechanistic biological understanding.", "tldr": "We introduce an end-to-end framework aligns neural and symbolic components and performs systematic knowledge refinement for genetic perturbation prediction.", "keywords": ["Genetic Perturbation", "Gene Expression Prediction", "AI for Biology", "Neuro-Symbolic AI"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2a1b98f41c2808e363fde0ed6520b6a55ccff2a.pdf", "supplementary_material": "/attachment/af3cbd8c8ac040080d75cc05049a39e79d32e2eb.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents ALIGNED, a method for predicting the effects of gene perturbations in single-cell transcriptomic data using prior gene regulatory networks (GRNs). Unlike earlier models that also leverage GRNs—such as GEARS—the proposed approach does not treat the regulatory information as static but instead refines it dynamically throughout training. The training process itself is complex, involving a multistage optimization scheme that combines gradient descent with a Monte Carlo, ultimately aiming to align predictions based on data with the ones based on GRN."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The authors employ a genuinely new strategy that leverages GRN for perturbation prediction that does not treat the GRN as static during training. They also devise a complex strategy of optimizing the neuro-symbolic alignment."}, "weaknesses": {"value": "In terms of the accuracy of perturbation respnse predictions, the algorithm provides only marginal gains, while being notably resource-intensive—requiring up to 12 hours of training on an 80 GB GPU. The authors, nevertheless, argue that the true advantage of their method lies not in predictive accuracy but in its improved alignment with the prior GRN and in the refinement of the GRN during optimization. However, the first claim is problematic: the reported GRN-alignment metric seems circular, as the GRN serves both as an input to the model and as a reference for evaluation. The second claim—that the optimization refines the GRN—is also inconclusive, since the authors do not quantify the number or directionality of corrected regulatory links but instead report pathway enrichment scores, which appear only tangentially related."}, "questions": {"value": "**I am willing to improve the score of the paper if:**\n\n1. The authors explain why the knowledge consistency metric is important and prove that their way of computing it is not circular.  \n2. The authors conclusively show that their approach provides substantial benefits in terms of perturbation prediction as compared to other methods in the field, including the current state-of-the-art STATE.  \n3. The authors describe the optimization procedure from a practical point of view, explaining how different parameters at each stage influence the accuracy of predictions.  \n4. *(Minor)* The authors mention that two of the datasets were split randomly — please clarify what is meant by that."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h4hjPMtJO6", "forum": "CxLaZWbUjc", "replyto": "CxLaZWbUjc", "signatures": ["ICLR.cc/2026/Conference/Submission3074/Reviewer_YBGA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3074/Reviewer_YBGA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761217883797, "cdate": 1761217883797, "tmdate": 1762916539861, "mdate": 1762916539861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to tackle a ternarized version of gene perturbation prediction through incorporating prior domain knowledge given by a knowledgebase in the learning process. They suggest to alternatingly refine the predicted outcomes and the knowledge base during the learning process and propose a gradient-free algorithm for this."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The presented paper is original in that it presents a novel way of introducing prior knowledge into gene perturbation prediction models."}, "weaknesses": {"value": "-\tThis paper considers a simplified ternarized version of the underlying problem of gene perturbation prediction, **which is not useful in practice**. The extent of change introduced by a prediction is essential to know. The de facto gold-standard challenge in the field right now is the virtual cell challenge held by the Arc institute (https://virtualcellchallenge.org/, [1]), for which the evaluation metrics already disclose that continuous predictions are necessary. I have seen academic works using this binary or ternary prediction framework to be able to use language models before, but that does not make the task in anyway useful.\n-\tThere are **strong assumptions on how inhibitory and activating effects are interacting** that are not discussed in the main paper. I.e., to perform a query on the KB (the $\\delta_{KB}$), it is assumed that activating and inhibitory effects are additive, which needs clear justification.\n-\tIt is **unclear how scalable this approach is** in practice, the adjacency matrices are #genes x # genes, which can go up to n>60k (all annotated genes) or n>24k (protein-coding genes), in both cases resulting in restrictively large matrices. It is unclear from the writing what subset of genes is used in the experiments, but it appears to be a small selection which needs justification.\n-\tThe writing is incomplete and in its current form the paper **can not be reproduced**.\n1. How is the threshold $\\theta$ defined and picked in practice? Why?\n2. How is weight vector $w$ constructed exactly? A vector that “contains the number of training data samples[…] and the number of annotations from gene Ontology[…]” leaves a lot of questions – these are two numbers (counts) which clearly can not make up $w$. Do you sum per gene? Then add up the two numbers (KB and GO)? What is the rationale?\n3. What are the number of genes (and which ones) considered for the experiments?\n4. **What are the model architectures you use exactly**?\n5. **Do you evaluate on hold-out data and KBs unseen during training**? What would the performance then be, e.g. evaluating on a PPI-derived KB or other datasets on the same (or even different) cell line, which is fairly common in the literature?\n-\tWhat is the comparison to SOTA methods, including the currently not discussed Morph [2], and STATE [3] models? Fig. 4,5 and Table 1 simply **miss a comparison of the actual prediction to any SOTA**.\n\n[1] Rohaani, Y H et al. * Virtual Cell Challenge: Toward a Turing test for the virtual cell.* Cell 188(13), pages 3370-3374, 2025.\n\n[2] He, C et al. * MORPH Predicts the Single-Cell Outcome of Genetic Perturbations Across Conditions and Data Modalities.* bioRxiv, DOI: https://doi.org/10.1101/2025.06.27.661992, 2025.\n\n[3] Adduri, A K et al. * Predicting cellular responses to perturbation across diverse contexts with State.* bioRxiv, DOI:https://doi.org/10.1101/2025.06.26.661135, 2025."}, "questions": {"value": "-\tWhat is the justification on the assumption made for how activating and inhibitory effects propagate and interact?\n-\tGiven that the KB is given as binary matrices, why can’t you do gradient-based optimization on this? You can follow a similar scheme as in 3.4 as far as the explanations go – what is prohibiting this?\n-\tWhat is the (exact!) architecture you are using in the experiments?\n-\tHow many and which genes do you consider in the experiments? What happens if you consider more/less?\n-\tWhat are the precision and recall, given that the KB is so sparse, which is hidden in the F1?\n-\tWhat is the performance on training data?\n-\tWhat is the performance on unseen (hold-out) data?\n-\tWhat is the performance of KB reconstruction on an unseen KB (e.g. PPI-derived)?\n-\tWhat is the comparison to SOTA methods using common benchmarking protocols, both regarding metrics as well as separate data (see Weaknesses)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PhWwZQ2AfJ", "forum": "CxLaZWbUjc", "replyto": "CxLaZWbUjc", "signatures": ["ICLR.cc/2026/Conference/Submission3074/Reviewer_nsYx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3074/Reviewer_nsYx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656473518, "cdate": 1761656473518, "tmdate": 1762916539593, "mdate": 1762916539593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ALIGNED, a neuro-symbolic learning framework that integrates gene regulatory knowledge bases (KBs) with data-driven learning to improve consistency between biological prior knowledge and empirical data. The core contribution is a bi-directional alignment mechanism:\nAlignment stage — adapts the model to existing biological knowledge.\nRefinement stage — updates and corrects the knowledge base itself using learned patterns.\nIt defines a novel Balanced Consistency metric to jointly evaluate data-knowledge coherence, addressing a gap in current hybrid bioinformatics methods that often treat biological priors as static and non-adaptive. The work is motivated by the high inconsistency (14–71%) between curated knowledge bases (OmniPath, GO, EcoCyc) and observed perturbation data, and proposes a continuous refinement cycle to resolve such mismatches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Timely problem framing: Tackles a critical limitation in hybrid biological modeling—static priors that cannot evolve alongside new data.\nMethodological novelty: The dual-stage alignment/refinement loop and Balanced Consistency metric provide a principled way to unify knowledge and data learning.\nStrong empirical evidence: Across datasets (Norman et al. 2019, Adamson et al. 2016, Precise1K, E. coli), ALIGNED consistently outperforms baselines like GEARS, scGPT, and scFoundation in both prediction accuracy and knowledge consistency metrics.\nBiological interpretability: Unlike pure neural models, the system supports symbolic reasoning and interpretable GRN (Gene Regulatory Network) updates."}, "weaknesses": {"value": "Limited ablation of symbolic components: While gradient-free and gradient-based updates are both introduced, their relative contributions are only superficially analyzed.\nDataset diversity: Most evaluations center on transcriptional perturbation data; extension to morphological or cross-modal biological data is not shown.\nTheoretical grounding: The abductive reasoning formulation (based on ABL) could benefit from clearer formal definitions and proofs of convergence or stability.\nPotential overfitting to KB biases: Since refinement still depends on noisy or incomplete KBs, it’s unclear how ALIGNED avoids reinforcing existing errors."}, "questions": {"value": "1. The paper introduces “bi-directional alignment” between the neural model and symbolic knowledge base. However, the exact mathematical formalization of the alignment operator and refinement update is somewhat ambiguous.\nHow is the “knowledge correction” signal computed and propagated?\nDoes the refinement step have convergence guarantees, or can it drift over repeated iterations?\n\n2. The model includes multiple interacting components: gradient-based learning, abductive reasoning (gradient-free correction), and symbolic consistency scoring.\nWhat is the contribution of each?\nWould ALIGNED still outperform baselines without symbolic refinement?\n\n3. The paper’s datasets are mostly transcriptomic (gene-expression–based). Could the same approach generalize to cross-modal or spatial data (e.g., imaging, proteomics)?\nDoes ALIGNED require structural priors specific to GRNs, or can it adapt to other biological or symbolic domains?\n\n4. Baseline comparisons (GEARS, scGPT, etc.) are strong, but all are data-driven. What about existing neuro-symbolic or knowledge graph–augmented models (e.g., DeepProbLog, NeuroLogic, or KG-BERT)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wsxkirXBhQ", "forum": "CxLaZWbUjc", "replyto": "CxLaZWbUjc", "signatures": ["ICLR.cc/2026/Conference/Submission3074/Reviewer_6cxn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3074/Reviewer_6cxn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796203014, "cdate": 1761796203014, "tmdate": 1762916539236, "mdate": 1762916539236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ALIGNED, a neuro-symbolic framework that jointly optimizes a neural predictor and a symbolic reasoning module over GRNs via abductive learning. The method introduces (i) a balanced consistency metric that merges agreement with data and with knowledge bases, (ii) an adaptive alignment mechanism (learned with REINFORCE) to select per-gene whether to trust neural or symbolic predictions, and (iii) gradient-based knowledge refinement with sparse regularization to update GRNs. Experiments on Norman, Dixit, Adamson (human/mouse) and an E. coli setting show higher “balanced consistency” than recent baselines and indicate that refinement can re-discover biologically meaningful relations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Overall, I think this paper has a potentially significant contribution to perturbation-response prediction and neuro-symbolic learning, with a thoughtful framing of data-vs-knowledge trade-offs. With clarifications on theoretical assumptions, ablation coverage, and computational costs, I’d be inclined to raise my score."}, "weaknesses": {"value": "1. The paper’s core ideas: balanced consistency, per-output adaptive selection, and sparse refinement—are compelling, but please delineate what is novel vs. adapted from prior ABL/REINFORCE literature (e.g., ABL variants, learnable trade-offs). A table that maps each ALIGNED component to closest prior art and states the delta would help.\n\n2. Results implicitly require that either the neural model can approximate the true response mapping or the KB’s transitive closure (up to path length k) can approximate symbolic ground truth. Please state these assumptions explicitly and discuss failure modes when (a) GRN coverage is sparse/bias-prone or (b) δ_{KB} is systematically wrong for some modules. What guarantees (if any) are possible for convergence of the alternating align/refine loop under misspecification?"}, "questions": {"value": "1. How are threshold θ and weights w tuned—per dataset or globally? Any risk of overfitting via these hyperparameters?\n2. For double-vs-single perturbations, does k need to scale, and does refinement overfit to short-cycle artifacts?  \n\nMinor comments / nits\n1. Typos/formatting: “Algin 2” → Align 2; “ALIGEND” → ALIGNED; ensure consistent “knowledge base (KB)” capitalization.\n2. In Fig. 3–5 captions, explicitly state metric definitions (macro vs micro F1)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qA7nZ4NK5i", "forum": "CxLaZWbUjc", "replyto": "CxLaZWbUjc", "signatures": ["ICLR.cc/2026/Conference/Submission3074/Reviewer_Zesh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3074/Reviewer_Zesh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762711302420, "cdate": 1762711302420, "tmdate": 1762916538768, "mdate": 1762916538768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}