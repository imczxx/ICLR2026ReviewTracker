{"id": "Ta98BucJY6", "number": 6735, "cdate": 1757993919066, "mdate": 1759897898022, "content": {"title": "DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement Learning Agents", "abstract": "Hierarchical Reinforcement Learning (HRL) agents often struggle with long-horizon visual planning due to their reliance on error-prone distance metrics. We propose Discrete Hierarchical Planning (DHP), a method that replaces continuous distance estimates with discrete reachability checks to evaluate subgoal feasibility. DHP recursively constructs tree-structured plans by decomposing long-term goals into sequences of simpler subtasks, using a novel advantage estimation strategy that inherently rewards shorter plans and generalizes beyond training depths. In addition, to address the data efficiency challenge, we introduce an exploration strategy that generates targeted training examples for the planning modules without needing expert data. Experiments in 25-room navigation environments demonstrate 100\\% success rate (vs 90\\% baseline) and shorter episode lengths. The method also generalizes to momentum-based control tasks and requires only $\\log N$ steps for replanning. Theoretical analysis and ablations validate our design choices.", "tldr": "Discrete Hierarchical Planning for Long-Horizon visual planning tasks using Hierarchical Reinforcement Learning Agents", "keywords": ["Discrete planning", "Hierarhical planning", "Subgoal discovery", "Hierarchical Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4339e2628dd19c8d1197a2562f007930a0a9d2aa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses long-horizon visual planning by proposing a discrete hierarchical planning framework. The authors train an encoder–decoder module to learn a discrete latent space, upon which a high-level policy plans subgoals recursively. The approach builds on prior work of Director for goal-conditioned visual planning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The problem of hierarchical visual planning in long-horizon settings is both relevant for the community.\n\n2. The idea of using a discrete latent space to constrain high-level planning is promising and can potentially improve sample efficiency."}, "weaknesses": {"value": "1. Limited and inconsistent evaluation.\n    - The experiments are restricted to a single environment (25-room navigation). Prior works such as GCP evaluated both 9-room and 25-room variants, as well as FrankaKitchen in LEXA's paper, which are missing here.\n\n    - In RoboYoga, the baselines are not fully reported; only the proposed method’s results are shown, and its performance on RoboYoga–Quadruped does not appear to improve.\n\n    - The paper reports episode rewards, but it is unclear whether these are evaluated using exploration or planning policies. If an explorer is active, the interpretation of \"the sharp rise indicates a switch from exploration to planning\" becomes confusing. The mechanism of this switch need clarification.\n\n2. Conceptual and presentation clarity.\n\n    - The paper assumes prior familiarity with Director and RSSM, making it difficult for readers not familiar with these models to follow. Section 2.1 should include a concise overview of these components.\n\n    - The notation in the planning section (e.g., $s_0$) is confusing, as it initially suggests the initial state rather than the subsequent state in the planning trajectory.\n\n    - The description of GC-Director is unclear.\n\n3. Insufficient ablations and analysis.\n\n    - The choice of tree depth (D=8) is not justified. Why was 8 chosen when D=3 yields comparable reward and fewer steps?\n\n    - There are no ablations analyzing the impact of removing or modifying the discrete CVAE (e.g., using a continuous version).\n\n    - The paper mentions providing memory of past states as additional input, even though the RSSM already encodes recurrent state, why is this necessary, and what empirical effect does it have?\n\n    - The paper claims GC-Director fails due to task complexity, but LEXA, which lacks explicit planning, performs adequately. This requires more rigorous reasoning or empirical support.\n\n4. Lack of experimental rigor.\n\n    - The final performance in Figure 7(a) is not substantially higher than LEXA, which does not use hierarchical planning.\n\n    - The similarity metric differs from LEXA: the paper uses cosine_max instead of temporal similarity, even though LEXA(Cos) performed worse. why not using the temporal similarity?\n\n    - There is no clear explanation of how SAC is used to optimize the managers and whether this differs from Director’s joint training scheme.\n\n**Minor Suggestions (Not affecting the score)**\n1. Improve Figure 1 so that the illustration aligns with its textual description. A well-designed figure should be interpretable without heavily relying on the section text. Consider showing both the training and exploration phases.\n\n2. Consider adding “Visual” to the title to better situate the work in the visual planning literature.\n\n3. Clarify the notation for trajectory states and unify the explanation of algorithms for smoother reading."}, "questions": {"value": "1. What justifies the “sharp rise” of DHP performance in Figure 7? How does this correspond to switching from exploration to planning?\n\n2. Regarding the CVAE:\n\n    - Is it identical to the one used in Director? If so, what constitutes the novelty of GCSR?\n\n    - Could you show ablations comparing the discrete and continuous variants of the CVAE?\n3. How do you choose $\\Delta_R$?\n\n4. How can DHP achieve higher episodic rewards than LEXA despite comparable or shorter episode lengths?\n\n5. Why was cosine_max used as the similarity measure rather than the temporal similarity metric used in LEXA?\n\n6. Could you elaborate on SAC’s role in optimizing the high-level policy, given Director trains managers jointly with workers?\n\n7. Please clarify Algorithm 6’s dataset collection process, does it occur sequentially with Algorithm 1, and can these be combined for clarity?\n\n6. Appendix D’s sample trajectories should explicitly indicate generated subgoals for interpretability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UaVRsFnhgB", "forum": "Ta98BucJY6", "replyto": "Ta98BucJY6", "signatures": ["ICLR.cc/2026/Conference/Submission6735/Reviewer_Eumj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6735/Reviewer_Eumj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949158667, "cdate": 1761949158667, "tmdate": 1762919021834, "mdate": 1762919021834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method that replaces continuous distance estimates with discrete reachability checks to evaluate subgoal feasibility. It recursively constructs tree-structured plans by decomposing long-term goals into sequences of simpler subtasks, using an advantage estimation strategy that inherently rewards shorter plans.\n\nIt seems clearly presented, and shows a clear improvement on a baseline task although performance improvements seem example dependent."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Reachability (binary) may avoid coupling to brittle distance metrics and naturally handles disconnected regions, as the authors claim.\n\n\nContraction property of the return operators.\n\n\nSuccessful results on 25-room benchmark and competitive path lengths, where ablations show training with shallow depths still provides advantage of the proposed methods."}, "weaknesses": {"value": "Easy to understand the flow and contribution of the paper. \n\nThe resulting model performs expertly on the standard 25-room task than the current SOTA approaches, but not on others. \n\nIt would be quite sensitive to model error.  The cosine_max similarity check may judge that two similar-looking states are close even though the underlying configurations differ. \n\nThe paper trains a static-state MLP as an approximation, so the planning can be sensitive to such approximation."}, "questions": {"value": "How often does imagination mark unreachable subgoals as reachable?  What if the world model quality is low? \n\nExamples such as maze-like environments with partial observation only? \n\nIt seems that the memory can be a limiting factor for complex problems. What if the memory needs to be truncated?\n\nWhat are the specific cases where min-child is especially helpful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WFujk5ufs3", "forum": "Ta98BucJY6", "replyto": "Ta98BucJY6", "signatures": ["ICLR.cc/2026/Conference/Submission6735/Reviewer_73z4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6735/Reviewer_73z4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979446889, "cdate": 1761979446889, "tmdate": 1762919021339, "mdate": 1762919021339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed method can be summarized as follows.\nGiven a task, it bisects the desired trajectory by generating mid-point states. \nThose mid-point states are stored in a binary tree. \nIn the experiment, a visual navigation domain with 25 rooms is used, and the tree was tested up to a depth of 3.\nThe state encoder uses RSSM, and SAC trains the RL policy with policy gradients.\nThe main paper doesn't show the overall algorithm for training high-level policy/exploration and the lower-level policy/exploration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Improved performance on the 25-room navigation domain from 90% rate to 100% as shown in Table 1.\nThe proposed approach improves the variance of the average episode length.\nThe proposed idea is simple compared with existing HRL methods.\nIt learns to bisect the initial state and the goal state, using the midpoint state for learning lower-level policy functions."}, "weaknesses": {"value": "The application of the proposed approach is limited, and it is not clear how the subtask generation returns meaningful subtasks/subgoals.\n\nMost of the related works were developed until 2020, except for a few.\nThere are many missing HRL approaches from 2020\nsuch as option-based HRLs, neuro-symbolic planning, and RL, or identifying subtasks/subgoals.\n\nHere is a partial list of such approaches (and there are more)\n* Reward machines: Exploiting reward function structure in reinforcement learning\n* Hierarchical Reinforcement Learning with AI Planning Models\n* Reinforcement Learning with Option Machines\n* Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making\n* Learning to represent action values as a hypergraph on the action vertices\n* Learning Parameterized Task Structure for Generalization to Unseen Entities\n* Fast inference and transfer of compositional task structures for few-shot task generalization\n* Unsupervised Task Graph Generation from Instructional Video Transcripts."}, "questions": {"value": "Q1 How does the trained policy generalize?\nWhat's the impact of permuting/modifying connections all patches of images in the 25 rooms?\nHow many steps are needed to move from the center of each room to the end via a straight line?\n\nQ2 In Figure 7, all figures show a sharp transition around 3M steps for the HDP configuration.\nCould you explain why?\n\nQ3 What is the DHP with the Depth 1 configuration?\nIs it dividing the initial-goal state with a mid-point state?\n\nQ4 How frequently does this trajectory bisection happen?\n\nQ5 As the proposed method can traverse the bi-sected tree in a depth-first manner, how does the return estimate from the whole tree give an advantage?\n\nQ6 Does the proposed approach identify re-usable/interpretable sub-goals?\n\nQ7 For state representation learning, what are the requirements for the computational resource/data?\nDoes it learn online while learning the RL policy?\n\nQ8 What limits the proposed approach from being applied to the problem domains used in the related papers listed above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "npzajnptPH", "forum": "Ta98BucJY6", "replyto": "Ta98BucJY6", "signatures": ["ICLR.cc/2026/Conference/Submission6735/Reviewer_tJYw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6735/Reviewer_tJYw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981526868, "cdate": 1761981526868, "tmdate": 1762919020394, "mdate": 1762919020394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a HRL algorithm that learns to propose sub-goals that are based on discrete notion of reachability rather than relying on continuous distance metrics in an embedding space. They propose a tree-structured decomposition for generating intermediate subgoals (with subgoals occurring around half of the time interval between start and desired goal). The method demonstrates improved success rates on long horizon tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear and the method outlined is mostly clear (see a few clarification questions below)"}, "weaknesses": {"value": "- The training procedure involves quite a few moving components. Especially the need for extensive exploration to ensure the CVAE offers enough coverage to select suitable sub-goals. This indicates a dependence on the base director architecture to be good enough to reach rewarding trajectories from which the explorer can further improve coverage, so might be critically dependent on the task’s reward structure.  \n  - The paper could benefit from a clear pseudocode / pictorial view of various stages of training.  \n- The predominant evaluation is limited to just a single domain of 25 room navigation. While still informative the paper could benefit from the inclusion of more long horizon environments typically benchmarked in HRL (AntMaze, OGBench tasks)."}, "questions": {"value": "- In section 2.2, are you using multiple CVAEs for different time scales (or tree depth) $Q$? Or are the encoder/decoder networks conditioned on the timescale?\n- In section 2.3.2, the reachability reward is based on model-predicted reachability i.e. by simulating the worker policy inside the RSSM. Could you clarify if the planning policy is trained after the worker and RSSM are trained using the exploration policy, or does the training require some special scheduling? How is the threshold parameter $\\\\Delta_R$ chosen – is it dependent on the maximum depth of the tree?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "54YqKR4Z0y", "forum": "Ta98BucJY6", "replyto": "Ta98BucJY6", "signatures": ["ICLR.cc/2026/Conference/Submission6735/Reviewer_ZZ6r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6735/Reviewer_ZZ6r"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991835905, "cdate": 1761991835905, "tmdate": 1762919019918, "mdate": 1762919019918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}