{"id": "8As5b8t83k", "number": 19438, "cdate": 1758296282744, "mdate": 1759897038981, "content": {"title": "Benchmarking Visual Fast Mapping: Probing VLMs' Test-time Image-text Alignment", "abstract": "Visual Fast Mapping (VFM) refers to the human ability to rapidly form new visual concepts from minimal examples based on experience and knowledge, a keystone of inductive capacity extensively studied in cognitive science. In the realm of computer vision, early endeavors attempt to achieve this capability through one-shot learning methods yet achieving limited generalization. Despite the recent advancements in Visual Language Models (VLMs), requiring large-scale image-text corpora, this human-like capability still has not been acquired. In this paper, we introduce VFM Bench, designed to evaluate the VFM ability in realistic industrial scenarios and reveal a performance gap over $19.0\\%$ between human and VLMs. Most VLMs tend to response on pure-vision discriminative features, rather than make use of prior language knowledge for test-time alignment. Notably, emerging visual reasoning models demonstrate early-stage performance improvement yet still with gap to average human, suggesting a promising direction for leveraging cross-modal information in context. The code and dataset for VFM Bench are anonymously available at: https://anonymous.4open.science/r/VisualFastMappingBenchmark.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Visual In-context Learning", "Visual Reasoning", "Cross-modal Alignment"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dab90242cce2cb80f9ee8f39441cf00ae4854690.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a benchmark for visual fast mapping, the test-time ability to align new visual categories with language from only a few support examples. The authors curate industry-grounded n-way k-shot tasks across four domains, assemble a high-quality query set via model-based difficulty filtering, CLIP-feature diversity sampling, and human QC, and evaluate leading VLMs with accuracy as well as efficiency and efficacy measures that track gains as k increases. Results show sizable gaps to human performance and indicate that current models rely more on visual contrast than on leveraging language priors for test-time alignment. The work contributes a concrete task formulation, a scalable dataset and protocol, and empirical evidence that fine-grained, few-shot concept learning remains a key weakness for modern VLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper defines visual fast mapping as a concrete, test-time few-shot alignment task and operationalizes it with a clean n-way k-shot formulation, which feels genuinely fresh.\n\n2. Curates an industry-grounded benchmark with clear protocols and informative metrics (accuracy, efficiency, efficacy), enabling straightforward comparison across models.\n\n3. Provides solid empirical evidence, including human baselines and targeted analyses, that isolates a real weakness in current VLMs and motivates follow-up research."}, "weaknesses": {"value": "1. Benchmark curation may be biased by using contemporary VLMs as difficulty filters and CLIP features for diversity sampling, potentially aligning the test set with those models’ failure modes and representation space.\n\n2. Support-set examples are randomly sampled per task, but variance across resamplings is not reported, so rankings and the metrics may be unstable.\n\n3. The data processing sections describe collection and formatting across 31 open-source datasets but do not report near-duplicate removal or overlap checks against model training corpora, which weakens the “novel concept” claim."}, "questions": {"value": "Refer to Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WqU5bQTxJO", "forum": "8As5b8t83k", "replyto": "8As5b8t83k", "signatures": ["ICLR.cc/2026/Conference/Submission19438/Reviewer_zSfZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19438/Reviewer_zSfZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710102155, "cdate": 1761710102155, "tmdate": 1762931360073, "mdate": 1762931360073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores Visual Fast Mapping (VFM), a cognitive ability in humans to rapidly form new visual concepts from minimal examples by leveraging prior knowledge and cross-modal (image-text) alignment. The authors argue that while early computer vision approaches like one-shot learning failed to generalize, modern Vision-Language Models (VLMs) trained on large image-text corpora should theoretically approximate this capability but they failed. Hence, the authors contribute a new VFM Bench, designed for realistic industrial scenarios. It includes 4,200 query images covering 512 concepts across 171 tasks in four domains. The benchmark is constructed through a pipeline involving difficulty filtering, diversity sampling, and manual quality checks. Problems are framed as n-way k-shot classification tasks (0-5 shots), where models must predict the category of a query image based on instructions and support examples. Experiments evaluate state-of-the-art VLMs and visual reasoning models against human performance and baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Focusing on industrial applications with diverse, high-quality data from real datasets is interesting and meaningful. This addresses gaps in existing benchmarks that emphasize abstract puzzles or language generation.\n2. The analysis of results in the benchmark is insightful and thorough. \n3. Experimental results include multiple SOTA models, human baselines, and novel metrics, and ablations reveal specific weaknesses in VLMs."}, "weaknesses": {"value": "1. My biggest worry is the scale of the proposed benchmark. With only 4,200 images and 171 tasks, the benchmark might not capture extreme edge cases or very large-scale scenarios.\n2. I would suggest the authors to discuss more about the real-world applications and meaning for the proposed benchmark."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nMs29RrDet", "forum": "8As5b8t83k", "replyto": "8As5b8t83k", "signatures": ["ICLR.cc/2026/Conference/Submission19438/Reviewer_64fe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19438/Reviewer_64fe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898542963, "cdate": 1761898542963, "tmdate": 1762931359709, "mdate": 1762931359709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "VFMBench introduces a new benchmark to evaluate the Visual Fast Mapping (VFM), an ability to learn new concepts with few examples leveraging pretrained knowledge akin to inductive reasoning. The benchmark created by drwaing samples from real-world datasets across four different domains. Authors evaluated both proprietary and open-source models via the API against human performance. The result shows there's a considerable gap in SOTA models in VFM task, and authors claim that this is mainly becuase the VLMs primarily focuses the visual contrast rather than language-informed alignment based on the analyses on inter/intra class reasoning and prior knowledge use. Beside this, the paper lacks in any concrete modeling or training improvements that can be drawn from the analyses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Motivation**: The motivation of the paper is grounded in cognitive science and human inductive reasoning and a critically important aspect of VLM studied in previous literatures [1-2].\n- **Benchmark**: The design of the VFMBench is clearly documented and cover real-world scenarios. The authors have critically evaluated commercial and open-source models via API on the benchmark and provided comprehensive analyses on why there's a gap in VFM task.\n\n- **Good resource**: The proposed task and the benchmark could be a valuable testing resource for future multimodal models.\n\n\n\n[1] Zhang, Y., Unell, A., Wang, X., Ghosh, D., Su, Y., Schmidt, L., & Yeung-Levy, S. (2024). Why are visually-grounded language models bad at image classification?. Advances in Neural Information Processing Systems, 37, 51727-51753.\n\n[2] Huang, K. H., Qin, C., Qiu, H., Laban, P., Joty, S., Xiong, C., & Wu, C. S. (2025). Why vision language models struggle with visual arithmetic? towards enhanced chart and geometry understanding. arXiv preprint arXiv:2502.11492."}, "weaknesses": {"value": "- **Weak empirical depth**: The mechnism and cause localization sections are informative, however largely qualitative and do not provide any strong causal or statistical evidene linking the observed behavour to model architecture or training paradigms.\n\n- **Definition**:  Based on API based evaluation, the paper focuses on trainining-free method for VFM. However, VFM does not look fundamentally a new task. It is more of a recontextualized version of in-context/test-time adaptation methods.  What is the core benefit of evaluating a model for VFM? What does it achieve over the other methods? If this used for gardient-optimization methods, how does it differ from few-shot methods? The paper lack in details of theoretical/mathematical definition of VFM and how it differs from other tasks.\n\n- **Findings**: The experimental results are comprehensive and reinforce well-known findings on VLMs' complications in cross domain adaptation. Furthermore, while ablation and cognitive analyses are useful diagnostics, the authors lack actionable guidance or concrete modelling improvements. As a result, the paper situates itself as an evaluation of a new benchmark.\n\n- **Human Evaluation**: The paper employed a limited human evaluation with only five participants to benchmark model performance. This small sample size limits the statistical reliability of the human baseline. With the small sample size, it is difficult to draw conclusive decision about model performance also human baseline."}, "questions": {"value": "1. How were human participants selected and trained for the VFM task? Were the participants screened for domain familiarity?\n2. How the authors handled overlapping of source datasets used to create VFMBench to the pretraining datasets of these models?\n3. How would the metrics efficiency ($\\eta$) and effective benefit ($\\delta$) to the dataset composition or task domain? \n4. What is the core benefit of performing well in VFM? How does this differ from other adaptation methods?\n5. Does the result suggest any architectural or training improvements for VLMs?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper has used human participants to create a performance baseline."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "abX1LUwvR8", "forum": "8As5b8t83k", "replyto": "8As5b8t83k", "signatures": ["ICLR.cc/2026/Conference/Submission19438/Reviewer_8EQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19438/Reviewer_8EQd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762168863181, "cdate": 1762168863181, "tmdate": 1762931359256, "mdate": 1762931359256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}