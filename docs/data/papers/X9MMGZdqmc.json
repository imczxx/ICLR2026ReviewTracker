{"id": "X9MMGZdqmc", "number": 22267, "cdate": 1758328673237, "mdate": 1759896876072, "content": {"title": "Open Character Training: Shaping the Persona of AI Assistants Through Constitutional AI", "abstract": "The character of the \"AI assistant\" persona generated by modern chatbot large language models influences both surface-level behavior and apparent values, beliefs, and ethics. These all affect interaction quality, perceived intelligence, and alignment with both developer and user intentions. The shaping of this persona, known as Character Training, is a critical component of industry post-training, yet remains effectively unstudied in the academic literature. We introduce the first open implementation of character training, leveraging Constitutional AI and synthetic introspective data to shape the assistant persona in a more effective and controlled manner than alternatives such as constraining system prompts or activation steering. Specifically, we fine-tune three popular open-weights models using 11 example personas, such as humorous, deeply caring, or even malevolent. With our methods, the expression of these personas is more robust to adversarial prompting than the above two alternatives, while also leading to more coherent and realistic generations. \nAdditionally, we demonstrate this fine-tuning has little to no effect on general capabilities as measured by common benchmarks. \nFinally, we also introduce a new method to track changes in character by analyzing the revealed preferences of the assistant, uncovering a clear and holistic change induced by our approach. We describe and open-source our full post-training method, the implementation of which can be found at https://anonymous.4open.science/r/OpenCharacterTraining.", "tldr": "We introduce the first open-source implementation of Character Training, shaping the values, beliefs, and ethics of the assistant persona in a more effective and controlled manner than alternatives like prompting or activation steering.", "keywords": ["llm", "persona", "character training", "large language models", "alignment", "value alignment", "ai safety", "ai ethics", "constitutional ai", "open-source"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/348c74a27e6b69c50215f5fc7649e757e923fd5b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an open implementation of character training, i.e., the post-training process aimed at shaping persona-driven behaviors in Large Language Models. To this aim, the authors design a three-stage training pipeline covering (i) the drafting of a constitution shaping personas, (ii) a distillation of behaviors from larger models via DPO, and (iii) a fine-tuning aimed at further enhancing the persona-driven behavior via self-generated introspective data. The authors compare the proposed approach with conditioning prompting and model steering, by also evaluating the robustness to adversarial attacks. Finally, a new strategy for measuring the \"depth\" of the persona expression in models is proposed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The overall idea is interesting and relevant, as character training is key for improving human-computer interactions, and having the possibility to use an open framework is valuable for improving human-AI interactions.\n- The \"adaptation\" of Constitutional AI for shaping synthetic personas in LLMs represents a proper choice for keeping valuable principles at the center of the stage while guiding LLMs towards preferred behaviors.\n- The proposed approach is actually more robust to adversarial attacks that might attempt to \"mask\" the persona-driven behavior, compared to alternative approaches like steering or prompting.\n- The introduction of a novel measure for quantifying the depth of persona expression in aligned models (despite the underlying alignment technique) is a great contribution to the community."}, "weaknesses": {"value": "- The alignment methodology has some key issues to address. Indeed, the proposed framework applies DPO before SFT, which reverses the standard post-training order. This might have a non-negligible effect on diluting the learned preferences, since the introspective SFT leverages the model's own outputs and might emphasize aspects that are far from the \"preferred\" ones. Some ablation tests would be needed to explore a more \"traditional\" ordering and its effect. \n- The evaluation processes throughout the manuscript are entirely based on a trained classifier. This is another key weakness, as only relying on an automated (potentially error-prone) classification might weaken the reported findings. I would suggest performing some human eval to strengthen the results.\n- In Section 2.4, the self-reflection example yields a potentially relevant issue, as it changes the Llama acronym with a fictitious name. This might suggest that character training could affect factuality, thus raising concerns about the usability of \"persona-aligned\" models.\n- While the proposed approach demonstrated better \"alignment\" compared to alternative approaches, the evaluation of how this translates into stronger character-dependent behavior is not properly investigated. Indeed, the only investigation is on coherence (see previous weakness) and \"preferences\". But the latter is influenced by the fact that the underlying models have been optimized for \"preferring\" certain character-dependent tokens during SFT and DPO. Some more behavioral investigations (e.g., decision/choice-based tests) would be valuable to better appreciate the alignment."}, "questions": {"value": "- Figure 3 suggests that the Llama model under the steering strategy is stronger than distill (+introspect). Is it due to a better \"choice\" of the steering parameter than for other models? If so, a grid-search or similar approaches would make the comparison stronger, at least for the quantitative aspects (given that Figure 4 suggests a diverse scenario for qualitative ones).  \n- In Section 3.2, the evaluator is the same model that generated traces upon which the \"character-driven\" one has been aligned. Could this introduce some bias in the evaluation? (see [r1])\n\n[r1] Wataoka, Koki, Tsubasa Takahashi, and Ryokan Ri. \"Self-preference bias in llm-as-a-judge.\" arXiv preprint arXiv:2410.21819 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kFZqy2Pe2r", "forum": "X9MMGZdqmc", "replyto": "X9MMGZdqmc", "signatures": ["ICLR.cc/2026/Conference/Submission22267/Reviewer_yL1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22267/Reviewer_yL1Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856973610, "cdate": 1761856973610, "tmdate": 1762942142784, "mdate": 1762942142784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper employs model distillation and fine-tuning for personalized model training, proposing a set of style categories to evaluate the style of model responses. Specifically, the paper constructs a DPO (Direct Preference Optimization) training dataset using responses generated by GLM4.5 Air and the model-to-be-trained to distill the capabilities of GLM4.5 Air. Subsequently, it further fine-tunes the model using self-reflection and self-multi-turn interaction data generated by the model-to-be-trained."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper is clearly written, the training methodology is comprehensively described, and the model is open-sourced.\n2.  The paper demonstrates good performance in terms of character robustness and response coherence.\n3.  The paper proposes a set of response style categories, enabling a more fine-grained representation of the character's degree of personalization."}, "weaknesses": {"value": "1.  The paper asserts that \"Character Training\" is an unexplored area in academia but a common consensus in industry, thereby emphasizing the pioneering nature of its exploration in the academic field. This claim is debatable.\n2.  The paper only compares its method with training-free approaches like Activation Steering and system prompts. It fails to compare with other post-training methods or the so-called \"industry consensus\" methods.\n3.  The paper generates 10,000 fine-tuning data entries per personalized character using 10 pre-set prompts. This stage lacks diversity in data synthesis, which might be the reason for the limited improvement over the distilled model. Furthermore, despite extensive post-training, the method's performance on Llama3.1 8B is inferior to the Activation Steering method.\n4.  The paper proposes a set of categories for measuring model response style but lacks a description of the methodology used to construct this set. Moreover, the paper does not integrate this set of categories with the personalized characters directly, making it difficult to judge the degree of personalization based on the response style categories."}, "questions": {"value": "1.  Regarding response coherence, the paper presents comparative experiments against distillation and Activation Steering. Given that the 10 pre-set prompts all contain potential guidance towards specific expression habits, does this observed coherence stem from biases introduced by a large amount of data with specific linguistic patterns generated during the subsequent fine-tuning? Can the authors supplement the paper with experiments, such as comparing the coherence gap between the proposed method and the original model, to verify whether this is merely a change in expression habits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rb4A11Kl3g", "forum": "X9MMGZdqmc", "replyto": "X9MMGZdqmc", "signatures": ["ICLR.cc/2026/Conference/Submission22267/Reviewer_d6X5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22267/Reviewer_d6X5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889365973, "cdate": 1761889365973, "tmdate": 1762942142545, "mdate": 1762942142545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the first open-source, end-to-end character training pipeline for LLM assistants, aiming to shape the persona rather than core task ability. The method has three stages: (1) write persona constitutions (~10 first-person assertions per persona), (2) distillation via DPO from a teacher prompted with the constitution while the student answers “naturally,” and (3) introspection, i.e., SFT on synthetic self-reflection and self-interaction dialogues generated by the distilled model. The authors apply this to three open models (LLaMA-3.1-8B, Qwen-2.5-7B, Gemma-3-4B) across 11 personas (e.g., sarcastic, nonchalant, poetic, flourishing, loving, misaligned). They evaluate (i) robustness to adversarial prompting using a persona classifier trained on non-adversarial data; (ii) coherence with an LLM-as-a-Judge (pairwise wins vs activation steering and vs distillation-only); and (iii) a new revealed-preference analysis that estimates how character training changes the relative likelihood (Elo) of expressing ~150 one-word traits. The Key findings of this paper: character-trained models preserve persona under adversarial “break role-play” prompts better than system-prompting and activation steering; they are judged more coherent than steering or distillation-only, with general benchmarks do not degrade, and revealed-preference plots indicate holistic shifts (desired traits increase; opposing traits decrease). Code and adapters are promised as anonymized releases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Persona adherence persists better under “break character” instructions versus system prompts and often versus steering, suggesting the method alters the assistant’s default behavior rather than merely role-playing.\n- The breadth of personas and models, eleven distinct personas spanning style and values (including flourishing, loving, misaligned) across three open-weights models, builds a useful testbed for future study."}, "weaknesses": {"value": "- The paper’s primary limitation lies in its heavy reliance on model-based evaluators, including an LLM-as-a-Judge for coherence assessment and a finetuned persona classifier for robustness measurement, but without proving reliability. While these automated evaluations enable large-scale, reproducible comparisons, they introduce potential circularity and bias—both evaluators are derived from similar distributional assumptions as the models being tested. As a result, improvements in persona persistence or coherence may partly reflect alignment with the evaluator’s inductive biases rather than genuine behavioral change. Without human raters or cross-family judge replication, the validity of these results remains uncertain.\n- The robustness evaluation framework lacks semantic clarity. The “break character” tests rely on a classifier trained exclusively on non-adversarial samples from the same persona pool. This design risks overestimating persona robustness by rewarding stylistic consistency over contextual adaptability."}, "questions": {"value": "Could the authors conduct a small-scale human validation study to verify that the reported coherence improvements and revealed-preference trait shifts align with human perception? Such a study would help determine whether the LLM-as-a-Judge assessments correlate meaningfully with human evaluations of persona consistency and behavioral coherence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SZmggxHkHU", "forum": "X9MMGZdqmc", "replyto": "X9MMGZdqmc", "signatures": ["ICLR.cc/2026/Conference/Submission22267/Reviewer_M1XF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22267/Reviewer_M1XF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898329123, "cdate": 1761898329123, "tmdate": 1762942142249, "mdate": 1762942142249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper introduces a new method for character training consisting of distillation and introspection components. Authors show that the method outperforms 2 other baselines on robustness and coherence metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Paper introduces a new method for character fine-tuning of LLMs in 2 stages: distillation via DPO from a teacher model and introspection.\n\n- Ablation study shows that introspection is a necessary component since it leads to improvement over distillation.\n\n- Open-source codebase could be of use for further research in the area of character training. However, there could be more emphasis on how the proposed methods are specific to the character training problem and not just general post-training of the model."}, "weaknesses": {"value": "- *Lack of grounding in psychology or theory*: the paper engages with concepts such as personality and personas, and introduces a set of personas in Table 1. However, the paper does not attempt to define what they mean by the personas. Are these personality traits (e.g. Impulsive) ? Styles (Poetic)? Moral values (Misaligned)? What about specific characters (e.g. movie personas etc.)? What about combinations of the \"personas\": as some of these are personality traits, can the model personality be assembled out of combinations of such traits? The authors also state that \"The flourishing, loving, and misalignment personas are all more directly related to values, ethics, and alignment than the others\", suggesting that value alignment is another objective of this training? In addition, Figure 1 presents Protective and Caring \"personas\" that also seem redundant (but they are not present in Table 1). Authors also suggest these personas should affect the model preferences or \"beliefs\", but at the same time that \"character training changes manner over content\", which should not affect the underlying \"model values\" but rather just the style of the outputs. Overall, the framing felt confusing and at times self-contradictory.\n\n- *Methodology*: The method requires hand-writing constitutions, which is not scalable to thousands of potential personas as broadly defined by the authors. It was not shown whether self-interaction or self-reflection were the more important components in the Introspection component, are both of them really necessary?\n\n- *Evaluation*: Coherence (one of the 3 main experiments) is evaluated exclusively with LLM-as-a-Judge. The llm judge performance is not validated."}, "questions": {"value": "- Experimental set up in 3.3 could be written in a more clear manner. It is unclear which LLM's preferences are measured and how exactly the data is generated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xv9iO3IbvE", "forum": "X9MMGZdqmc", "replyto": "X9MMGZdqmc", "signatures": ["ICLR.cc/2026/Conference/Submission22267/Reviewer_pZBZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22267/Reviewer_pZBZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933400451, "cdate": 1761933400451, "tmdate": 1762942142017, "mdate": 1762942142017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}