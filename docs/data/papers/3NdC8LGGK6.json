{"id": "3NdC8LGGK6", "number": 16592, "cdate": 1758266483013, "mdate": 1759897230903, "content": {"title": "One Direction to Rule Them All: Toward Generalizable Solving Strategies Across Combinatorial Optimization Problems", "abstract": "Many combinatorial optimization problems (COPs) share latent structure despite differing in surface form, allowing classical heuristics to transfer with minimal adaptation. In contrast, most learning-based solvers are trained in isolation and fail to leverage cross-problem commonalities. This paper explores the possibility of learning generalized solving strategies that capture shared structures across different COPs, enabling easier adaptation to new tasks. We leverage a header-encoder-decoder architecture in which light problem-specific headers and decoders handle inputs and outputs, while a shared, heavy encoder is trained to capture problem-agnostic solving strategies. The key is to align the encoder’s optimization behavior across tasks by enforcing gradient consistency, making updates induced by different COP objectives point in similar directions with comparable magnitudes. We realize this via task-specific feature rotation matrices and loss weights that steer the encoder’s gradients, learned alongside the solver in a bi-level procedure: an inner loop optimizes each task with reinforcement learning on its true objective, and an outer loop tunes rotations and weights through a gradient consistency loss. Experiments on six COPs show that it enhances the model's ability to generalize COPs. The learned encoder on several problems can directly perform comparably on new problems to models trained from scratch, suggesting its potential to support developing the foundational model for combinatorial optimization.", "tldr": "", "keywords": ["Neural Combinatorial Optimization", "Generalization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b7bfc9543b07c3b1460503ce19d71dcaa8cb31f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the possibility of learning common strategies for solving multiple routing combinatorial optimization problems. The main idea is to use a shared encoder with problem-specific headers and decoders. The paper also investigates the potential for transferring the pretrained encoder to new, unseen tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using gradient homogenization appears to be original in the domain of NCO.\n\n2. The multi-task model is trained with RL, the training process does not require labeled data."}, "weaknesses": {"value": "1. Lack of novelty. This approach with task-specific headers and decoders, and a common encoder is presented in [1]. Furthermore, the same idea has been well-studied in [2], even in a more general way (more CO problems, richer experiments, and transfer to new problems) in [1]. The method presented in this work seems to be essentially the same, with only slightly different terminology. What the authors of this paper refer to as an \"encoder\", the authors in [1] called a \"backbone\"; the \"headers\" and \"decoders\" in this paper are equivalent to the \"input and output adapters\".  The proposed gradient homogenization methods are also well-known and have been investigated in the domain of multi-task training.\n\n2. This paper claims to propose generalizable solving strategies across combinatorial optimization problems, but the experiments are conducted only on six routing problems that share many common features, and it is not clear whether this approach would work beyond the routing domain. Furthermore, experiments involving generalization to only one unseen task are not sufficient to support the claims about generalization.\n\n3. Experiments on problems of size 20 and 50 are below the current standards of the NCO community, where state-of-the-art (SOTA) models typically solve problems with hundreds or even thousands of nodes. The baselines are very modest, and many existing multi-task models for routing are missing from the list of baselines (e.g., [3], [4], [5], together with [2]).\n\n[1] Wang et al. Efficient Training of Multi-task Neural Solver for Combinatorial Optimization, TMLR 03/2025\n\n[2] Drakulic et al. GOAL: A Generalist Combinatorial Optimization Agent Learner, ICLR 2025\n\n[3] Berto et al. RouteFinder: Towards Foundation Models for Vehicle Routing Problems, TMLR 09/2025\n\n[4] Li et al. CaDA: Cross-Problem Routing Solver with Constraint-Aware Dual-Attention, ICML 2025\n\n[5] Zhou et al. MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts, ICML 2024"}, "questions": {"value": "In addition to the questions raised in the weaknesses section, I would like to ask for further clarification on the following topics:\n\n1. The overview of the model is not clear. Figure 1 illustrates that each problem has its own header and decoder, but Figure 2 again shows mappings involving the header, encoder, and decoder. In addition, the claim that different headers may process the same instance (with slight variations, such as additional constraints) is confusing. Does this mean that different parts (or features) of a single instance can pass through the headers of different problem encoders? \n\n2. If I understand correctly, for each of the six problems, you pretrained the model on the remaining five and then tested its generalization - is that correct? In that scenario, I am not sure how zero-shot generalization is possible without any additional problem-specific adaptation. While it is clear how the pretrained encoder can be reused for solving a new, unseen task, how are the parameters of the problem-specific header and decoder initialized? During zero-shot generalization, if there is no training of these modules, it seems highly unlikely that the model would perform well with some random initialization.\n\n3. The experimental settings are unclear. Does Table 4 represent a pretrained model on six tasks with additional fine-tuning on each individual task? It seems that you describe this around lines 407-408. What are the fine-tuning settings; for example, how many steps and how much data were used for fine-tuning? Did you apply the same fine-tuning procedure to MCOMAB? I would also like to see the model’s performance after multi-task training, without problem-specific fine-tuning.\n\n4. The same unclear aspects apply to fine-tuning for cross-problem generalization. There is no mention of fine-tuning steps; the only claim is that “the model adapts well to new tasks when a small amount of task-specific data is available.” In Appendix D.3, you mention that fine-tuning is performed \"by freezing the encoder layers for 100-150 epochs, followed by fine-tuning all model parameters on the chosen task for another 100-150 epochs\". Considering that pretraining takes 500 steps, fine-tuning for 300 epochs is extremely long, looks more like training from scratch than fine-tuning. It would be useful to see how fine-tuning of your pretrained model performs versus the single-task scenario, trained from scratch for the same number of epochs. \n\n5. You explicitly defined the TSP as an edge-decision problem - but later, at line 196, you defined the TSP solution as a permutation of nodes? So, do you really make decisions based on edges or nodes? What about other problems - are they node-decision or edge-decision problems?\n\n6. I do not see a value in including the AM model as a baseline. First, it is not a multi-task model, and that work is seven years old. Moreover, AM has not been a state-of-the-art model for a long time."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "C0KPpIinnk", "forum": "3NdC8LGGK6", "replyto": "3NdC8LGGK6", "signatures": ["ICLR.cc/2026/Conference/Submission16592/Reviewer_gaqK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16592/Reviewer_gaqK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760948653446, "cdate": 1760948653446, "tmdate": 1762926667064, "mdate": 1762926667064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the possibility of learning common strategies for solving multiple routing combinatorial optimization problems. The main idea is to use a shared encoder with problem-specific headers and decoders. The paper also investigates the potential for transferring the pretrained encoder to new, unseen tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using gradient homogenization appears to be original in the domain of NCO.\n\n2. The multi-task model is trained with RL, the training process does not require labeled data."}, "weaknesses": {"value": "1. Lack of novelty. This approach with task-specific headers and decoders, and a common encoder is presented in [1]. Furthermore, the same idea has been well-studied in [2], even in a more general way (more CO problems, richer experiments, and transfer to new problems) in [1]. The method presented in this work seems to be essentially the same, with only slightly different terminology. What the authors of this paper refer to as an \"encoder\", the authors in [2] called a \"backbone\"; the \"headers\" and \"decoders\" in this paper are equivalent to the \"input and output adapters\".  The proposed gradient homogenization methods are also well-known and have been investigated in the domain of multi-task training.\n\n2. This paper claims to propose generalizable solving strategies across combinatorial optimization problems, but the experiments are conducted only on six routing problems that share many common features, and it is not clear whether this approach would work beyond the routing domain. Furthermore, experiments involving generalization to only one unseen task are not sufficient to support the claims about generalization.\n\n3. Experiments on problems of size 20 and 50 are below the current standards of the NCO community, where state-of-the-art (SOTA) models typically solve problems with hundreds or even thousands of nodes. The baselines are very modest, and many existing multi-task models for routing are missing from the list of baselines (e.g., [3], [4], [5], together with [2]).\n\n[1] Wang et al. Efficient Training of Multi-task Neural Solver for Combinatorial Optimization, TMLR 03/2025\n\n[2] Drakulic et al. GOAL: A Generalist Combinatorial Optimization Agent Learner, ICLR 2025\n\n[3] Berto et al. RouteFinder: Towards Foundation Models for Vehicle Routing Problems, TMLR 09/2025\n\n[4] Li et al. CaDA: Cross-Problem Routing Solver with Constraint-Aware Dual-Attention, ICML 2025\n\n[5] Zhou et al. MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts, ICML 2024"}, "questions": {"value": "In addition to the questions raised in the weaknesses section, I would like to ask for further clarification on the following topics:\n\n1. The overview of the model is not clear. Figure 1 illustrates that each problem has its own header and decoder, but Figure 2 again shows mappings involving the header, encoder, and decoder. In addition, the claim that different headers may process the same instance (with slight variations, such as additional constraints) is confusing. Does this mean that different parts (or features) of a single instance can pass through the headers of different problem encoders? \n\n2. If I understand correctly, for each of the six problems, you pretrained the model on the remaining five and then tested its generalization - is that correct? In that scenario, I am not sure how zero-shot generalization is possible without any additional problem-specific adaptation. While it is clear how the pretrained encoder can be reused for solving a new, unseen task, how are the parameters of the problem-specific header and decoder initialized? During zero-shot generalization, if there is no training of these modules, it seems highly unlikely that the model would perform well with some random initialization.\n\n3. The experimental settings are unclear. Does Table 4 represent a pretrained model on six tasks with additional fine-tuning on each individual task? It seems that you describe this around lines 407-408. What are the fine-tuning settings; for example, how many steps and how much data were used for fine-tuning? Did you apply the same fine-tuning procedure to MCOMAB? I would also like to see the model’s performance after multi-task training, without problem-specific fine-tuning.\n\n4. The same unclear aspects apply to fine-tuning for cross-problem generalization. There is no mention of fine-tuning steps; the only claim is that “the model adapts well to new tasks when a small amount of task-specific data is available.” In Appendix D.3, you mention that fine-tuning is performed \"by freezing the encoder layers for 100-150 epochs, followed by fine-tuning all model parameters on the chosen task for another 100-150 epochs\". Considering that pretraining takes 500 steps, fine-tuning for 300 epochs is extremely long, looks more like training from scratch than fine-tuning. It would be useful to see how fine-tuning of your pretrained model performs versus the single-task scenario, trained from scratch for the same number of epochs. \n\n5. You explicitly defined the TSP as an edge-decision problem - but later, at line 196, you defined the TSP solution as a permutation of nodes? So, do you really make decisions based on edges or nodes? What about other problems - are they node-decision or edge-decision problems?\n\n6. I do not see a value in including the AM model as a baseline. First, it is not a multi-task model, and that work is seven years old. Moreover, AM has not been a state-of-the-art model for a long time."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "C0KPpIinnk", "forum": "3NdC8LGGK6", "replyto": "3NdC8LGGK6", "signatures": ["ICLR.cc/2026/Conference/Submission16592/Reviewer_gaqK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16592/Reviewer_gaqK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760948653446, "cdate": 1760948653446, "tmdate": 1763023561899, "mdate": 1763023561899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to develop a foundation model capable of solving multiple vehicle routing problems (VRPs) simultaneously. The proposed approach employs a shared encoder combined with problem-specific headers and decoders across six tasks: the Travelling Salesman Problem (TSP), Vehicle Routing Problem (VRP), Split Delivery VRP (SDVRP), Orienteering Problem (OP), Prize Collecting TSP (PCTSP), and Stochastic PCTSP (SPCTSP). To enhance cross-task consistency, the method enforces gradient alignment across different problems using learnable rotation matrices. The experiments evaluate both in-distribution performance and cross-problem generalization, including zero-shot transfer and fine-tuning settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation and methodology are clear.\n2. The idea of a multi-task NCO solver is good.\n3. The analysis of the gradient similarity is interesting."}, "weaknesses": {"value": "1. Missing several baselines mentioned in the related work section.\n2. No results are provided for 100-node synthetic instances.\n3. Training details for the frozen and fine-tuning settings in Figure 5 are not described.\n4. Inference time comparisons should be included in Figure 4.\n5. Minor: Figures 4–6 would be clearer if presented as tables."}, "questions": {"value": "What is the problem size in the experiment of Section 5.5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YPh0MFFGeo", "forum": "3NdC8LGGK6", "replyto": "3NdC8LGGK6", "signatures": ["ICLR.cc/2026/Conference/Submission16592/Reviewer_aeat"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16592/Reviewer_aeat"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943214857, "cdate": 1761943214857, "tmdate": 1762926666607, "mdate": 1762926666607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GCNCO (Gradient-Consistent Neural Combinatorial Optimization), a multi-task framework designed to learn generalized solving strategies across diverse combinatorial optimization problems (COPs).\n\nThe key idea is to enforce gradient consistency—that the encoder’s gradient directions and magnitudes remain aligned across tasks—so that updates from one problem benefit others.\nThe model adopts a header–encoder–decoder architecture, with light problem-specific modules (headers/decoders) and a heavy shared encoder.\nTo homogenize gradients, GCNCO introduces:\n- Gradient magnitude normalization (Eq. 6–8)\n- Gradient direction alignment via rotation matrices (Eq. 9)\n\nTrained jointly on six routing-type COPs (TSP, VRP, SDVRP, OP, PCTSP, SPCTSP), the model demonstrates promising zero-shot and fine-tuning generalization to unseen tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Clear architectural design\n  - The separation between problem-specific and problem-agnostic components (header/encoder/decoder) is clean and aligns with modular multi-task learning practice.\n\n2) Solid empirical scope within routing COPs\n  - Six representative COPs (deterministic and stochastic) are covered, demonstrating consistent performance improvements over baselines.\n\n3) Gradient analysis as an interpretability tool\n  - The use of cosine similarity heatmaps between encoder gradients gives qualitative insight into inter-problem relationships—an appealing analysis direction rarely seen in NCO papers."}, "weaknesses": {"value": "1) Limited differentiation from prior work.\n  - The approach is conceptually close to Rotograd (Javaloy & Valera 2021), extended to the header–encoder–decoder setting. Compared to MCOMAB (Wang et al., 2025), the main novelty is the alignment mechanism, but the discussion of why gradient alignment outperforms task selection is superficial.\n\n2) Restricted experimental diversity.\n  - All six tasks are routing variants; other combinatorial families (e.g., knapsack, scheduling, matching) are missing, which weakens the claim of “generalizable solving strategies across COPs.”"}, "questions": {"value": "1) Can you provide quantitative evidence that higher gradient cosine similarity indeed correlates with better zero-shot performance across problems?\n\n2) How many fine-tuning iterations or epochs were required for the model to reach its reported post-adaptation performance? and Does gradient homogenization during pretraining empirically accelerate convergence during fine-tuning, or only improve final performance?\n\n3) How does GCNCO perform on non-routing COPs (e.g., knapsack, scheduling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w6ZBBlTR7z", "forum": "3NdC8LGGK6", "replyto": "3NdC8LGGK6", "signatures": ["ICLR.cc/2026/Conference/Submission16592/Reviewer_V1or"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16592/Reviewer_V1or"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992586994, "cdate": 1761992586994, "tmdate": 1762926666150, "mdate": 1762926666150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents GCNCO, a framework designed to overcome the single-task limitation of most neural solvers for combinatorial optimization problems (COPs). The central idea is to train a shared, powerful encoder on multiple tasks simultaneously, using a novel technique called \"gradient consistency.\" By optimizing special rotation matrices and loss weights, the framework forces the learning signals from different COPs to align, ensuring that the encoder develops a unified and transferable problem-solving strategy. Experiments across six routing problems show that this approach is highly effective. A key finding is that the pre-trained GCNCO encoder can be applied to a new, unseen problem and achieve performance comparable to a specialist model trained from scratch, highlighting its potential as a foundational tool for the field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. To break the isolation of single-task solvers, this paper introduces GCNCO, which learns a general optimization strategy using a shared encoder trained with an innovative gradient consistency mechanism. \n\n2.Validated across six COPs, the model shows superior performance and strong generalization.   It not only outperforms baselines on the problems it was trained on but also shows exceptional cross-problem generalization.   Its effectiveness is validated by comprehensive experiments: gradient similarity analysis confirms the alignment works, while in-distribution tests show no performance degradation on trained tasks\n\n3. Its pre-trained encoder achieves competitive zero-shot results on unseen tasks and surpasses baselines after fine-tuning, demonstrating its potential as a foundational model for rapid, resource-efficient optimization."}, "weaknesses": {"value": "1. Unproven Scalability to Larger Instances:\nThe experiments are confined to small-scale problems (20-50 nodes), so the model's performance on larger, industrially relevant instances remains an open question. \n\n2. The title is \"ONE DIRECTION TO RULE THEM ALL: TOWARD GENERALIZABLE SOLVING STRATEGIES ACROSS COMBINATORIAL OPTIMIZATION PROBLEMS\". However, currently all problems are routing based. A clarification on this could be helpful."}, "questions": {"value": "1.Task Selection and Similarity: The six COPs chosen for the experiments (TSP, VRP, SDVRP, OP, PCTSP, SPCTSP) are all variants of routing problems on graphs. How does the choice of included tasks affect\nlearning? If a new problem is very different (say, a scheduling problem), would the\nencoder still transfer? Have you tried adding a completely unrelated COP to the\ntraining set, and if so, how does it impact the shared encoder?\n\n2. Scalability to Larger Problem Instances:\nThe experiments are conducted on relatively small problem sizes (20 and 50 nodes). While common for foundational research in neural combinatorial optimization, the performance on these scales does not guarantee effectiveness on larger, more industrially relevant instances (e.g., hundreds or thousands of nodes). The computational complexity of the attention-based encoder and the overhead of the bi-level optimization might become prohibitive as the problem size increases.\n\n3. A discussion on robustness of the model when used different training size data of problems are used could be helpful as in reality different sized problems occur commonly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Pnuo3w51GP", "forum": "3NdC8LGGK6", "replyto": "3NdC8LGGK6", "signatures": ["ICLR.cc/2026/Conference/Submission16592/Reviewer_NcGk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16592/Reviewer_NcGk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762175884985, "cdate": 1762175884985, "tmdate": 1762926665673, "mdate": 1762926665673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}