{"id": "2uctT30vTS", "number": 19631, "cdate": 1758297830744, "mdate": 1759897029287, "content": {"title": "KnItLM: Weaving Knowledge into Instruction-Tuned LLMs via Continual Pre-Training and Merging", "abstract": "RAG has become the de facto method for incorporating new, corpus-specific knowledge into an instruction following LLM (Instruct LLM). Although RAG-based prompting improves factual grounding, it fails when retrieval is incorrect or incomplete, leading to hallucinations. Finetuning methods such as RAFT and PA-RAG enhance RAG by ingesting new knowledge into the parameters of the model,  but require generating massive amount of synthetic QA that covers the entire corpus. Continued Pre-Training (CPT) on the text corpus avoids the need for comprehensive synthetic data generation but breaks the instruction following capabilities of an Instruct LLM, necessitating instruction fine-tuning (IFT) post CPT. However, IFT is costly and may be infeasible due to the unavailability of an instruction tuning corpus. In this work, we propose KnItLM - KNowledge IngesTion via LoRA Merging. Instead of doing CPT on the Instruct LLM, KnItLM performs CPT with Low-Rank Adapters (LoRA) on its corresponding base LLM  to infuse new knowledge. These knowledge-infused LoRA weights are then merged with the Instruct LLM, imparting new knowledge without impacting their instruction following capabilities. KnItLM avoids expensive instruction fine-tuning and relies on model merging to infuse the new knowledge into the Instruct LLM without destroying its instruction following capabilities. Empirical results show that KnItLM significantly improves the performance of RAG by taking accuracy from $54.17$% to $79.26$% for retrieval failure cases. In addition, the proposed method achieves superior performance to existing approaches while requiring substantially less training data.", "tldr": "Knowledge Ingestion into Instruction-Tuned LLMs via Continual Pre-Training and Merging for Improved RAG Performance", "keywords": ["Knowledge Ingestion", "RAG", "Lora Transfer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c44e11f896b10c24194114aa815dbdac628170e0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The KNITLM (Knowledge Ingestion via LoRA Merging) framework proposed in this paper addresses the core issues of \"continuous pre training (CPT) disrupting instruction following ability\" and \"large dependence on synthetic data\" in the domain knowledge injection of Instruction LLM. It innovatively adopts the technical route of \"training knowledge LoRA on the Base LLM+integrating with Instruction LLM\" to achieve collaborative retention of knowledge injection and instruction ability."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. By training knowledge LoRA on the base model and integrating it with Instruction LLM, the synergy between \"knowledge vector\" and \"instruction vector\" is theoretically achieved using \"task vector addition\", which not only retains the instruction following ability of Instruction LLM, but also injects new domain knowledge.\n\n2. By using the GRPO algorithm and a mixed reward of \"effectiveness-efficiency-structural quality\", the accuracy of knowledge injection is ensured while overfitting is suppressed, achieving lightweight training"}, "weaknesses": {"value": "1. In the relevant work section, although task arithmetic is mentioned, there is no in-depth comparison of the core differences between KNITLM and other model editing methods.\n\n2. Using LLM as Jade binary scoring (0/1) as the core indicator, other key indicators of knowledge injection were not reported.\n\n3. Verified only on two Redbook technical document datasets and not extended to other fields such as healthcare, finance, short text conversations, and long document reports. The model has not been tested on larger-scale models (20-80B) or different architecture models, making it impossible to determine the model compatibility of the method.\n\n4. There are too few baselines used, and more SOTA models need to be compared."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UeGzetDonl", "forum": "2uctT30vTS", "replyto": "2uctT30vTS", "signatures": ["ICLR.cc/2026/Conference/Submission19631/Reviewer_Db8X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19631/Reviewer_Db8X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761286865294, "cdate": 1761286865294, "tmdate": 1762931481863, "mdate": 1762931481863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of incorporating domain-specific knowledge into instruction-tuned LLMs without compromising their instruction-following capabilities. The authors propose KnitLM, which performs continual pretraining (CPT) with LoRA adapters on base LLMs and transfers these adapters to instruction-tuned models. The authors also propose to use instruction model token embeddings during training to improve adapter compatibility."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper tackles an important practical challenge in LLM: how to efficiently incorporate new domain knowledge without expensive instruction fine-tuning.\n- The experimental setup carefully avoids data contamination, and the evaluations are reasonably thorough."}, "weaknesses": {"value": "- The core contribution is incremental and somewhat limited. The central idea of combining LoRA-based continual pretraining with model merging (task arithmetic) is not novel. Prior work has explored task vector merging and LoRA transfer. The only substantive twist is using instruct-model token embeddings during LoRA training, which is a minor technical detail rather than a conceptual advance.\n- The paper only compares against RAFT and PA-RAG. Stronger baselines (e.g., direct LoRA tuning on the instruct model, or parameter interpolation approaches) are missing.\n- Section 3 uses $\\Delta\\theta$ to denote both task vectors (differences between model weights) and LoRA adapter weights. This is confusing because task vectors are full-rank weight differences, while LoRA adapters are low-rank decompositions.\n- Minor comment: Citation formatting needs correction. Citations should appear in parentheses rather than as part of the text: Zhang et al. (2024b) -> (Zhang et al., 2024b). In the abstract, many abbreviations such as RAG,  LLM are used before defining them."}, "questions": {"value": "1. Why not simply fine-tune base model fully on new knowledge (not LoRA), then compute true knowledge task vector and add to instruct model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JHl80bULRg", "forum": "2uctT30vTS", "replyto": "2uctT30vTS", "signatures": ["ICLR.cc/2026/Conference/Submission19631/Reviewer_4Ptt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19631/Reviewer_4Ptt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980231547, "cdate": 1761980231547, "tmdate": 1762931480988, "mdate": 1762931480988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a lightweight knowledge-ingestion method that performs CPT with LoRA on the base model and merges the knowledge adapter with the instruction model. An insightful technique is taking the instruction model's token embeddings during CPT to reduce distribution shift. The proposed method targets preserving the model's instruction-following ability while improving closed-book and RAG performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Training a knowledge LoRA on the base model and merging it with the instruction model is a lightweight technique that avoids re-running the SFT and may be RL for preserving the instruction following ability.\n2. Using token embeddings from the instruct LLM during CPT is insightful and reasonable."}, "weaknesses": {"value": "I am satisfied with the method itself, but have concerns about the experiment settings. \n1. Limited baselines and benchmarks. The evaluation is restricted to two technical Redbooks. While the authors discuss test set quality and knowledge cutoff issues, the current scale is small and may not fully establish generality. The cutoff may be avoidable by testing the direct QA performance. If the model performs poorly for the test questions, it could imply that the model's parameterized knowledge does not contain the test set knowledge. In this way, this work can be comparable with board baseline models on more benchmarks."}, "questions": {"value": "1. For the ablation study in section 5.4, the performance drop of e-KNITLM seems not very significant. The usefulness of replacing the embedding with the instruction model may contribute to two aspects, 1) as the author stated, avoiding OOD tokens like the special tokens used in the chat template, 2) for the tokens that are well-trained in the base model, the instruction model has a better representation. I am curious which one contribute more to the performance gain?\n2. For the section 5.2, IMPACT OF THE SIZE OF THE SYNTHETIC DATA, I agree with the statement \"access to QA from only a part of the corpus, will still show gains over the remaining data\", but this seems to be a generic property. Does the KnItLM benifit more from this and why the baseline methods can not, since they also adopt synthetic data for training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zhxcE6KtuR", "forum": "2uctT30vTS", "replyto": "2uctT30vTS", "signatures": ["ICLR.cc/2026/Conference/Submission19631/Reviewer_TYoE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19631/Reviewer_TYoE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987464277, "cdate": 1761987464277, "tmdate": 1762931480595, "mdate": 1762931480595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel continued pre-training approach for incorporating new knowledge into instruction-tuned LLMs, addressing the issue that conventional methods often degrade instruction-following capabilities. The authors introduce a method that merges two task vectors: a knowledge vector (representing the new knowledge to be injected) and an instruction task vector, obtained as the difference between the baseline model vector and the instruction-tuned model vector. Experimental results show that KnItLM achieves improvements over existing baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proposed merging of the instruction-following vector and the knowledge vector based on task vectors is novel, well motivated, and technically interesting.\n2.\tThe special treatment of token embeddings adds further depth and elaboration to the method.\n3.\tThe paper is clearly written and easy to follow, with detailed explanations.\n4.\tThe experimental results demonstrate that the proposed methods achieve performance improvements over the baselines."}, "weaknesses": {"value": "1.\tIn Table 1, it is unclear whether the reported improvements are substantial.\n2.\tIt is not clear whether the proposed method can be applied in an incremental manner when additional knowledge is introduced. Once the knowledge vector is injected into the base model, does the updated model then serve as the new base model for the next stage?\n3.\tAdditional comparisons with other continual learning baselines would strengthen the experimental evaluation."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AQP6lJdjUN", "forum": "2uctT30vTS", "replyto": "2uctT30vTS", "signatures": ["ICLR.cc/2026/Conference/Submission19631/Reviewer_saay"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19631/Reviewer_saay"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183516904, "cdate": 1762183516904, "tmdate": 1762931479975, "mdate": 1762931479975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}