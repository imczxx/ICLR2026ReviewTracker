{"id": "5fN48w1lhy", "number": 9800, "cdate": 1758141052533, "mdate": 1763600079493, "content": {"title": "Heterogeneous Front-Door Effects: Debiased Estimation with Quasi-Oracle Guarantees", "abstract": "In many applications, treatment and outcome are confounded by unobservables, yet mediators remain unconfounded. The front‑door (FD) adjustment identifies causal effects through mediators even with unmeasured confounding. However, most estimators focus on *average treatment effects*, and work on *heterogeneous treatment effect* (HTE) estimation remain scarce. We address this gap with two *debiased* learners for heterogeneous FD effects: *FD‑DR‑Learner* and *FD‑R‑Learner*. Both attain fast, quasi-oracle rates (i.e., performance comparable to an oracle that knows the nuisances) even when nuisance functions converge as slowly as $n^{-1/4}$. Beyond theory, we demonstrate fast convergence and debiasedness in synthetic and real-world evaluations. Our results show that the proposed learners deliver robust and debiased HTE estimates under the FD scenario.", "tldr": "We developed debiased estimators for heterogeneous front-door causal effects.", "keywords": ["front-door", "heterogeneous-treatment-effect.orthogonal-statistical-learning"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad7e919c62b1f09ec068018a34ca788aeb60d4c2.pdf", "supplementary_material": "/attachment/6defe7cc0c8eb217b1b0115a274546b9e6d4002d.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose an adaptation of the R-learner for CATE estimation using a front-door structure with a binary mediator (instead of a back-door adjustment which the R-learner was developped for). They fill the gap with prior work that had been focusing on targeted averages, while their method targets conditional effects $\\tau(.)$. To do this, they adapt two learners (DR and R-learners) to the FD setting. They reach quasi-oracle"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The theoretical properties are clear and the setting too. The subject is interesting and well positioned. The empirical evaluation design seems sound.\n\nI especially like the apparent soundness of the results"}, "weaknesses": {"value": "I found the paper's notation quite extensive and hard to read. Especially equations 1, 2 and 3 need to be described, and more importantly there is a lack of explanation in the. main text of the nuisance functions defined in Eq. 15, 16 and 17. The authors need to explicitly state what $\\bar x$ refers to too.\n\nWhy did you not consider comparing your performances with Chen et al. 2025? \n\nI am not sure whether the environments for definitions, propositions and theorems are in accordance to the conference's template"}, "questions": {"value": "Could you consider a simulation with controlled FD violation to show the behavior of the estiamtor under misspecification?\n\nCan you give more details on density ratios?\n\nWhat happens in small sample sizes? The sizes considered in the synthetic study are very big and FD-PI seems to have better RMSE in lower sample sizes, can you comment on this?\n\nCan your method provide confidence intervals for $\\tau(.)$?\n\nOther baselines could be considered in the experiments such as the oracle estimator and the mentionned Chen et al. 2025 method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "csGI8e7lXI", "forum": "5fN48w1lhy", "replyto": "5fN48w1lhy", "signatures": ["ICLR.cc/2026/Conference/Submission9800/Reviewer_mEQU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9800/Reviewer_mEQU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575343144, "cdate": 1761575343144, "tmdate": 1762921287117, "mdate": 1762921287117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the front-door criterion to identify heterogeneous treatment effect where there exists unobserved confounding. The authors proposed two estimator for this regime: FD-DR-learner and FD-R-learner. The FD-DR-learner follows the DR-learner framework that constructs a doubly robust pseudo-outcome leveraging FD identification of the treatment effect first, then regresses the covariates on the pseudo-outcome. The FD-R-learner first uses the (backdoor) R-learner to fit some of the nuisances, then constructs a \"pseudo-outcome/function\" from the learned nuisances to obtain the final estimator. Moreover, the author also provides an error analysis for the proposed estimator."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Proposed two novel estimators with discussion on their performance, and when one is preferred over the other. \n\n- Provided error analysis that demonstrated the robustness of the proposed estimators."}, "weaknesses": {"value": "- Proposed estimators are mainly built on top of ideas from exiting estimators.\n\n- The FD-R-learner requires the estimation of many nuisance functions (has to split data into 3 folds). This could suffer when the sample size is small. \n\n- The paper discussed cases where one estimator is preferred over the other, but did not include experiments to support the claims."}, "questions": {"value": "- This paper only considers binary mediators, can the estimators be extended to higher dimensional or continuous mediators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8YMcpj9BwB", "forum": "5fN48w1lhy", "replyto": "5fN48w1lhy", "signatures": ["ICLR.cc/2026/Conference/Submission9800/Reviewer_KeNg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9800/Reviewer_KeNg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579659838, "cdate": 1761579659838, "tmdate": 1762921286141, "mdate": 1762921286141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of estimating heterogeneous treatment effects (HTEs) under unmeasured confounding using the front-door (FD) criterion. It introduces two novel debiased estimators—FD-DR-Learner and FD-R-Learner—that achieve quasi-oracle convergence rates even when nuisance functions converge slowly at $n^{-1/4}$. FD-DR leverages doubly robust pseudo-outcomes, while FD-R decomposes effects into interpretable causal pathways. Both methods demonstrate superior performance and robustness in synthetic and real-world experiments, providing accurate and stable HTE estimates where traditional plug-in estimators fail. Overall, the study advances causal inference by enabling reliable, personalized effect estimation under front-door settings with theoretical and empirical validation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is among the first to tackle HTEs under FD.\n2. The proposed method is supported by rigorous theoretical analysis.\n3. Experimental results demonstrate that the proposed approach."}, "weaknesses": {"value": "1. The proposed methods and theory are restricted to binary mediators.\n2. The paper does not include empirical comparisons with existing HTE-FD methods (e.g., Chen et al., 2025, LobsterNet).\n3. Beyond double robustness, a key advantage of debiased estimators lies in their asymptotic normality, which enables valid statistical inference. However, the paper does not provide any inference results or variance estimation, leaving the uncertainty quantification of the proposed estimators unexplored."}, "questions": {"value": "1. While theoretically appealing, the front-door identification relies on strong and often unverifiable assumptions—such as the absence of unmeasured confounders between mediators and outcomes. As noted by Imbens and others [1], these conditions are rarely satisfied in practice, raising concerns about the empirical credibility and real-world applicability of the proposed methods.\n2.  Figure 3(a)’s histogram suggests that the sampling distribution of the estimator deviates noticeably from normality.\n\n\n\n[1] Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lRE8IA7ix5", "forum": "5fN48w1lhy", "replyto": "5fN48w1lhy", "signatures": ["ICLR.cc/2026/Conference/Submission9800/Reviewer_sZgx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9800/Reviewer_sZgx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805339946, "cdate": 1761805339946, "tmdate": 1762921284561, "mdate": 1762921284561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents estimators for the CATE under a front-door adjustment formula. This contrasts with existing works, which focus on a back-door formula. The authors derive variants of the DR- and R-leaners for this setting and provide an error analysis that reveals double robustness."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "To my knowledge, this the first estimator of the CATE under front-door adjustment. This is important since it makes it possible to estimate the CATE in a wider variety of settings than was previously possible.\n\nThe theoretical arguments appear to be sound."}, "weaknesses": {"value": "The theoretical analysis ends up being somewhat similar to that of existing backdoor methods, but that's no fault of the authors - that analysis strategy works well."}, "questions": {"value": "Minor comment on references on line 41-42:\n\n* R-learner was first proposed in Corollary~9.1 of\nRobins, James M. \"Optimal structural nested models for optimal sequential decisions.\" Proceedings of the Second Seattle Symposium in Biostatistics: analysis of correlated data. New York, NY: Springer New York, 2004.\n* DR-learner was proposed in Section 3.1 of \nvan der Laan, Mark J. \"Targeted Learning of an Optimal Dynamic Treatment, and Statistical Inference for its Mean Outcome.\" (2013)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TB8IT3f53w", "forum": "5fN48w1lhy", "replyto": "5fN48w1lhy", "signatures": ["ICLR.cc/2026/Conference/Submission9800/Reviewer_YM1c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9800/Reviewer_YM1c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935925876, "cdate": 1761935925876, "tmdate": 1762921284054, "mdate": 1762921284054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response 1: Comparison with Chen et al. (LobsterNet)"}, "comment": {"value": "Reviewers sZgx and mEQU asked about comparisons with Chen et al. (2025) [1], who proposed LobsterNet, a neural-network–based estimator for heterogeneous front-door effects. A key clarification is that LobsterNet is a *plug-in front-door estimator (FD-PI)*, where the nuisance components are modeled using a neural network. Their method does **not** incorporate debiasing, orthogonality, or quasi-oracle guarantees. Since our proposed estimators are agnostic to any nuisance learners, our Theorem (1,3) guarantees outperformance of FD-DR and FD-R learners. Indeed, you may think our estimators as a principled way of enhancing Neural-network based FD-CATE learners such as LobsterNet. \n\nGiven this, we compared our debiased estimators (FD-DR and FD-R) to a plug-in baseline under the same simulation settings as Chen et al. (2025), using sample sizes n=[1000, 2500, 5000, 10000, 20000, 50000]\n\n---\n\n**1. Simulation without nuisance noise**\n\n| **n** | **FD-PI** | **FD-DR** | **FD-R** |\n| --- | --- | --- | --- |\n| 1000 | 0.228 ± 0.044 | 0.228 ± 0.029 | 0.292 ± 0.025 |\n| 2500 | 0.165 ± 0.024 | 0.206 ± 0.035 | 0.188 ± 0.022 |\n| 5000 | 0.142 ± 0.026 | 0.143 ± 0.016 | 0.138 ± 0.011 |\n| 10000 | 0.108 ± 0.015 | 0.093 ± 0.007 | 0.086 ± 0.005 |\n| 20000 | 0.096 ± 0.006 | 0.077 ± 0.004 | 0.075 ± 0.003 |\n| 50000 | 0.078 ± 0.010 | 0.055 ± 0.003 | 0.054 ± 0.002 |\n\nAs predicted by Theorems 1 and 3, **FD-DR** and **FD-R** outperform the plug-in estimator as n increases. \n\n---\n\n**2. Simulation with nuisances converging at the $n^{-1/4}$ rate.** \n\n| **n** | **FD-PI** | **FD-DR** | **FD-R** |\n| --- | --- | --- | --- |\n| 1000 | 0.341 ± 0.022 | 0.294 ± 0.046 | 0.263 ± 0.035 |\n| 2500 | 0.248 ± 0.022 | 0.183 ± 0.017 | 0.181 ± 0.017 |\n| 5000 | 0.203 ± 0.008 | 0.147 ± 0.020 | 0.140 ± 0.016 |\n| 10000 | 0.162 ± 0.007 | 0.100 ± 0.009 | 0.097 ± 0.007 |\n| 20000 | 0.133 ± 0.006 | 0.079 ± 0.006 | 0.072 ± 0.003 |\n| 50000 | 0.104 ± 0.004 | 0.053 ± 0.002 | 0.054 ± 0.002 |\n\nThese results demonstrate the robustness of the debiased learners. Even when nuisances converge slowly, **FD-DR** and **FD-R** maintain nearly the same performance as in the noiseless case, which achieves precisely the quasi-oracle behavior guaranteed by their orthogonal structure (Theorems 1,3). The plug-in estimator degrades substantially under this setting, consistent with the theoretical expectation. \n\n---\n\n**Takeaway** \n\nLobsterNet is a FD-PI with neural nuisance learners, without debiasing or orthogonality. Our methods provides principled method to enhance those non-debiased estimators. As gauranteed in Theorems (1,3), our comparison shows that **FD-DR** and **FD-R** consistently improve upon plug-in estimators, owing to their debiased construction and quasi-oracle error control. \n\n---\n\n[1] Winston Chen, Trenton Chang, Jenna Wiens Proceedings of the sixth Conference on Health, Inference, and Learning, PMLR 287:194-230, 2025."}}, "id": "Ww4BAPJ5w4", "forum": "5fN48w1lhy", "replyto": "5fN48w1lhy", "signatures": ["ICLR.cc/2026/Conference/Submission9800/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9800/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9800/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763601120295, "cdate": 1763601120295, "tmdate": 1763601401977, "mdate": 1763601401977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response 2: Comparison with Small Sample"}, "comment": {"value": "This response addresses the requests from Reviewers KeNg and mEQU regarding the performance of our estimators in small sample regimes. We conducted new simulations comparing our debiased estimators (FD-DR and FD-R) to the plug-in baseline (FD-PI) using small sample sizes, specifically $n=[100, 250, 500, 1000, 2000, 5000]$.\n\nThe results confirm a known trade-off in orthogonal debiased machine learning: FD-PI temporarily outperforms FD-DR and FD-R at very small $n$ due to the high finite-sample variance imposed by cross-fitting. However, our debiased learners quickly dominate once sample size increases, where their theoretically guaranteed bias reduction takes effect.\n\n---\n\n**1. Simulation without nuisance noise**\n\n| **n** | **FD-PI** | **FD-DR** | **FD-R** |\n| --- | --- | --- | --- |\n| 100 | 0.348 ± 0.018 | 1.092 ± 0.618 | 2.213 ± 0.382 |\n| 250 | 0.315 ± 0.020 | 0.644 ± 0.250 | 0.606 ± 0.051 |\n| 500 | 0.271 ± 0.015 | 0.300 ± 0.042 | 0.432 ± 0.049 |\n| 1000 | 0.212 ± 0.007 | 0.272 ± 0.030 | 0.267 ± 0.022 |\n| 2000 | 0.185 ± 0.016 | 0.188 ± 0.028 | 0.202 ± 0.025 |\n| 5000 | 0.147 ± 0.021 | 0.125 ± 0.014 | 0.144 ± 0.021 |\n\n**2. Simulation with nuisances converging at the $n^{-1/4}$ rate.**\n\n| **n** | **FD-PI** | **FD-DR** | **FD-R** |\n| --- | --- | --- | --- |\n| 100 | 0.394 ± 0.023 | 2.057 ± 0.559 | 1.679 ± 0.467 |\n| 250 | 0.404 ± 0.020 | 0.901 ± 0.181 | 0.579 ± 0.097 |\n| 500 | 0.356 ± 0.039 | 0.446 ± 0.039 | 0.359 ± 0.049 |\n| 1000 | 0.307 ± 0.021 | 0.307 ± 0.036 | 0.233 ± 0.043 |\n| 2000 | 0.264 ± 0.005 | 0.215 ± 0.033 | 0.201 ± 0.025 |\n| 5000 | 0.196 ± 0.007 | 0.125 ± 0.017 | 0.137 ± 0.013 |\n\n---\n\nFor $n \\le 500$, the FD-PI baseline yields lower error and lower variance than our debiased learners. This degradation is expected and can be explained by the necessary structural overhead of orthogonal methods:\n\n1. **Cross-Fitting Penalty:** Both FD-DR (2-split) and especially FD-R (3-split) require splitting the data to ensure orthogonality and manage asymptotic bias. At $n=100$, the training samples used for the nuisance functions (e.g., XGBoost) are severely reduced ($n/2$ or $n/3$), leading to poorly estimated nuisance models and unreliable pseudo-outcomes. The FD-PI estimator temporarily avoids this penalty by using the entire dataset. \n2. **Variance Inflation (FD-DR):** FD-DR uses inverse density weights (density ratios $\\xi_{\\overline{x}}$). In small samples, these denominators are poorly estimated, causing the weights to inflate drastically, resulting in the variance explosion seen at $n=100$.\n3. **Nuisance Error Multiplication:** Theorems 1 and 3 establish that FD-DR and FD-R achieve quasi-oracle rates because their errors are controlled by higher-order products of nuisance errors (e.g., $||\\hat{\\eta}_1 - \\eta_1|| \\times ||\\hat{\\eta}_2 - \\eta_2||$ or $||\\hat{\\eta} - \\eta||^4$). This multiplication ensures that if the individual nuisance errors are $\\mathcal{O}(n^{-1/4})$, the total bias term is $\\mathcal{O}(n^{-1/2})$ or higher, effectively making the debiased estimator converge at the fast oracle rate. However, if the nuisance errors are large (as they are at $n=100$), then the error terms are multiplied into larger, rather than smaller, values, causing initial error inflation. Since flexible machine learning estimators like XGBoost or Neural Networks (NNs) are used, as data grows, we expect these nuisance errors to become moderate, allowing the quasi-oracle properties of FD-DR and FD-R to dominate for $n \\ge 1000$."}}, "id": "5dqLYivNgJ", "forum": "5fN48w1lhy", "replyto": "5fN48w1lhy", "signatures": ["ICLR.cc/2026/Conference/Submission9800/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9800/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9800/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763601384547, "cdate": 1763601384547, "tmdate": 1763601384547, "mdate": 1763601384547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}