{"id": "cdTCrRmV2v", "number": 15174, "cdate": 1758248577908, "mdate": 1759897323512, "content": {"title": "FAME : Factor-aware Mixture-of-Experts with Pretrained Encoder for combinatorial generalization", "abstract": "The integration of pretrained encoders with diffusion policies has emerged as a dominant paradigm for visual robotic manipulation. However, it still struggles to generalize across complex environments with varying factors like lighting and surface textures.\nTo address this, we propose FAME, a framework that integrates a factor-aware mixture-of-experts (MoE) with a pretrained encoder to significantly enhance generalization to environmental variations. FAME involves a three-stage training process. (1) policy warmup, where a diffusion policy is trained on data from a standard environment using a frozen encoder. (2) factor-specific adapter training, where we separately train a series of lightweight adapters, inserted between the frozen encoder and the temporally frozen policy, on customized datasets, each focusing on a distinct environmental variation. (3) joint fine-tuning, where we simultaneously train a centric router and the warmed policy on a mixed dataset to handle multiple factors at once. We say FAME is ``factor-aware'' because the central router organizes the frozen factor-specific adapters as a MoE, allowing for combinatorial generalization for multiple factors.\nEvaluations on the Meta-World benchmark with various environmental factors show that our proposed FAME significantly outperforms existing diffusion policy baselines. Furthermore, FAME demonstrates remarkable scaling properties as the number of demonstrations increases. We believe our FAME provides an effective solution for achieving combinatorial generalization in visual robotic control tasks.", "tldr": "", "keywords": ["Mixture-of-Experts (MoE)", "Pretrained Encoder", "Diffusion Policy", "Robotic Manipulation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7e18b78ebbd3bd0cb87ab398b33b711324eae8e.pdf", "supplementary_material": "/attachment/32624ce4d896d512dbfad2a2671abbcf6c2858a5.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces FAME, a novel framework that addresses the challenge of combinatorial generalization in visual robotic manipulation by decomposing environmental variations into independent factors. The key innovation is a three-stage training approach: (1) warming up a diffusion policy with a frozen pretrained encoder on standard data, (2) training separate lightweight adapters for each environmental factor (lighting, table texture, camera position, arm pose, floor texture) while keeping the encoder and policy frozen, and (3) learning a gating network that dynamically combines these factor-specific adapters via a Mixture-of-Experts architecture to handle multiple simultaneous variations. This design reduces the data complexity from exponential ($N^K$ for K factors) to approximately linear ($N\\times K$), and demonstrates strong empirical results on Meta-World benchmarks, while exhibiting excellent scaling properties, robustness to dataset composition, and cross-task transfer capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel perspective on compositional generalization: The paper presents a valuable insight that environmental variations in robotic manipulation can be decomposed into independent factors, transforming the data complexity from exponential ($N^K$) to linear ($N×K$) through a divide-and-conquer strategy.\n- Effective architectural design: The integration of Mixture-of-Experts with factor-specific adapters proves to be a well-motivated solution. The dynamic gating mechanism successfully achieves combinatorial generalization by learning to compose specialized adapters based on environmental context.\n- Comprehensive experimental validation: The paper provides thorough empirical evidence across 9 Meta-World tasks with systematic evaluation under varying complexity levels (1-5 simultaneous factors)."}, "weaknesses": {"value": "- Limited scalability and unclear factor selection strategy:\n  - The paper only demonstrates effectiveness with $K=5$ carefully designed factors. It remains unclear how the framework would scale when facing environments requiring a larger number of factors. As $K$ increases, the MoE would require more adapters, potentially diminishing the efficiency gains and introducing challenges in training stability and inference complexity.\n  - The paper provides no principled methodology for factor selection and decomposition. For new tasks or environments, practitioners lack clear guidance on: (i) how to identify which environmental variations should be treated as separate factors, (ii) how to determine the appropriate granularity of factor decomposition, and (iii) how to design corresponding datasets for each factor. This absence of a systematic factor identification framework limits the practical applicability of FAME beyond the specific MetaWorld setup presented.\n\n- All experiments are conducted exclusively in the MetaWorld simulation benchmark, lacking real-world validation to demonstrate practical effectiveness. Moreover, the three-phase training protocol critically depends on access to factor-specific datasets ($D_k$) where individual factors can be precisely controlled and isolated. In real-world scenarios, collecting such controlled single-factor variation data is often impractical or prohibitively expensive. The paper does not discuss how to adapt the training methodology when such idealized factor-specific datasets are unavailable."}, "questions": {"value": "- Does FAME train a separate policy for each of the 9 Meta-World tasks, or does it employ a single multi-task policy that handles all tasks simultaneously?\n\n- In Sec.3.1, the Mix Gen Dataset ($D_\\textbf{multi}$) is defined to include environments with $i \\in $ {2, 3, 4, K} simultaneously varying factors. Have the authors conducted ablation studies comparing different compositions of $D_\\textbf{multi}$ (e.g., including $i=1$, using only $I=K$, or using subsets like {2, K})?\n\n- The conclusion in Sec.4.3 that \"the gate trained on the Handle Pull task can be directly and effectively transferred to the Peg Insert Side task in a zero-shot manner\" lacks justification. What demonstrate that this transfer is \"effective\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review is needed since experiments are conducted in simulation."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jqRe7xhIuq", "forum": "cdTCrRmV2v", "replyto": "cdTCrRmV2v", "signatures": ["ICLR.cc/2026/Conference/Submission15174/Reviewer_NNJy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15174/Reviewer_NNJy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760879590185, "cdate": 1760879590185, "tmdate": 1762925483751, "mdate": 1762925483751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework to adapt pre-trained vision encoders (e.g., DINOv2) to robotic manipulation tasks, where certain visual factors in the environment—such as lighting and surface textures—can change. The authors propose training one visual adapter per environment factor and using a gating network to combine the adapter representations as the final representation for policy training. Experiments are conducted in a simulation environment with full control over the variation of factors.\n\nOverall, the motivation of the work needs to be justified more clearly. On one hand, the authors claim that pre-trained visual representations are rich and powerful. On the other hand, however, they curate domain-randomized data to fine-tune these representations for individual tasks. These claims appear contradictory. If the assumption is that the pretrained representations are not suitable for the test environment, why not train a policy from scratch?\n\nThe overall framework relies on substantial assumptions about the environment: that factors can be independently identified and modified, and that none of these factors affect the environment’s dynamics. Finally, the environment evaluations are not convincing. It seems that the policies are trained per task with all factor variations and tested on the same variations. It is unclear whether there is any distribution shift between the training and testing environments. I have written my further questions in the later sections. The authors should explain the environment settings, which should include the exact train-test distribution shift, and consider using a prior benchmark such as Factor World [1] and LIBERO[2].\n\nReference:\n\n[1] Xie, Annie, et al. \"Decomposing the generalization gap in imitation learning for visual robotic manipulation.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.\n\n[2] Liu, Bo, et al. \"Libero: Benchmarking knowledge transfer for lifelong robot learning.\" Advances in Neural Information Processing Systems 36 (2023): 44776-44791."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Adapting pre-trained vision encoders to robotic manipulation tasks is an exciting domain. Finding efficient ways to robustify foundation models is an interesting area of research.\n* The paper is mostly easy to follow.\n* There is substantial documentations on the implementation in the Appendix for reproducibility of the paper."}, "weaknesses": {"value": "* The motivation of the paper needs to be made more clear. Currently, I don't see the need to use a pre-trained visual feature for these curated simulation tasks. The authors should consider adding a train-from-scratch baseline.\n* Line 41 has a sentence starting with \"which\" \n* The paper claims that FAME is \"compatible to any other encoders\", but only shows results on DINOv2\n* The assumption on the independence as well as the identifiability of the factors seems to be very strong. Some discussion on real-world applicability needs to be added.\n* The authors claim that their algorithm reduces data complexity to $N \\times K$. However, they still train on the dataset that has a combinatorial number of factors changed. ($D_{multi}$)\n* Related work section does not inlcude any prior works related to to robustify policies(e.g.[1], [2], [3]), which is the focus of the work.\n* Methods are evaluated on only 5 evaluation trajectories, causing high variance.\n* Experiments are conducted in one simulated environment, with each policy trained to solve a single task, even though that the policies have a pre-trained visual backbone.\n* Ablations are not done to ablate different components. It is unclear how much the gating network v.s. the adapters contributed to the performance gain.  \n* Training 2000 epochs on ~100 trajectories seems to be substantially overfitting. More information on train/eval loss can be helpful.\n* Table 1 consist of 2 tables. Please consider merging them for better organization.\n\nReferences:\n\n[1] Akkaya, Ilge, et al. \"Solving rubik's cube with a robot hand.\" arXiv preprint arXiv:1910.07113 (2019).\n\n[2] Tobin, Josh, et al. \"Domain randomization for transferring deep neural networks from simulation to the real world.\" 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2017.\n\n[3] Xie, Annie, et al. \"Decomposing the generalization gap in imitation learning for visual robotic manipulation.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024."}, "questions": {"value": "* In Training Phase 3, why training a separate DP head? this seems to be very wasteful.\n* Is the Mix Gen Dataset data consist of ordered changes in factors? i.e. does i=3 mean that you will change Camera-Pos, Lighting, and floor texture? What about during test time? are the orders of the factors the same between train and test?\n* What are the train/test distribution shifts, if there is any?\n* Are all the baselines trained on all of the data as well?\n* In ablation study, the experiments show that removing $(D_{multi}, i=5)$ causes no performance loss and even reduces the variance. Why does removing data improve performance?\n* How does the gating network perform in zero-shot cross-task generalization when evaluated on task execution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mcl6vvBJb8", "forum": "cdTCrRmV2v", "replyto": "cdTCrRmV2v", "signatures": ["ICLR.cc/2026/Conference/Submission15174/Reviewer_hCPY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15174/Reviewer_hCPY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537206173, "cdate": 1761537206173, "tmdate": 1762925483075, "mdate": 1762925483075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FAME, a three-phase training framework that combines frozen pretrained encoders with factor-specific adapters organized as a Mixture-of-Experts to improve generalization of diffusion policies across environmental variations. Experiments on Meta-World benchmark show improvements over baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally well-written with clear figures illustrating the framework and environmental variations.\nThe paper includes ablation studies, scaling experiments, and detailed per-environment results in the appendix."}, "weaknesses": {"value": "1.The method assumes access to clean, factor-separated datasets D_k where only a single environmental factor varies at a time. This is essentially assuming the solution to the problem.\n\n2.The paper provides no discussion of how one would obtain these factor-separated datasets in real robotic systems. How do you systematically vary only lighting while keeping everything else constant? How do you identify and isolate the \"right\" factors beforehand? \n\n3.All experiments are conducted purely in simulation with artificially created variations. Without any real robot experiments.\n\n4.The approach straightforwardly combines three well-established techniques: pretrained visual encoders (DINOv2), parameter-efficient fine-tuning (adapters), and Mixture-of-Experts. Each component is standard, and their combination for this application represents an incremental engineering contribution rather than a significant methodological or conceptual advance."}, "questions": {"value": "What happens when factors are correlated or when factor decomposition is incorrect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sz1TKqPLrE", "forum": "cdTCrRmV2v", "replyto": "cdTCrRmV2v", "signatures": ["ICLR.cc/2026/Conference/Submission15174/Reviewer_tBKb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15174/Reviewer_tBKb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935877461, "cdate": 1761935877461, "tmdate": 1762925482149, "mdate": 1762925482149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a novel framework FAME by adding factor-aware mixture-of-experts (MoE) structure between a visual encoder and diffusion policy head at the end, with the goal of achieving combinatorial generalization in visual robotic control tasks while maintaining the efficiency. More specifically, FAME goes through a three-staged training process to derive base policy together with the weights of Adaptors and Gating Network. Experiment results on this model shows potential in generalization ability comared to frameworks without MoE."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Though this is not the first case of mixture-of-experts (MoE) being considered in visual robotic control [1][2], the authors' motive to integrate MoE to enhance generalization abilities is insightful.\n\n2. The proposed Gaiting Network working together with a independently-trained experts on specific interference factors is a promising method to counter the generalization problem.\n\n3. The authors were able to prove the advantage of their framework compared to a standard \"Encoder+Diffusion Policy\" one in expriment section, in which ablation studies also give an insight into where FAME possibly benefit from.\n\n[1] Acquiring Diverse Skills using Curriculum Reinforcement Learning with\nMixture of Experts.\n\n[2] Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"}, "weaknesses": {"value": "1. During training, the number of factors is fixed, which makes it challenging to generalize and deploy the model to robots in the open world.\n\n2. The added module in FAME can be seen in Computer Vision as a Neck network, meaning that FAME does the job of further process the extracted feature to ensure consistency and robustness, yet many visual encoders [3,4] have considered that and already possess one. \n\n3. The inclusion of \"ResNet-DP\" and other frameworks in Tables 1 and 2, which differ only in the encoder, seems unnecessary. These comparisons mainly show that DINO-v2 outperforms other encoders under the Diffusion Policy, which, while highlighting the encoder's effectiveness, is not directly relevant to the paper's main contribution.\n\nPlease refer to Questions for more details.\n\n[3] Tang F, Lim S H, Chang N L, et al. A novel feature descriptor invariant to complex brightness changes[C]//CVPR, 2009.\n\n[4] Lin B, Ye Y, Zhu B, et al. Video-LLaVA: Learning United Visual Representation by Alignment Before Projection[C]//EMNLP. 2024."}, "questions": {"value": "1. It is suggested to add the related literature [1–4] to this paper and discuss the similarities and differences between these methods and FAME.\n\n2. Section 4.3 claims the framework gains zero-shot capability after training, but provides no quantitative results. Could the authors provide relevant experimental results?\n\n3. Following Question 2, could the authors clarify the differences in environmental settings between training and testing, given the model's zero-shot capability?\n\n4. Typos: In Figure 8, add labels to both the horizontal and vertical axes; ensure consistent capitalization of \"mixture-of-experts.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ARrpMYNZhN", "forum": "cdTCrRmV2v", "replyto": "cdTCrRmV2v", "signatures": ["ICLR.cc/2026/Conference/Submission15174/Reviewer_1bZo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15174/Reviewer_1bZo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989499529, "cdate": 1761989499529, "tmdate": 1762925481782, "mdate": 1762925481782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}