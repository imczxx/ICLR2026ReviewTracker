{"id": "xFo13SaHQm", "number": 1622, "cdate": 1756898119756, "mdate": 1759898198091, "content": {"title": "WithAnyone: Toward Controllable and ID Consistent Image Generation", "abstract": "Identity-consistent (ID-consistent) generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets—containing multiple images of the same individual—forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation.\nTo address these limitations, we (1) construct a large-scale paired dataset, MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive experiments—both qualitative and quantitative—demonstrate that WithAnyone substantially reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive, controllable generation.", "tldr": "", "keywords": ["AIGC", "ID-Consistent Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2836e9dbbcf8c87c6565e11a2ce9cf3ca99ffa60.pdf", "supplementary_material": "/attachment/a263cd8124590030cf620ce6d5104a919ad6cb90.zip"}, "replies": [{"content": {"summary": {"value": "To address the conflict between identity consistency and controllability in text-to-image generation—where existing models tend to suffer from the \"copy-paste\" issue —the authors construct the large-scale MultiID-2M dataset, which contains 500k group photos with paired references and 1.5 million unpaired group photos. They also design MultiID-Bench, a benchmark for quantifying copy-paste artifacts and the trade-off between ID fidelity and variation. Additionally, the authors propose the WithAnyone model based on the FLUX architecture. By leveraging an ID contrastive loss (with an extended negative sample pool) and a ground-truth-aligned ID loss, combined with a four-stage training pipeline (reconstruction pre-training with fixed prompts, reconstruction pre-training with full captions, paired fine-tuning, and quality tuning), WithAnyone reduces copy-paste artifacts while maintaining high ID similarity. Experiments validate its advantages in pose/expression controllability and perceptual quality."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Solid experiments: A large number of data experiments and qualitative results support the observations presented in this paper.\n2. Promising research direction: Most existing works on ID consistency focus on ID consistency itself, while systematic research on editability-related topics is lacking. This paper conducts in-depth exploration of this idea, proposing corresponding data-driven solutions and relevant benchmarks. I believe the research community should invest more efforts in this area to achieve a better trade-off between similarity and editability, rather than overemphasizing similarity."}, "weaknesses": {"value": "1. Lack of innovation in the experimental design: Most of the methods adopted have been explored by previous studies (e.g., contrastive loss), or only involve minor modifications based on prior work (e.g., GT-Align).\n2. The experimental results are suboptimal: The reduction in similarity is quite significant. For instance, the second ID from the right in the bottom image of Figure 12 does not closely resemble the reference identity."}, "questions": {"value": "1. All cases in this paper are based on celebrity images. What are the results when applying the model to ordinary people or synthetic portraits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qnzuJE3iUL", "forum": "xFo13SaHQm", "replyto": "xFo13SaHQm", "signatures": ["ICLR.cc/2026/Conference/Submission1622/Reviewer_6GUJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1622/Reviewer_6GUJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699256370, "cdate": 1761699256370, "tmdate": 1762915835598, "mdate": 1762915835598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies identity-consistent image generation conditioned on a reference face image and a text prompt. The authors argue that current identity-preserving generators often fall into a \"copy-paste\" failure mode: instead of synthesizing a coherent new face, they directly paste the reference face into the generated scene, leading to visible artifacts in pose, expression, and lighting mismatch.\n\nTo address this, the authors introduce (1) a large-scale paired dataset containing multi-person images together with target face identities, aimed at training models that handle multi-person composition; and (2) a new evaluation benchmark with metrics designed to distinguish between naive face copy-paste and proper identity transfer, by explicitly comparing the generated face to both the provided reference and the intended target identity.\n\nBuilding on these resources, the paper proposes a four-phase finetuning pipeline based on the Flux image generation model, following the PuLID architecture, to improve multi-identity conditioning and reduce copy-paste behavior. Empirically, the finetuned model WithAnyone is reported to perform competitively with SOTA approaches in both single-person and multi-person generation settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes a conceptual step forward by explicitly distinguishing between the reference face and the intended target face, moving toward more controllable identity-consistent person generation.\n\n2. It contributes a large-scale paired dataset with diverse identities and multi-person compositions, which is likely to be valuable for future work on identity-conditioned generation.\n\n3. It proposes new evaluation metrics designed to capture the copy-paste failure mode in identity-consistent generation, offering a more diagnostic way to assess whether a model is truly preserving identity rather than naively pasting faces.\n\n4. The proposed model achieves strong results and is competitive with SOTA methods in both single-person and multi-person generation settings."}, "weaknesses": {"value": "1. *Single-person quality not clearly superior*. In the single-subject setting, the qualitative comparisons do not convincingly demonstrate an advantage of WithAnyone. In particular, the claim that the method avoids copy–paste style artifacts would be more credible if shown under challenging conditions (extreme lighting, large pose changes, strong facial expressions). The revision should include more targeted visual examples in these stress cases.\n\n2. *Underpowered user study*. The user study is based on only 10 raters, which limits statistical reliability and generalizability. A larger participant pool is needed, ideally with demographic diversity and significance reporting.\n\n3. *Limited ablations*. The ablation analysis is incomplete. It would be valuable to (i) report performance after each training phase/stage to show incremental gains, and (ii) include a baseline trained with only the diffusion/denoising loss to isolate the benefit of the proposed objectives.\n\n4. *Unclear ID contrastive loss formulation*. Equation 5 defines the ID Contrastive Loss using features from the reference image rather than the generated (target) image, but the paper does not justify this design choice or discuss alternatives. The rationale and expected effect on identity preservation need to be explained.\n\n5. *Inconsistent table highlighting*. The visual highlighting in Tables 1 and 2 appears misleading. For example, in Table 1(a), Qwen-Image-Edit seems to have the best CP score among non-ground-truth methods, but WithAnyone is marked as “best.” The authors should fix or clarify the ranking/formatting so that highlighted cells actually correspond to the strongest scores."}, "questions": {"value": "How Ground-truth-Aligned ID Loss is computed is still unclear to me. How do authors obtain the generated face image to compute this loss at high noise level? Is it to estimate an approximate $z_0$ or to run the whole denoising process to obtain a clean $z_0$? I would expect a more detailed explanation as the current description in the paper doesn't distinguish WithAnyone with existing methods in obtaining generated images."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sG4YPHwYeb", "forum": "xFo13SaHQm", "replyto": "xFo13SaHQm", "signatures": ["ICLR.cc/2026/Conference/Submission1622/Reviewer_VDLA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1622/Reviewer_VDLA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842420253, "cdate": 1761842420253, "tmdate": 1762915835451, "mdate": 1762915835451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper terms the failure mode of the reconstruction-based diffusion training where it fails to produce diversity as **copy-paste**, such that it focuses too much on fidelity and cannot adapt to human preference and diversity requirement. This paper proposes a well annotated MultiID-2M benchmark to provide diverse references for each identity under multi-person scenarios, addressing balance between identity preservation and generation diversity with a contrastive identity loss (and corresponding metrics to measure performance)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. There are previous benchmark papers which propose datasets of similar functionalities, but MultiID-2M is larger in scale (2M images) and uniquely focuses on group photos with labeled identities, beyond objects, anime characters, and human faces. The rich annotation also enables fine-grained control aspects. Thus, I think the high quality of MultiID-2M is the main strength/contribution of this paper and cannot be substituted by simply assembling a few previous datasets.\n\n2. Using contrastive loss directly on reference images sounds clever to me. There are previous work which adds contrastive loss to diffusion model, e.g. Customcontrast [1] pulls representations of the same subject closer. WithAnyone links target, generated, and reference images together via reconstruction loss + contrastive loss, which repurposes contrastive learning in another way, but I think this paper is among the first papers to leverage contrastive learning here, which sounds interesting.\n\n[1] Chen, Nan, et al. \"Customcontrast: A multilevel contrastive perspective for subject-driven text-to-image customization.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 2. 2025."}, "weaknesses": {"value": "1. The intuition/motivation from the ''copy-paste'' effect has been proposed and studied [2,3], where they also leverage multiple images per identity to resolve overfitting and target leakage that leads to copy-paste effect. Then, it will be better for the authors to compare their datasets and proposed methods to demonstrate the improvements from this work -- I agree MultiID-2M is more comprehensive and unique as I summarized in strength 1, but it will be good to discuss it in the manuscript to better illustrate the motivation and improvement. \n\n2. Following weakness 1, since it is a grounded problem with previous studies, this paper shares some similarities in their approach and evaluation. For example, [2,3] use multiple different images of the same identity as the target image, and [3] also proposes metrics to measure the alignment/balance between fidelity and diversity. This weakens the novelty of this work for the motivation part.\n\n3. I hope there can be more ablation studies to demonstrate what makes this method works better, besides the good MultiID-2M data. A. I see ablation on number of negative examples for InfoNCE, but I think it is more of parameter tuning, can the authors show results w/o contrastive loss at all? B. Compared to previous work, is the capability of learning from MultiID reference images brought by larger models than initially SD 1.4/1.5? I am curious about what contributes the most to make WithAnyone works well on MultiID-2M, whether it is due to contrastive loss or larger model's expressivity. I suggest try baseline minus CL loss, baseline with reduced model size, and baseline minus CL loss and with reduced model size.\n\n[2] Li, Zhen, et al. \"Photomaker: Customizing realistic human photos via stacked id embedding.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024.\n\n[3] Tang, Haoran, et al. \"Retrieving conditions from reference images for diffusion models.\" arXiv preprint arXiv:2312.02521 (2023)."}, "questions": {"value": "Please see Weaknesses for full details. In summary, weakness 1 and 2 need clarifications and updating manuscript to more precisely summarize the motivation and contribution of this paper, and weakness 3 needs additional experiments and explanations to help audience better understand the proposed method, e.g. which components/properties contribute the most. I am happy to raise my rating if my concerns are well addressed, especially for weakness 3, because I think such ablations provide new understanding and shed light on future work, adding strength to this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sXeBG9EK45", "forum": "xFo13SaHQm", "replyto": "xFo13SaHQm", "signatures": ["ICLR.cc/2026/Conference/Submission1622/Reviewer_wdT7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1622/Reviewer_wdT7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886026118, "cdate": 1761886026118, "tmdate": 1762915835322, "mdate": 1762915835322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper makes a strong and timely contribution to controllable, identity-consistent image generation by introducing WithAnyone, a diffusion-based framework trained with a novel contrastive identity loss and a ground-truth–aligned ID loss to mitigate “copy-paste” artifacts—where models overfit to reference faces instead of generalizing identity"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s main strengths lie in its clear problem formulation, methodological thoroughness, and comprehensive evaluation. It identifies and formalizes the “copy-paste” artifact in identity-consistent image generation, offering a concrete metric to quantify this often-overlooked issue. The introduction of the MultiID-2M dataset and MultiID-Bench benchmark represents a valuable infrastructural contribution, enabling more systematic and reproducible research on multi-identity generation. The proposed WithAnyone framework integrates a ground-truth–aligned identity loss and a contrastive loss with extended negatives in a diffusion-based pipeline, leading to measurable improvements in controllability and identity preservation. Extensive experiments, strong ablation studies, and user evaluations convincingly support the claimed benefits, while the paper’s clear presentation, detailed appendix, and ethical considerations reflect a high level of polish and maturity in execution."}, "weaknesses": {"value": "The proposed “copy-paste” failure mode, while intuitively appealing, overlaps conceptually with previously discussed overfitting and reconstruction issues in personalization models such as DreamBooth or PuLID, making the novelty somewhat incremental rather than fundamental.\n\nThe dataset MultiID-2M, though large, is primarily composed of celebrity data scraped from the web, raising concerns about bias, consent, and representativeness; furthermore, the lack of public access to the full dataset severely limits reproducibility and undermines the paper’s claim to openness. Evaluation metrics like the Copy-Paste score depend heavily on specific embedding spaces (e.g., ArcFace), which could bias results toward models aligned with that embedding and may not fully capture perceptual similarity.\n\nThe use of a similarity metric is clearly insufficient to justify identity consistency. Additional face verification metrics are needed, EER, FRM1000 etc."}, "questions": {"value": "Generalization to Non-Celebrity or Low-Quality Data: Since MultiID-2M primarily contains celebrity faces with high-quality, well-lit imagery, how does WithAnyone perform on non-celebrity or in-the-wild datasets with lower resolution, occlusions, or varied lighting conditions? Would the model’s performance degrade significantly outside of this curated domain?\n\nDataset Ethics and Bias: Although the paper addresses ethical data sourcing and the exclusion of minors, could the authors elaborate on how potential biases (e.g., nationality, ethnicity, or gender imbalance) in MultiID-2M might impact the model’s fairness and generalization? Have any bias analyses or mitigation strategies been conducted?\n\nReproducibility and Accessibility: The paper states that only a small subset of MultiID-2M is included in the supplementary material. Are there concrete plans or timelines for releasing the full dataset and benchmark, or alternative means (e.g., data access agreements) to allow other researchers to replicate results?\n\n\nValidity of the Copy-Paste Metric: The proposed Copy-Paste (MCP) score depends on cosine distances in face embedding space. How sensitive is this metric to the choice of embedding model (e.g., ArcFace vs. ElasticFace or CurricularFace etc)? What about face verfic?ation metrics, e.g., FMR10000? Have the authors verified that the metric aligns with perceptual similarity judgments beyond the limited user study?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "While the paper includes an explicit ethics statement and claims that all data were collected from publicly available sources, it heavily relies on large-scale scraping of celebrity face images. This raises several ethical and legal concerns, including potential lack of informed consent from identifiable individuals, even if images are publicly accessible."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GXP6vrKwS2", "forum": "xFo13SaHQm", "replyto": "xFo13SaHQm", "signatures": ["ICLR.cc/2026/Conference/Submission1622/Reviewer_YRvY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1622/Reviewer_YRvY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085517512, "cdate": 1762085517512, "tmdate": 1762915835091, "mdate": 1762915835091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}