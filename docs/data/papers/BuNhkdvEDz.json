{"id": "BuNhkdvEDz", "number": 12178, "cdate": 1758206195694, "mdate": 1759897527126, "content": {"title": "Can LLMs Reliably Evaluate Themselves? A Probabilistic VC Framework", "abstract": "As large language models (LLMs) find increasing use in critical applications, evaluating their ability to assess their own outputs has become crucial. Our work presents a theoretical and empirical framework that examines whether LLMs can differentiate between correct and incorrect solutions while maintaining properly calibrated confidence. We build upon classical Vapnik-Chervonenkis (VC) dimension theory, adapting it to probabilistic predictors through two new complexity metrics: Probabilistic VC (PVC), which measures a model's ability to confidently classify across problem types, and Calibration-aware PVC (C-PVC), which demands alignment between confidence scores and actual success rates. Unlike traditional metrics such as Expected Calibration Error (ECE) and Actual Error (AE), these measures provide an integrated assessment of self-evaluation expressiveness and calibration, yielding sample complexity bounds and generalization guarantees comparable to traditional VC theory. In our study, we tested eleven models (7-8B parameters) across three diverse benchmarks: 360 mathematical reasoning problems, TruthfulQA for factual accuracy, and CommonsenseQA for commonsense reasoning. Each model had to choose between two of its own generated solutions and report its confidence level—a direct test of self-evaluation capability—with ground-truth determined by a larger model ensemble. The experimental results empirically substantiate a systematic inverse correlation: models exhibiting enhanced self-evaluation expressiveness consistently demonstrate diminished calibration fidelity. Models like s1.1-7B and Qwen2.5-7B-Instruct achieve high PVC-VUS scores, indicating strong discriminative self-assessment capacity, while JiuZhang3.0-7B demonstrates superior calibration with the lowest ECE and smallest PVC-VUS gap. Interestingly, we observe domain-specific variations in self-evaluation abilities, with some models performing better on mathematical reasoning tasks while others excel in factual or commonsense domains. Our analysis suggests complex interactions between training methodologies and self-evaluation performance, indicating that multiple factors beyond training approach influence a model's ability to accurately assess its own outputs. The fundamental trade-off between calibration and expressiveness constitutes a persistent phenomenon transcending architectural variations, training paradigms, and cognitive domains, pointing to a fundamental challenge in developing self-reflective LLMs. The framework we've developed offers practical tools for identifying and addressing these limitations, helping create LLMs that can not only tackle complex problems but also recognize when they might be wrong—an essential capability for safe deployment and meaningful self-improvement in autonomous systems.", "tldr": "We propose a calibration-aware probabilistic VC framework to measure LLMs' self-evaluation capacity, assess when they can reliably trust their own answers, and enable targeted self-improvement.", "keywords": ["Self-Evaluation Capacity", "Introspective Reliability", "Uncertainty Calibration", "Probabilistic VC (PVC)", "Calibration-Aware PVC (C-PVC)", "Sample Complexity"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d5df8e09e4772d3599d22b6b82a355f7d263bd49.pdf", "supplementary_material": "/attachment/c40a227ae5058f98c45fd69a482a69a0f31c2cff.zip"}, "replies": [{"content": {"summary": {"value": "The abstract of this paper is excessively lengthy and fails to concisely communicate the key insights of the work. Consequently, the overall presentation does not meet the standards I expect.\n\nThis paper explores the ability of Large Language Models (LLMs) to reliably evaluate their own reasoning, specifically by distinguishing correct solutions from incorrect ones with well-calibrated confidence. To address this, the authors propose two novel theoretical frameworks: PVC and C-PVC."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed theoretical framework is novel, however, its practical applications require further clarification. The paper would benefit from additional details and examples illustrating how PVC and C-PVC can be effectively utilized in real-world scenarios."}, "weaknesses": {"value": "1. The paper lacks a comprehensive discussion of related work and baselines concerning the calibration of LLM reasoning. For practical methods, ASC [1] introduces a self-calibration approach across various benchmarks, including mathematics, counting, and intelligence tests. On the theoretical side, RPC [2] proposes a framework for analyzing LLM reasoning performance from the perspective of confidence estimation. The authors should thoroughly review and discuss these closely related works, either in the technical section or the related work section, to provide a more complete context for their research.\n2. It is confusing why the authors focus on dataset categories in the experiments, which seems to be of limited relevance to practical applications. Therefore, it is important for the authors to clarify how the proposed theoretical framework addresses real-world problems and applications, and how it extends to different LLMs and reasoning paradigms.\n3. The presentation of this paper requires significant improvement. As discussed in the `Summary` section, the abstract is overly long and fails to effectively highlight the main contributions. Moreover, there are typos, such as in Line 210, where a verb is missing after `Select_f(q)`.\n4. As mentioned at the end of the abstract and contribution section, the paper claims to be critical for building reliable autonomous systems. However, I have not seen any related experiments to support this claim. In other words, how does this paper substantiate its importance for building reliable autonomous systems?\n\n**Reference**\n\n[1] Efficient Test-Time Scaling via Self-Calibration. Arxiv 2025.\n\n[2] A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning. NeurIPS 2025."}, "questions": {"value": "Please refer to the `Weaknesses` section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bqmX6Bk0xC", "forum": "BuNhkdvEDz", "replyto": "BuNhkdvEDz", "signatures": ["ICLR.cc/2026/Conference/Submission12178/Reviewer_9qjG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12178/Reviewer_9qjG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657761859, "cdate": 1761657761859, "tmdate": 1762923128259, "mdate": 1762923128259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces new complexity metrics based on Vapnik-Chervonenkis (VC) dimension theory suitable for Large Language Models (LLMs), which encode the ability of LLMs to classify its own answers correctly above a given confidence, and to be calibrated when doing so. Moreover,  the work establishes connections to existing metrics and a generalisation result based on the newly introduced ones, and empirically estimates there metrics for several LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- originality: although I am not closely familiar with VC theory and statistical learning theory, it seems to me that the introduced concepts are novel and insightful\n- quality: extensive experimental setup, connection with other existing metrics, and large body of additional results in appendix\n- significance: being able to bound generalisation for LLMs’ ability to assess their own answers is important; therefore, the aim of the paper is commendable"}, "weaknesses": {"value": "What mostly affects my scores negatively is the lack of clarity in some aspects. While I understand that this is a complex area of mathematics and I don’t possess full familiarity with the field (thus, my comments may be obvious to people more familiar with it), I found quite hard to understand some of the theoretical parts of the paper as well as their connection with the empirical part: \n\n- line 101: “with high probability”: it may be worth specifying what this means. Is the probability over the sample from a given distribution?\n- the meaning of the “probability” term $\\mathbb P$: in line 146, it seems that the probability is what $f$ assigns while, in line 159, it comes from the data distribution.\n- the definition in line 159 conditions on $f(x)$ assigning confidence $p$, but $p$ is a continuous value. Are we not conditioning on an event with probability 0? How is this treated?\n- I don’t understand $\\hat p$ in Definition 3: why is this additional quantity needed? How is it linked to $f$? Shouldn’t the C-PVC dimension also depend on the way in which $\\hat p$ is obtained from $f$?\n- I don’t get how the VUS quantities are useful, as it seems they do not appear in the generalisation result\n- I also don’t understand how the PVC and C-PVC are estimated from the datasets: how is the estimation procedure described in Sec 3.3 linked to the actual definitions? Those dimension metrics are defined as a max over set sizes for which there exists at least one set satisfying some property, and this seems hard to estimate in practice as, naively, we’d need to check all possible sets of a given size?\n- Putting aside my confusion regarding the estimation of PVC and C-PVC, I am not sure what the experimental setup is aiming to do: is it attempting to validate the theoretical bounds between the PVC and C-PVC dimensions and generalisation? Or is it exploring for unrelated trends between those quantities and ECE/AE? If so, why is this interesting? I am asking as it seems to me that the PVC and C-PVC are not interesting by themselves, but rather they are only interesting as they appear in the generalisation bounds.\n\nMinor comments:\n\n- it seems to me that the abstract is longer than the traditional length for AI conferences; a shorter version may be more suitable to quickly let readers understand the goal and scope of the paper.\n- line 116 uses $\\epsilon$, which was already used in Sec 2.1 with an apparently different meaning. Moreovoer, line 117 includes $\\epsilon_t$ which was not introduced\n- Line 205-206: “External judge LLMs determine the objectively superior solution” this seems debatable. Why this reliance on a judge and not on checks to automatically determine solution correctness?"}, "questions": {"value": "- I am intrigued by the lack of anything similar to the PVC dimension before this paper: while I see this is needed for LLMs, LLMs are not special in producing probability distributions. Do the authors have any idea for this lack? Or, is there a 1-1 connection with the fat-shattering dimension? If so, was this obvious with other ML models, so that people did not feel the need to introduce something similar to the PVC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pUqtCyvfWv", "forum": "BuNhkdvEDz", "replyto": "BuNhkdvEDz", "signatures": ["ICLR.cc/2026/Conference/Submission12178/Reviewer_CrB3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12178/Reviewer_CrB3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737516129, "cdate": 1761737516129, "tmdate": 1762923127882, "mdate": 1762923127882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank Reviewers **BYpE**, **qaEw**, **CrB3**, and **9qjG** for their thoughtful and constructive feedback. We are genuinely encouraged that the reviewers recognized the core contributions of our work - including the novelty of the PVC and C-PVC framework, the importance of studying LLM self-evaluation, and the breadth of our empirical analysis. At the same time, we fully acknowledge that many of the concerns raised relate mainly to clarity, exposition, and additional explanation, **rather than fundamental issues with the theoretical or empirical foundations**.\n\nWith this in mind, we are approaching the rebuttal with genuine sincerity and a strong sense of responsibility. We recognize that some of the ideas in the initial manuscript could have been communicated more clearly, and we are working to refine these explanations so that the contributions of the work are easier to understand and evaluate. We believe that once these improvements are clearly reflected, the significance of the work will be more evident, and we reasonably expect that the evaluation could improve by several steps, based on our trust in the reviewers’ fairness and expertise.\n\nOur goal in the rebuttal is not only to address each comment, but to do so in a way that meaningfully strengthens the work. We are fully committed to providing careful, honest, and constructive responses to every point raised by the reviewers. We will soon provide an updated manuscript and detailed, reviewer-specific replies, and we sincerely hope that the effort we are putting into improving the clarity and presentation of the work will be taken into consideration during re-evaluation. We are truly grateful for the reviewers’ time, attention, and thoughtful engagement."}}, "id": "e1GMK2yUBE", "forum": "BuNhkdvEDz", "replyto": "BuNhkdvEDz", "signatures": ["ICLR.cc/2026/Conference/Submission12178/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12178/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12178/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763172447318, "cdate": 1763172447318, "tmdate": 1763172447318, "mdate": 1763172447318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank Reviewers **BYpE**, **qaEw**, **CrB3**, and **9qjG** for their thoughtful and constructive feedback. We are genuinely encouraged that the reviewers recognized the core contributions of our work - including the novelty of the PVC and C-PVC framework, the importance of studying LLM self-evaluation, and the breadth of our empirical analysis. At the same time, we fully acknowledge that many of the concerns raised relate mainly to clarity, exposition, and additional explanation, **rather than fundamental issues with the theoretical or empirical foundations**.\n\nWith this in mind, we are approaching the rebuttal with genuine sincerity and a strong sense of responsibility. We recognize that some of the ideas in the initial manuscript could have been communicated more clearly, and we are working to refine these explanations so that the contributions of the work are easier to understand and evaluate. We believe that once these improvements are clearly reflected, the significance of the work will be more evident, and we reasonably expect that the evaluation could improve by several steps, based on our trust in the reviewers’ fairness and expertise.\n\nOur goal in the rebuttal is not only to address each comment, but to do so in a way that meaningfully strengthens the work. We are fully committed to providing careful and constructive responses to every point raised by the reviewers. We will soon provide an updated manuscript and detailed, reviewer-specific replies, and we sincerely hope that the effort we are putting into improving the clarity and presentation of the work will be taken into consideration during re-evaluation. We are truly grateful for the reviewers’ time, attention, and thoughtful engagement."}}, "id": "e1GMK2yUBE", "forum": "BuNhkdvEDz", "replyto": "BuNhkdvEDz", "signatures": ["ICLR.cc/2026/Conference/Submission12178/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12178/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12178/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763172447318, "cdate": 1763172447318, "tmdate": 1763325187963, "mdate": 1763325187963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether large language models (LLMs) can reliably evaluate their own outputs through a novel theoretical and empirical framework grounded in statistical learning theory. The authors propose two new measures: Probabilistic VC (PVC) and Calibration-aware PVC (C-PVC) to quantify a model’s self-evaluation expressiveness and calibration reliability. They provide probabilistic generalization bounds and validate the framework across 11 open-source 7–8B models on mathematical, factual, and commonsense reasoning benchmarks. The experiments reveal a trade-off between self-evaluation expressiveness and calibration quality, suggesting that stronger introspection often comes at the cost of confidence misalignment. Overall, the paper contributes an original theoretical perspective on model self-evaluation, offering tools for understanding and improving introspective reliability in future LLMs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents an innovative and theoretically grounded framework for assessing the self-evaluation reliability of large language models (LLMs). The introduction of Probabilistic VC (PVC) and Calibration-aware PVC (C-PVC) extends classical VC theory to probabilistic predictors, offering a rigorous approach to quantify self-assessment expressiveness and calibration simultaneously. The combination of formal theoretical derivations, generalization bounds, and extensive empirical validation across 11 models and three reasoning domains makes this study both ambitious and timely. The motivation—understanding when models “know they are wrong”—aligns closely with the broader agenda of trustworthy and introspective AI systems."}, "weaknesses": {"value": "1. The PVC and C-PVC definitions are conceptually appealing but insufficiently formalized; key probabilistic assumptions and measurable function spaces are not clearly defined.\n\n2. The C-PVC bound lacks a complete derivation or closed-form upper bound, relying on intuition rather than proof.\n\n3. The VUS (Volume Under Surface) metric is introduced without mathematical rigor—its integration domain and theoretical justification remain vague.\n\n4. There is no statistical significance testing or confidence interval reporting, making it difficult to judge robustness.\n\n5. The ground-truth evaluation relies solely on other LLMs (Claude, Nova, DeepSeek-R1) instead of human annotation, which weakens objectivity.\n\n6. Prompt formats and sampling parameters differ across models, introducing uncontrolled variance.\n\n7. Only 7–8B scale models are tested, limiting claims about scalability or generalization trends.\n\n8. Ablation and scaling analyses are missing.\n\n9. The paper contains a small typo (“sysem card” instead of “system card”)."}, "questions": {"value": "How does the proposed PVC metric behave under stochastic sampling noise—does it remain stable across decoding seeds?\n\nCould the authors clarify whether PVC ≥ C-PVC is theoretically guaranteed or merely empirically observed?\n\nWhat is the computational cost of estimating VUS and calibration surfaces, and how does it scale with model size or dataset size?\n\nHave the authors tested the sensitivity of self-evaluation accuracy to prompt templates or confidence calibration methods?\n\nCan the PVC framework be extended to multi-turn reasoning or tool-augmented LLMs where the “self” boundary is ambiguous?\n\nIs there a plan to validate the framework with human-judged correctness or cross-model consensus instead of single-model baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "snGG9EyWNF", "forum": "BuNhkdvEDz", "replyto": "BuNhkdvEDz", "signatures": ["ICLR.cc/2026/Conference/Submission12178/Reviewer_qaEw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12178/Reviewer_qaEw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836234864, "cdate": 1761836234864, "tmdate": 1762923127240, "mdate": 1762923127240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies whether LLMs can reliably evaluate their own outputs by proposing a theoretical framework based on statistical learning theory. The authors proposes the Probabilistic VC (PVC) and Calibration-aware PVC (C-PVC) dimensions to jointly quantify a model’s discriminative self-assessment ability and calibration fidelity. They derive generalization and sample-complexity bounds analogous to classical VC theory. Empirically, the study tests several 7/8B model, where each model must select between two of its own answers and rate its confidence. Authors show a consistent trade-off in results where models with higher discriminative expressiveness tend to be less calibrated."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces the Probabilistic VC (PVC) and Calibration-aware PVC (C-PVC) dimensions\n- Connects discriminative self-assessment ability and calibration quality, with provable generalization and sample-complexity bounds\n- Evaluates 11 open-source 7–8B models"}, "weaknesses": {"value": "Model choice of s1.1-7B seems questionable. s1K-1.1 is essentially a small and difficult SFT dataset targeting math reasoning, which is very effective for training large models (32B or larger) but causes significant performance degrade on small models without careful tuning and processing. Authors of s1 have also suggested using s1.1-32B instead of the 7B version (https://huggingface.co/simplescaling/s1.1-7B). The model's output traces could be excessively long and its accuracy (avg@k) could be low on benchmarks. \n- Besides the metrics presented in this paper, can the authors also present actual benchmark performance of each model? If the accuracy is too low, I don't think the metrics in the paper are enough to make the conclusion.\n\nBenchmark choice is another problem. Math-360 seems to be a dataset constructed by the authors, and reading from Table 7, the questions seem very simple and synthetic. More standard benchmarks such as MATH, minerva math and AIME should be included. For the other two selected benchmarks (TruthfulQA and CommonsenseQA), it's also unclear how authors processed these data just by reading the sentence \"we grouped each of the latter two datasets into 10 broad categories and sampled 240 questions per benchmark\". More standard and transparent evaluation protocol is necessary here."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kVLgaWcqRi", "forum": "BuNhkdvEDz", "replyto": "BuNhkdvEDz", "signatures": ["ICLR.cc/2026/Conference/Submission12178/Reviewer_BYpE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12178/Reviewer_BYpE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762416141932, "cdate": 1762416141932, "tmdate": 1762923126853, "mdate": 1762923126853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}