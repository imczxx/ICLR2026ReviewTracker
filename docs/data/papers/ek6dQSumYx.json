{"id": "ek6dQSumYx", "number": 2370, "cdate": 1757065053268, "mdate": 1759898152737, "content": {"title": "The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM", "abstract": "Neural network pruning is a promising technique to mitigate the excessive computational and memory requirements of large language models (LLMs).\nDespite its promise, however, progress in this area has diminished, as conventional methods are seemingly unable to surpass moderate sparsity levels (50-60\\%) without severely degrading model accuracy.\nThis work breaks through the current impasse, presenting a principled and effective method called $ \\text{Elsa}$, which achieves extreme sparsity levels of up to 90\\% while retaining high model fidelity.\nThis is done by identifying several limitations in current practice, all of which can be traced back to their reliance on a surrogate objective formulation.\n$ \\text{Elsa}$ tackles this issue directly and effectively via standard and well-established constrained optimization techniques based on ADMM.\nOur extensive experiments across a wide range of models and scales show that $ \\text{Elsa}$ achieves substantial improvements over existing methods;\ne.g., it achieves 7.8$ \\times$ less perplexity than the best existing method on LLaMA-2-7B at 90\\% sparsity.\nFurthermore, we present $ \\text{Elsa}_ {-L}$, a quantized variant that scales to extremely large models (27B), and establish its theoretical convergence guarantees.\nThese results highlight meaningful progress in advancing the frontier of LLM sparsity, while promising that significant opportunities for further advancement may remain in directions that have so far attracted limited exploration.", "tldr": "We achieve extreme LLM sparsity via surrogate-free constrained optimization.", "keywords": ["sparsity", "pruning", "LLMs"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f25c3434df7ce76583aa4e4dccca8e030fdb420.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the \"sparsity wall\" (50--60\\%) in large language model (LLM) pruning by introducing ELSA (Extreme LLM Sparsity via Surrogate-Free ADMM). ELSA formulates pruning as minimizing the true task loss $f(x)$ under an $\\ell_0$ sparsity constraint and solves it via ADMM, which alternates between weight optimization and projection onto the sparse set. Instead of layer-wise reconstruction surrogates, it performs an objective-aware projection using diagonal Fisher/Adam second-moment statistics to guide weighted top-$k$ selection. Experiments on OPT, Gemma-2, and LLaMA-2 models show stable perplexity up to 90\\% sparsity and consistent gains over SparseGPT, Wanda, ALPS, L-ADMM, and SAFE. The paper also mentions a quantized extension (ELSA-L) for 27B-scale models with theoretical guarantees, though detailed results appear later in the text."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The curvature-weighted projection is simple yet effective, using readily available second-moment statistics to guide pruning decisions.\n\n2. Empirical results are strong and consistent, showing stable perplexity up to 90% sparsity across diverse LLM architectures.\n\n3. The paper is well written and conceptually cohesive. The additional theories round up a solid work."}, "weaknesses": {"value": "1. The experiments convincingly show perplexity improvements, but omit practical metrics such as wall-clock time, memory footprint, and actual inference acceleration. In reality, pruning methods—whether extremely local (e.g., SparseGPT, Wanda) or global (e.g., this work, SparseLLM and etc)—ultimately represent different points on a performance–cost tradeoff curve. With sufficient GPU resources, even global pruning becomes computationally feasible, reducing the motivation for additional algorithmic complexity like ADMM. It would strengthen the paper to explicitly highlight the practical scenarios or deployment constraints where ELSA provides tangible benefits over simpler baselines.\n\n2. The evaluation focuses solely on perplexity and task accuracy, without measuring actual inference acceleration, FLOPs reduction, or memory throughput gains after pruning. This limits the practical impact, as sparsity alone does not guarantee real-world speed-ups. \n\n3. While the paper frames pruning as a constrained optimization problem and leverages ADMM elegantly, both the global-pruning perspective and ADMM-style decomposition have precedents (e.g., SparseLLM, L-ADMM, SAFE). The novelty lies mainly in unifying these ideas under a “surrogate-free” formulation rather than introducing a fundamentally new optimization principle.\n\n4. [minor] Despite claiming global sparsity, the projection step is performed per-tensor using diagonal curvature estimates, so cross-layer dependencies are not modeled. The approach therefore remains an efficient approximation to the ideal global objective rather than a full solution."}, "questions": {"value": "1. How does ELSA translate its high sparsity into real inference acceleration or memory savings on modern hardware?\n\n2. Can you quantify the additional compute cost (training or calibration time) introduced by ADMM compared to simpler methods like SparseGPT?\n\n3. How robust is the whole approach to the hyper-parameters? ADMM-based approaches performance could be quite sensitive to hyper-parameters and hard to tune in practice, especially for large-scale LLM pruning problem IMHO."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ot1uTRUNJ0", "forum": "ek6dQSumYx", "replyto": "ek6dQSumYx", "signatures": ["ICLR.cc/2026/Conference/Submission2370/Reviewer_oqwk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2370/Reviewer_oqwk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864768750, "cdate": 1761864768750, "tmdate": 1762916210733, "mdate": 1762916210733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ELSA, a surrogate-free ADMM-based method for pruning large language models (LLMs) to extreme sparsity levels (up to 90\\%) while preserving performance. It critiques existing layer-wise reconstruction approaches for their limitations and compounding errors. ELSA directly optimizes the sparsity-constrained objective, achieving significantly lower perplexity (e.g., 7.8× better on LLaMA-2-7B at 90\\% sparsity) and higher zero-shot accuracy across models like OPT, Gemma, and LLaMA."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper has a reasonable structure and is clearly written. The authors explain their method in detail and make substantial theoretical contributions.\n2. The authors conducted extensive experiments across various model architectures and scales, evaluating metrics including perplexity and zero-shot task performance (e.g., ARC, BoolQ) and performing fair comparisons with state-of-the-art methods (e.g., SparseGPT, ALPS, SAFE). The results show that ELSA exhibits significant performance advantages at high sparsity levels (80-90\\%)."}, "weaknesses": {"value": "1. Both ALPS and L-ADMM have also proposed ADMM-based pruning algorithms for LLMs. The authors should clearly elaborate on the differences between ELSA and ALPS to highlight the unique contributions of ELSA.\n2. At low sparsity rates (50% and 60%), the accuracy of ELSA is significantly lower than that of the optimal L-ADMM baseline (Tables 7 and 8).\n3. Although ELSA has advanced the performance of LLMs at high sparsity rates (70%-90%), there remains a substantial performance gap between ELSA and dense LLMs.\n4. Pruning a 7B-parameter model requires 4 A100 GPUs, while pruning 13B and 27B-parameter models requires 4 H200 GPUs. Compared with Wanda and SparseGPT, this constitutes a much higher computational overhead—since pruning a 27B-parameter model using the above two methods only requires at most 1 A100 GPU."}, "questions": {"value": "1. The authors used such a large number of GPUs to prune LLMs, so why not perform LoRA fine-tuning on the pruned model to obtain a better pruned model? It is suggested that the authors compare the accuracy, computational resources used, and time overhead for obtaining sparse LLMs between ELSA and the \"Wanda + LoRA\" approach.\n2. Can the LLM sparsification technique used in this paper improve the model's inference speed? What advantages does it have compared with quantization techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BG6uMTS9jg", "forum": "ek6dQSumYx", "replyto": "ek6dQSumYx", "signatures": ["ICLR.cc/2026/Conference/Submission2370/Reviewer_KMnE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2370/Reviewer_KMnE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881219404, "cdate": 1761881219404, "tmdate": 1762916210606, "mdate": 1762916210606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ELSA (Extreme LLM Sparsity via Surrogate-free ADMM), a method to prune large language models to extreme sparsity levels (up to 90\\%) while preserving performance. It identifies limitations in existing pruning techniques, which rely on surrogate objectives like layer-wise reconstruction error minimization, leading to performance collapse beyond 50-60\\% sparsity due to compounding errors and suboptimality. ELSA directly optimizes a sparsity-constrained problem using ADMM, incorporating objective-aware projections and avoiding surrogates. A quantized variant, ELSA-L, scales to 27B-parameter models with reduced memory. Experiments across models (OPT, Gemma, LLaMA) show ELSA achieves 5-30× lower perplexity and up to 6\\% higher zero-shot accuracy at 90\\% sparsity compared to baselines. Theoretical convergence is proven, highlighting potential for further LLM efficiency advancements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper is well-written and highly readable.\n\n2.\tAt high sparsity rates, ELSA achieves significantly better accuracy than other baselines and successfully breaks through the performance limit of LLM sparsification.\n\n3.\tThe experiments cover models of various parameter scales and different benchmarks, and conduct comprehensive evaluations of the LLMs' performance, including perplexity and zero-shot accuracy.\n\n4.\tThe theoretical foundation is solid: convergence proofs for ELSA and ELSA-L are provided, based on standard assumptions (such as weak convexity and smoothness). This enhances the reliability of the method and aligns with the empirical results."}, "weaknesses": {"value": "1.\tELSA's accuracy at low sparsity rates (50% and 60%) is lower than that of the baselines.\n\n2.\tIt remains unclear how ELSA performs on larger models. Although the parameter scales of the models tested in the experiments range from 125 million to 27 billion, there is a lack of experimental results on even larger models, such as Llama-3-80B."}, "questions": {"value": "1.\tWhat is the computational efficiency of ELSA? How much time does it take to prune LLMs with different parameter scales?\n\n2.\tCan ELSA be used to prune Mixture-of-Experts (MoE) models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VfksOUPaMo", "forum": "ek6dQSumYx", "replyto": "ek6dQSumYx", "signatures": ["ICLR.cc/2026/Conference/Submission2370/Reviewer_v5xh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2370/Reviewer_v5xh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912604016, "cdate": 1761912604016, "tmdate": 1762916210447, "mdate": 1762916210447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}