{"id": "QJYmhiRKU3", "number": 4944, "cdate": 1757816668096, "mdate": 1759898003655, "content": {"title": "Mitigating Data Heterogeneity Effect in Client-Reshuffling-Based Federated Learning", "abstract": "Data heterogeneity and low client participation have been the key challenges in federated learning. \nClient-reshuffling-based federated learning methods were recently introduced to improve the client participation efficiency. However, the client-reshuffling-based methods still suffer from the data heterogeneity issue. To fill in this gap, we propose a new algorithm, FedCDR, to mitigate the data heterogeneity challenge in client-reshuffling-based federated learning. Our algorithm achieves the state-of-the-art $O(\\epsilon^{-2})$ convergence rate for finding an $\\epsilon$-approximate stationary point under standard assumptions. Unlike previous works, our method achieves convergence \\textbf{independent} of the degree of data heterogeneity, \\emph{i.e.} our algorithm converges fast in highly heterogeneous data environments, whereas previous methods suffer from non-convergence or slow convergence rates. Moreover, our algorithm uses inexact local solvers, which are essential for practical\nimplementation and requirements. In our theoretical analysis, client-reshuffling-based approaches introduce a new technical challenge: non-i.i.d. sampling bias, which complicates the convergence analysis. We design a novel potential function and adopt advanced analytical techniques to address this challenge. Our experimental results demonstrate the advantages of our method over existing algorithms on both synthetic and benchmark datasets.", "tldr": "", "keywords": ["Federated Learning", "Data Heterogeneity", "Client Reshuffling"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3192290c0b588e2bc2e23a96c4b9437f4225e11.pdf", "supplementary_material": "/attachment/4cd5d8d1b2bad36ea59686e4dc577d2184c89202.pdf"}, "replies": [{"content": {"summary": {"value": "This paper addresses the data heterogeneity challenge in client-reshuffling-based federated learning (FL). The authors propose FedCDR (Federated Client Reshuffling Douglas-Rachford Method), which combines client reshuffling with Douglas-Rachford splitting to handle non-IID data distributions. The key contribution is achieving an O(ε⁻²) convergence rate that is independent of data heterogeneity, unlike previous methods whose convergence degrades with increasing heterogeneity. The paper provides theoretical analysis for both exact and inexact variants of the algorithm and validates the approach on synthetic and benchmark datasets (MNIST, CIFAR-10, CIFAR-100), demonstrating superior performance compared to client-reshuffling variants of FedAvg, FedProx, SCAFFOLD, and FedDC."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality:**\n\n1. **Novel problem formulation**: The paper identifies and addresses a significant gap in the literature—mitigating data heterogeneity in client-reshuffling-based FL. While client reshuffling has been studied for improving participation efficiency, its interaction with data heterogeneity has not been adequately addressed.\n2. **Theoretical innovation**: The convergence analysis introduces novel techniques to handle non-i.i.d. sampling bias inherent in client reshuffling. The use of conditional expectations over entire meta-epochs and the construction of a new potential function represent meaningful technical contributions.\n3. **Heterogeneity-independent convergence**: Unlike prior work where convergence rates depend on heterogeneity measures, this method achieves convergence independent of data heterogeneity degree, which is a significant theoretical advancement.\n\n**Quality:**\n\n1. **Rigorous theoretical analysis**: The paper provides detailed convergence proofs for both exact (Theorem 2) and inexact (Theorem 1) variants. The analysis properly accounts for the technical challenges introduced by client reshuffling, including non-i.i.d. sampling across communication rounds within meta-epochs.\n2. **Comprehensive experimental validation**: The experiments span multiple datasets with varying heterogeneity levels (controlled by different parameters in synthetic data and Dirichlet distribution parameter α in benchmark datasets), demonstrating consistent improvements over strong baselines."}, "weaknesses": {"value": "**Technical Issues:**\n\n1. **Hyperparameter sensitivity**: The convergence results depend on choosing η < η₀ (Equation 59) or η < η₁ (Equation 99), but these bounds are not explicitly stated in the main text. The paper should discuss how restrictive these conditions are and provide guidance on practical hyperparameter selection. Corollaries 1 and 2 give specific choices, but sensitivity analysis is missing.\n2. **Limited theoretical comparison**: While the paper claims O(ε⁻²) is \"state-of-the-art,\" there is insufficient comparison with the convergence rates of prior client-reshuffling methods (Malinovsky et al., 2023a; Demidovich et al., 2024). A table comparing convergence rates, assumptions, and dependence on problem parameters would strengthen the contribution.\n\n**Clarity Issues:**\n\n1. **Notation complexity**: The paper uses heavy notation with multiple subscripts and superscripts. A notation table would improve readability. Additionally, the relationship between meta-epochs t, communication rounds r, and local iterations is not immediately clear from Algorithm 1.\n2. **Missing algorithmic details**: Algorithm 1 states \"x^t_{i,r+1} ≈ prox_{ηf_i}(y^t_{i,r+1})\" but does not specify the stopping criterion or how the approximation quality is measured. The experimental section mentions using SGD for 10 epochs, but the connection to the theoretical accuracy requirement is unclear.\n\n**Significance Issues:**\n\n1. **Comparison fairness**: The baselines (CR+FedAvg, CR+FedProx, etc.) are client-reshuffling variants, but it's unclear if these are the authors' implementations or if they follow specific prior work. The paper should clarify whether these baselines have been previously studied or are novel adaptations.\n2. **Practical considerations underexplored**: The paper does not discuss important practical aspects such as stragglers, communication failures, or privacy considerations. The assumption that all clients in a batch participate perfectly may be unrealistic."}, "questions": {"value": "1. **Proof technique**: Can you provide more intuition about the potential function design in your convergence analysis? How does it differ from potential functions used in analyzing standard FL methods, and why is it necessary for the client-reshuffling setting?\n2. **Experimental setup clarification**: In Section 6.1, you mention selecting \"10% of clients to participate in each communication round.\" Does this mean C = 0.1n in Algorithm 1? If so, how does this align with client reshuffling, which should eventually use all n clients across R rounds in a meta-epoch?\n3. **Convergence rate comparison**: Can you provide a detailed comparison table showing the convergence rates, assumptions, and heterogeneity dependence of your method versus prior client-reshuffling methods (Malinovsky et al., 2023a; Demidovich et al., 2024)?\n4. **Scalability**: Have you tested or do you have insights on how FedCDR scales to larger federated learning scenarios (e.g., 10,000+ clients, larger models like Vision Transformers)?\n5. **Communication-computation trade-off**: In Table 3, the communication speedup is less than 1× for local epoch = 3 and 5, meaning more communication rounds are needed. Can you explain when practitioners should prefer fewer local epochs despite higher communication costs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e4BSQRbjy7", "forum": "QJYmhiRKU3", "replyto": "QJYmhiRKU3", "signatures": ["ICLR.cc/2026/Conference/Submission4944/Reviewer_bxfU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4944/Reviewer_bxfU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761287668771, "cdate": 1761287668771, "tmdate": 1762917785080, "mdate": 1762917785080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is mainly a theoretical paper that proposes algorithm FedCFR that achieves faster convergence ($O(\\epsilon^{-2})$) even with the non-iidness data heterogeneity effect in federated learning with client reshuffling. The work shows that the algorithm has the corresponding convergence rates for finding an $\\epsilon$-approximate stationary point. The main takeaway is using the Douglas-Rachford splitting technique in federated learning to improve the convergence rate. Although the main contributions of this paper is theoretical, it includes empirical results on benchmark datasets showing improved convergence over existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provide insight into how client reshuffling can be utilized to mitigate the effect of data-heterogeneity of slowing down the convergence. \n- The authors provide a rigorous convergence analysis for the proposed FedCDR to show its improved convergence rate with the presence of data heterogeneity through in-exact local solvers and standard assumptions used for FL convergence analysis. \n- The paper is clear and easy to follow."}, "weaknesses": {"value": "- The practical usage for FedCDR seems difficult due to the difficulty in parameter selection. There seems to be a lot of different combination of parameters that have to be set in order to satisfy the fast convergence rate condition, and the search according to the ablation studies seem like a costly process. The connection between the theory and practice is not really clear due to the complexity of the algorithm and conditions for fast convergence.\n\n - The novelty of the work is rather limited other than the solving technique of the convergence rate since it applies the DR technique to FL, and DR has been proposed before to be used in the context of FL as well as cited in the paper (Themelis 2020, Tran-Dinh 2021)\n\n- This is more minor, but the work does not include the relevant literature for this work as much as needed I believed. For instance, other work that has looked in to client shuffling in FL with similar insights are not cited such as:\n\n  - Cho et al., On the Convergence of Federated Averaging with Cyclic Client Participation, ICML, 2023\n\n  - Wang et al., A Unified Analysis of Federated Learning with Arbitrary Client Participation, NeurIPS, 2022\n\n  - Samuel Horváth et al., FedShuffle: Recipes for Better Use of Local Work in Federated Learning, TMLR 2024"}, "questions": {"value": "Please address the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WRctqxiLkb", "forum": "QJYmhiRKU3", "replyto": "QJYmhiRKU3", "signatures": ["ICLR.cc/2026/Conference/Submission4944/Reviewer_HbWk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4944/Reviewer_HbWk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607085230, "cdate": 1761607085230, "tmdate": 1762917784727, "mdate": 1762917784727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a new algorithm in client-reshuffling-based federated learning, aiming at mitigating data heterogeneity . While client reshuffling improves fairness and participation efficiency, existing reshuffling-based methods still suffer from slow or unstable convergence when data across clients is highly non-i.i.d. The authors propose FedCDR, a federated optimization method that integrates Douglas–Rachford splitting with client reshuffling, and supports inexact proximal local solvers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The analysis shows that the convergence bound is heterogeneity-independent, which is notably stronger than prior work where error grows with heterogeneity.\n\n2. Allowing inexact proximal operators improves the real-world usability of the method"}, "weaknesses": {"value": "1. One of the claimed contriution is the Our method \"the best-known $O(\\epsilon^{-2})$\" communication complexity for finding an $\\epsilon$-approximate stationary point under standard assumptions in nonconvex FL. However, as I know, most of the federated learning algorithm can achieve this communication complexity. More importantly, many algorithms can achieve the speedup in terms of the number of clients. A comparion table would be greatly appreciated.\n\n2. Another contribution is the convergence rate is independent of data heterogeneity. But it is unclear to me if the proposed algorithm uses SGD or GD or other methods within meta-epoch t. As a result, I wonder if any assumptions about the stochastic gradient are needed. It is not common without any assumptions because in standard analysis, we at least need bounded variance assumption and one definition to bound the data heterogeneity. In this case, I assume the algorithm use full gradient within the meta-epoch?\n\n3. The convergence explicitly depends on the accuracy of the inexact variant, which should be very small ($\\sim 1/(TR)^2$ from Remark 1).\n\n4. It seems a simple combination of Douglas-Rachford splitting (e.g., FedDR)) method and client-reshuffling-based FL setting."}, "questions": {"value": "1. One concern is the limited comparison to non-DR methods designed specifically for heterogeneity. Some of them are widely used in practice and strong under heterogeneity.\n\n2. See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hjjcw4wfob", "forum": "QJYmhiRKU3", "replyto": "QJYmhiRKU3", "signatures": ["ICLR.cc/2026/Conference/Submission4944/Reviewer_ShC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4944/Reviewer_ShC1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766127482, "cdate": 1761766127482, "tmdate": 1762917784302, "mdate": 1762917784302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FedCDR, a novel federated learning algorithm that addresses the persistent challenge of data heterogeneity in client-reshuffling-based federated learning.\nThe authors integrate the Douglas–Rachford (DR) splitting method with client reshuffling and propose both inexact and exact variants of FedCDR. The key theoretical result is that FedCDR achieves the optimal convergence rate of $O(\\epsilon^{-2})$, which is independent of the degree of data heterogeneity. \nExperiments on synthetic data and three vision benchmarks (MNIST, CIFAR-10/100) show consistent accuracy gains over reshuffled versions of FedAvg, FedProx, SCAFFOLD, and FedDC, and an ablation confirms that reshuffling accelerates DR-style methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The convergence proof is comprehensive, with detailed treatment of sampling bias due to reshuffling, which is a technically challenging departure from i.i.d. assumptions.\n2. Introduction of a new potential function and use of conditional expectations over meta-epochs is a creative analytical approach to tackle the bias introduced by reshuffling.\n3. The convergence rate is independent of data heterogeneity, which is a clear step beyond existing reshuffling-based FL algorithms."}, "weaknesses": {"value": "1. The theoretical analysis is conducted under a deterministic setting, which is uncommon in practical federated learning scenarios. Extending the analysis to a stochastic setting would enhance its applicability and relevance to real-world deployments.\n2. Both the theoretical analysis and experimental setup implicitly assume homogeneous local computation budgets and participation probabilities. However, in real-world federated learning systems, such homogeneity is rarely observed. A discussion or extension to accommodate such heterogeneous settings would strengthen the work.\n3. Although the motivation for client reshuffling is clear, comparison against the baseline algorithms without reshuffling (e.g., standard FedAvg, FedProx, SCAFFOLD, and FedDC) would help isolate the effect of the reshuffling protocol itself. This would also clarify the incremental contribution of DR splitting versus reshuffling.\n4. The empirical evaluation is restricted to image classification tasks. Including additional modalities (e.g., NLP) would strengthen the generality of the proposed method and demonstrate its robustness across diverse federated learning applications."}, "questions": {"value": "1. While inexact proximal updates are practical, the cost of solving (even approximately) proximal subproblems may be higher than standard SGD-based methods. This overhead may hinder the applicability of FedCDR in resource-constrained edge environments. Have the authors quantitatively analyzed the additional computational cost introduced by the proximal steps?\n2. The theoretical analysis relies on a uniform client reshuffling strategy. Can the proposed method accommodate non-uniform reshuffling or client-availability-aware reshuffling schemes? If so, how would such variations affect the convergence guarantees and empirical performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OMMcxMIjQ3", "forum": "QJYmhiRKU3", "replyto": "QJYmhiRKU3", "signatures": ["ICLR.cc/2026/Conference/Submission4944/Reviewer_ekkD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4944/Reviewer_ekkD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4944/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936766142, "cdate": 1761936766142, "tmdate": 1762917783931, "mdate": 1762917783931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}