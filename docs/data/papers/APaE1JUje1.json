{"id": "APaE1JUje1", "number": 19829, "cdate": 1758299808879, "mdate": 1759897017047, "content": {"title": "Benchmarking Prompt-Injection Attacks on Tool-Integrated LLM Agents with Externally Stored Personal Data", "abstract": "Tool-integrated agents often access users’ externally stored personal data to complete tasks, creating new vectors for privacy leakage. We study indirect prompt-injection attacks that exfiltrate such data at inference time and propose a data-flow–aware threat model requiring actual leakage, rather than mere task hijacking, to count as success. We (i) extend InjecAgent's threat model to include externally stored personal data and actual leakage measurement; (ii) integrate our threat model into AgentDojo's Banking suite, extend its user tasks from 16 to 48 across nine service categories by adding 11 new tools; (iii) evaluate six LLMs and four defense strategies; and (iv) (ii) analyze various factors affecting leakage. On the original 16-task suite, most models reach $\\approx$20\\% targeted attack success rates (ASR), with Llama-4 17B peaking at 40\\%; on the expanded 48-task suite, ASR averages 11–15\\%. For GPT-4o, task utility drops by 12–22\\% under attack. Exfiltration of high-sensitive fields alone is less common, but risk rises sharply when combined with one or two less-sensitive fields, specially when injections are semantically aligned with the original task. Some defenses eliminate leakage on the 16-task suite and can reduce ASR to $\\approx$1\\% on the expanded suite, often with utility trade-offs. These findings underscore the importance of data-flow–aware evaluation for developing agents resilient to inference-time privacy leakage.", "tldr": "We extend InjecAgent’s model to cover externally stored personal data, measure actual leakage in multi-step tasks, and find that while attacks succeed at notable rates, existing defenses substantially reduce leakage.", "keywords": ["LLM", "Prompt Injection", "Privacy", "AI Agent"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67ebca2e8fd82efe79c0fb4c77c549c58e238856.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the vulnerability of tool-integrated LLM agents to indirect prompt-injection attacks aimed at exfiltrating a user's externally stored personal data. The authors introduce a data-flow-aware threat model that requires actual data leakage at inference time to count as a successful attack. \n\nThe primary contribution involves extending the AgentDojo Banking suite from 16 to 48 multi-step user tasks, alongside 11 new tools, creating a more comprehensive security benchmark. The study evaluates six LLMs and four defense mechanisms, revealing that vulnerability persists across models and that leakage risk is particularly high when combining high-sensitive and low-sensitive data fields."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The effort to expand the benchmark to 48 tasks and cover a broader range of banking services addresses a major limitation in previous work.\n\n2. The empirical results on how the combination of data sensitivities (high vs. low) influences attack success provides an immediate, actionable insight for researchers designing security protocols.\n\n3. The paper introduces a new attack vector in the form of external sources."}, "weaknesses": {"value": "1. The paper did not evaluate with SOTA defenses like LLaMA-Firewall, Granite Guardian, PromptArmor or using GPT-4/5 as a firewall defense.\n\n2. The defenses proposed in the paper do have a good effect on reducing the ASR.\n\n3. Latest GPT-5 models are not tested in the paper, especially given that this version is trained with \"safe completions\"."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lnF3cdzcgF", "forum": "APaE1JUje1", "replyto": "APaE1JUje1", "signatures": ["ICLR.cc/2026/Conference/Submission19829/Reviewer_FzTg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19829/Reviewer_FzTg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616528963, "cdate": 1761616528963, "tmdate": 1762932007441, "mdate": 1762932007441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates indirect prompt-injection attacks on tool-integrated agents, specifically focusing on privacy leakage of externally stored personal data. The authors propose a data-flow–aware threat model and conduct comprehensive empirical evaluation across multiple LLMs and defense mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Prompt injection is a critical security concern for tool-integrated agents, and this paper addresses the gap by focusing on actual data exfiltration rather than mere task hijacking, providing a more rigorous threat model for privacy leakage.\n2. The authors conducted extensive experiments across six LLMs and four defense strategies, significantly expanding the evaluation suite from 16 to 48 tasks with 11 new tools, demonstrating thorough empirical coverage of the problem space.\n3. The paper introduces a principled framework that requires measurable personal data leakage to count as attack success, moving beyond superficial task hijacking metrics and enabling more meaningful security assessment. The analysis of sensitivity combinations, semantic alignment, and defense trade-offs provides actionable findings for practitioners building secure agent systems."}, "weaknesses": {"value": "I think the main problem is the limited novelty in threat model and benchmark design.\n\n1. While the experimental work is appreciated, the core contribution appears incremental. The threat model is essentially an extension of existing indirect prompt injection frameworks (InjecAgent) with data-flow awareness added as a constraint. The benchmark itself is narrowly confined to AgentDojo's banking domain, expanding from 16 to 48 tasks within the same application context. This limited scope and incremental nature raise concerns about whether the contribution meets the novelty threshold expected for a main conference venue.\n\n2. The exclusive focus on banking services significantly limits the applicability of findings. Real-world tool-integrated agents operate across diverse domains (e-commerce, healthcare, productivity tools, etc.) with varying data sensitivity levels and interaction patterns. The paper's conclusions about \"inference-time privacy leakage\" in agentic systems rest on evidence from a single vertical, making it difficult to assess whether the observed attack success rates, defense trade-offs, and risk factors generalize beyond financial applications.\n3. While the paper reports that some defenses reduce ASR to 1%, there is inadequate investigation into the mechanisms underlying defense effectiveness and failure modes. The mention of \"utility trade-offs\" lacks quantitative depth—how severe are these trade-offs across different task types? What specific agent capabilities are compromised? Without understanding the cost-benefit profile of each defense strategy, practitioners cannot make informed deployment decisions.\nLack of comparison with alternative mitigation approaches: The evaluation covers only four defense strategies without justifying this selection or comparing against other promising approaches in the literature (e.g., input sanitization, output filtering, sandboxing, runtime monitoring). This narrow scope leaves open questions about whether the evaluated defenses represent best practices or if more effective alternatives exist.\n\nthere are some typos in the paper, e.g, line 021, line 106."}, "questions": {"value": "no"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JErPMqKtJJ", "forum": "APaE1JUje1", "replyto": "APaE1JUje1", "signatures": ["ICLR.cc/2026/Conference/Submission19829/Reviewer_ozxA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19829/Reviewer_ozxA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676203512, "cdate": 1761676203512, "tmdate": 1762932007012, "mdate": 1762932007012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper assess risk of sensitive personal data exfiltration by agents with access to tools that can reveal sensitive information.  They integrate their agent into AgentDojo's banking suite.  Throughout the rest of the paper, the authors test their exfiltration attacks, which rely on various types of prompt injection, across various models and data types.  They also check the efficacy of prompt-injection defenses as a function of agent utility on their benchmark.  Perhaps the most interesting finding is that false assumptions made by the attacker reduce ASR, while correctly identifying the user and model increase it."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "This is certainly a very important area of research given the increasingly widespread use of LLM agents.  Industries should definitely pay attention to and use this benchmark.\n\nThe authors thoroughly test their attacks against many different models and do strong ablation studies to verify when and where their attacks are successful.\n\nThe paper is very well structured, has attractive graphics, and performs most of the tests that I would like to see in such a setting."}, "weaknesses": {"value": "Major:\n\nWhile I don't see flaws in the methodology, the main problem with this paper, in my opinion, is lack of novelty.  We already knew about the threat of exfiltration, and we knew that agents were vulnerable to exfiltration attacks through prompt injection.  The scientific claims of this paper seem to be mostly in the convex hull of papers like DoomArena and the original InjecAgent benchmark.  There are some mildly novel claims in the analysis of when attacks are successful, but on the whole I don't think that this paper makes any significant scientific contributions.  \n\nThis is a potentially useful benchmark for industry to test their models on, but after reading this paper, I don't feel that I've learned anything that I didn't know before.\n\nMinor:\n- You're missing a citation on line 106"}, "questions": {"value": "Can you help me understand the novel contributions in this paper relative to past-work?  I think that all of your claims are sound, I just don't feel like this paper has any novelty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d9C5ORIZpg", "forum": "APaE1JUje1", "replyto": "APaE1JUje1", "signatures": ["ICLR.cc/2026/Conference/Submission19829/Reviewer_ysWG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19829/Reviewer_ysWG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761690466721, "cdate": 1761690466721, "tmdate": 1762932006536, "mdate": 1762932006536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They construct a benchmark for indirect prompt injection attacks that exfiltrate user private data at inference time. They extend InjecAgent’s data-exfiltration threat model to include externally stored personal data accessed only during task execution, measuring data\nleakage at inference time. Then they build their evaluation tasks by extending AgentDojo’s Banking suite. They evaluate six models, including open-weight and closed-weight. They show that models show some vulnerabilities to indirect prompt injection attacks regarding user privacy (ASR 11–15% on average). They also explore four defense strategies, including detector and prompt-repetition defenses, and show their effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- They consider prompt injection attacks that actually lead to data leakage at inference time\n- They expand AgentDojo’s banking suite from 16 to 48\n- They evaluate the ability of existing defense methods"}, "weaknesses": {"value": "I find the paper somewhat incremental. It focuses on indirect prompt injection attacks that exfiltrate users’ private data at inference time. However, there already exist multiple studies exploring indirect prompt injection and user data exfiltration attacks. The work would be strengthened if it proposed new attack methods or mechanisms that enable data exfiltration more effectively or in novel settings.\n\nMoreover, while extending AgentDojo is valuable, the benchmark construction effort relies heavily on the existing benchmark, and the defenses evaluated are all existing ones.\n\nIn fact, even if the attacks don't lead to data exfiltration but only change model behavior, they might still cause harm to users. For example, the model might leak data in future interactions as a result of the attack. Limiting the focus to immediate explicit data exfiltration may overlook other realistic threats. Thus, emphasizing only the “upper bound” of vulnerability to direct data exfiltration might overestimate the overall safety of current models.\n\nThe writing could be improved. Please constantly use \\citep where appropriate (e.g., AgentDojo Debenedetti et al. (2024), InjecAgent’s Singh et al. (2024), etc). There is a typo in the abstract ((iv) (ii) analyze various factors...)"}, "questions": {"value": "Please see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ccf46QPZWg", "forum": "APaE1JUje1", "replyto": "APaE1JUje1", "signatures": ["ICLR.cc/2026/Conference/Submission19829/Reviewer_Saz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19829/Reviewer_Saz9"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission19829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762497241186, "cdate": 1762497241186, "tmdate": 1762932005857, "mdate": 1762932005857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}