{"id": "LhwPy8NMoN", "number": 7445, "cdate": 1758022408356, "mdate": 1759897852493, "content": {"title": "PLP-RC:Point–Line–Plane Fusion for Discriminative Relation Classification with LLMs", "abstract": "Relation classification is a fundamental NLP task that involves identifying the semantic relations between entity pairs in a given text. While pre-trained language models have advanced this area, effectively integrating local entity information with global context remains a key challenge. Large Language Models offer rich world knowledge, but their generative use often suffers from hallucinations, limiting reliability. To address these issues, we propose a Point–Line–Plane fusion framework for discriminative relation classification with LLM embeddings. Entity spans are modeled as local point representations, the end of sequence token provides a global plane representation, and an attention-based line representation aligns the two. This discriminative paradigm avoids hallucinations while fully exploiting LLM representations. Our method achieves new SOTA performance on TACRED, TACREV, and RE-TACRED benchmarks,  outperforming both discriminative and generative baselines. Ablation studies provide further evidence for the effectiveness of our design in achieving context-aware relation classification.", "tldr": "PLP-RC is a discriminative LLM-based framework that fuses point, line, and plane representations, avoiding hallucinations and achieving SOTA on TACRED, TACREV, and RE-TACRED.", "keywords": ["LLM", "Embedding", "Relation Classification"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2caaaea38c7e4c828020c8e3a93591870ae86bbf.pdf", "supplementary_material": "/attachment/7e5672f95be3eda96a0e20dd9100399da31e5814.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes PLP-RC, a framework for Relation Classification that leverages Large Language Models (LLMs) embeddings but avoids the hallucination issues common in LLM generative usages. The core innovation is a \"Point-Line-Plane\" geometric fusion mechanism: Point represents local entity span information, Plane encodes global context  with the [EOS] token, and Line (attention scores between entities and [EOS]) bridges the two levels of granularity. These features are integrated into fused entity representations for relation prediction. PLP-RC achieves new state-of-the-art results on the TACRED, TACREV, and RE-TACRED benchmarks, outperforming both discriminative and generative baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper proposes a discriminative approach with LLM embeddings instead of directly using the LLM generative power\n* The Point-Line-Plane geometric analogy for feature fusion is interesting\n* The experimental evaluation is comprehensive, including benchmarks, baselines, ablation studies, model scaling and computational costs\n* The paper is well-written and clear"}, "weaknesses": {"value": "* The evaluation benchmarks primarily consist of single-sentence contexts. The authors may briefly discuss the proposed approach's adaptation to long-range dependencies.\n* The ablation studies show that the \"Line\" component's contribution is relatively modest compared with others.\n* It would be nice if the authors could analyze further whether specific types of relations affect the overall performance"}, "questions": {"value": "1. Given the most contribution from \"Plane\" and instruction, how does the model's performance change if the instruction changes, e.g., placed at the beginning instead of the end?\n2. The \"Line\" feature uses attention scores from the [EOS] token to entity tokens. Did you try other attention scores like those between the subject and object entities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zZlaH9jmAQ", "forum": "LhwPy8NMoN", "replyto": "LhwPy8NMoN", "signatures": ["ICLR.cc/2026/Conference/Submission7445/Reviewer_zsse"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7445/Reviewer_zsse"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761037494871, "cdate": 1761037494871, "tmdate": 1762919558992, "mdate": 1762919558992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel framework, Point–Line–Plane Fusion (PLPF), for relation classification using Large Language Model (LLM) embeddings. The work addresses a long-standing challenge in balancing local entity representation with global contextual understanding while mitigating hallucination issues common in generative models. The proposed geometric abstraction models entities as points, context alignment as lines, and overall semantic scope as planes, which is conceptually elegant and empirically validated."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper introduces an innovative geometric fusion paradigm based on the point–line–plane concept, which offers a clear and interpretable approach to integrating local and global features for relation classification.\n\n2.The proposed method leverages Large Language Model embeddings in a discriminative framework, effectively mitigating hallucination issues that are commonly observed in generative LLM applications."}, "weaknesses": {"value": "1.The core \"Point-Line-Plane (PLP)\" fusion mechanism lacks sufficient theoretical justification. The paper frames the mechanism as \"conceptually grounded in geometric and information-theoretic principles\" but provides no formal connection to these principles (e.g., how line/plane representations map to information-theoretic metrics like mutual information).\n\n2.The methodological description lacks mathematical rigor and theoretical foundation. The \"geometric perspective\" remains largely metaphorical without formal mathematical formulation or theoretical guarantees about the representation properties.\n\n3.The paper fails to address cross-sentence relation classification, a critical limitation of existing methods highlighted in the Introduction. All experiments are conducted on sentence-level datasets, yet the PLP framework is claimed to \"capture long-range dependencies\"—no evidence is provided for this capability, and the [EOS] token’s causal attention (autoregressive) cannot model cross-sentence context effectively.\n\n4.The paper confuses discriminative vs. generative paradigms. It claims PLP-RC avoids hallucinations by using a \"discriminative framework\" but uses decoder-only LLMs (Qwen3) pretrained with generative next-token prediction."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "03USgbBZf5", "forum": "LhwPy8NMoN", "replyto": "LhwPy8NMoN", "signatures": ["ICLR.cc/2026/Conference/Submission7445/Reviewer_4Tdp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7445/Reviewer_4Tdp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761500455240, "cdate": 1761500455240, "tmdate": 1762919558436, "mdate": 1762919558436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Point-Line-Plant fusion framework based on LLM embeddings for entity relation classification, which leverages the representational capacity of LLMs while mitigating their hallucination issues. Experiments were conducted to validate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach utilizes a LLM as text encoder, effectively transferring its rich representational capacity to the discriminative task of relation classification. This enables the generation of semantically richer embeddings, leading to improved classification performance. Furthermore, the method requires no fine-tuning of the LLM, thereby avoiding substantial computational costs."}, "weaknesses": {"value": "1. Lines 52-54 of the paper mention that LLMs have inherent deficiencies in capturing contextual content, yet there is no further explanation or citation of relevant arguments in the paper. Taking the Qwen3 model used in the paper as an example, it can support a maximum context length of 128K, which is fully capable of covering some basic tasks including Relation Classification.\n\n2. Experiments demonstrate that the PLP-RC method proposed in this paper is effective in relation classification, significantly outperforming other approaches. However, PLP-RC uses Qwen3 as its backbone, while the comparison methods adopt GLM-10B, Mistral-7B, and LlaMA2-7B. It is important to note that Qwen3 is a new-generation model; its 4B version even outperforms Qwen2.5-7B, its predecessor. Moreover, Qwen2.5-7B itself shows better performance than models like Mistral-7B and LlaMA2-7B. Therefore, it remains unclear whether the advantage of PLP-RC over other models stems from the method itself or from Qwen3, resulting in a lack of fair comparison.\n\n3. As a mature and fundamental task, relation classification can already achieve good results by directly using LLMs to generate answers. PLP-RC treats LLMs as encoders and transforms relation classification into a discriminative task, but the paper lacks experiments to illustrate the advantages of PLP-RC compared to direct answer generation.\n\n4. The writing expression of the paper needs further polishing, and the presentation should be consistent throughout the text. Some parts of the article lack necessary citations, such as the reference to the strategies of previous work in lines 222-223."}, "questions": {"value": "Refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y8DFHWnExK", "forum": "LhwPy8NMoN", "replyto": "LhwPy8NMoN", "signatures": ["ICLR.cc/2026/Conference/Submission7445/Reviewer_UvcE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7445/Reviewer_UvcE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639061965, "cdate": 1761639061965, "tmdate": 1762919558026, "mdate": 1762919558026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Point–Line–Plane (PLP) framework for relation classification, demonstrating limited performance improvements on TACRED and related datasets. However, the work lacks substantial novelty and does not meet high standards for innovation and impact."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper would be a good negative example to warn students not to write similar papers."}, "weaknesses": {"value": "The paper lacks novelty, practical application, and sufficient experimental rigor. To improve, the authors should focus on contemporary challenges, explore more innovative approaches, and validate real-world applicability."}, "questions": {"value": "Suggestion：The authors should consider more research-worthy directions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FiPXRW9vL2", "forum": "LhwPy8NMoN", "replyto": "LhwPy8NMoN", "signatures": ["ICLR.cc/2026/Conference/Submission7445/Reviewer_v33w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7445/Reviewer_v33w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897262762, "cdate": 1761897262762, "tmdate": 1762919557628, "mdate": 1762919557628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}