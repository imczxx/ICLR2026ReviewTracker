{"id": "X9D5MVpPJ9", "number": 5868, "cdate": 1757942535092, "mdate": 1759897948710, "content": {"title": "TROLL: Trust Regions Improve Reinforcement Learning for Large Language Models", "abstract": "On-policy Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). \nAlthough recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched.\nOriginally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. \nWe replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints.\nThe projection operates on a sparse subset of the model’s most important token logits to balance computational cost and projection effectiveness. \nOur approach, Trust Region Optimization for Large Language Models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model’s inference behavior.\nAcross datasets, model families, and advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates.", "tldr": "Replacing PPO's clipping objective with more principled trust regions improves RL from verifiable rewards.", "keywords": ["RL from verifiable rewards", "Finetuning LLMs", "Trust Regions"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4abd285acbf96cfe068b0d3d00ff3904bdd2f909.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, authors considers PPO-like approaches, more specifically approaches that make use of a clipped importance sampling ratio. Instead of clipping this ratio, they propose to use a projection that keeps the KL within a certain range. In practice, this is an extension of Otto et al (2021) from Gaussian distributions to categorical distributions, with an additional spare and efficient representation of token distributions to cope with the usually huge vocabulary / action space of LLMs. The proposed approach is quite general (in replacing clipping by this projection for any methods making use of this clipping). A quite thorough experimental study is provided, showing the advantage of the approach compared to clipping, and providing interesting ablations, including illustrating the small computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is overall well written and structured\n* Clipping is a commonly used heuristic, and any approach improving it can have a quite wide impact. This approach in particular is well motivated (clipping is indeed a heuristic approx of trust region) and seems to be quite efficient, with specific care to specificities of LLMs (like huge action space)\n* The experimental study is quite thorough, provide interesting ablations, and showcase the advantage of the proposed approach in many cases."}, "weaknesses": {"value": "* There are clarity issues about the proposed approach, notably what optimization problem is really solved, how the trust-region projection interplays with the more general RL problem, or what justifies the heuristic of Eq (5). See questions below for more details.\n* Given that the paper studies an alternative to clipping, that’s a bit a pity that the authors do not consider as related works or even possibly baselines other alternatives to clipping, including approaches that even avoid relying on importance sampling. See questions below for more details and specific references."}, "questions": {"value": "### Clarity\n\n* Maybe nitpicking, but the paper talks about on-policy approaches all along. If it was truly on-policy, there would be no importance sampling and then no clipping.\n* It is unclear what is the overall optimization problem. It seems that problem (1) (RL objective) is solved with problem (3) (trust region projection), but how do they interplay? The term $\\tilde{\\pi}$ is the optimization variable of Eq (1) (as a side note, it is much clearer to write $J_{ratio}(\\tilde{\\pi})$ rather than just $J_{ratio}$, it is a function, making the argument explicit helps). Let call $P(\\tilde{\\pi})$ the result of Eq (3) (the argmin). Do we solve for $J_{ratio}(P(\\tilde{\\pi}))$ or something else? How does it relate to the classic KL constrained RL problem (eg as presented in TRPO)?\n* In Eq (5), it is not clear what gradient clipping means in this context, how it can be applied (and why) only partly, or what is the overall justification for doing Eq (5), and its implications. It is not even clear what policy is optimized there. Overall, this should be really clarified. \n* What is also unclear is towards what we regularize (what is $\\pi_{old}$). We can regularize towards two policies, the initial one, and the previous one (in a policy iteration context, what is fundamentally PPO or TRPO). Often, regularization is made towards both, through different term (eg, GRPO consider regularization towards both, the previous one through importance ration and clipping and the inital policy through the regularization term). Please clarify what $\\pi_{old}$ is in your case.\n\n### Related works and baselines \n\nGiven that the paper addresses the limitations of clipping, it misses a few approaches from the literature, in the context of LLMs, with the same or a very similar objective, either proposing an alternative to clipping the importance sampling ratio, or even removing the importance sampling ratio. Notably, the following papers could be discussed, and possibly considered as baselines:\n* [A] generalizes clipping by clipping the importance ratio within the gradient rather than within the objective, asymmetrically \n* [B] is a policy-gradient approach (in the sense of optimizing for a policy), relying on an additional value network but without any importance sampling, while being off-policy\n* [C,D] is an off-policy policy gradient without importance sampling\n* [E] is a method relying rather on Q-functions, but as so being off-policy without importance sampling (see also references therein for older approaches)\n\n[A] Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs, Le Roux et al, 2025.    \n[B] Offline Regularised Reinforcement Learning for Large Language Models Alignment, Richmond et al, 2024.   \n[C] Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion, Flet-Berliac et al, 2024.   \n[D] Command A: An Enterprise-Ready Large Language Model, Cohere, 2025.   \n[E] ShiQ: Bringing back Bellman to LLMs, Clavier et al, 2025.   \n\n\n### Misc\n* Eq (4) there’s probably a $\\log$ missing around $\\tilde{\\pi}$"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GbGoi9Vl2l", "forum": "X9D5MVpPJ9", "replyto": "X9D5MVpPJ9", "signatures": ["ICLR.cc/2026/Conference/Submission5868/Reviewer_RHmH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5868/Reviewer_RHmH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760726930886, "cdate": 1760726930886, "tmdate": 1762918312973, "mdate": 1762918312973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a way of fine-tuning LLM with RL that uses a more principled approach to ensure the network updates are within a trust reagion by using a token-wise kl constraint objective. This is different from the standard way of fine-tuning LLM's with algorithms like MPO that use clipping. The trust reagion is also differentuable which is important for updates. The paper also presents a way of sparcificaition for scalability where only a subset of tokes is used to compute the trust region."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper and the algorithm are presented well and intuitively. TROLL does outperform the baseline even if only by a small margin, nevertheless I think there is a big value for a more principled approach than clipping."}, "weaknesses": {"value": "The gain in performance is not super big, also the scale of evaluation is a bit limited as only smaller models have been tested so it is unclear how well this will scale. Additionally was seems a bit narrow is the choice of task domains that are heavily focused on mathematical reasoning tasks. TROLL should be useful in a variety of tasks and not only for one specific domain."}, "questions": {"value": "How would this scale to bigger models?\ncan you provide evaluations on other tasks than mathematical reasoning to show that TROLL is generally outperforming the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qXlE6sMrIF", "forum": "X9D5MVpPJ9", "replyto": "X9D5MVpPJ9", "signatures": ["ICLR.cc/2026/Conference/Submission5868/Reviewer_YCtx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5868/Reviewer_YCtx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585132041, "cdate": 1761585132041, "tmdate": 1762918312682, "mdate": 1762918312682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing reinforcement learning (RL) algorithms for large language models (LLMs) including PPO and GRPO rely on clipping on policy ratios. While theoretically motivated by trust region optimization, clipping is only a crude approximation of the trust region. To this end, the authors propose TROLL, a differentiable trust region projection approach that directly enforces token-level KL constraints between discrete distributions. TROLL can directly replace the clipping objectives in PPO-like algorithms. Notably, TROLL projects the output distribution of the new, updated policy onto a KL-trust region around the old policy, and the projection can be computed in closed form.  To implement TROLL in practice, the authors introduce a sparsification scheme that discards the vast majority of effectively irrelevant, low-probability tokens. The authors verify the effectiveness of TROLL in RLVR settings on multiple LLM families across math datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This work addresses an important challenge for LLM post-training.\n- The proposed method TROLL is novel. \n- The authors compare multiple LLM families with various sizes in their experiments."}, "weaknesses": {"value": "## Presentation \nI personally find Section 3 (the most important algorithm section) to be poorly written and organized. In particular, I recommend the authors to include a pseudocode for the complete algorithm, from computation of $\\Tilde{\\pi}$, to its projection, to the calculation of final loss. Then write a separate subsection about the how the gradients are propagated and move the key results to the main text. As of now, it is difficult to understand both the forward and backward pass of the TROLL algorithm. This is the most critical concern to me. And I will update the evaluation if the presentation problem can be sufficiently addressed.\n\n## Related Work\nAs PPO is one of the most classical algorithms for RL, there have been many attempts to enhance its clipping objective, e.g., \n- with adaptive clipping [1],\n- with KL-regularized surrogates [3].\n\nIn particular, I am wondering why the following works of PPO (although not designed for LLMs) does not address the clipping problem of PPO. In other words, what are the unique challenges about LLMs that TROLL addresses and these prior RL works cannot. \n\nIn addition, I think [2] addresses the same challenge and the authors should consider comparing it. \n\n[1] Trust Region-Guided Proximal Policy Optimization.\n\n[2] BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping.\n\n[3] V-MPO: ON-POLICY MAXIMUM A POSTERIORI POLICY OPTIMIZATION FOR DISCRETE AND CONTINUOUS CONTROL\n\n\n## Experiments \n**Marginal Performance Gain.** Despite of the complex algorithm, the performance increase on the most popular combination for the math benchmark (Qwen + GRPO) is marginal. \n\n**Only Math Benchmark.** It would be beneficial if the authors can include more diverse RLVR benchmarks other than the math benchmarks. While it is appreciated that the authors conduct experiments on extensive benchmarks, all of them are math benchmarks and this can lead to questions about TROLL's scalability to other types of post-training tasks."}, "questions": {"value": "- Why the authors claim that TROLL is for LLMs only? TROLL is an attempt to improve PPO, which is a general RL algorithm. Hence, TROLL should also be effective for standard (non-LLM) RL tasks. \n- If I understand correctly, it seems that the dual step size $\\eta^\\star$ needs to be calculated for every token?\n- Why the gain for GSPO much larger than GRPO and DrGRPO?\n- Could the authors please explain in more details why TROLL is much more complex than"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TW66hArReL", "forum": "X9D5MVpPJ9", "replyto": "X9D5MVpPJ9", "signatures": ["ICLR.cc/2026/Conference/Submission5868/Reviewer_cNjy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5868/Reviewer_cNjy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987373458, "cdate": 1761987373458, "tmdate": 1762918312331, "mdate": 1762918312331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel method that replaces the standard PPO-like clip objective used in RL-based LLM fine-tuning. The authors posit that PPO's clipping is a crude approximation of a more principled KL-based trust region. TROLL substitutes this heuristic with a discrete differentiable trust region projection. For each token, the method solves a convex optimization problem that projects the new policy's output distribution onto a KL-ball centered around the old (sampling) policy's distribution. To make this practically feasible for large vocabularies, the authors additionally sparsify the predictive distribution and operate only on a small subset of the most probable logits, capturing the majority of the probability mass. Using implicit differentiation, this projection is fully differentiable, allowing gradients to propagate, unlike the gradient-cutting nature of PPO's clip. Experiments on mathematical reasoning benchmarks (DAPO-Math, GSM8K) show that TROLL consistently outperforms clipping in training stability, speed, and final success rates across various models (e.g., Qwen3, LLaMA 3)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Novel and innovative approach to trust region based RL under the practical constraints of modern LLM finetuning. \nThe approach is theoretically well motivated and the paper shows convincing empirical results.\nThe paper is very well written; the appendix with detailed deviations is easy to follow. \nExperiments and empirical evidence is collected using a suite of differently open-weight model sizes and families; generally with very consistent and impressive results."}, "weaknesses": {"value": "The experimental section focuses on consistently outperforming established benchmarks using a wide range of models and multiple datasets. This unfortunately leaves little space for more detailed analysis and ablations. The appendix D.6 however has at least some interesting additional detailed analysis."}, "questions": {"value": "I am surprised about the low 0.1% clipping/projection rate displayed in Figure 14. Have you run ablations with different batch-sizes and model-staleness to better understand the effects of being on- vs. slightly off-policy?\n\nHave you investigated how the policy entropy evolves over the course of learning compared to baseline methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VWUzNzmc0J", "forum": "X9D5MVpPJ9", "replyto": "X9D5MVpPJ9", "signatures": ["ICLR.cc/2026/Conference/Submission5868/Reviewer_uGtd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5868/Reviewer_uGtd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216326040, "cdate": 1762216326040, "tmdate": 1762970647771, "mdate": 1762970647771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}