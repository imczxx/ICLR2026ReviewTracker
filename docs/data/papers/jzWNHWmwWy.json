{"id": "jzWNHWmwWy", "number": 6758, "cdate": 1757994747731, "mdate": 1759897895862, "content": {"title": "Generative Actor Critic", "abstract": "Conventional Reinforcement Learning (RL) algorithms, typically focused on estimating or maximizing expected returns, face challenges when refining offline pretrained models with online experiences. This paper introduces Generative Actor Critic (GAC), a novel framework that decouples sequential decision-making by reframing policy evaluation as learning a generative model of the joint distribution over trajectories and returns, $p(\\tau, y)$, and policy improvement as performing versatile inference on this learned model. To operationalize GAC, we introduce a specific instantiation based on a latent variable model that features continuous latent plan vectors. We develop novel inference strategies for both exploitation, by optimizing latent plans to maximize expected returns, and exploration, by sampling latent plans conditioned on dynamically adjusted target returns. Experiments on Gym-MuJoCo and Maze2D benchmarks demonstrate GAC's strong offline performance and significantly enhanced offline-to-online improvement compared to state-of-the-art methods, even in absence of step-wise rewards.", "tldr": "Generative Actor Critic decouples decision-making by learning a generative model over trajectories and returns, then uses versatile inference on that model for policy improvement.", "keywords": ["Sequential Decision Making", "Generative Model", "Reinforcment Learninig"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/099069256c316a3fd6729fdb657df623da45b0bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The submission introduces Generative Actor Critic (GAC), a latent-variable framework that models the joint distribution over trajectories and total returns and treats decision-making as test-time inference on that model. Building on Latent Plan Transformer, the authors split the latent plan into trajectory and return components, train via an ELBO with per-trajectory variational inference, and instantiate separate inference procedures: gradient ascent in latent space to maximize predicted returns for exploitation, and an exploration mode that samples latent plans conditioned on optimistic targets drawn from a replay buffer. Experiments on D4RL MuJoCo and Maze2D benchmarks with only final-return signals report competitive offline performance and staged offline-to-online improvements relative to baselines adapted to the same trajectory-return-only setting; the paper also presents qualitative visualizations (e.g., t-SNE clusters of latent plans) though the interpretive value of those plots is debatable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles the challenging trajectory-return-only setting, demonstrating an approach that does not rely on step-wise rewards while still maintaining competitive performance across several D4RL MuJoCo and Maze2D benchmarks.\n- By modeling the joint distribution \\(p(\\tau, y)\\) and performing decision-making as latent-space inference, the framework offers a principled route toward planning and replanning capabilities within a unified generative formulation.\n- The separation of exploitation (gradient ascent in latent space) and exploration (conditioning on optimistic targets) provides a clear conceptual structure that could be useful for offline-to-online adaptation scenarios."}, "weaknesses": {"value": "- The paper leans on t-SNE visualizations to argue for a “sophisticated internal representation,” yet $z$ is generated by a network conditioned on $s_{0}$; because standard neural networks are Lipschitz and map nearby inputs to nearby outputs, similar clustering would appear even without meaningful learning, so the claim is unconvincing [1].\n- The paper does not quantify the training-time or inference-time overhead introduced by its design choices (e.g., latent optimization, staged fine-tuning), so readers cannot assess whether the method is practical relative to the baselines.\n- Exploration depends on adding a hand-tuned $\\Delta y$ to top-quantile returns. There is no principled rule for choosing $\\Delta y$ or detecting over-optimism, despite large values in Table 6, which is at odds with the Section 3.1 criticism of fixed overly optimistic targets $y^{*}$.\n- The paper never specifies how baselines that rely on step-wise rewards (e.g., IQL, CQL, DT) were adapted to the trajectory-return-only setting, so the fairness and reproducibility of the comparison cannot be verified.\n- Reported variances (e.g., Table 1) are high, yet there is no analysis of instability or sensitivity; the paper tunes wide ranges of critical hyperparameters (notably $\\Delta y$, outer-loop steps, and context length) but provides no sensitivity analysis or ablations, making robustness unclear.\n- GAC does not clearly outperform step-wise-reward baselines after fine-tuning, yet the paper provides only qualitative motivation for why modeling $p(\\tau, y)$ is preferable, and it omits comparisons to recent strong offline RL methods such as Learning to Trust Bellman Updates [2], leaving the practical advantage in question.\n\n[1] Goodfellow et al., Deep Learning, MIT Press, 2016\n\n[2] Luo et al., Learning to Trust Bellman Updates, arXiv preprint, 2025"}, "questions": {"value": "- Could the authors provide ablations on key hyperparameters such as $\\Delta y$, the outer-loop training steps, and the context length $M$ to assess sensitivity and robustness?\n- How is $\\Delta y$ selected in practice, how sensitive are results to this hyperparameter, and how does this choice avoid the optimism issues attributed to fixed $y^{*}$ in Section 3.1?\n- Please detail how each baseline that depends on step-wise rewards (e.g., IQL, CQL, DT) was adapted to the trajectory-return-only setting so the comparison can be reproduced.\n- Can the authors compare against recent selective regularization methods such as Learning to Trust Bellman Updates [1], or explain why such comparisons are infeasible?\n- Reported standard deviations are large for several tasks; can the authors analyze what drives this variance and provide evidence that the results are statistically meaningful?\n- Please report wall-clock training time (and online fine-tuning time) for GAC versus each baseline.\n- The latent prior $p_{\\alpha}(z \\mid s_{0})$ already conditions on $s_{0}$, so nearby start states should naturally map to nearby latents (due to Lipschitzness explained above) even without meaningful structure. Could the authors (i) clarify why the t-SNE clusters in Fig.~3(b) are not a trivial consequence of this and (ii) provide additional evidence to substantiate the claim of “sophisticated internal representations”?\n- What is the runtime cost of closed-loop replanning (Eq.~3) compared with open-loop execution, especially in the online fine-tuning stages?\n\n\n[1] Luo et al., Learning to Trust Bellman Updates, arXiv preprint, 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3g9NeVP1gv", "forum": "jzWNHWmwWy", "replyto": "jzWNHWmwWy", "signatures": ["ICLR.cc/2026/Conference/Submission6758/Reviewer_SU1Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6758/Reviewer_SU1Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760912577136, "cdate": 1760912577136, "tmdate": 1762919041128, "mdate": 1762919041128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript provides an interesting framework Generative Actor-Critic to decouple the decision-making process. In the policy evalution step, the authors learn a generative distribution for the joint distribution of trajectory and returns, instead of the conventional mean return. For the model inference, the authors suggests a novel exploration and exploitation method. The offline experiments from MuJoCo and Maze2D indicate superiority of the proposed method."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* The first strength of this manuscript is the joint distribution estimation for trajectory and return, which differs from the  method in traditional RL, and this may be particularly suitable for long horizon planning. \n\n* The online fine-tuning step in GAC is critical. The exploration idea of $y^{+} = y+\\Delta y$ seems an efficient way for better exploration. \n\n* The idea of applying the latent variable in generative modeling is appealing. It combines the good merits in generative modeling (dimension reduction and re-parametrization), and applies well in exploration and exploitation."}, "weaknesses": {"value": "1. Some key hyper-parameters relies on manual tuning, and the robustness of the proposed method needs to be improved. For example, the $\\Delta y$ is important for online fine tuning, however, the authors have not proposed the auto-tuning method. This also applies for the hyper-parameters in latent variable modeling. \n\n2. The reviewer appreciate the idea of modeling the joint distribution of trajectory and returns. However, it would be better if additional theoretical analysis are provided. Why this idea could work and how does it perform in convergence, especially why it could avoid the traditional Out-Of-Distribution issue in RL. \n\n3. The idea in the manuscript is good. However, the model design, which includes the ELBO optimiation, generative modeling (latent variable) and online fine-tuing, the cost of model training should be much high the conventional RL method. Have the authors consider about this issue?"}, "questions": {"value": "See the weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VhEKSF8zUI", "forum": "jzWNHWmwWy", "replyto": "jzWNHWmwWy", "signatures": ["ICLR.cc/2026/Conference/Submission6758/Reviewer_xAm5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6758/Reviewer_xAm5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546313285, "cdate": 1761546313285, "tmdate": 1762919040710, "mdate": 1762919040710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce the Generative Actor-Critic (GAC), a framework that learns the joint distribution between trajectories and returns, in contrast to most prior work that focuses on generating trajectories that simply maximize returns. GAC differs from LPT in both its parameterization and training procedure:\n- GAC generates a plan for each initial state.\n- GAC uses separate conditioning latents for trajectories and returns.\n- GAC learns an approximate posterior using variational Bayes, rather than MCMC as in LPT.\n\nThe authors further leverage their framework to design a novel inference and data collection strategy, in which the trained model is conditioned on a translated version of the return to guide it toward high-reward regions. This approach enables the model to produce high-quality samples that differ from those in the offline dataset, as demonstrated in Figure 4.\n\nAdditionally, GAC supports closed-loop re-planning, with promising results shown in Figure 5. Overall, the proposed method outperforms baselines both after offline pre-training and subsequent online fine-tuning. The actor and critic derived from the learned joint model exhibit good alignment with the ground-truth returns, and the approach demonstrates versatility when conditioned on the initial state, as illustrated in Figure 3."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and analyzes the proposed approach holistically on many aspects and does so very well with convincing evidence. The paper also well motivates the problem and it's easy to follow the motivation all the way through the proposed solutions. I also liked the analysis done in Figure 3 that intuitively shows the effect of online-finetuning as well as Figure 4 which analyzes the quality of the actor and the critic. Figure 5 also motivates a natural extension and future work by showing the potential for integrating closed-loop re-planning during inference."}, "weaknesses": {"value": "- As pointed out by the authors, GAC and LPT both suffer from the inability to incorporate step-wise rewards which should be used whenever available and Table 8 reflects the consequence of not incorporating them. That being said, it would be interesting to see a heuristic for autonomous re-planning that can for example incorporate those step-wise rewards to \"mine\" for interesting states and use them.\n - On one hand, the GAC paper builds on top of the LPT paper and proposes some modifications to its core architecture and training. On the other hand it still introduces some novel ways for conducting inference. Looking at Table 1, it seems that the architectural and training changes actually perform worse than LPT when we look at the p(y|z)p(z) column which calls into question the proposed GAC pre-training scheme and its architecture. This also begs the following question: what happens when we keep LPT yet use the inference schemes from GAC? I propose the following ablations:\n\t - Inference strategies: Keep the core LPT training and architecture but compare against the same introduced inference strategies, namely: p(y+), E[y] and y*. Also, in LPT they use Exploitation-inclined inference (EI) which I see it as close to p(y+) and E[y] so it would also be interesting to highlight results for that inference scheme for both LPT and GAC.\n\t - Training: \n\t\t - What happens to GAC when you don't split the conditioning vector?\n\t\t - What happens when the plan doesn't depend on $s_0$?\n\t\t - What happens when you use MCMC instead of a variational approximation to the posterior?"}, "questions": {"value": "- In line 189, do you mean $p_\\alpha$?\n- Since the paper modifies the LPT, it would be good to have a table summary summarizing differences\n- It would be good to know how fast is this approach compared to existing methods. LPT uses MCMC to get the posterior samples that \n- Would be good to have the algorithms for the other inference strategies\n- it would be good to highlight the second and third best just to be able to easily see how other inference strategies fare with the baselines\n- In Table 2, it would be good to indicate what delta is in the caption. I understand it to be the difference between the returns after the online-finetuning and the returns from the offline-pretrained model. Also, some cells in the table give the sign for delta and some do not. I suggest you standardize that over all cells where it applies.\n- line 424 typo: alomst -> almost\n- How is re-planning done exactly? Is the latent plan generation triggered from the partial trajectory from A to B whenever the agent gets to the waypoint B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T5DwYyykRm", "forum": "jzWNHWmwWy", "replyto": "jzWNHWmwWy", "signatures": ["ICLR.cc/2026/Conference/Submission6758/Reviewer_UgEN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6758/Reviewer_UgEN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938470557, "cdate": 1761938470557, "tmdate": 1762919039929, "mdate": 1762919039929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Generative Actor-Critic (GAC), a new framework that formulates reinforcement learning as generative modeling and probabilistic inference over trajectory–return pairs. Instead of estimating expected returns, GAC learns a joint generative model $p(\\tau,y)$ over trajectories $\\tau$ and returns $y$ using a latent-variable model $p(z)p(\\tau∣z)p(y∣z)$. Decision-making (both exploration and exploitation) is then performed via posterior inference on the latent plan variable $z$.\n\nThe approach allows effective offline-to-online RL adaptation, supports trajectory-level reasoning without step-wise rewards, and achieves state-of-the-art results on several continuous control and maze benchmarks.\n- An LLM was used to improve writing."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Principled formulation — Viewing RL as generative inference offers a clear conceptual bridge between diffusion-style generative models and reinforcement learning.\n\n2. Trajectory-level modeling — Operates on trajectory distributions rather than step-wise transitions, enabling sparse-reward settings.\n\n3. Solid performance on standard benchmarks — Demonstrates strong offline-to-online adaptation even without step-level reward signals."}, "weaknesses": {"value": "1. Outdated baselines\n\nThe experimental comparison primarily uses older O2O RL methods (e.g., ODT, Cal-QL, LPT). Missing comparisons with more recent approaches. This makes it difficult to judge whether improvements stem from the GAC formulation or simply from implementation differences.\n\n2. Limited novelty / incremental over Decision Diffuser\n\nConceptually, GAC can be seen as a latent-variable extension of Decision Diffuser with a minor modification of the target formulation (the optimistic $y^+$). The “guided generation via optimistic target” resembles classifier-free guidance (CFG) and does not constitute a major conceptual departure. Lastly, the online fine-tuning stage essentially performs supervised fine-tuning (SFT) on new trajectories, rather than a fundamentally new RL adaptation mechanism.\n\n\n3. Empirical scope\n\nWhile GAC is evaluated on D4RL, evaluation is limited to low-dimensional control benchmarks, excluding kitchen dataset or adroit. Evaluation on mujoco is also excluding hopper, halfcheetah, and walker2d medium expert, which is one of the most common benchmark in d4rl mujoco."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EAzqD8vYOY", "forum": "jzWNHWmwWy", "replyto": "jzWNHWmwWy", "signatures": ["ICLR.cc/2026/Conference/Submission6758/Reviewer_s55t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6758/Reviewer_s55t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953760565, "cdate": 1761953760565, "tmdate": 1762919039421, "mdate": 1762919039421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}