{"id": "yEpE0QPpf8", "number": 4351, "cdate": 1757665754408, "mdate": 1763655370195, "content": {"title": "Grounding-IQA: Grounding Multimodal Language Model for Image Quality Assessment", "abstract": "The development of multimodal large language models (MLLMs) enables the evaluation of image quality through natural language descriptions. This advancement allows for more detailed assessments. However, these MLLM-based IQA methods primarily rely on general contextual descriptions, sometimes limiting fine-grained quality assessment. To address this limitation, we introduce a new image quality assessment (IQA) task paradigm, **grounding-IQA**. This paradigm integrates multimodal referring and grounding with IQA to realize more fine-grained quality perception, thereby extending existing IQA. Specifically, grounding-IQA comprises two subtasks: grounding-IQA-description (GIQA-DES) and visual question answering (GIQA-VQA). GIQA-DES involves detailed descriptions with precise locations (e.g., bounding boxes), while GIQA-VQA focuses on quality QA for local regions. To realize grounding-IQA, we construct a corresponding dataset, GIQA-160K, through our proposed automated annotation pipeline. Furthermore, we develop a well-designed benchmark, GIQA-Bench. The benchmark comprehensively evaluates the model grounding-IQA performance from three perspectives: description quality, VQA accuracy, and grounding precision. \nExperiments demonstrate that our proposed task paradigm, dataset, and benchmark facilitate the more fine-grained IQA application. Code will be made public.", "tldr": "", "keywords": ["Grounding", "IQA", "MLLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75c1f113d2c664b80a0fba2321a936aae7f07d05.pdf", "supplementary_material": "/attachment/84ae349663a775e9f0c61f33225f7f4afbd77c5f.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces grounding-IQA, a new task paradigm that integrates multimodal referring and grounding capabilities with image quality assessment (IQA) to enable fine-grained quality perception. The authors identify that existing MLLM-based IQA methods primarily rely on general contextual descriptions, limiting their ability to perform detailed quality assessment. To address this, grounding-IQA consists of two subtasks: GIQA-DES (grounding-IQA-description), which provides detailed quality descriptions with precise spatial locations such as bounding boxes, and GIQA-VQA (visual question answering), which focuses on quality-related questions for local image regions. The authors contribute GIQA-160K, a dataset constructed through an automated annotation pipeline, and GIQA-Bench, a comprehensive benchmark that evaluates model performance from three perspectives: description quality, VQA accuracy, and grounding precision. Experimental results demonstrate that the proposed task paradigm, dataset, and benchmark effectively facilitate more fine-grained IQA applications."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper is well-organized with a clear logical flow, making it easy for readers to understand the motivation, methodology, and contributions. The task paradigm is clearly defined with concrete subtasks (GIQA-DES and GIQA-VQA), facilitating comprehension of the proposed approach.\n\n2.  The proposed automated annotation pipeline for constructing GIQA-160K represents a valuable contribution to the field. This approach addresses the scalability challenge of obtaining fine-grained quality annotations with spatial grounding, potentially benefiting future research in grounded IQA tasks.\n\n3. GIQA-Bench provides a well-designed evaluation framework that assesses model performance from three complementary perspectives: description quality, VQA accuracy, and grounding precision. This multi-faceted evaluation approach enables thorough analysis of grounding-IQA capabilities and sets a solid foundation for future work in this area."}, "weaknesses": {"value": "1. Questionable Task Scope and Definition: The paper claims to address image quality assessment (IQA), which is a broad concept encompassing various quality dimensions such as saturation, color distortion, noise, compression artifacts, etc. However, the proposed method primarily focuses on motion blur of objects, which requires enhanced referring and grounding capabilities. This narrow focus does not adequately represent the full spectrum of IQA. The work would be more convincing if the problem were explicitly defined as \"motion blur assessment\" or \"local distortion grounding\" rather than claiming to solve general fine-grained IQA. The current framing creates a mismatch between the stated goal and actual contribution.\n\n2. Limited Generalization Evidence: Even for motion blur assessment, the method's scope appears limited. Most examples showcase salient objects (e.g., people, animals, vehicles), but it remains unclear whether the approach generalizes to other scenarios such as text blur, background element blur, or subtle quality degradations in non-salient regions. The proposed benchmark does not sufficiently demonstrate the model's generalization capability across diverse object categories and blur scenarios, raising concerns about practical applicability.\n\n3. Lack of Evaluation on Public Benchmarks: A critical experimental gap is the absence of evaluation on established public IQA benchmarks. The authors only test their trained model on the self-proposed GIQA-Bench, which limits the ability to assess the method's performance relative to existing approaches and raises questions about potential dataset-specific overfitting. Evaluation on standard IQA datasets would provide stronger evidence of the method's effectiveness and facilitate fair comparison with prior work."}, "questions": {"value": "1. The automated annotation process relies on an existing MLLM (Q-Instruct) to label blur regions, which raises concerns about robustness and error propagation. Can the authors provide evidence demonstrating the reliability of this approach? A potentially more robust alternative would be to synthesize the dataset using pristine images and applying controlled blur degradations at different levels, which would provide natural bounding box annotations and standardized quality labels. How does the current approach compare to such synthetic data generation methods?\n\n2. The paper lacks detailed description of how data contamination between GIQA-160K and GIQA-Bench is prevented. Can the authors explicitly explain the data split strategy and provide evidence (e.g., image similarity checks, source verification) to ensure that the benchmark is completely disjoint from the training data?\n\n3. Given that models like Q-Instruct already possess strong IQA capabilities, wouldn't it be more efficient to enhance their grounding abilities rather than training a new model from scratch? Can the authors justify this design choice and discuss whether a fine-tuning or adapter-based approach on existing IQA-capable MLLMs might achieve comparable or better results with fewer resources?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1dNiHUmnEk", "forum": "yEpE0QPpf8", "replyto": "yEpE0QPpf8", "signatures": ["ICLR.cc/2026/Conference/Submission4351/Reviewer_9RGv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4351/Reviewer_9RGv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761311556336, "cdate": 1761311556336, "tmdate": 1762917311180, "mdate": 1762917311180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new IQA paradigm named Grounding-IQA. By combining multimodal referring/grounding with IQA, the framework achieves more fine-grained image quality evaluation. The authors design two tasks, GIQA-DES and GIQA-VQA, and construct an automated annotation pipeline to build a dataset named GIQA-160K. The authors also propose GIQA-Bench for performance evaluation. Both quantitative and qualitative results demonstrate the effectiveness of the proposed Grounding-IQA."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introducing multimodal grounding into IQA is a reasonable idea that aligns with human assessment logic. It also improves interpretability. \n2. The dataset GIQA-160K and the benchmark GIQA-Bench are well-designed contributions to the community. The automated pipeline is well-structured, which supports further research.\n3. Both quantitative and qualitative results validate the effectiveness of the proposed Grounding-IQA. Comparisons with various MLLMs (general, grounding, IQA) further confirm its advantages.\n4. Additional experiments in the supplementary material, including traditional score-based IQA tasks, user studies, and downstream tasks, provide further evidence of effectiveness.\n5. Ablation studies and visualizations support the validity of the designed pipeline and dataset.\n6. The paper is well written, with extensive figures, tables, and pseudocode that facilitate understanding.\n7. The supplementary material includes detailed implementation information, visualizations, and analytical results, making the content comprehensive."}, "weaknesses": {"value": "1. Although implementation details of the pipeline are provided, some settings lack explanation or analysis. For example, why the discrete coordinates are set to 20×20, and why the number of tasks differs in GIQA-Bench?\n2. The work applies Llama-3 for evaluation. Considering the existence of more advanced models such as Llama-4, using them could provide more accurate evaluation results.\n3. The impact of dataset scale on performance is not investigated. It is not certain whether 160K data is necessary."}, "questions": {"value": "1. Explain the considerations of specific parameter settings in the pipeline.\n2. Evaluate LLM-Score and Acc (W) using Llama-4 and analyze the results.\n3. Conduct ablation studies on the dataset (GIQA-160K) size."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mldjQk9h1P", "forum": "yEpE0QPpf8", "replyto": "yEpE0QPpf8", "signatures": ["ICLR.cc/2026/Conference/Submission4351/Reviewer_hAUf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4351/Reviewer_hAUf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479922538, "cdate": 1761479922538, "tmdate": 1762917310863, "mdate": 1762917310863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Grounding-IQA, a new paradigm for Image Quality Assessment (IQA) that couples spatial grounding with quality reasoning so models can locate and describe the local regions responsible for perceived degradations. It introduces two tasks: GIQA-DES (quality description with precise locations) and GIQA-VQA (region-aware quality QA where questions/answers include coordinates). To train and evaluate, the authors build GIQA-160K via an automatic pipeline and a 100-image evaluation set GIQA-Bench that scores description quality, VQA accuracy, and localization. Fine-tuning several mainstream MLLMs on GIQA-160K yields consistent gains on GIQA-Bench over generic MLLMs, pure grounding models, and pure IQA models, indicating better fine-grained perceptual quality understanding."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper integrates grounding with IQA to localize the regions causing quality defects—making it highly actionable for editing and restoration.\n2. Its end-to-end labeling/training pipeline—detection, IQA filtering, box merging, and coordinate–text fusion—is scalable and reproducible.\n3. A large training corpus and a targeted benchmark with multi-faceted metrics (generation, QA, localization) enable comprehensive evaluation.\n4. Consistent gains across multiple MLLM backbones, with ablations indicating the effectiveness of multi-task training and box-handling strategies."}, "weaknesses": {"value": "1. Fix typographical errors in the references.\n2. Some novel IQA methods like (VisualQuality-R1) should be included.\n3. Conduct user studies to evaluate how well the output descriptions align with the corresponding images."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kg6sJ8ux8O", "forum": "yEpE0QPpf8", "replyto": "yEpE0QPpf8", "signatures": ["ICLR.cc/2026/Conference/Submission4351/Reviewer_WkTJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4351/Reviewer_WkTJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578491638, "cdate": 1761578491638, "tmdate": 1762917310516, "mdate": 1762917310516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Grounding-IQA, a novel Image Quality Assessment (IQA) approach tailored for multimodal large model learning (MLLM). The framework decomposes IQA into two subtasks: (1) GIQA-DES, which performs fine-grained, subject-level quality evaluation via grounding-based descriptions, and (2) GIQA-VQA, which assesses low-level attributes at localized regions using a visual question answering paradigm. The authors further present GIQA-160K, a large-scale dataset constructed with an automated annotation pipeline, and GIQA-Bench, an evaluation benchmark that jointly measures description quality, VQA accuracy, and grounding precision."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The manuscript is clearly organized, with a strong logical flow and well-designed figures and tables that effectively illustrate the framework and experimental setup. The writing quality is generally high, facilitating easy comprehension of complex ideas. Furthermore, the benchmark construction is comprehensive and well-motivated — evaluating models from multiple dimensions (descriptive, interrogative, and spatial grounding) offers a holistic view of multimodal IQA performance."}, "weaknesses": {"value": "While the contribution of a large-scale dataset and benchmark is substantial, the main novelty resides primarily in dataset creation and annotation procedures rather than in methodological innovation. The proposed framework mainly adapts existing MLLM capabilities and fine-tuning strategies to the IQA domain. As such, the work may align more closely with a dataset or benchmark track rather than the ICLR main track, where new learning methodologies or theoretical insights are typically emphasized.\nAdditionally, there are a few minor grammatical issues throughout the text—for example, “provide” should be “provides” (line 53), and “new coordinates is” should be “new coordinates are” (line 268)."}, "questions": {"value": "1.Dataset Construction Details：The paper mentions that GIQA-160K was built through an “automated annotation pipeline.” Could the authors elaborate on how annotation quality and consistency were ensured? Was there any human verification or filtering process?How are noisy or ambiguous quality descriptions handled? Are there examples of failure cases in the annotation process?\n2.Whether human evaluation was included to validate these metrics.Without this information, it’s difficult to judge how faithfully GIQA-Bench reflects perceptual IQA quality.\nThe framework decomposes into GIQA-DES and GIQA-VQA. How much does each component individually contribute to overall IQA performance? An ablation study showing the relative importance of each subtask (and their potential synergy) would clarify whether the decomposition is truly beneficial or merely conceptual.\n4.It would be constructive if the authors could discuss potential limitations — for instance, the reliance on large-scale multimodal models that are computationally expensive to train and deploy, or the dataset’s possible bias toward certain types of degradation. A discussion on how future work might reduce these dependencies or enhance scalability would improve the paper’s outlook."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LQtuGd2p1P", "forum": "yEpE0QPpf8", "replyto": "yEpE0QPpf8", "signatures": ["ICLR.cc/2026/Conference/Submission4351/Reviewer_U2TN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4351/Reviewer_U2TN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985555090, "cdate": 1761985555090, "tmdate": 1762917310168, "mdate": 1762917310168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}