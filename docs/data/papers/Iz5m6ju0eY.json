{"id": "Iz5m6ju0eY", "number": 5170, "cdate": 1757860282421, "mdate": 1763103317583, "content": {"title": "Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment", "abstract": "Despite the recent progress, reinforcement learning (RL)-based fine-tuning of diffusion models often struggles with generalization, composability, and robustness against reward hacking. Recent studies have explored prompt refinement as a modular alternative, but most adopt a feed-forward approach that applies a single refined prompt throughout the entire sampling trajectory, thereby failing to fully leverage the sequential nature of reinforcement learning. To address this, here we introduce *PromptLoop*, a plug-and-play RL framework that incorporates latent feedback into step-wise prompt refinement. Rather than modifying diffusion model weights, a multimodal large language model (MLLM) is trained with RL to iteratively update prompts based on intermediate latent states of diffusion models. This design achieves a structural analogy to the Diffusion RL approach, while retaining the flexibility and generality of prompt-based alignment. Extensive experiments across diverse reward functions and diffusion backbones demonstrate that PromptLoop (i) achieves effective reward optimization, (ii) generalizes seamlessly to unseen models, (iii) composes orthogonally with existing alignment methods, and (iv) mitigates over-optimization and reward hacking.", "tldr": "PromptLoop is a PnP framework that achieves structural equivalence to parameter-level fine-tuning by aligning diffusion models through RL-tuned prompt refinement with step-wise latent feedback, while preserving generalization and composability.", "keywords": ["Diffusion Models", "Reinforcement Learning", "Reward Alignment", "Prompt Refinement", "Latent Feedback"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/13fd7e36b5abaabe89451fe8065c582c310deb59.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes PromptLoop, a plug-and-play RL framework that refines prompts step-wise during diffusion sampling using a policy MLLM that reads intermediate (denoised) latents. This yields a closed loop analogous to Diffusion RL while keeping the base diffusion weights frozen. The policy is trained with GRPO; to control cost, refinement is done at a sparse set of timesteps, and the authors note visual feedback is needed in training but not necessarily at inference. Experiments show modest but consistent gains over single-shot prompt engineering across standard reward/quality metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method layers on top of existing diffusion pipelines without touching generator weights, making adoption low-risk and compatible with multiple backbones/schedulers.\n\n2. Iterative, timestep-aware prompt edits leverage denoising feedback to steer trajectories, offering finer control than one-shot prompt engineering.\n\n3. Across several reward/quality metrics and backbones, the approach yields modest but reliable improvements, suggesting robustness to architectural details.\n\n4. Because it operates purely in the text channel, it can stack with DDPO/DPO/ReFL-style methods, providing additive gains without retraining the base model."}, "weaknesses": {"value": "1. The method is primarily empirical. The paper does not provide a formal analysis explaining how or when intermediate states $x_t$ contain sufficient semantic information to guide effective prompt refinement.\n\n2. Predicting meaningful guidance about $x_0$ from $x_t$ is inherently difficult—the very reason why diffusion models require multiple denoising steps. It is unclear how often the VLM can extract stable, causal cues rather than reacting to transient artifacts.\n\n4. The reported improvements over strong single-step prompt engineering baselines are modest. It is not obvious that the additional latency and system complexity are justified by the measured benefits.\n\n5. It remains unclear whether the gains arise from (i) step-wise conditioning on intermediate latents or (ii) simply learning a better global rewrite policy with more compute. The current ablations do not fully disentangle these effects.\n\n6. The approach relies heavily on learned preference models (e.g., ImageReward, HPS), which raises concerns about potential reward hacking or overfitting to specific evaluation models. Human evaluation is minimal."}, "questions": {"value": "1.  When does it help most? Break down results by prompt type (composition, fine-grained attributes, rare objects). If the method mainly helps with certain failure modes (e.g., compositional binding), highlight that.\n\n2.  How does performance change with perturbed or biased reward models? Any evidence of reward gaming (e.g., aesthetics gains but fidelity drops)?\n\n3. Provide qualitative “prompt trajectories” across timesteps, and analyze which edits persist to the final image vs. vanish.\n\n4. Report end-to-end generation time/VRAM for common configs and the marginal cost per additional edit step."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "onSP0tPWHZ", "forum": "Iz5m6ju0eY", "replyto": "Iz5m6ju0eY", "signatures": ["ICLR.cc/2026/Conference/Submission5170/Reviewer_kBiC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5170/Reviewer_kBiC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666364558, "cdate": 1761666364558, "tmdate": 1762917928047, "mdate": 1762917928047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We would like to thank the reviewers for their time and constructive feedback. After careful consideration, we have decided to withdraw our submission. We appreciate the reviewers’ efforts and thoughtful evaluations."}}, "id": "QptSa8ZC7r", "forum": "Iz5m6ju0eY", "replyto": "Iz5m6ju0eY", "signatures": ["ICLR.cc/2026/Conference/Submission5170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5170/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763103316876, "cdate": 1763103316876, "tmdate": 1763103316876, "mdate": 1763103316876, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper formulates T2I prompt engineering as a multi-step decision problem, where the T2I model serves as the environment and intermediate noisy images x_t provide feedback during sampling. Experiments show this approach outperforms both single-step prompt engineering and traditional RL methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Effectiveness: Figure 4 and Table 2 demonstrate clear improvements over existing methods like RePrompt\n2. The method addresses an important baseline (prompt engineering) widely used in applications like nano banana, showing significant potential"}, "weaknesses": {"value": "The core concern is that the VLM receives noisy images x_t as input. While the paper uses approximation methods to predict x_0 from x_t, these approximations are known to be unreliable. Direct input of x_t to VLM is also problematic since most existing VLMs haven't seen noisy data during training, potentially causing severe out-of-distribution (OOD) issues.\n\nThe handling of this critical aspect appears highly empirical and lacks sufficient theoretical guarantees. Have the authors observed such phenomena? It would be valuable to provide concrete examples showing:\n\n- Actual images fed to the VLM during sampling\n- Corresponding VLM prompt refinements\n- Visual demonstration of the refinement process\n\nMissing Comparisons:\n\nThe paper lacks head-to-head comparison with simple prompt engineering. A more compelling evaluation would show performance vs. number of VLM refinement steps (where 1 step = traditional method), better highlighting PromptLoop's necessity over existing approaches."}, "questions": {"value": "The idea is interesting but needs stronger empirical validation of the noisy input handling and clearer demonstration of when multi-step refinement is truly beneficial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lUft6tZ3q1", "forum": "Iz5m6ju0eY", "replyto": "Iz5m6ju0eY", "signatures": ["ICLR.cc/2026/Conference/Submission5170/Reviewer_8yXn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5170/Reviewer_8yXn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987421207, "cdate": 1761987421207, "tmdate": 1762917927721, "mdate": 1762917927721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PromptLoop, a plug-and-play framework for aligning text-to-image diffusion models with various reward functions using reinforcement learning (RL). Instead of directly fine-tuning the diffusion model's weights, PromptLoop trains a separate multimodal large language model (MLLM) to act as a policy that iteratively refines the input prompt at different steps of the diffusion sampling process. The MLLM receives feedback from the intermediate latent states of the diffusion model, allowing for a closed-loop refinement process that is structurally analogous to direct RL-based fine-tuning of the diffusion model itself. The authors argue that this approach retains the benefits of prompt-based alignment while achieving more effective reward optimization and mitigating issues like reward hacking. The effectiveness of PromptLoop is demonstrated through extensive experiments on single and composite reward functions across different diffusion models like SD1.5 and SDXL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The core idea of using an MLLM to perform stepwise prompt refinement based on latent feedback is innovative. It cleverly bridges the gap between direct parameter fine-tuning (like DDPO) and feed-forward prompt optimization methods. The formulation of this process as a Markov Decision Process (MDP) where the prompt is the \"action\" is elegant and provides a solid theoretical grounding.\n- The paper presents a thorough evaluation across multiple diffusion backbones (SD1.5, SDXL, SDXL-turbo), various baseline methods (DDPO, ReFL, RePrompt), and different reward settings (single reward like ImageReward, and composite rewards). \n- The paper provides qualitative evidence suggesting that PromptLoop is more robust against over-optimization and reward hacking compared to some baselines like ReFL."}, "weaknesses": {"value": "- The primary weakness of this method is the significant computational overhead introduced. Invoking a large MLLM multiple times within a single image generation pass is computationally expensive and increases inference latency substantially. The paper acknowledges this and proposes a \"sparse refinement strategy\" and an inference-time optimization where prompts are generated a priori without visual feedback. However, this optimization seems to contradict the core premise of the paper, which is the importance of latent feedback. The ablation study (Figure 6) shows that performance degrades without visual feedback.\n- The success of PromptLoop heavily relies on the capabilities of the policy MLLM (Qwen2.5-VL-3B). This introduces another large, complex model into the pipeline, which may have its own biases, failure modes, and training instabilities. The framework's performance is therefore bounded by the MLLM's ability to understand the subtle changes in noisy latent states and generate meaningful prompt modifications. This dependency makes the overall system more complex and potentially less robust than self-contained fine-tuning methods.\n- The authors did not compare with recent diffusion RL methods, such as FlowGRPO/DanceGRPO. They achieve significant reward improvement (e.g., 90+ score on GenEval), while also mitigating reward hacking by KL regularization. Compared to these methods, the improvement brought by PromptLoop is marginal.\n- The claim of \"plug-and-play\" is questionable. \"Plug-and-play\" typically refers to training-free methods, while PromptLoop requires RL tuning of MLLMs. The generalization to unseen diffusion backbones is not notable given the marginal quality improvement."}, "questions": {"value": "- Could you provide a more detailed analysis of the computational costs (both in terms of VRAM and time) during inference for PromptLoop compared to the baselines? Specifically, how much latency does performing 5 steps of prompt refinement add to the generation process with and without the a priori prompt generation strategy?\n- The ablation study in Figure 6 shows a clear performance drop when visual feedback is removed. Given that this feedback is central to your method's novelty, how do you justify the claim that the a priori generation strategy (which omits this feedback at inference) \"yields substantial generalization capability and efficiency gains while... uniquely retaining the advantages of closed-loop RL fine-tuning\"? It appears to abandon the closed-loop nature of the process.\n- Is there any advantage of adopting PromptLoop compared to advanced diffusion RL methods like FlowGRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XtwHvFOvtZ", "forum": "Iz5m6ju0eY", "replyto": "Iz5m6ju0eY", "signatures": ["ICLR.cc/2026/Conference/Submission5170/Reviewer_35hH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5170/Reviewer_35hH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050058945, "cdate": 1762050058945, "tmdate": 1762917927363, "mdate": 1762917927363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a plug-and-play prompt refinement framework -- PromptLoop, which uses latent feedback (intermediate states) rather than just fixed prompts. It shows that the method generalizes across unseen diffusion models (since prompts transfer) and composes orthogonally with other alignment methods. It also provides ablations showing the benefit of step‐wise prompt updates (rather than single static prompt) and of using latent feedback."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written, and the idea is clear. As the prompt policy is model‐agnostic, it can in principle be applied to unseen diffusion models — a major practical benefit. Also framing prompt refinement as a sequential policy interacting with intermediate latent states is a conceptually neat bridge between prompt‐engineering and full model fine‐tuning. The experiments are suitably reasonable."}, "weaknesses": {"value": "Several questions:\n\n(1) What about the failure cases?\n\n(2) The idea can also been applied to other methods, e.g., DPO. See the paper Fine-Tuning Diffusion Generative Models via Rich Preference Optimization, arXiv:2503.11720, Zhao et al., for using the VL feedback to prompts for data curation and generation."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mq2g9dSzbz", "forum": "Iz5m6ju0eY", "replyto": "Iz5m6ju0eY", "signatures": ["ICLR.cc/2026/Conference/Submission5170/Reviewer_bD7z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5170/Reviewer_bD7z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762365285528, "cdate": 1762365285528, "tmdate": 1762917926885, "mdate": 1762917926885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}