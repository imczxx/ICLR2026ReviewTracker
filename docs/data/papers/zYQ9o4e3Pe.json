{"id": "zYQ9o4e3Pe", "number": 5443, "cdate": 1757910216012, "mdate": 1759897974901, "content": {"title": "Unifying Counterfactual Data Augmentation and Architectural Inductive Biases in Offline Reinforcement Learning", "abstract": "Transformer-based models have recently achieved strong results in offline reinforcement learning by casting decision-making as sequence modeling. However, when trained purely on fixed datasets, they are prone to causal confusion: reliance on spurious correlations that predict reward in the data but do not reflect the true causal mechanisms of the environment. This issue is exacerbated by the weak inductive bias of Transformers, whose global attention is not aligned with the Markovian and causal dependencies of decision processes. We introduce the Unified Causal Transformer (UCF), a framework that strengthens both the data and the model with causal consistency. On the data side, UCF employs a causal reward model to abduce exogenous factors and a counterfactual state generator to produce reward-preserving augmentations, yielding counterfactual trajectories that expose causal variability absent in observational data. On the model side, UCF integrates a causally structured hybrid architecture that combines disentangled convolutional encoders for local dynamics with supervised attention for global reasoning, guiding the model to allocate representational capacity according to true causal dependencies. We evaluate UCF on two distinct sequential decision-making tasks—robotic control and recommendation—and demonstrate consistent gains in robustness and generalization over Transformer-based baselines. These results highlight the importance of causal consistency in both data and architecture for reliable offline policy learning.", "tldr": "", "keywords": ["Causality", "Offline Reinforcement Learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96854620cfdb3487843d5b0ffde4893fd7fb6678.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper discusses the issue of poor causal understanding of incumbent transformer-based solutions for offline reinforcement learning tasks. The authors state the issue of spurious correlations due to weak inductive biases of the transformer architecture that does not take into account the markovian and causal dependencies of their operating environments. As a solution to this issue, the authors introduce UCF framework which employs a causal reward model and a counterfactual data generator to address this problem both from a data and model perspective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written, and easy to follow.\n2. The paper presents a way to model the problem of Offline RL as a SCM, and introduces methods to apply it to problems via Counterfactual State Generators and a Counterfactual Reward Model.\n3. Empirical Evaluation of the proposed method along with comparison against competing solutions has been provided on multiple datasets.\n4. Ablation study of each introduced component has been provided."}, "weaknesses": {"value": "Weak points of the paper:\n1. The authors here claim to be modelling the exogenous variables, which are external to the system and is not observable. Exogenous factors are aleatoric, thus it is not possible to attain a point-estimate of their value as the authors claim to do via the counterfactual state generator. You may be able to model parameters of its distribution (such as the residual variance) but you certainly cannot attain a point estimate.\n2. While the authors do address the question of reward consistence, no consideration has been made about the consistency of the counterfactual transitions. The creation of the counterfactual state, s_c, implies the transition (s_c, a_t, s_t+1), however there is no guarantee for the validity of this transition and could be an impossible transition from state s_c, leading to incorrect representations being learnt by the policy model.\n3. The authors state that they use “disentangled” convolutional encoders. Disentangled encoders and representations have a very specific definition in the literature, in that the learnt representations have distinct subsets of latent variables that maps to specific abstractions/concepts of the input, allowing for granular control (see Locatello et. al.[1]). The authors do not provide a clear definition of what they mean by disentangled encoders and by the looks of it it seems like it just means that separate encoders are used for state, RTG and action, which does not match the definition of disentanglement with respect to encoders and representations.\n4. It also seems that the strict Markovian condition that has been enforced via the causal attention supervision contradicts the claim that the multi-head self-attention block is performing global reasoning regarding the policy. This is in contrast to the Decision Transformer, where the offline RL setting is modelled as a sequence-modelling and has access to the entirety of the previous states, actions and RTGs. Moreover, from the ablation study, any large introduction of L_mask (\\lambda > 0.1) seems to hinder performance. Thus, it seems to suggest that it is a better idea to do a shorter time-horizon sequence modelling of the offline RL task, rather than enforcing the Markovian condition or looking at the full time-horizon as is the case with the standard Decision Transformer. Therefore, it raises a question of whether the Causal Attention Supervision is needed at all, and if we would be able to obtain similar performance by just using the DT formulation with a shorter window, providing the best of both worlds without the added complexity.\n\n[1] Locatello, Francesco, et al. \"Weakly-supervised disentanglement without compromises.\" International conference on machine learning. PMLR, 2020."}, "questions": {"value": "Thus, it would be useful to obtain the answers to the following questions:\n1. Why are the encoders considered to be disentangled?\n2. It seems that the Causal attention supervision is counterproductive and the same could be obtained via a shorter horizon DT formulation. Would it possible to demonstrate this with a set of updated experiments?\n\nGiven the above reasons, and my view that there are conceptual errors in the work, I recommend a rejection of the work. The idea presented in the paper is quite interesting and would be a nice contribution, however, I currently have a few conceptual concerns (as mentioned above) and questions surrounding one of the introduced components (Causal Attention Supervision)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VQDxaS98qk", "forum": "zYQ9o4e3Pe", "replyto": "zYQ9o4e3Pe", "signatures": ["ICLR.cc/2026/Conference/Submission5443/Reviewer_L8qK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5443/Reviewer_L8qK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922434081, "cdate": 1761922434081, "tmdate": 1762918064990, "mdate": 1762918064990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UCF (Unified Causal Transformer), a framework for strictly offline RL that aims to mitigate causal confusion in sequence‑modeling policies by jointly augmenting the dataset with counterfactual, reward‑preserving states and imposing causal architectural inductive bias on the policy network. On the data side, the authors train a conditional VAE as Causal Reward Model (CRM) to abduce exogenous factors, and a Counterfactual State Generator (CSG) to proposes edited states subject to acceptance gates. On the model side, UCF replaces a full Transformer with a hybrid “local‑global” policy: depthwise 1‑D convolutions for local information aggregation including state and action, fused into a final multi‑head self‑attention block trained with a causal mask loss that encourages action tokens to attend predominantly to their causal parents. Experiments on D4RL locomotion and AntMaze show small but consistent gains over Q-learning and Decision Transformer (DT) baselines; a spurious‑feature stress test suggests improved robustness. The approach is also tested on three recommender datasets (KuaiRand, KuaiRec, VirtualTB), where it outperforms DT‑style baselines. \n\nThis work builds on Decision Transformer by recognizing that generic attention lacks the right inductive bias for MDPs, and it complements causal augmentation methods (e.g., MoCoDA, CAIAC) by proposing an offline‑only, reward‑preserving edit scheme without assuming factored entities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper presents a clear problem statement: addressing causal confusion and weak inductive bias of decision transformers in offline RL. \n\n* This paper provides a unified framework for both data and model. The ablations are convincing about complementarity. \n\n* This method does not rely on environment interaction and graph assumptions for factorization. \n\n* The breadth of evaluation is good, covering locomotion, navigation, and recommendation datasets, demonstrating its generality."}, "weaknesses": {"value": "* As the base of the framework, the causal reward model lacks identifiability support, specifically assumptions required to make it identifiable. The authors may refer to the identification conditions in the nonlinear ICA literatures for references [1][2].\n\n* The direct replacement of $s_{t}$ and keeping $s_{t+1}$ might create impossible triplets ( $s_{t}^{c}$, $a_{t}$, $s_{t+1}$ ). Learning a dynamics model as an extra acceptance gating would further help keep the transition feasible and consistent.\n\n* While the mask loss is a helpful bias, the causal mask is not very flexible and general. It is self-adaptive and requires manual tuning for different environments (as authors also find in Figure 2 that it is a bit sensitive ). The causal mask is purely pre-defined, which is a bit rigid and it might face information loss in long context modeling.\n\n[1] Khemakhem, Ilyes, et al. \"Variational autoencoders and nonlinear ica: A unifying framework.\" International conference on artificial intelligence and statistics. PMLR, 2020.\n\n[2] Yao, Weiran, Guangyi Chen, and Kun Zhang. \"Temporally disentangled representation learning.\" arXiv preprint arXiv:2210.13647 (2022)."}, "questions": {"value": "* Could authors elaborate that, in the state edition, what's the intuition behind using ( $s_{t}$, $a_{t}$,  $z_{t}$) as input? Why all of them are necessary to include here?\n\n* How often are augmented transitions dynamics‑inconsistent? Can you estimate the fraction of accepted that violate a crude learned dynamics model? If this fraction is non‑trivial, how sensitive are policy returns to filtering them out?\n\n* Beyond Markov/faithfulness/minimality, what inductive biases make z a useful proxy for exogenous factors (not just a nuisance latent)? Could you strengthen identifiability in the sense of nonlinear ICA? \n\n* If you replace the reward‑derived bit with a sensor distractor correlated with reward but not deterministically constructed from it, does the same robustness ranking (DT < DC < UCF) hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rNe6aJJ41a", "forum": "zYQ9o4e3Pe", "replyto": "zYQ9o4e3Pe", "signatures": ["ICLR.cc/2026/Conference/Submission5443/Reviewer_m5nA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5443/Reviewer_m5nA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966842710, "cdate": 1761966842710, "tmdate": 1762918064710, "mdate": 1762918064710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Unified Causal Transformer (UCF), a framework for offline reinforcement learning (RL) that integrates counterfactual data augmentation with causally structured model architectures. It addresses causal confusion in Transformer-based RL—where models rely on spurious correlations—by strengthening both data and inductive biases. On the data side, a Causal Reward Model (CRM) infers latent exogenous factors to guide a Counterfactual State Generator (CSG), creating reward-preserving counterfactual trajectories. On the model side, UCF introduces a Causally-Structured Hybrid Architecture combining disentangled convolutional encoders (for local dynamics) with causally supervised attention (for global reasoning). Experiments on MuJoCo locomotion, AntMaze navigation, and real-world recommendation datasets show that UCF achieves consistent improvements in generalization, robustness, and resistance to spurious correlations compared with state-of-the-art offline RL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and quite easy to follow.\n\n- It is quite reasonable to integrate counterfactual data augmentation and architectural causal priors into a single coherent framework for offline RL.\n\n- The paper shows clear causal grounding: Theoretical formulation via Structural Causal Models (SCM) and abduction–action–prediction reasoning provides conceptual rigor."}, "weaknesses": {"value": "- The proposed data augmentation intervenes on states rather than actions, keeping actions and inferred latent context fixed while enforcing reward preservation. This departs from the standard causal direction in RL where intervening on actions while keeping the rest fixed. I am wondering if intervening on states would lead to highly diverse trajectories. Instead, it may produce samples that lack physical or causal plausibility.\n\n- Because the augmented data are \"reward-preserving state edits\", it is unclear whether they correspond to valid trajectories under any real environment dynamics, potentially conflating causal variability with adversarial perturbation.\n\n- Any approximation error in the causal reward model directly affects the plausibility of generated counterfactual states. How to deal with this issue?\n\n- The causal supervision and augmentation scheme remain untested in more complex or high-dimensional domains where the Markov structure or causal graph is less explicit.\n\n- From the experimental results (Tables 1,2&3), it seems that the performance of the proposed method does not show obvious advantage compared to the baselines, with a bit improvement only."}, "questions": {"value": "- What is the causal justification for intervening on states rather than actions?\n\n- How does the method ensure that reward-preserving state perturbations remain within the true support of the environment’s dynamics? I do not think the proposed mitigation method is valid enough to guarantee this.\n\n- Would performing standard action-level interventions yield stronger or more interpretable results?\n\n- Could the proposed counterfactual augmentation be reframed as a form of invariance or robustness regularization rather than genuine counterfactual reasoning?\n\n- In real-world domains where the causal structure is unknown or high-dimensional, how would the authors obtain or approximate the causal masks required for attention supervision?\n\n- Could the causal supervision be learned automatically or relaxed into a differentiable causal-discovery process?\n\n- How sensitive is the model’s performance to mis-specified causal masks or incomplete parent sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3rAl03HzIp", "forum": "zYQ9o4e3Pe", "replyto": "zYQ9o4e3Pe", "signatures": ["ICLR.cc/2026/Conference/Submission5443/Reviewer_D5mJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5443/Reviewer_D5mJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975418910, "cdate": 1761975418910, "tmdate": 1762918064322, "mdate": 1762918064322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of \"causal confusion\" in Transformer-based offline reinforcement learning, where models learn to exploit spurious correlations in a fixed dataset instead of the true causal mechanisms of the environment. The authors propose the Unified Causal Transformer (**UCF**), a framework that addresses this issue from both a data and a model perspective.\n\nThe key contributions are:\n\n1. **Counterfactual Data Augmentation**: A novel pipeline for data augmentation in a strictly offline setting. It first trains a Causal Reward Model (CRM) using a CVAE to abduce latent exogenous factors from transitions. It then uses a Counterfactual State Generator (CSG) to produce new, reward-preserving states by editing existing states while keeping the action and inferred exogenous factors constant. This exposes the model to causal variations not present in the observational data.\n\n2. **Causally-Structured Hybrid Architecture**: A hybrid model that combines modality-specific 1D convolutions to capture local Markovian dynamics with a final global self-attention layer for policy reasoning.\n\n3. **Causal Attention Supervision**: The global attention mechanism is explicitly supervised with a causal mask, forcing attention heads to focus only on the direct causal parents of an action (the current state and return-to-go), thereby instilling a strong and correct inductive bias.\n\n\nThe authors validate UCF on standard D4RL locomotion and Antmaze navigation tasks, as well as on sequential recommendation datasets. The results show consistent performance gains over state-of-the-art baselines and demonstrate significantly improved robustness against engineered spurious correlations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  Novel and Principled Framework: The core idea of jointly enforcing causal consistency at both the data level (via counterfactuals) and the model level (via architectural priors and supervision) is elegant, powerful, and novel. It provides a holistic solution to the well-known problem of causal confusion.\n\n2. Robust Offline Counterfactual Generation: The proposed mechanism for generating reward-preserving counterfactuals (CRM + CSG) is well-designed for the strictly offline setting. The inclusion of a \"move band\" to ensure edits are neither trivial nor off-manifold is a thoughtful and critical detail that enhances the quality of the augmentation.\n\n3. Direct and Effective Inductive Bias: The causally supervised attention mechanism is a very direct and effective way to bake the known causal structure of an MDP into a Transformer. This is a much stronger and more targeted inductive bias than simply using convolutions for locality.\n\n4. Ablation clarity: The study isolates the contributions of counterfactual augmentation, architectural causal supervision, and mask weight λ, providing clear insights."}, "weaknesses": {"value": "1. **Dependence on CRM accuracy (i.e., accurate reward modeling)**: The success of the counterfactual augmentation hinges on the fidelity of the CRM; inaccuracies can propagate to poor or unrealistic augmentations.\n\n2. **Evaluation domains**: Despite covering robotics and recommendation, all tasks rely on relatively standard benchmarks; evaluation on complex partially observable or high-dimensional real-world domains would further strengthen claims.\n\n3. **No explicit causal discovery**: The causal mask structure is predefined (state $\\rightarrow$  action, RTG $\\rightarrow$ action). The method does not learn or adapt causal graphs dynamically, which may limit scalability.\n\n4. **Fixed Causal Graph for Supervision** (building on [2+3] above): The supervised attention relies on a predefined causal mask based on the known MDP structure (i.e., $a_t$ depends on $s_t$ and $G_t$). While effective, this may be less straightforward to apply in settings where the causal structure is unknown, more complex, or only partially observable (e.g., in POMDPs)."}, "questions": {"value": "1. How sensitive is UCF’s performance to inaccuracies in the causal mask (e.g., if an incorrect parent set is imposed)? Could the mask be learned or adapted via causal discovery methods?\n\n2. CRM Robustness: What mechanisms prevent the CVAE-based Causal Reward Model from overfitting to dataset noise or producing implausible latent factors? Was any regularization or early stopping based on held-out validation used?\n\n3. Did you observe cases where the augmentation harmed performance (e.g., under poor reward modeling or narrow data coverage)? If so, how were these mitigated?\n\n4. The counterfactual generation pipeline is a key component. Could you provide some insight into the acceptance rate of the generated states? How does the choice of the move band (p_low, p_high) and the reward tolerance affect both the acceptance rate and the diversity of the final augmented dataset?\n\n### Minor Typos\n\n- Line 446: ref. missing for ACE"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fewZnvaUZI", "forum": "zYQ9o4e3Pe", "replyto": "zYQ9o4e3Pe", "signatures": ["ICLR.cc/2026/Conference/Submission5443/Reviewer_okqb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5443/Reviewer_okqb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241382904, "cdate": 1762241382904, "tmdate": 1762918063927, "mdate": 1762918063927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}