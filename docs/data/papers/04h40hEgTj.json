{"id": "04h40hEgTj", "number": 23149, "cdate": 1758340263445, "mdate": 1759896830101, "content": {"title": "Decomposing Prediction Mechanisms for In-context Recall", "abstract": "We introduce a new family of toy problems to explore challenges with long context learning and associative recall in transformer models. Our setup involves interleaved segments of observations from randomly drawn linear deterministic dynamical systems. Each system is associated with a discrete symbolic label that must be learned in-context since these associations randomly shuffle between training instances.\n\nVia out-of-distribution experiments we find that learned next-token prediction for this toy problem involves at least two separate mechanisms. One \"label-based\" mechanism uses the discrete symbolic labels to do the associative recall required to predict the start of a resumption of a previously seen system's observations. The second ``observation-based'' mechanism largely ignores the discrete symbolic labels and performs a prediction based on the state observations previously seen in context. These two mechanisms have different learning dynamics: the second mechanism develops much earlier than the first.\n\nThe behavior of our toy model suggested concrete experiments that we performed with OLMo training checkpoints on an ICL translation task. We see a similar phenomenon: the model learns to continue a translation task in-context earlier than it decisively learns to in-context identify the meaning of a symbolic label telling it to translate.", "tldr": "We introduce a new family of toy problems that combine features of linear-regression-style continuous in-context learning (ICL) with discrete associative recall and find distinct learning dynamics for different prediction mechanisms.", "keywords": ["emergence", "in-context learning", "time-series", "associative recall", "learning dynamics"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/874dd26fa4acf6f26e690461d6232071b158fd84.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors aimed at creating a family of toy models for exploring the known challenge of long-context learning for LLM. The proposed toy model have different time series data interleaved with distinct labels. The authors found that LLM developed two distinct learning mechanisms in performing next token prediction on the toy model. The first mechanism focuses on identity regime change in the data, and the second one perform next token prediction based the data observed. The two mechanism also seem to follow different learning dynamic, and the second one developed earlier than the first."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The author aimed at a crucial problem in understanding LLM, namely the challenge of  challenge of long-context learning.\n2. The designed toy model indeed is simple in structure on one hand, but capturing some nature of human languages on the other hand. \n2. Quite extensive numerical experiments are conducted."}, "weaknesses": {"value": "1. The main message the author intended to convey is not very clearly presented. It appears to be the discovery of the capability of Transformers on developing distinct mechanisms for predicting different token positions in a single task via a study of a specially designed toy model. Although related statements in various places of the paper do not seem to always be precisely the same.  While the first two hypotheses are shown not to hold, the language in the description of the conjecture and its confirmation is very vague and puzzling.  Moreover, to confirm or deny such s strong conjecture, a much more thorough set of experiments needs to be designed and carried out, not simply an observation continuation and new initiation can not be distinguished. \n\n2. The connections between the observations, existence of distinct learning mechanism to the challenge of long-context learning are not explicitly stated."}, "questions": {"value": "1. I feel that the critical question is that why and how the understanding of this toy model can help us understand the capability of long  long-context learning.of LLM. I don't see this is addressed in the paper. \n2. It appears that at any fixed time of learning, there will be much more time series data than discrete symbolic labels, is that the reason that the second mechanism develops much earlier than the first. \n2. How are the issues presented in Sec. 5(discussion) related to the problems and contributions presented in introduction? I failed to see the clear connections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xGetJAj2RR", "forum": "04h40hEgTj", "replyto": "04h40hEgTj", "signatures": ["ICLR.cc/2026/Conference/Submission23149/Reviewer_Rust"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23149/Reviewer_Rust"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761347270875, "cdate": 1761347270875, "tmdate": 1762942533630, "mdate": 1762942533630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ICL is a well studied phenomenon in the ML community. Various tasks, such as MQAR and regression, have been proposed to test the ICL capabilities of models in the past. The beauty of each is it both tests the model's ability to perform lookup operations (MQAR) and more complex operations only depending on the previous token (regression). This work combines these into a task using linear dynamical systems, where each system is marked in-context by a specific query label. Two observations are seen: the model uses the open-query label to perform the correct task, and the model uses past elements in the sequence to continue the task. These observations are validated by configuring the systems and states to align, allowing for a clear test of these observations in a controlled setting. Further investigating that these different mechanisms exist within these learned models, a mechanistic study is conducted separating out two circuits from within the model that have markedly distinct performance on the two different subtasks of recall and execution."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This new tasks used to test ICL is appealing. It provides a nice link between standard ICL problems while keeping almost everything continuous, hence interpretable. The dynamics are quite intuitive yet retain significant depth to make an interesting analysis.\n- The experiments investigate a variety of different interventions to test the hypotheses H1 and H2 and show, clearly, that models will learn to perform the correct task on the first token after the new-task identifier, and then relying on previous outputs to generate more tokens.\n- The results regarding the disparity between 1-after and 2-after display a very interesting aspect of how these models learned to solve a task composed of a mixture of regression and associative recall\n- The circuit analysis added depth into the difference between these two mechanisms as two truly separate aspects of the model.\n- The writing is very clear, with claims and hypotheses which are most relevant to the reader highlighted in boxes, along with the distinct mechanisms all given separate colors for the reader to decern them.\n- The toy model architectures are cleanly described in Appendix I."}, "weaknesses": {"value": "- Much of the work (specifically the figures) is focused on the training dynamics of these model. While interesting, and should certainly be highlighted, the claims of the resulting model having these two distinct mechanisms to understand these linear-dynamics inputs typically are most important at the very end of training. This wasn't particularly central in the played results and rather had to be pulled out from the training dynamics\n- The paper focuses on one specific task and found a property about ICL performance on this specific hybrid task. There was not any investigation into whether these same behaviors can be seen with other tasks (possibly other hybrid tasks), resulting in a possibly narrow applicability\n- A few (and only a few) task design choices were not clearly described (see questions)"}, "questions": {"value": "- Is there any reason for selecting 5 dimensions? Did this coincide with the model able to learn it at the scales tested, where too much greater led to bad performance while any smaller made the task incredibly easy?\n- Why train OLMo specifically and not some other language model like LLaMa? Was there any investigation into these tasks trained using different foundation models?\n\n- Why specifically this setup? Did the close tokens help the model find the last token in the output of this current sequence? Why not always use the last token of any of the systems to be the input of the next one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Akv2lLYAWU", "forum": "04h40hEgTj", "replyto": "04h40hEgTj", "signatures": ["ICLR.cc/2026/Conference/Submission23149/Reviewer_7gvh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23149/Reviewer_7gvh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929808655, "cdate": 1761929808655, "tmdate": 1762942533400, "mdate": 1762942533400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies mechanisms through which transformers can perform in-context prediction. \nIn models trained on a novel synthetic task, the paper discovers two mechanisms (\"label-based\" and “observation-based”).\nA further experiment on OLMo checkpoints provides further evidence from a translation task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a new family of well-specified toy problems to study mechanisms used in Transformers for in-context recall"}, "weaknesses": {"value": "- The setup, as motivated in Section 1.1, appears quite specific. I was missing a motivation of why the setup is of broader relevance or interest, e.g. to language models, or the transformer architecture, etc. This is a concern especially as the paper mainly concerns empirical studies of toy models trained on a toy task.\n- Interpretation of Section 3: Section 3.3, line 400: \"0% edge overlap between the 1-after query and 2-after query circuits\": As far as I understood the description in the section, the circuit finding strategy used here imposes no pressure towards overlapping circuits. Hence, it is conceivable that the reason for 0% edge overlap is just that the model has multiple redundant mechanisms for the two tasks, and the circuit finding algorithm happened to find different mechanisms when run on the two tasks. I'd appreciate if the authors can comment on this.\n- Section 4: I didn't understand the task used here. On the one hand, the task is English-to-Spanish translation, on the other hand \"we also change our analogous natural language setup to have in-context labels with no semantic meaning\" What does this mean? How does this relate to the examples given in Appendix G?"}, "questions": {"value": "I'd appreciate if the authors can clarify if any of the weaknesses listed above may result from misunderstandings on my end. I'm happy to reevaluate my score on the basis of the response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5QYEAhhMIz", "forum": "04h40hEgTj", "replyto": "04h40hEgTj", "signatures": ["ICLR.cc/2026/Conference/Submission23149/Reviewer_sGTe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23149/Reviewer_sGTe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936205566, "cdate": 1761936205566, "tmdate": 1762942533204, "mdate": 1762942533204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new methodology to study in-context behaviors in transformer models. They create a sequence which consists of segments of observations drawn from different distributions. Each segment begins with a special token, termed \"symbolic punctuation label\" (SPL), so model must choose between inferring the next observation based on the SPL or the observations in the context. They provide experimental evidence suggesting that the latter choice develops earlier in training than the first."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a new synthetic needle in a haystack task, which is interesting and novel. The experimental design of using 1) misdirected SPL and 2) synchronized observation are well-motivated. \nThey also extend their analysis on OLMo checkpoints on translation tasks, which ground their findings with real-world evidence."}, "weaknesses": {"value": "I find that many of the claims in the paper could be considerably strengthened and simplified. \n* I am not fully convinced that the label-based recall hypothesis (H1) is decisively ruled out. One could explain Figure 1b) simply from the fact that the model sees more observation tokens (the 1-after and 2-after query) than the open SPL token itself. It would be valuable to test whether increasing the representational weight or length of the SPL (e.g., by replacing each open-label with a multi-token sequence or embedding-enlarged symbol) causes the model to rely more on label information. \n* Figure 1a is not clear. the query and misdirection tokens (the parentheses) are identical. \n* The results on observation-based recall degrades with more systems (relative to the performance of 1-after query) is quite surprising and not well explained. Section 3 devotes much time to validating H1 and H2, but in my opinion does not spend enough discussion on how to explain the phenomena in Figure 3. For example, whether it reflects interference among observation traces, reduced signal-to-noise in embedding space, or limitations of attention span. A deeper discussion or ablation (e.g., varying the degree of interleaving or the correlation among systems) would strengthen the empirical interpretation.\n* The paper concludes that “the model mostly leverages mechanistically different learned mechanisms for consecutive tokens,” but it remains unclear how many mechanisms exist in total (two, or more?). The pruning analysis isolates only two (corresponding to the 1-after and 2-after query positions), and while these show 0% edge overlap, this alone does not establish that the model’s behavior decomposes neatly into exactly two circuits. Clarifying the scope of this claim would strengthen the mechanistic argument of the paper."}, "questions": {"value": "The paper would benefit from an ablation on the length of each interleaved segment. Since segment length determines how many observation tokens are available for inferring each system’s dynamics, varying it could reveal whether the distinction between label-based and observation-based recall arises from token exposure rather than a fundamentally different mechanism. For instance, longer continuous segments might strengthen observation-based continuation, while shorter segments could force greater reliance on symbolic labels."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fvnfFvblDP", "forum": "04h40hEgTj", "replyto": "04h40hEgTj", "signatures": ["ICLR.cc/2026/Conference/Submission23149/Reviewer_hXkf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23149/Reviewer_hXkf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975940060, "cdate": 1761975940060, "tmdate": 1762942532938, "mdate": 1762942532938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}