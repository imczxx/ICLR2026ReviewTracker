{"id": "hwZi9LOTwS", "number": 9036, "cdate": 1758108181140, "mdate": 1759897746968, "content": {"title": "Spatial-Temporal-Spectral Unified Modeling for Remote Sensing Dense Prediction", "abstract": "The proliferation of multi-source remote sensing data has propelled the development of deep learning for dense prediction, yet significant challenges in data and task unification persist. Current deep learning architectures for remote sensing are fundamentally rigid. They are engineered for fixed input-output configurations, restricting their adaptability to the heterogeneous spatial, temporal, and spectral dimensions inherent in real-world data. Furthermore, these models fail to leverage the intrinsic correlations across different remote sensing dense prediction tasks, necessitating the development of distinct models or task-specific decoders. This paradigm is also limited to a fixed set of output semantic classes that must be learned during training, where any change to the classes requires costly retraining. To overcome these limitations, we introduce the Spatial-Temporal-Spectral Unified Network (STSUN) for unified modeling. STSUN can adapt to input and output data with arbitrary spatial sizes, temporal lengths, and spectral bands by leveraging their metadata for a unified representation. Moreover, STSUN unifies disparate dense prediction tasks within a single architecture by conditioning the model on trainable task embeddings. STSUN enables flexible prediction across multiple sets of semantic categories by integrating trainable category embeddings as metadata. Extensive experiments on multiple datasets with diverse Spatial-Temporal-Spectral configurations in multiple scenarios demonstrate that a single STSUN model effectively adapts to heterogeneous inputs and outputs, unifying various dense prediction tasks and diverse semantic class predictions.", "tldr": "Proposed a unified remote sensing dense prediction model that adapt to arbitrary input and output, multiple tasks and semantic classes.", "keywords": ["Remote Sensing", "Dense Prediction", "Data Unification", "Task Unification", "Category Unification"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36bb6111ff7f5d902b06b75a4bdded4fc0b0d309.pdf", "supplementary_material": "/attachment/c242e48b2710779292c557b793c4bbccfa796f3f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel framework for remote sensing dense prediction. The core contribution is a unified model capable of handling heterogeneous inputs and outputs across spatial, temporal, and spectral dimensions. Furthermore, the model unifies three distinct dense prediction tasks and supports flexible semantic categories, therefore achieving data-task-class unification. The authors conduct experiments on seven diverse datasets of building and land use/land cover scenarios, demonstrating that their single unified model not only adapts to varied data-task-class configurations but also achieves state-of-the-art performance across all of them."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.\tThe primary strength lies in its novel approach to unification. While prior work has used hypernetworks or other approaches for adaptive unification, it was typically limited to the channel dimension to handle multi-modal data [1-4]. This work innovatively connects the diversity across spatial, temporal, and spectral dimensions with the non-uniformity challenges in remote sensing concerning data, tasks, and classes. By unifying these core dimensions via a hypernetwork, the proposed method provides a natural and effective solution for a unified data-task-class framework.\n2.\tThe decision to treat the temporal dimension separately, acknowledging its unique characteristics and designing a specific unification module for it, is an interesting and convincing design choice.\n3.\tThe proposed method for unifying the spatial, temporal, and spectral dimensions of inputs and outputs appears to be general and highly adaptable, suggesting potential applicability to other research domains.\n4.\tThe paper is supported by comprehensive and convincing experimental results. In particular, the ablation studies on multi-task and multi-class unification demonstrate the necessity and benefits of the proposed unified approach.\n5.\tThe paper is well-written, clearly structured, and easy to follow.\n\n[1] Xiong, Z., Wang, Y., Zhang, F., Stewart, A. J., Hanna, J., Borth, D., ... & Zhu, X. X. (2024). Neural plasticity-inspired multimodal foundation model for earth observation. arXiv preprint arXiv:2403.15356.\n[2] Li, X., Li, C., Ghamisi, P., & Hong, D. (2025). Fleximo: A flexible remote sensing foundation model. arXiv preprint arXiv:2503.23844.\n[3] Zhang, Y., Li, W., Zhang, M., Han, J., Tao, R., & Liang, S. (2025). SpectralX: Parameter-efficient domain generalization for spectral remote sensing foundation models. arXiv preprint arXiv:2508.01731.\n[4] Sumbul, G., Xu, C., Dalsasso, E., & Tuia, D. (2025). SMARTIES: Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing Images. arXiv preprint arXiv:2506.19585."}, "weaknesses": {"value": "1.\tSome implementation details are not fully clear.\n2.\tThe unification of each dimension is formalized with notation and some equations. However, the presentation could be made even clearer and more rigorous with a more comprehensive mathematical formulation.\n3.\tMoving some of the key visualization results from the appendix to the main paper would make the results more compelling and easier for the reader to interpret.\n4.\tIt is recommended to tone down claims like \"for the first time\" in the introduction to avoid potential disputes and strengthen the paper's scholarly tone."}, "questions": {"value": "1.\tThe proposed model not only achieves data-task-class unification but also outperforms the baseline models. What are the factors contributing to this superior performance?\n2.\tIn Appendix A.1, the hypernetwork generates adaptive weights and biases for some dimensions, but only adaptive weights for others. What is the reasoning behind this design choice?\n3.\tThe model requires explicit metadata as input. How could this metadata be inferred implicitly in future work to make the model more streamlined and user-friendly?\n4.\tThis work primarily focuses on supervised models. What role could this unification framework play when extended to vision foundation models or large multi-modal models?\n\nMinor Comments\n1.\tIn Figure 2, consider adding \"×N\" to the encoder and decoder blocks to indicate that they are repeated.\n2.\tIn Figure 3, the text should be ordered from top to bottom to maintain consistency with the flow in Figure 2.\n3.\tIn Table 1, it would be helpful to include the size (e.g., number of samples) for each dataset.\n4.\tIn Table 1, \"Image Size\" should be changed to \"(H, W)\" to be consistent with the dimensional notation used in the text.\n5.\tFor all tables reporting results, consider adding arrows next to each metric to improve clarity, similar to the presentation in Table 8.\n6.\tThe layout of some equations in Figure 5 needs adjustment for better readability.\n7.\tThe positioning of Figure 6 and Figure 7 could be modified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "njLJQwWAxz", "forum": "hwZi9LOTwS", "replyto": "hwZi9LOTwS", "signatures": ["ICLR.cc/2026/Conference/Submission9036/Reviewer_tKWs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9036/Reviewer_tKWs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760791688043, "cdate": 1760791688043, "tmdate": 1762920753960, "mdate": 1762920753960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a unified framework addressing the challenge of inconsistent input and output configurations across different dense-prediction tasks in remote sensing. The proposed Spatial-Temporal-Spectral Unified Network (STSUN) integrates two key components: the Dimension Unified Module (DUM), which employs a transformer-based hypernetwork conditioned on metadata to adaptively map variable dimensions, and the Local-Global Window Attention (LGWA) module, which captures multi-scale contextual relationships. The model is designed to handle multiple dense-prediction tasks, including semantic segmentation, binary change detection, and semantic change detection, and can be trained in either single-task or multi-task settings, flexibly adapting to different spatial, temporal, and spectral domains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and practical issue in remote sensing: heterogeneity of input and output structures. Encoding spatial, temporal, and spectral configurations as metadata is a smart, scalable idea. The local-global attention design helps handle multi-resolution dependencies, and multi-task training improves performance. Experiments are extensive, and results are strong across benchmarks."}, "weaknesses": {"value": "1. Despite the strong empirical results, the core components are based on existing ideas. The DUM is a direct application of a transformer-based hypernetwork, and the LGWA is conceptually similar to other multi-scale windowed attention mechanisms found in models like the Swin Transformer or SegFormer.\n2. The paper lacks theoretical analysis or deeper insight into its decoupled unification strategy, relying solely on experimental ablation (Table 11) to justify its design.\n3. Although the appendix includes implementation details, the main exposition can be conceptually unclear. Notation is inconsistent, metadata definitions are vague, and the link between input and output dimensions must be inferred. These issues hinder a full understanding and make reimplementation challenging. For example, the notational system ($T_1$, $T_2$, $C_1$, $C_2$) is confusing, and metadata ($M_{in}$, $M_{out}$) lacks clear definition. Please clarify these structures and their correspondence.\n4. The description of data dimensions (on page 1) and Figure 2 is overly verbose and could be condensed for readability."}, "questions": {"value": "1. The \"flexible category set\" capability (Section 4.4) relies on selecting from a predefined and trainable set of class embeddings. Could the authors clarify the model's behavior with a truly 'new' or 'unseen' category not included in this predefined set? Would this scenario require retraining to add a new embedding, or can the model generalize in a zero-shot manner?\n2. The main comparison tables (e.g., Tables 2-8) compare the STSUN_unified model (trained on combined data) against SOTA methods trained on single datasets. This makes it difficult to distinguish architectural benefits from the benefits of multi-task/multi-dataset training. Could the authors add the STSUN_single results (from Table 9) to these main tables for a more direct comparison against the SOTA baselines?\n3. Please provide parameter counts or FLOPs for the STSUN model and the key baselines. This would help clarify whether the performance gains stem from the proposed architecture or from a significantly larger model capacity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NnPIEA7yVe", "forum": "hwZi9LOTwS", "replyto": "hwZi9LOTwS", "signatures": ["ICLR.cc/2026/Conference/Submission9036/Reviewer_xSfp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9036/Reviewer_xSfp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983687334, "cdate": 1761983687334, "tmdate": 1762920753338, "mdate": 1762920753338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes the Spatial-Temporal-Spectral Unified Network (STSUN), a framework designed to achieve unified dense prediction across diverse remote sensing tasks and data configurations. The authors identify key limitations in existing deep learning models for remote sensing, including fixed input-output configurations, task-specific architectures, and rigid category sets, which hinder adaptability to heterogeneous data and multi-task scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is clearly written and easy to follow, with a well-structured presentation of the problem and proposed approach.\n2.\tThe idea of unifying spatial, temporal, and spectral dimensions within a single framework is interesting and relevant to challenges in remote sensing dense prediction."}, "weaknesses": {"value": "1.\tThe unification of spatial, temporal, and spectral dimensions has already been explored in several recent remote sensing foundation models, such as RingMo-Agent [1] and Falcon [2], which aim to build unified representations across multi-platform and multi-modal data. The paper does not discuss or compare its approach with these existing large-scale models, limiting the clarity of its novelty and positioning. [1] RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning [2] Falcon: A Remote Sensing Vision-Language Foundation Model\n2.\tThe idea of flexible semantic class sets is not entirely novel. Prior works on open-vocabulary and open-set segmentation in remote sensing already address similar challenges. The paper does not discuss or position its proposed trainable category-embedding mechanism relative to these approaches, which reduces the clarity of its contribution in this context.\n3.\tThe method relies on metadata and hypernetworks to generate adaptive linear layers for unifying arbitrary spatial, temporal, and spectral dimensions. In practice, the claimed “complete unification of STS dimensions” may be constrained by variations in spatial resolution, spectral coverage, and temporal sampling intervals. The authors do not provide sufficient experimental validation to support the generalizability of this approach.\n4.\tThe feasibility of the Temporal Unified Module (TUM) is unclear. TUM fuses multi-temporal features using hypernetworks and metadata, mapping them to arbitrary output temporal lengths. For high-temporal-resolution change detection or long sequence data, such linear mappings may not adequately capture complex temporal dynamics, potentially causing information loss or performance degradation. The paper does not include experiments on long temporal sequences or ablation studies to validate TUM’s effectiveness.\n5.\tThe Local-Global Window Attention (LGWA) module uses multiple local windows of predefined shapes alongside a single global window to capture features at different scales. Fixed window sizes and shapes may not adapt well to varying spatial resolutions or object scales. When input data vary substantially, for example in satellite type, spatial resolution, or spectral channels, this strategy may lead to unstable performance. Furthermore, no experiments are provided comparing LGWA with other adaptive attention mechanisms such as Swin Transformer or CSWin.\n6.\tThe experimental results show that the proposed method’s performance is not particularly strong. Compared with existing state-of-the-art methods for building extraction and building change detection, the accuracy exhibits a noticeable gap. Additionally, the paper does not include comparisons with large foundation models such as the Segment Anything Model or remote sensing models based on SAM, which would help contextualize the method’s practical effectiveness.\n7.\tThe ablation studies and analysis are limited, making it difficult to fully support the authors' claim of achieving unification across arbitrary spatial, temporal, and spectral dimensions. More comprehensive experiments are needed to demonstrate the contribution of each component and to validate the generalization of the proposed framework."}, "questions": {"value": "1.\tRelation to existing foundation models: Could the authors clarify how STSUN differs from recent remote sensing foundation models such as RingMo-Agent and Falcon? Have the authors considered including a comparison or discussion of these models to better position the novelty of their approach?\n2.\tFlexible semantic class sets: How does the proposed trainable category-embedding mechanism compare with prior open-vocabulary or open-set segmentation approaches in remote sensing? Could the authors provide experiments or analysis to demonstrate the advantage of their method over these existing paradigms?\n3.\tSTS dimension unification: The method relies on metadata and hypernetworks to unify spatial, temporal, and spectral dimensions. Can the authors provide more empirical evidence to show that this approach generalizes across varying spatial resolutions, spectral coverage, and temporal sampling intervals? For example, have they tested the model on datasets with highly heterogeneous input configurations?\n4.\tTemporal Unified Module (TUM): For long temporal sequences or high-temporal-resolution change detection, how does TUM handle complex temporal dynamics? Could the authors include ablation studies or experiments on longer sequences to validate the effectiveness and stability of TUM?\n5.\tLocal-Global Window Attention (LGWA): How sensitive is LGWA to the choice of local window sizes and shapes, particularly when input data vary in spatial resolution, object scale, or spectral channels? Have the authors compared LGWA with other adaptive attention mechanisms such as Swin Transformer or CSWin to verify its effectiveness?\n6.\tExperimental performance and comparisons: The current experiments show a noticeable gap in accuracy compared with state-of-the-art building extraction and change detection methods. Could the authors provide comparisons with foundation models such as the Segment Anything Model or RS models based on SAM to better contextualize the performance?\n7.\tAblation studies: The current ablation experiments appear limited. Could the authors provide more detailed component-level analyses to demonstrate the contribution of each module and to support their claim of achieving full unification across arbitrary spatial, temporal, and spectral dimensions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vMk7zupGBM", "forum": "hwZi9LOTwS", "replyto": "hwZi9LOTwS", "signatures": ["ICLR.cc/2026/Conference/Submission9036/Reviewer_NX23"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9036/Reviewer_NX23"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079160830, "cdate": 1762079160830, "tmdate": 1762920753060, "mdate": 1762920753060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}