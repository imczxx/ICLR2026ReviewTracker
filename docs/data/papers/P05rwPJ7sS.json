{"id": "P05rwPJ7sS", "number": 12841, "cdate": 1758210763708, "mdate": 1759897481919, "content": {"title": "Continual Learning via Continual Weighted Sparsity and Meta-Plasticity Scheduling", "abstract": "Continual Learning (CL) is fundamentally challenged by the stability-plasticity dilemma: the trade-off between acquiring new information and maintaining past knowledge. To address the stability, many methods keep a replay buffer containing a small set of samples from prior tasks and employ parameter isolation strategies that allocate separate parameter subspaces for each task, reducing interference between tasks. To get more refined, task-specific groups, we adapt a dynamic sparse training technique and introduce a continual weight score function to guide the iterative pruning process over multiple rounds of training. We refer to this method as the continual weighted sparsity scheduler. Furthermore, with more incremental tasks introduced, the network inevitably becomes saturated, leading to a loss of plasticity, where the model's adaptability decreases due to dormant or saturated neurons. To mitigate this, we draw inspiration from biological meta-plasticity mechanisms, and develop a meta-plasticity scheduler to dynamically adjust these task-specific groups' learning rates based on the sensitive score function we designed, ensuring a balance between retaining old knowledge and acquiring new skills. The results of comparison on popular datasets demonstrate that our approach consistently outperforms existing state-of-the-art methods, confirming its effectiveness in managing the stability-plasticity trade-off.", "tldr": "", "keywords": ["Continual learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/098864806716460f55d9a0b476421f6cc991bfae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a continual learning approach using the parameter isolation technique and regularization through learning rate tuning. It claims to address the stability-plasticity dilemma in CL."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) This paper is well-written;\n2) although the idea is common, the algorithm instantiation is new."}, "weaknesses": {"value": "1) contribution of this paper should be elaborated more in section 1 of this paper. currently, it is hard for readers to grasp the key contribution of this paper.\n\n2) why is memory still required in your case? If the parameter isolation or regularization strategy work properly, the memory is not required at all.\n\n3) Controlling the learning rates for addressing the catastrophic forgetting problem is not new in CL. Authors should review these works in the related works section.\n\nOvercoming Catastrophic Forgetting by Neuron-level Plasticity Control\n\nContinual learning via inter-task synaptic mapping\n\n4) datasets used are not sufficiently complex. I suggest add experiments with ImageNet-R or CUB-200\n\n5) algorithm's complexity should be analyzed using a big O notation"}, "questions": {"value": "1) contribution of this paper should be elaborated more in section 1 of this paper. currently, it is hard for readers to grasp the key contribution of this paper.\n\n2) why is memory still required in your case? If the parameter isolation or regularization strategy work properly, the memory is not required at all.\n\n3) Controlling the learning rates for addressing the catastrophic forgetting problem is not new in CL. Authors should review these works in the related works section.\n\nOvercoming Catastrophic Forgetting by Neuron-level Plasticity Control\n\nContinual learning via inter-task synaptic mapping\n\n4) datasets used are not sufficiently complex. I suggest add experiments with ImageNet-R or CUB-200\n\n5) algorithm's complexity should be analyzed using a big O notation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sd4xvwDzpL", "forum": "P05rwPJ7sS", "replyto": "P05rwPJ7sS", "signatures": ["ICLR.cc/2026/Conference/Submission12841/Reviewer_HHV3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12841/Reviewer_HHV3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761004542661, "cdate": 1761004542661, "tmdate": 1762923638767, "mdate": 1762923638767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel framework for addressing the stability-plasticity trade-off in Continual Learning (CL). The proposed approach integrates two components: the Continual Weighted Sparsity Scheduler and the Meta-Plasticity Scheduler. The former utilizes a dynamic sparse training technique to refine task-specific groups of neurons, guiding iterative pruning processes over several rounds. The latter draws inspiration from biological models to adjust learning rates dynamically based on a sensitive score function, ensuring balanced retention of old knowledge and acquisition of new skills. The experimental results demonstrate that this method consistently surpasses existing state-of-the-art approaches across several standard datasets, such as CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K. The framework achieves this by optimizing both stability and plasticity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel Approach**: The combination of continual weighted sparsity and meta-plasticity scheduling seems to be interesting and novel, providing a sophisticated mechanism to balance stability and plasticity.\n    \n2. **Relatively Thorough Evaluation**: The authors evaluated their method against several baselines combined with ablations to validate their approach.\n    \n3. **Biologically Inspired Design**: Drawing inspiration from the brain’s meta-plasticity mechanisms adds theoretical depth and potentially broadens applicability to more dynamic and complex environments."}, "weaknesses": {"value": "1. **Lack of Clarity:** The explanation or notation for some definitions and concepts is missing, e.g.,\n    *  In eq.(6), the gradient is w.r.t. $w$, but the strict definition of $w$ is missing, and the losses do not contain a parameter for $w$.\n    * Again in eq.(6), there is no definition for $\\hat{\\mathcal{L}}_{\\text{new}}(D_t;\\theta)$.\n    * In experiments, the definition of stability, plasticity, and their trade-off are missing in the main paper. Please move them from the Appendix to the main paper. \n\n2. **Lack of Related Work Discussion:** The paper only discussed replay-based and parameter isolation methods, while regularization techniques and meta-learning-based approaches are not discussed, especially those explicitly studied dynamic network expansion, stability and plasticity trade-off, such as [1-8].\n\n3. **Complex Implementation**: The continual weighted sparsity scheduler and meta-plasticity scheduling require careful tuning of numerous parameters, which might demand substantial computational resources and expertise for effective deployment.\n    \n    \n4. **Sparsity Allocation**: The paper mentions performance degradation when varying task-specific sparsity ratios, indicating potential limitations in adjusting these parameters.\n\n5. **Limited Task Length:** Even though the authors claim a long task sequence, only $20$ tasks were considered. Previous works have considered super long tasks [2, 4, 8]. \n\n[1] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In International Conference on Machine Learning, pages 1920–1930. PMLR, 2019.\n\n[2] Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Page-Caccia, Issam Hadj Laradji, Irina Rish, Alexandre Lacoste, David Vázquez, et al. Online fast adaptation and knowledge accumulation (osaka): a new approach to continual learning. Advances in Neural Information Processing Systems, 33:16532–16545, 2020.\n\n[3] Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochastic gradient descent with biased regularization. In International Conference on Machine Learning, pages 1566–1575. PMLR, 2019.\n\n[4] Qi Chen, Changjian Shui, Ligong Han, and Mario Marchand. On the stability-plasticity dilemma in continual meta-learning: Theory and algorithm. Advances in Neural Information Processing Systems, 36:27414–27468, 2023.\n\n[5] Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-based meta-learning. In International Conference on Machine Learning, pages 424–433. PMLR, 2019.\n\n[6] Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-learning methods. arXiv preprint arXiv:1906.02717, 2019.\n\n[7] Qiang Zhang, Jinyuan Fang, Zaiqiao Meng, Shangsong Liang, and Emine Yilmaz. Variational continual Bayesian meta-learning. Advances in Neural Information Processing Systems, 34: 24556–24568, 2021.\n\n[8] Zhenyi Wang, Li Shen, Tiehang Duan, Donglin Zhan, Le Fang, and Mingchen Gao. Learning to learn and remember super long multi-domain task sequence. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 7982–7992, 2022."}, "questions": {"value": "1.  **Unclear where the Improvements are from:**\n    * The method filters the neurons into groups, and neurons with lower activation values are pruned. What's the difference compared to normal NN updates, where neurons with lower activation will be changed less (a soft regularization version)? \n    * The sparsity seems to be from the $L_1$ regularization in eq. (6). What's the effect of the $\\alpha$s? As this already puts a constraint on sparsity, why do you need the above Sparsity scheduling? \n    * How does the memory buffer affect the results? Is it fairly set for all baselines?\n\n2. **Dynamic Network Expansion**: The authors suggest integrating dynamic network expansion in future work. How might this change the current framework's effectiveness on tasks with blurred boundaries or real-world applications with numerous tasks? \n    \n3. **Sensitivity to Initial Conditions**: Given the different initializations mentioned, how sensitive is the model to such variations in practice, and what implications might this have on its deployment in diverse real-world scenarios? \n    \n4. **Hyper-parameter**: How are the hyper-parameters tuned?  \n\nI am happy to increase the score if most of my concerns are well addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U58hlqFPWZ", "forum": "P05rwPJ7sS", "replyto": "P05rwPJ7sS", "signatures": ["ICLR.cc/2026/Conference/Submission12841/Reviewer_XYUV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12841/Reviewer_XYUV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525450676, "cdate": 1761525450676, "tmdate": 1762923637685, "mdate": 1762923637685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for addressing the stability-plasticity dilemma in Continual Learning (CL). To maintain model stability, it proposes a dynamic sparse training scheme that iteratively prunes redundant parameters when learning a new task, preserving past knowledge while resulting in a more refined parameter group. Moreover, to enhance model plasticity, it computes the parameter sensitivity of each group and assigns dynamic learning rates to different parameters according to sensitivity scores. Extensive experiments validate the effectiveness of each component of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The overall framework is well-motivated. This paper proposes a dynamic pruning strategy to preserve sparse but important parameters for old tasks, achieving better stability than post-training pruning algorithms. It also introduces a dynamic learning rate adjustment scheme rather than directly freezing task-specific parameter groups, resulting in better model plasticity.\nThe experiments are comprehensive. The proposed framework demonstrated both improvement in stability and plasticity. The paper is well-written and easy to follow."}, "weaknesses": {"value": "The contributions of the sparse pruning scheme appear to be moderate. Network pruning and parameter isolation have been extensively studied in the context of continual learning [1, 2, 3]. The proposed scheduling strategy seems to be a variant of these prior approaches, transforming static post-training pruning into a dynamic and iterative process. It appears more like a heuristic trick rather than a fundamentally advanced framework. The authors are encouraged to conduct a deeper analysis of related works and provide a more thorough explanation of the underlying mechanism behind the proposed iterative pruning strategy.\nThe proposed meta-plasticity strategy modulates the learning rate based on group-wise sensitivity, which appears conceptually similar to other parameter sensitivity–based regularization methods such as EWC [4] and UPGD [5]. The authors are encouraged to further clarify the distinctions and advantages of the proposed approach over these prior works. For example, how the group-wise sensitivity mechanism improves upon the parameter-wise sensitivity adopted in existing methods.\nThe authors are suggested to provide details of the memory buffer size of the proposed method and existing SOTAs in Table 1.\n[1] Wang Z, Zhan Z, Gong Y, et al. Sparcl: Sparse continual learning on the edge[J]. Advances in Neural Information Processing Systems, 2022, 35: 20366-20380.\n[2] Kang H, Mina R J L, Madjid S R H, et al. Forget-free continual learning with winning subnetworks[C]//International conference on machine learning. PMLR, 2022: 10734-10750.\n[3] Gao Q, Shan X, Zhang Y, et al. Enhancing knowledge transfer for task incremental learning with data-free subnetwork[J]. Advances in Neural Information Processing Systems, 2023, 36: 68471-68484.\n[4] Kirkpatrick J, Pascanu R, Rabinowitz N, et al. Overcoming catastrophic forgetting in neural networks[J]. Proceedings of the national academy of sciences, 2017, 114(13): 3521-3526.\n[5] Elsayed M, Mahmood A R. Addressing loss of plasticity and catastrophic forgetting in continual learning[J]. arXiv preprint arXiv:2404.00781, 2024. (ICLR 2024)"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nqJrW5xRYm", "forum": "P05rwPJ7sS", "replyto": "P05rwPJ7sS", "signatures": ["ICLR.cc/2026/Conference/Submission12841/Reviewer_ZmNd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12841/Reviewer_ZmNd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733760482, "cdate": 1761733760482, "tmdate": 1762923637326, "mdate": 1762923637326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new framework for continual learning that addresses the stability–plasticity dilemma. The method introduces two key components: a continual weighted sparsity scheduler, which iteratively prunes neurons and connections to form refined, task-specific subnetworks, and a meta-plasticity scheduler, which dynamically adjusts learning rates based on sensitivity scores inspired by biological mechanisms. Together, these techniques enhance stability through parameter isolation and preserve adaptability by regulating neural plasticity. Experiments on several benchmarks (CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K) demonstrate that the proposed approach consistently outperforms existing replay-based and parameter-isolation methods in both class-incremental and task-incremental settings."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The method is technically sound. Results show consistent and significant improvements over strong baselines, supported by ablation studies and visual analyses that validate the method’s effectiveness."}, "weaknesses": {"value": "1. The paper lacks clarity in presentation. The methodology is confusing, with several important details insufficiently explained.\n2. The main contributions is introducing the new loss term $L_{\\text{new}}$ in CWS and the meta-plasticity scheduler. But these contributions are relatively incremental. The paper does not clearly justify the motivation or necessity of these components.\n3. The design and role of $L_{\\text{new}}$ are unclear. It appears to introduce an auxiliary classification objective and potentially additional parameters, yet this loss is not explicitly optimized or detailed in the algorithm.\n4. The rationale for defining the sensitivity score $SS$ based on $C_e$ is not well-motivated. The paper does not sufficiently explain how this formulation helps mitigate catastrophic forgetting since it does not freeze any parameters.\n5. Experimental coverage is incomplete. Some relevant baselines (e.g., PackNet) are missing, and certain implementation details or results are not reported. More experiments beyond accuracy, i.e., sparsity, plasticity, etc. would be more helpful to demonstrate the effectiveness of the proposed method."}, "questions": {"value": "1. Does the proposed method rely on class meta-information (e.g., class names or descriptions) during training?\n2. How does the sensitivity score $SS$  evolve across tasks as training progresses?\n3. How does the meta-plasticity scheduler specifically prevent catastrophic forgetting? Since learning rates are reduced only after significant parameter changes, could forgetting have already occurred before learning rates reduction?\n4. Are any parameters frozen during training, or are all non-pruned parameters continuously updated across tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZgDcSGJbEH", "forum": "P05rwPJ7sS", "replyto": "P05rwPJ7sS", "signatures": ["ICLR.cc/2026/Conference/Submission12841/Reviewer_42VU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12841/Reviewer_42VU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762355979248, "cdate": 1762355979248, "tmdate": 1762923637088, "mdate": 1762923637088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}