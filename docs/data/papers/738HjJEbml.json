{"id": "738HjJEbml", "number": 15489, "cdate": 1758251871485, "mdate": 1759897303570, "content": {"title": "QueryStream: Advancing Streaming Video Understanding with Query-Aware Pruning and Proactive Response", "abstract": "The increasing demand for real-time interaction in online video scenarios necessitates a new class of efficient streaming video understanding models. However, existing approaches often rely on a flawed, query-agnostic ``change-is-important'' principle, which conflates visual dynamics with semantic relevance, leading to computational waste and interaction errors. To address this, we propose QueryStream, a novel framework that instills query-awareness into the core of video processing and response scheduling. QueryStream features two synergistic components: (1) Query-Aware Differential Pruning (QDP), a policy that filters the token stream by jointly assessing semantic relevance to the query and temporal novelty against a dynamically smoothed history, and (2) Relevance-Triggered Active Response (RTAR), a dual-gated mechanism that schedules responses based on both high query relevance and significant information density. As a lightweight, training-free module, QueryStream establishes a new state-of-the-art on benchmarks like StreamingBench and OVO-Bench, matching or exceeding the performance of full-token baselines while pruning over 70\\% of visual tokens. Notably, our pruning mechanism generalizes to offline tasks, where it functions as an effective context-denoising module to improve accuracy on long-form videos. This work not only reveals the vast semantic redundancy in video streams relative to user intent but also establishes a promising, intent-driven direction for truly efficient and robust online video understanding.", "tldr": "", "keywords": ["Streaming Video Understanding", "Query-Aware Pruning", "Proactive Response"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58b0768291af6b9294c348c9268a79a7df34397e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces QueryStream, a training-free, plug-and-play framework that improves streaming video understanding by aligning video token processing and response timing with the semantic intent of a user’s query. Traditional streaming models follow a “change-is-important” rule that triggers responses based on visual variation, leading to irrelevant activations and inefficiency. QStream aims to solve this."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Shifts from the long-standing “visual-change” heuristic to a query-centric paradigm for efficient video understanding.\n* Requires only a small OpenCLIP encoder and simple logic-based gating, making it suitable for real-time applications.\n* Works with any existing Video-LLM (e.g., Qwen2.5-VL) without retraining.\n* The combination of semantic relevance and temporal novelty is intuitive, interpretable, and effective.\n* Sets new SOTA on StreamingBench and OVO-Bench, surpassing trained baselines with far fewer tokens."}, "weaknesses": {"value": "* Despite strong framing, the core mechanisms (cosine similarity, thresholding, dual gating) are algorithmically simple and largely heuristic.\n* Several thresholds may need tuning for each setup; no adaptive or learned strategy is explored.\n* Tests are limited to mid-scale models (7B) and benchmarks where scalability to larger multimodal LLMs or real-time deployment remains unclear.\n* While efficiency is discussed, actual wall-clock or energy improvements are not quantified.\n* The novelty primarily lies in combining existing ideas (query relevance + temporal novelty + dual gating) rather than developing a fundamentally new principle.\n* Table 1 bolds the results for QStream but there are baselines with better performance even though using more frames.\n* Table 3 could benefit from stronger baselines.\n\nMinor comments:\n* Authors use \\citet instead of \\citep.\n* No need to repeat QDP many times."}, "questions": {"value": "* Why do you need the semantic relevante filtering and the relevance condition? Aren't they doing the same? It seems unclear to me and not well explained the difference. I'd provide more details.\n* Could you report latency and throughput improvements to validate real-time claims?\n* How easy would it be to implement an adaptive threshold tuning approach?\n* Could you evaluate on other models besides QwenVL2.5 (eg. VideoLLaMA3, InternVL3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not identify any significant ethical issues in this paper. The method operates on publicly available video datasets commonly used in the community, and there is no indication of privacy violations, harmful content generation, or misuse potential beyond standard concerns in visual understanding research. Therefore, I do not see any ethical concerns requiring further attention."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9rRNe2es87", "forum": "738HjJEbml", "replyto": "738HjJEbml", "signatures": ["ICLR.cc/2026/Conference/Submission15489/Reviewer_ioxx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15489/Reviewer_ioxx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761221394700, "cdate": 1761221394700, "tmdate": 1762925780002, "mdate": 1762925780002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the inefficiencies of existing streaming video understanding models, where many rely on a flawed \"change-is-important\" principle that confuses visual dynamics with semantic relevance, leading to computational waste and interaction errors. To solve this, the authors propose QueryStream, a lightweight, training-free framework that embeds query-awareness into video processing and response scheduling. It integrates two main contributions: 1. Query-Aware Differential Pruning, which filters visual tokens by jointly evaluating semantic relevance to the user’s query and temporal novelty. 2. Relevance-Triggered Active Response, which dynamically determines optimal response moments by monitoring two key signals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Query-Awareness Solves Core Inefficiencies: Unlike query-agnostic methods, it avoids spurious triggers from irrelevant visual changes and captures subtle but query-relevant events, reducing computational waste and improving response accuracy.\n2. Requires no additional fine-tuning, enabling plug-and-play integration with pre-trained Video-LLMs. \n3. Clear presentation and illustrations, making the paper easy to follow.\n4. Sufficient experiments, establishing the robustness of the proposed method."}, "weaknesses": {"value": "1. QDP’s performance is limited by the representational capacity of the pretrained encoder. It may struggle with fine-grained details or abstract semantic relationships, leading to missed subtle events in nuanced scenarios.\n2. The framework assumes a single, fixed user query and cannot handle dynamic conversational contexts where user intent evolves over multiple turns\n3. RTAR and QDP rely on static thresholds tuned on a validation set. These thresholds may not be optimal across all video domains, reducing the adaptability."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J4AX235OHb", "forum": "738HjJEbml", "replyto": "738HjJEbml", "signatures": ["ICLR.cc/2026/Conference/Submission15489/Reviewer_UzVR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15489/Reviewer_UzVR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835837063, "cdate": 1761835837063, "tmdate": 1762925779627, "mdate": 1762925779627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces QueryStream, a novel framework designed to enhance the efficiency and interactivity of streaming video understanding models by incorporating query-awareness into the core processing loop. This work addresses the limitations of existing approaches, which typically rely on a flawed, query-agnostic “change-is-important” principle that conflates raw visual dynamics with true semantic relevance, leading to computational waste and interaction errors in real-time online video scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Query-Aware Differential Pruning (QDP): QDP is a novel token pruning mechanism that is unique because it employs a dual criterion that jointly assesses semantic relevance to the user's query and temporal novelty. Previous token pruning methods were often query-agnostic.\n2. Dynamically Smoothed History (DSH): Within QDP, temporal novelty is assessed not against the immediately preceding frame, but against a Dynamically Smoothed History (DSH). This adaptive historical context ensures the pruning is robust to transient noise and slow visual drifts.\n3. Relevance-Triggered Active Response (RTAR): RTAR is a novel, logic-driven dual-gated mechanism that schedules responses based on a specific confluence of two signals: high query relevance and significant information density. This departs from prior reactive (passive) models or proactive models relying on heavily trained, specialized modules.\n4. Lightweight and Training-Free: QueryStream is designed as a lightweight, training-free module that operates as an intelligent pre-processing gateway, allowing for seamless, zero-shot integration with existing off-the-shelf Video-LLMs."}, "weaknesses": {"value": "1. The effectiveness of the core Query-Aware Differential Pruning (QDP) mechanism is \"fundamentally bound by the representational quality of the pre-trained OpenCLIP encoder\". The authors utilized OpenCLIP-ViT-L/14, which was chosen as a pragmatic trade-off between feature quality and real-time computational efficiency. However, the inherent constraints of this encoder in discerning \"fine-grained details or abstract relationships\" may challenge the \"pruning precision in semantically nuanced scenarios\". This reliance risks causing critical but subtle events to be erroneously pruned and missed.\n2. The logic-based gating mechanisms (QDP and RTAR) rely on a \"set of fixed hyperparameters\". These values were determined empirically via sweeps and grid searches on a held-out validation set to find an optimal balance. However, these static thresholds, while effective in the evaluated benchmarks, \"may not be universally optimal\" and could reduce the model's robustness across diverse video domains, streaming qualities, or different types of user queries.\n3. The crucial ablation study for the Relevance-Triggered Active Response (RTAR) policy (Table 5), which demonstrates its superiority in timeliness-aware scoring, was conducted using a simulated evaluation protocol. This simulation was necessary because the official real-time evaluation infrastructure for the OVO-Bench benchmark and the TimeChat-Online code were unavailable at the time of experiments. While the simulation aimed to be fair by adhering to temporal constraints during response generation, the initial identification of trigger points leveraged the full video stream."}, "questions": {"value": "See weakness above, please."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1TO1oaa1iD", "forum": "738HjJEbml", "replyto": "738HjJEbml", "signatures": ["ICLR.cc/2026/Conference/Submission15489/Reviewer_vYGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15489/Reviewer_vYGP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977327272, "cdate": 1761977327272, "tmdate": 1762925778885, "mdate": 1762925778885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents QueryStream, a framework for streaming video understanding that builds query-aware temporal representations to improve performance. The method dynamically fuses incoming frame features with past context, guided by query-conditioned similarity maps, allowing the model to emphasize temporally relevant segments as the video unfolds. The paper reports improvements on multiple benchmarks (streming + long video benchmarks) compared with existing streaming or offline VLMs."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "- The introduction is very well written. It clearly defines the gap between offline video-language models and real-time streaming setups, motivating the need for a query-aware temporal design.\n\n- The method is well organized, with a clear hierarchy between query-aware temporal modules, memory updates, and frame-level fusion."}, "weaknesses": {"value": "- The method is presented as a new query-aware temporal representation, but in practice it relies on several manually tuned parameters and heuristic weighting functions between query and frame embeddings. The current mothod design appears quite sensitive to these hyperparameters even though they have discussed in A.4. Despite the hierarchical structure, the approach still feels heuristic and lacks a clear principle.\n\n- The proposed framework mainly reorganizes existing components such as query-conditioned similarity and temporal fusion into a clean, well-structured pipeline. In the end, it looks more like a refined arrangement of standard similarity-based scoring rather than a fundamentally new approach, so the novelty feels limited.\n\n- The paper emphasizes low-latency and lightweight design, but there is no analysis of runtime, throughput, or resource usage. In streaming scenarios, maintaining query-aware similarity maps and recurrent updates for each frame can be computationally heavy. Without latency or efficiency analysis, it is unclear whether the method is actually suitable for real-time use."}, "questions": {"value": "- Why was OpenCLIP chosen as the encoder instead of more recent options like SigLIP that are already aligned with VLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7A5ecxnc9V", "forum": "738HjJEbml", "replyto": "738HjJEbml", "signatures": ["ICLR.cc/2026/Conference/Submission15489/Reviewer_6ZdJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15489/Reviewer_6ZdJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993647246, "cdate": 1761993647246, "tmdate": 1762925778502, "mdate": 1762925778502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}