{"id": "60949HHudh", "number": 9353, "cdate": 1758119812035, "mdate": 1759897729666, "content": {"title": "Learning to Reward: A Contextual Bandit Framework for Distributional Reward Policy Optimization", "abstract": "Reward models (RMs) are a cornerstone of aligning large language models (LLMs) with human preferences, yet their ability to faithfully represent the nuance and uncertainty of these preferences remains a critical challenge. Existing RMs can be classified by their output (point-estimate vs. distributional) and training paradigm (supervised vs. reinforcement learning). \nWhile prior work has addressed three of these four quadrants, no existing framework combines the dynamic optimization of reinforcement learning with the rich, uncertainty-aware representation of a distributional model. \nTo fill this gap, we propose \\textbf{D}istributional \\textbf{R}eward \\textbf{P}olicy \\textbf{O}ptimization (\\textbf{DRPO}). DRPO formulates the learning of a distributional reward model as a contextual bandit problem, where the RM itself acts as a stochastic policy. This policy is trained using a uncertainty-aware meta-reward signal derived directly from the statistics of the reward distributions. We provide a algorithm analysis of DRPO's gradient dynamics and conduct extensive experiments to demonstrate its effectiveness. Our code and data are available at \\url{https://anonymous.4open.science/r/DRPO/}.", "tldr": "", "keywords": ["LLM", "Reward model", "Preference learning", "RL"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a0d25597e5c2e50559df19004537f5051bb4714.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduce DRPO (Distributional Reward Policy Optimization). It frames reward-model training as a contextual bandit where the reward model itself is considered as a policy that outputs a distribution over rewards. The reward model is trained by preference-based samples with an uncertainty-aware meta-reward. Experiments evaluate across seven benchmarks, showing competitive results. Certain ablation studies are also included."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is conceptually new as far as I know. It introduces a principled formulation of reward modeling training within an RL framework. The idea is easy to grasp. The paper also provides comprehensive empirical validation, including comparisons across diverse benchmarks and ablations. Collectively they demonstrate the method’s practicality."}, "weaknesses": {"value": "My main concern is the performance. In Table 2, the proposed RL-based distributional models achieve only comparable results to existing approaches. This raises the question of why one should prefer RL-based distributional methods, given that SL approaches are typically simpler and more computationally efficient to train. (Please correct me if it is wrong)\n\nIn addition, the meta-reward formulation appears somewhat heuristic. The motivation behind the specific definition in Definition 1 is heuristic as well as the design choice of $m_{ij}$. While I admit that it is encouraging this design works empircally, a deeper explanation would be helpful."}, "questions": {"value": "The choice of a Gaussian head may be restrictive. It would be desirable to see non-Gaussian heads such as quantiles and categorical distributions. \n\nIt would be nice to see some downstream impact directly. People may wonder the performance of the learned policy if it is fine-tuned via the proposed RM.\n\nThe framework is bandits. Can we extend it to Markov decision processes by fitting value models? What will be the challenges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q2nh7PBPaq", "forum": "60949HHudh", "replyto": "60949HHudh", "signatures": ["ICLR.cc/2026/Conference/Submission9353/Reviewer_bz4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9353/Reviewer_bz4m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761446256506, "cdate": 1761446256506, "tmdate": 1762920978339, "mdate": 1762920978339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Distributional Reward Policy Optimization (DRPO), a novel framework for training reward models (RMs) that combines reinforcement learning (RL) with distributional reward modeling to better capture uncertainty and improve alignment in large language models (LLMs). DRPO treats reward modeling as a contextual bandit problem, where the reward model acts as a stochastic policy. The paper presents both theoretical contributions, including an analysis of DRPO’s gradient dynamics, and practical experiments comparing DRPO to existing baseline models. The results show DRPO's efficacy in producing well-calibrated reward models that outperform traditional approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed DRPO framework is a good idea. By treating reward modeling as a contextual bandit problem, the paper provides a perspective on how reward models can be trained dynamically, with the reward model acting as a stochastic policy."}, "weaknesses": {"value": "The paper does not adequately highlight the novelty and distinctions of DRPO in comparison to prior work in reward modeling. While DRPO extends scalar reward learning to a distributional paradigm, it mainly focuses on parameterizing the reward distribution using a Normal distribution [1]. However, the paper could provide a clearer explanation of how this approach improves on existing models.\n\n **contextual bandit problem :** The paper introduces the contextual bandit formulation but does not clearly explain how it applies to reward modeling. The core idea behind contextual bandits lies in the exploration-exploitation trade-off in a one-step decision environment. However, DRPO does not seem to fully align with this concept. The GRPO objective averages over a sequence, but DRPO does not. Does this mean DRPO's approach can still be classified as a contextual bandit formulation? The paper would benefit from more clarification, especially whether the framework operates at a sequence-level or token-level.\n\n**Limited explanation of Hyperparameters:** While the hyperparameters for DRPO-Zero and DRPO-Refine are mentioned, the paper lacks an in-depth explanation of why these specific hyperparameters were chosen and how they impact model performance. For example, the confidence-controlling hyperparameter $K$ is a critical setting, but its influence on the model's performance is not fully discussed. Is there any sensitivity analysis performed for $K$?\n\n[1]  Probabilistic uncertain reward model."}, "questions": {"value": "1. Could you clarify why the standard normal distribution is chosen as the fixed prior in Equation (1)?\n\n2. I have some confusion regarding the two-stage RL method. Could you explain the difference between Equation (1) and Equation (2)? What motivates and justifies the need for DRPO-Refine? Additionally,  is there a particular reason clipping and importance sampling are not used in the objective function, as it is in GRPO (though I understand it could be beneficial for gradient analysis)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns are apparent in the paper."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qh6fgjeGG0", "forum": "60949HHudh", "replyto": "60949HHudh", "signatures": ["ICLR.cc/2026/Conference/Submission9353/Reviewer_jXS5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9353/Reviewer_jXS5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574962319, "cdate": 1761574962319, "tmdate": 1762920977964, "mdate": 1762920977964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**The paper *“Learning to Reward: A Contextual Bandit Framework for Distributional Reward Policy Optimization”*** proposes **Distributional Reward Policy Optimization (DRPO)**, a two-stage RL-style training framework for reward models used in large language model (LLM) alignment.  \n\nThe authors start from the observation that current reward model (RM) work mostly fills three quadrants of a \\\\( 2 \\times 2 \\\\) taxonomy:\n\n\\\\[\n\\\\begin{array}{c|cc}\n & \\\\textbf{Supervised} & \\\\textbf{Reinforcement Learning (RL)} \\\\\\\\ \\\\hline\n\\\\textbf{Point-Estimate} & \n\\\\text{Existing Work} &\n\\\\text{Existing Work} \\\\\\\\[2ex]\n\\\\textbf{Distributional} &\n\\\\text{Existing Work} &\n\\\\boxed{\\\\textbf{--- Missing Quadrant (DRPO) ---}} \\\\\\\\\n\\\\end{array}\n\\\\]\n \nThe missing quadrant is the **distributional + RL** setting.  \n\nDRPO is designed to fill exactly this gap: it treats the reward model itself as a **stochastic policy** in a **contextual bandit** setting, where the “action” is the reward value (drawn from a learned distribution) assigned to a prompt–response pair, thereby **explicitly quantifying uncertainty** in the reward assignment process.  \n\nThe core technical piece is an **uncertainty-aware meta-reward** (Definition 1) that compares samples from the preferred and dispreferred reward distributions but only gives credit when the difference exceeds a **variance-dependent margin**. This design prevents reward hacking via variance inflation and makes the gradient **uncertainty-aware**.  \n\nDRPO is trained in two curriculum stages:  \n\n1. **DRPO-Zero**: initialized from a standard normal prior and regularized by KL divergence to \\( \\mathcal{N}(0,1) \\).  \n2. **DRPO-Refine**: initialized and KL-anchored to the DRPO-Zero model, with stricter margins and stronger KL regularization.  \n\nExperiments on seven RM benchmarks using **LLaMA-3.1-8B**, **Qwen3-8B**, and **Qwen3-4B** backbones show that RL–distributional DRPO is **competitive with or better than** strong supervised (SL) point-estimate and SL-distributional baselines, sometimes matching larger models.  \n\nThe uncertainty-aware meta-reward successfully avoids the **variance-hacking failure mode** observed in simpler hinge-based meta-rewards. The paper also provides a **gradient-level comparison** to the Bradley–Terry model, showing that DRPO naturally downweights low-confidence cases.  \n\nParts of this review were discussed with a colleague to ensure clarity and accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "### Novelty\nA major strength of this work is its clear identification of a missing quadrant in the \n$2 \\times 2$ taxonomy of reward modeling: \\emph{distributional} vs. \\emph{point-estimate} \nand \\emph{supervised} vs. \\emph{RL}. While prior studies cover the other three quadrants, \nthe \\textbf{distributional + RL} setting remained unexplored. DRPO is the first framework \nto fill this gap, enabling explicit \\textbf{uncertainty modeling} through its stochastic \npolicy formulation. The framing is compelling, though the contribution would be more robust \nif the paper more clearly articulated \\textbf{why reinforcement learning is necessary} \nbeyond supervised distributional approaches.\n\n### Uncertainty-Aware Meta-Learning Formulation\n\nAnother key strength lies in the paper’s proposed \\textbf{uncertainty-aware meta-learning} \nframework. By comparing sampled rewards from preferred and dispreferred responses and scaling \nthe update by a variance-dependent margin, the method adapts learning to the model’s own \nconfidence. This prevents reward hacking through variance inflation and yields more stable \ngradients. The formulation effectively couples \\textbf{meta-learning and uncertainty modeling}, \nallowing the reward model to self-calibrate and improve robustness across preference data.\n\nThe power of this method is evident in ablations. \n\n\nAdditionally, the paper provides a clear \\textbf{theoretical comparison with the Bradley–Terry (BT)} \nmodel, showing that DRPO generalizes BT by incorporating variance as a learned parameter. \nThis analysis strengthens the conceptual grounding of DRPO, illustrating how it extends classical \nreward modeling frameworks to explicitly reason about uncertainty.\n\n\n### Experiments\nDRPO’s empirical results represent a clear strength of the work. Evaluated across seven diverse reward modeling benchmarks, DRPO consistently matches or outperforms strong supervised point-estimate and distributional baselines, rivaling larger models. Notably, it performs well across different model architectures, demonstrating strong \\textbf{cross-architecture generalization}. These achievements highlight the \\textbf{scalability, generality, and practical effectiveness} of combining distributional reward modeling with reinforcement learning, providing strong empirical validation for the proposed framework."}, "weaknesses": {"value": "### Guassian Assumption\n\nA key limitation of DRPO is its reliance on a **Gaussian assumption** for modeling reward distributions. \nWhile this simplifies training and allows for a closed-form uncertainty term, it limits expressiveness—real preference \ndata can be asymmetric or multimodal. This assumption may therefore yield miscalibrated uncertainty estimates, \nreducing robustness in more complex or non-Gaussian reward settings.\n\n### Explanation of RL\n\nWhile DRPO effectively motivates the need for **distributional** reward modeling, \nit provides less clarity on why **reinforcement learning** is necessary over \nsupervised alternatives. The paper frames the problem as a contextual bandit and \nuses RL to optimize a stochastic reward policy, but it does not convincingly \ndemonstrate where supervised training fails or how RL uniquely improves performance. \nA stronger justification for the RL component would make the framework’s design \nmore compelling and conceptually grounded.\n\n### Experiments\n\nAll experiments are trained on a single dataset. \n\n### Presentation\n\nThe paper contained minor grammatical errors and **Table 1** was difficult to read because of the formatting of the citations."}, "questions": {"value": "Why is reinforcement learning strictly necessary for the proposed formulation—could supervised distributional training achieve similar outcomes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aFf2GB58DE", "forum": "60949HHudh", "replyto": "60949HHudh", "signatures": ["ICLR.cc/2026/Conference/Submission9353/Reviewer_3euk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9353/Reviewer_3euk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903566960, "cdate": 1761903566960, "tmdate": 1762920977569, "mdate": 1762920977569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Distributional Reward Policy Optimization (DRPO), a framework that formulates the training of uncertainty-aware, distributional reward models as a contextual bandit problem. The approach uses reinforcement learning, treating the reward model as a stochastic policy and optimizing it with an uncertainty-aware meta-reward signal. A two-stage curriculum is employed: DRPO-Zero (prior-guided pre-training) and DRPO-Refine (self-refinement), with extensive empirical evaluation across multiple benchmarks for language model alignment and comprehensive algorithmic analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors provide a thorough mathematical analysis of the proposed framework, comparing DRPO's gradient dynamics to standard point-estimate approaches and offering explicit derivations (see Section 4.3 and equations for the gradient derivations).\n\n2. The uncertainty-aware meta-reward design is justified both intuitively and mathematically. Experiments and ablations in Figures 3a-3d  illustrate that DRPO's method avoids pathological increases in variance and reward hacking seen in naive hinge-based approaches-this is particularly compelling as it anchors the value of the proposed uncertainty calibration."}, "weaknesses": {"value": "1. **Empirical Performance Margins Are Modest.** While Table 2 shows that DRPO is highly competitive, performance gains over the best prior methods are relatively incremental or size-dependent. For many tasks, DRPO is indistinguishable from, or slightly behind, strong SL distributional baselines like URM-Skywork-8B or Skywork-Llama3.1-8B. In the 4B configuration, DRPO does not consistently outperform the best prior models. The claimed advantage would be more persuasive with statistical significance tests, effect size reporting, or robust anomaly analysis.\n2. **The claim of filling the (RL, Distributional) quadrant is weak.** The model is trained entirely offline on a fixed dataset, eliminating the fundamental exploration-exploitation trade-off required in true contextual bandit problems. The \"policy gradient\" is merely a term for a complex, sampling-based loss calculation, not a true interactive learning process. The framework is more accurately described as an advanced (SL + Distributional) method, making the \"RL\" claim tenuous\n3. **The paper adopts a Gaussian distribution to model human preferences without adequately justifying this choice.** A Gaussian is unimodal and cannot capture the complex, often multi-modal (e.g., polarized) nature of human preferences. The paper fails to discuss why this restrictive model is superior to more flexible alternatives (like Categorical distributions) mentioned in the (SL, Distributional) context."}, "questions": {"value": "1. Could the authors elaborate on why a Gaussian was chosen over more flexible alternatives, such as a Categorical distribution (which is noted in the SL quadrant).\n\n2. A more direct and insightful ablation would be to test the framework using the exact formulation from Definition 1, but simply setting $K=0$. This would create a sampling-based baseline that is \"uncertainty-agnostic.\" Can the authors provide experimental results for this $K=0$ baseline? This would more directly demonstrate the importance of the $\\sigma_{diff}$ term in stabilizing training and preventing the \"variance inflation\" seen in the hinge-based model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ixqBSdOJMw", "forum": "60949HHudh", "replyto": "60949HHudh", "signatures": ["ICLR.cc/2026/Conference/Submission9353/Reviewer_Vphz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9353/Reviewer_Vphz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186232282, "cdate": 1762186232282, "tmdate": 1762920977349, "mdate": 1762920977349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}