{"id": "HfRJDTBuSr", "number": 1254, "cdate": 1756867848118, "mdate": 1759898218811, "content": {"title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control", "abstract": "The rapid advancement of large vision language models (LVLMs) and agent systems has heightened interest in mobile GUI agents that can reliably translate natural language into interface operations. Existing single-agent approaches, however, remain limited by structural constraints. Although multi-agent systems naturally decouple different competencies, recent progress in multi-agent reinforcement learning (MARL) has often been hindered by inefficiency and remains incompatible with current LVLM architectures. To address these challenges, we introduce SWIRL, a staged workflow for interleaved reinforcement learning designed for multi-agent systems. SWIRL reformulates MARL into a sequence of single-agent reinforcement learning tasks, updating one agent at a time while keeping the others fixed. This formulation enables stable training and promotes efficient coordination across agents. Theoretically, we provide a stepwise safety bound, a cross-round monotonic improvement theorem, and convergence guarantees on return, ensuring robust and principled optimization. In application to mobile GUI control, SWIRL instantiates a Navigator that converts language and screen context into structured plans, and an Interactor that grounds these plans into executable atomic actions. Extensive experiments demonstrate superior performance on both high-level and low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong capability in multi-agent mathematical reasoning, underscoring its potential as a general framework for developing efficient and robust multi-agent systems.", "tldr": "We present SWIRL, a staged workflow for interleaved reinforcement learning designed for GUI multi-agent systems.", "keywords": ["Multi Agent", "Mobile Control", "LVLM Application", "Multi Agent Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/619e484439daa0292e0f189e2d0f678d5c02aa94.pdf", "supplementary_material": "/attachment/0a7d44520a22a08d4e78b8fae77c7cc0e751db46.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SWIRL, a novel RL framework for multi-agent systems.\nSpecifically, SWIRL conducts training of every single agent sequentially, thus stabilising the entire training process of the multi-agent system.\nThe authors provide a theoretical analysis for step-wise safety, monotonic improvement, and convergence of SWIRL.\nEmpirical results on GUI tasks and mathematical reasoning tasks demonstrate the superior performance of SWIRL."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Different from step-level alternation [1], SWIRL present a novel round-level alternation, sequentializing single-agent updates while maintaining multi-agent coordination.\n2. Presenting theoretical results based on [1].\n3. Extensive experiments on different benchmarks, including GUI tasks and math reasoning tasks.\n\n\n\n>Reference\n>>[1] Heterogeneous-agent reinforcement learning."}, "weaknesses": {"value": "1. First concern is the unclear structure, presentation and writing, especially in Section 2. The main contribution is the round-level alternation, while section 2.1 (Lines 59-200) is unclear in illustrating the underlying motivation. Additionally, the theoretical results are based on previous work [1]; it is important to illustrate clearly how to adapt from the previous one to address the challenges in this work (e.g., from step-level to round-level).\n2. For GUI tasks, a bunch of related works are missing, including [2-7].\n3. Limited novelty. The navigator-interactor pipeline is a common approach in the literature. For example, existing work [6] leverages VLM as the subgoal-generator to provide low-level instructions for the low-level agent.\n\n\n>Reference\n>>\n>>[1] Heterogeneous-agent reinforcement learning\n>>\n>>[2] You only look at screens: Multimodal chain-of-action agents\n>>\n>>[3] Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning\n>>\n>>[4] Digi-q: Learning q-value functions for training device-control agents\n>>\n>>[5] Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning\n>>\n>>[6] Vsc-rl: Advancing autonomous vision-language agents with variational subgoal-conditioned reinforcement learning\n>>\n>>[7] Distrl: An asynchronous distributed reinforcement learning framework for on-device control agents"}, "questions": {"value": "See Weaknesses.\nAdditionally, it would be great if more GUI experiments, comparing with existing baselines [1-6] on the AitW benchmark [7].\n\n\n>Reference\n>>\n>>[1] You only look at screens: Multimodal chain-of-action agents\n>>\n>>[2] Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning\n>>\n>>[3] Digi-q: Learning q-value functions for training device-control agents\n>>\n>>[4] Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning\n>>\n>>[5] Vsc-rl: Advancing autonomous vision-language agents with variational subgoal-conditioned reinforcement learning\n>>\n>>[6] Distrl: An asynchronous distributed reinforcement learning framework for on-device control agents\n>>\n>>[7] A Large-Scale Dataset for Android Device Control"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vNPSRHn8QL", "forum": "HfRJDTBuSr", "replyto": "HfRJDTBuSr", "signatures": ["ICLR.cc/2026/Conference/Submission1254/Reviewer_myFk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1254/Reviewer_myFk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562500761, "cdate": 1761562500761, "tmdate": 1762915718594, "mdate": 1762915718594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SWIRL, a staged, interleaved multi-agent reinforcement learning (MARL) workflow that decomposes multi-agent training into sequential single-agent updates. It presents formal convergence and safety proofs for interleaved updates, empirical validation showing superior zero-shot performance and robustness, and transferability to mathematical reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper gives a clever recombination of MARL, alternating optimization, and LVLM integration, executed with clear theoretical rigor and practical benefit.\n2. Experiments are diverse, spanning GUI control (high-level and low-level) and math reasoning. Baselines include major prior systems (GUI-R1, MARFT, OS-Atlas, ReMA), with clear evidence of superior performance and resource efficiency (O(1) memory scaling)."}, "weaknesses": {"value": "1. As a critical stability factor in MARL, interleaving frequency determines how quickly agents co-adapt. This paper does not empirically investigate how interleaving frequency or synchronization cadence affects performance, convergence, or stability.\n2. Although SWIRL is compared against OS-Atlas, GUI-R1, and MARFT, it omits recent multi-agent finetuning frameworks such as MultiAgent Debate (Du et al., 2023), AutoGen (Wu et al., 2024a), and Rema (Wan et al., 2025) in full experimental benchmarking. Comparing against them would help position SWIRL within the broader multi-agent LVLM training ecosystem.\n3. The paper emphasizes efficiency (O(1) actor memory), but does not quantify training sample efficiency or scaling behavior. The total training set (3,500 samples) is small, yet the authors do not clarify if this is sufficient for convergence or merely a constraint of resources.\n4. SWIRL’s performance may be conflated with the capabilities of the underlying Qwen2.5-VL-3B models. The absence of experiments with smaller or weaker backbones makes it difficult to isolate what proportion of improvement derives from the interleaved RL algorithm itself.\n5. The mathematical reasoning experiments (Section 3.4) show numerical improvements but do not explain why interleaving helps reasoning tasks or provide qualitative examples of improvement."}, "questions": {"value": "See the Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "stkRNiz2Oy", "forum": "HfRJDTBuSr", "replyto": "HfRJDTBuSr", "signatures": ["ICLR.cc/2026/Conference/Submission1254/Reviewer_dLTb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1254/Reviewer_dLTb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636049232, "cdate": 1761636049232, "tmdate": 1762915718476, "mdate": 1762915718476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SWIRL, a method that frames a MARL problem but leverages single-agent algorithms through sequential freezing and interleaving of agents during training. Agents included are Navigator and Interactor, dealing with UI screenshots and user requests as well as low level instructions from the Navigator and UI screenshots to derive actions (Interactor). Authors claim for this setup to outperform SOTA models such as GPT5, R1 etc."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting idea\n- Good experiments on replacing the Navigator with SOTA models such as GPT5 etc. to get  Interactor performance"}, "weaknesses": {"value": "- It is not clear to me why this is formulated as a multi-agent problem, as the setup is stated as a \"...sequential single-agent... enabling reuse of standard RL...\"\n\n- I believe the main weakness of the paper is the experiment design to answer the question above. In the experiment, I would have expected the following experiments:\n1. End-to-end RL training of Navigator and Interactor, potentially sharing rewards\n2. Isolated training of the Navigator until convergence and then isolated training of the Interactor (with frozen, well-performing Navigator)\n3. Interleafed Navigator / Interactor (SWIRL, yours) training\n\nThis is in combination with my main concern: The Indicator is clearly dependent on the performance of the Navigator. If the Navigator performs badly, the Interactor has no chance of performing well.\n\n- I don't see clear conclusion and discussion why the interleafed setup performs better than all other setups."}, "questions": {"value": "Why are you reporting performance on pre- and post-training language benchmarks (GSM8K, MATH500 etc.) when the paper is looking at vision-language models?\n\nI would be interested in the isolated performance of the Navigator and the Interactor. Why did you decide not to show that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "15Go2EJzM4", "forum": "HfRJDTBuSr", "replyto": "HfRJDTBuSr", "signatures": ["ICLR.cc/2026/Conference/Submission1254/Reviewer_Qt5R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1254/Reviewer_Qt5R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777220034, "cdate": 1761777220034, "tmdate": 1762915718363, "mdate": 1762915718363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SWIRL, a framework for reformulating MARL as a sequence of single-agent tasks, therefore sidestepping many of the known challenges, such as stability. The authors provide theoretical guarantees including per-step safety bounds, monotonic improvement across rounds, and convergence properties. In particular, this paper focuses on improving the capabilities of mobile GUI agents, while also testing on mathematical benchmarks. The paper achieves very strong empirical results."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Impressive empirical results** - SWIRL outperforms or achieves near state-of-the-art in a variety of benchmarks. This is a major strength of the paper.\n\n**Problem is important** - Mobile GUI control is a useful area of research, so I believe the paper is well-motivated and will be of use to the community.\n\n**Transfer beyond GUI** - I appreciate that this paper extends its results to more than one benchmark, namely mathematical benchmarks. Validity of the work is improved by showing results are not just closely tied to one benchmark.\n\n**Source Code** - The source code being included in the supplementary material improves the validity and reproducibility of this work."}, "weaknesses": {"value": "- **Support for $>2$ agents** - While the proposed SWIRL framework is reported to support N different agents, all experiments focus on settings with only two agents. Do you have any results demonstrating a setting with more than 2 agents?\n\n- **Walltime** - Currently, the paper does not disclose any information about the wall time of this method. I would imagine that the alternating nature of the approach may increase this substantially. \n\n- **Limitations Section?** - There appears to be no limitations section in this paper. This is a considerable red flag, as this is almost always an important section of research papers.\n\n- **Wording of Claims** - From the wording of this paper (such as line 20), it seems as though the idea of interleaved training with fixed agents is novel. While the authors acknowledge that this has been done before (via citing numerous prior works), I still feel that the authors should be careful not to overreach in terms of their claims."}, "questions": {"value": "- How does this approach work when scaling to more than 2 agents?\n\n- What impact does this approach have on walltime?\n\n- Where is the limitations section of this paper? What are the limitations?\n\nI will cautiously provide a score of 4, given that this is not my primary area of expertise; however, I will raise my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qqVlRzRP5d", "forum": "HfRJDTBuSr", "replyto": "HfRJDTBuSr", "signatures": ["ICLR.cc/2026/Conference/Submission1254/Reviewer_t83B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1254/Reviewer_t83B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841286055, "cdate": 1761841286055, "tmdate": 1762915718170, "mdate": 1762915718170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SWIRL splits a GUI agent into a Navigator (plans) and an Interactor (executes) and trains them alternately, freeze one, optimize the other, so multi-agent training reduces to standard trust-region single-agent updates. This staged setup targets stability/efficiency and shows gains on mobile GUI control, with spillover benefits to math reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear role decomposition (auditable plan to act chain). Practical pipeline that reuses mature single-agent RL tooling. Theoretical footing (safety bound, round-wise monotonic improvement). Solid empirical wins for low-level execution and planner replacement. Transferable beyond GUI with modest data."}, "weaknesses": {"value": "Training always freezes one agent (Navigator or Interactor) and optimizes the other, effectively decomposing the multi-agent problem into a sequence of single-agent GRPO/PPO/TRPO steps. This is closer to modular alternating optimization than classic concurrent cooperative MARL. As a result, non-stationarity and coordination that arise when both agents evolve simultaneously are not directly learned.\n\nThe GUI evaluation relies on offline single-step accuracy (Type/GR/SR)—e.g., a click landing in the box counts as correct, without reporting episode success rate, number of clicks, latency, or recovery/rollback behavior. High offline SR does not guarantee closed-loop, end-to-end performance.\n\nThe paper removes certain QA-style samples from GUI-Act-Web and evaluates on an older GUIOdyssey Test-Random split due to version changes. While this sharpens focus on manipulation skills, it alters the test distribution and complicates apples-to-apples comparison with prior work.\n\nThe reported stability stems from “alternating updates + online reweighting,” and ablations (e.g., removing reweighting) show clear degradation. This indicates robustness depends heavily on sample filtering and the training schedule, rather than fundamentally resolving non-stationarity under concurrent cooperative learning."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zD3ikkb7Hf", "forum": "HfRJDTBuSr", "replyto": "HfRJDTBuSr", "signatures": ["ICLR.cc/2026/Conference/Submission1254/Reviewer_yu3B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1254/Reviewer_yu3B"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049426352, "cdate": 1762049426352, "tmdate": 1762915718008, "mdate": 1762915718008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}