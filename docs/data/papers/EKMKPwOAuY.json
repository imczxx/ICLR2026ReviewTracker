{"id": "EKMKPwOAuY", "number": 1695, "cdate": 1756907917337, "mdate": 1759898194209, "content": {"title": "Atrous Learning for Diffusion Models", "abstract": "Locality has been recognized as a crucial mechanism that enables diffusion models to generalize to unseen images. However, this property may also lead to spatially inconsistent generations. For example, a diffusion model trained on natural images might generate hands with six fingers. To mitigate this issue, we propose atrous learning for diffusion models, a simple yet effective masking strategy. Specifically, our method randomly masks pixel positions when computing the regression loss, thereby encouraging the model to exploit broader contextual information for prediction. The proposed approach is lightweight and can be implemented with only a few lines of code. Experiments show that our method consistently outperforms baselines.", "tldr": "We propose a simple masking strategy to improve the contextual representation in diffusion models.", "keywords": ["generative models", "flow matching", "locality"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83d5dd076b6d3c20e73e84ce5569f3a77624f66c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose \"atrous learning for diffusion models\", where they introduce masking in the denoising objective in an effort to avoid \"spatial inconsistencies\". They claim that their approach mitigates undesirable effects of locality biases that are inherent in diffusion models. Experiments are conducted on synthetic data and standard image datasets, where improved performance is observed compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to read and the overall methodology is simple and clear.\n2. Along with the experiments, the authors offer some theoretical discussion and attempt to connect their work with existing literature.\n3. The authors have taken care to include details such as masking ratio ablations and performance as a function of the training epochs.\n4. I found the angle explored in Section 5.4, regarding memorization, particularly interesting. I encourage the authors to explore this further."}, "weaknesses": {"value": "1. While the methodology is clear to me, I am not convinced about the motivation of this work. The connection with prior work on Ω-locality is, in my opinion, loose. I include specific questions relating to this point in the Questions section below.\n\n2. The demonstrations are not convincing. The experiment in Figure 3 is not particularly informative as I struggle to see differences in the gradient heatmaps. I fail to see any correlation between the mask ratio and the standard metrics reported in Tables 1, 2 and it is not clear whether the reported improvements are statistically significant. For the experiment in Figure 6, none of the the samples appear sufficiently close to the training data and the proposed method's effect is therefore unclear.\n\n3. The paper advertises the proposed method as consistently outperforming baselines in the abstract. I feel that this framing is not appropriate as the reported improvements are questionable. Larger scale (e.g. ImageNet) and more thorough benchmarking with competitors would be required to properly validate such claims. The angle explored in Section 5.4, i.e., mitigating memorization, is, in my opinion, a better fit for this work (e.g., see [1] for similar analysis on language models)."}, "questions": {"value": "1. As I understand, the binary masks do not impose any locality constraints, i.e., they are iid. How is it possible then that SMD mitigates locality bias? Increased variance, as mentioned in Proposition 1, is not sufficient to force the networks to explore non-local structures. For example, one can also decrease the training batch size to achieve higher variance but there is no expectation that this mitigates locality.\n\n2. I fail to see any evidence for the claims made in Section 6 (starting at line 471). For example, how might SMD promote \"large-scale structure first and filling in finer detail later\"?\n\n3. Beyond the samples of Figure 2, could the authors provide more examples of locality bias and how this might be mitigated by SMD? At present, I am not convinced with the visualization on this toy dataset.\n\n[Minor] Figure 1: Supervised Singals -> Supervised Signals\n\n[1] Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs, NeurIPS 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UIxZVOeZQX", "forum": "EKMKPwOAuY", "replyto": "EKMKPwOAuY", "signatures": ["ICLR.cc/2026/Conference/Submission1695/Reviewer_UQdc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1695/Reviewer_UQdc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760813156775, "cdate": 1760813156775, "tmdate": 1762915858710, "mdate": 1762915858710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors point out that recent work on diffusion/flow models where it has been shown that for a given coordinate in the output space, information from a local neighborhood is mainly used to score or predict at the coordinate. They hypothesize that this *locality* of the diffusion process harms the model performance and that more global representations can be learned. To encourage such behaviour, they propose masking the training data such that the models are forced to consider more spread out information. Qualitative and quantitative analysis is done on outputs from models trained with the masked approach (SMD), suggesting that masking does in fact help."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well motivated, and the theory is mostly easy to follow. The idea that encouraging more global use of information is promissing (and you could maybe have referenced other work encouraging such behaviour, e.g. dino). The flow is good and the theoretical analysis of the SMD gradient is nice."}, "weaknesses": {"value": "The main weakness of the paper is the empirical validation. The datasets used are small and have low resolution, qualitative analysis needs to be more thorough, and quantitative analysis is lacking. The FID table is alright (50k images is good!) but still only compares a single model per setting, right?\n\nMore importantly, the main hypothesis that global information is not being used without masking is also not tested well enough. One could imagine masking having a different role, such as being a regularizer.\n\nFinally, there are no statistics or error bars, for this to be a convincing study you need to train multiple models. Pointwise evaluations of FID scores are not enough. This is also seen in figure 4 where the curves are not exactly smooth. Or in table 2 where it is not clear whether the fluctuations in the numbers are due to noise.\n\nBecause of this, I can't recommend acceptance (empirical evidence for the local/global claim and some statistics are needed, happy to reconsider if shown some)\n\n\nMore comments:\n\nI found figure 1 slightly confusing, i recommend explaining the variables and labels in the caption (also a typo in \"Singals\"). It is not clear how the multiple masks for SMD are used (the PSPC dynamics seem somewhat illustrated with the colors, maybe something similar can be done).\n\nIn section 5.1 on spatial consistency, it would be nice to see some kind of quantitative evaluation rather than just 9 samples from each setup.\n\nIn section 5.2. on contextual representations, the single outputs are nice but again, some kind of quantitative result would be nice, and more examples.\n\nFigure 3 needs much larger text, and the plots are barely different to the untrained eye, especially for the same timesteps, maybe just crop around the center?\n\nFigre 4 and 5 are also way too small. and the qualitative results unclear."}, "questions": {"value": "I am somwhat confused by the statement in section 5.2 that says that the same noise provided to different models leads to two different images. Why shouldn't it? (the gradient difference could be visualized/quantified better)\n\nCan you think of a concrete test for whether a model has learned to use global information instead of local information to predict?\n\nHow exactly are the recall and precision in Table 2 defined/calculated?\n\nSmall nit, why do you have different masking ratios in 4a than in 4b and 4c?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wRXIy1lysI", "forum": "EKMKPwOAuY", "replyto": "EKMKPwOAuY", "signatures": ["ICLR.cc/2026/Conference/Submission1695/Reviewer_J3hx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1695/Reviewer_J3hx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729441462, "cdate": 1761729441462, "tmdate": 1762915858559, "mdate": 1762915858559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission proposes a new diffusion model to mitigate spatial inconsistency caused by the models' locality.\nThe proposed method utilizes atrous (dilated) convolution strategy combined with Simplified Masked Diffusion (SMD) training.\nCompared with the original diffusion models, SMD provided training gradients with higher variance without bias.\nExperiments are conducted using some toy data and CIFAR10 32×32, CelebA-50K 64×64, and LSUN Bedroom 32×32. Improved FIDs and precision and recall of generated images are reported."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Masking strategy is . Deeply investigating the effect of masking in convolutional diffusion models is a meaningful direction of research. \n- Mitigating locality dependency seems a promising direction, especially non-attentive convolutional diffusion models."}, "weaknesses": {"value": "- The organization/technical clarity of the paper is not excellent. \"Atrous learning\" is featured in the title and Introduction, but the relationship between atrous learning and SMD is not clear. Details of the network architecture and other training parameters are missing.\n\n- Omega locality is defined in Sec 4.2 using a certain amount of text but is not used in the following theoretical and empirical analyses, which causes a feeling of somewhat shallow discussion.\n\n- Experiments are limited in the small-scale datasets, and the generality of the method for larger scales are unclear."}, "questions": {"value": "- Which network architecture was used and how atrous convolution was incorporated? Or atrous convolution is not used but the masked training is termed \"à trou\" (holed)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2Acoe7ykt5", "forum": "EKMKPwOAuY", "replyto": "EKMKPwOAuY", "signatures": ["ICLR.cc/2026/Conference/Submission1695/Reviewer_zgz8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1695/Reviewer_zgz8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962997076, "cdate": 1761962997076, "tmdate": 1762915858366, "mdate": 1762915858366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}