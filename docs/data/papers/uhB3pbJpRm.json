{"id": "uhB3pbJpRm", "number": 3648, "cdate": 1757491283300, "mdate": 1759898076841, "content": {"title": "Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization", "abstract": "Vision-Language-Action (VLA) models have shown remarkable achievements, driven by the rich implicit knowledge of their vision-language components. However, achieving generalist robotic agents demands precise grounding into physical interactions, especially in contact-rich scenarios where fine-grained force control is essential. We advance VLAs' implicit knowledge beyond identifying what to do, towards guiding how to physically interact with real world. This paper introduces Tactile-VLA, a novel framework that deeply fuses vision, language, action, and tactile sensing. This framework incorporates a hybrid position-force controller to translate the model's intentions into precise physical actions and a reasoning module that allows the robot to adapt its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's effectiveness and generalizability in three key aspects: (1) enabling tactile-aware instruction following, (2) utilizing tactile-relevant commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key finding is that the VLM's prior knowledge already contains semantic understanding of physical interaction; by connecting it to the robot's tactile sensors with only a few demonstrations, we can activate this prior knowledge to achieve zero-shot generalization in contact-rich tasks.", "tldr": "", "keywords": ["tactile", "VLA", "robot manipulation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/229247b27161727cc5c520e6cb079e9f0a4e6dc2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel VLA framework integrated with tactile feedback for unlocking the physical knowledge and resolving some contact-rich manipulation tasks. More than this, this paper introduces the position-force hybrid control strategy for impedence close-loop control. In experiment, this paper conducts three experiments in demonstrating that this pipeline can be better than traditional VLA model and generalize to new task via CoT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper demonstrates that intergrating tactile feedback as an input for tuning VLA with a small set of data can be helpful for robotics control contact-rich manipulation tasks.\n\n2. This paper utilize hybrid force-position controls for adaptive impedence control, which is helpful for close-loop control in contact-rich manipulation tasks.\n\n3. Using CoT, the policy can recover from failure and generalize to new scenerio.\n\n4. Demonstrate its effectiveness on three robotics tasks"}, "weaknesses": {"value": "1. There are multiple other concurrent Tactile VLA papers, such as [1, 2]. It's better to discuss the difference between this one and the others.\n\n2. Pre-trained visual and language encoders are strong and aligned with each other. However, tactile encoder used in this paper is not. The other works on Tactile VLA [1,3 ] utilizes contrastive learning to address this challenge. Some other prior works [4, 5] also did constrastive pretraining. Highlight this challenge and disscuss those related work is helpful\n\n3. One major and interesting contribution of this paper is the hybrid force-posiiton controller. However, here is no experiment to show that VTLA with only position control is worse than this hybrid controller. (the baseline should be using tactile input but only inference positions)\n\n4. It's also helpful to do more experiments for highlighting the contribution for adding tactile feedback into the pretrained vlm for physical understanding. One baseline which is helpful should be inferencing the hybrid force and position control signals without tactile input (where pi0 only inference positions).\n\n5. The CoT method might limit to use failure data to train on a single task and cannot be generalized to new tasks. Also, this method is just tested with the wiping task, where the failure of this specific task is easy to be detected by visual feedback and recovered by language instructions.\n\n6. This paper didn't compare with other SOTA multisensory imitation learning methods, such as Reactive Diffusion Policy. Since this paper claims that VLM has semantic understanding for physical interactions, it's good to valid it by comparing with learning from scratch.\n\n7. I think the writing has some space to improve since some key concepts are still not clear. For more details, check questions.\n\n[1]. Cheng et al., OmniVTLA: Vision-Tactile-Language-Action Modelwith Semantic-Aligned Tactile Sensing, 2025\n\n[2]. Zhang et al., VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation, 2025\n\n[3]. Jones et al., Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding, ICRA 2025\n\n[4]. Yang et al., Binding Touch to Everything: Learning Unified Multimodal Tactile Representations, CVPR 2024\n\n[5]. Fu et al., A Touch, Vision, and Language Dataset for Multimodal Alignment, ICML 2024"}, "questions": {"value": "1. What is the tactile sensor used in this paper?\n\n2. Is the force gotten from tactile sensor or any other sensor? I guess it shown be from tactile sensor since it can be collected by UMI. Then, how to get it from tactile sensor?\n\n3. Using Tactile-VLA-CoT, what are the final language input for vla should be like? The new instruction will be added to the initial prompt, or it will be a entire instruction? Give some examples for this inference process should be helpful\n\n4. What are the \"fixed intervals\" of CoT? How much time required for each generation / replanning?\n\n5. What is the inference frequency of VLA? What is the control frequency for the hybrid position / force control?\n\n6. Why the performance is even worse for in-domain scenerio with CoT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Y2bX8u5XpS", "forum": "uhB3pbJpRm", "replyto": "uhB3pbJpRm", "signatures": ["ICLR.cc/2026/Conference/Submission3648/Reviewer_Ntzj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3648/Reviewer_Ntzj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768281599, "cdate": 1761768281599, "tmdate": 1762916898750, "mdate": 1762916898750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Tactile-VLA, a framework that fuses tactile sensing with vision, language and action modalities for pretrained VLAs. They compare the proposed framework with existing VLAs and show superior performance in terms of both task success across varied objects and alignment of applied force with language instructions. The paper also introduces chain-of-thought (CoT) for tactile sensing to help generalize to novel observations unseen during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a novel framework for merging tactile sensing into existing VLAs. The authors demonstrate results on 3 tasks and on a varied set of objects to highlight the force-steering as well as task completion ability of the method.\n- The paper introduces a CoT method for reinforcing tactile feedback into VLAs for improving task performance.\n- The paper uses a hybrid position-force controller to acquire force targets as opposed to directly predicting actions. This design choice seems to help the model disambiguate semantic queues in the language instruction better."}, "weaknesses": {"value": "- A clear description of the action space for the model would help to make the method clearer for the reader. What is the current action space for the network? From what I understand, the network predicts robot pose, continuous gripper values (?), and a target force (?) for the hybrid controller. Is this correct? If yes, how do you obtain the target force values during training?\n- How do the authors control for “soft” versus “hard” grasps during data collection? My guess is that the demonstrator is instructed to perform soft or hard grasps but is the data filtered when the demonstrator makes a mistake? If yes, what is this filtration strategy? Further, it would be great if the authors could comment on the difficulty of collecting such a semantically aligned dataset for human demonstrators who cannot directly feel touch during data collection.\n- The authors must cite prior works that also use similar position-force controllers. For example, FTF [1].\n- What is the frequency at which the policy can be deployed? One advantage of tactile sensing is that tactile readings can often be obtained at a higher frequency than visual observations and hence, can enable more reactive control [2]. I am curious about the authors’ thoughts about this since I believe that directly giving tactile input to a VLA would limit the deployment frequency (due to the large size of VLAs).\n- It would be interesting to include CoT results for other baselines. I am curious if CoT can improve performance even without tactile sensing (i.e. with vision only observations).\n- Are the pi-0 baselines finetuned on the same datasets?\n- In Section 3.4, is each failure demo manually annotated with appropriate language label?\n- The paper must discuss limitations of the proposed method.\n\n[1] Adeniji, Ademi, et al. \"Feel the Force: Contact-Driven Learning from Humans.\" arXiv preprint arXiv:2506.01944 (2025).\n[2] Xue, Han, et al. \"Reactive diffusion policy: Slow-fast visual-tactile policy learning for contact-rich manipulation.\" arXiv preprint arXiv:2503.02881 (2025)."}, "questions": {"value": "It would be great if the authors could address questions in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YVui1YAiZs", "forum": "uhB3pbJpRm", "replyto": "uhB3pbJpRm", "signatures": ["ICLR.cc/2026/Conference/Submission3648/Reviewer_5tMd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3648/Reviewer_5tMd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849667397, "cdate": 1761849667397, "tmdate": 1762916897910, "mdate": 1762916897910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper present Tactile-VLA, a unified vision–language–action–tactile framework for physically grounded control. It propose several key insight and contribution: 1. Hybrid position-force controller, translating VLM-inferred intentions into physically feasible motion (balancing position tracking with adaptive force). 2. Tactile-VLA-CoT, a reasoning-augmented variant that uses Chain-of-Thought (CoT) reasoning over tactile feedback to detect and recover from failures.\n\nExperiemtns shows stronger performance than baselines that only have visual/proprio data.\n\nHowever, there are some concerns and limitations. For example: 1. The experiment uses small amount of data to train a task-specific model, which cannot make the large VLA fully unleash its performance. 2. A lot of details about real world experiments and training process are missing, making the experiments not sound enough."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Tactile-VLA moves VLAs from understanding what to do → understanding how to feel while doing it. By coupling tactile sensing with language-driven reasoning and hybrid force control, it achieves interpretability, adaptability, and genuine physical grounding that current VLAs lack.\n\nIt introduces Hybrid position-force controller and Tactile-VLA-CoT to further improve the performance and complete the whole pipeline.\n\nThe real world experiments show improved performance than baselines"}, "weaknesses": {"value": "1. The experiment uses small amount of data to train a task-specific model, which cannot make the large VLA fully unleash its performance. \n\n2. A lot of details about real world experiments and training process are missing, making the experiments not sound enough. Please provide details about exact model architecture, hyper-parameter, training details, data collection details and so on.\n\n3. Please explain more details about why you think the comparison between the proposed method and pi-0 baselines are fair. In fact, Pi-0 is pretrained on it's own robot without domain specific data. I cannot find any details about if you finetune Pi-0 with the collected data and how to perform fair comparison. Did Pi-0 also use the failure data somehow? Please explain this."}, "questions": {"value": "There are many details and information missing in both main paper and supp materials. Please provide them to make a sound paper and fit for top conference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7V4rT0SVV1", "forum": "uhB3pbJpRm", "replyto": "uhB3pbJpRm", "signatures": ["ICLR.cc/2026/Conference/Submission3648/Reviewer_3TPo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3648/Reviewer_3TPo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762078650285, "cdate": 1762078650285, "tmdate": 1762916896754, "mdate": 1762916896754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Tactile-VLA, a vision-language-action framework that fuses tactile sensing with visual, language, and proprioceptive inputs to enable contact-rich manipulation. A token-level multimodal fusion policy outputs target pose and target force, executed by a hybrid position–force controller that adjusts position commands to meet force targets; a CoT variant (Tactile-VLA-CoT) periodically reasons over tactile feedback to replan after failures. Demonstrations are collected with a UMI-based handheld device equipped with high-resolution tactile sensors. Experiments on USB/charger insertion, tabletop grasping, and board wiping show (i) tactile-aware instruction following and force scaling from language, (ii) tactile-relevant commonsense for different objects, and (iii) adaptive tactile-involved reasoning that boosts zero-shot success on a new substrate (i.e., the blackboard of wiping task)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths are of this works are\n\n- Makes a compelling case for integrating tactile signals into pre-trained VLA models, elevating touch from an auxiliary cue to a first-class signal for contact-rich manipulation.\n\n- Clear writing and structure. The problem setup, model components, and training/execution loops are well organized and easy to follow."}, "weaknesses": {"value": "The weaknesses are shown below:\n\n- Please specify the exact architecture of the tactile-aware action expert—this detail is missing from the Methods section. Figure 2 appears technically inconsistent: it only shows proprioceptive states as inputs, whereas the text states the model consumes the full multimodal prefix (vision, language, proprioception, tactile). Please correct the figure and clearly depict how all modalities are processed via the expert policy.\n\n- The “hybrid position–force” controller is effectively PD tracking with a force error injection, which carries little algorithmic novelty. Given the emphasis on language terms like “soft”/“hard,” why not learn a simple force selector/classifier from a small set of labeled demonstrations (or even use a rule-based heuristic. Given the reliance on foundation models, a simpler alternative is to query a VLM for the desired force profile from the language prompt and use that directly.) to map language to desired force profiles directly? This seems to be what the action expert implicitly learns—please justify the chosen complexity and if possible, compare against these simpler baselines.\n\n- The baseline comparison are not fair enough. The paper compares only against original VLA models without tactile input, which predictably underperform. Given that the showcased tasks are primarily grasping and insertion, the evaluation should include vision–tactile policy baselines (e.g., recent visuotactile grasping/in-hand works) for a fair comparison. Alternatively, this omission would be less concerning if the paper matched the breadth and diversity of experiments seen in pi_0-style evaluations—but in its current scope, that’s not the case.\n\n[1]. Han, Yunhai, et al. \"Learning generalizable vision-tactile robotic grasping strategy for deformable objects via transformer.\" IEEE/ASME Transactions on Mechatronics 30.1 (2024): 554-566.\n\n[2]. Suresh, Sudharshan, et al. \"NeuralFeels with neural fields: Visuotactile perception for in-hand manipulation.\" Science Robotics 9.96 (2024): eadl0628.\n\n[3]. Li, Hao, et al. \"See, hear, and feel: Smart sensory fusion for robotic manipulation.\" arXiv preprint arXiv:2212.03858 (2022).\n\n- Tasks are relatively simple and largely grasping-centric, yet require ~100 trajectories per task. What is the task parameter range (object poses, tolerances, textures)? If narrow, the high data requirement raises concerns about memorization rather than robust generalization. Please report distribution ranges and show performance under broader sampling.\n\n- In line 70, the statement that haptics is treated merely as supplementary and “not directly involved in the action” is not accurate. There is a substantial body of vision–tactile policy learning where tactile signals directly inform actions; even in the VLA fine-tuning space, recent work integrates tactile signals (e.g., [4]). Please revise the positioning and cite appropriately.\n\n[4]. Cheng, Zhengxue, et al. \"OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing.\" arXiv preprint arXiv:2508.08706 (2025)."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pAGW6mHMVm", "forum": "uhB3pbJpRm", "replyto": "uhB3pbJpRm", "signatures": ["ICLR.cc/2026/Conference/Submission3648/Reviewer_r9or"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3648/Reviewer_r9or"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762108273187, "cdate": 1762108273187, "tmdate": 1762916895691, "mdate": 1762916895691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}