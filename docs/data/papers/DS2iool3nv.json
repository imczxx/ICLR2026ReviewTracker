{"id": "DS2iool3nv", "number": 6614, "cdate": 1757990655182, "mdate": 1759897905049, "content": {"title": "AutoFloorplan: Evolving Heuristics for Chip Floorplanning with Large Language Models and Textual Gradient-Guided Repair", "abstract": "Chip floorplanning is the cornerstone of modern Very Large Scale Integration (VLSI) design, but it remains an impenetrable NP-hard combinatorial optimization problem. Designing effective heuristic algorithms to explore its large solution space under complex constraints is a challenging task that traditionally relies on rich human expertise. In this study, we propose AutoFloorplan, a novel evolutionary learning framework that automatically discovers complex floorplanning heuristics. We utilize Large Language Models (LLMs) as intelligent population generators capable of creating diverse and semantically rich heuristics expressed as code. However, a fundamental challenge is that many LLM-generated heuristics are invalid and do not conform to the strict geometric and topological constraints of floorplanning. To address this problem, we design a novel repair operator based on textual gradients. This operator analyzes the causes of inefficiencies in the generated heuristics and provides corrective feedback to steer the algorithmic structure towards effective and high-performance alternatives. Our framework significantly improves the speed of discovering legitimate and effective heuristics and iterating on algorithm performance. Extensive experiments on eight different public circuits show that AutoFloorplan outperforms current State-of-the-Art floorplanning algorithms. The code of AutoFloorplan can be found at https://anonymous.4open.science/r/AutoFloorplan-main.", "tldr": "This paper presents FloorplanEvo, an AI framework using Large Language Models to automatically generate chip floorplanning heuristics. A novel repair mechanism fixes invalid AI-generated code, creating algorithms that outperform SOTA methods.", "keywords": ["Heuristic Search", "Evaluation and Analysis", "Algorithm Design Automation", "Chip Floorplan"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c5f041f4501c782978ad092f68f29b21891a8a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes AutoFloorplan, an evolutionary learning framework that leverages LLMs to generate and refine heuristic algorithms for chip floorplanning, which aims to explore strategies automatically in such a human expertise-driven problem. The key innovation of the work lies in applying automated heuristic design techniques to chip floorplanning, an NP-hard combinatorial optimization problem with complex constraints, and incorporating a textual gradient-guided repair mechanism to improve the efficiency and effectiveness of designing legitimate floorplan algorithms. Experiments on public benchmarks show that proposed method achieves improvements on area and wirelength metrics, and outperforms current state-of-the-art floorplanning algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: The work is the first to introduce automated heuristic design to the chip floorplanning problem, and design a mechanism with TextGrad to handle the complex constraints of chip floorplanning.\n\n2. Significance: The proposed method demonstrates effectiveness in tackling the chip floorplanning problem, which is an NP-hard combinatorial optimization problem with complex constraints.\n\n3. Quality: The proposed method AutoFloorplan outperforms current state-of-the-art floorplanning methods on several cases from GSRC and MCNC datasets."}, "weaknesses": {"value": "1.  Insufficient Contribution: The work combines EoH and TextGrad and apply them to chip floorplanning problem, where the incremental contribution may not be that novel or  significant, and the idea is somewhat simple and straightforward.\n\n2.  Lack of Visualization: The experimental section lacks visualization of floorplan results.\n\n3. Writing Issues: Several typos (e.g., \"Freedback\" in Figure 2, \"bshenaselines\" on line 314) and formatting irregularities (e.g., captions are below the tables) are found, and the structure of Related Work section is somewhat disorganized."}, "questions": {"value": "1. Except for those mechanisms designed for applying AHD techniques to chip floorplanning, are there any other new insights or algorithmic innovations that AutoFloorplan introduce? \n\n2. A number of prompts are designed for exploring chip floorplanning heuristics, are these prompts still required to be designed with sufficient expert knowledge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nwuoz6vK1d", "forum": "DS2iool3nv", "replyto": "DS2iool3nv", "signatures": ["ICLR.cc/2026/Conference/Submission6614/Reviewer_Fmdo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6614/Reviewer_Fmdo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400216022, "cdate": 1761400216022, "tmdate": 1762918934624, "mdate": 1762918934624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AutoFloorplan, a framework that uses Large Language Models (LLMs) to automatically generate and refine chip floorplanning algorithms. The core contribution is a \"textual gradient-guided repair\" mechanism. Instead of discarding invalid algorithms generated by the LLM, this mechanism analyzes the cause of failure and provides corrective textual feedback to the LLM, guiding it to repair the algorithm's logic. The authors present this as a two-loop process: an outer evolutionary loop that generates a diverse population of algorithms, and an inner repair loop that corrects invalid solutions. The authors test this method on public MCNC and GSRC benchmark circuits."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper gets at the interesting idea of using language-based feedback to guide floorplanning optimizers. As the capabilities of large language models improve over time, this direction of research could become increasingly significant for automated algorithm design in complex problem spaces like EDA."}, "weaknesses": {"value": "The primary weakness of this paper is its clarity. The central ideas are promising, but the explanation could be significantly improved to help readers fully appreciate the contribution.\n\n-   Clarity of the Central Premise: A core challenge in understanding the paper is the ambiguity around what is being optimized. It was not immediately clear which \"heuristics\" are the actual targets of the optimization process, and this lack of clarity persists deep into the methodology section. The paper would benefit significantly from a more explicit definition of the optimization target early on.\n\n-   Clarity of Terminology: The paper would be improved by providing clearer definitions for some key terms. For instance, the term \"semantically invalid\" could be explained more concretely in the context of floorplanning algorithms. Similarly, \"mapping planning\" is introduced without a definition, which may be unfamiliar to some readers.\n\n-   Completeness of Literature Review: The related work section could be strengthened by including and discussing several highly relevant recent papers which also explore the use of LLMs for placement optimization. These include, but are not limited to:\n    -   Yao, X., Jiang, J., Zhao, Y., Liao, P., Lin, Y. and Yu, B. \"Evolution of Optimization Algorithms for Global Placement via Large Language Models.\" *arXiv preprint arXiv:2504.17801* (2025).\n    -   Agnesina, A., Rajvanshi, P., Yang, T., Pradipta, G., Jiao, A., Keller, B., Khailany, B. and Ren, H. \"AutoDMP: Automated dreamplace-based macro placement.\" In *Proceedings of the 2023 International Symposium on Physical Design* (2023).\n\n-   Explanation of the Core Methodology: The explanation of the novel \"textual gradient\" mechanism could be more detailed. While the paper mentions adopting prompts from TextGrad, it would be helpful to include a brief, self-contained explanation of how this concept works. Additionally, the process by which the system \"deeply analyzes the root cause of the algorithm's failure\" could be further elaborated.\n\n-   Justification for Design Choices: The paper would benefit from additional justification for some key design choices. For example, explaining the rationale for including both chip area and HPWL in the fitness function would help readers understand the trade-offs involved. A brief discussion on why these metrics were chosen over others, like congestion or density, would also be valuable.\n\n-   Structural Organization: The paper's organization could be adjusted to improve readability. The introduction could be made more self-contained to give readers a clearer initial overview. It would also be helpful to list key components or steps before their corresponding subsections, such as for the evolutionary process. Guiding the reader through Figure 2 more explicitly and presenting the baseline method names in the main text would also enhance the flow.\n\n-   Minor Issues: The manuscript has several minor presentation issues that could be addressed.\n    -   Citations should be checked for proper formatting.\n    -   There are occasional grammatical errors (e.g., \"LLM\" could be pluralized to \"LLMs\" where appropriate).\n    -   Some sentences can be reworded for clarity (e.g., \"The more reasonable way we treat to the invalid outputs...\").\n    -   Figure references should be consistently capitalized (e.g., \"figure 2\" → \"Figure 2\")."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "urjP9yacGN", "forum": "DS2iool3nv", "replyto": "DS2iool3nv", "signatures": ["ICLR.cc/2026/Conference/Submission6614/Reviewer_a1Vg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6614/Reviewer_a1Vg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761591527832, "cdate": 1761591527832, "tmdate": 1762918934108, "mdate": 1762918934108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AutoFloorplan to discover complex floorplanning heuristics using LLM based evolutionary search. Additionally, this paper introduces a novel repair operator based on textual gradients. The experimental results show that the method can achieve better results than the methods compared."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, This paper leverages the advance of code generation using LLM into the problem of floor planning, in which heuristic rules are preferred.\n2. The text-based repair operators are reasonable in the LLM-based code evolution.\n3, The experimental results illustrate the promise of the proposed methods"}, "weaknesses": {"value": "1: Novelty of the proposed method should be further strengthened.\n2: It is better to include more experiments like on larger dataset.\n3. More aspects of the proposed method should be discussed."}, "questions": {"value": "1: Most of the methods have been proposed in the previous work. From Figure 2. The evolutionary learning process is similar to other LLM based evolutionary code search. The textual gradient-guided repair comes from the textgrad or other similar work. \n\n2. The distinguished requirement from floorplanning (if exists) should be further considered in the method design. Currently, it is more like the application of the LLM based code generation to the floorplanning. Note that each domain has its own specific errors. Thus, errors, I think, do not bring much impacts on the code evolutionary.\n\n3. The method does not exhibit overwhelming advantages to other competitors. By combining the results from Table 1, 2 and 3, we can see that the proposed method achieves the similar performance to TODAES and ICCDE 24, but with high time cost.\n\n4. More recent works and large data set should be included in the experimental study\n\n5. LLM does not certainly produce the proper optimization hints. In such a case, the text hint will play a negative role.\n\n6.The generalization of the generated code to other similar problem should be discussed.\n\n7. It seems that it is not necessary to introduce formula 2 and 3. It is the optimization hint in the text form."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "byilFztXMb", "forum": "DS2iool3nv", "replyto": "DS2iool3nv", "signatures": ["ICLR.cc/2026/Conference/Submission6614/Reviewer_c5VC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6614/Reviewer_c5VC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829861084, "cdate": 1761829861084, "tmdate": 1762918933551, "mdate": 1762918933551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AutoFloorplan, a novel evolutionary learning framework that leverages Large Language Models (LLMs) to automatically discover high-quality heuristics for VLSI chip floorplanning. The key innovation lies in the textual gradient-guided repair mechanism, which enables LLMs to learn from invalid solutions by analyzing failure causes and generating structured corrective feedback. This approach significantly improves the efficiency of exploring valid and high-performance heuristic algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper gives the textual gradient-guided repair mechanism, that enables LLM to learn from invalid solutions rather than discarding them. \n- Addresses a real and hard problem in VLSI physical design, automatic discovery of valid and high-performance floorplanning heuristics under strict geometric and topological constraints.\n- Outperforms six state-of-the-art baselines on eight widely used benchmarks (MCNC and GSRC)."}, "weaknesses": {"value": "- This paper lacks of theoretical justification, and the concept of 'textual gradient' is intuitively appealing but theoretically vague. It is unclear whether it satisfies properties like directionality, smoothness, or convergence.\n- Limited statistical rigor since the experiments are single-run or poorly aggregated, with no standard deviation or statistical significance tests. It is unclear whether reported improvements are consistent or just due to randomness.\n- All experiments are conducted on small to medium-scale circuits (<=300 blocks), no validation on industrial-scale or modern SoC designs with > 1000 macros, fixed outlines, or timing constraints.\n- Ablation studies are incomplete, the ablation only with or without textual gradient repair, but does not explore impact of different repair iteration limits, effect of different prompt designs (such as loss, gradient or updated prompt)."}, "questions": {"value": "- What exactly is a “textual gradient” in your framework? Can it satisfy any properties analogous to classical gradients (e.g, descent direction, smoothness, convergence guarantees)\n- Can authors provide a formal or intuitive justification that the LLM-based feedback monotonically improves the heuristic? Does the repair success rate degrade as circuit size or complexity increases? Can the experiences data (success or failure) be SFT or RFT into the LLM? Is there a failure rate in this framework (heuristics that remain invalid after repairs)?\n- Are prompts transferable across different LLMs (chatGPT, deepseek, qwen), do this framework need model-specific tuning?\n- Could authors go one step further that feed the data generated by this framework, then back these into the LLM, thereby enabling it to directly produce floorplans (like chipFormer,https://arxiv.org/abs/2306.14744)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "64FaHwLnJJ", "forum": "DS2iool3nv", "replyto": "DS2iool3nv", "signatures": ["ICLR.cc/2026/Conference/Submission6614/Reviewer_Wjok"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6614/Reviewer_Wjok"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923859228, "cdate": 1761923859228, "tmdate": 1762918933103, "mdate": 1762918933103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}