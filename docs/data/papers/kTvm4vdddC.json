{"id": "kTvm4vdddC", "number": 15963, "cdate": 1758257801700, "mdate": 1759897270505, "content": {"title": "QuAC: Quality-Adaptive Activation for Degraded Image Understanding", "abstract": "Degraded image understanding remains a significant challenge in computer vision. To mitigate the domain shift between high-quality and low-quality image distributions, we propose an adaptation approach based on activation functions rather than adjusting convolutional parameters. First, inspired by physiological findings in the human visual system, we introduce Quality-adaptive Activation (QuAC), a novel concept that automatically adjusts neuron activations based on input image quality to enhance essential semantic representations. Second, we implement Quality-adaptive meta-ACON (Q-ACON), which incorporates hyperparameters learned from image quality assessment functions. Q-ACON is efficient, flexible, and plug-and-play. Extensive experiments demonstrate that it consistently improves the performance of various networks—including convolutional neural networks, transformers, and diffusion models—against challenging degradations across multiple vision tasks, such as semantic segmentation, object detection, image classification, and image restoration. Furthermore, QuAC integrates effectively with existing techniques like knowledge distillation and image restoration, and can be extended to other activation functions. The code will be released after peer review.", "tldr": "", "keywords": ["dynamic activation", "image quality assessment", "image segmentation", "knowledge distillation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0de47368b6525a023e6e2e034d6dc88eeba5baa5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper is targeted at image understanding task when the image is of low-quality. The authors propose to insert one quality-adaptive activation module into the backbone model, which can adjust the model activation based on the image quality. In this way, the model could be adapted to image degradations while few parameters are altered. Experiments on various image understanding tasks show the effectiveness of the method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method can be adopted by different image understanding tasks. This shows the generalization of the method.\n\n2. Instead of concatenating one image restoration module before the backbone, the proposed method is more effective through coping with the feature of the backbone model.\n\n3. The writing is easy-to-follow."}, "weaknesses": {"value": "1. The proposed method is still one implementation of hypernetwork so that the comparison between \"dynamic activation\" and \"quality-adaptive activation\" in Figure 1(b) is not reasonable.\n\n2. Rectifying the feature of model to deal with image degradation has already been proposed previously in [1],[2] and their methods are highly similar to this paper.\n\n3. The degradation types are pretty limited in experiments. Only blurring, noise, JPEG compression are considered, which are not enough to represent most conditions in reality.\n\n4. The comparison is not sufficient. The method is only compared with backbone with different activations. There should be comparison with models when concatenating image restoration models before the backbone.\n\n[1] Wang, Yang, et al. \"Deep degradation prior for low-quality image classification.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[2]Zhou, Shengchao, et al. \"Robust feature rectification of pretrained vision models for object recognition.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 3. 2023."}, "questions": {"value": "1. Since a new module is plugged into the backbone network, will the performance on high-quality images be affected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D4NQRMTNaT", "forum": "kTvm4vdddC", "replyto": "kTvm4vdddC", "signatures": ["ICLR.cc/2026/Conference/Submission15963/Reviewer_4rBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15963/Reviewer_4rBy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552059042, "cdate": 1761552059042, "tmdate": 1762926171455, "mdate": 1762926171455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes QuAC, a quality-adaptive activation mechanism that modulates neuron activations using image-quality cues to reduce the HQ-LQ domain gap. An instantiation, Q-ACON, plugs into existing models with minimal overhead and yields consistent gains across segmentation, detection, classification, and restoration, including under unseen degradations and real-world conditions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Adapting activations via BIQA-derived quality representations is a simple, plug-and-play idea applicable to multiple activation families and architectures (CNNs, transformers, diffusion), with clear formalization and minimal architectural disruption.\n2. Broad experiments across tasks/datasets plus ablations show consistent improvements with small parameter cost."}, "weaknesses": {"value": "1. While the paper reports extensive experiments, the motivation needs to be articulated more clearly, particularly the rationale for introducing a quality-perception module.\n2. Different tasks often require different architectures (e.g., super-resolution may favor diffusion-based networks). These design choices should be taken into account; the current implementation does not feel simple or elegant.\n3. The related-work section omits several advanced BIQA methods, such as VLM-based Q-Instruct and VisualQuality-R1.\n4. The approach fails when a corresponding high-quality reference image is unavailable."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vuotxrw8WT", "forum": "kTvm4vdddC", "replyto": "kTvm4vdddC", "signatures": ["ICLR.cc/2026/Conference/Submission15963/Reviewer_CeJy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15963/Reviewer_CeJy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585873521, "cdate": 1761585873521, "tmdate": 1762926171022, "mdate": 1762926171022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces QuAC (Quality-Adaptive Activation), an innovative approach to improving the performance of image understanding tasks under degraded conditions. Inspired by human visual system mechanisms, QuAC dynamically adjusts neuron activations based on the input image's quality, enhancing the semantic representations for degraded images. The approach is implemented using Quality-Adaptive Meta-ACON (Q-ACON), a flexible, efficient, and plug-and-play method that integrates seamlessly with convolutional networks, transformers, and diffusion models. Extensive experiments demonstrate that QuAC consistently improves the performance of various vision tasks, including segmentation, detection, classification, and restoration, especially in the presence of complex image degradations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1.\tInnovative Approach: The paper introduces the novel concept of Quality-adaptive Activation (QuAC), which dynamically adjusts activation functions based on the quality of the input image. This is a unique and meaningful contribution to improving image understanding tasks under degraded conditions.\n2.\tWide Applicability: QuAC is shown to be effective across different neural network architectures such as CNNs, transformers, and diffusion models, demonstrating its versatility in multiple vision tasks like segmentation, detection, and restoration.\n3.\tPerformance Improvement: Experimental results show that QuAC, specifically the quality-adaptive meta-ACON (Q-ACON), significantly enhances model performance, particularly in challenging low-quality image conditions, offering improvements over traditional activation functions."}, "weaknesses": {"value": "1. Limited Evaluation on Real-World Datasets: The paper primarily focuses on synthetic degradations and might benefit from more comprehensive evaluation on diverse real-world datasets to demonstrate its robustness and generalizability beyond controlled experiments.\n2， Hyperparameter Sensitivity: The dependency on quality assessment models (e.g., BRISQUE) for the quality tensor may raise concerns about the sensitivity of the method to these models. It would be helpful to explore the impact of different quality assessment models (Like LIQE, CLIP-IQA, Q-Align, DpeictQA and some Full-reference IQA metric: LPIPS, etc) on the performance of QuAC."}, "questions": {"value": "please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LM3K4EMUEA", "forum": "kTvm4vdddC", "replyto": "kTvm4vdddC", "signatures": ["ICLR.cc/2026/Conference/Submission15963/Reviewer_4KCV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15963/Reviewer_4KCV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974252139, "cdate": 1761974252139, "tmdate": 1762926170616, "mdate": 1762926170616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors develop a framework for incorporating perceptual quality into a network, so as to improve performance under input degradation. Specifically, quality information is used to gate activations using the ACON activation function of Ma et al., 2021.\n\nExtensive experiments show modest but consistent performance improvements on a diverse set of networks and applications (segmentation, detection, restoration, classification, etc)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Robustness to degradations is important.\nThe method is simple, and tested extensively."}, "weaknesses": {"value": "The paper is dense in details, and some aspects of the text and figures are not clear.\n\nMethod: please define the function sigma (eq. 3), which is critical part of the method.\n\n- For results in table 1, and elsewhere, are the networks (including Q-ACON)  trained jointly on both conditions (Clear / Degrade), or are they trained separately for each?  Also, can you compute standard deveations for the reported values?  Without them, it is not possibe to know how significant the differences between methods are.\n\n- Fig 2:  This figure shows both the archecture and pipeline for computing QuAC, as well as the training setup.   caption should provide more information (top box expresses a QuaC-enabled application (e.g., segmentation).  Bottom left box computes training signals for high-quality images. etc.   \n\n- Fig 3: It is difficult to see much difference between the images arising from different methods.  Provide indications in the text for what readers should look for.  Same for figure A5."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yjkQsL7K7m", "forum": "kTvm4vdddC", "replyto": "kTvm4vdddC", "signatures": ["ICLR.cc/2026/Conference/Submission15963/Reviewer_hPif"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15963/Reviewer_hPif"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762196114186, "cdate": 1762196114186, "tmdate": 1762926169982, "mdate": 1762926169982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}