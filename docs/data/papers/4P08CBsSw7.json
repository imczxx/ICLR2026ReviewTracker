{"id": "4P08CBsSw7", "number": 13139, "cdate": 1758213990301, "mdate": 1759897461476, "content": {"title": "Intervening to learn and compose causally disentangled representations", "abstract": "In designing generative models, it is commonly believed that in order to learn useful latent structure, we face a fundamental tension between expressivity and structure. In this paper we challenge this view by proposing a new approach to training arbitrarily expressive generative models that simultaneously learn causally disentangled concepts. This is accomplished by adding a simple decoder-only module to an existing decoder that can be arbitrarily complex. The module learns to process concept information by implicitly inverting linear representations from an encoder. Inspired by the notion of intervention in a causal model, our module selectively modifies its architecture during training, allowing it to learn a compact joint model over different contexts. We show how adding this module leads to causally disentangled representations that can be composed for out-of-distribution generation on both real and simulated data. To further validate our proposed approach, we prove a new identifiability result that extends existing work on identifying structured representations in nonlinear models.", "tldr": "", "keywords": ["causal disentanglement", "generative models", "intervention", "composition", "abstraction", "causality"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44ebfc97f0002208502752d679799b3068b03549.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper adds a simple decoder module that helps complex generative models learn causal and composable latent factors for out-of-distribution generation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. I like the idea not assuming independent factors and instead keeping a reduced-form SCM inside the module.\n\n2. Plug-in, decoder-only model and easy to use for black box encoder decoder structure."}, "weaknesses": {"value": "1. The problem formalization/setup part is not clear (lines 161–237).  \nThe authors should clarify the formalization of the problem setup and assumptions in the main text. Since this paper aims to contribute both theoretically and empirically, explicitly expressing the definitions and assumptions in more rigors way would make the presentation much clearer.\n\nFor example, you assume c = C e, but it is unclear whether this means that for a specific dataset with fixed concepts, all possible encoders (and their embeddings) are assumed to satisfy this property, or whether you only assume that there exists some encoder such that c = C e (∃ e).\n\nAnother example: in line 212, the phrase “as a result” lacks logical justification for moving from c = C e to a linear SEM. The equation c = C e does not imply that the internal relationships among the components of c are linear or that they can be represented as a linear SEM. I think c = C e and the linear SEM should be treated as two parallel assumptions rather than one deriving from the other.\n\n2. Lines 263–269 are also confusing.  \nIn Remark 3.1, you say “Due to the reduced-form SEM above, our approach does not and cannot model the structural causal model encoded by α_kj,” but when explaining how you perform interventions, you repeatedly mention “setting α·j = 0,” “zero out the row α·j,” etc. I suggest rephrasing the Concept Interventions section in the main text (perhaps adding pseudocode or a clear figure) to make this crucial part easier for readers to understand.  \nBy the way, “we zero out the row α·j while replacing the column α_j· with a new column β_j·” — here “column” and “row” might be a typo.\n\n3. Most of the experiments appear to be conditional generation/controllable generation rather than true interventions.  \nThis is becaus the factors in the datasets used for experiments seem to be independent. For example, in the appendix:\n''The remaining six concepts were sampled as follows:  \n• The observational context was generated by randomly sampling (c₁, …, c₆) independently from [0, 0.5];  \n• The interventional contexts were generated by isolating a particular concept and sampling it uniformly from [0.5, 1], while sampling the rest from [0, 0.5].  ''\n\nIn these cases, I do not see a significant difference between your claimed “intervention” and conditional/controllable generation. Introducing an SCM to represent independent factors would be clearly redundant and unnecessary. For a paper titled “Intervention …,” I would expect to see real interventions where changing one factor modifies the distributions of its downstream factors according to a causal order.\n\n4. It would be better if the paper included a comparison with other works on OOD generation that propose lightweight modules or regularization terms easily integrated into existing encoder–decoder architectures (if have any)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M93PF68a1b", "forum": "4P08CBsSw7", "replyto": "4P08CBsSw7", "signatures": ["ICLR.cc/2026/Conference/Submission13139/Reviewer_K2Xc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13139/Reviewer_K2Xc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760658803012, "cdate": 1760658803012, "tmdate": 1762923857827, "mdate": 1762923857827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a plug-in “context module” that turns any black-box VAE decoder into a causally disentangled generator. Concepts are modeled as linear projections of the original latent code; a reduced-form SEM with learnable intervention slices is appended to the decoder. Single- and multi-concept interventions can then be performed at test time without ever seeing the corresponding combinations during training. A new semi-synthetic benchmark (quad) is introduced for controlled evaluation. Theoretical justification is provided via an identifiability result under single-node interventions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Minimal architectural commitment: The module is small, differentiable, and works on top of existing pre-trained VAEs without fine-tuning the encoder/decoder.  \n2. Strong empirical gains: CM+NVAE achieves ≈ 2× lower sliced-Wasserstein distance on OOD compositions vs. pooled baselines on quad, 3DIdent, MNIST, CelebA.  \n3. Novel evaluation protocol: Shifts the focus from OOD reconstruction to OOD generation, a strictly harder task that better reflects causal disentanglement.  \n4. Identifiability guarantee: First result to show that single-node concept interventions (not embeddings) suffice to recover linear concept representations up to scale/permutation, without assuming a known causal graph.  \n5. Reproducibility: Code, hyper-parameters, and the quad dataset are publicly provided."}, "weaknesses": {"value": "1. Limited concept granularity: All experiments assume 1-D scalar concepts; extension to vectorial or hierarchical concepts is mentioned but not evaluated.  \n2. Intervention design: Interventions are synthetic (uniform shift in colour/scale); real-world interventions (e.g. medical imaging artefacts) may violate linearity or Gaussian exogeneity assumptions.  \n3. Missing baselines: No comparison with recent weakly-supervised disentanglement methods (e.g. CausalVAE, Ada-GVAE) or energy-based OOD generators; only β-VAE and NVAE are used.  \n4. Reconstruction penalty : CM+NVAE loses ≈ 0.2 bits/dim on 3DIdent (Table 3); acceptability for high-fidelity domains (faces, video) is not discussed.  \n5. Theoretical gaps：Identifiability proof assumes injective, differentiable f; real VAE decoders are piece-wise linear (ReLU) and rarely injective over the entire prior support."}, "questions": {"value": "1. How does the method behave if the encoder’s original latent space is not approximately linear in the chosen concepts?  \n2. Can the SEM tensor be factorised or made low-rank to scale to high-dimensional concepts?  \n3. Why not compare with CausalVAE or Disentangled GANs that also perform interventions?  \n4. What happens if interventions are correlated (e.g. colour & texture change together), violating the independent exogenous noise assumption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s84DFJaYpt", "forum": "4P08CBsSw7", "replyto": "4P08CBsSw7", "signatures": ["ICLR.cc/2026/Conference/Submission13139/Reviewer_NfYz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13139/Reviewer_NfYz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801040529, "cdate": 1761801040529, "tmdate": 1762923857574, "mdate": 1762923857574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a context module (CM) — a decoder-only extension for existing encoder-decoder models (e.g., VAE, NVAE) — to learn causally disentangled representations and enable OOD compositional generation. The idea is elegant and supported by theory, but the empirical evidence does not fully support the claim of resolving the expressivity–structure tradeoff. Reconstruction quality drops notably, scalability is unclear, and dependence on interventional data limits practical utility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Novel and modular design: A practical decoder-only component that adds causal structure to standard generative models.\n\n+ OOD focus: The paper evaluates that generation rather than reconstruction is conceptually strong.\n\n+ Theory and experiments: This work provides an identifiability proof and well-structured experiments with meaningful ablations."}, "weaknesses": {"value": "+ Poor scalability: The intervention layer grows rapidly with the number of concepts; experiments are limited to small-scale setups.\n\n+ Dependence on interventional data: Results degrade on CelebA, suggesting limited applicability to observational datasets."}, "questions": {"value": "1. Does the approach extend to diffusion or transformer-based decoders?\n\n2. How does the CM scale to larger concept sets (e.g., >10)?\n\n3. How much computational overhead does the context module add?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZzjZH6GtJ1", "forum": "4P08CBsSw7", "replyto": "4P08CBsSw7", "signatures": ["ICLR.cc/2026/Conference/Submission13139/Reviewer_XtZU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13139/Reviewer_XtZU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964868393, "cdate": 1761964868393, "tmdate": 1762923857264, "mdate": 1762923857264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}