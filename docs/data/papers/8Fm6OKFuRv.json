{"id": "8Fm6OKFuRv", "number": 916, "cdate": 1756823473156, "mdate": 1759898235843, "content": {"title": "Beyond Text-to-SQL: Can LLMs Really Debug Enterprise SQL?", "abstract": "SQL is the core of data analysis and engineering across industries, powering large-scale workflows for data extraction, transformation, and loading. However, in enterprise-level scenarios, it is challenging to generate fully correct SQL code in a single attempt—even for experienced developers or advanced \\ttsql LLMs. Multiple iterations of debugging are usually required, yet LLMs often get lost in multi-turn correction.\nTo address this gap, we introduce \\textbf{\\ourbench}, the first benchmark designed for enterprise-level SQL reasoning and debugging. Our benchmark is built upon two key innovations: (1) an \\textbf{automated construction workflow} that employs reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an \\textbf{execution-free evaluation framework} tailored for enterprise settings, providing fast, accurate, and resource-efficient assessment.\n%\n\\ourbench comprises $469$ \\ourbenchsyn queries featuring syntax errors with explicit error messages, and $516$ \\ourbenchsem queries targeting semantic errors where SQL fails to meet the requirement. The SQLs are substantially complex, averaging over $140$ lines with abstract syntax trees of high complexity (average width $>11$, depth $>8.7$).\n%\nWe evaluate nearly $30$ LLMs on \\ourbench. Even state-of-the-art reasoning models struggle: Claude-4-Sonnet achieves only 36.46\\% success on \\ourbenchsyn and 32.17\\% on \\ourbenchsem. Most models fail to reach 20\\% success, underscoring the significant gap between current LLM capabilities and the demands of enterprise SQL debugging.\n%\nTo bridge this gap, we systematically \\textbf{explore four potential solution strategies and conduct extensive experiments} to evaluate and compare their effectiveness.  Our experiments not only highlight the challenges but also shed light on effective strategies for advancing SQL debugging with LLMs.", "tldr": "", "keywords": ["SWE", "LLM", "benchmark", "SQL", "Bugfixing", "Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9196a2ee1b9b8ef893e0932cce7f528b30e6e7e9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents \"Squirrel Benchmark\", a dataset for evaluating LLMs on enterprise-level SQL debugging by automated bug injection, a hierarchical SQL error taxonomy, and an execution-free evaluation framework. Conduct experiments on 30 models and some analysis. Some of them echo with conclusions in existing works."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The authors are targeting a good research problem (SQL debugging) beyond Text-to-SQL only.\n2. This paper introduces an interesting dataset synthesis method for construction and evaluation of AST-Depth SQL debugging with construction and evaluation.\n3. Provides many empirical analysis across LLMs, exposing limitations and exploring future research directions."}, "weaknesses": {"value": "### 1. Overstatement of Claims\n\nI believe the main issue with this paper is the **overstatement** of how well the proposed benchmark reflects real-world enterprise features. Actually, the benchmark only covers a limited scope and is not very realistic. Let me specify point by point:\n\n1. Limited and Unclear SQL Dialect Coverage: As noted in [1], enterprise-level SQL often includes dialect-specific functions and analytical tools across platforms (e.g., BigQuery, T-SQL, Snowflake). However, it seems the authors rely solely on SQLite (inferred from their use of BIRD and Spider 1.0, both implemented in SQLite). SQLite is not representative of real enterprise SQL usage due to its limited optimization tools and lack of cloud integration. Popular enterprise dialects such as PostgreSQL, T-SQL, and Snowflake SQL are entirely absent. This narrow scope significantly limits the benchmark’s realism. And I did not find any section in the paper discussing this issue. In real industry situations, such issues that happen in professional dialects are also very common, where [1, 3] discuss this in their papers.\n\n2. In real enterprise settings (as emphasized in Line 015), **database scale** and **schema structure** are crucial. Prior works such as [1] includes metadata on large-scale databases (e.g., TB-level size, hundreds of columns), [2] collects from the real-world data science platforms, such as Kaggle. These works also provide *external knowledge banks* to support ambiguous queries, aligning with real-world engineering practice. In contrast, this paper provides no details about statistics, external references, or anything related. According to Lines 197–198 (“curated table definitions”), it seems their databases do not even include values, which seems like generated by AI or easily manually designed? This is questionable whether their databases are aligned with enterprise-level scenarios.\n\n4. Scalability vs. Bias Tradeoff: Authors claim scalability through LLM-based data synthesis, but this introduces potential bias. There is no evidence that they monitored or aligned LLM-generated content with human bias. Using LLMs to generate both domain scenarios and data conflicts with their claim of reflecting *real-world* enterprise applications. Moreover, post-filtering often requires expert intervention. It's not clear whether such external costs are even larger than human effort in annotation without LLM. So, scalability should be compared against purely human-annotated data before statement of scalability. In my opinion, post-filtering requests more professions because it requires understanding the generated data.\n\n### 2. Unreliable Bug Sources and Reproduction Methods\n\nThe authors emphasize “real-world enterprise-level complexity,” but their method for bug sourcing and reproduction is not clear.\n\n1. Unclear Data Sources for SQL Bugs: Prior work ([3], [4]) sourced bugs directly from real-world forums like Stack Overflow, ensuring authenticity, since numerous users and developers really post their issues there. Here, authors only mention “manual annotation” and “summarization” of bugs without specifying anything where the bugs come from (e.g., DBT, BigQuery [1], or Kaggle [2]). Without clear details, the taxonomy of bugs cannot be trusted to represent real enterprise issues.\n\n2. Lack of Contextual Coherence when Bug Injection: **In realistic settings, developers’ expertise levels affect bug types.** Skilled SQL professionals rarely make simple constraint errors, while novices typically do not use complex functions. Authors do not discuss how they ensure such contextual dependencies in their bug injection. As a result, the simulated buggy scenarios may be unrealistic.\n\n### 3. Unrealistic Scenarios\n\n1. **Potential Conflict Between “Minimal-Change” and Enterprise Complexity**  \n   The “minimal-change” principle conflicts with real enterprise debugging scenarios. Real-world SQLs, especially those lengthy and complex ones, often contain *multiple interdependent errors* (such as “chains of errors” shown in [3]). Such simplification in this paper can not represent such complexity. Without empirical evidence (e.g., a survey of real bug distributions), their assumption that enterprise-level bugs are typically isolated is unconvincing.\n\n2. Line 199 explicitly mentions filtering out SQLs below a certain **complexity threshold**. This selective filtering biases the benchmark toward long and complex SQLs, disregarding simple but common SQLs that happened in enterprise tasks. Simple SQLs also exist in enterprise like overview of DB schemas or changing privilege issues. Thus, methods developed on this benchmark may overfit to long queries while failing on practical, simpler requests. Simple queries do not happen in enterprise? I don't think so. Because some usage is simple but out of user knowledge, which may also invoke bugs. A good benchmark should reflect *true task diversity*, not just complexity. This raise a concern where authors try to make this benchmark as complex as possible; in fact, models may not that bad.\n\n3. In this paper, authors consider SQL patches as final output. I think the patch make sense in repo-level code debugging like Python, Java. Since those languages are procedural, however, SQL is a declarative language, in which the generation orders may differ from top to bottom. This might introduce additional challenges in generation for debugging. For example, in [10], they provided a one-shot example to teach the model how to use a patch. I’m curious how many of the errors actually stem from formatting issues rather than from a lack of SQL knowledge. The paper doesn’t seem to include any error analysis.\n\n### 4. Questionable Metrics\n\n1. **Use of EM for Evaluation**  \n   Authors themselves note that Exact Match (EM) is “notoriously strict” (Section 3.5), but still use it as the main metric. EM fails to capture semantic equivalence, especially in debugging. For example, a model might improve a query’s logic, e.g., replacing `ORDER BY LIMIT 1` with `MIN()`, but still be penalized under EM or GM. Such metrics do not reflect practical correctness.\n\n2. **Ambiguous Definition of pass@1**  \n   The paper claims to use *execution-free* evaluation but still reports pass@1 (Appendix Line 926). Traditionally, pass@1 implies passing all unit tests at first attempt. Without execution or unit tests, it is unclear what *pass* means here. This inconsistency questions the validity of their evaluation setup.\n\n### 5. Incomplete and Nontransparent Implementation\n\n1. The ReAct framework is classically single-agent, but the authors describe a “main” and “code” agent system. The roles, toolkits, and observation mechanisms for each are not clearly explained. Moreover, if SQLs are executed, why is evaluation still “execution-free”? This inconsistency needs clarification. Additionally, existing agent frameworks such as *Spider-Agent*, *BIRD-Fixer*, or *SQL-Act* [3] should have been used for comparison to justify design choices.\n\n2. **Incomplete Comparison of Training Techniques**  \n   Lines 475–476 claim that the SFT variants offer a comprehensive view, but widely used methods such as DPO [5], and GrPO [6,7,8] are missing. Without these, the training comparison is incomplete.\n\n3. **Potential Data Leakage in SFT Data**  \n   Lines 916–917 suggest that the SFT data were constructed using the same procedures and possibly the same annotators as the benchmark itself. This overlap could lead to unfair advantages and inflated performance.\n\n### 6. Missing Discussion of Related Work\n\n1. It is unclear whether each SQL contains a comparable mix of bug types. If not, some bug categories may appear easier simply because they occur in simpler contexts. The paper should clarify this design and compare difficulty across categories.\n\n2. **Lack of Comparison with Comprehensive Taxonomies:** Prior works such as [9] provide extensive taxonomies of SQL errors. The authors’ taxonomy requires direct comparison to illustrate the contribution.\n\n### 7. Similarity to Existing Works\n\n1. The benchmark synthesis method resembles Six-Gym [3], which also uses *reverse engineering* from correct SQLs, bug taxonomy, and bug reproduction. This raises concerns about originality.\n\n2. The paper fails to acknowledge [9], which also provides a comprehensive error taxonomy. This omission further weakens claims of novelty.\n\nIn conclusion, while this paper introduces an **interesting approach to data synthesis for SQL debugging**, it falls short of being a **rigorous benchmark for real-world enterprise use**. The reliance on **LLMs for scenario generation, bug injection, and filtering** introduces significant bias and undermines the realism that the paper claims to capture. Furthermore, the **absence of real database values**, **exclusion of simple cases**, **lack of dialect diversity**, and **unclear evaluation metrics** collectively reduce its practical and research usefulness.\n\n[1] Lei, et al. \"Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows.\" ICLR 2025.\n\n[2] Li, et al. \"Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls.\" NeurIPSAdvances 2023.\n\n[3] Li, et al. \"SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications.\" ICML 2025.\n\n[4] Lai, et al. \"DS-1000: A natural and reliable benchmark for data science code generation.\" ICML 2023.\n\n[5]: Yang, et al. \"Synthesizing Text-to-SQL Data from Weak and Strong LLMs.\" ACL 2024.\n\n[6]: Pourreza, et al. \"Reasoning-sql: Reinforcement learning with sql tailored partial rewards for reasoning-enhanced text-to-sql.\" arXiv (2025).\n\n[7]: Yao, et al. \"Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL.\" arXiv (2025).\n\n[8]: Ma, et al. \"Sql-r1: Training natural language to sql reasoning model by reinforcement learning.\" arXiv (2025).\n\n[9]: Liu, et al. \"Nl2sql-bugs: A benchmark for detecting semantic errors in nl2sql translation.\" SIGKDD 2025.\n\n[10]: Yang, et al. \"Swe-agent: Agent-computer interfaces enable automated software engineering.\" NeurIPS 2024."}, "questions": {"value": "1. How do authors justify claiming enterprise-level realism when the benchmark appears limited to SQLite?\nCould you clarify whether and how other major SQL dialects (e.g., PostgreSQL, T-SQL, Snowflake) were considered, emulated, or excluded? And how this affects benchmark generalizability to enterprise contexts?\n2. What are the concrete sources of the SQL bugs used in the benchmark? Were any real-world datasets, developer forums (e.g., Stack Overflow), or production logs referenced to ensure that bug types and contexts reflect actual enterprise issues rather than synthetic patterns?\n3. How did authors monitor or mitigate bias introduced by LLM-generated databases and bug scenarios?\nCan you provide quantitative or qualitative evidence that LLM-based synthesis truly improves scalability compared to traditional human annotation, rather than introducing new biases or costs (e.g., in post-filtering)?\n4. Could authors explain the rationale for using Exact Match (EM) and “pass@1” under an execution-free evaluation?\nHow are these metrics defined to capture semantic correctness or debugging success when SQL execution is not performed?\n5. Why did authors introduce a complexity threshold that filters out simple SQL queries?\nIn real enterprise environments, simple SQLs (e.g., schema exploration, permission changes) often cause critical bugs due to complex contexts rather than query length.\nDid the authors conduct any analysis or survey to understand how such “simple-but-error-prone” SQLs appear in enterprise settings before deciding to exclude them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yRLuXM6xGD", "forum": "8Fm6OKFuRv", "replyto": "8Fm6OKFuRv", "signatures": ["ICLR.cc/2026/Conference/Submission916/Reviewer_VdLV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission916/Reviewer_VdLV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760739532306, "cdate": 1760739532306, "tmdate": 1762915640516, "mdate": 1762915640516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Squirrel Benchmark, a novel benchmark designed to evaluate LLMs on enterprise-level SQL debugging tasks. It contains two tracks, Squirrel-Syntax and Squirrel-Semantic, targeting syntax and semantic bugs, respectively. The benchmark is constructed via a reverse engineering pipeline that injects bugs into large, complex SQL queries (>140 lines on average). It also introduces an execution-free evaluation framework using Exact Match, Graph Match, and Modify-Better scores. The authors evaluate nearly 30 LLMs, showing that even state-of-the-art models (e.g., Claude-4) struggle, with most models performing below 20% success on the semantic track. They also explore strategies like fine-tuning (SFT) and agent-based methods to improve performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addresses an Important and Underexplored Problem: The paper tackles a real and practical challenge: debugging large, complex SQL queries in enterprise settings where existing text-to-SQL benchmarks fall short.\n\n2. Comprehensive Evaluation: The authors evaluate ~30 LLMs, including both open-source and closed-source models, and show that even state-of-the-art models underperform, highlighting a large gap between current capabilities and real-world needs.\n\n3. The proposed bug taxonomy is useful for future text2sql research. The proposed benchmark includes more complex SQL queries than existing benchmarks and may better reflect the enterprise setting."}, "weaknesses": {"value": "1. Insufficient detail on quality control: The paper briefly mentions that all SQL scripts are \"rigorously validated to be bug-free,\" but does not clearly explain the validation process. Were domain experts involved? How many annotators were used?\n\n2. Lack of clarity on seed SQL curation: While the benchmark is said to be based on “high-quality SQL scripts from real-world enterprise applications,” the process for ensuring representativeness and diversity across domains is underexplained.\n\n3. Semantic correctness assumptions in SQL generation: During SQL synthesis, correctness is determined by whether the query executes successfully. This execution-based validation does not guarantee semantic correctness, especially since a syntactically valid query can still be logically wrong.\n\n4. Concerns about evaluation metrics: The use of Graph Match and Exact Match is too strict and may lead to false negatives. Execution accuracy, although imperfect, can still be a useful complementary metric. Moreover, I don’t think execution accuracy introduces many false positives, since it's unlikely that a randomly generated SQL query would produce exactly the same result as the correct one."}, "questions": {"value": "1. >“Contribution 1: The First Enterprise-level SQL Benchmark”\nWhile the benchmark is certainly valuable, the claim of being the first enterprise-level SQL benchmark might be too strong. For example, Spider 2.0 also includes SQLs exceeding 100 lines and incorporates ETL-style workloads. It may be worth rephrasing this claim to better reflect the relative novelty.\n\n2. How do you control the quality of the reference SQL queries? Are they semantically correct?\n\n3. Have you conducted any manual analysis of low EM or GM predictions to check whether they are actually incorrect or if they represent false negatives (e.g., semantically correct queries that differ structurally from the reference)?\n\n4. Would it be possible to report execution accuracy for the evaluated methods as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "R9hHESzSmE", "forum": "8Fm6OKFuRv", "replyto": "8Fm6OKFuRv", "signatures": ["ICLR.cc/2026/Conference/Submission916/Reviewer_JYwx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission916/Reviewer_JYwx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867342541, "cdate": 1761867342541, "tmdate": 1762915640274, "mdate": 1762915640274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Squirrel Benchmark, a synthetic benchmark for evaluating LLMs on enterprise-scale SQL debugging rather than generation. The authors construct large, complex SQL queries via LLM-based synthesis, inject bugs using a taxonomy derived from enterprise logs, and evaluate ~30 LLMs. They also compare SFT-based methods and an agentic debugging loop. Results show state-of-the-art models still struggle on SQL correction tasks, particularly semantic debugging."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper addresses an emerging and important problem, using LLMs for iterative SQL debugging. It is not only relevant to enterprise text-to-SQL but also important as a standalone problem.\n\nS2. The newly created Squirrel benchmark is significantly longer and more complex than Spider/BIRD, with >140 lines and richer nesting, aiming to reflect real ETL workloads.\n\nS3. The evaluation demonstrates that even top LLMs (Claude, GPT-5, Qwen3) have difficulty, particularly on semantic bugs, reinforcing the difficulty of enterprise SQL debugging.\n\nS4. The work describes a systematic reverse-engineering-style pipeline for bug injection and adversarial filtering."}, "weaknesses": {"value": "W1. The primary contribution is a benchmark, not new methods. Most components—synthetic SQL generation, taxonomy-guided corruption, execution-free matching—are incremental extensions of existing ideas (e.g., BIRD-Critic reverse debugging, Spider2.0 industrial focus). Core technical innovation is modest.\n\nW2. Despite cross-checking with some enterprise logs, the majority of data is still LLM-generated, raising concerns about over-alignment with prompting style of the generator model, divergence from messy, schema-sprawling real SQL found in enterprises.\n\nW3. The work largely evaluates the performance of out-of-the-box LLMs. Additional baselines (e.g., NL2SQL, SQL error detect/correction) should be included in the evaluation.\n\nW4. The graph-match and modify-better metrics may favor structurally close but semantically incorrect SQL, especially in complex and large query DAGs. This undermines the enterprise debugging claim unless validated carefully with execution traces. Also what \n\nW6. The agent & SFT experiments are weak. While results are promising, but the paper does not provide enough details to fully understand the pros and cons of these two methodologies in the context of SQL debugging. Related to W3, certain NL2SQL solutions, such as DIN-SQL, SQLens, Reforce, Arctic-Text2SQL-R1, adopted agent framework or SFT/PT, which can be better candidates to answer the two questions(SFT/agent methods solving the SQL debugging)."}, "questions": {"value": "Q1. How can we validate that LLM-synthesized “enterprise SQL” truly reflects ETL pipelines, cross-db joins, dbt lineage trees, UDFs, or vendor dialects?\n\nQ2. What percentage of bugs in the taxonomy were seen in actual production logs vs synthetic augmentation?\n\nQ3. Given that the benchmark is not publicly available, can you release examples of real enterprise-authored SQL and bug metadata (even if anonymized / masked) in addition to the info described in Section 4?\n\nQ4. How stable is the benchmark to using different seed models, or is it biased toward the Claude generator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "R3DthSdiS1", "forum": "8Fm6OKFuRv", "replyto": "8Fm6OKFuRv", "signatures": ["ICLR.cc/2026/Conference/Submission916/Reviewer_z7M7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission916/Reviewer_z7M7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762210469536, "cdate": 1762210469536, "tmdate": 1762915640046, "mdate": 1762915640046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Squirrel Benchmark, a large-scale benchmark for enterprise-level SQL debugging that moves beyond Text-to-SQL generation towards iterative fault localization and repair. It contributes: (a) an automated reverse-engineering pipeline that injects realistic, minimally invasive bugs into long, schema-heavy enterprise SQL; (b) a hierarchical SQL bug taxonomy; and (c) an execution-free evaluation framework using three metrics: Exact Match (EM), Graph Match (GM) for AST-level semantic equivalence, and Modify-Better (MB) for iterative improvement. The dataset includes 469 syntax and 516 semantic tasks; the SQL is notably large and complex (high AST width/depth). The paper further explores SFT baselines (vanilla, DM-SFT, Diff-SFT) and an agentic ReAct-style approach; SFT improves performance and Diff-SFT speeds inference; agents can help but are sensitive to the main agent’s reasoning quality. Overall, the work addresses a real gap in semantic correctness and debugging at enterprise scale, aligning with recent interest in semantics-focused evaluation and repair."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. Enterprise SQL debugging benchmark with realistic scale and complexity; minimal-change, taxonomy-guided bug injection.\nS2. Careful construction (adversarial model filtering + human verification); clear, multi-metric, execution-free evaluation that captures structure-level semantics and iterative improvement.\nS3. Show large performance gaps of state-of-the-art LLMs on enterprise debugging."}, "weaknesses": {"value": "W1.  Reliance on LLMs (Claude) for scenario transfer and bug injection may imprint generator-specific artifacts or bias\nW2. GM is a strong step but may still miss semantically inequivalent but structurally similar cases or over-credit equivalent rewrites that would fail on real data.\nW3. Many examples reference Hive/Calcite; discuss portability to Postgres/MySQL/Spark SQL and mixed-dialect quirks.\nW4. Agentic results hinge on the main agent; provide standardized action budgets and failure analysis to separate planning vs. codegen limitations."}, "questions": {"value": "1. What SQL, DDLs, bugged scripts and/or prompts will be released?\n2. How do you ensure no contamination between generator LLM training data and evaluation queries/templates?\n3. Have you tested portability to Postgres/BigQuery/Snowflake? Any dialect-specific error patterns that change difficulty?\n4. How do you handle multiple valid fixes for semantic tasks beyond reference matching? Any mechanism to register alternative valid solutions?\n5. How does this work compare with SQLens? https://arxiv.org/abs/2506.04494"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rwuNN5CDOG", "forum": "8Fm6OKFuRv", "replyto": "8Fm6OKFuRv", "signatures": ["ICLR.cc/2026/Conference/Submission916/Reviewer_2TR9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission916/Reviewer_2TR9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225661773, "cdate": 1762225661773, "tmdate": 1762915639899, "mdate": 1762915639899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}