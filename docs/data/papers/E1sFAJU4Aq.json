{"id": "E1sFAJU4Aq", "number": 24460, "cdate": 1758357126517, "mdate": 1763122043244, "content": {"title": "Learning Pyramid Representations from Gigapixel Histopathological Images", "abstract": "Whole slide images (WSIs) pose fundamental computational challenges due to their gigapixel resolution and the sparse distribution of informative regions. Existing approaches often treat image patches independently—discarding spatial structure—or reshape them in ways that distort spatial context, thereby obscuring the hierarchical pyramid representations intrinsic to WSIs. We introduce Sparse Pyramid Attention Networks (SPAN), a hierarchical framework that preserves spatial relationships while efficiently allocating computation to informative regions. SPAN constructs multi-scale representations directly from single-scale inputs, enabling precise WSI modeling without sacrificing efficiency. We demonstrate SPAN’s versatility through two variants: SPAN-MIL for slide classification and SPAN-UNet for segmentation. Comprehensive evaluations across multiple public datasets show that SPAN captures the hierarchical structure and contextual relationships that existing methods fail to model. Our results provide clear evidence that architectural inductive biases and hierarchical representations enhance both slide-level and patch-level performance. By overcoming long-standing computational barriers, SPAN establishes a new paradigm for computational pathology and reveals foundational design principles for large-scale medical image analysis.", "tldr": "Learning Pyramid Representations from Gigapixel Histopathological Images", "keywords": ["Computer Vision", "Transformer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f5c60bd33ef2e5aa9c93900fcbf86a3abd986fe9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper describes a hierarchical framework for analysis of histopathology images. Authors propose a Sparse Pyramid Attention mechanism integrating a spatial adaptive condensation module and a context aware feature refinement module. Authors assess their method on various benchmarks related to slide classification and segmentation against a variety of baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Paper well written and clear. I enjoyed reading it, and it is generally well organized.\n2. Methodology described with enough details and technically sound.\n3. Extensive assessment and benchmarking against competing methods."}, "weaknesses": {"value": "1. Limited technical novelty:  despite the conceptual definition of sparse attention rulebook, that appears like an incremental contribution, the usage of pyramid attention\nhas been already proposed in the literature:\nYu, Yang, et al. \"Multi-scale spatial pyramid attention mechanism for image recognition: An effective approach.\" Engineering Applications of Artificial Intelligence 133 (2024): 108261.\nHua, X., Xiang, G., Yuan, H., Zou, L., Wang, L., & Hong, H. (2025). An Efficient and Lightweight Pyramid Attention for Image Deblurring. Pattern Recognition, 112506.\n\n2. Positioning with respect to SOTA: Authors fail in providing a convincing discussion of related work and detailing the actual contributions, and I find unfortunate that a short related work section is provided in the literature, and the critical discussion of relevant methods is scattered here and there in the introduction and various parts.\nI don't find particularly convincing the claim in the introduction \n\"For example, TransMIL (Shao et al.,\n2021) relies on re-squaring with Nystr ̈om attention and [CLS] tokens, while others introduce region\nattention after dense reshaping (Tang et al., 2024). These approaches inevitably distort positional\ninformation and restrict modeling to isotropic representations, failing to exploit the hierarchical\nstructures that have proven vital in general computer vision\"  while in the result section the reported improvement of SPAN with respect to these methods appear  slightly incremental. \n\n3. Limited Assessment: only few qualitative results are provided and showcasing limitations of the method (in Fig. 5 the detected neoplastic areas from SPAN appear underestimated, and this would be a strong limitation from diagnostic perspective). Also, it is not clear at what level of zoom the method works: authors mention 20X magnification, but it is not clear how their framework would work at nuclear level (I am not sure that they managed to consider the highest magnification level of slides). Finally, apart of memory consuption, no information is provided about efficiency, especially in comparison with baselines. How much does it take to train? How much for inference?"}, "questions": {"value": "1. Provide more details about architectural choice, and training of the model\n2. Provide more qualitative results, and how the framework works at highest magnification level (nuclear).\n3. Reorganize the related work section in the main manuscript, and position the proposed framework with respect to SOTA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zlrlXgfmz2", "forum": "E1sFAJU4Aq", "replyto": "E1sFAJU4Aq", "signatures": ["ICLR.cc/2026/Conference/Submission24460/Reviewer_cmyf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24460/Reviewer_cmyf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843473780, "cdate": 1761843473780, "tmdate": 1762943087610, "mdate": 1762943087610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "oGYWrG24z7", "forum": "E1sFAJU4Aq", "replyto": "E1sFAJU4Aq", "signatures": ["ICLR.cc/2026/Conference/Submission24460/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24460/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763121708496, "cdate": 1763121708496, "tmdate": 1763121708496, "mdate": 1763121708496, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SPAN, a \"sparse-native\" framework for analyzing gigapixel WSIs. It aims to address a key challenge where previous methods distort spatial context by either discarding patch shapes or forcibly reshaping them. SPAN, in contrast, preserves the exact spatial relationships of tissue patches and builds a hierarchical pyramid representation from a single-scale input to improve WSI classification and segmentation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed method directly processes irregular tissue patches, preserving their exact spatial relationships rather than distorting them by reshaping.\n\n2.\tIt builds a hierarchical pyramid representation directly from a single-scale input, unlike other approaches that require multiple magnification levels."}, "weaknesses": {"value": "1.\tA major part of Section 3 (Method) describes well-established methods, such as Sparse Conv, local window attention, etc., indicating that most of the proposed mechanism is established techniques from prior research in the field. This makes the approach in this work more of an engineering assembly than a methodological innovation. \n\n2.\tPlease mark the best and second-best models in Tables 1,2, and 3. The current version shows poor readability.\n\n3.\tThe CAMELYON16 dataset loses its significance as a benchmark because all models can achieve near-perfect scores (with Virchow2 or other pathology foundation models as the feature extractor). Thus, it only shows the strong capability of Virchow2, instead of the proposed method.\n\n4.\tMoreover, the usage of ResNet50 as the feature extractor does not make much sense since there are many pathology foundation models available, and they are much better than ResNet50.\n\n5.\tSeveral highly related papers are not properly cited or compared:\n\n$\\quad$ [1] (Hierarchical Pooling / Condensing) Guo, Zhengrui, et al. \"Histgen: Histopathology report generation via local-global feature encoding and cross-modal context interaction.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2024.\n\n$\\quad$ [2] (Spatial-aware MIL) Yang, Zekang, Hong Liu, and Xiangdong Wang. \"Scmil: Sparse context-aware multiple instance learning for predicting cancer survival probability distribution in whole slide images.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2024.\n\n$\\quad$ [3] (Context-aware long sequence modeling) Guo, Zhengrui, et al. \"Context matters: Query-aware dynamic long sequence modeling of gigapixel images.\" arXiv preprint arXiv:2501.18984 (2025).\n\n$\\quad$ [4] (Spatial-aware Aggregation) Xu, Hanwen, et al. \"A whole-slide foundation model for digital pathology from real-world data.\" Nature 630.8015 (2024): 181-188.\n\n$\\quad$ [5] (Local Attention / Long sequence modeling) Li, Honglin, et al. \"Rethinking transformer for long contextual histopathology whole slide image analysis.\" Advances in Neural Information Processing Systems 37 (2024): 101498-101528.\n\n$\\quad$ [6] (Pathology Image Segmentation) Chen, Zhixuan, et al. \"Segment Anything in Pathology Images with Natural Language.\" arXiv preprint arXiv:2506.20988 (2025)."}, "questions": {"value": "1.\tFor the processed WSI (after feature extraction, before feeding to the first SAC module), how did the authors maintain its spatial structure in the first place? And does it contain background tokens (or is it solely the tissue tokens)?\n\n2.\tCould the authors please use other datasets than CAMELYON16 for comparison?\n\n3.\tCould the authors please use other feature extractors, such as UNI [1], GPFM [2], instead of ResNet50? \n\n$\\quad$ [1] (Pathology Foundation Model) Chen, Richard J., et al. \"Towards a general-purpose foundation model for computational pathology.\" Nature medicine 30.3 (2024): 850-862.\n\n$\\quad$ [2] (Pathology Foundation Model) Ma, Jiabo, et al. \"A generalizable pathology foundation model using a unified knowledge distillation pretraining framework.\" Nature Biomedical Engineering (2025): 1-20.\n\n4.\tThe authors mentioned “This success stems from undistorted hierarchical spatial encoding that preserves precise patch relationships” in the second paragraph of Section 4. How to prove this? In several datasets, ABMIL based on Virchow2 feature already achieved the best AUC.\n\n5.\tHow are models like ABMIL, TransMIL, and RRT used for segmentation?\n\n6.\tCould the author please cite these papers in the Introduction or Related Work sections, since they are relevant:\n\n$\\quad$ [1] (Hierarchical Pooling / Condensing) Guo, Zhengrui, et al. \"Histgen: Histopathology report generation via local-global feature encoding and cross-modal context interaction.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2024.\n\n$\\quad$ [2] (Spatial-aware MIL) Yang, Zekang, Hong Liu, and Xiangdong Wang. \"Scmil: Sparse context-aware multiple instance learning for predicting cancer survival probability distribution in whole slide images.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2024.\n\n$\\quad$ [3] (Context-aware long sequence modeling) Guo, Zhengrui, et al. \"Context matters: Query-aware dynamic long sequence modeling of gigapixel images.\" arXiv preprint arXiv:2501.18984 (2025).\n\n$\\quad$ [4] (Spatial-aware Aggregation / Pathology Foundation Model) Xu, Hanwen, et al. \"A whole-slide foundation model for digital pathology from real-world data.\" Nature 630.8015 (2024): 181-188.\n\n$\\quad$ [5] (Local Attention / Long sequence modeling) Li, Honglin, et al. \"Rethinking transformer for long contextual histopathology whole slide image analysis.\" Advances in Neural Information Processing Systems 37 (2024): 101498-101528.\n\n$\\quad$ [6] (Pathology Foundation Model) Chen, Richard J., et al. \"Towards a general-purpose foundation model for computational pathology.\" Nature medicine 30.3 (2024): 850-862.\n\n$\\quad$ [7] (Pathology Foundation Model) Wang, Xiyue, et al. \"A pathology foundation model for cancer diagnosis and prognosis prediction.\" Nature 634.8035 (2024): 970-978.\n\n$\\quad$ [8] (Pathology Foundation Model) Ma, Jiabo, et al. \"A generalizable pathology foundation model using a unified knowledge distillation pretraining framework.\" Nature Biomedical Engineering (2025): 1-20.\n\n$\\quad$ [9] (Pathology Image Segmentation) Chen, Zhixuan, et al. \"Segment Anything in Pathology Images with Natural Language.\" arXiv preprint arXiv:2506.20988 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tUGWhUR291", "forum": "E1sFAJU4Aq", "replyto": "E1sFAJU4Aq", "signatures": ["ICLR.cc/2026/Conference/Submission24460/Reviewer_kHnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24460/Reviewer_kHnC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906227090, "cdate": 1761906227090, "tmdate": 1762943087356, "mdate": 1762943087356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Learning Whole Slide Image (WSI) representation is challenging due to large size/resolution and the sparse distribution of information/regions of interest.  The paper proposes a new method, SPAN, to learn WSI representation using a transformer-style architecture. SPAN incorporates two fundamental ideas: learning hierarchical features corresponding to different resolutions and learning the spatial relationships between patches.\nTo achieve this, SPAN uses two distinct modules: Spatial-Adaptive Feature Condensation (SAC) and Context-Aware Feature Refinement (CAR). SAC learns hierarchical representations from a single input in an end-to-end fashion, while CAR uses attention to learn local and global context about the patches.\nTo avoid the quadratic complexity of vanilla attention, the authors propose using a pre-defined sparse attention rulebook. Extensive experiments and comparisons with multiple baselines are done to evaluate the efficacy of the method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is overall well-written and tries to address an important problem in learning WSI representation.\n\n* Compared to prior, which use image at different resolution to learn multi-resolution features, SPAN can learn multi-resolution feature from a single input in an end-to-end manner. \n\n* The idea of using sparse and convolution rulebook to circumvent high complexity of attention is interesting and novel.\n\n* The method is extensively benchmarked against multiple baselines (Table 1 and 3). \n\n* While the improvement for classification tasks in not significant (explained in the weakness section), SPAN outperforms existing baselines for segmentation task (table 3). \n\n* Ablation in table 2. shows that SAC and CAR modules helps in learning better features."}, "weaknesses": {"value": "* While the method sounds intuitive, the improvement over baselines in Table 1 is not convincing. In particular, for the CAMELYON16 and BRACS datasets, the improvement over the baseline is not significant. For e.g., SPAN-MIL achieves 72.5% acc for the BRACS dataset compared to 72% with ACMIL using ResNet features. Similarly with Virchow2 features, SPAN-MIL achives 77.8% compared to 77.3% with CLAM. \n\n* SPAN does not improve AUC scores for the classification tasks in Table 1. In my opinion, AUC is a more important metric than accuracy, especially for unbalanced datasets. \n\n* Details about the total number of parameters for each baseline in Tables 1 and 3 are missing, making it difficult to understand if the improvement is due to a larger number of parameters or the SPAN methodology. \n\n* It is not clear how the rulebooks are constructed. Providing more implementation details would be helpful in making the methodology easier to understand. \n\n* Similarly, more details about how matrices I and P are constructed are required in the methodology section."}, "questions": {"value": "1. Do you construct different rulebook for different inputs? What's the computational overhead?\n\n2.   > we temporarily densify I ∈ NN into a regular grid using patch coordinates P ∈ NN × 2 with zero padding.\n\n       How do you densify I? Do you use patch selection? \n\n3. Do you have any insights why the method works better for segmentation task and not  for classification tasks? I think understanding how SPAN learns features differently will be important and it might give you insights why AUC scores are not improved (in table 1)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s9SR1l3Oiv", "forum": "E1sFAJU4Aq", "replyto": "E1sFAJU4Aq", "signatures": ["ICLR.cc/2026/Conference/Submission24460/Reviewer_XHtw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24460/Reviewer_XHtw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762249965174, "cdate": 1762249965174, "tmdate": 1762943087105, "mdate": 1762943087105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}