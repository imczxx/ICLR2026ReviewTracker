{"id": "WBys7MARr3", "number": 1031, "cdate": 1756829271620, "mdate": 1758806383140, "content": {"title": "Damon: Dynamic model pruning for Dense Large Language models", "abstract": "With a vast number of parameters, Large Language Models (LLMs) have shown great potential across a wide range of tasks.\nTo reduce model size and accelerate inference, structured pruning is a widely adopted technique. However, conventional structured pruning, as a static technique, permanently discards model components, leading to a significant and irreversible degradation in performance. To address this limitation, we propose a **D**yn**a**mic **mo**del pru**n**ing framework for dense LLMs. \nOur approach employs token-level routers to selectively activate a subset of model structures for each forward pass, rather than permanently removing unactivated ones. This mechanism enables the dynamic allocation of the computational budget according to input difficulty, striking an effective trade-off between performance and efficiency. Extensive experiments on four families of LLMs demonstrate that our method outperforms both static and dynamic structured pruning baselines under the same computational budgets.", "tldr": "", "keywords": ["LLMs", "structured pruning", "dynamic sparsity"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "", "supplementary_material": ""}, "replies": [], "withdrawn": true}