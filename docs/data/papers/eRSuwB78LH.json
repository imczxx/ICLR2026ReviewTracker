{"id": "eRSuwB78LH", "number": 6644, "cdate": 1757991165837, "mdate": 1759897903251, "content": {"title": "CoCoDiff: Correspondence-Consistent Diffusion Model for Fine-grained Style Transfer", "abstract": "Transferring visual style between images while preserving semantic correspondence between similar objects remains a central challenge in computer vision. While existing methods have made great strides, most of them operate at global level but overlook region-wise and even pixel-wise semantic correspondence. To address this, we propose **CoCoDiff**, a novel *training-free* and *low-cost* style transfer framework that leverages pretrained latent diffusion models to achieve fine-grained, semantically consistent stylization. We identify that correspondence cues within generative diffusion models are under-explored and that content consistency across semantically matched regions is often neglected. CoCoDiff introduces a pixel-wise semantic correspondence module that mines intermediate diffusion features to construct a dense alignment map between content and style images. Furthermore, a cycle-consistency module then enforces structural and perceptual alignment across iterations, yielding object and region level stylization that preserves geometry and detail. Despite requiring no additional training or supervision, CoCoDiff delivers state-of-the-art visual quality and strong quantitative results, outperforming methods that rely on extra training or annotations.", "tldr": "", "keywords": ["Style Transfer", "Diffusion Model", "Computer Vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86bdfcd8d12fb1e16198a5227cb26f52018f89ad.pdf", "supplementary_material": "/attachment/8067b7b95196f965cc3ea5a89e20b4c35722978a.zip"}, "replies": [{"content": {"summary": {"value": "This work proposed a training-free diffusion-based framework for fine-grained, correspondence-consistent style transfer. It leverages pretrained diffusion models to extract pixel-level semantic correspondences between content and style images, enabling region- and object-aware stylization.\n\nThe main contribution includes 1. Pixel-Wise Semantic Correspondence Module to extract intermediate diffusion features to build dense, semantically meaningful alignment maps between content and style images; and 2. Cycle-Consistency Optimization which \nintegrates attention-guided feature injection with iterative refinement to enhance structural stability and stylization coherence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work has enough metrics to prove that the solution is effective, across FID, LPIPS, ArtFID, and CFSD. The ablation and user studies further validate design choices such as the feature injection weight and AdaIN harmonization.\n\n2. This paper is well written and easy to follow."}, "weaknesses": {"value": "1. This work claimed training free but it still need iteration during sampling. the complexity is not stated in the work\n2. Some innovations were introduced rather suddenly, lacking sufficient theoretical foundation and clear explanations.\n3. Why sobel and g works for style and context similarity. I look forward to a more reasonable explanation\n4. This work lacks many recent references. I hope the author can include it as a baseline for comparison. e.g. [1] [2]\n\n\n[1] Ahn, Namhyuk, et al. \"Dreamstyler: Paint by style inversion with text-to-image diffusion models.\" aaai24\n\n[2] He, Huiang, et al. \"Semantix: An Energy Guided Sampler for Semantic Style Transfer.\" iclr25"}, "questions": {"value": "1. I hope the author can provide the specific time and memory consumption of the method.\n2. The work lacks ablation experiments. I hope the authors can demonstrate the role of each module they proposed. And provide more credible theoretical explanations for sobel and g network.\n3. I hope the author can add some newer baselines. Or provide some objective theoretical analysis to explain the similarities and differences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P7GEqqSlKO", "forum": "eRSuwB78LH", "replyto": "eRSuwB78LH", "signatures": ["ICLR.cc/2026/Conference/Submission6644/Reviewer_1orb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6644/Reviewer_1orb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760539968349, "cdate": 1760539968349, "tmdate": 1762918960681, "mdate": 1762918960681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on enhancing the performance of artistic style transfer between images with similar semantics. A key limitation of most existing works is that they operate at the global level, while overlooking region-wise and even pixel-wise semantic correspondence. To address this gap, the authors propose CoCoDiff, a training-free diffusion-based framework. This framework extracts intermediate diffusion features to establish pixel-wise correspondences and leverages cyclic optimization techniques, achieving fine-grained stylization with semantic consistency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The integration of semantic consistency into diffusion-based style transfer is interesting and beneficial for stylization tasks between images with similar semantics.\n\n+ The proposed method is training-free."}, "weaknesses": {"value": "- My primary concern is that this paper appears to overlook a crucial research direction in style transfer, namely patch-based style transfer. Aligned with the motivation of this paper, works in this direction primarily leverage semantic correspondence between features to perform style transfer between objects with similar semantics. Representative methods include CNNMRF [A], Style-Swap [B], DIA [C], DivSwapper [D], Avatar-Net [E], and SCSA [F]. This paper neither discusses these methods in the related work section nor compares with them in experiments. Consequently, it is hard to effectively evaluate the technical innovation and performance of this work.\n\n[A] Chuan Li and Michael Wand. Combining markov random fields and convolutional neural networks for image synthesis. In CVPR, pages 2479–2486, 2016.\n\n[B] Tian Qi Chen and Mark Schmidt. Fast patch-based style transfer of arbitrary style. arXiv preprint\narXiv:1612.04337, 2016.\n\n[C] Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, and Sing Bing Kang. Visual attribute transfer through deep image analogy. TOG, 2017.\n\n[D] Zhizhong Wang, Lei Zhao, Haibo Chen, Zhiwen Zuo, Ailin Li, Wei Xing, and Dongming Lu. Divswapper: Towards diversified patch-based arbitrary style transfer. In IJCAI, pages 4980–4987, 2022.\n\n[E] Lu Sheng, Ziyi Lin, Jing Shao, and Xiaogang Wang. Avatarnet: Multi-scale zero-shot style transfer by feature decoration. In CVPR, pages 8242–8250, 2018.\n\n[F] Chunnan Shang, Zhizhong Wang, Hongwei Wang, Xiangming Meng. SCSA: A Plug-and-Play Semantic Continuous-Sparse Attention for Arbitrary Semantic Style Transfer. In CVPR, pages 13051–13060, 2025.\n\n- In L247–248, the authors state: “The optimal pair $(t^∗, l^∗)$ is selected by maximizing a correspondence quality metric $\\mathcal{M}(t, l)$ over predefined candidate sets $\\mathcal{T}$ and $\\mathcal{L}$” How are $\\mathcal{T}$ and $\\mathcal{L}$ determined? Are they set based on empirical values?\n\n- It seems that the reason that the proposed method can perform style transfer in a training-free manner mainly relies on the adjustment of attention weights in Eq. (7). Why can this adjustment effectively enhance the semantic consistency of the feature maps? Some necessary theoretical explanations are lacking.\n\n- In L314–316, it is mentioned that “the fitting cycle's iteration process terminates when both ... are predefined thresholds.” It is unclear what kind of constraint a content loss ($\\mathcal{L}_{content}$) greater than a threshold ($\\tau_c$) can impose. What is the difference between this and constraining only the style loss?\n\n- Section 4.6 (User Study) lacks necessary details. For example: what were the scoring instructions provided to users? What were the scoring rules? Which specific aspects were evaluated under the “Style” and “Content” dimensions in Table 3?\n\n- Some minor detail issues: In L144–145, ArtFlow does not belong to the diffusion-based methods. In Eq. (6), $F_s$ is not defined."}, "questions": {"value": "Please see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e9OFQLeQGP", "forum": "eRSuwB78LH", "replyto": "eRSuwB78LH", "signatures": ["ICLR.cc/2026/Conference/Submission6644/Reviewer_yGas"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6644/Reviewer_yGas"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760950850749, "cdate": 1760950850749, "tmdate": 1762918960097, "mdate": 1762918960097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper  proposes a training-free, diffusion-based framework for fine-grained, structure-preserving style transfer that operates directly on pretrained backbones without additional supervision or fine-tuning. The propose method delivers state-of-the-art visual quality and strong quantitative results, outperforming methods that rely on extra training or annotations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The proposed training-free style transfer method, which ensures style consistency, represents a valuable contribution to the field.\n(2) A clear algorithm is presented for better understanding.\n(3) CoCoDiff outperforms the six representative methods to some extend."}, "weaknesses": {"value": "(1) Line 252 says M(t, l) that evaluates the alignment quality based on the extracted feature maps at timestep t and layer l. What exactly is M(t, l)?\n\n(2) Fig. 2 requires significant refinement for improved clarity. FFM appears to have two inputs, but only I_sty_c is explicitly shown in the main framework. It seems that I_sty is inputted and reconstructed within the U-Net. If this is used for self-attention extraction, I suggest that the output should also be included. If this is the case, the self-attention section should be moved from Section 3.1 to Section 3.2 for better understanding. Besides, FIC also has two inputs where I can see only one.\n\nThe structure of Fig.2 should be better organized. The input images are put in the center, which is not easy to understand\n\n(3) The used SDv1.4 is a very old version. Do the authors try newer diffusion models like SD2.1, SDXL or FLUX.\n\n(4) The inference time should be listed as it consists of many complex steps like feature exchange, fitting cycle, iterative control."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I have no specific concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bHVlMvDZb3", "forum": "eRSuwB78LH", "replyto": "eRSuwB78LH", "signatures": ["ICLR.cc/2026/Conference/Submission6644/Reviewer_gHaT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6644/Reviewer_gHaT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721126814, "cdate": 1761721126814, "tmdate": 1762918959483, "mdate": 1762918959483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CoCoDiff, a training-free fine-grained style transfer framework based on pretrained latent diffusion models. The method establishes dense pixel-level semantic correspondences by mining intermediate diffusion features and introduces a cycle-consistency module to enforce structural and perceptual alignment. The approach achieves good visual quality and quantitative results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method demonstrates certain innovation by mining intermediate diffusion features to construct dense pixel-level semantic correspondences between content and style images, and introducing a cycle-consistency module to enhance structural and perceptual alignment.\n\n2. The paper achieves superior quantitative and qualitative results compared to existing methods across multiple datasets."}, "weaknesses": {"value": "1. Why does cycle consistency improve semantic correspondence? The paper lacks theoretical analysis or deeper explanation.\n\n2. The correspondence quality metric M(t,l) in Equation (5) is not clearly defined. How is \"correspondence quality\" quantified?\n\n3. Inconsistent notation usage: symbols such as p_c, p_s, p*_s are defined inconsistently across different sections.\n\n4. Figure 8 has low comparison quality with text labels that are too small.\n\n4. Insufficient experiments. Table 2 shows that on Mip-NeRF 360, GENIE's PSNR is 5-7 dB lower than Mip-NeRF, with even larger gaps in SSIM and LPIPS metrics. The authors attribute this to Gaussian volume density, but lack systematic analysis. The authors need to strengthen the persuasiveness.\n\n5. Missing training time comparisons."}, "questions": {"value": "1. The grid search for finding optimal (t*, l*) lacks theoretical guidance.\n\n2. Why does \"first converting the style image to content style\" improve matching? Please provide theoretical analysis or more detailed mechanism explanation.\n\n3. The authors need to provide sufficient validation on different diffusion models.\n\n4. The authors need to report runtime and computational resource consumption.\n\n5. How are the candidate sets T and L for grid search selected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xQrQpIs8jo", "forum": "eRSuwB78LH", "replyto": "eRSuwB78LH", "signatures": ["ICLR.cc/2026/Conference/Submission6644/Reviewer_hz3B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6644/Reviewer_hz3B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000600739, "cdate": 1762000600739, "tmdate": 1762918959000, "mdate": 1762918959000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}