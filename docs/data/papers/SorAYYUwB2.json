{"id": "SorAYYUwB2", "number": 4335, "cdate": 1757664302801, "mdate": 1759898038934, "content": {"title": "Diffusion Fine-Tuning: Iterative Refinement for Advanced Grounding with Diffusion Large Language Models", "abstract": "While Large Vision-Language Models (LVLMs) excel at simple bounding box grounding, they reveal fundamental limitations in tasks requiring precise spatial localization, most notably polygon grounding. We identify this bottleneck as stemming from two fundamental flaws of the autoregressive (AR) paradigm: 1) irreversible error accumulation, where early vertex errors propagate uncorrected through the sequence; and 2) a lack of global planning, which leads to a suboptimal allocation of the finite number of vertices (16 points) on the object's contour. We propose Diffusion Finetuning (DFT) to reframe visual grounding as a robust, parallel global optimization. Its core is a `sculpture-like', coarse-to-fine generation process, where coordinate digits are predicted hierarchically (e.g., hundreds, then tens, then units) to progressively refine the shape from a coarse outline to precise details. We use a novel Hierarchical Curriculum Learning strategy that progressively refines the loss supervision, guiding the model from a rough outline to a precise delineation. Extensive experiments show that DFT achieves state-of-the-art on both 2D bounding box and 16-point polygon grounding, and demonstrates its strong performance on the complex 9-DoF 3D bbox grounding task.", "tldr": "We employ Diffusion Fine-Tuning (DFT) to train masked Large Diffusion Language Models , enabling them to perform visual grounding tasks through a \"coarse-to-fine\" parallel optimization process.", "keywords": ["Diffusion Fine-Tuning", "Large Vision-Language Models", "Visual Grounding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49c7e6fc4f2f013a7d2c9d7dd60fe5805e4d4f73.pdf", "supplementary_material": "/attachment/d9b50b044cfb7b53095e4d4c6654303e0ee4c82c.zip"}, "replies": [{"content": {"summary": {"value": "The authors argue that autoregressive (AR) approaches to grounding are prone to error accumulation and cannot be optimized in parallel. Therefore, they propose a diffusion-like denoising method that generates object vertices in parallel for more effective grounding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors' HCD and HCL designs are highly reasonable, effectively reducing regression difficulty and improving the stability of predicted contours.\n2. DFT achieves strong performance on both 2D and 3D grounding, significantly outperforming previous vision-language models."}, "weaknesses": {"value": "1. The authors' motivation and proposed concept are extremely similar to those of PVD [1]; the only difference is that the authors focus on vision-language models (VLMs). However, they overlook a direct comparison with this closely related approach. Moreover, the paper fails to cite several highly relevant prior works in the traditional visual grounding literature, such as SeqTR [2], PVD [1], Pix2Seq [3], VisionLLM [4].\n\n2. Furthermore, the paper presents an ambiguous model positioning. If the model is intended to be general-purpose, the authors do not demonstrate its general multimodal understanding capability when performing polygon-based grounding. If it is a specialized model, they omit comparisons with existing dedicated 2D or 3D grounding models.\n\n3. Given the method's complexity, the authors do not provide a comparison of inference speed against conventional VLMs for grounding tasks.\n\n[1] Parallel Vertex Diffusion for Unified Visual Grounding\n[2] SeqTR: A Simple yet Universal Network for Visual Grounding\n[3] A Language Modeling Framework for Object Detection\n[4] VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"}, "questions": {"value": "1. I believe that although the proposed model utilizes a vision-language model (VLM), it functions more like a specialized grounding model; therefore, the authors should include comparisons with existing specialized grounding models.\n2. The authors need to discuss how their approach differs from conceptually similar works such as PVD, SeqTR, Pix2Seq, and VisionLLM.\n3. The authors should analyze the inference overhead of their current approach to evaluate the cost-effectiveness of the performance gains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ricrVaLG2y", "forum": "SorAYYUwB2", "replyto": "SorAYYUwB2", "signatures": ["ICLR.cc/2026/Conference/Submission4335/Reviewer_yKMe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4335/Reviewer_yKMe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833170393, "cdate": 1761833170393, "tmdate": 1762917304571, "mdate": 1762917304571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a diffusion fine-tuning framework that reformulates visual grounding as a coarse-to-fine parallel global optimization process, effectively overcoming AR models’ irreversible error accumulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and the motivation from AR to diffusion is well explained and illustrated with intuitive figures. The technical formulation and execution are solid, and the experimental evaluation is thorough and convincing. Overall, the work demonstrates clear quality and significance for improving precise grounding performance."}, "weaknesses": {"value": "1.The method may have non-trivial latency/compute overhead due to iterative diffusion + hierarchical refinement, but the paper does not quantify the speed–accuracy trade-off or compare runtime against strong AR baselines.\n\n2.The method assumes a fixed 16-vertex polygon, and it is unclear how well it generalizes to extremely simple/complex shapes or variable-length polygons. \n\n3.It is better to analyze limit cases (e.g., extremely complex shapes, ambiguous referring expressions, or heavy occlusion)."}, "questions": {"value": "1.I would like to see a concrete analysis of runtime / latency / compute cost, especially to quantify the diffusion vs AR speed–accuracy trade-off.\n\n2.Although the method achieves SOTA compared to baselines, I would like to see more visual comparisons on difficult regimes (e.g., highly complex shapes, ambiguous text, heavy occlusion) to see the limit of AR and diffusion model respectively.\n\n3.Since AR v.s. diffusion is a long-standing modeling choice trade-off, and this paper articulates clear advantages of diffusion for grounding, do you think this paradigm could generalize and surpass AR models in more general tasks beyond grounding? If yes, which categories do you believe are the most promising candidates (This is just a discussion, no additional experiments needed.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pc8n5bPujo", "forum": "SorAYYUwB2", "replyto": "SorAYYUwB2", "signatures": ["ICLR.cc/2026/Conference/Submission4335/Reviewer_tJXR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4335/Reviewer_tJXR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969709467, "cdate": 1761969709467, "tmdate": 1762917304278, "mdate": 1762917304278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Diffusion Fine-Tuning (DFT), a new framework that adapts large vision-language models for high-precision visual grounding tasks. Unlike autoregressive (AR) LVLMs (e.g., LLaVA, Qwen-VL) that predict coordinates sequentially and accumulate irreversible errors, DFT treats grounding as a global, parallel optimization. Besides, a Hierarchical Curriculum Learning strategy is adopted to progressively refine the loss supervision. Experiments show the advantages of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper reframes grounding from sequential prediction to diffusion-based iterative refinement. The hierarchical decomposition is intuitive and effective.\n\n2. Training Stability. HCL convincingly stabilizes diffusion training for discrete coordinate sequences.\n\n3. Large performance jumps (especially on polygon grounding).\n\n4. Extension to 3D bounding box grounding (9-DoF), showing flexibility across 2D/3D domains.\n\n5. New Benchmarks: RefCOCO-Polygon and Ref3D."}, "weaknesses": {"value": "1. Why is the finite number of vertices (16 points) on the object’s contour chosen as the spatial localization goal instead of segmentation? Polygon grounding is less accurate and has fewer downstream applications compared with segmentation.\n\n2. Lack of discussion and comparison with non-LVLM models. For example, PolyFormer.\n\n3. It is unclear whether the method is based on LLaVA OneVision’s weights or LLaDA-V’s weights. The description is confusing. In the experiments, it states: “The architecture of our DFT model is adapted from a LLaVA OneVision (Zhang et al., 2024)-like autoregressive model. We therefore use it as a baseline to measure the performance gains of our diffusion paradigm.” Why is pure DLM fine-tuning not used as a comparison baseline, e.g., fine-tuning on LLaDA-V, MMaDA, or LaViDa?\n\n4. Limited ablation scope. Most ablations isolate coordinate representation or HCL stages. There is no explicit comparison with non-hierarchical diffusion training."}, "questions": {"value": "Please see the Weaknesses section.\n\nOne more question about HCL: if the ground truth is 299, it might be reasonable for the model to predict “3” for the hundreds digit. How does the loss function account for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sjtnZ29dU5", "forum": "SorAYYUwB2", "replyto": "SorAYYUwB2", "signatures": ["ICLR.cc/2026/Conference/Submission4335/Reviewer_afmP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4335/Reviewer_afmP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978679624, "cdate": 1761978679624, "tmdate": 1762917303976, "mdate": 1762917303976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Diffusion Fine-Tuning (DFT), reframing grounding as a robust, parallel global optimization process through a “sculpture-like” coarse-to-fine generation scheme. DFT introduces two key components: (1) Hierarchical Coordinate Decomposition (HCD), which splits coordinates into hundreds, tens, and units digits to shrink the vocabulary (1000→10) and encode spatial priors; and (2) Hierarchical Curriculum Learning (HCL), a stage-wise “easy-to-hard” training strategy with Teacher Forcing to stabilize and refine learning. DFT achieves state-of-the-art results on RefCOCO-Polygon and RefCOCO 2D grounding, and generalizes well to 9-DoF 3D grounding. Ablations highlight HCL’s importance for complex tasks, the benefit of normalized integer coordinates, and the necessity of z-axis normalization for monocular 3D detection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Effectively mitigates irreversible error accumulation and lack of global planning in sequential vertex prediction by enabling iterative, bidirectional refinement—akin to a “sculpture-like” coarse-to-fine generation.\n* Innovative Technical Design:\n  *   Hierarchical Coordinate Decomposition (HCD): Splits coordinates into hundreds, tens, and units digits, reducing vocabulary size and embedding numerical/geometric priors.\n  *   Hierarchical Curriculum Learning (HCL): A three-stage training strategy that progressively supervises from macro-contour to fine details, ensuring stable and accurate convergence."}, "weaknesses": {"value": "* The method is limited to producing a fixed number of vertices, with a notable performance drop once the count surpasses 16.\n* The coordinate quantization to 0–999 may introduce discretization error, and performance is sensitive to normalization strategies.\n* The diffusion-based approach to the visual grounding was first introduced by [1], which also employed a similar coarse-to-fine modeling paradigm.\n\n[1] Cheng, Zesen, et al. \"Parallel vertex diffusion for unified visual grounding.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 2. 2024."}, "questions": {"value": "* Advanced LLMs can already stably handle contexts up to 32k or 128k tokens. Why can't the proposed method generate more vertices?\n* Will the diffusion process be more time-consuming than the autoregressive one? Could you provide a comparison of inference speed between the two methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "myJO6cK5oP", "forum": "SorAYYUwB2", "replyto": "SorAYYUwB2", "signatures": ["ICLR.cc/2026/Conference/Submission4335/Reviewer_EV3T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4335/Reviewer_EV3T"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762260536460, "cdate": 1762260536460, "tmdate": 1762917303729, "mdate": 1762917303729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}