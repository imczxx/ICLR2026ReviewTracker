{"id": "y3UkklvoW9", "number": 25299, "cdate": 1758366342298, "mdate": 1763745880382, "content": {"title": "THEMIS: Towards Holistic Evaluation of MLLMs for Scientific Paper Fraud Forensics", "abstract": "We present **THEMIS**, a novel multi-task benchmark designed to comprehensively evaluate Multimodal Large Language Models  (MLLMs) on visual fraud reasoning within real-world academic scenarios. Compared to existing benchmarks, THEMIS introduces three major advancements. (1) **Real-world Scenarios & Complexity**: Our benchmark comprises over 4K questions spanning 7 scenarios, derived from authentic retracted-paper cases and carefully curated multimodal synthetic data. With 73.73% complex-texture images,  THEMIS bridges the critical gap between existing benchmarks and the complexity of real-world academic fraud. (2) **Task Diversity & Granularity**: THEMIS systematically covers five challenging tasks and introduces 16 fine-grained manipulation operations. On average, each sample undergoes multiple stacked manipulation operations, with the diversity and difficulty of these manipulations demanding a high level of visual fraud reasoning from the models. (3) **Multi-dimensional Capability Evaluation**: We establish a mapping from fraud tasks to five core visual fraud reasoning capabilities, thereby enabling an evaluation that reveals the distinct strengths and specific weaknesses of different models across these core capabilities. Experiments on 11 leading MLLMs show that even the best-performing model still falls below the passing threshold, demonstrating that our benchmark presents a stringent test. We expect THEMIS to advance the development of MLLMs for complex, real-world fraud detection tasks. The data and code will be updated on url: https://anonymous.4open.science/r/themis1638.", "tldr": "We present THEMIS, a holistic multi-task benchmark of over 4K questions derived from authentic retracted-paper cases and realistically simulated synthetic data, to systematically evaluate the fine-grained visual fraud reasoning abilities of MLLMs.", "keywords": ["Multimodal Large Language Model", "Vision Fraud Reasoning", "Scientific Paper Fraud Detection", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a0c9477a5233fbf5e0563f788de5ee5dd9505de.pdf", "supplementary_material": "/attachment/e2a2f3eb921ca27e65b555516bf4c6240cb8a5af.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents THEMIS, a novel multi-task benchmark for evaluating Multimodal Large Language Models (MLLMs) in scientific paper fraud forensics. It features over 4K questions across 7 real-world academic scenarios (from retracted papers and synthetic data), covers 5 fraud tasks with 16 fine-grained manipulations, and maps tasks to 5 core reasoning capabilities. Experiments on 11 leading MLLMs show even top models (e.g., GPT-5 with 56.29% BRI) fall below passing thresholds, revealing limitations in handling compound manipulations and imbalanced capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The starting point is novel and has practical application value.\n* The data construction is complete, clear and reproducible."}, "weaknesses": {"value": "* Typo: There is an issue with the citation in 4.4 Appendix\n* From the perspective of the benchmark, it is quite well done. However, in the long run, this topic should be more suitable for optimizing, training, and fine-tuning models. If there are fine-tuning results, the value of this benchmark will be even higher."}, "questions": {"value": "* In your opinion, how should the community use your benchmark? Is it to evaluate a model's ability to detect cheating and then select an excellent model to serve as a judge? Or are there more ambitious goals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BD79wc0DHZ", "forum": "y3UkklvoW9", "replyto": "y3UkklvoW9", "signatures": ["ICLR.cc/2026/Conference/Submission25299/Reviewer_W3Ck"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25299/Reviewer_W3Ck"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546484201, "cdate": 1761546484201, "tmdate": 1762943391217, "mdate": 1762943391217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new benchmark, termed THEMIS, to comprehensively evaluate the capability of current MLLMs in scientific paper fraud forensics. Compared with existing visual fraud reasoning benchmarks, the proposed THEMIS exhibits several advantages, including real-world scenarios & complexity, tasks diversity & granularity, and multi-dimensional capability evaluation. Based on THEMIS, the authors extensively evaluate recent open-source and proprietary MLLMs and conduct in-depth analysis."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. THEMIS defines a comprehensive taxonomy for the field of scientific paper fraud forensics.  Specifically, THEMIS covers 7 academic scenarios, 5 tasks, 16 manipulation operations, and 5 core reasoning capabilities, which is more diverse than existing visual fraud reasoning benchmarks.\n2. The data quality of THEMIS is high. The synthetic data is rigorously reviewed by human experts. Moreover, THEMIS contains real samples in addition to synthetic samples, which makes it closer to real-world applications. \n3. The authors provide the details of data curation and question generation, which lays a soild groundwork for future research.\n4. The authors conduct comprehensive evaluation and in-depth analysis based on THEMIS. The MLLMs involved includes 6 proprietary ones and 5 open-source ones. The evaluation results demonstrate the significant challenges posed by this benchmark. Furthermore, the analysis conclusions provides deep insights into the related field."}, "weaknesses": {"value": "1. The MLLMs evaluated in this paper are not comprehensive enough. It is recommended to supplement the results of InternVL3.5, GLM4.5V, Gemini-2.5-Pro, Claude, etc.\n2. There is a lack of comparsion on difference parameter sizes of the same series of MLLMs (e.g., Qwen2.5-VL-3B/7B/32B/72B).\n3. The conclusion in lines 373-375 is not well explained."}, "questions": {"value": "In lines 1443-1444, \"However, performance drops markedly on synthetic data, with some tasks almost completely failing (e.g., LLaMA approaches zero on both splicing and copy-move)\". This statement may be inconsistent with Table 10."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d3p6Mxt1XZ", "forum": "y3UkklvoW9", "replyto": "y3UkklvoW9", "signatures": ["ICLR.cc/2026/Conference/Submission25299/Reviewer_f9Wq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25299/Reviewer_f9Wq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636681585, "cdate": 1761636681585, "tmdate": 1762943390846, "mdate": 1762943390846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper constructs a benchmark for forgery forensics in academic papers, covering various forms of academic misconduct across multiple research domains. The benchmark includes both simple manipulations such as copy-move and fully AI-generated forgeries, and provides a comprehensive evaluation of existing MLLMs. The results show that current MLLMs perform poorly on this benchmark, with localization tasks performing significantly worse than attribution tasks, highlighting the substantial room for improvement in this area.  \n**As I am not deeply familiar with this specific domain, I would prefer to refer to the opinions of other reviewers when determining the final score.**"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The constructed benchmark covers a wide range of academic disciplines and forgery types, and the evaluation includes a relatively comprehensive set of model categories.  \n2. Although there are a few typos, the paper is overall well-written, and the figures and visual presentations are clear and well-designed."}, "weaknesses": {"value": "1. The authors mention using the **Fitz** library and **YOLOv7** for information extraction and segmentation. However, to my knowledge, there are now more accurate tools in the document extraction domain, such as **dots.ocr** and **MinerU**. Given the diversity of samples, the effectiveness of simply applying Fitz (PyMuPDF) and YOLOv7 is questionable. Since subsequent steps rely heavily on accurate information extraction, this stage could be improved to ensure higher benchmark quality and reliability.  \n2. The appendix should include more **case examples** from different categories within the benchmark to give readers an intuitive understanding of its scope and difficulty level."}, "questions": {"value": "Please refer to the *Weakness* section for detailed explanations.  \nIf the authors can adequately address my concerns, I would be very willing to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QVF3dLigcM", "forum": "y3UkklvoW9", "replyto": "y3UkklvoW9", "signatures": ["ICLR.cc/2026/Conference/Submission25299/Reviewer_bNnH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25299/Reviewer_bNnH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656542926, "cdate": 1761656542926, "tmdate": 1762943390586, "mdate": 1762943390586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}