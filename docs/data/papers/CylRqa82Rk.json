{"id": "CylRqa82Rk", "number": 18411, "cdate": 1758287426579, "mdate": 1759897105140, "content": {"title": "TS$^2$: Training with Sparsemax+, Testing with Softmax for Accurate and Diverse LLM Fine-Tuning", "abstract": "Large Language Models (LLMs) typically rely on Supervised Fine-Tuning (SFT) with Cross-Entropy (CE) loss to specialize in downstream tasks. However, CE forces the distribution toward one-hot targets and ignores alternative continuations, thereby limiting output diversity—a key drawback for generative applications that rely on sampling-based exploration.\nIn this paper, we propose ``Training with Sparsemax+, Testing with Softmax (TS$^2$)''. Intuitively, sparsemax and its tailored loss mask the gradients of probabilities outside the support set, leaving excessive probability mass on irrelevant tail classes when evaluating with softmax. To address this issue, we propose an improved variant, Sparsemax+, for training, which augments the sparsemax loss with a suppression term that penalizes the out-of-support probabilities. At testing, we decode with softmax, yielding calibrated, non-degenerate probabilities where plausible near-ties survive.\nWe fine-tuned Llama-3.1-8B and Qwen-2.5-7B with TS$^2$, achieving consistent improvements in accuracy and output diversity across chat, code, and open-domain benchmarks. Together, these results demonstrate that TS$^2$ provides a practical, drop-in solution for fine-tuning LLMs that are both more accurate and more creative.", "tldr": "", "keywords": ["Large Language Models", "Supervised Fine-Tuning", "Output Diversity"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58736a11fde58844db0271eb164236b31608404b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an alternative to the cross-entropy loss for language model SFT. The main motivation for this is that CE loss pushes all the probability mass under the golden token, killing the diversity in the pre-trained model. The proposed method uses sparse activation (sparsemax+) to train the model on only plausible tokens. This makes the model output diverse when used with softmax while keeping it accurate. The paper shows better diversity and accuracy compared to existing methods like GEM, CE+weighting (CE regularization), and CE."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The results show strong performance and diversity on a variety of benchmarks and are measured on a variety of metrics for diversity.\n- Results show that better diversity also leads to sample efficiency when doing BON\n- Ablation studies show holding out one of the component leads to drop in performance, indicating the need for all components (sparsemax+, softmax decoding) for the best performance\n- The method is compared against existing SOTA (GEM)\n- The motivation is clearly laid out, and the research questions are made clear"}, "weaknesses": {"value": "- The paper discusses the overconfidence due to the SFT issue, but doesn't discuss existing work on it\n- Other relevant methods to prevent diversity collapse are mentioned, but don't have a comparison against (Label smoothing, unlikelihood)"}, "questions": {"value": "- For baselines, did you consider label smoothing and α-Entmax since they directly address the overconfidence problem? \n- I am curious to know the effects of the TS2 loss on factual accuracy when the diversity is high? As in, when you sample multiple times, do the facts stay consistent? \n- Can authors comment on the effectiveness of this method when training on datasets with long reasoning traces? Does the model still give coherent reasoning steps when diversity is maintained?\n- Similar to above, how does it perform on a multi-turn dataset (for example, on tau-bench)?\n- Could you share the decoding hyperparameters? Since this could influence the measured diversity"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uQt6s8MHN8", "forum": "CylRqa82Rk", "replyto": "CylRqa82Rk", "signatures": ["ICLR.cc/2026/Conference/Submission18411/Reviewer_jH36"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18411/Reviewer_jH36"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709636204, "cdate": 1761709636204, "tmdate": 1762928113807, "mdate": 1762928113807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of output diversity loss in supervised fine-tuning (SFT) of large language models. Standard SFT with cross-entropy (CE) tends to over-fit the model's output distribution to one-hot targets, suppressing alternative correct continuations and thus reducing generative diversity. The authors propose TS$^2$ (Training with Sparsemax+, Testing with Softmax) as a drop-in replacement for the usual fine-tuning procedure. During training, they replace the softmax+CE loss with a modified Sparsemax+ loss, which encourages a sparse probability distribution over next-token outputs while adding a suppression term to push down probabilities of tokens outside the support of the target. At inference, the model still uses a normal softmax for decoding. This strategy preserves plausible near-tie candidates in the output distribution (instead of forcing a single high-probability token as CE does) while still penalizing irrelevant tokens. The paper's contributions include: (1) a new Sparsemax+ transformation and loss function that fixes a known issue of vanilla sparsemax (which left too much residual probability on tail classes), and (2) demonstrating that TS$^2$ yields consistent improvements in both accuracy and output diversity over the baseline CE fine-tuning. The results suggest that TS$^2$ is an effective, easy-to-implement approach to obtain LLMs that are more accurate while also more creative (producing varied outputs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Novelty:** The paper tackles the problem of maintaining output diversity in supervised fine-tuning. It introduces a creative solution by leveraging sparsemax (a non-standard probability transform) in a new way. The proposed Sparsemax+ improves upon sparsemax by addressing its known gradient masking issue for out of support set classes. This combination (train with sparsemax+, test with softmax) is a novel idea that, to my knowledge, has not been explored in prior LLM fine-tuning literature.\n\n* **Quality:** The authors fine-tune two open models (Llama 3.1 8B and Qwen 2.5 7B) on multiple benchmarks covering different domains (chat/dialog, code generation, and open-domain tasks). The results consistently show improvements over the baseline cross-entropy fine-tuning. Importantly, both accuracy (task performance) and diversity metrics improve, which demonstrates the method's effectiveness without trade-offs. The paper also provides intuition (and some theoretical discussion) for why TS$^2$ works, grounding the empirical findings.\n\n* **Clarity:** The paper is clearly written with a logical flow. The authors motivate the problem (lack of alternative continuations under CE) convincingly, and then explain their solution step-by-step. Key terms like sparsemax and the new loss are defined, and the intuition is easy to grasp.\n\n* **Significance:** The findings have practical significance for the field. Many applications of LLMs (creative writing, coding assistant, dialogue systems) benefit from diverse outputs, and TS$^2$ provides a simple drop-in technique to achieve that without sacrificing accuracy. This is significant because typically one might expect a trade-off between diversity and correctness. The method is also easy to implement for practitioners (just changing the loss function during training), which means it could be adopted widely if these results hold."}, "weaknesses": {"value": "* **Missing Baseline Comparisons:** The experimental section might be missing some baselines for a fair assessment. In particular, it's important to know how TS$^2$ compares against simpler fixes to the one-hot problem, such as label smoothing during CE training or using entmax loss [1] (which is a family that includes sparsemax and softmax). If those were not evaluated, the authors should clarify why. Including such baselines would strengthen the paper by demonstrating that Sparsemax+ indeed outperforms these alternatives in balancing accuracy and diversity. Without them, it's harder to quantify the gain from the new method versus existing tricks.\n\n* **Impact on Downstream Use Cases:** The paper shows benchmark improvements, but have the authors examined qualitative examples or specific scenarios illustrating the benefit? For instance in a chat setting, does TS$^2$ produce more varied and interesting responses to open-ended questions (compared to CE which might produce more templated answers)? A few concrete examples in the rebuttal version would be great to demonstrate the practical difference in model behavior.\n\n* **Hyperparameter Sensitivity:** How sensitive is the Sparsemax+ training to its hyperparameter - the strength of the out-of-support suppression term $\\alpha$? The paper mentions that $\\alpha$ is empirically determined for each model architecture, but additional analysis on its effect (example varying $\\alpha$ to show robustness or convergence behavior) would help readers better understand how critical this choice is and how generalizable the method is.\n\n[1] \"Sparse Sequence-to-Sequence Models\", Ben Peters and Vlad Niculae and André F. T. Martins, https://arxiv.org/abs/1905.05702."}, "questions": {"value": "The questions are the same as those listed in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PMjEWPD5F5", "forum": "CylRqa82Rk", "replyto": "CylRqa82Rk", "signatures": ["ICLR.cc/2026/Conference/Submission18411/Reviewer_uXcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18411/Reviewer_uXcp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958882944, "cdate": 1761958882944, "tmdate": 1762928113336, "mdate": 1762928113336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TS2 (Train with Sparsemax+, Test with Softmax), a novel fine-tuning framework for large language models that aims to mitigate distributional collapse and preserve useful diversity. The key idea is to decouple the probability mappings used in training and inference. During training, the model uses Sparsemax+, an enhanced version of sparsemax that includes a tail-suppressing penalty to eliminate residual probability mass on implausible tokens, thereby enforcing “Tail-Suppressed Plausible Diversity (TSPD).” At inference, the model reverts to the softmax mapping to restore smooth, calibrated probabilities, allowing exploration among plausible alternatives. The authors provide theoretical analysis on gradient masking and sparsity behavior, proving that sparsemax expands pairwise logit gaps faster than softmax and yields a more decisive support set. Empirically, TS2 is benchmarked against CE, NEFT, and GEM on both chat and code-generation tasks, showing consistent improvements in win rate, pass@k, and diversity metrics. Ablation studies further confirm that the combination of sparsemax training, softmax inference, and suppression penalty is crucial to the method’s success."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Empirically, the results are comprehensive and convincing, spanning multiple domains (chat, code, creative writing, and reasoning benchmarks) and multiple base models (Llama-3.1-8B, Qwen-2-7B). \n\nThe method outperforms strong baselines such as GEM not only in accuracy but also in diversity metrics, demonstrating improvement both in quality and variety. The clarity of presentation and reproducibility documentation is also commendable."}, "weaknesses": {"value": "1. The main limitation is the upper bound of the cumulated tail mass of softmax outside the top-$m$,\n$$\n\\sum_{k > m} p^{\\mathrm{sf}}_{(k)} \\le \\frac{(K - m)e^{-\\gamma}}{A_m}.\n$$\nIt is said that this upper bound is strictly increasing in $K$ (for fixed $m, \\gamma$) and approaches $1$ as $K \\to \\infty$. Finally, the authors state that ''with large vocabularies, the admissible tail under softmax at inference becomes non-negligible.''\n\nHowever, the statement is not support by the theorem. The upper bounds goes to 1 does not tell anything about the probability. To support the claim, the authors should prove that the upper bound is tight, or provide a lower bound on the probability.\n\n2. The proposed loss is simple, compressing the tailed probability to 1 single probability. To support that the proposed one is better, the authors should provide a result like Corollary 5 with a tighter bound (if possible). Otherwise, it is hard to tell whether the proposed loss resolved the raised theoretical issue.\n\nWriting and presentation issues:\n\n1. The definition of sparsemax (lines 154–157) appears to be ill-defined. Specifically, the threshold function is defined in terms of the support set, yet the support set itself is defined based on the threshold function, resulting in an apparent circular definition. I suspect that sparsemax has a formal, non-circular definition in its original paper, and that the circular definition presents only two derived properties rather than the definition itself.\n\n2. Lemma 3, Lsp(z,y) is not defined before.Leaving readers to guess whether it is refered to “we train with sparsemax and optimize a modified Fenchel–Young loss tailored to this mapping” mentioned in the introduction."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gUDNimkole", "forum": "CylRqa82Rk", "replyto": "CylRqa82Rk", "signatures": ["ICLR.cc/2026/Conference/Submission18411/Reviewer_2nRT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18411/Reviewer_2nRT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961060762, "cdate": 1761961060762, "tmdate": 1762928112879, "mdate": 1762928112879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new normalization method from logits to the probability distribution. By analyzing the behavior of the new normalization method, the authors point out that the method is only good for training but not for inference. Thus, the authors give the overall framework that the model is trained with the new method (i.e., Sparsemax+) and is inferred with softmax. The experimental results show that the framework increases the pass@N a lot when N is large."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. To preserve diversity, the authors introduce a new normalization layer called Sparsemax+.\n\n2. After analyzing the properties of Sparsemax and softmax, the authors choose to use different normalization layers for training and inference.\n\n3. The performance increases a lot when the number of rollout samples is large."}, "weaknesses": {"value": "1. It is unclear what the mismatch of using Sparsemax+ and softmax will cause.  The model performance is not as good as GEM for pass@N when N is small (e.g., N = 2). Is it because of the mismatch?\n\n2. Sparsemax+ does not seem to be tunable. In softmax, there is a temperature parameter that can control the final results. Also, in GEM, there is a hyperparameter that can balance the KL term and the objective.\n\n3. For $i,j \\notin S^{sp}$, $\\frac{\\partial }{\\partial u}(p_i - p_j) < 1$, what is different between this result and softmax?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DHATR7ckQX", "forum": "CylRqa82Rk", "replyto": "CylRqa82Rk", "signatures": ["ICLR.cc/2026/Conference/Submission18411/Reviewer_GaaU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18411/Reviewer_GaaU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762488226720, "cdate": 1762488226720, "tmdate": 1762928112405, "mdate": 1762928112405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}