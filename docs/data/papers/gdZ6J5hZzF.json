{"id": "gdZ6J5hZzF", "number": 8100, "cdate": 1758063017511, "mdate": 1759897807537, "content": {"title": "Sequences of Logits Reveal the Low Rank Structure of Language Models", "abstract": "A major problem in the study of large language models  is to understand their inherent low-dimensional structure.  We introduce an approach to study the low-dimensional structure of language models at a model-agnostic level: as sequential probabilistic models. We first empirically demonstrate that a wide range of modern language models exhibit low-rank structure: in particular, matrices built from the model's logits for varying sets of prompts and responses have low approximate rank. We then show that this low-rank structure can be leveraged for generation --- in particular, we can generate a response to a target prompt using a linear combination of the model's outputs on unrelated, or even nonsensical prompts.\n\nOn the theoretical front, we observe that studying the approximate rank of language models in the sense discussed above yields a simple universal abstraction whose theoretical predictions parallel our experiments. We then analyze the representation power of the abstraction and give provable learning guarantees.", "tldr": "We exploit the low-rank structure of the logit matrices of LLMs to draw new empirical and theoretical conclusions.", "keywords": ["Large language models", "low-rank structure"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0eb637b83293b1d7c0e39ff1137fa3ec8529cb87.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel, model-agnostic framework for understanding the internal structure of language models by studying their \"extended logit matrices.\" The authors define this matrix based on the log probabilities of sequences (futures) given various contexts (histories).\n\nThe paper's contributions are twofold:\n\n- Empirical: They demonstrate across a range of modern LLMs that these extended logit matrices have a consistent, approximate low-rank structure. This structure is shown to emerge early in the pre-training process. This low-rank property is then exploited to create a generation procedure \"LINGEN\"， which can generate a response to a target prompt by querying the model on unrelated or even nonsensical prompts.\n\n- Theoretical: They connect this empirical finding to a formal generative model, the Input Switched Affine Network (ISAN). They prove that having an exact low logit rank is equivalent to being expressible as a time-varying ISAN and provide provable learning guarantees for this model class under logit query access."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel Framework**: The concept of the \"extended logit matrix\" is a simple and powerful model-agnostic abstraction. It provides a new lens to study the intrinsic dimensionality and structure of LLMs beyond just analyzing weights or single-token logits.\n\n- **Comprehensive Validation**: The paper does an excellent job supporting its claims with both strong empirical evidence and solid theoretical grounding. The empirical study is thorough, covering multiple model architectures and sizes, and the theoretical connection to ISANs is elegant.\n\n- **Novel Generation & Safety Implications**: The \"LINGEN\" generation method is a surprising consequence of the observed low-rank structure. The ability to generate text from a target prompt using only queries to unrelated, nonsensical prompts is a significant finding. This has clear and important implications for AI safety, as it suggests a potential new vector for bypassing prompt-based safety filters."}, "weaknesses": {"value": "- **Safety Implications Discussed but Not Fully Explored**: The primary weakness is that the safety implications, while highlighted, are not demonstrated in practice. The paper suggests that LINGEN could be used to \"circumvent input filters,\" but it does not provide a concrete experiment showing such a jailbreak. The analysis is currently a proof-of-concept (i.e., generating coherent text) rather than a practical demonstration of a safety bypass. This is acknowledged as future work (Appendix E), but it leaves the practical severity of this potential vulnerability unclear."}, "questions": {"value": "- Following up on the weakness: Could you elaborate on the practical feasibility of using LINGEN to bypass existing safety alignments? How many histories would be required? Is it feasible to perform the attack on large LLMs? How much cost would it take to construct these histories and their corresponding weights for linear combination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "s80Ps3za4i", "forum": "gdZ6J5hZzF", "replyto": "gdZ6J5hZzF", "signatures": ["ICLR.cc/2026/Conference/Submission8100/Reviewer_L59i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8100/Reviewer_L59i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8100/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830815443, "cdate": 1761830815443, "tmdate": 1762920083711, "mdate": 1762920083711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present an analysis of low-rank structures for logit matrices in language models. The paper idea is very interesting and new, since previous analyses limit to analyze logit structure for a prompt, here the authors consider logit matrices over sets of histories and futures. The experimental analysis reveals interesting aspects of low-eank structures, which are of extreme interest for the community working in interpreting language models internals."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "I found that the experimental section is almost excellent and appreciate how the authors give a comprehension of the phenomena observed, state new hypothesis and propose future directions. On its own, this is a great contribution which opens the doors for more future research. \n\nAbout experiments:\n1.  While measuring the complete logit matrix is unfeasible for all tokens and only top-100 are considered, the analysis clearly shows low rank structures across different datasets with histories and futures size $n \\in (10^3,10^4)$, both for a logit matrix  $n \\times n \\times k$ for full set of histories and futures, and also for downsized $n/t \\times n/t \\times k$, varying $t$ (correct me if I got it wrong).  The scaling of eigenvalues is an interesting observation, which is well explained by the authors through the analysis of  the coefficient $\\alpha$. \n2. The experimental outcome of principal angles between logit matrix of sensical  (A) and non-sensical futures (A') is also an interesting observation. As explained by the authors, this shows that despite low-rank approximation of A and A',  the two spaces have high intersection, with not much beyond the intersection. The finding that this also holds between different language models is even more interesting, it should be highlighted more in the paper. \n3. Using these linear dependencies between histories to control generation with LINGEN is an interesting application and a nice methodological contribution. The only limitation is that LINGEN requires having access to (many) logits of the target history for many futures to evaluate the corresponding $v_{h_{tar}}$. Still, the method is interesting and the authors can highlight more how to improve such limitation in evaluating $v_{h_{tar}}$ (even in a limitation section). In my opinion, a careful choice of futures could help in that direction.\n\nAbout theory:\n* The analysis on time-varying ISAN is valuable, with novel theoretical insights, although with very limited explanation in the text. I am particularly interested in how such a linear model can capture generation in a generic language model, see my questions below."}, "weaknesses": {"value": "While the material is excellent, with new interesting results and a theoretical reformulation of the ISAN model, the clarity should be improved. \nMy comment arise from the fact that I had to re-read the text many times and continue jumping to appendix before completely capturing the message or familiarizing with the quantities the authors introduce. In fact, while theoretical explanations are useful in the experimental sections, it overloads the reader trying to grasp both the theory and interpret the experiments at the same time. \nI suggest the authors to move, as much as possible, theoretical matherial in an apposite section while leaving more space to experimental details alone. \n\nOther points that are weak:\n1. **Notation** Denoting with $\\mathcal H$ and $\\Sigma$ the sets of histories and of tokens, it is a bit imprecise to write $\\mathbb R^{\\mathcal H}$ and \n$\\mathbb R^{\\Sigma}$ the vector spaces with dimension of the sets. It would be more convenient to use the cardinality or introduce naturals $M$ and $N$ for that. To be clear, this was a bit confusing when reading the manuscript. \n2. **Notation** From other papers, feature mappings $\\phi, \\psi$ are also called the embeddings and unembeddings of the model [1,2]. \nI also noticed that while $\\phi: \\mathcal H \\to \\mathbb R^d$, with $d$ dimension of vector embeddings, $\\psi: \\Sigma \\to \\mathbb R^d$. \nSo it is a bit imprecise to write $\\psi(f)$, if $f \\in \\mathcal F$  is a sentence. Can the authors explain the relation to formalism in [1,2]? \\\nI noticed that the authors make use of $\\phi(h \\circ f)$ in Def. 2.2, which makes sense, but suggest that $\\psi(f)$ is never used in any step of the computations.\n3. **Experiments** Some quantitative results should be made more clear. In 3.2, what is the proportion of \"small\" principal angles of the two ranks approximations? And how this impacts to finding transfer across different futures? The claim \"Thus, linear relationships between histories transfer to significantly different sets of futures.\" should be substantiated more. In light of this precisation, it would be beneficial to highlight more the results on different language models, which essentially test a different hypothesis (which tells that the vector $v$ of the null space is almost shared between models, if I got it correctly). This aspect should be explained further. \n4. **Notation** In lines 359/360 I guess there is a typo in saying $t\\geq 1$, since you would have $z_{1:0}$ for $t=1$. \nAlso denoting with $v$ the vector associated to $h_{tar}$ is a bit confusing. I suggest to denote it as $v_{tar} \\in \\mathbb R^{|\\mathcal H|}.$ \n5. **Method** LINGEN is interesting, but as far as I can see, estimatingthe vector $v$ requires prompting many times the language model with the history $h_{tar}$. Given the scope of the paper, this is not a serious limitation, but the discussion should be enlarged on this. I expect that some tricks can be created to estimate the vector $v$ for $h_{tar}$ without prompting all the futures. What is the authors' opinion on this matter?\n6. The connection to ISAN is interesting but received far too limited space. I could not toatally grasp the message in that section, especially for why a time-varying ISAN would be more preferable than a language model with low-rank. My doubt arises as you need to instantiate $A_{z, t}$ and $B_t$ for each element $t$ in the chain, and each token $z$. Withouth sharing $A$ and $B$ across elements $t$ of the chain, what is the advantage of this construction?  Can you further elaborate? \\\nThere is also a lot of material on ISAN which would constitute a separate contribution and that I could not review from the Appendix. Because of the lenght of the material in the Appendix and the statements not appearing in the main text, I could not verify the correctedness of the claims. I really suggest the authors to take more space to explain the construction and relevant results of ISAN.\n\n**Clarity comment** Overall, some results could be moved to Appendix or shortned. E.g., the whole introduction of KL in Def. 3.1 and discussion can be easily moved to the Appendix without compromising much the text, as they are quite standard,  while leaving space for expanding on some points I mentioned above. \n\n**Comment** Improving clarity would much increase the impact of the authors' contribution, which is extremely timely and relevant for research in language models structure learning. Depending on the rebuttal, I am available to further review a restructuring of the paper with my and other reviewers comments. \n\n-------\n\n[1] Park et al. \"The linear representation hypothesis and the geometry of large language models\" ICML 2024 \\\n[2] Marconato et al.  \"All or none: Identifiable linear properties of next-token predictors in language modeling\" AISTATS 2025"}, "questions": {"value": "**Note that:** There is much intersection with works that have treated embeddings and unembeddings in language models. From [1,2], the logit matrix which is used in this paper has a peculiar relation to embeddings and unembeddings. By defining the mean-unembedding vector $\\psi_m := \\sum_{y \\in \\Sigma} \\psi(y) / |\\Sigma|$, then (if I'm not wrong) the logit matrix can be written as:\n$$\n{L_M}(\\mathcal H, \\mathcal F)_{(h, (f,z))} = \\phi(h \\circ f)^\\top (\\psi(z) - \\psi_m)\n$$\nwhich is again equal to considering the pivoting step in [2] and similar to as it is done in [3]. Interestingly, this casts a connection to theoretical results in linear properties of language models [1,2] for next-token predictors. Low-rank is new in this sense, but there can be some interesting connection to what are parallel vectors and parallel histories (as you notice in your paper), but also to the effective complexity of the language model [2].  \n\n**Q** For the versions of OLMO, what is the representation dimensionality $d$? Did you check if using $d$ components you recover the distribution of original LLM? \\\nI think that drawing a connection to effective complexity/dimensionality of representations can highlight distinction from the setup in [1,2] which is not concerned with embeddings of futures. This may inspire an update of effective complexity of the model based on histories and futures.\n\n**Q** About results in Sec 3.2, what is the difference in evaluating the principal angles between rank reduced A and A' w.r.t. ones? For rank reduction of A and A' with rank k, if you keep the k-th higher components, should that be the same if you consider rank 2k? I don't understand how this measure depends on rank reduction. Can you further explain?\n\n**Q** What is the authors opinion about the relation to linear properties that are observed for single-token logits? For example, the ISAN construction much resembles relational linearity in a sense. From [2,4,5], the mapping from context to tokens can be seen as relational linear if you have that $z = A_f \\phi(h)$, for some specific matrix $A_f$ that only depends on the future $f$. It seems the ISAN model implies something similar for all members of the chain. \n\n------\n\n[3] Hinton et al. \"Distilling the knowledge in a neural network\" (2015) \\\n[4] Hernandez et al. \"Linearity of relation decoding in transformer language models\" ICLR 2024 \\\n[5] Paccanaro and Hinton \"Learning Distributed Representations of Concepts using Linear Relational Embedding\" 2020"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zp4uj81OW2", "forum": "gdZ6J5hZzF", "replyto": "gdZ6J5hZzF", "signatures": ["ICLR.cc/2026/Conference/Submission8100/Reviewer_PfEp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8100/Reviewer_PfEp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8100/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923794327, "cdate": 1761923794327, "tmdate": 1762920083261, "mdate": 1762920083261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to analyze the low-dimensionality of LLMs' generation process by studying a curious construction called the extended logit matrix, which consists of the log-probability of the topK predicted tokens $\\log P(Z|H\\circ F)$, where $H$ and $F$ are segments of text randomly sampled from a certain source. They found low-dimensional behaviour in the resulting logit matrix, which suggests linear dependence - the predicted logits to some prompts could be closely approximated as an linear combination of predicted logits of other, statistically unrelated prompt. To prove this point, they introduce LINGEN, a linear generation algorithm that reconstructs the output distribution for a target prompt by linearly combining logits from unrelated prompts via a weighting vector $v$ obtained via linear regression. Surprisingly, this produces coherent text—even when the basis prompts are nonsense—revealing strong linear constraints in LLM's autoregressive generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The framework is architecture agnostic: Studies LLMs through their output distribution geometry rather than internal activations.\n\n- The analysis of extended logit matrix is relatively straightforward and tractable via matrix decomposition algorithms.\n    \n- The fact that one could infer LLM generation of a target prompt using unrelated or even non-sensical prompts is very surprising."}, "weaknesses": {"value": "- While the low-rank property is convincingly demonstrated, the paper stops short of providing a theoretical account for why transformer models should exhibit such linearity in distribution space. However, I do not hold this against the paper since the paper is already phenomenologically rich and self-contained.\n- The explanation of how the extended logit matrix is constructed, as well as its motivation, is quite abstruse. I think the paper could benefit from a cartoon illustration of the structure of the extended logit matrix, as well as the LINGEN algorithm. \n- Although LINGEN reproduces plausible text, the qualitative examples remain small-scale, and quantitative evaluations of long-horizon coherence or perplexity are limited."}, "questions": {"value": "- Why is $v$ fit in log-probability space rather than pre-softmax logits, and how sensitive are results to this choice?\n- Could the observed low-rank structure be derived from, or correspond to, linear relationships in the model’s internal hidden states (e.g., out embedding)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SwUm2nPspu", "forum": "gdZ6J5hZzF", "replyto": "gdZ6J5hZzF", "signatures": ["ICLR.cc/2026/Conference/Submission8100/Reviewer_7Jjj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8100/Reviewer_7Jjj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8100/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963438352, "cdate": 1761963438352, "tmdate": 1762920082531, "mdate": 1762920082531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}