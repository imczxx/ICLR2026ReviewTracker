{"id": "uFvGSsTMZN", "number": 1124, "cdate": 1756844912653, "mdate": 1759898226291, "content": {"title": "HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks", "abstract": "Simulating human reasoning in open-ended tasks has been a long-standing aspiration in AI and cognitive science. While large language models now approximate human responses at scale, they remain tuned to population-level consensus, often erasing the individuality of reasoning styles and belief trajectories. To advance the vision of more human-like reasoning in machines, we introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for average-to-individual reasoning adaptation. The task is to predict how a specific person would reason and update their beliefs in novel scenarios, given partial evidence of their past views. HugAgent adopts a dual-track design: a synthetic track for scale and systematic stress tests, and a human track for ecologically valid, “out-loud” reasoning data. This design enables scalable, reproducible evaluation of intra-agent fidelity: whether models can capture not just what people believe, but how their reasoning evolves. Experiments with state-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent as the first extensible benchmark for aligning machine reasoning with the individuality of human thought. Our benchmark and chatbot are open-sourced as HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking (https://anonymous.4open.science/r/trace-your-thinking).", "tldr": "HugAgent benchmarks whether LLMs can simulate human-like individual reasoning in open-ended tasks by predicting belief and reasoning trajectories from partial “out-loud” data.", "keywords": ["LLMs", "human simulation", "human-like reasoning", "individual reasoning", "open-ended tasks", "agent simulation", "benchmarking", "cognitive AI", "cognitive science"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be8cf8cfb9ad9e0e178f24ea8e52dda01a329389.pdf", "supplementary_material": "/attachment/761b86e78d8c668931d08da43f61ac327acfedbe.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a benchmark to evaluate LLMs at simulating individuals, predicting the person's belief state and belief updates. For benchmark creation, an LLM chatbot interacted with humans to infer their beliefs and updates over 8-20 QA pairs on 3 topics, where the survey also collects gold labels based on human reported stances and reasoning weights. The task for a language model is to then predict the structured belief state and updates of humans. Results show that global majority baseline leads to low performance, demonstrating individual reasoning is needed for the task. Evaluations are conducted on low-cost or old models, which with individual context perform near (best 79.12 for Qwen 2.5 32b instruct) the human baseline (80.6) for belief state inference, but are behind (best 63.38 for Llama 3.3 70b) for belief updates (human = 79.70). The paper includes a preliminary discussion of many possible design choices and details for the task of simulating individual reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper studies an interesting problem of LLMs simulating individual reasoning as beliefs and belief updates.\n\n2. The paper conducts a human study to collect data, and also collects human performance estimates by computing test-retest agreement.\n\n3. The paper shows evidence that models find it harder to simulate human belief updates, in comparison to predicting initial human beliefs given the context of their conversation."}, "weaknesses": {"value": "1. The methodology is not written clearly and is hard to follow. For example, the dataset construction is almost entirely described in Appendix C. A chatbot is used to converse with humans to elicit their beliefs and belief updates, but it is unclear how this chatbot is created, what these conversations look like, and how effective this methodology is. Then in section 4.2, the term \"attribution\" is introduced for the first time as a metric being evaluated, even though section 3.5 on evaluation protocols has no mention of it. The paper could benefit from a significant rewrite to make it clear what the hypotheses and claims are, and the methodology used to study them.\n\n2.  It seems like the results are extremely noisy, especially for the belief inference task where most models have very similar performance ranging from 74.5-79.1. There seems to be no clear interpretation of the results even for belief updates, where for example DeepSeek R1 has really poor belief inference performance with strong belief update prediction performance. The stated reason is \"over-elaboration\", but it is unclear what that means, and no evidence or justification is provided for why it is the correct explanation. Moreover, GPT 5 Mini results are reported but not Gemini 2.5 pro, GPT 5 High thinking, Claude etc. which makes it unclear what the state of the art with modern LLMs is. \n\n3. In the section on \"main findings\", the findings seem quite preliminary with not enough convincing evidence. For example in \"more context doesnt always help\", its unclear whether: does the dialogue provided as context already leak the prediction task answers? the claim that performance peaks at 5-10 questions before declining does not hold true for many datapoints in the tables eg Qwen 2.5 32B. For finding 2, I would be interested to konw what the human attribution accuracy generalization is, as I find it unclear why personalization transfer is expected from a single conversation about a person's beliefs about \"zoning\" to \"healthcare\".\n\nThese are just examples of my broader concern: at the end of reviewing this paper, I have little clear takeaways, and much confusion. To improve the presentation in future versions, I suggest a) reporting error bars b) reporting results as bar graphs instead of tables for clearer comparison c) clearly stating hypotheses, why the specific test used to test them is valuable, and what it shows d) giving details on benchmark construction early, and more analysis of benchmark quality and finally more discussion on prior work like [1] that studies individual simulations could help position the contribution.\n\n[1] Generative Agent Simulations of 1,000 People\nJoon Sung Park, Carolyn Q. Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, Michael S. Bernstein"}, "questions": {"value": "1. How good is the benchmark data constructed using a chatbot that elicits human beliefs and belief updates?\n\n2. What is the alignment between the synthetic agent and human study? Why were only \"50 agents\" (where I believe agent means causal bayesian network?) used for the synthetic track if the motivation was scaling (the human study already has 36 participants). Can synthetic agents really be used to reliably scale the data? How ecologically valid are they?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4pzxF6OcbM", "forum": "uFvGSsTMZN", "replyto": "uFvGSsTMZN", "signatures": ["ICLR.cc/2026/Conference/Submission1124/Reviewer_z6pz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1124/Reviewer_z6pz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761523466512, "cdate": 1761523466512, "tmdate": 1762915686765, "mdate": 1762915686765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of evaluating large language models (LLMs) in simulating human-like individual reasoning, moving beyond population-level consensus. The authors introduce HugAgent, a benchmark for \"average-to-individual reasoning adaptation,\" which requires predicting a specific person's belief states and reasoning trajectories given partial evidence of their past views. HugAgent employs a dual-track design: a synthetic track for scalable, controlled stress tests and a human track for ecologically valid \"out-loud\" reasoning data. The benchmark formalizes two core tasks, i.e., Belief-State Inference and Belief Dynamics Update, and evaluates state-of-the-art LLMs, revealing persistent adaptation gaps. Contributions include a formalized task definition, baseline results, error analyses, and open-source release of the benchmark and data-collection chatbot."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is strong and interesting. The paper formalizes \"average-to-individual reasoning adaptation\" as a measurable task, addressing a critical gap in LLM evaluation.\n\n2. The use of first-person, out-loud self-reports as ground truth enhances ecological validity compared to static survey responses and the dual-track design allows for both controlled stress tests (synthetic) and real-world validation (human), which supports robust evaluation.\n\n3. Experiments cover multiple state-of-the-art LLMs. Providing broad baseline results and comprehensive ablation studies further reveal some meaningful findings toward cross-domain generalization, context-length, and so on.\n\n4. Detailed pipeline, including benchmark data collection, annotation, and chatbot code, is well illustrated and released as open source, promoting community adoption and extension."}, "weaknesses": {"value": "1. The human track includes only 36 participants after quality control, which may limit statistical power and generalizability. While the authors mention it in limitations, my primary concern is that are these participants sufficient to represent the thinking ways of people with different characteristics in the real society? To ensure diversity, one way is to increase the sample size. Otherwise, you can report more detailed information about these 36 participants (e.g., their occupations and characteristics) to show that the sample size is enough for diversity consideration. \n\n2. In Sec 4.2, the authors mention that `` ﻿﻿Tables 2 summarize performance. … Open-source LLaMA and Qwen rival GPT-4o’’, but there is no GPT-4o results in Table 2.\n\n3. While the authors observe many findings through ablation studies, they do not analyze deeply for each finding and many of these points were glossed over without in-depth analysis. I think some ideas can be further developed. \n\n4. The update operator U in Eq. (2) is introduced without justification in Line 121."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N3GVVJM65W", "forum": "uFvGSsTMZN", "replyto": "uFvGSsTMZN", "signatures": ["ICLR.cc/2026/Conference/Submission1124/Reviewer_VpnV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1124/Reviewer_VpnV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816920615, "cdate": 1761816920615, "tmdate": 1762915686662, "mdate": 1762915686662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HugAgent, a benchmark for evaluating whether LLMs can simulate individual reasoning rather than just population-level consensus. The key innovation is framing \"average-to-individual reasoning adaptation\" as a measurable task: predicting how a specific person would reason and update their beliefs in novel scenarios given their past views. \n\nThe benchmark features a dual-track design combining synthetic agents (for controlled testing) with human participants (for ecological validity), and evaluates two core tasks 1/ Belief-State Inference (predicting current stance and reasoning), and 2/ Belief Dynamics Update (predicting how beliefs change under counterfactual evidence). Experiments with 9 state-of-the-art LLMs reveal systematic failure modes, with models achieving 74-79% accuracy on belief-state inference but struggling more with belief dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The shift from simulating \"the average\" to \"the individual\" is critical for applications like digital twins, personalized AI assistants, and social simulation. This is well-motivated.\n\nThe paper goes beyond accuracy to include error analysis, cross-domain transfer tests, context ablations, and systematic failure mode analysis (directional bias, domain sensitivity, tail-driven errors).\n\n\nPromising to release the full pipeline, including the chatbot, is valuable for the community.\n\n\nThe formalization using belief states, Bayesian updating, and structural causal models (Section 2) provides principled anchors. The four guiding hypotheses (H1-H4) make the evaluation partially interpretable"}, "weaknesses": {"value": "1. Missing important related work: The paper cites Agent Bank (Park et al., 2024) and discusses ToM benchmarks briefly, unless I miss it -several highly relevant recent works should be discussed more thoroughly:\n\nPersonalLLM (ICLR 2025) - directly addresses personalization with heterogeneous preferences, very similar to HugAgent's goals\n\nUniToMBench (2025) - unified ToM benchmark with multi-interaction tasks and evolving scenarios\n\n\n2. 36 humans is quite limited for establishing generalizable conclusions about individual reasoning. The paper acknowledges this but doesn't adequately address:\n\n - Demographic diversity concerns\n\n- Whether 36 participants provide sufficient coverage of reasoning patterns\n\n- Statistical power for cross-domain transfer claims\n\n\n3. I feel there is a theoretical grounding overclaim. Section 2.2 invokes Bayesian updating, PLoT, and SCMs as \"anchors\". But: 1) These aren't actually used in the evaluation or analysis, 2) The connection between theory and practice is unclear, and 3) H2 (cross-domain transfer) references graph similarity but admits it's \"left as future work\"\n\n4. Table 2 shows that DeepSeek-R1 performs poorly (40.16% on belief-state inference, 42.25% on update). The explanation (\"over-elaboration diverges from human reasoning\") is speculative and not empirically validated."}, "questions": {"value": "For the claim that \"more context doesn't always help\" is interesting but needs deeper analysis, \n- Why does update accuracy peak at 5-10 questions, then decline?\n- Is this cognitive overload or increasing noise?\n\n\nSection 7 proposes \"guiding principles\" but admits these are \"left as future work.\" Did you test any of these principles? If so, what were the results?\n\nThe claim is that synthetic agents \"approximate the structural statistics of human belief graphs.\" Can you provide evidence in the main paper that synthetic agents exhibit similar error patterns to humans when evaluated by LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eV3coiAwlb", "forum": "uFvGSsTMZN", "replyto": "uFvGSsTMZN", "signatures": ["ICLR.cc/2026/Conference/Submission1124/Reviewer_pX6y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1124/Reviewer_pX6y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927522723, "cdate": 1761927522723, "tmdate": 1762915686492, "mdate": 1762915686492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HugAgent, a dataset for evaluating LLM agents' capabilities on simulating human-like reasoning. It features a human track for ecological validity and a synthetic track for scalable stress tests. Results reveal persistent adaptation gaps: top LLMs trail human retest ceilings, with larger deficits on belief dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novel problem formulation assessing LLMs’ capability to simulate *individual* human reasoning.\n- Results provide interesting insights into LLM failure modes\n- Open-source pipeline (chatbot, data, evaluation code) supports reproducibility and extension."}, "weaknesses": {"value": "- Confusing use of “open-ended”: in LLM evaluation, this often implies systems auto-generating novel tasks; here it seems to refer to diverse human reasoning traces in curated domains—clarify terminology to avoid misinterpretation.  \n- Scalar score calibration: predicting 1–10 stance or 1–5 reason weights is known to be noisy in LLMs; no analysis of calibration error or distribution shift vs. human ratings.\n- Synthetic track lacks human validation: 50 scripted agents use deterministic causal graphs. Is there a study that confirms humans perceive these as plausible or follow similar update paths?"}, "questions": {"value": "see above weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "43mZ8osKgS", "forum": "uFvGSsTMZN", "replyto": "uFvGSsTMZN", "signatures": ["ICLR.cc/2026/Conference/Submission1124/Reviewer_imYS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1124/Reviewer_imYS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959484887, "cdate": 1761959484887, "tmdate": 1762915686308, "mdate": 1762915686308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}