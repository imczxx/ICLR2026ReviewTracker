{"id": "8zbWMaREah", "number": 22125, "cdate": 1758326454193, "mdate": 1759896884961, "content": {"title": "Neighborhood Sampling Does Not Learn the Same Graph Neural Network", "abstract": "Neighborhood sampling is an important ingredient in the training of large-scale graph neural networks. It suppresses the exponential growth of the neighborhood size across network layers and maintains feasible memory consumption and time costs. While it becomes a standard implementation in practice, its systemic behaviors are less understood. We conduct a theoretical analysis by using the tool of neural tangent kernels, which characterize the (analogous) training dynamics of neural networks based on their infinitely wide counterparts---Gaussian processes (GPs). We study several established neighborhood sampling approaches and the corresponding posterior GP. With limited samples, the posteriors are all different, although they converge to the same one as the sample size increases. Moreover, the posterior covariance, which lower-bounds the mean squared prediction error, is uncomparable, aligning with observations that no sampling approach dominates.", "tldr": "We analyze the training dynamics of graph neural networks under neighborhood sampling by using graph neural tangent kernels.", "keywords": ["graph neural network", "neighborhood sampling", "neural tangent kernel", "Gaussian process posterior inference"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44778b24879290a2e1a8d5a6cf693a4a69cff030.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a theoretical analysis of neighborhood sampling in GNNs through the lens of neural tangent kernels (NTKs), which connect the training dynamics of infinitely wide networks to Gaussian processes (GPs). By examining several established sampling strategies and their corresponding posterior GPs, the paper shows that while all posteriors converge in the infinite-sample limit, they differ under limited sampling. This paper further demonstrates that the posterior covariances are incomparable, consistent with empirical findings that no single sampling method universally outperforms others."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper derives the posterior inference for evolving GNN-GPs, providing a novel and rigorous extension of prior GNN-GP theory that had only been sporadically explored.\n\n2. This paper offers a clear comparative analysis of major neighborhood sampling methods, revealing their convergence behaviors and theoretical incomparability, which aligns with empirical GNN performance differences.\n\n3. This paper introduces a general, programmable framework for constructing GNTKs in arbitrary GNNs, extending composability theory to graph domains and demonstrating its utility through GraphSAGE examples."}, "weaknesses": {"value": "**1. Theoretical motivation and methodological justification**\n\nThis paper conducts a theoretical study on neighborhood sampling. \n\n1). However, it remains unclear whether or how prior works provide theoretical explanations of neighborhood sampling. Does this paper fill any specific existing gaps? The authors should clarify the related works and what the precise theoretical gaps are.\n\n2). Why NTK is chosen as the main analytical tool? Have alternative analyses (e.g., gradient norm bounds, variance reduction perspectives) been considered? Moreover, the infinite-width assumption in NTK may average out per-neuron gradient noise, potentially obscuring the distinct effects of different sampling methods. Could this limit the conclusions in this paper when comparing different neighbor sampling methods?\n\n**2. Practical meaning and theoretical contribution**\n\n1). The paper‚Äôs extension to posterior inference needs further clarification of its practical meaning. What specific GNN phenomena does this posterior analysis help explain? \n\n2). The extension of GNTK to multiple neighborhood sampling schemes is interesting, but what are the key challenges in making this extension? Does the analysis introduce any conceptual or theoretical breakthroughs beyond the existing GNTK framework? \n\n3). Finally, the conclusion that different sampling methods are ‚Äúuncomparable‚Äù does not seem to provide new theoretical insights or trade-offs. Please elaborate on the novelty of this finding.\n\n**3. Minor suggestion (Figure 1)**\n\nFor Figure 1, please clarify the meaning of the vertical axis (which is not described in Appendix A or the main text). The title states ‚Äúneighborhood sampling drives the GCN-GP to evolve faster to the limit,‚Äù but it is unclear how to visually interpret ‚Äúfaster.‚Äù If t denotes time, the differences among ùë°=0,10,100 appear small, as the blue and red curves nearly coincide at t=100. Can this ‚Äúfaster‚Äù convergence be quantified or made more explicit in the figure or text?\n\n**4. Interpretation of the main conclusion**\n\nGiven the conclusion that ‚Äúneighborhood sampling does not learn the same graph neural network,‚Äù what are the practical implications of learning different GNNs? Does this lead to measurable changes in test accuracy or any other metrics? Please clarify what specific differences these ‚Äúdifferent learned GNNs‚Äù induce. Could these distinctions be demonstrated empirically on real datasets?"}, "questions": {"value": "See weaknesses.\n\nIf the authors can address these questions, I am willing to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yPnkeaFQIx", "forum": "8zbWMaREah", "replyto": "8zbWMaREah", "signatures": ["ICLR.cc/2026/Conference/Submission22125/Reviewer_AFpS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22125/Reviewer_AFpS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654668510, "cdate": 1761654668510, "tmdate": 1762942077846, "mdate": 1762942077846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies neighborhood sampling in GNNs through the lens of Gaussian processes and neural tangent kernels. It derives GNTKs and posterior means and covariances of the associated Gaussian processes for several GNN sampling strategies, aiming to understand how sampling affects learning dynamics and predictive performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper studies neighborhood sampling in GNNs through the lens of Gaussian processes and neural tangent kernels (NTKs). It derives GNTKs and posterior means covariances of the associated Gaussian processes (GPs) for several GNN sampling strategies, aiming to understand how sampling affects learning dynamics and predictive performance. This is a relevant topic.\n- The analysis of the posterior covariance is quite relevant since, as the authors note, the posterior covariance is a lower bound of the mean squared prediction error for GPs. However, this is not emphasized enough in the paper.\n- The conclusion that the posterior mean is unbiased in the infinite-width limit for FastGCN, even though FastGCN is biased, is intriguing and perhaps worthy of further exploration."}, "weaknesses": {"value": "The topic is relevant, but the analysis is mostly mechanical. The paper presents a sequence of derivations for different setups without developing a clear theoretical message or providing intuition about what these results reveal. There is no unifying perspective on the role of sampling, and when the discussion turns to finite-width or finite-sample regimes, the authors simply note that these are difficult to analyze. The results appear to be technically correct, but are a quite shallow exploration of the problem.\n\nMajor comments:\n\n- The derivations do not lead to conceptual insights. The main technical conclusion is that as the number of samples tend to infinity, the GNTK converges to the GNTK of the same architecture without sampling. This is unsurprising as the considered sampling-based GNNs themselves converge to the corresponding GNNs without sampling, as noted in the original references. The paper would benefit from a clearer discussion of what the theoretical results mean for learning or generalization.\n\n- The structure reads as a disconnected, somewhat random assortment of GNN sampling algorithms rather than the analysis of a more general unified sampling framework.\n\n- The paper stops short of addressing or interpreting both the finite-width setting but more importantly the finite-sample setting, which limits its practical relevance.\n\n- The use of the posterior covariance as a lower bound on predictive performance should be better emphasized. Explicitly stating this lower bound for the different sampling strategies and carrying out a more careful analysis of these lower bounds‚Äô implications would greatly improve the paper."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZnShGoSGsa", "forum": "8zbWMaREah", "replyto": "8zbWMaREah", "signatures": ["ICLR.cc/2026/Conference/Submission22125/Reviewer_EbBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22125/Reviewer_EbBb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681424204, "cdate": 1761681424204, "tmdate": 1762942077503, "mdate": 1762942077503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the convergence of neighborhood sampling on GNNs. The paper introduces a study in the neural tangent kernel -- which is an analogous analysis which looks at the infinitely wide neural network associated with it. The problem is therefore recasted as a Gaussian processes, and different standard GNN architectures are considered."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written, and it covers a relevant topic which is neighborhood sampling and efficient training of GNNs,"}, "weaknesses": {"value": "The main issues with the paper are it's novelty and its relevance. \n\nRegarding novelty, the same analysis has already been covered in \n\"For graphs, the NTK becomes a GNTK (Du et al., 2019;\nHuang et al., 2022; Krishnagopal & Ruiz, 2023) and it governs the evolution of a GNNGP (Niu\net al., 2023), which is the infinite-width counterpart of a GNN.\"\nWhat is the main advantage of this work? \nI understand that the authors do an individual characterization of each GNN type in Table 1, but this is esoteric and with little use in practice. \n\nRegarding the relevance of the work, this paper introduces little to none practical implications, and it is therefore very difficult to asses its relevance. What are the real world implications of the work? Not learning the same GNN remains of little use given that \"different GNNs\" might (and do in practice) evaluate to the same test error. And, they are therefore equally good."}, "questions": {"value": "Can the authors add the missing citations:\n\nGraph neural networks (GNNs) are widely used models (Zhou et al., 2020; Wu et al., 2021) for\ngraph-structured data, such as financial transaction networks, power grids, and molecules and crystals. They encode the relational information present in the data through message passing (Gilmer\net al., 2017) on the graph and support a wide array of tasks, including predicting node and graph\nproperties, generating novel graphs, and forecasting interrelated time series.\n\nAlso, the authors are missing citations to the whole Graphon -  Transferability line of work of authors like Luana Ruiz, Ron Levie, Sohir Maskey and Soledad Villar to name a few. I strongly suggest including a parragraph with their relevant works."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B6lJvn1NYr", "forum": "8zbWMaREah", "replyto": "8zbWMaREah", "signatures": ["ICLR.cc/2026/Conference/Submission22125/Reviewer_KHc1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22125/Reviewer_KHc1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950598127, "cdate": 1761950598127, "tmdate": 1762942077075, "mdate": 1762942077075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper does an interesting and important theoretical study of the effect of different graph sampling techniques on the convergence of graph algorithms. Building on the foundations of Neural networks as Gaussian Process (GP) and Neural Tangent Kernel (NTK), the authors analyze how the NTK and the covariance function evolve over time, which gives us an idea on their convergence. The main highlight is Theorem 7 and the following discussion, which concludes two major things: 1) At the infinite sampling limit, all sampling methods converge to the same posterior distribution. 2) There is a theoretical limitation from this perspective that two sampling strategies cannot be compared."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper analyses an important aspect in GNNs, that how sampling affects the learned network function and the convergence of the algorithm.\n- Such insight is valuable to the GCN community."}, "weaknesses": {"value": "Considering that Theorem 7 and the subsequent discussion constitute the core contribution of this paper, while it is a knowledge in itself that sampling methods are theoretically incomparable in general. However, this statement alone does not yield practical guidance for choosing a sampling algorithm in practice. In that regard, I would be particularly interested in further analyses along the following directions:\n\n**While sampling algorithms might be incomparable in general, can they be meaningfully compared under certain structural assumptions on the graph (i.e. their adjacency matrices)?**\n\n- e.g., does a specific sampling algorithm perform better for graphs with high vs. low clustering coefficient?\n\n- do some sampling strategies work better in heterophilic vs. homophilic graphs?\n\nI encourage the authors to explore these scenarios more deeply. There may not exist a universally optimal sampling method for all graphs, but certain methods may be more suitable for particular graph classes. With answers to these questions, I believe the analysis in the paper would be more complete.  \n\nAt this point, my evaluation is borderline. I will reconsider my score based on the authors‚Äô response and the feedback from the other reviewers."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OI6EsX3lxG", "forum": "8zbWMaREah", "replyto": "8zbWMaREah", "signatures": ["ICLR.cc/2026/Conference/Submission22125/Reviewer_8FHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22125/Reviewer_8FHy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017235823, "cdate": 1762017235823, "tmdate": 1762942076541, "mdate": 1762942076541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}