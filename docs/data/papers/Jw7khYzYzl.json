{"id": "Jw7khYzYzl", "number": 13372, "cdate": 1758217077767, "mdate": 1759897441930, "content": {"title": "Benchmarking Optimizers for Large Language Model Pretraining", "abstract": "The recent development of Large Language Models (LLMs) has been accompanied by an effervescence of novel ideas and methods to better optimize the loss of deep learning models. Claims from those methods are myriad: from faster convergence to removing reliance on certain hyperparameters. However, the diverse experimental protocols used to validate these claims make direct comparisons between methods challenging. This study presents a comprehensive evaluation of recent optimization techniques across standardized LLM pretraining scenarios, systematically varying model size, batch size, and training duration. Through careful tuning of each method, we provide guidance to practitioners on which optimizer is best suited for each scenario. For researchers, our work highlights promising directions for future optimization research. Finally, by releasing our code and making all experiments fully reproducible, we hope our efforts can help the development and rigorous benchmarking of future methods.", "tldr": "Benchmarking of 11 optimizers with ablations along the way", "keywords": ["Large Language Models", "Deep Learning", "Optimization", "Optimizers"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/980bfb94fdac73dafda364dbc4402971f36443cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper points out that during the standard pre-training phase of Large Language Models (LLMs), a comprehensive evaluation of recent optimization techniques is conducted. Specifically, the paper systematically varies model scale, batch size, and training duration, and through careful tuning, provides appropriate optimizer selections for various scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-written, with clear logic and concise readability.\n\n2. It conducts extensive experiments, including comparative experiments on models of different types and scales."}, "weaknesses": {"value": "1. First, the paper claims to provide the most suitable optimizer selections for various scenarios. However, as is widely known, different optimizers not only differ in performance but also have significant disparities in computational resources and time required. Conducting a detailed comparison of the optimizer selection schemes provided in the paper from multiple perspectives would enhance the paper’s comprehensiveness.\n\n2. Second, the paper uses relatively small-scale models for experiments. The scale of existing Large Language Models (LLMs) is all larger than that of the models used in the paper’s experiments. This increases the uncertainty of the paper’s conclusions in practical applications, making a broader range of experimental comparisons necessary.\n\n3. Finally, from the perspective of experimental results, the optimizer selection schemes proposed in the paper are appropriate under the current experimental settings. Nevertheless, these selection schemes are heuristic and lack theoretical guidance or selection principles—this fails to address the paper’s core question: \"What are the key factors for selecting optimizers in different scenarios?\" More descriptions should be added regarding future work or limitations."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DOlXZLc9mx", "forum": "Jw7khYzYzl", "replyto": "Jw7khYzYzl", "signatures": ["ICLR.cc/2026/Conference/Submission13372/Reviewer_Sd77"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13372/Reviewer_Sd77"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760607319276, "cdate": 1760607319276, "tmdate": 1762924014526, "mdate": 1762924014526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a benchmark of 11 optimization methods for LLM pre-training across varying model sizes, batch sizes, and training durations, challenging AdamW's long-standing dominance in the field. The study reveals that newer optimizers like AdEMAMix and MARS can significantly outperform AdamW, particularly at larger scales, while batch size critically affects optimizer performance. The research provides insights for practitioners regarding learning rate schedules, momentum parameters, and weight decay that substantially impact both absolute performance and relative optimizer rankings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Developing a better understanding of optimizer dynamics beyond the go-to AdamW optimizer is important for building more efficient training pipelines\n- I do appreciate the overview of recent developments in the optimizer field. This can be valuable for practitioners in building more effective training pipelines."}, "weaknesses": {"value": "- I am generally missing deep insights. All highlights in the paper appear to be trivial to come up with by systematically comparing the optimizers with one another. I would have hoped to read more about _why_ certain optimizers exhibit their unique dynamics rather than just a a vs b comparison (e.g., Takeaway 1: ... sign-based methods (Signum, Lion), and MARS greatly benefit from the increased batch size; (III) Sophia diverges in the small-batch setting, when trained beyond the Chinchilla optimal horizon...). What are the general insights that will help a practitioner or researcher build better (more sample-efficient) training routines? Also, some of the Takeaways are well known and not new (e.g., Takeaway 5). \n- I am missing the \"so what?\" The paper ends with results and does not provide a discussion of what might be the best optimizer going forward (incl. hyperparameter ranges that work well in a wide number of settings) or even edge cases where certain optimizers fail. Also, in the end the optimizer performance depends on the underlying data. FineWeb is considered to be a curated pre-training dataset. Exploring the performance under data heterogeneity would yield more meaningful insights, I believe.\n- I do see a major issue with the chosen model sizes. Typically, ablation studies use models that are twice or even 10x the size of what is being discussed in the paper (e.g., GneissWeb uses 1B and 7B models to explore data dynamics, https://arxiv.org/pdf/2502.14907).\n\n**Formatting concerns**\n- The paper appears to make too much use of vertical spaces. Some figures overlap the text, making some section a bit difficult to read (e.g., l. 294)"}, "questions": {"value": "- How do you make sure you are actually providing an apples-to-apples comparison when you need to tune multiple hyperparameters at once (e.g., line 266f.) where you cannot be sure how the changes individually affect the optimizer dynamics? \n- How do the optimizers behave with noisy data or with datasets that contain samples with very little relevant information? \n- How do the computational cost of all the optimizers compare?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "65ITDcGvtR", "forum": "Jw7khYzYzl", "replyto": "Jw7khYzYzl", "signatures": ["ICLR.cc/2026/Conference/Submission13372/Reviewer_yoQR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13372/Reviewer_yoQR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761342216376, "cdate": 1761342216376, "tmdate": 1762924013890, "mdate": 1762924013890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a comprehensive evaluation of 12 optimizers for LLM pretraining. With systematic experiments on varying model size, model type (dense/MoE), training duration, batch size, etc, the paper reports rankings across regimes, and releases code/configs to encourage reproducibility. Comprehensive ablation on hyperparameters provides practical insights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic is highly relevant and significant to LLM pretraining. While AdamW is the de facto optimizer, numerous new optimizers have been proposed, claiming to outperform AdamW. It is worthwhile to conduct controlled benchmark experiments to evaluate these optimizers and compare their performance. \n\n2. Experiments seem comprehensive and rigorous. It covers a wide range of optimizers, model size, batch size, training duration, and other configurations. The experiments are standardized, and hyperparameters are carefully tuned for each optimizer. \n\n3. Releasing the full benchmarking toolkit greatly enhances reproducibility and facilitates future research."}, "weaknesses": {"value": "1. The experiments are limited in scale. The largest model in experiments is only 720M, while concurrent work [1] scales up to 1.2B. This could affect the results and claims, as larger models can be less sensitive to optimizers. \n\n2. The evaluation metric is limited to validation loss. Downstream performances like MMLU and GLUE are not included.\n\n3. The presentation is a bit crowded for now (due to page limit). \n\n\n[1] Wen, Kaiyue, et al. \"Fantastic pretraining optimizers and where to find them.\" arXiv preprint arXiv:2509.02046 (2025)."}, "questions": {"value": "1. In your experiments, MARS and AdEMAMix are promising. However, in [1], MARS performs worse than Muon and SOAP. What could be the reason for this difference?\n\n2. Have you considered performing even limited downstream evaluations (e.g., on GLUE, MMLU) for models trained with the top-performing optimizers vs. AdamW to confirm if the validation loss improvements translate to task performance gains in your setting?\n\n[1] Wen, Kaiyue, et al. \"Fantastic pretraining optimizers and where to find them.\" arXiv preprint arXiv:2509.02046 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1Esz1PK2kx", "forum": "Jw7khYzYzl", "replyto": "Jw7khYzYzl", "signatures": ["ICLR.cc/2026/Conference/Submission13372/Reviewer_Tb1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13372/Reviewer_Tb1Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729097619, "cdate": 1761729097619, "tmdate": 1762924013527, "mdate": 1762924013527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducted a systematic benchmark of optimizers for LLM pretraining, evaluating over many choices (e.g. AdamW, Muon, D-Muon, AdEMAMix, SOAP, MARS, etc) across multiple model sizes and architectures, ranging from 124 M to 720 M parameters. For each optimizer and model, the author finetune the hyperparameters, schedulers, erc based on the scaling law, and rank optimizers based on the final validation ppl using FineWeb dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper conduct systematic evaluation of the optimizer's performance with finetuned hyperparameters. There approach seems to be solid (**under their setup**). But I have concerns regarding their setup, and I will elaborate about this later. From these benchmarking, it reveals several interesting findings for practitioners, e.g. AdamW can catchup in the end when iterations is longer. There rankings and empirical findings are useful for the community."}, "weaknesses": {"value": "I have several concerns regarding the setups.\n\nFirst, the choice of context length (as a benchmarking paper) is not practical. $512$ is too short for any production-level LLM, even in the pretraining phrase. Changing the context length can have effects on the conclusions. In practice, $1K$ to $4K$ or even $8K$ context sizes are often adopted. \n\nSecond, the training iterations (or equivalently the token batch size) is too long. This may also alter the conclusions. Note that training with longer iterations and compare the checkpoints is **NOT** the same as training directly with short iterations due to the lr scheduling and varied batch sizes. For example, for 124M model, you uses upto $1024$K iterations to finish 16.8B token. However, even for industrial-level model training, it does not use that many iterations. For example, [1] trains a MoE model with roughly 35K steps with 1.2T data. In this setup, Muon does show a clear advantages over AdamW.\n\nThird, the current setup does not consider severe over-train regime. For 124M model, only $\\approx 6.7$x overtrain is considered. It would be good to test one sever over-train regime (like 10x?) with relatively short training iterations (within 10K steps).\n\nLast but not least, I wonder is it possible that some optimizers just use a larger effective learning rate? Therefore, without fine-grained tuning of lr, we may have different conclusions. One possible solution is to align their RMS norm. For an optimizer output $\\Delta W$, we can post-process it to be $\\frac{\\Delta W}{||\\Delta W||} * \\rho$, where $\\rho$ is some target norm. This eliminates the different effective learning rate. Will this change the claims?"}, "questions": {"value": "My questions are in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DGi5oru9bq", "forum": "Jw7khYzYzl", "replyto": "Jw7khYzYzl", "signatures": ["ICLR.cc/2026/Conference/Submission13372/Reviewer_mntc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13372/Reviewer_mntc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755857483, "cdate": 1761755857483, "tmdate": 1762924013256, "mdate": 1762924013256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}