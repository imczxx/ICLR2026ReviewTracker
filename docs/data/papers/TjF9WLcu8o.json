{"id": "TjF9WLcu8o", "number": 25599, "cdate": 1758369381874, "mdate": 1759896713789, "content": {"title": "Contrastive-Online-Meta (COM): A Dynamic Adaptation Mechanism for Instruction-Tuned CodeLLMs", "abstract": "We propose Contrastive-Online-Meta (COM), a dynamic adaptation framework for instruction-tuned CodeLLMs that coefficients to the issues of catastrophic forgetting and noisy feedback at the time of deployment. The framework combines contrastive pre-training and online meta-learning to separate the task-invariant representation learning and fast adaptation, which helps preserve core programming knowledge while achieving real-time adaptation. A contrastive pre-training module takes a first step at clustering semantically similar instructions and unionizing dissimilar ones, to guarantee its robustness to task variations. During inference, an online meta-learner takes pairs of instruction-feedback streaming and does a light-weight gradient-based update to meta-parameters, which dynamically adjust the model behavior in a way that does not destabilize the pre-trained behavior-effective thing. Furthermore, the dynamic memory buffer simply retains coherence with recent interactions by deriving pairs stored in the buffer for the sake of contrastive match. Unlike monolithic fine-tuning or prompt engineering, COM explicitly separates the processes of representation learning and adaptation, hence avoiding forgetting and overfitting. Experiments using benchmark datasets show that the framework has a better capacity for adaptation efficiency and task generalization than static and incremental tuning baselines. The proposed method fills in the missing link between the offline pre-training and the online accelerated deployment, which provides a scalable solution to real-world code generation systems that require continuous learning. And, its modular nature also supports integration with existing CodeLLMs, which makes it practical for different programming assistance scenarios.", "tldr": "", "keywords": ["Instruction-Tuned CodeLLMs"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4de9e49419c7f6d5b1e39417896aecd9ce88ec85.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Contrastive-Online-Meta, a new dynamic tuning method for instruction-tuned CodeLLMs. COM mixes contrastive pre-training to learn stable, task-invariant code representations and online meta-learning to adapt quickly to new instruction-feedback streams. It also uses a memory buffer to keep recent context and prevent forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and clearly structured, making it easy to follow despite the technical density.\n\n2. The proposed framework demonstrates a new combination of contrastive learning and online meta-learning, which is conceptually very interesting for adapting CodeLLM."}, "weaknesses": {"value": "1. This paper lacks experimental validation. It proposes a framework but provides no empirical results, ablation experiments, or comparisons to demonstrate its effectiveness. In the absence of experiments, claims about the efficiency or robustness of adaptation remain speculative.\n\n2. Its innovation is limited. The combination of contrastive learning and meta-learning has been previously studied. Beyond its application to CodeLLM, this paper does not clearly articulate its fundamental innovation.\n\n3. The technical approach appears incomplete, key details such as update frequency, computational cost, and data requirements are unclear, making reproducibility difficult."}, "questions": {"value": "1. Where are your experimental results?\n\n2. Can you explain the computational cost and update frequency of the online meta-learner? How feasible is it for real-time deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l3eItqPRW0", "forum": "TjF9WLcu8o", "replyto": "TjF9WLcu8o", "signatures": ["ICLR.cc/2026/Conference/Submission25599/Reviewer_Ww5e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25599/Reviewer_Ww5e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760853942832, "cdate": 1760853942832, "tmdate": 1762943490458, "mdate": 1762943490458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a dynamic adaptation framework, Contrastive-Online-Meta (COM), for instruction-tuned CodeLLMs. The framework combines contrastive pre-training and online meta-learning to separate the task-invariant representation learning and fast adaptation. The proposed method fills in the missing link between the offline pre-training and the online accelerated deployment."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. **New Approach**: This paper proposes a new dynamic adaptation framework for instruction-tuned CodeLLMs. Related work and background have been included to make this idea easy to understand.\n2. **Experimental Setup**: Experimental benchmarks and implementation details have been described and future work has been discussed."}, "weaknesses": {"value": "1. **Absence of Experimental Results**: The most critical issue is the lack of empirical results within the main text. Without this evidence, it is impossible for readers to verify the effectiveness of the proposed idea and validate the claims made by the authors.\n2. **Insufficient In-depth Analysis**: The paper would be substantially strengthened by including further analysis like ablation study and case study.\n3. **Limited Discussion of Impact**: The paper's contribution is currently undersold due to an overemphasis on engineering details. The discussion lacks a thorough analysis of the idea's broader significance and potential long-term impact."}, "questions": {"value": "This paper seems unfinished. I would suggest authors to add sufficient empirical results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fbu7sWzZdg", "forum": "TjF9WLcu8o", "replyto": "TjF9WLcu8o", "signatures": ["ICLR.cc/2026/Conference/Submission25599/Reviewer_5ypu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25599/Reviewer_5ypu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897917277, "cdate": 1761897917277, "tmdate": 1762943490289, "mdate": 1762943490289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Contrastive-Online-Meta (COM), a deployment-time adaptation framework for instruction-tuned CodeLLMs. COM combines (i) contrastive pre-training of an instruction encoder to learn task-invariant representations and (ii) an online meta-learner that updates a small set of parameters per interaction with an ℓ2 drift penalty; the base CodeLLM remains frozen. A FIFO dynamic memory buffer supplies recent interactions to compute an auxiliary contrastive loss that encourages temporal consistency. Additional stabilizers include a projection-space temporal regularizer and spectral normalization of the meta-learner.  \nFor evaluation, the manuscript describes datasets (CodeAlpaca-20k; a custom sequential benchmark “StreamCode”; and CrossLang-Eval), baselines (SFT, ER, MIT, CPT), and metrics (Adaptation Accuracy, Forgetting Rate, Generalization Gap, Update Efficiency), and provides implementation details (frozen CodeGen-16B; 6-layer encoder; 2-layer MLP; 5k buffer; τ=0.1; α=1e-4; 8×A100). However, no quantitative results are reported in the text; the only figure shown is a system diagram of the framework. The paper nonetheless claims performance gains over baselines without presenting corresponding tables or plots."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "It proposes the COM method—combining contrastive pre-training with online meta-learning, plus a dynamic memory buffer, to decouple task-invariant representation learning from fast adaptation for deployment-time CodeLLM updates."}, "weaknesses": {"value": "This is an unfinished submission. It reports no quantitative results—no tables, no plots—only a high-level system diagram. The “evaluation” section merely lists datasets, baselines, metrics, and implementation settings but provides zero numbers. Yet the manuscript still asserts gains (e.g., “3–5× fewer updates,” “12–18% on unseen languages”) without evidence. In its current form, it is not ready for peer review and should not have been submitted."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "R5HYMn8xnF", "forum": "TjF9WLcu8o", "replyto": "TjF9WLcu8o", "signatures": ["ICLR.cc/2026/Conference/Submission25599/Reviewer_6CyU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25599/Reviewer_6CyU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973703068, "cdate": 1761973703068, "tmdate": 1762943489539, "mdate": 1762943489539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Contrastive-Online-Meta (COM), a framework designed to enable real-time adaptation of instruction-tuned CodeLLMs while mitigating catastrophic forgetting. The approach separates representation learning and fast adaptation, allowing efficient updates without destabilizing the base model."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The work aims to address a real deployment issue: CodeLLMs need to adapt continuously while preserving prior knowledge."}, "weaknesses": {"value": "1. The narrative is unclear and hard to follow. \n\n2. Descriptions of methods are confusing and lack clarity. \n\n3. Missing experimental results and lack of enough experimental details. \n\n4. Many claims in the paper such as good performance of the proposed method lack support."}, "questions": {"value": "The description of the pre-training procedure lacks clarity. Could the authors elaborate further, perhaps by including an algorithm outline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bu2OBY6Nl5", "forum": "TjF9WLcu8o", "replyto": "TjF9WLcu8o", "signatures": ["ICLR.cc/2026/Conference/Submission25599/Reviewer_Nndz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25599/Reviewer_Nndz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986535275, "cdate": 1761986535275, "tmdate": 1762943489099, "mdate": 1762943489099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}