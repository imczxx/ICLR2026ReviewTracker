{"id": "y39XbEp1vK", "number": 11810, "cdate": 1758203996084, "mdate": 1759897553368, "content": {"title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Model", "abstract": "Advancements in diffusion models have significantly improved video quality, directing attention to fine-grained controllability. However, many existing methods depend on fine-tuning large-scale video models for specific tasks, which becomes increasingly impractical as model sizes continue to grow. In this work, we present Frame Guidance, a training-free guidance for controllable video generation based on frame-level signals, such as keyframes, style reference images, sketches, or depth maps. By applying guidance to only a few selected frames, Frame Guidance can steer the generation of the entire video, resulting in a temporally coherent controlled video. To enable training-free guidance on large-scale video models, we propose a simple latent processing method that dramatically reduces memory usage, and apply a novel latent optimization strategy designed for globally coherent video generation. Frame Guidance enables effective control across diverse tasks, including keyframe guidance, stylization, and looping, without any training, and is compatible with any models. Experimental results show that Frame Guidance can produce high-quality controlled videos for a wide range of tasks and input signals.", "tldr": "We present Frame Guidance, a training-free framework that supports diverse control tasks using frame-level signals.", "keywords": ["controllable video generation", "training-free guidance", "video diffusion models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/417e3f0dbc3ff6b94f64a2bff472b17726708463.pdf", "supplementary_material": "/attachment/afef7eb047972865d8cc8e1db34815378286d1f5.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Frame Guidance, a novel training-free and model-agnostic framework for controlling video generation in pre-trained Video Diffusion Models (VDMs). The method aims to provide general-purpose, frame-level control using diverse input signals like keyframes, style images, depth maps, or sketches. By applying guidance to only selected frames, it steers the entire video generation process towards temporally coherent results. Key technical contributions include \"Latent Slicing,\" which leverages observed temporal locality in VDM latents (specifically CausalVAE) to reduce memory usage during gradient computation, and \"Video Latent Optimization\" (VLO), a hybrid optimization strategy balancing early deterministic updates for global layout and later stochastic updates for detail refinement.\n\nHowever, I think the evaluation seems a little bit out of dated. More recent Vbench and Vbench2.0 evaluation results are needed to further verify the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  Training-Free and Model-Agnostic Control: The paper proposes Frame Guidance, a novel training-free approach for controlling video generation in pre-trained Video Diffusion Models (VDMs). This is a significant advantage as it avoids the need for costly fine-tuning for specific tasks or models, making it broadly applicable to various existing and future VDMs. The model-agnostic nature is well-demonstrated across different architectures.\n\n2.  General-Purpose Frame-Level Guidance: The method offers a unified framework for diverse frame-level control tasks. It supports a wide range of input signals beyond simple keyframes, including style reference images, depth maps, sketches, and color blocks, showcasing its versatility for different creative applications (e.g., keyframe guidance, stylization, looping, structure guidance).\n\n3.  Efficient Guidance Strategy for Large Models: The paper introduces two key technical contributions, Latent Slicing and Video Latent Optimization (VLO), to enable efficient training-free guidance on large-scale VDMs. Latent Slicing cleverly exploits the observed temporal locality in CausalVAE latents to dramatically reduce memory consumption during gradient computation. VLO provides a tailored optimization strategy for videos, balancing global layout coherence (early deterministic updates) with detail refinement (later stochastic updates)."}, "weaknesses": {"value": "The evaluation metrics used (FID, FVD) are insufficient for this task. While I am not an expert in training-free video editing, relying solely on FID/FVD seems questionable for evaluating frame-level control. These metrics may not adequately capture temporal consistency, fine-grained adherence to control signals, or motion quality. The paper should ideally include results from more comprehensive video generation benchmarks like VBench to provide a more convincing assessment. (If the authors can supplement the evaluation with VBench results, I will raise my score based on discussions with other reviewers.)"}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ng8qvDtoi5", "forum": "y39XbEp1vK", "replyto": "y39XbEp1vK", "signatures": ["ICLR.cc/2026/Conference/Submission11810/Reviewer_esr1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11810/Reviewer_esr1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761135604516, "cdate": 1761135604516, "tmdate": 1762922832288, "mdate": 1762922832288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This passage introduces Frame Guidance, a training-free method for controllable video generation that uses frame-level signals like keyframes, style references, or depth maps applied to selected frames to guide entire videos while maintaining temporal coherence. To enable this approach on large-scale models, the authors propose a memory-efficient latent processing method and a novel optimization strategy for globally coherent generation. The method works across diverse tasks including keyframe guidance, stylization, and looping, is compatible with any video model, and produces high-quality controlled videos without requiring training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a training-free guidance method that eliminates the need for fine-tuning large-scale video models for controllable generation. \n2. This paper introduces a memory-efficient latent processing technique and optimization strategy that enables practical application on large-scale models while ensuring temporal coherence. \n3. This paper demonstrates versatility across diverse control tasks including keyframes, stylization, and looping, with compatibility across any video generation model."}, "weaknesses": {"value": "1. The authors mention in the limitations that \"The computational cost of guidance sampling is higher than that of training-based methods.\" Please provide timing comparisons in the rebuttal to better benchmark the method against alternatives.\n2. This paper focuses on training-free controllable video generation, but the related works section lacks discussion of previous relevant methods, such as Tune-A-Video[1], Text2Video-Zero[2], and ControlVideo[3].\n3. Can the proposed method be extended to control video generation using canny edges, depth maps, and other modalities?\n\n[1] \"Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation.\" In ICCV 2023\n[2] \"Text2video-zero: Text-to-image diffusion models are zero-shot video generators.\" In ICCV 2023\n[3] \"ControlVideo: Training-free Controllable Text-to-Video Generation.\" In ICLR 2024."}, "questions": {"value": "Please refer to above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8wTVgHIBDI", "forum": "y39XbEp1vK", "replyto": "y39XbEp1vK", "signatures": ["ICLR.cc/2026/Conference/Submission11810/Reviewer_TPuU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11810/Reviewer_TPuU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827806872, "cdate": 1761827806872, "tmdate": 1762922831766, "mdate": 1762922831766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Frame Guidance, a training-free framework for controllable video generation using video diffusion models. It enables frame-level control through latent optimization and guidance, supporting tasks like stylization, looping, and keyframe-based generation. By operating in the latent space, the method reduces memory usage while enhancing global coherence across frames. Its plug-and-play compatibility with existing video diffusion models allows versatile application across tasks without retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: While the paper builds on existing concepts like latent optimization, its integration into a training-free video generation framework with frame-level control is novel and practical.\n2. Quality: The method is well-engineered and demonstrates compatibility with multiple video diffusion backbones. The results show improved coherence and controllability across tasks.\n3. Clarity: The paper is generally well-written and structured. The methodology is explained clearly, and the figures help illustrate the effects of guidance.\n4. Significance: Training-free methods are increasingly important for accessibility and scalability. Frame Guidance contributes meaningfully to this direction by enabling flexible control without retraining, which is valuable for both research and real-world applications."}, "weaknesses": {"value": "1. Limited Novelty: The core techniques (e.g., latent optimization and latent sliding) are adaptations of existing methods. The novelty lies more in the integration and application than in the underlying algorithms. The proposed did show improved performance but with basis from its underlying video diffusion model.\n2. The latent slicing strategy, while memory-efficient, may be sensitive to frame rate and motion magnitude. In cases of large motion or occlusion, it may fail to maintain coherence. No quantitative evaluation is provided to assess this. The meaning of “N-length latent” in Fig.19 is unclear, and it is not specified which frames are anchored for each latent segment. Some frames show object disappearance or occlusion, raising questions about how they are decoded reliably for other cases.\nCompared to more robust video compression models like LTX-Video use 8 temporal compression and 32 spatial compression, the slicing strategy feels more like a heuristic trick than a principled solution.\n3. The looping task lacks detail on how the method avoids generating identical frames, which is critical for realistic looping.\n4. The time-travel strategy lacks runtime analysis, and the pseudo-code appears incorrect or incomplete.\n5. The paper does not include a discussion of failure cases."}, "questions": {"value": "1. Are consistent seeds used in Fig. 6 and 7? In Fig. 7, can the method improve style alignment without altering content? Was the same seed used? \n2. What is the runtime overhead of the time-travel strategy? \n3. How does the method handle large motion or occlusion? Is the slicing strategy sensitive to motion scale and object scale?\n4. How does the method prevent generating identical frames in looped videos?\n5. What does “N-length latent” mean in Figure 19? Which frames are anchored for each latent segment? How are occluded or missing objects decoded reliably?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e9GTcAG59M", "forum": "y39XbEp1vK", "replyto": "y39XbEp1vK", "signatures": ["ICLR.cc/2026/Conference/Submission11810/Reviewer_qnqQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11810/Reviewer_qnqQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957831636, "cdate": 1761957831636, "tmdate": 1762922831342, "mdate": 1762922831342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Frame Guidance - a novel training-free guidance framework for controllable video generation using frame-level signals such as keyframes, style references, sketches, or depth maps. The authors claim three primary contributions: (1) a model-agnostic, training-free approach compatible with diverse video diffusion models (VDMs); (2) \"latent slicing,\" a memory-efficient technique for partial decoding of video latents based on temporal locality in CausalVAEs; and (3) \"video latent optimization (VLO),\" a hybrid strategy combining deterministic updates in early denoising steps for global coherence and stochastic updates later for detail refinement. The method is demonstrated across tasks like keyframe-guided generation, stylization, and looping, achieving competitive results without fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Among the strong points of the paper I would focus on the following ones:\n1.  Introduces a general-purpose, training-free framework for frame-level control, addressing a gap between task-specific training-free methods and general-purpose fine-tuning approaches.\n2. Rigorous experiments across multiple VDMs (e.g., CogVideoX, Wan, SVD) and tasks, supported by human evaluations and metrics (FID, FVD, CLIP scores).\n3. Latent slicing reduces GPU memory usage by up to 60×, enabling application to large-scale models.\n4. Detailed algorithms, hyperparameters, and ablation studies are provided, including adaptations for flow-matching models.\n\nThe method’s generality across models (CogVideoX, Wan, SVD) and tasks (keyframing, stylization, looping) demonstrates its broad applicability. Human evaluations and quantitative metrics show superior or competitive performance against training-based baselines. The memory efficiency achieved via latent slicing is critical for scaling to modern VDMs. The hybrid VLO strategy effectively balances layout coherence and detail preservation, addressing limitations of prior training-free methods. These contributions are valuable for the community, enabling accessible and flexible video control without costly fine-tuning."}, "weaknesses": {"value": "The weak points are:\n1. Guidance increases inference time by 2–4×, limiting real-time applicability.\n2. Performance is constrained by the base VDM’s capabilities, especially for dynamic or fine-grained content.\n3. While latent slicing is justified via experiments on CausalVAE, broader validation across architectures is limited.\n4. Although multi-condition guidance is shown, combining losses for complex controls (e.g., motion + style) is not deeply explored.\n5. As you know, one of the main problems in video generation is the object state change, like melting ice, etc. There is a number of training-free solutions for such tasks, e.g., \"State & Image Guidance: Teaching Old Text-to-Video Diffusion Models New Tricks\" (https://openreview.net/forum?id=zkGxROm7D3). Deeper comparison would be a benefit of the proposed paper."}, "questions": {"value": "1. How does the temporal locality of CausalVAE generalize to other VAE architectures or non-causal latent spaces?\n2. Could the gradient propagation analysis (Fig. 4c) be extended to quantify temporal coherence, e.g., via optical flow consistency?\n3. For multi-condition guidance, how are conflicting losses (e.g., style vs. depth) balanced? Is there a risk of gradient domination?\n4. Have you explored adaptive strategies for selecting guided frames (e.g., based on motion complexity) to further reduce computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uUMJdgKkaQ", "forum": "y39XbEp1vK", "replyto": "y39XbEp1vK", "signatures": ["ICLR.cc/2026/Conference/Submission11810/Reviewer_3A8P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11810/Reviewer_3A8P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762195198232, "cdate": 1762195198232, "tmdate": 1762922830155, "mdate": 1762922830155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}