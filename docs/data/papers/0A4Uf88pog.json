{"id": "0A4Uf88pog", "number": 13925, "cdate": 1758225274549, "mdate": 1759897403022, "content": {"title": "VERINA: Benchmarking Verifiable Code Generation", "abstract": "Large language models (LLMs) are increasingly integrated in software development, but ensuring correctness in LLM-generated code remains challenging and often requires costly manual review. Verifiable code generation---jointly generating code, specifications, and proofs of code-specification alignment---offers a promising path to address this limitation and further unleash LLMs' benefits in coding. Yet, there exists a significant gap in evaluation: current benchmarks often focus on only individual components rather than providing a holistic evaluation framework of all tasks. In this paper, we introduce VERINA (Verifiable Code Generation Arena), a high-quality benchmark enabling a comprehensive and modular evaluation of code, specification, and proof generation as well as their compositions. VERINA consists of 189 manually curated coding tasks in Lean, with detailed problem descriptions, reference implementations, formal specifications, and extensive test suites. Our extensive evaluation of state-of-the-art LLMs reveals significant challenges in verifiable code generation, especially in proof generation, underscoring the need for improving LLM-based theorem provers in verification domains.\nThe best model, OpenAI o4-mini, achieves a 61.4% code correctness rate, 51.0% for specification soundness and completeness, and a mere 3.6% proof success rate (based on one trial per task).\nWe hope VERINA will catalyze progress in verifiable code generation by providing a rigorous and comprehensive benchmark.", "tldr": "", "keywords": ["code generation", "formal verification", "verifiable code generation", "AI for math", "theorem proving", "AI for code"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4006dc82db471198ed3d9f554b01e5c1b118c85d.pdf", "supplementary_material": "/attachment/8a4f0137ad2c8b021a2908eea1a659d6e34d6d0f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces VERINA (Verifiable Code Generation Arena), a benchmark comprising 189 manually curated programming tasks in Lean for evaluating end-to-end verifiable code generation. The benchmark assesses three foundational tasks—code generation (CodeGen), specification generation (SpecGen), and proof generation (ProofGen)—along with their flexible combinations. The best model (o4-mini) achieves 61.4% code correctness, 51.0% specification soundness/completeness, and only 3.6% proof success."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear Presentation: The presentation is well-executed.\n- Extensive Task Support: VERINA comprehensively supports all three core tasks: CodeGen, SpecGen, and ProofGen, utilizing a modular and compositional approach.\n- Thorough Experimental Evaluation: The evaluation is extensive, involving 8 general-purpose and 3 specialized LLMs. It includes detailed ablations on the impact of contextual information, iterative refinement, and a breakdown of difficulty levels."}, "weaknesses": {"value": "- Limited Language/Tool Coverage: The reliance on Lean significantly restricts the generalizability of the findings. The paper fails to justify how Lean-specific results would translate to other widely used verification ecosystems like Dafny, Verus, or Frama-C, which often feature different proof automation capabilities and specification styles.\n- Unclear Justification for Lean in Code Verification: Lean is primarily known for mathematical verification rather than production code verification. The paper does not adequately explain its choice of Lean for this context.\n- Insufficient Analysis of Proof Generation Failures: Despite proof generation being a major hurdle (with a mere 3.6% success rate, the lowest among all tasks), the paper offers minimal systematic analysis of the underlying causes of failure or actionable insights.\n- Task Simplicity: The tasks appear to be overly simplistic. The paper should provide information on the average number of branches per task."}, "questions": {"value": "- What evidence demonstrates Lean is used for production code verification (not mathematics)? How do you justify that insights from ITP-based verification in Lean generalize to ATP-based systems (Dafny, Verus, Frama-C) that dominate production use?\n- Why not include at least one ATP-based system for comparison to validate generalizability?\n- Can you provide systematic breakdown of proof failure types (wrong strategy, missing lemmas, syntax errors, etc.) with percentages?\n- How do LLM-generated proof structures differ from the 46 human-written ground truth proofs? Are the remaining tasks really provable in Lean?\n- What is the actual end-to-end success rate (NL → verified code) for each model?\n- What are the average branches of the task?\n\n\n\nComments:\n- Incorrect citations: some paper references are incorrect, like SAFE, AutoVerus, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Un0Re2smcs", "forum": "0A4Uf88pog", "replyto": "0A4Uf88pog", "signatures": ["ICLR.cc/2026/Conference/Submission13925/Reviewer_a4Ez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13925/Reviewer_a4Ez"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832999685, "cdate": 1761832999685, "tmdate": 1762924431218, "mdate": 1762924431218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VERINA (Verifiable Code Generation Arena), a new benchmark designed to evaluate the ability of Large Language Models (LLMs) to perform verifiable code generation. The authors define this as the joint task of generating code, formal specifications, and proofs that the code aligns with the specifications. The benchmark consists of 189 manually curated programming tasks in the Lean language, each with detailed descriptions, reference implementations, formal specifications, and comprehensive test suites. The authors evaluated state-of-the-art LLMs on VERINA and found that these models face significant challenges, especially in proof generation. The paper concludes that VERINA provides a rigorous framework for measuring and advancing LLM capabilities in producing formally verified software."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1). VERINA covers CodeGen, SpecGen, and ProofGen and allows their compositions, which matches realistic verification workflows rather than isolated tasks.\n\n2). The formal treatment of soundness and completeness for pre- and post-conditions is crisp. The two-stage evaluator (prover first, then property-based testing) gives a practical way to score imperfectly decidable relationships and communicates uncertainty via “R might hold” and error bars."}, "weaknesses": {"value": "1). Modest scale and representativeness are not convincing. With 189 single-file tasks, the benchmark remains small for broad performance claims. The focus on stand-alone snippets under-represents the multi-module, dependency-heavy settings that matter for real systems. The paper acknowledges this limitation but it still constrains the practical impact.\n\n2). The “100% coverage” is established on Python references rather than the Lean artifacts under evaluation, due to missing Lean tooling. This creates a fidelity gap between what is instrumented and what is graded. Mutation analysis or Lean-side coverage proxies would reduce this gap.\n\n3). Sources include MBPP, LeetCode, and LiveCodeBench. The paper notes risk but does not present a systematic decontamination audit or overlap analysis against model training corpora. This could inflate performance on familiar patterns. Besides, these sources from popular platforms are almost certainly present in the training corpora of the state-of-the-art LLMs evaluated in the study.\n\n4). This contamination risk in  3) may directly undermine the evaluation of the \"CodeGen\" task. The paper reports that the best model, 04-mini, achieves a 61.4% code correctness rate. This relatively high score—especially when contrasted with the dismal 3.6% proof success rate—is ambiguous. It may not reflect the model's genuine ability to generate correct Lean code from a description, but rather its ability to recall a memorized algorithm and \"translate\" it into the required Lean syntax.\n\n5). The central premise of VERINA is to provide a \"holistic evaluation framework\" for the joint task of generating code, specifications, and proofs. However, if the CodeGen task is compromised by memorization, the benchmark is no longer evaluating a \"generation\" pipeline. Instead, it may be evaluating a disjointed task: recalling code, and then attempting to generate specifications and proofs for that recalled code. This is a fundamentally different and less challenging task than true end-to-end generation from a novel problem description, which is what the paper claims to benchmark. Also, the contamination risk makes the paper's main findings difficult to interpret. The key result:  ProofGen is the primary bottleneck, is called into question. Is proof generation difficult in general, or is it difficult when the associated code is recalled from memory? The latter is a much weaker claim and provides less insight. The benchmark fails to isolate whether the models are failing at reasoning or simply failing to formally verify a solution they do not understand but have memorized.\n\n6). The evaluator may return “R might hold” when proofs are inconclusive and tests pass. This is reasonable, yet it weakens the claim of verifiable benchmarking; readers would benefit from a quantitative breakdown of proof-decided vs. test-backed cases and sensitivity to the testing budget.\n\nIn general, the proposed SpecGen metric and the compositional framing are valuable. However, the dataset scale is modest, coverage is measured on proxies, “R might hold” weakens formal guarantees without a thorough sensitivity analysis, and contamination controls are limited. The high probability of data contamination is critical. It strikes at the construct validity of the benchmark, making it unclear what is actually being measured. For a paper whose primary contribution is a new evaluation benchmark, this ambiguity regarding what it validly measures renders its conclusions unreliable and its utility to the community questionable. Addressing these issues may lift the work to a acceptance."}, "questions": {"value": "1). How sensitive are “R might hold” outcomes to the property-based testing budget and random seeds? Please report the split of “proved vs. tested vs. unknown,” and provide seed-averaged results.\n\n2). Beyond Python, did you attempt Lean-level mutation testing or adversarial test synthesis to ensure that the Lean implementations are exercised comparably to the Python references?\n\n3). Can you provide n-gram or signature overlap checks between VERINA items and public code corpora likely used for pretraining? A filtered subset with low overlap would help calibrate headline metrics.\n\n4). What is the marginal utility curve of iterative refinement (cost vs. accuracy) and how does Copra’s benefit scale across difficulty strata? A budget-normalized comparison would help practitioners.\n\n5). Were prompts fully identical across models for each task? If not, please quantify the effect of template differences on pass@1 and pass@k.\n\n6). Could you report a control where the code is summarized into behavior-only comments or contracts before SpecGen to test whether verbosity or implementation detail confounds the result?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KFocKYbhDX", "forum": "0A4Uf88pog", "replyto": "0A4Uf88pog", "signatures": ["ICLR.cc/2026/Conference/Submission13925/Reviewer_6DqP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13925/Reviewer_6DqP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876747648, "cdate": 1761876747648, "tmdate": 1762924430761, "mdate": 1762924430761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VERINA, a benchmark for verifiable code generation built on the Lean 4 proof assistant. VERINA provides a framework for evaluating three core tasks: code generation (CodeGen), formal specification generation (SpecGen), and proof generation (ProofGen), as well as their modular compositions. The benchmark consists of 189 manually curated problems, each with a natural-language description, reference implementation, formal specification, and a test suite. The authors propose a hybrid evaluation pipeline for SpecGen that combines automated proof attempts with property-based testing. An extensive evaluation of general-purpose LLMs and specialized theorem provers reveals that ProofGen is the primary bottleneck in the end-to-end pipeline, with even iterative refinement showing limited success."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper makes a well-scoped and valuable contribution by introducing a unified benchmark for code, specification, and proof generation.\n2. The proposed hybrid evaluation metric for SpecGen is a practical approach to a difficult problem, leveraging both formal proving and extensive testing to assess specification quality.\n3. The paper provides a thorough experimental evaluation across a diverse set of models, including general-purpose LLMs, specialized provers, and an agentic baseline, offering a snapshot of current capabilities."}, "weaknesses": {"value": "1. Gaps in Evaluation Metrics: The SpecGen evaluation logic is potentially not perfect. It uses negative test cases to evaluate both the soundness and completeness of different specification components, but does not appear to distinguish between tests that violate the pre-condition versus those that violate the post-condition. This can lead to noisy or incorrect soundness/completeness judgments.\n2. The paper's claims of test suite adequacy rely on coverage metrics from Python reference implementations, as Lean lacks mature coverage tooling. This is a significant gap, as coverage in one language does not guarantee equivalent coverage in a translated Lean implementation. Can authors discuss more about it?\n3. The evaluation lacks a publicly available frontier reasoning model (e.g., OpenAI's o3 or GPT-5, Google's gemini-2.5-pro or Claude-4/4.1 opus), while it currently claims o4-mini to be a frontier model. While the conclusions may not be wildly different, given this is a benchmark paper, evaluating frontier models remains essential.\n4. While LLM-based provers are part of the SpecGen evaluator, their extremely low success rates (<4%) and inherent non-determinism make their inclusion in the primary metric questionable. Some manual evaluations may either strengthen the claim or clarify the limitations.\n\n\n**Other Minor Weaknesses**\n1. Despite the emphasis on compositionality, the paper omits results for the full end-to-end task (Description → Code + Spec + Proof). Reporting this, along with the success rate for generated code satisfying generated specifications, wuld further strengthen the paper.\n2. Insufficient Failure Analysis: The qualitative case studies of failure modes are useful, but the paper would be significantly strengthened by a quantitative breakdown of error categories (e.g., syntax errors, API hallucinations, tactic misuse, logical errors). Correlating these failure types with the dataset origin (VERINA-A vs. VERINA-B) would provide more actionable insights for future research.\n3. Limited Scope and Generalizability: The benchmark's reliance on Lean and the Interactive Theorem Proving (ITP) paradigm necessarily limits the external validity of its findings to other verification ecosystems. Furthermore, the paper could characterize the classes of real-world programs (e.g., those with effects, IO, or concurrency) that are out of scope for the current benchmark design.\n4. Presentation and Framing Improvements: a.) > \"The high-quality samples and robust metrics\"  in the introduction. Robust metrics have not been defined yet, or clarified why exactly they are robust. b.) \"top-performing general-purpose LLM\" is referred to o4-mini. c.) \"ITPs support constructing proofs with explicit intermediate steps. This visibility enables LLMs to diagnose errors, learn from unsuccessful steps, and iteratively refine their proofs,\" in related work. However, iterative refinement is also possible with ATP."}, "questions": {"value": "Please see the Weaknesses above. In addition:\n\n1. The paper states the SpecGen evaluator compares outcomes for R and ~R to select the \"more accurate\" result. Can you please clarify what \"more accurate\" means here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RoUiS6l85X", "forum": "0A4Uf88pog", "replyto": "0A4Uf88pog", "signatures": ["ICLR.cc/2026/Conference/Submission13925/Reviewer_sru5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13925/Reviewer_sru5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998720334, "cdate": 1761998720334, "tmdate": 1762924430351, "mdate": 1762924430351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}