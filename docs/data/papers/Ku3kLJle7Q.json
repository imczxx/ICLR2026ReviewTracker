{"id": "Ku3kLJle7Q", "number": 13081, "cdate": 1758213401311, "mdate": 1759897466794, "content": {"title": "Self-Supervised Evolution Operator Learning for High-Dimensional Dynamical Systems", "abstract": "We introduce an end-to-end approach to learn the evolution operators of large-scale non-linear dynamical systems, such as those describing complex natural phenomena. Evolution operators are particularly well-suited for analyzing systems that exhibit spatio-temporal patterns and have become a key analytical tool across various scientific communities. As terabyte-scale weather datasets and simulation tools capable of running millions of molecular dynamics steps per day are becoming commodities, our approach provides an effective tool to make sense of them from a data-driven perspective. The core of it lies in a remarkable connection between self-supervised representation learning methods and the recently established learning theory of evolution operators. We deploy our approach across multiple scientific domains: explaining the folding dynamics of small proteins, the binding process of drug-like molecules in host sites, and autonomously finding patterns in climate data. Our code is available open-source at: https://anonymous.4open.science/r/encoderops-5F67.", "tldr": "", "keywords": ["Operator", "Koopman", "Transfer", "Contrastive", "Self-Supervised", "DMD", "Modes", "Dynamics", "ENSO", "Climate", "Molecular Dynamics", "Protein", "TICA", "Slow Modes"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff4641edd5158f952cc9ff147b1f6df487c99181.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an end-to-end deep learning framework for learning evolution operators of high-dimensional dynamical systems. It establishes a novel connection between self-supervised contrastive learning and operator learning theory.\n\nThrough experiments, the paper demonstrates that the proposed method can extract meaningful dynamical modes that reflect physically relevant processes such as folding-to-unfolding transition, molecular binding, and climate oscillation. It shows that these learned representations can be reused effectively across related systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper established a novel and rigorous theoretical connection between evolution operator learning and self-supervised contrastive learning. A major contribution is the scalability of the proposed framework to high-dimensional system. Empirically, the model demonstrates stable training and meaningful spectral decomposition in different scientific domians, even in terabyte-scale dataset."}, "weaknesses": {"value": "(1) The finite-sample analysis is not provided;\n(2) The paper lacks a clear comparison against other (Koopman) operator-learning approaches (e.g., kernel EDMD, Neural Operator frameworks)."}, "questions": {"value": "(1) Your Lemmas assume the evolution operator $E$ is Hilbert-Schmidt, yet you acknowledge this is often violated by deterministic dynamical systems. The Lorenz '63 system in your experiments is deterministic (or near-deterministic). So, what happens to the approximation quality when $E$ is not Hilbert-Schmidt?\n(2) In the deterministic case, the Koopman operator acts on delta functions. How does your bilinear model $\\langle \\phi(x_t), P\\phi(x_{t+1}) \\rangle$ approximate this structure?\n(3) How does the approximation error related to the latent dimension?\n(4) In the climate experiment, how does your method perform if one applies a kernel-based approach directly to the raw 29,040-dimensional input features, instead of using the learned 128-dimensional embedding? Such a comparison would clarify if standard dimensionality reduction techniques could achieve similar results.\n(5) Could you discuss any scenarios or system characteristics under which the learned representations may fail to transfer effectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lpPBm2tfYW", "forum": "Ku3kLJle7Q", "replyto": "Ku3kLJle7Q", "signatures": ["ICLR.cc/2026/Conference/Submission13081/Reviewer_eZUJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13081/Reviewer_eZUJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760782599559, "cdate": 1760782599559, "tmdate": 1762923809436, "mdate": 1762923809436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an end-to-end, self-supervised approach to learning evolution operators for high-dimensional nonlinear dynamical systems, with a particular focus on scientific domains such as molecular dynamics and climate modeling. The authors connect contrastive, self-supervised representation learning techniques with the operator-theoretic framework, providing theoretical grounds for their approach and demonstrating its equivalence to established least-squares operator learning methods. Comprehensive experiments are reported on protein folding, molecular binding, and climate data, with claims of scalability, interpretability via spectral decomposition, and transferability of learned representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors make a deliberate connection between self-supervised contrastive learning and classical operator-theoretic approaches (notably, the least-squares estimation of evolution operators). This is not only highlighted at an intuitive level but also underpinned with theoretical results.\n- The paper supports claims with diverse, high-dimensional benchmarks, spanning molecular simulations (protein folding, ligand binding) and challenging climate datasets.\n- The spectral decomposition of the learned operator yields interpretable, physically relevant modes, e.g., hydrogen bonding patterns in protein folding."}, "weaknesses": {"value": "- While the core mathematical exposition is sound, there are places where notation could be clarified for easier accessibility:\n  - In Lemma 2 and Appendix A, the formula for the predictor $P_*$ involves both $C_X^{-1}$ and $C_Y^{-1}$, but the mapping from the loss gradient to the closed-form $P_*$ is sketched rather than fully elucidated.\n  - Equation (9) relates the action of E to the covariance of futures, but implementation choices for non-stationary or non-ergodic systems are not fully discussed.\n- The limitations section (Conclusion) briefly notes the Hilbert-Schmidt assumption and qualitative evaluation. However, broader issues are not meaningfully discussed. For example, the treatment of deterministic versus stochastic systems, failure cases when the operator is not Hilbert-Schmidt, and scalability bottlenecks in training or memory for extremely large state spaces are not addressed.\n- While the online/offline covariance ablation adds value, ablations for the choice of network architectures (GNN vs. CNN vs. MLP), size/sparsity of $P$, or impact of history/context window on model performance would strengthen the empirical narrative."}, "questions": {"value": "- Can the authors provide (or reference) quantitative metrics for spectral decomposition accuracy on real-world, high-dimensional datasets, or propose benchmarks for such evaluation? For example, can clustering purity or precision/recall be reported for folded/unfolded distinctions in protein folding, or event detection in climate data?\n- Can the authors report on representation transfer performance when the target system is more dissimilar to the source (e.g., transfer across molecular families, not just ligands, or different climate regimes)? What are the limits of transferability in practice?\n- In extremely large-scale systems, does online or EMA-based covariance computation pose bottlenecks or stability issues? Can sparse or approximate methods be used safely?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6F0WrYmkt5", "forum": "Ku3kLJle7Q", "replyto": "Ku3kLJle7Q", "signatures": ["ICLR.cc/2026/Conference/Submission13081/Reviewer_3ogW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13081/Reviewer_3ogW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638233537, "cdate": 1761638233537, "tmdate": 1762923808963, "mdate": 1762923808963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an encoder-only method for learning evolution operators of nonlinear dynamical systems by connecting self-supervised learning with operator theory. The approach is demonstrated across multiple domains, including protein folding, molecular binding, and climate pattern discovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical analysis provides a thorough and rigorous discussion of the paper's core claims.\n\n2. The experimental design is strengthened by the use of multi-disciplinary datasets, ensuring the scenarios are both diverse and representative.\n\n3. The integration of self-supervised representation learning with evolution operator theory represents a novel and promising research direction."}, "weaknesses": {"value": "1. When the target function *f* does not lie in the linear span of the encoder, it is unclear how the method should be adjusted. The paper lacks discussion on this pointâ€”for instance, whether simply increasing the embedding dimension would suffice to satisfy this assumption.\n\n2. The paper lacks practical guidance on how to choose the embedding dimension in real-world systems.\n\n3. The authors should more clearly distinguish their approach from existing methods that combine deep learning with Koopman theory or DMD, and better highlight what additional problems their method can solve."}, "questions": {"value": "1. How does your method handle cases where the target function *f* is not in the encoder's linear span?\n\n2. What is the key advantage of your method over existing Koopman or DMD-based approaches, and what new problem does it enable you to solve?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rRtI8Sqokc", "forum": "Ku3kLJle7Q", "replyto": "Ku3kLJle7Q", "signatures": ["ICLR.cc/2026/Conference/Submission13081/Reviewer_FVgy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13081/Reviewer_FVgy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991087655, "cdate": 1761991087655, "tmdate": 1762923808586, "mdate": 1762923808586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a self-supervised approach to learn the evolution operators, which characterize the temporal dynamics of complex stochastic/deterministic systems. Different from the forecasting or reconstruction type models, the method uses a contrastive self-supervised loss from the spectral contrastive learning to estimate the transfer operators and spectral decomposition directly from data.  The main contribution is to theoretically show the equivalence between the self-supervised loss and minimizing the operator regression objective, connecting the least-square estimator, VAMP-2 score, and HS norms. Experiments on three domains, including protein folding, molecular binding, and climate dynamics, show the effectiveness and generalization of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Theoretically explain the operator learning theory and self-supervised contrastive learning, which provides justification using contrastive objectives in scientific dynamic systems. \n2. The method avoids computationally expensive matrix inversions and is implemented with simple matrix multiplications and covariance updates, which are very suitable for large-scale high-dimensional data. \n3. The paper demonstrates the effectiveness through diverse and convincing experimental validation."}, "weaknesses": {"value": "1. Although the qualitative results, such as the eigenfunction visualizations, look good. The paper does not report any standardized quantitative metrics and comparisons with the baseline on some experiments. \n2. This paper is missing an ablation study that compares different encoder architectures or embedding dimensionalities. \n3. Although admitted by authors that the Hilbert-Schmidt assumption is often not held in deterministic systems. The paper lacks a discussion of how the performance will degrade when the assumption fails. ( Can be one of your ablation studies.)"}, "questions": {"value": "1. It would be a benefit to elaborate more on Equation 8. How can this be viewed as a contrastive learning paradigm? Should state more if positive and negative pairs are not explicitly defined."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pK0t8TheE1", "forum": "Ku3kLJle7Q", "replyto": "Ku3kLJle7Q", "signatures": ["ICLR.cc/2026/Conference/Submission13081/Reviewer_NFuM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13081/Reviewer_NFuM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762361845937, "cdate": 1762361845937, "tmdate": 1762923808214, "mdate": 1762923808214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}