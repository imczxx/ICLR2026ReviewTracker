{"id": "yResLmrVO1", "number": 18260, "cdate": 1758285727469, "mdate": 1759897115991, "content": {"title": "Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models", "abstract": "A safe and trustworthy use of Large Language Models (LLMs) requires an accurate expression of confidence in their answers. We propose a novel Reinforcement Learning approach that allows to directly fine-tune LLMs to express calibrated confidence estimates alongside their answers to factual questions. Our method optimizes a reward based on the logarithmic scoring rule, explicitly penalizing both over- and under-confidence. This encourages the model to align its confidence estimates with the actual predictive accuracy. The optimal policy under our reward design would result in perfectly calibrated confidence expressions. Unlike prior approaches that decouple confidence estimation from response generation, our method integrates confidence calibration seamlessly into the generative process of the LLM. Empirically, we demonstrate that models trained with our approach exhibit substantially improved calibration and generalize to unseen tasks without further fine-tuning, suggesting the emergence of general confidence awareness. We provide our training and evaluation code in the supplementary and will make it publicly available upon acceptance.", "tldr": "We propose Rewarding Doubt, an RL-based approach that models confidence estimation as a betting game, optimizing LLMs for calibrated confidence in factual answers.", "keywords": ["Confidence Calibration", "Uncertainty Estimation", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5069693dc13f947a7784a9a558def7cfba152f77.pdf", "supplementary_material": "/attachment/1f253721527f4d3de722abad005e507b64631d76.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies LLM calibration on Q&A. Their goal is to train the LLM to generate a predicted probability $\\hat{p}$ of correctness in addition to its answer to the question. To do so, they use RL on a reward function that depends both on the correctness of the answer and the estimated probability, using a standard logarithmic scoring rule. Essentially, if the LLM is wrong and reports a large $\\hat{p}$, or the LLM is correct and reports a small $\\hat{p}$, it is penalized. It is well-known from forecasting that the scoring rule is optimized by reporting the agent's true best estimate of the correctness probability (i.e., it is a \"proper\" scoring rule). The authors study several Q&A benchmarks and show the following:\n1. Their method is more effective in calibrating LLMs than existing methods.\n2. The AUROC of their method (whether it is effective at distinguishing correct answers from incorrect answers) outperforms existing methods. 3. Their method does not  affect the Q&A accuracy of the model (i.e., fraction of correct answers)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Calibration in LLMs is an important problem, and the authors justify it well. Related work is discussed well. I think the idea of RL post-training on a proper scoring rule to incentivize accurate confidence prediction in LLMs is very natural (arguably more natural than existing methods). The choice of datasets is appropriate, as is the choice of which existing methods the authors compare to. I appreciate that the authors tested their method on several LLMs to ensure that the results are robust across models. The presentation is generally good, although I am confused about one point, which I discuss below. I also appreciated the authors' societal impact statement."}, "weaknesses": {"value": "I have two main concerns:\n\n1. **Joint vs separate optimization of answer choice and confidence level**. Lines 279-284 say:\n\n> To calibrate and reward the model only on the confidences and not the answers we separate generation in two steps during training: Answer and confidence generation. Answers are generated first and afterwards treated as fixed inputs alongside the question, while the\nconfidence is generated in a separate generation step and considered as sole target for optimization. Like this, we ensure that answer generation is disentangled from the optimization process, ensuring the answer correctness is not affected by our confidence calibration training.\n\nI think this is crucial, or else the model could strategically choose an answer that it knows is incorrect and report $\\hat{p} \\approx 0$ to guarantee high reward. However, Table 6 says that the prompt is\n\n> You will get questions. Answer with the correct answer. Additionally provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct...\n\nwhich makes it seem like the model jointly generates the answer and confidence level. Could the authors please clarify this?\n\n2. **No statistical significance analysis**. While the results are fairly impressive, I did not find any statistical significance analysis in the paper. Even basic error bars would significantly improve my confidence in the results.\n\nI also have a third more minor concern:\n\n3. I think Proposition 1 as stated is incorrect. The statement on line 242 refers to R(a,\\hat{p},j) which was just defined on line 237 using the epsilon clipping. However, what the proof actually shows is that the *non-clipped* version of the reward function is maximized at \\hat{p} = p*. In fact, the clipped version is not perfectly maximized at \\hat{p} = p*. The authors argue informally that the epsilon clipping is negligible in practice, but the proof of the proposition still needs to align with its statement. I think what the authors want to do is have Proposition 1 state that the non-clipped version is maximized at \\hat{p} = p*, then say that they add the clipping to avoid computing log 0 and it's negligible in practice."}, "questions": {"value": "My primary question is stated above in Weakness 1. I would also be interested to hear the authors' responses to Weaknesses 2 and 3. I would be happy to raise my score if these weaknesses are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "in5OQQOHix", "forum": "yResLmrVO1", "replyto": "yResLmrVO1", "signatures": ["ICLR.cc/2026/Conference/Submission18260/Reviewer_C9QF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18260/Reviewer_C9QF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761255410225, "cdate": 1761255410225, "tmdate": 1762927985621, "mdate": 1762927985621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an RL-based fine-tuning approach that makes an LLM generate both an answer and calibrated confidence of its answer being factually correct. The approach uses the logarithmic scoring rule as the reward, which is maximized when the reported confidence matches the true probability of being correct. Experiments show that approach yields improved calibration while not having major impacts on the model's task performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper studies the important and interesting problem of teaching an LLM to reliably communicate calibrated confidence in its answer.\n2. The training objective is based on the logarithmic scoring rule which is theoretically grounded.\n3. Experiments show strong improvement in the model's calibrated confidence, significantly reducing overconfidence. Moreover, the method does not have a big influence on the model's accuracy."}, "weaknesses": {"value": "1. The proposed method relies on a \"judge\" to check the correctness of the LLM's answer. This limits its applicability to questions for which the correctness of an answer can be easily verified. The approach does not handle subjective or open-ended questions.\n2. While the method teaches an LLM to report a calibrated confidence score, it is not entirely clear to me how this should be used in practice. I imagine that a user would need to read the LLM's answer, interpret the confidence score, and decide whether or not to trust the answer based on the confidence score. (For example, if the LLM reports a confidence score of 7 out of 10, how should one decide whether or not to trust the LLM's answer?) This can be inconvenient and add cognitive burden to the user."}, "questions": {"value": "1. Do the authors have an idea on how can the proposed method be combined with standard alignment objectives in LLM fine-tuning (e.g., RLHF) which use different rewards?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XekBUcELWK", "forum": "yResLmrVO1", "replyto": "yResLmrVO1", "signatures": ["ICLR.cc/2026/Conference/Submission18260/Reviewer_mas6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18260/Reviewer_mas6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877491267, "cdate": 1761877491267, "tmdate": 1762927984988, "mdate": 1762927984988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the new approach Rewarding Doubt that uses reinforcement learning to enhance large language models’ ability to express calibrated confidence scores better aligned with the actual predictive accuracy. The paper presents a practical idea and direction, with the proposed approach demonstrating effective results through experimental validation. However, the experimental depth and generalization are limited, and the applicability of the method to broader scenarios remains to be explored."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Leveraging reinforcement learning (RL) to improve the expression of confidence scores in large language models is a compelling idea, and the reward function proposed in this work is theoretically sound and well-motivated."}, "weaknesses": {"value": "1. Limited Experiments. Experiments are confined to Knowledge-based QA tasks (TriviaQA, CommonsenseQA, MedQA, QAMPARI); no evaluation on other domains like math reasoning (e.g., GSM8K) or open-ended generation, limiting claims of general confidence awareness.\n2. Constrainted Applicability. The method requires binary correctness judgments, making it unsuitable for open-ended tasks without ground truth (e.g., summarization, dialogue); no discussion or experiments on adaptations for such scenarios.\n3. Potential Negative Impact. Accuracy is nearly identical pre/post-training on TriviaQA, but no broader benchmarks (e.g., MMLU) or ablation studies assess potential degradation in reasoning or generation quality due to confidence optimization."}, "questions": {"value": "1. The paper does not clearly articulate the necessity of RL; what specific advantages does this PPO-based method offer over supervised fine-tuning (SFT) on approximate confidence labels, and why is SFT insufficient for optimizing the proper scoring rule?\n2. No experiments are provided on larger models (e.g., >30B parameters) or full-parameter fine-tuning; how does the method perform at scale, and does full fine-tuning improve calibration compared to LoRA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "npofWyxPcY", "forum": "yResLmrVO1", "replyto": "yResLmrVO1", "signatures": ["ICLR.cc/2026/Conference/Submission18260/Reviewer_8U17"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18260/Reviewer_8U17"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889279521, "cdate": 1761889279521, "tmdate": 1762927984479, "mdate": 1762927984479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the problem of calibrated confidence expression in Large Language Models (LLMs). The authors propose a novel Reinforcement Learning (RL) approach to directly fine-tune LLMs to express calibrated confidence estimates alongside their answers to user questions. The experimental results suggest the emergence of a general confidence awareness ability in the models."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The verbalized confidence calibration of LLMs is a highly important topic, and this research holds significant practical relevance.\n2. The experimental setup is comprehensive, and the analysis is in-depth.\n3. The proposed RL-based method is solid. While the experiments are conducted on smaller models, the approach appears to be naturally scalable to large-scale models.\n4. A key contribution is the effective transformation of a binary, difficult-to-train accuracy signal into a continuous confidence signal (distributed between 0 and 1) suitable for RL training. This represents a significant advancement in the domain of confidence calibration."}, "weaknesses": {"value": "1. The paper lacks a comparison with some key baseline models in the verbalized confidence literature, such as [1].\n2. The writing could be improved. For instance, Section 3 does not fully elaborate on the deeper implications or the interpretable motivation behind the design of the reward distribution function (shown in Figure 3). A more detailed analysis of the formula on line 237, specifically the concavity/convexity introduced by the *log* function, would be valuable. Explaining why this design encourages continuous confidence scores (e.g., 0.1-0.9) rather than binary (0 or 1) outputs would significantly enhance the paper's clarity. \n3. While understandable, given the computational cost, providing results on larger models would make the findings more persuasive. (This is noted as a limitation, but it does not diminish the paper's core contributions, and my evaluation score will not be reduced on this basis.)\n\n[1] Zhou Z, Jin T, Shi J, et al. SteerConf: Steering LLMs for Confidence Elicitation[J]. NeurIPS 2025."}, "questions": {"value": "1. Could the formula on line 237 be numbered? This would adhere better to academic formatting conventions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1mPZTuOqms", "forum": "yResLmrVO1", "replyto": "yResLmrVO1", "signatures": ["ICLR.cc/2026/Conference/Submission18260/Reviewer_rSe4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18260/Reviewer_rSe4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898335561, "cdate": 1761898335561, "tmdate": 1762927983946, "mdate": 1762927983946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Rewarding Doubt**, a reinforcement learning (RL)–based approach to encourage large language models (LLMs) to explicitly output calibrated confidence scores alongside their answers. The reward function is derived from the **logarithmic scoring rule**, penalizing overconfidence when the answer is wrong and rewarding confidence when correct. The method is evaluated on several question-answering benchmarks (TriviaQA, QAMPARI, CommonsenseQA, MedQA), showing improved **Expected Calibration Error (ECE)** and **AUROC** compared to baselines such as self-consistency, trained probes, and DPO-based calibration (LACIE). The authors argue that this approach yields more trustworthy confidence estimation and generalizes across domains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Empirical improvement**: The proposed method achieves lower ECE and higher AUROC than baseline methods across several datasets.\n* **Safety motivation**: The work aligns with the broader goal of improving AI trustworthiness and uncertainty awareness.\n* **Generalization potential**: The cross-domain generalization results are encouraging."}, "weaknesses": {"value": "Major Concern:\n* **Limited evaluation scope**: Experiments are confined to factual and multiple-choice QA datasets, lacking tests in open-ended, multi-hop, or reasoning-intensive scenarios where uncertainty expression is more critical. The cross-domain results are encouraging, but do not convincingly demonstrate scalability beyond simple QA benchmarks.\n* **Questionable necessity of RL**: It remains unclear why RL is essential. Similar effects might be achieved via supervised fine-tuning (SFT) using ground-truth confidence labels derived from accuracy statistics.\n* **Artificial confidence expression**: The model is forced to output explicit numerical confidences, which may not generalize well to natural text generation or more integrated forms of uncertainty communication.\n\nMinor Concern:\n* **Inconsistency in figures**: Figure 3 has a labeling error in the confidence scale (0–1 vs. 0–10).\n* **Lack of probing analysis**: It would strengthen the paper to analyze whether internal model representations after “Rewarding Doubt” training exhibit improved epistemic calibration, e.g., via **probing classifiers** on hidden states."}, "questions": {"value": "1. Why is reinforcement learning necessary in this framework? Could the same reward structure be incorporated via supervised fine-tuning using ground-truth labels?\n2. Have the authors examined how internal model representations (e.g., via **probing** or **representation similarity analysis**) change after training? Does the model’s epistemic awareness genuinely improve?\n3. Would the method remain effective in more complex tasks such as long-form generation, reasoning chains, or dialogue systems where confidence may need to be expressed implicitly?\n4. Could the authors clarify the inconsistency in Figure 3’s confidence scale (0–1 vs. 0–10)?\n5. How sensitive is the method to the confidence-output format or discretization (e.g., integer vs. continuous confidence)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8pbGLmGH7y", "forum": "yResLmrVO1", "replyto": "yResLmrVO1", "signatures": ["ICLR.cc/2026/Conference/Submission18260/Reviewer_aEuF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18260/Reviewer_aEuF"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929567292, "cdate": 1761929567292, "tmdate": 1762927983462, "mdate": 1762927983462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a RL-based approach that finetunes LLMs for producing well-calibrated confidence estimates, by optimizing confidence tokens with a reward using logarithmic scoring rule over both correctness and the estimated probability. This penalizes both overconfident wrong answers and under-confident correct answers. Results on single and multi-answer QA tasks demonstrate an improved confidence calibration compared to verbalized, consistency-based, probing, and another RL-based approach, without harming task accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* While there are a few existing works using RL training to improve calibration, directly optimizing the RL objective with a proper scoring rule is an original contribution.\n* The results show a strong gain in calibration performance over the baselines across a wide range of QA tasks, without hurting task accuracy. The results also seem robust and generalize well to out-of-domain datasets.\n* The approach provides a practical path to efficiently elicit calibrated confidence during inference, without requiring ensembling or sampling while substantially improving calibration."}, "weaknesses": {"value": "* Besides LACIE, there are other potentially strong RL-base baselines [1,2] that would be helpful to include. For multi-answer setting with QAMPARI dataset, only the verbalized, sequence probability and trained probe baselines are compared. More consistency or RL-based baselines could be helpful. Also, temperature scaling could be applied on top of baselines.\n* The main experiments and comparisons are done with Llama-3-8B-Instruct, but it would be helpful to include results on other model architectures and sizes (beyond comparing with verbalized confidence in the ablation).\n\n---\n\n[1]. When to Trust LLMs: Aligning Confidence with Response Quality\n\n[2]. Taming Overconfidence in LLMs: Reward Calibration in RLHF"}, "questions": {"value": "* What if the model learns to game the reward by always answering incorrectly with 0 confidence? This might be an edge case that was not seen empirically (the accuracy does seem unaffected), but the current reward formulation does not explicitly handle this. \n* Related to the last question, does it make sense to add a term or constant in the reward to further encourage accuracy? While separating the answer generation and optimizing only the confidence tokens make sense to minimize side effects on accuracy, a positive incentive might encourage correctness (e.g. if we allow updating the answer based on optimized beliefs/confidence).\n* For the clipping, what epsilon value did you use for the experiments? How sensitive are the results with respect to epsilon?\n* Why set the out-of-format penalty to −3? How frequent did the format failure occur?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0XgZQUWstY", "forum": "yResLmrVO1", "replyto": "yResLmrVO1", "signatures": ["ICLR.cc/2026/Conference/Submission18260/Reviewer_NHX7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18260/Reviewer_NHX7"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission18260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955769623, "cdate": 1761955769623, "tmdate": 1762927982920, "mdate": 1762927982920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}