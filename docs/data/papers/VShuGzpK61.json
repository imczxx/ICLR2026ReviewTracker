{"id": "VShuGzpK61", "number": 12381, "cdate": 1758207454382, "mdate": 1763566792708, "content": {"title": "Calibrated Uncertainty Sampling for Active Learning", "abstract": "We study the problem of actively learning a classifier with a low calibration error. One of the most popular Acquisition Functions (AFs) in pool-based Active Learning (AL) is querying by the model's uncertainty. However, we recognize that an uncalibrated uncertainty model on the unlabeled pool may significantly affect the AF effectiveness, leading to sub-optimal generalization and high calibration error on unseen data. Deep Neural Networks (DNNs) make it even worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we propose a new AF by estimating calibration errors and query samples with the highest calibration error before leveraging DNN uncertainty. Specifically, we utilize a kernel calibration error estimator under the covariate shift and formally show that AL with this AF eventually leads to a bounded calibration error on the unlabeled pool and unseen test data. Empirically, our proposed method surpasses other AF baselines by having a lower calibration and generalization error across pool-based AL settings.", "tldr": "We propose a new Acquisition Function by estimating calibration errors and query samples with the highest calibration error before leveraging DNN uncertainty.", "keywords": ["Active Learning", "trustworthy machine learning", "uncertainty estimation", "calibration"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/004ee847e5449bf0438e990dd62508831afd8b52.pdf", "supplementary_material": "/attachment/d1a51346b037d8fa485d8076db66d71126354c6f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a calibration aimed AL sampling method that estimates unlabeled sample calibration error using a kernel based local averaging estimator and prioritizes samples with the largest estimated calibration error for labeling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses AL from a calibration perspective, which I believe is relatively underexplored. the attempt to connect model calibration quality with sampling decisions introduces an interesting conceptual bridge between uncertainty estimation and data selection.\n2The bias variance analysis and convergence discussion (Theorems 4.1, 4.2)shows an effort to provide formal grounding for their estimator, which is important in works. While the assumptions are strong, the theoretical treatment gives the work analytical depth."}, "weaknesses": {"value": "1.The proposed problem and its motivation is unclear to me. The authors claim two critical issues arising from uncalibrated uncertainty, yet the first one, namely  the uncertainty quantification quality of existing AL baselines on unseen test data is not verified, is a general evaluation concern, not a problem specific to Active Learning. It does not logically motivate a new acquisition function. Only the second issue (uncalibrated uncertainty causing unreliable sampling) is actually relevant to AL.\n2.The demonstrative example lacks key information and renders confusing messages.  Specifically, In figure1, the caption references T = 50 and k = 10 without prior explanation; these variables are not introduced when the figure is first cited in the text. It is therefore impossible to know whether T denotes total query rounds or training epochs, or whether k is the batch size per query or the number of queried samples. What is more, the use of Least-conf as a baseline is not adequately justified. The paper did not define how “confidence” is computed (by the time this demo example is referred to). whether it is the maximum predicted probability, the softmax margin, or another uncertainty metric. Moreover, least-confidence is not a standard or competitive AL method in recent literature; \n3.The theoretical analysis (4.1, 4.2)only guarantees the consistency and boundedness of calibration error under active learning, but not its effect on classification risk or accuracy. While improved calibration is desirable, AL aims to reduce generalization error with limited labeling budget. Without establishing or empirically validating a link between calibration improvement and boundary learning efficiency, the method falls short of fully achieving AL’s goal of enhancing both calibration and accuracy. Furthermore, even if we focus on the claimed calibration consistency only, its theoretical guarantee has limited relevance to AL. Theorem 4.1 explicitly assumes an infinitely large labeled set, to achieve point-wise consistency of the estimator. This assumption contradicts the low-label regime that defines Al. In realistic settings with few labeled samples, the kernel estimator may have high variance and unreliable calibration estimates, weakening the practical value of the theoretical bound.\n4.For the methodology, it is straightforward. However, from the mathematical perspective, the Dirichlet kernel appears to be chosen mainly for analytical convenience (please correct me if I was wrong) it simplifies the derivation of bias and variance bounds under the simplex constraint. Experimentally, the justification reduces to a smoothness argument when tuning the bandwidth. However, this rationale does not sufficiently support the choice of this kernel over other possible kernels. The fact that a bound is easier to derive does not imply that alternative kernels are less effective in practice. In the absence of a stronger theoretical or empirical argument, an ablation comparing different kernel types （Gaussian, etc) should be included to validate whether the proposed choice meaningfully contributes to performance.\n5.(This is my major concern.)\nThe proposed method aims to improve Active Learning by selecting samples that indirectly enhance model calibration. However, once labeled data are available, calibration quality can always be improved more directly through post-hoc methods (e.g., temperature scaling or isotonic regression) using the same labeled set. In that case, any calibration-driven sampling provides no fundamental advantage: both approaches use identical supervision, yet explicit calibration guarantees strictly better or equal ECE. Moreover, if the authors’ argument is that better calibration leads to better acquisition and final accuracy, then a model explicitly calibrated after each AL round would necessarily yield more reliable uncertainty and thus stronger sampling performance. From this perspective, their method merely approximates what direct calibration already achieves through an indirect and less efficient route. While earlier issues such as motivation ambiguity and unclear experimental setup are secondary, this major concern lies in the legitimacy of the proposed approach itself. The paper’s central idea offers no demonstrable benefit over existing, theoretically simpler, and empirically stronger alternatives. In short, if labeled data exist, calibration should be achieved by calibration, not by sampling.\nTo properly validate the claimed contribution, the authors should include a control experiment under an identical labeling regime:\n1.Run a standard AL method (e.g., Entropy or BALD) with the same acquisition budget and identical labeled sets St as those used by the proposed method.\n2.After each training round, apply a standard calibration procedure (e.g., temperature scaling) using exactly the same labeled data that the proposed approach employs for its kernel-based estimator.\n3.Compare both ECE and accuracy across rounds."}, "questions": {"value": "Does the proposed calibration-driven sampling actually outperform a standard Active Learning pipeline that uses the same labeled data but applies explicit post-hoc calibration (e.g., temperature scaling) after each training round?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KOjchWw9a7", "forum": "VShuGzpK61", "replyto": "VShuGzpK61", "signatures": ["ICLR.cc/2026/Conference/Submission12381/Reviewer_cCs3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12381/Reviewer_cCs3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760650967189, "cdate": 1760650967189, "tmdate": 1762923284039, "mdate": 1762923284039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Calibrated Uncertainty Sampling for Active Learning (CUSAL), a new acquisition function that explicitly targets low calibration error. For every unlabeled instance, a Dirichlet-kernel estimator approximates the expected calibration error under covariate shift; samples are then chosen by lexicographic ordering that first maximizes the estimated calibration error and, on ties, maximizes model uncertainty. The authors prove that the estimator is point-wise consistent and that the resulting classifier enjoys a bounded expected calibration error on both the unlabeled pool and the unseen test set. Extensive experiments on MNIST, SVHN, CIFAR-10, ImageNet, and long-tail CIFAR-10-LT show consistent reductions in ECE and accuracy gains over six strong baselines."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. By directly using calibration error as the primary criterion for the active learning acquisition function, the paper breaks away from the conventional framework that focuses solely on uncertainty.\n2. The authors provide solid theoretical contributions by proving the pointwise consistency of the kernel calibration estimator under covariate shift and deriving an upper bound on the final model’s expected calibration error."}, "weaknesses": {"value": "1. The method may suffer from high computational overhead, since a kernel matrix needs to be computed for all unlabeled samples in each active learning round.\n2. In the third subfigure from the left in Figure 1, is the gap at position 0.4 plotted incorrectly?"}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZwYxuQjd3d", "forum": "VShuGzpK61", "replyto": "VShuGzpK61", "signatures": ["ICLR.cc/2026/Conference/Submission12381/Reviewer_7cmR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12381/Reviewer_7cmR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703256174, "cdate": 1761703256174, "tmdate": 1762923283658, "mdate": 1762923283658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an active learning algorithm that addresses calibrated models over conventional active learning. The authors consider a calibration metric based on classical kernel estimators of statistics and demonstrate better performance in terms of calibration and prediction, as well as acquisition steps. For the query function, selecting the informative instances is designed according to the lexicographical order. The experiments were conducted on MNIST, SVHN, Fashion MNIST, and CIFAR-10 with the baselines of Random Least-conf Margin, BALD Coreset, and BADGE. Various aspects of calibration approaches are examined in the ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The algorithm is simple, and the performance is better. The improvement of calibration is based on a well-designed procedure for kernel estimation using the pooling data. Though the improvements are relatively smaller, in active learning, it is often observed that the impressive point is to improve the calibration and prediction simultaneously."}, "weaknesses": {"value": "The dataset is relatively small; in many papers on topics of active learning, usually 6~8 datasets are examined. Also, the calibration is not studied thoroughly. There are many metrics for the calibration, such as the ECE and the KS-metric-based. Also, the calibration algorithms in processing or post-processing can cooperate with the proposed alg. The merit or reason for the use of a specific kernel type is not thoroughly validated. Furthermore, it is not clear to me why the lexicographical order is essential, as pointed out in the ablation study. Since there are two components in the loss function, what’s the dominant part or what’s the effective part in the prediction? In some cases, regularizing the calibration error can cause the degradation of prediction power. Why does the proposed algorithm not suffer from this phenomenon?"}, "questions": {"value": "1) The effect of the kernel can be crucial in practice. Do you have any way or guidelines to select the appropriate kernel?\n2) Basically, the proposed CE (loss) is based on the estimation of h, which has vagueness, especially in the early stage. Is there any clue to check this problem?\n3) The baseline for the post-calibration in the ablation studies looks insufficient. Can you consider 2~3 baselines for the calibration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KwcE9VnS7q", "forum": "VShuGzpK61", "replyto": "VShuGzpK61", "signatures": ["ICLR.cc/2026/Conference/Submission12381/Reviewer_1gMt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12381/Reviewer_1gMt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716861950, "cdate": 1761716861950, "tmdate": 1762923283367, "mdate": 1762923283367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Motivated by the issue of uncalibrated models in uncertainty-based active learning (AL), the article advocates for a query strategy using calibration error. It selects first the least uncalibrated samples,  then the least confident ones when the calibration error is uniformly small in the remaining unlabeled pool. This strategy encourages the training to focus on the calibration error at early query rounds in order to improve the uncertainty estimation. The calibration error is estimated with kernel method, with theoretical results on the consistency of kernel estimation and the bounds of calibration error on unlabeled and unseen test data. The experimentation on large-scale benchmarks show competitive results by the proposed method in comparison to a series of AL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is well motivated by the importance of well estimated uncertainty in active learning and the calibration issue that is common in large models.\n\n- The use of kernel estimation for evaluating calibration errors on unlabelled data is an interesting and reasonable idea.\n\n- The proposed method is tested on several benchmarks, compared to a series of active learning baselines. The experimental results show interesting gains by the proposed method.\n\n- The discussion on related work is carefully developed."}, "weaknesses": {"value": "- Arguments and results on the effectiveness of the lexicographic order during query over the least-calibrated only strategy might lack consistency (see Questions).\n\n- There might be some issues in the proofs (see Questions)\n\n- Some notations are ambiguous or erroneous. According to the setup in the beginning of Section 2, $x_1$ stand for the first data vector in the initial labeled and unlabeled set at the same time. In Lines 221-222, the index $i$ goes from $0$ to $k$, querying $k+1$ samples instead of $k$, and creating repeated indices for samples queried at different rounds. The distribution of the cumulative labeled dataset $S$ is different from the initial label set $S_0$, meaning that we cannot have $S\\sim\\mathbb{P}(X)\\mathbb{P}(Y\\vert X)=\\mathbb{P}(X,Y)$ and also $S_0\\sim\\mathbb{P}(X,Y)$, in contrary to the formula in (14). The abbreviation ECE for Expected Calibration Error is used before the specification of its meaning in Line 311."}, "questions": {"value": "I have several questions regarding the lexicographic order for query and the proof of theoretical guarantees. First on the lexicographic order:\n\n- At each query round, the proposed query strategy select first the least calibrated samples, then the least confident ones with zero calibration error. Does it mean that there is a Dirac at zero in the distribution of calibrated errors? If so, how to understand it? \n\n- As explained by the authors, their method focuses on least-calibrated samples at early query rounds, then moves to the least confident ones later when the trained model is well calibrated. To illustrate the effectiveness of this lexicographic order, the authors compared the performance of their query strategy with querying by least-calibrated only in Figure 12, where the performance of least-calibrated only diverges already from the proposed method at very early rounds. Could the authors explain this observation while the two methods should be nearly identical at early query rounds?\n\nOn the proof of theoretical guarantees:\n\n- Could the authors explain why the result of Popordanoska et al. (2022) can be applied to obtain (20), despite that the sample in the cumulative labeled set are not i.i.d. ?\n\n- Could the authors explain in detail the passage from (42) to (43)? From where I see it, the fact that the average calibration error is smaller than $\\epsilon$ on the samples queried at the $t$-th round does not imply that it is also the case for the samples queried at earlier rounds."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uRyItYRkEX", "forum": "VShuGzpK61", "replyto": "VShuGzpK61", "signatures": ["ICLR.cc/2026/Conference/Submission12381/Reviewer_yiPZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12381/Reviewer_yiPZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762274689461, "cdate": 1762274689461, "tmdate": 1762923282957, "mdate": 1762923282957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}