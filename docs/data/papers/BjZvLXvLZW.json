{"id": "BjZvLXvLZW", "number": 7205, "cdate": 1758011607442, "mdate": 1759897866524, "content": {"title": "MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use", "abstract": "Large Language Models (LLMs) are evolving from text generators into reasoning agents. This transition makes their ability to use external tools a critical capability. However, evaluating this skill presents a significant challenge. Existing benchmarks are often limited by their reliance on synthetic tools and severely constrained action spaces. To address these limitations, we introduce MCPVerse, an expansive, real-world benchmark for evaluating agentic\ntool use. MCPVerse integrates more than 550 real-world, executable tools to create an unprecedented action space exceeding 140k tokens, and employs outcome-based evaluation with real-time ground truth for time-sensitive tasks. We benchmarked the state-of-the-art LLMs across three modes (Oracle, Standard, and Max-Scale), revealing that while most models suffer performance degradation when confronted with larger tool sets, the agentic models, such as Claude-4-Sonnet, can effectively leverage expanded exploration spaces to improve accuracy. This finding not only exposes the limitations of state-of-the-art models in complex, real-world scenarios but also establishes MCPVerse as a critical benchmark for measuring and advancing agentic tool use capabilities.", "tldr": "", "keywords": ["(Large) Language Models", "Evaluation of NLP Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c639f0185ed071b1ccf7a271a9bd4165ea9e6da.pdf", "supplementary_material": "/attachment/4dbf52bed022440b587a0c9c64918ae32d588b9c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MCPVerse, a large-scale, real-world benchmark and evaluation system to evaluate the agentic tool use capabilities of LLMs.\nThe benchmark aggregates large-scale real-world tools and real execution system via MCP and support multiple evaluation modes  (Oracle, Standard, Max-Scale), hybrid outcome-based evaluation, and automated assessment pipelines.\nExperiments with the best LLMs highlight practical challenges and expose current limitations, especially in handling large toolsets at scale."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- MCPVerse uses real-world tools to evaluate the performance in real environments. Even the best LLMs cannot solve most of the tasks.\n\n- MCPVerse has much more tools than the previous benchmarks and discover the effect of different action spaces.\n\n- Section. 4 is detailed and brings new insights about LLM tool-use. It calls the larger tool number limitation for real-world application of future LLMs."}, "weaknesses": {"value": "- There are a total of 250 tasks, and most tasks (L1 & L2) require fewer than five steps. I am concerned whether most tools have been sufficiently utilized. \n- The tasks are manually designed, which ensures naturalness and reliability, but this approach may not be sustainable.\n- The changes of remote MCP servers may change the evaluation results."}, "questions": {"value": "See the \"weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "z48JCV1Llj", "forum": "BjZvLXvLZW", "replyto": "BjZvLXvLZW", "signatures": ["ICLR.cc/2026/Conference/Submission7205/Reviewer_mGrE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7205/Reviewer_mGrE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760864337489, "cdate": 1760864337489, "tmdate": 1762919356432, "mdate": 1762919356432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MCPVerse is a large-scale, real-world benchmark for evaluating agentic tool use, designed to address two major limitations of existing benchmarks: reliance on synthetic tools (existing benchmarks predominantly use simulated tools, lacking realism) and constrained action space (due to context length limitations, typically only a small number of tools can be mounted). MCPVerse integrates 65 MCPs, 552 tools, and 250 tasks, featuring three evaluation modes with different action space scales and three complexity levels. The evaluation focuses on outcome-based assessment rather than specific tool invocation trajectories. MCPVerse reveals a crucial insight: an expanded action space is advantageous rather than detrimental for SOTA agentic models, as they can leverage the extended toolset to explore diverse solution paths."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark integrates 65 MCPs with 552 tools, with tool definitions exceeding 147k tokens. This is a leap from prior work that relied on simulated or mock tools, bringing evaluations much closer to real-world deployment scenarios.\n\n2. The 250 tasks span realistic scenarios across information retrieval and system operations, with a three-level complexity taxonomy. The inclusion of time-sensitive tasks with real-time ground truth validation is particularly noteworthy.\n\n3. The three evaluation modes (Oracle, Standard, Max-Scale) provide a systematic way to probe model capabilities under varying action space scales. This design choice is well-motivated and allows to study how model performance changes with toolset complexity.\n\n4. The hybrid outcome-based evaluation approach is sensible—using LLM-as-a-judge for semantic consistency and automated scripts for verifying state changes. Importantly, the focus on final outcomes rather than prescribed tool-calling trajectories avoids penalizing models for finding alternative valid solutions.\n\n5. The experiment is comprehensive and insightful, covering 12 models across different modes. The experiments reveal a counter-intuitive finding: certain agentic models (Claude-4-Sonnet, Qwen3-235B-2507, GLM-4.5) actually improve when given access to larger toolsets, demonstrating emergent solution paths. The analysis of prompt-based vs. native function calling, retrieval-based approaches, and various model limitations adds significant depth.\n\n6. The paper is well-written with clear figures (especially Figure 1 and Figure 4) and comprehensive tables. The methodology is described in sufficient detail for reproducibility, and the appendices provide thorough documentation of implementation details."}, "weaknesses": {"value": "Refer to questions section."}, "questions": {"value": "**Question 1: Counter-intuitive difficulty ranking in Max-Scale Mode**\n\nIn Max-Scale Mode, both Claude-4-Sonnet and Qwen3-235B-2507 exhibit a phenomenon where L2 success rates are lower than L3 success rates. Does this counter-intuitive pattern suggest that the difficulty rating system may be biased or miscalibrated? Could the authors explain the underlying causes of this phenomenon and provide specific case studies demonstrating why certain L3 tasks might be easier to solve than L2 tasks in the Max-Scale setting?\n\n**Question 2: Performance gains from prompt-based function calling**\n\nPROMPT-BASED FUNCTION CALLING shows performance improvements for certain models in both Oracle and Standard modes. Does this suggest that the prompt-based method provides certain advantages in tool selection or reasoning that native function calling lacks? Or does this indicate that the native function calling APIs of these models are not well-optimized? Could the authors explain the causes of these counter-intuitive performance gains and provide specific case studies illustrating when and why prompt-based function calling outperforms native approaches?\n\n**Question 3: Source of the 70% hallucination rate**\n\nIn line 449 (Section 4.2.4), the paper states: \"This discrepancy caused a high rate of hallucination, exceeding 70%, where the model frequently fabricated tool responses despite explicit instructions to the contrary.\" How was this 70% hallucination rate calculated? Could the authors provide more details on the methodology used to compute this metric and whether similar hallucination rates were observed in other models?\n\n**Question 4: Exploring retrieval performance with varying top-K**\n\nRegarding EVALUATION UNDER RETRIEVE MODE (Section 4.3), the current top-5 retrieval limit may be the primary cause of the poor performance compared to Standard mode (which uses 32 MCPs with 220+ tools). In fact, the fixed use of large MCP sets, though potentially yielding higher performance, occupies substantial context that is infeasible in practice. Exploring how to improve the performance of dynamically retrieved MCPs may be a more promising direction with greater practical value.\n\nCould the authors show how model performance changes as top-K increases? Specifically, is there a value of K (smaller than 32) where performance approaches that of Standard Mode (which contains 32 MCPs and 220+ tools)? This analysis would help identify whether the poor Retrieve mode performance stems from insufficient recall (top-5 being too small) or from fundamental limitations in the retrieval method itself. Understanding the performance-context tradeoff curve would provide valuable insights for practical deployment scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zkuPwo8810", "forum": "BjZvLXvLZW", "replyto": "BjZvLXvLZW", "signatures": ["ICLR.cc/2026/Conference/Submission7205/Reviewer_6SCH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7205/Reviewer_6SCH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883497771, "cdate": 1761883497771, "tmdate": 1762919356071, "mdate": 1762919356071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes \"LLMs are becoming tool-using agents, but current benchmarks rely on synthetic tools and tiny action spaces. MCPVerse introduces a real-world benchmark with 550+ executable tools, an action space >147k tokens, and outcome-based, real-time evaluation across Oracle, Standard, and Max-Scale modes. Results show most models degrade as tool sets grow, while agentic models (e.g., Claude-4-Sonnet) can exploit larger tool spaces—highlighting both current limitations and MCPVerse’s value.\" The main contributions of this paper are,\n\n- MCPVerse: A real-world tool-use benchmark with 550+ executable tools and carefully designed tasks, each with ground truth (or scripts for time-sensitive cases) and metadata like required MCPs, difficulty, and task type.\n\n- Evaluator: An end-to-end automated system for multi-step agent–tool interactions, scoring final outcomes with a hybrid metric—LLM-as-judge for text plus scripts to verify state changes.\n\n- Findings: Testing SOTA models reveals clear limits with large toolsets; importantly, bigger action spaces help agentic models rather than just hindering them."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths of this paper can be summarized as,\n\n- The article is written in plain, easy-to-understand language.\n\n- The proposed dataset size is acceptable."}, "weaknesses": {"value": "The  main weaknesses and questions of the paper are listed below,\n\n- The issues in Introduction section:\n\n    - Over-generalization of “artificial tools.” Claiming “many benchmarks rely on artificial tools” ignores widely used real-execution suites (e.g., SWE-bench runs tests on real repos; WebArena/BrowserGym execute live web actions; OpenDevin/AgentBench run OS/terminal tools). The statement needs qualifiers or counter-examples. Meanwhile, the abundance of prior similar work significantly diminishes the paper’s novelty and value.\n\n    - “Stop short of actual execution.” Saying benchmarks that include real APIs usually avoid execution is factually shaky—several do execute API calls or environment steps and verify outcomes. Provide specific citations that truly lack execution, or soften the claim. \n\n    - Evaluation “confined to tool name/parameters.” Many benchmarks score functional outcomes (tests pass, task success on websites, file created, answer matches). Portraying evaluation as name/arg matching is misleading.\n\n    - Unproven causality (“superficial patterns → success”). The text asserts models succeed via pattern matching rather than planning without evidence. Either cite studies demonstrating this failure mode or rephrase as a hypothesis.\n\n    - Conflation of action space with context length. Equating “large API list” with tokens in the prompt is a category error: tool registries can be programmatic/dynamic; agents can retrieve tool specs on demand. Context limits don’t force a tiny tool set.\n\n    - Unsubstantiated quantities. “a few dozen relevant options per query” is asserted without data; some frameworks surface 5–10, others allow hundreds via paging/ID lookups. Provide measured ranges or drop the number.\n\n    - “Artificial tools,” “real-world APIs,” and “action space” aren’t operationalized, making several claims unfalsifiable. Define them before arguing limitations.\n\n- The issues in MCPVERSE DESIGN section:\n\n    - Key-policy conflict. The authors “minimize reliance on API keys” for reproducibility but also “retain a small number of highly valuable MCPs that do require API keys.” This is contradictory and needs a clear policy (e.g., provide test keys, mock servers, or a keyed vs. keyless track).\n\n    - Drift risk. The authors acknowledge providers “add or deprecate tools over time,” yet claim “long-term usability.” Without version pinning/snapshotted specs, the benchmark isn’t reproducible across time.\n\n    - Distractor design. Excluding Gmail/Notion from task construction but keeping them as distractors may unfairly penalize agents that (reasonably) select them; also many such tools require OAuth/keys, so they’re non-actionable distractors.\n\n    - Counting tools. “65 MCPs expose 552 tools” likely includes overlapping/aliased endpoints across providers. If duplicates aren’t deduplicated or typed, the reported action-space size can be misleading.\n\n    - Real-time flight booking. Executing live bookings can incur costs and create ethical/compliance issues. The authors should state that only sandbox/test endpoints are used and how the authors prevent charges or personal-data handling.\n\n    - Minor but impactful wording issues: “As a Results” → “As a result.”; “some MCP (e.g., Gmail, Notion) are …” → “some MCPs … are …”; Ensure consistent capitalization (MCP/tool names) and commas (“e.g., …”).\n\n    - Category mismatch. Text says tasks are in two main types (Information Retrieval & System Operation), but Fig. 2a shows three (adds “Hybrid”) and the label uses “System Interaction.” Define types consistently and say whether “Hybrid” is a third class or a bookkeeping tag.\n\n    - Percentages vs. total count. The authors claim 250 tasks, yet the one-decimal percentages imply non-integer counts (e.g., 40.2% of 250 = 100.5 tasks; 14.2% = 35.5). Report integer counts or percentages that sum from integers.\n\n    - Time-sensitive ground truth ≠ reproducible. The authors “retrieve real-time answers for validation,” which means results change over time; this undermines comparability and “long-term usability.” The authors need timestamped snapshots, cached labels, or versioned replay logs.\n\n    - Complexity definition is circular/confounded. L1/L2/L3 are defined by step counts and number of tools (e.g., “about 5 steps,” “multi-tool”). That makes complexity a property of the chosen solution path/annotation, not the underlying task, and confounds tool routing with difficulty. The authors should measure complexity from execution traces (median steps, tool entropy) rather than prescribe it.\n\n    - “Must require external tools” is unverified. Many IR tasks (e.g., academic facts) can be answered from parametric knowledge. There’s no baseline check showing no-tool models fail these items. Without dynamic/ephemeral instances, this requirement isn’t guaranteed.\n\n    - Annotator protocol leaves quality gaps. “All undergraduate-level or above” annotators, using an IDE assistant (Cursor), but no inter-annotator agreement, adjudication policy, or qualification tests are reported. For financial/news/OS tasks, domain competence matters.\n\n    - Outcome clarity for IR subtypes. Subtypes like “Academic Research (retrieving papers/citations)” can hit paywalls or return non-deterministic results across providers. The page doesn’t state which APIs/endpoints are allowed, how duplicates are handled, or how success is verified (exact DOI match? title?).\n\n    - Security & sandboxing for system operations. The authors allow “executing system commands” and file ops but don’t specify the sandbox/permissions model. Without isolation, evaluations are unsafe or non-portable.\n\n    - “As retrievers may hinder exploration” is asserted without data. Prior work often shows retrieval helps by reducing distraction. Provide ablations (full list vs. retriever) or soften.\n \n    - “44k tokens for definitions, ~20k left for prompt+response (64k window).” This couples task solvability to vendor token limits rather than capability. It also ignores multi-turn overhead. The authors need a fixed, model-agnostic tool-description budget (e.g., 80–120 tokens/tool) or a paging API.\n\n    - Only models with huge windows (Claude-4-Sonnet, Gemini-2.5-Pro) “complete the evaluation.” Results will primarily reflect context capacity, not tool-use skill. That hampers comparisons with mainstream/open-source models.\n\n    - The text cites “up to 1M tokens,” but the benchmark actually uses ~140k. Long-context quality often degrades; without showing retention/attention stability, the premise that “bigger window → better tool use” is speculative. The authors motivate three modes but don’t state ablations (e.g., same tasks under Oracle vs. Standard vs. Max-Scale) to quantify how action-space growth changes success independent of context size.\n\n- Limited analysis in Experiments Section. Unexplained asterisk. The “23.4*” for GPT-5 lacks a footnote/explanation (e.g., context limit, truncation). Claim contradicts Table 4. Text says retrieve-mode is “consistently and significantly lower than oracle and standard,” but for GPT-5 it’s higher than Standard (45.0 vs 23.4*). Reproducibility/fairness. Using OpenAI text-embedding-3-large (both index and query) is closed and vendor-specific; release frozen embeddings or ablate with open models (e.g., bge-large, e5). Retrieval confounded with model self-description. The LLM must first generate tool descriptions; failure to name or phrase them well sinks recall, so results mix retrieval quality with model prior/tool-naming knowledge. Evaluate retrieval with task-derived queries and report recall@k for required servers.\n\nOverall, this paper is unsatisfactory. First, there are many ambiguous statements, and numerous claims are unsupported or unreliable. Second, the dataset construction section is overly brief and lacks sufficient detail for reproducibility. Third, if the appendix is meant to be important, its organization and formatting need careful revision. Finally, including an acknowledgments section at submission risks disclosing the authors’ collaborating institutions (compromising anonymity)."}, "questions": {"value": "Please refer to the Weaknesses section. Overall, the paper is unsatisfactory: the motivation shows limited novelty, many critical details are missing, and the experimental design and analysis are sparse. I encourage the authors to thoroughly revise the manuscript."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gKZMikxteP", "forum": "BjZvLXvLZW", "replyto": "BjZvLXvLZW", "signatures": ["ICLR.cc/2026/Conference/Submission7205/Reviewer_cTyv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7205/Reviewer_cTyv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937154457, "cdate": 1761937154457, "tmdate": 1762919355730, "mdate": 1762919355730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new benchmark for agentic tool use called MCPverse. This benchmark includes 65 MCPs and 552 tools, with a variety of task types, complexities, and time sensitivities. The authors evaluate models from a variety of providers and share the results across three modes (Oracle, Standard, Max-Scale)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Relevance: MCPs are gaining popularity and measuring agentic tool use in the context of MCPs makes a lot of sense and is timely.\nUnsaturated: The maximum score on the benchmark is still <70%, so the benchmark is far from saturated.\nModel evaluation: A diversity of models are evaluated which is great to see. The specific evaluation of large context is also useful. \nJudging: Model scores seem relatively stable across judge models (e.g., GPT-4o vs QwQ)."}, "weaknesses": {"value": "Sharing tasks: This benchmark would benefit substantially from a more thorough discussion of example tasks so that quality of the evaluation can be better understood. The task types, complexity, and sensitivity are included in the paper, but it is not possible to vet, for example, whether the geographical information retrieval tasks are high-quality unless we get more detail. Moreover, details on the review process to ensure quality are not discussed (beyond the 1 sentence of \"After initial construction, every task undergoes a comprehensive review\"); we are missing key details on rounds of review, qualifications of reviewers, etc. which are required to verify that the quality is worthwhile. Moreover, full examples of the tasks are never given; a diagram of actual example tasks or a link to a dataset viewer would be much more useful than the pie charts shown.  Finally, open-sourcing/sharing the tasks, even if just a subset (while keeping another subset held-out), would be important for this work to advance the field of benchmarking. This is the biggest current gap in the paper that prevents me from 1) verifying the tasks are actually of high quality and 2) sizing the magnitude of this contribution as substantial. I would be open to reconsidering my review if this detail was provided and the tasks were possible to inspect directly.\nEvaluation settings: Much of the evaluation is dependent on the API settings used (e.g., tool cap from the API provider) and this prompt-based function calling workaround; it's hard to tell whether the results are due to limitations of the API endpoint or the scaffold versus the models themselves. \nEase of running: This benchmark seems quite difficult to run and sensitive to scaffold; combined with the lack of open-sourcing of the benchmark, it is not possible to run or reproduce to better understand.\nMetrics: The binary evaluation metric for success is a bit confusing -- is there a reason we didn't consider partial credit?\nTypos: There are also some small typos, e.g., \"Hybird\" instead of hybrid on page 2, \"As a Results\" on page 4."}, "questions": {"value": "What were the settings used for each model? (e.g., GPT-5: what was the reasoning effort?) This could also go in appendix.\n\nThe authors mention \"After initial construction, every task undergoes a comprehensive review.\" --> what was this review, and how can we trust the quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LDbxbt7uKr", "forum": "BjZvLXvLZW", "replyto": "BjZvLXvLZW", "signatures": ["ICLR.cc/2026/Conference/Submission7205/Reviewer_UkF5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7205/Reviewer_UkF5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958047890, "cdate": 1761958047890, "tmdate": 1762919355342, "mdate": 1762919355342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}