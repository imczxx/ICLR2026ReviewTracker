{"id": "ngfIm9aPsH", "number": 11539, "cdate": 1758201155795, "mdate": 1763725021390, "content": {"title": "Object Fidelity Diffusion for Remote Sensing Image Generation", "abstract": "High-precision controllable remote sensing image generation is both meaningful and challenging. Existing diffusion models often produce low-fidelity objects due to their inability to adequately capture morphological details, which may affect the robustness and reliability of object detection models. To enhance the accuracy and fidelity of generated objects in remote sensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which effectively improves the fidelity of generated objects. Specifically, we are the first to extract the prior shapes of objects based on the layout for diffusion models in remote sensing. Then, we introduce a self-distillation diffusion model with consistency distillation loss, which can generate high-fidelity remote sensing images without providing real images during the sampling phase. Furthermore, we introduce DDPO to fine-tune the diffusion process, making the generated remote sensing images more diverse and semantically consistent. Comprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art methods in the remote sensing across key quality metrics. Notably, the performance of several polymorphic and small object classes shows significant improvement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for airplanes, ships, and vehicles, respectively.", "tldr": "", "keywords": ["Image generation", "remote sensing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/276a0731ac34a575ec446b15ad21138b9270523f.pdf", "supplementary_material": "/attachment/f67b7e654656ae389817c6eb87579827fbc3ecf5.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an object fidelity diffusion method, which extends the layout-to-image paradigm into the remote sensing sense. This method combines many existing modules to achieve controllable image generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This is a layout-to-image generation method for the remote sensing field.\n2. The proposed method integrates an online-distillation strategy and DDPO fine-tuning to achieve high-fidelity image generation."}, "weaknesses": {"value": "1. The proposed OF-Diff is primarily an assembly of existing, well-known components: ControlNet for conditioning, a form of online distillation for feature alignment, and DDPO for fine-tuning. \n2. The central claim of \"reducing reliance on real images\" is misleading; the method heavily depends on real images during training to extract shape masks via RemoteSAM and to train the teacher model in the distillation process. The \"prior shape extraction\" is essentially semantic segmentation, which is a standard technique. The paper fails to convincingly demonstrate what the key, novel insight is, beyond a specific combination of these existing blocks. \n3. The comparison methods discussed in the paper (such as LayoutDiffusion and GLIGEN) were not specifically designed for remote sensing imagery. Applying them directly to remote sensing scenarios and using them as baselines is unfair. Furthermore, the authors did not provide sufficiently detailed training configurations (such as whether all comparison methods underwent adequate adaptation to remote sensing data), which may have led to biased comparison results.\n4. Although the paper demonstrates improvements in mAP for object detection tasks, it does not explicitly prove whether these gains stem from enhanced image quality or merely increased data volume. There is a lack of quantitative analysis regarding the actual contribution of generated images during detector training, such as through visualization or feature distribution alignment analysis.\n5. The entire pipeline is critically dependent on the quality of masks produced by the ESGM, which itself relies on external models (RemoteCLIP, RemoteSAM). The paper provides no analysis of what happens when this module fails or produces noisy/incorrect masks, which is inevitable in practice. How robust is OF-Diff to errors in the initial shape prior? If the extracted mask is distorted, will the generation process fail catastrophically? This is a major point of practical vulnerability that is completely unaddressed.\n6. The proposed system is exceptionally complex, involving multiple stages: ESGM, a dual-branch diffusion model with online distillation, and a subsequent DDPO fine-tuning step. The computational cost, memory footprint, and training time must be enormous compared to baselines like AeroGen. The paper completely omits any discussion of efficiency, training time, or inference speed. The practical utility is questionable if the method is an order of magnitude more expensive to train and deploy."}, "questions": {"value": "1. Can you precisely state the novel algorithmic contribution of OF-Diff, distinct from the existing components (ControlNet, distillation, DDPO) it builds upon?\n2. Have you attempted to fine-tune the CC-Diff baseline on your specific datasets to ensure a fair comparison? Could its distribution shift be mitigated?\n3. How does the performance of OF-Diff degrade when the input shape masks from ESGM are noisy or partially incorrect? Please provide a robustness analysis.\n4. Can you provide data on the computational cost (e.g., GPU hours, memory usage) of training OF-Diff compared to the key baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5MAJfBn6PW", "forum": "ngfIm9aPsH", "replyto": "ngfIm9aPsH", "signatures": ["ICLR.cc/2026/Conference/Submission11539/Reviewer_7tpr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11539/Reviewer_7tpr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459142101, "cdate": 1761459142101, "tmdate": 1762922632863, "mdate": 1762922632863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a a method for generating satellite images conditioned on object layouts. Motivation for this work is the need of augmenting existing training datasets, thus, proposing a method for generating additional label-image pairs. Furthermore, this work focuses specifically on instance/object level generations (instead of the commonly adopted semantic maps conditioning or text conditioning)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Authors identify a lack in current literature: few works sucessfully tackle instance-level generation given the difficulty of the task. Instance-level (layout-to-image paradigm) gives more precise control over the generations and alignment with the ground truth conditions.\n- Authors propose a realiable pipeline for achieving high layout fidelity generations by DDPO finetuning, and without the need of using real control images.\n- Authors provide ablation studies for the design decisions.\n- Extensive evaluation is carried out.\n- Qualitative results look strong compared to other models."}, "weaknesses": {"value": "Authors do not provide any dataset augmentation experiment for OOD-datasets. Such experiment would be useful to prove the usefulness of the model beyond their training dataset distribution, to see if their generations are actually useful for other downstream datasets. I believe this is an important experiment that should be carried out, as it determines the overall usefulness of the generated images not just within the training distribution.\n\nI suggest authors to select some other dataset (not DOTA or DIOR) and compare a baseline model trained on the original dataset and an augmented version of the target dataset.\n\nIn fact, Tables 6 and 7 show very small downstream improvements in terms of trainability when compared with other methods. Could authors provide some intuition why generations are not always profitable for training? Trainability is an important part of the work carried out. Authors could provide more ablation experiments showing whether the generated images are usefulness for training."}, "questions": {"value": "- Given a baseline model trained on DIOR/DOTA default dataset, authors show downstream improvements when baseline model is trained with original + OF-Diff generations (Figure 5). It would be interesting to see the AP evolution given different amounts of synthetic vs real data. For instance:\n   - Train a baseline model on 100% real images\n   - Train a baseline model on 100% generated images\n   - Train a baseline model on 50% real + 50% generated images\n   - etc.\n   - Train a baseline model on 100% real + 50% generated images\n   - Train a baseline model on 100% real + 100% generated images\n   - Train a baseline model on 100% real + 200% generated images\n   - etc.\n- Do authors have any intuition when the baseline model performance plateaus? In other words, the point at which generating more images will not improve downstream performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CGcUf2IzFb", "forum": "ngfIm9aPsH", "replyto": "ngfIm9aPsH", "signatures": ["ICLR.cc/2026/Conference/Submission11539/Reviewer_8kyc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11539/Reviewer_8kyc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935274112, "cdate": 1761935274112, "tmdate": 1762922632326, "mdate": 1762922632326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Object Fidelity Diffusion (OF-Diff), a novel diffusion-based model for layout-to-image generation in remote sensing (RS). The key idea is to improve object fidelity and layout controllability without relying on real-image references at inference time. The authors introduce:\n1. Enhanced Shape Generation Module (ESGM) to extract object shape priors from bounding box layouts.\n2. Online distillation to align shape-based generation with real-image features during training.\n3. DDPO (Denoising Diffusion Policy Optimization) to fine-tune the model for better diversity and semantic consistency.\nThe model is evaluated on DIOR-R and DOTA datasets using a comprehensive set of metrics including FID, KID, YOLOScore, and downstream detection mAP. Results show superior fidelity, layout consistency, and downstream utility, especially for small and polymorphic objects."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. OF-Diff does not require real-image references at inference, a significant practical improvement.\n2. State-of-the-art results on both DIOR-R and DOTA datasets, with mAP improvements of up to 8.3% on airplane and 7.7% on ship categories.\n3. The paper is well-structured, with clear problem motivation, method description, and experimental analysis."}, "weaknesses": {"value": "1. The online distillation and DDPO fine-tuning steps are computationally expensive, but the paper does not report training time, GPU usage, or memory overhead.\n2. The paper shows that adding captions improves aesthetics but hurts fidelity (Fig. 7). However, this trade-off is not deeply analyzed. A user study or perceptual evaluation would help clarify when and why to use captions.\n3. The method heavily relies on ESGM-generated shape masks. While the paper mentions that distorted masks lead to poor generation, it does not quantify how robust the model is to noisy or incomplete masks?\n4.The model is only evaluated on two datasets (DIOR-R and DOTA), both of which are airborne/satellite optical imagery."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9sEkyYYBtA", "forum": "ngfIm9aPsH", "replyto": "ngfIm9aPsH", "signatures": ["ICLR.cc/2026/Conference/Submission11539/Reviewer_HT6t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11539/Reviewer_HT6t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993738811, "cdate": 1761993738811, "tmdate": 1762922631548, "mdate": 1762922631548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Object Fidelity Diffusion (OF-Diff), a novel layout-to-image diffusion model designed specifically for generating high-fidelity remote sensing (RS) imagery. The primary motivation is to address critical failure modes in existing methods, such as control leakage, structural distortion, and dense generation collapse, which limit their utility for downstream tasks like object detection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. ESGM: Leverages pre-trained vision-language and segmentation models (RemoteCLIP and RemoteSAM) to extract precise object shape masks, providing strong geometric priors beyond simple bounding boxes.\n\n2. Employs a teacher-student architecture where a \"teacher\" decoder (conditioned on both image and shape features) guides a \"student\" decoder (conditioned only on shape features). This allows the model to learn to generate high-fidelity textures and details without requiring real image references during inference.\n\n3. Applies Denoising Diffusion Policy Optimization (DDPO) as a post-training step, using a reward function based on KNN distance and KL divergence to enhance the diversity and distributional consistency of the generated images.\n\nComprehensive experiments on the DIOR and DOTA datasets demonstrate that OF-Diff outperforms state-of-the-art methods in generation fidelity, layout consistency, and its utility in improving downstream object detection performance."}, "weaknesses": {"value": "1. The ESGM module is critically dependent on two large, specialized models: RemoteCLIP and RemoteSAM. While effective, this raises questions about the framework's scalability, accessibility, and potential biases inherited from these foundational models. The paper could benefit from a discussion on the computational cost of this \"template extraction\" phase and an analysis of how errors from ESGM might propagate through the diffusion pipeline.\n\n2. The paper clearly defines the DDPO reward function (Equation 9) but omits crucial implementation details for the KNN component. As the authors know, computing KNN in the high-dimensional pixel space is infeasible and perceptually meaningless. It is standard practice to compute this in a low-dimensional embedding space (e.g., using a CLIP or VAE encoder). While this is likely what the authors did, this critical detail should be explicitly stated in the implementation section to ensure reproducibility and clarity. Mentioning the specific pre-trained encoder used would be essential.\n\n3. The paper exclusively focuses on object detection as the downstream task. While this is a highly relevant application, remote sensing involves many other perception tasks, such as semantic segmentation and change detection. Demonstrating the utility of OF-Diff for these other tasks could further strengthen the paper's claims of general applicability."}, "questions": {"value": "Robustness of ESGM: The quality of the generated images seems highly dependent on the quality of the shape masks produced by ESGM. How does OF-Diff perform when ESGM fails or produces a distorted mask (e.g., for objects with complex boundaries or under heavy occlusion)? Is there a mechanism to handle such failures, or does the model simply replicate the distorted shape?\n\nOn the DDPO Reward Function: The KNN reward term encourages diversity by pushing generated samples away from the nearest neighbors in the real dataset. Could this potentially penalize the generation of \"typical\" or common instances and favor only rare or outlier-like objects? How was the balance between the KNN and KL terms (controlled by $\\omega$) determined to prevent this?\n\nInference Speed and Cost: Could you provide details on the inference speed of OF-Diff compared to other methods? Specifically, since ESGM is only used to populate a mask pool for inference, how large does this pool need to be for good performance, and does the selection from this pool add any significant overhead?\n\nRegarding the use of captions (Section 4.5): You note that including captions improves aesthetic appeal but harms downstream performance by deviating from the real data distribution. This is a very interesting finding. Does this imply that for data augmentation purposes, it is better to have models that are \"faithful\" to the original dataset's quirks and potential imperfections rather than models that generate more \"idealized\" or aesthetically pleasing images? I would appreciate it if you could elaborate on this insight."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Prf4Qvcajh", "forum": "ngfIm9aPsH", "replyto": "ngfIm9aPsH", "signatures": ["ICLR.cc/2026/Conference/Submission11539/Reviewer_3Som"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11539/Reviewer_3Som"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005497408, "cdate": 1762005497408, "tmdate": 1762922631096, "mdate": 1762922631096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}