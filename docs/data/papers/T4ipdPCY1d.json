{"id": "T4ipdPCY1d", "number": 5165, "cdate": 1757858236518, "mdate": 1762920407977, "content": {"title": "UniFlow: Zero-Shot LiDAR Scene Flow via Cross-Domain Generalization", "abstract": "Scene flow estimation is an important primitive for 3D motion understanding and dynamic scene reconstruction. Recent LiDAR-based methods have made significant progress in achieving centimeter-level accuracy on popular autonomous vehicle (AV) datasets. Notably, such methods typically only train and evaluate on the same dataset because each dataset has its own unique sensor setup. Motivated by recent work in zero-shot image-based scene flow, we argue that multi-dataset training is essential for scaling up LiDAR-based methods. However, prior work in LiDAR-based semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single-dataset models. We re-examine this conventional wisdom in the context of LiDAR-based scene flow. Contrary to popular belief, we find that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration than high-level tasks such as detection. Informed by our analysis, we propose UniFlow, a feedforward model that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse point density and velocity distributions. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 16.4% and 34.5% respectively. Moreover, UniFlow achieves state-of-the-art zero-shot accuracy on TruckScenes, outperforming prior dataset-specific models by 38.4%!", "tldr": "We present UniFlow, LiDAR scene flow method that combines existing datasets and demonstrates state-of-the-art performance on both in-domain and out-of-domain generalization.", "keywords": ["LiDAR Scene Flow", "Zero-Shot", "Autonomous Veicles"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9032ee51e6c406327f9e3fedbe5dc02a2c02d70e.pdf", "supplementary_material": "/attachment/adad90c7d12e10127135b3601d72d926be9e0de2.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposed a multi-dataset training strategy for better performance and generalization ability in scene flow models. Although the multi-dataset training can improve the performance, I have concerns about the zero-shot ability and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The overall motivation seems great. Some test-time adaptation techniques show potential in zero-shot generalization, thus it makes sense transferring them into scene flow topic.\n\n2.Experiments demonstrate the state-of-the-art performance when adding the proposed multi-dataset training.\n\n3.Authors showcase some failure cases and illustrate the reason."}, "weaknesses": {"value": "1.The main concern is limited novelty, especially for the technique contribution. Authors only leverage more datasets for training, thus providing a better accuracy. However, there seem to be no technique contributions to this community. Plus, a better performance by using more data is somehow intuitive.\n\n2.As for unifying different frame rates in sensors, will the linear interpolation method incur inaccuracy? Some recent works, e.g., LiDAR4D (CVPR24), NeuralPCI (CVPR23) indicated that the 4D motion in real worlds are non-linear and using a linear model will introduce large errors. Plus, in the ablation study, it seems that the different frame setting can significantly affect the performance. Does this prove the model is sensitive to frame rating?\n\n3.In the test-time optimization section, since NSFP has been criticized by large time overhead (FastNSF, ICCV23), could this model introduce large computational overhead?\n\n4.In the forward and backward loss, why authors first add Pt and then minus it? How about directly using the subtract between forward and backward flows?\n\n5.‘so we only advect the residual to moving points according to UniFlow’s original predictions’. Here, there lack some descriptions about what kind of classification methods are used, i.e., how to distinguish dynamic and static points? Also, will the threshold be too high to capture some subtle motions? Since 10m/s is basically the speed of vehicles. But some other agents like pedestrians, cyclists, slowly-moving cars also have motions. More explanations and ablation studies about this threshold are expected.\n\n6.I doubt the setting in Table 6. Authors compare other methods trained on TrackScene and the multi-dataset training (UniFlow) trained on other datasets like Waymo. But the overall performance is worse than methods trained on TrackScene. So what conclusion can this experiment lead to? I doubt authors should train other methods on out-of-domain datasets like waymo and maybe the performance is bad. Then, adding multi-dataset training can improve the performance. This can demonstrate the advantages of the proposed method.\n\n7.Authors should give an evaluation about efficiency. In AD scenes, real-time application is crucial."}, "questions": {"value": "Please see the weakness parts. I think the current version cannot backup the zero-shot claim. In contrast, Zero-msf (CVPR25) can predict scene flow from any online videos."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "I3YbZ1nxAM", "forum": "T4ipdPCY1d", "replyto": "T4ipdPCY1d", "signatures": ["ICLR.cc/2026/Conference/Submission5165/Reviewer_xV5t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5165/Reviewer_xV5t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577627042, "cdate": 1761577627042, "tmdate": 1762917923492, "mdate": 1762917923492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "oNNdC1gLmU", "forum": "T4ipdPCY1d", "replyto": "T4ipdPCY1d", "signatures": ["ICLR.cc/2026/Conference/Submission5165/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5165/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762920406696, "cdate": 1762920406696, "tmdate": 1762920406696, "mdate": 1762920406696, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that the lidar scene flow estimation can be trained together on multiple datasets and achieves better generalizable performance across different datasets. The authors have proposed to re-train the previous SOTA methods across different lidar scene flow datasets, such as Argoverse2, Waymo, nuScenes, and TruckScenes, improving their performance on “out-of-distribution” testing. In addition, the authors also propose to use NSFP to finetune the scene flow estimation when testing on a specific dataset, further mitigating the residual errors. Using this cross-domain dataset, re-training and test-time fine-tuning, the proposed technique has achieved good performance across different lidar scene flow datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The re-training and fine-tuning technique used in the paper is good for cross-domain lidar scene flow estimation.\n- Focusing on data distribution of lidar scene flow estimation and how to mitigate the cross-domain training is interesting and has a potential benefit for the autonomous driving research."}, "weaknesses": {"value": "- The authors have stated that one of the contributions of the paper is that “dataset-specific scene flow models already achieve strong performance across datasets, challenging conventional wisdom about LiDAR-based cross-domain generalization”. However, when we talk about cross-domain generalization in scene flow, we often argue about cross-domain distribution shifts, especially for synthetic datasets and real-world datasets. When focusing on lidar scene flow cross-domain generalization, we discuss the sensor difference, the long-tailed distribution in motions, etc. As also shown in the paper, Argoverse2 and Waymo datasets share a similar data distribution with the same lidar sampling rate, except for the fast-moving objects. And the estimation of previous data-specific training methods on both datasets shares a similar performance. However, nuScenes has a much lower sampling rate, while TruckScenes has more fast-moving objects. The estimation of previous data-specific training methods cannot generalize well. This does not “challenge conventional wisdom about LiDAR-based cross-domain generalization”. In addition, to make this contribution valid, the authors should provide more detailed analysis and discussions.\n- The paper has shown different training settings with different methods on different datasets. However, to further explain the validation of the quantitative results, there lacks reasonable analysis, such as theoretical analysis, data distribution analysis, training analysis, etc. Otherwise, this paper only shows the results/performance, but lacks reasoning and intuitive discussions, making it more like a technical report without a detailed analysis.\n- When looking into the failure cases, it seems that the proposed re-training on multiple datasets does not solve the existing issues in the current lidar scene flow methods.\n- The performance on TruckScenes is not “universally good” for improving the original methods.\n\nMinor suggestions:\n- The reference link is not working in the paper, and there might be some omitted references.\n- Figure 1 could add a zoomed-in figure to show the moving objects.\n- Table 6 omitted the performance for the original SSF method."}, "questions": {"value": "Please see the above comments for details. I encourage the authors to look into the grounded analysis of this cross-domain generalization problem for the lidar scene flow and revise the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sHUcbRXhwR", "forum": "T4ipdPCY1d", "replyto": "T4ipdPCY1d", "signatures": ["ICLR.cc/2026/Conference/Submission5165/Reviewer_vim5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5165/Reviewer_vim5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961746618, "cdate": 1761961746618, "tmdate": 1762917923067, "mdate": 1762917923067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the conventional wisdom in LiDAR perception. Previously, researchers think that multi-dataset training is harmful due to domain gaps in sensor setup and environment. The authors argue that scene flow is less sensitive to these domain shifts compared to high-level tasks like detection and segmentation. They propose UniFlow, a method that involves training scene flow models (Flow4D, SSF) on a mixture of multiple autonomous driving datasets (Argoverse 2, Waymo, nuScenes). The key finding is that this multi-dataset training not only improves in-domain performance but also leads to strong zero-shot generalization (e.g., TruckScenes)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The key finding that multi-dataset training is beneficial for LiDAR scene flow is interesting, which contradicts  common beliefs in the field. This insight could significantly influence how the community approaches data scaling and generalization for 3D motion tasks.\n- The experimental results are impressive to me. The UniFlow achieves new state-of-the-art or competitive performance on several major benchmarks (Waymo, nuScenes, Argoverse 2) and demonstrates zero-shot capabilities on TruckScenes. \n- The claimed pipeline is easy to follow."}, "weaknesses": {"value": "- The most significant concern to me is the lack of an investigation into why scene flow is less sensitive to domain gaps (e.g., scenes, LiDAR config). The authors explained that  this is because scene flow is a low-level task. A key experiment would be to take a single dataset (e.g., Waymo) and artificially degrade the point clouds (e.g., from 64-beam to 32-beam) to create a controlled domain gap. Training and evaluating on this controlled setup would isolate the effect of point density and provide much stronger evidence for their hypothesis.\n- Using mixtures of training datasets is a common practice for  low-level domains such as  depth estimation. For AD, many works train their monocular depth estimator in KITTI, nuScenes, DAAD, and etc.\n- The performance gains might be primarily attributable to the increased volume and diversity of data. It is plausible that scene flow is a data-hungry task where any additional data is beneficial, and that the harmful effects of domain gap would become apparent if even more heterogeneous datasets were added. \n- The work is compared to dataset-specific models. A more robust evaluation would include comparisons with modern domain adaptation."}, "questions": {"value": "- Could you design an experiment to disentangle the benefit of simply having more data from the benefit of overcoming a domain gap? For instance, does adding more data from a single dataset provide similar gains to adding a different dataset?\n\n- Have you considered or experimented with any domain adaptation methods? How does your multi-dataset training approach compare?\n\n- Given the computational cost of NSFP, can you discuss the real-world applicability of your full pipeline?\n\nI give a minor rejection at this stage because I have some concerns about data augmentation. I hope the authors can discuss these questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nkn7AehzNs", "forum": "T4ipdPCY1d", "replyto": "T4ipdPCY1d", "signatures": ["ICLR.cc/2026/Conference/Submission5165/Reviewer_YXRw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5165/Reviewer_YXRw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975760601, "cdate": 1761975760601, "tmdate": 1762917922394, "mdate": 1762917922394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UniFlow, a “frustratingly simple” recipe for zero-shot LiDAR scene flow via cross-dataset training. The authors unify multiple AV datasets (Argoverse 2, Waymo, nuScenes; TruckScenes for zero-shot) by standardizing frame rate and annotations, then retrain existing feedforward SOTA models (e.g., Flow4D, SSF) jointly across datasets without dataset-specific reweighting or bespoke augmentations. UniFlow achieves SOTA on nuScenes and Waymo and strong performance on AV2, while also exhibiting remarkable zero-shot generalization on the out-of-domain TruckScenes benchmark. The paper further introduces a test-time optimization (TTO) add-on—residual refinement using NSFP—trading extra latency for small gains on fast-moving objects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. UniFlow demonstrates consistent improvements across AV2/nuScenes/Waymo when training jointly, overturning the common belief that LiDAR cross-dataset training hurts performance for perception tasks.\n\n2. UniFlow shows strong zero-shot ability. A single checkpoint generalizes to TruckScenes (highway, truck platform) with big margins under speed-normalized metrics—despite never seeing that dataset during training.\n\n3. The approach reuses prior SOTA architectures (Flow4D, SSF) and shows that cross-data training plus lightweight augmentations already confer large performance benefits—highlighting the scaling value of diverse data over bespoke architecture.\n\n4. Thorough comparisons and ablations across datasets/frame rates, plus a failure-case analysis and scaling-law study.\n\n5. Presentation is clear, with intuitive figures and tables."}, "weaknesses": {"value": "1. Test-time optimization overhead. The NSFP residual-refinement step adds latency and compute; it yields modest improvements on fast movers but may undermine real-time applicability.\n\n2. The paper argues that low-level tasks such as scene flow are less sensitive to sensor differences and benefit more from cross-dataset training. However, the underlying reasons for their strong performance—especially compared to label-intensive tasks like detection and segmentation—call for more causal analysis beyond the compelling velocity-distribution evidence provided.\nOn the other hand, it is well-established in the literature that high-level tasks like 3D detection can achieve improved performance through multi-dataset training. This makes it particularly interesting why low-level tasks—previously claimed not to benefit from such training—now demonstrate clear gains. The shift deserves further explanation.\n\n3. The paper unifies frame rate to 10 Hz (interpolation) and applies simple augmentations; tighter ablations on these standardization choices could clarify which components matter most."}, "questions": {"value": "How do rain/occlusion/noise distributions affect failures? Could adaptive confidence weighting or uncertainty-aware post-processing mitigate the artifacts shown?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TdGLJdPCt8", "forum": "T4ipdPCY1d", "replyto": "T4ipdPCY1d", "signatures": ["ICLR.cc/2026/Conference/Submission5165/Reviewer_GBR1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5165/Reviewer_GBR1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984340028, "cdate": 1761984340028, "tmdate": 1762917919446, "mdate": 1762917919446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}