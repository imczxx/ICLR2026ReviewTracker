{"id": "o4VooeFajh", "number": 8814, "cdate": 1758099013538, "mdate": 1762950218681, "content": {"title": "Dual-Stage Gradient Projection Based Continual Learning: Enhancing Plasticity and Preserving Stability", "abstract": "In continual learning, gradient projection algorithms avoid forgetting by projecting the gradient onto the orthogonal complement of the feature space of previous tasks, thereby ensuring the model’s stability. However, strict orthogonal projection can cause the projected gradient to deviate sharply from the original gradient, damaging the model’s learning ability to new tasks and reducing its plasticity. Gradient-projection methods that relax the orthogonality constraint alleviate the deviation introduced by strict projection, yet the degree of gradient distortion remains large and the model’s plasticity still needs improvement. To address such an issue, we propose a continual-learning method based on two-stage gradient projection that improves the model’s plasticity for new tasks while preserving its stability on previous tasks. Specifically, in the first stage, we design a loss-sensitive space (LSS) regularization term (soft regularization) on top of the cross-entropy loss to constrain the gradient to update as closely as possible along directions orthogonal to the feature space of previous tasks, thereby maintaining plasticity. In the second stage, a scaled projection (hard projection) further constrains the gradient to update along directions approximately orthogonal to the feature space of previous tasks, thus ensuring stability. Experimental results on three benchmark image classification datasets demonstrate that our method, for the first time, reduces the gap between the achieved classification accuracy and the task-specific upper bound (multitask) to within roughly 2\\%, indicating that the model possesses both strong plasticity and stability.", "tldr": "We propose a loss-sensitive, two-stage gradient projection for continual learning: it steers updates toward directions nearly orthogonal to the protected subspace, reducing direction and magnitude distortion from the final projection.", "keywords": ["Continual Learning; Gradient Projection; Catastrophic Forgetting; Stability–Plasticity Trade-off; Curvature Information"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b24bd51725a4b96f953c5c62c7abe507f9424c66.pdf", "supplementary_material": "/attachment/8577556056a2a4489a03899f447e4194aa77cb4d.zip"}, "replies": [{"content": {"summary": {"value": "This paper develops a new orthogonal gradient projection–based method, which adopts a two-stage approach: first, it uses a regularization constraint to encourage the gradient direction of the new task to align with the orthogonal complement space; second, it performs orthogonal projection to prevent task conflicts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper argue that existing orthogonal projection methods may harm the plasticity of new tasks when gradients are constrained to be strictly orthogonal.\n- It proposes to design the gradient regularization term based on the Fisher Information Matrix. \n- The paper is well-structured and clearly written."}, "weaknesses": {"value": "- In each optimization step, this paper requires two rounds of gradient computation (two-stage process), which significantly increases time and memory costs. However, the paper lacks a detailed comparison of time and memory efficiency with existing orthogonal projection methods.\n- All experiments are conducted on small-scale datasets such as CIFAR-100 and MiniImageNet, lacking validation on larger and more challenging datasets like ImageNet.\n- The method is only verified on small or outdated architectures such as AlexNet, LeNet, and ResNet, without evaluation on Transformer-based architectures.\n- The paper lacks sufficient discussion and comparison with related works [1–3].\n- The font in Figure 4 is inconsistent with the main text.\n\n[1]Liang, Yan-Shuo, and Wu-Jun Li. \"Adaptive plasticity improvement for continual learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\n[2]Yang, E., Shen, L., Wang, Z., Liu, S., Guo, G., & Wang, X. (2023). Data augmented flatness-aware gradient projection for continual learning. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 5630-5639).\n\n[3]Yang, Z., Yang, Z., Liu, Y., Li, P., & Liu, Y. (2023). Restricted orthogonal gradient projection for continual learning. AI Open, 4, 98-110."}, "questions": {"value": "- Does the regularization term in Equation (4) constrain the performance of the new task compared with directly using the cross-entropy loss?\n- In Figure 4, why is the lambda hyperparameter set to such a large value? How is the balance between the regularization loss and the original cross-entropy loss achieved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rbRgt8rAGw", "forum": "o4VooeFajh", "replyto": "o4VooeFajh", "signatures": ["ICLR.cc/2026/Conference/Submission8814/Reviewer_3g4e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8814/Reviewer_3g4e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728423444, "cdate": 1761728423444, "tmdate": 1762920583142, "mdate": 1762920583142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "FGn9t27EOZ", "forum": "o4VooeFajh", "replyto": "o4VooeFajh", "signatures": ["ICLR.cc/2026/Conference/Submission8814/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8814/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762950217782, "cdate": 1762950217782, "tmdate": 1762950217782, "mdate": 1762950217782, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a gradient-projection-based task-incremental learning method. The authors argue that the strict orthogonal projection used in existing methods causes deviations in parameter updates, which negatively affect plasticity. To address this issue, they propose a loss-sensitive space (LSS) regularization term that encourages gradients to update along directions approximately orthogonal to the feature space of previous tasks. Since LSS gradually aligns the parameters during training, the gradients become partially orthogonal in advance, making subsequent orthogonal gradient projection easier and less distortive. The proposed method is evaluated on CIFAR-100 and MiniImageNet using AlexNet, LeNet, and a reduced ResNet-18."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is well designed and supported by mathematical justification.\n\n- The motivation is reasonable: direct gradient projection can indeed distort the optimal update direction."}, "weaknesses": {"value": "- Although LSS regularization encourages orthogonal alignment and alleviates the drawbacks of hard gradient projection, it does not fully resolve the authors’ main concern about gradient deviation, since gradients must still be moved into the orthogonal space.\n\n- The technical novelty is limited. The contribution lies primarily in introducing a single regularization term, which—while conceptually fine—is not particularly innovative or significant.\n\n- The experimental design requires major improvement. The authors use AlexNet and LeNet, which, while historically important, are outdated. They should employ at least ResNet-18 for all datasets and compare results against baselines using that architecture. Ideally, more modern backbones such as transformers should also be tested.\n\n- The performance results are not impressive. For example, [1] reports 95.3% accuracy on Split-CIFAR-10 with ResNet-18 for task-incremental learning, whereas the proposed method achieves only 78.05% using AlexNet. The authors should evaluate their approach on Split-CIFAR-100 using ResNet-18 and compare it with this state-of-the-art method to justify their contribution.\n\n[1] A Theoretical Study on Solving Continual Learning"}, "questions": {"value": "Please refer to my comments in weaknesses\n\nAdditionally, \n\nI think the gradient-projection method is inherently limited by the dimensionality of each subspace, which increases with the number of tasks. How efficiently does it utilize the subspace? Could you try evaluating it on a more challenging dataset such as Tiny-ImageNet, so that the proposed method is demonstrated on a problem with more classes per task and a larger number of tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UZWJmz0S8g", "forum": "o4VooeFajh", "replyto": "o4VooeFajh", "signatures": ["ICLR.cc/2026/Conference/Submission8814/Reviewer_NrB1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8814/Reviewer_NrB1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948869949, "cdate": 1761948869949, "tmdate": 1762920582829, "mdate": 1762920582829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Dual-Stage Gradient Projection, a continual learning method aiming to improve plasticity in gradient-projection-based approaches while maintaining stability. The authors argue that conventional projection methods such as GPM and SGP cause strong gradient distortion when enforcing strict orthogonality to past-task feature subspaces. To address this, they introduce a two-stage scheme: a soft regularization stage that guides gradients toward directions approximately orthogonal to past subspaces using a loss-sensitive space (LSS), followed by a hard projection stage that applies a conventional orthogonal projection with minimal distortion. The LSS is computed using curvature information (approximated via Fisher matrices) from previous tasks to estimate the importance of each basis direction. Experiments on Split CIFAR-100, CIFAR-100 Superclass, and Split MiniImageNet show that proposed method improves average accuracy and reduces forgetting compared to several baselines such as SGP, TRGP, and GPM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important and well-known problem in continual learning: the plasticity–stability trade-off in gradient-projection methods. The general intuition of minimizing projection-induced distortion before enforcing orthogonality is conceptually appealing and addresses a legitimate limitation of prior work. The method is well-positioned within the literature, and the authors include a range of comparisons against relevant baselines, showing consistent accuracy improvements on multiple datasets. The idea of using a loss-sensitive scaling space to modulate gradient alignment through curvature information is theoretically interesting, and the overall framework appears compatible with existing projection-based algorithms."}, "weaknesses": {"value": "The presentation of the paper is often unclear, and several crucial aspects of the method are underspecified or insufficiently justified. The role and necessity of the loss-sensitive space (LSS) are not well explained beyond formal equations. Theoretical analysis is largely superficial: the paper presents a theorem relating curvature information to projection scaling but provides almost no discussion or intuition about how this contributes to reducing gradient distortion or improving learning dynamics. Merely stating the theorem without elaborating on its implications or limitations leaves the reader uncertain about why the approach should work in practice.\n\nThe implementation details are also ambiguous. Since the proposed total loss includes a regularization term that depends on the gradient of the cross-entropy loss, it is unclear whether this formulation requires computing second-order derivatives (Hessian–vector products or Fisher estimates) during backpropagation. If so, this would entail a considerable computational and memory cost compared to first-order projection methods such as GPM or SGP. However, no analysis or empirical measurement of runtime or memory overhead is provided, which makes it difficult to assess the method’s practicality.\n\nThe ablation studies are confusing and lack sufficient explanation. For example, the “MTL + soft” configuration is described but not well motivated,  in multitask learning, all tasks are trained jointly, so it is unclear how a continual setting with “MTL + soft” is implemented or what it represents conceptually. Furthermore, while the paper reports numerical gains, it does not adequately analyze why the improvements occur or how sensitive they are to specific hyperparameters or architecture choices.\nIn addition, the paper omits important related work and citations. For instance, more recent methods that address gradient-space regularization or continual subspace updates (e.g., CODE-CL [1], DFGP[2], and others) are missing. This omission weakens the paper’s positioning in the current literature and makes it harder to assess its novelty relative to ongoing work.\n\nFinally, the clarity and organization of the text need significant improvement. Many sections repeat material or present overly long mathematical derivations without intuitive commentary, while the motivation and implications of each design choice remain vague. The result is a paper that feels formally complex but conceptually underexplained.\n\n[1] Apolinario, M.P., Choudhary, S. and Roy, K., 2025. CODE-CL: Conceptor-Based Gradient Projection for Deep Continual Learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 775-784).\n\n[2] Yang, E., Shen, L., Wang, Z., Liu, S., Guo, G. and Wang, X., 2023. Data augmented flatness-aware gradient projection for continual learning. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 5630-5639)."}, "questions": {"value": "•\tHow exactly is the loss-sensitive regularization implemented in practice? Does the optimization require computing second-order derivatives or Hessian–vector products, and if so, what is the additional time and memory cost compared to GPM or SGP?\n\n•\tWhat specific benefit does the LSS provide compared to simpler soft projection regularization (e.g., scaling by singular values or energy norms)? Could similar effects be achieved with first-order measures of importance?\n\n•\tTheoretical analysis (Theorem 3.1) is presented without a clear interpretation. Can the authors provide an intuitive explanation or visualization showing why the curvature-based scaling reduces projection distortion?\n\n•\tIn the ablation study, what does “MTL + soft” correspond to? How is it implemented if multitask learning already assumes access to all task data simultaneously?\n\n•\tThe paper should include computational complexity and runtime comparisons against prior works. How does the cost of constructing and updating the Fisher-based LSS scale with the number of tasks and layers?\n\n•\tSeveral relevant prior works, such as CODE-CL and other gradient-projection continual learning methods, are missing in the references. Could the authors clarify how their approach differs conceptually and algorithmically from these recent developments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZLMC9w3PoP", "forum": "o4VooeFajh", "replyto": "o4VooeFajh", "signatures": ["ICLR.cc/2026/Conference/Submission8814/Reviewer_Gkbc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8814/Reviewer_Gkbc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998610463, "cdate": 1761998610463, "tmdate": 1762920582367, "mdate": 1762920582367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a two-stage gradient projection method to effectively mitigate forgetting in continual learning. In the first stage, soft regularization is applied, and in the second stage, hard projection is performed to balance plasticity and stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The approach using LSS regularization intuitively encourages more plastic updates for new tasks, as clearly illustrated in Fig. 1, effectively addressing the problem with a simple idea.\n- The paper is well-structured and clearly written.\n- A wide range of experiments, including ablation studies, are conducted to support the proposed method.\n\nThe paper is clearly presented, methodologically sound, and supported by comprehensive experiments. The proposed two-stage gradient projection method is conceptually simple yet effective in addressing the plasticity–stability trade-off in continual learning. Minor presentation issues do not detract from the overall quality."}, "weaknesses": {"value": "- A minor issue, but the paper would benefit from explicitly defining symbols and abbreviations before their first use. For instance, terms like GPM and SGP in Fig. 1 can be inferred from context but are not clearly defined, while LSS is redundantly defined multiple times, which slightly lowers the readability and overall polish of the paper.\n- Excessive use of bold formatting throughout the paper reduces visual readability."}, "questions": {"value": "- **Deco-SAM hyperparameter clarity.** The paper lacks detail and sensitivity analysis for **τ**. Could you report the default τ, the rationale for its choice, and a brief ablation (e.g., τ ∈ {…}) showing how **worst-class accuracy**, **class-wise variance**, and **PGD/AA robustness** change across τ and learning-rate schedules?\n- **ε-sweep robustness.** Experiments fix the perturbation budget at **ε = 8/255** under PGD-20 and AutoAttack. Have you evaluated whether the **fairness improvements persist across different ε values** (i.e., varying attack strengths)? A compact ε-sweep would clarify the stability of the effect."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XGoGIopx8x", "forum": "o4VooeFajh", "replyto": "o4VooeFajh", "signatures": ["ICLR.cc/2026/Conference/Submission8814/Reviewer_JboY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8814/Reviewer_JboY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762089631285, "cdate": 1762089631285, "tmdate": 1762920581972, "mdate": 1762920581972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}