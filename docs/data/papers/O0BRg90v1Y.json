{"id": "O0BRg90v1Y", "number": 15131, "cdate": 1758248060125, "mdate": 1759897326224, "content": {"title": "From Compression to Specialization: An Information-Preserving Approach for Dense to Mixture-of-Experts Construction", "abstract": "The high cost of training Mixture-of-Experts (MoE) models from scratch has spurred interest in converting pre-trained dense models into sparse MoE models.\nHowever, existing dense-to-sparse MoE methods are constrained by a fundamental trade-off between initial expert diversity and knowledge inheritance, often requiring extensive post-training to be effective.\nWe address this by proposing a new expert construction paradigm that repurposes data-driven model compression, and validate that low-rank factorization is uniquely effective at balancing this trade-off.\nBased on this insight, we introduce MIDAS, a framework that crafts specialized experts by applying low-rank factorization to a base model, guided by distinct calibration datasets.\nUnder limited compute budgets, MIDAS significantly outperforms existing dense-to-sparse approaches through a parameter-efficient strategy that trains only its gating network and low-rank adapters.\nCrucially, we demonstrate that MIDAS improves model stability by mitigating the severe load imbalance found in prior work, while also producing experts with clear, interpretable specializations that align with established Transformer functional theory.\nOverall, MIDAS presents a robust and efficient pathway for MoE construction, addressing the diversity-knowledge trade-off through an information-preserving approach.", "tldr": "", "keywords": ["Mixture-of-Experts", "Dense-to-Sparse Conversion", "Data-Driven Model Compression", "Expert Construction", "Parameter-Efficient Training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8757b75f84d4724af1acb5108e9c7a8f9795ce8b.pdf", "supplementary_material": "/attachment/78fc8512481302ca2c7806c368369b9e415d9987.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MIDAS, a method that transforms dense LLMs into sparse Mixture-of-Experts models via low-rank decomposition and parameter-efficient fine-tuning. Using Llama-2-7B as the base, each expert is derived from calibration data, followed by 1.3 B-token CPT and 0.4 B-token SFT. The authors claim improved data efficiency (DES) and specialization with minimal training cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Framing: interprets low-rank compression as a route to expert specialization.\n2. Analyses on expert load distribution and calibration sensitivity.\n3. Lightweight tuning scheme using LoRA is practical in principle."}, "weaknesses": {"value": "1. DES metric: Since all MIDAS experiments are conducted using Llama-2 as a backbone, it is inappropriate to claim superiority over Llama-2 in terms of DES.\n2. Accuracy degradation ignored: MIDAS (CPT + SFT) consistently underperforms the Llama-2 baseline on several downstream tasks.\n3. Lack of compute transparency: The paper fails to report fundamental cost statistics such as FLOPs or GPU hours for training.\n4. Outdated setup: All experiments are limited to Llama-2-7B. Stronger modern dense models, such as Llama-3 or Qwen-3, are not tested, leaving it unclear whether the claimed benefits of MIDAS would hold with more capable backbones.\n5. Lack of task coverage: The evaluation omits critical domains such as mathematical and coding reasoning (e.g., HumanEval+, LiveCodeBench, MATH-500, BBH).\n6. Missing relevant baselines: Contemporary dense-to-sparse conversion methods such as Sparse Upcycling and Drop-Upcycling are not included as baselines under the same computational budget, making it difficult to contextualize MIDASâ€™s effectiveness."}, "questions": {"value": "Please clarify the points raised in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pBDmIVMlU3", "forum": "O0BRg90v1Y", "replyto": "O0BRg90v1Y", "signatures": ["ICLR.cc/2026/Conference/Submission15131/Reviewer_mSEG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15131/Reviewer_mSEG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919666694, "cdate": 1761919666694, "tmdate": 1762925448511, "mdate": 1762925448511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an expert-initialization method for converting dense models to MoE models. Specifically, the paper proposes to use different calibration datasets to initialize different experts via low-rank factorization. The specially initialized model is trained to close the gap between the MoE model and its parent dense model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Interesting observation about the sensitivity of data-dependent compression of LLMs on the selection of calibration data, specifically for SVD-based compression\n\n2. The paper is easy to follow"}, "weaknesses": {"value": "1. The main goal of the paper is to convert a dense model into an MoE model. The motivation is that training an MoE model from scratch is challenging. From this perspective, the paper didn't provide any comparison with MoE models trained from scratch.\n\n2. It has already been established in the literature that training MoE is computationally efficient. Therefore, to achieve similar performance, a dense model needs far more training compute. However, the proposed method loses performance significantly compared to its parent dense model, even after training the initialized MoE model.\n\n3. The proposed method can't outperform other dense-to-MoE baselines, despite having a significant load imbalance for the baseline.\n\n4. The proposed expert-initialization method heavily depends on the diversity of calibration data. Therefore, the unavailability of diverse calibration data may undermine the effectiveness of the proposed method. \n\n5. No formal theoretical justification has been provided for the proposed initialization of the experts."}, "questions": {"value": "1. What is the Sharing-Inter method? I can't find any citation of Sharing-Inter in the paper.\n\n2. Can the authors provide a clear justification of why one should convert a dense model into MoE, rather than training MoE from scratch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kDd71lz35n", "forum": "O0BRg90v1Y", "replyto": "O0BRg90v1Y", "signatures": ["ICLR.cc/2026/Conference/Submission15131/Reviewer_uCQf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15131/Reviewer_uCQf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956185424, "cdate": 1761956185424, "tmdate": 1762925448015, "mdate": 1762925448015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge covnerting pre-trained dense LLMs into sparse MoE architectures. The authors identify a trade-off between inheriting knowledge from the base models vs diversity of expert modules. They propose an approach that uses low-rank factorization (SVD) with distinct calibration datasets to construct specialized experts, demonstrating that the approach exhibits high sensitivity to calibration data, enabling diversity, while preserving knowledge better in comparison to methods such as structured pruning. Experiments seem to show competitive performance, data efficiency, and improved load balancing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the framing of the problem is intuitive, and a preliminary analysis demonstrates the choice for using SVD and low rank decommposition in this manner\n\n- Experimental analysis covers 12 benchmark datasets\n\n- Section 4.5 shows useful analysis of expert specialization (heatmaps)\n\n- Load balancing insights reveal stability issues in prior works, demonstrating further the advantage of the proposed approach"}, "weaknesses": {"value": "-The baseline comparisons are limited. The paper does not compare against a wider range of recent upcycled-MoE baselines such as Sparse Upcycling (Komatsuzaki et al., 2023), Drop-Upcycling (Nakamura et al., 2025), Auxiliary-Loss-Free Load Balancing (Wang et al., 2024).\n\n- All experiments only have 4 experts - no expert number ablation. No ablation studies on key desgin choices (E.g., lora rank)\n\n- The compression ratio is set to 25%, but this is not a well explained choice\n\n- It is claimed that sharing-inter will degrade with continued training due to load imbalance. Can experiments be provided that validate this?\n\n- There is no indication about the proper choice of datasets and how this choice induces specialization equivalent to training MoEs from scratch. What if the test examples do not clearly match calibration datasets?\n\n- It appears that the method does not allow for any overlap between experts (no shared expert). Could this be a downside in some cases?\n\n- There is no clear quantitative comparison of total computation (construction, training, inference) with other MoE upcycling methods. \n\n- How sensitive is performance to the number and selection of fine-tuning datasets used to form experts? Would including additional baselines such as DeepSeek Balancing or BTX change the conclusions?What is the trade-off between expert diversity and computational cost when scaling to more fine-tuning datasets"}, "questions": {"value": "please see weaknesses above!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OEic98O3n5", "forum": "O0BRg90v1Y", "replyto": "O0BRg90v1Y", "signatures": ["ICLR.cc/2026/Conference/Submission15131/Reviewer_6iog"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15131/Reviewer_6iog"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762192001397, "cdate": 1762192001397, "tmdate": 1762925447436, "mdate": 1762925447436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}