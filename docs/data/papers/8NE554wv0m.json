{"id": "8NE554wv0m", "number": 17411, "cdate": 1758275691871, "mdate": 1759897177015, "content": {"title": "ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix Factorization", "abstract": "Large language models (LLMs) present significant deployment challenges due to their immense computational and memory requirements. While semi-structured pruning, particularly 2:4 sparsity, offers a path to practical hardware acceleration, existing methods often incur substantial performance degradation. To bridge this gap, we introduce ARMOR: (Adaptive Representation with Matrix-factORization), a novel one-shot post-training pruning algorithm. Instead of directly pruning weights, ARMOR factorizes each weight matrix into a 2:4 sparse\ncore wrapped by two low-overhead, block diagonal matrices. These wrappers act as efficient pre- and post-transformation error correctors, offering greater flexibility to preserve model quality compared to conventional 2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen through a block coordinate descent algorithm that minimizes a layer-wise proxy loss. We theoretically prove this optimization is guaranteed to converge to a solution with a proxy loss less than or equal to state-of-the-art pruning algorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and Qwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and significantly outperforms state-of-the-art 2:4 pruning methods across a wide range of downstream tasks and perplexity evaluations. ARMOR achieves this superior performance while retaining the inference speedups and substantial memory usage reductions of 2:4 pruning, establishing a more effective trade-off between model compression and task accuracy", "tldr": "New LLM pruning method that utilizes block diagonal matricies to preserve more model performance post-pruning.", "keywords": ["Pruning", "Model Compression", "Block Coordinate Descent"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b52df288e00dbfc16cf2b67286e9d9e78486dce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors address the trade-off between hardware acceleration and performance degradation in semi-structured pruning. They propose an adaptive matrix factorization framework that decomposes the original weight matrix W into three components (A, W’xM, B). Then they employ a block coordinate descent algorithm to iteratively optimize these parameters. Experimental results on LLaMA and Qwen demonstrate the effectiveness of the proposed approach compared to existing methods. The inference analysis further indicates that the method introduces a slight computational overhead relative to standard 2:4 pruning. Moreover, it provides a theoretical proof to guarantee the algorithm’s convergence.\n\nOverall, the paper presents a decomposable and learnable 2:4 pruning scheme for LLMs, offering a meaningful alternative for LLM compression research. Therefore, I lean to marginally accept this paper, and I would be willing to raise my score if the authors address my main concerns during the rebuttal phase."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\nThis paper proposes a decomposable and learnable 2:4 pruning scheme for large language models (LLMs), which is both novel and valuable for LLM compression research. It provides a promising approach to enhance the performance of 2:4 pruning while preserving practical inference efficiency.\n\nQuality:\nThe paper is well-written and well-organized, presenting its ideas in a coherent and logical manner.\n\nClarity:\nThe content is clearly presented, and the mathematical formulations are easy to read and follow.\n\nSignificance:\nThe study introduces an alternative approach to improving the performance of 2:4 pruning for LLMs, making it a meaningful contribution toward efficient and practical model deployment."}, "weaknesses": {"value": "Main concerns:\n1. The paper’s main weakness lies in limited novelty relative to existing learnable semi-structured pruning frameworks. The proposed method introduces learnable transformations (A, W' B) to improve 2:4 LLM pruning, but this concept is similar to prior learnable pruning works such as MaskLLM [1], RotPruner [2], and DenoiseRotator [3], which also learn rotation or masking matrices to enhance pruning robustness and recovery. To convincingly demonstrate novelty, the authors should explicitly position their method against these approaches—highlighting what differentiates their transformation learning (e.g., new optimization formulation, stability, or efficiency aspects).\n\n2. The main tables compare against SparseGPT, Wanda, and NoWag, which are not the current state-of-the-art methods for semi-structured pruning. Without comparisons to MaskLLM, RotPruner, and DenoiseRotator, it is difficult to assess the real performance improvement or contribution of the proposed method.\n\n3. It lacks insufficient explanation of why a block-diagonal structure is preferred over a simple diagonal form for A and B matrix.\n\n4. the inference acceleration results appear underwhelming. For 2:4 sparsity, the theoretical speedup ratio should ideally approach 2×, but the reported gain is marginal. This suggests the need for more detailed discussion or optimization strategies to bridge the gap between theoretical sparsity and actual hardware acceleration.\n\nMinor comments:\n1. Line 78-79 syntax error, “Thus it is difficult to translate the theoretical model size reductions into practical inference speedups is difficult”\n\n\n[1] MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models. NeurIPS 2024.  \n[2] RotPruner: Large Language Model Pruning in Rotated Space.  \n[3] DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration. NeurIPS 2025."}, "questions": {"value": "1. Comparison with SOTA methods:\nCan the authors include results comparing their approach with recent learnable semi-structured pruning methods such as MaskLLM [1], RotPruner [2], and DenoiseRotator [3]? This would help clarify whether the proposed learnable transformation provides measurable benefits beyond these baselines.\n\n2. Generality across N:M patterns:\nDoes the proposed method extend naturally to other N:M pruning settings (e.g., 4:8, 5:8, or 6:8)? If so, how should hyperparameters like block size and iteration count be adjusted?\n\n3. Structure of transformation matrices:\nWhat is the rationale behind adopting a block-diagonal structure for matrices A and B? Would using a pure diagonal structure simplify computation while maintaining comparable performance?\n\n4. Inference efficiency:\nThe observed inference speedup for 2:4 sparsity is much smaller than the theoretical 2× expectation. Are there software or hardware factors limiting this gain? Could kernel fusion or custom sparse-GEMM implementations improve it?\n\nMinor issue:\nPlease correct the syntactic error at lines 78–79 (“...speedups is difficult”)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VIlm53wuRu", "forum": "8NE554wv0m", "replyto": "8NE554wv0m", "signatures": ["ICLR.cc/2026/Conference/Submission17411/Reviewer_G27K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17411/Reviewer_G27K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761037468106, "cdate": 1761037468106, "tmdate": 1762927308940, "mdate": 1762927308940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ARMOR, a post-training semi-structured pruning method for LLMs that factorizes each weight matrix into a 2:4 sparse core surrounded by block-diagonal transformation matrices (A and B) to mitigate accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The factorization approach is a novel way to add flexibility to rigid 2:4 pruning, potentially bridging the gap between unstructured and semi-structured methods.\n2. The theoretical analysis in Appendix C is sound."}, "weaknesses": {"value": "1. The proposed ARMOR method does not support unstructured pruning. It is specifically designed for semi-structured pruning (with a focus on 2:4 sparsity patterns), as the core optimization algorithm relies on the constrained group structure of semi-structured masks to remain computationally tractable. However, baseline methods such as Wanda support unstructured pruning.\n2. The related work is sufficient. There are other refinement methods that also mitigate the gap between a dense model and a pruned model. For example, low-rank refinement [1, 2] and adaptive layer-wise sparsity control [3].\n3. Minor concern: evaluations are restricted to dense models.\n\n[1] Y. Li et al., “LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation,” 2023.  \n[2] L. Shen, A. Tang, Y. Luo, T. Sun, H. Hu, and X. Cao, “Targeted Low-rank Refinement: Enhancing Sparse Language Model with Precision,”  2025.   \n[3] L. Yin et al., “Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity,” 2024"}, "questions": {"value": "1. Does the proposed method support other models, such as MoE models? Does ARMOR degrade on them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ex9fdFy6Sw", "forum": "8NE554wv0m", "replyto": "8NE554wv0m", "signatures": ["ICLR.cc/2026/Conference/Submission17411/Reviewer_Ze6L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17411/Reviewer_Ze6L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730436622, "cdate": 1761730436622, "tmdate": 1762927308272, "mdate": 1762927308272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ARMOR, a one-shot 2:4 semi-structured pruning method for large language models. Instead of directly pruning weights, each matrix is factorized into a 2:4 sparse core surrounded by two block-diagonal “wrapper” matrices that act as lightweight error correctors. The method is optimized via a block coordinate descent alternating between gradient updates for the wrappers and greedy least-squares updates for the sparse core. ARMOR achieves large accuracy gains over prior 2:4 pruning methods (Wanda, SparseGPT, NoWag-P) while keeping almost identical inference speedups and memory savings. This is an excellent paper that combines a simple yet powerful idea with strong empirical validation and theoretical rigor. It clearly advances the state of the art in semi-structured pruning, a practically important domain for LLM deployment."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Recasts semi-structured pruning as a matrix-factorization problem instead of a masking problem, providing a new theoretical and practical perspective. The idea of block-diagonal wrappers acting as error-correcting transformations is elegant and hardware-friendly.\n2. Consistent, substantial improvements over multiple strong baselines (Wanda, SparseGPT, NoWag-P). Performance nearly matches dense models on some reasoning benchmarks.\n3. Provides a convergence guarantee showing that ARMOR’s proxy loss never exceeds that of the initialization (NoWag-P). Proxy loss design is interpretable and data-aware (weighted Frobenius norm).\n4. Includes both perplexity and task-based evaluation, block-size ablation, and correlation between proxy loss and perplexity. Shows convincing analysis of trade-offs between accuracy, overhead, and speed."}, "weaknesses": {"value": "1. Although the proxy loss correlates with perplexity, a stronger justification (e.g., empirical correlation coefficients across models) would improve interpretability."}, "questions": {"value": "1. How does the performance of ARMOR change with smaller calibration datasets or fewer proxy-loss iterations (e.g., 5k vs 20k)? Is the method robust to low-data calibration settings?\n2. Could the block-diagonal wrappers be fine-tuned jointly with the model in a lightweight manner (e.g., LoRA-style), further improving performance?\n3. Have you explored extending the framework to N:M patterns beyond 2:4? Since the design is general, it may adapt naturally."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6G20UtNKBW", "forum": "8NE554wv0m", "replyto": "8NE554wv0m", "signatures": ["ICLR.cc/2026/Conference/Submission17411/Reviewer_4fMQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17411/Reviewer_4fMQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959533374, "cdate": 1761959533374, "tmdate": 1762927307885, "mdate": 1762927307885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **ARMOR**, a one-shot, *semi-structured* pruning method for N:M (notably **2:4**) that re-parameterizes each weight matrix as\n\\[\n\\hat W \\;=\\; A \\; (W' \\odot M) \\; B,\n\\]\nwhere \\(M\\) is a hardware-friendly **2:4** mask, and \\(A,B\\) are block-diagonal wrappers that act as lightweight, learnable pre-/post-transformations to **correct pruning error. The optimization alternates between: (i) continuous updates of \\(A,B,W'\\) under a NoWag-style proxy loss, and (ii) a greedy sparse-core update that exhaustively selects the best 2-of-4 configuration per group via small least-squares solves. The authors prove monotone convergence of the proxy loss and initialize at **NoWag-P**, guaranteeing a proxy loss no worse than that baseline. Empirically, on Qwen/Llama families, ARMOR reports **consistent accuracy gains over state-of-the-art **2:4** methods (Wanda / SparseGPT / NoWag-P) and provides inference measurements showing it largely retains 2:4 speed/memory benefits despite wrapper overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Theoretically supported and rigorous.** Clear proxy-loss formulation, greedy 2:4 subproblem with closed-form LS updates, and a convergence theorem (monotone decrease of proxy loss; initialization at NoWag-P).\n- **Strong accuracy vs. 2:4 baselines.** On Qwen/Llama, ARMOR consistently outperforms Wanda / SparseGPT / NoWag-P under 2:4 across PPL and downstream tasks; in some settings it narrows the dense–pruned gap by ~50%. \n- **Insightful idea for future work.** Reframing pruning as adaptive factorization (sparse core + learnable wrappers) suggests a promising path for co-design—optimizing both representational form and hardware mapping, not only masks."}, "weaknesses": {"value": "1. **Speedups are modest vs. expectations.** Reported generation speedups are **~1.041×–1.141×** on Qwen-2.5-14B/7B, and **batched MatVec** shows **1.57×** vs **1.86×** for plain 2:4—indicating nontrivial overheads or kernel gaps for wrappers. This is **far below** what a dense model with 50% fewer parameters could achieve, and below the **best-case 2:4** micro-kernel gains. A detailed **profile** of where time is spent (wrapper GEMMs, cache/bandwidth, kernel launch) is needed. :contentReference[oaicite:7]{index=7}  \n2. **Performance degration is significant compared to dense model and far below equal-parameter dense counterparts.** For example, ARMOR-pruned Qwen-2.5-14B can underperform Qwen-2.5-7B on some tasks; the absolute accuracy gap to dense remains notable in places, limiting immediate deployability. Consider exploring targeted post-training (e.g., LoRA or small SFT) to recover quality while preserving efficiency.\n3. **Efficiency gap vs. plain 2:4 in microbenches.** The 1.57× batched MatVec vs 1.86× for 2:4 suggests wrapper-cost or fusion inefficiency. Without kernel-level fusion/tiling tailored to (block-diag × 2:4 × block-diag), ARMOR may remain below the Pareto frontier on pure efficiency. More benchmarks are needed to claim a better performance–efficiency trade-off overall. \n4. **Tied strongly to NoWag-P and 2:4.** While the method is framed generally, in practice it heavily relies on the NoWag proxy + NoWag-P init and target **2:4**. Please clarify **algorithmic novelty** beyond NoWag-P, and quantify how much of the gain derives from more effective parameterization rather than more effective parameter count.\n5. **Limited exploration.** It remains unclear whether combining ARMOR with structured pruning (e.g., head/column removal) and quantization (e.g., AWQ/GPTQ) yields **practical** end-to-end speedups while keeping quality. A hybrid study would strengthen deployability claims."}, "questions": {"value": "1. **Where exactly is the runtime gap vs. 2:4?** Can wrapper fusion (e.g., pre-/post-accumulation, tiling into the 2:4 kernel) close the 1.57× vs 1.86× gap? \n2. **Post-training recovery.** Have you tried small **LoRA/SFT** passes after ARMOR to close the accuracy gap without losing efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r15ZQqXN5U", "forum": "8NE554wv0m", "replyto": "8NE554wv0m", "signatures": ["ICLR.cc/2026/Conference/Submission17411/Reviewer_HqUF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17411/Reviewer_HqUF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982622139, "cdate": 1761982622139, "tmdate": 1762927307463, "mdate": 1762927307463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}