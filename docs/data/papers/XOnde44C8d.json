{"id": "XOnde44C8d", "number": 20306, "cdate": 1758304620752, "mdate": 1763582805993, "content": {"title": "Structured Contrastive Learning for Interpretable Latent Representations", "abstract": "Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as \"laissez-faire\" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, transforming neural networks from black boxes into interpretable glass box systems.", "tldr": "To address neural networks’ brittleness to transformations, we propose Structured Contrastive Learning (SCL), which partitions latent representations into meaningful groups, enabling robust behavior without architectural changes.", "keywords": ["Structured Representation Learning", "Contrastive Learning", "Neural Network Interpretability", "Transformation Invariance", "Feature Decoupling"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/eaf2e4bc6233fa5695ffac5da08b4c7807d16423.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Structured Contrastive Learning (SCL), a framework designed to improve the robustness of neural network representations against irrelevant input transformations (e.g., phase shifts in ECGs, rotations in IMU data). SCL partitions latent representations into invariant, variant, and free features, leveraging a variant mechanism to enhance contrastive learning. Experiments are conducted on ECG similarity tasks and IMU-based activity recognition."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The three-part feature partitioning is well-motivated and directly addresses practical challenges, such as phase sensitivity in ECG analysis.\n2. The paper is clearly and effectively written, making the proposed method accessible.\n3. The experiments demonstrate the method’s advantages over the selected baselines (and datasets)."}, "weaknesses": {"value": "1. The paper overlooks key works in time series contrastive learning, such as T-Loss [1] and TS2Vec [2], which should be discussed and included as baselines for a comprehensive comparison.\n2. The evaluation is limited to only two datasets, which is relatively narrow for a generalist conference like ICLR (see, for example, the experimental section of [1]).\n3. The training process—specifically, how the task and contrastive losses are combined—lacks clarity, making reproducibility challenging.\n\n[1] Unsupervised Scalable Representation Learning for Multivariate Time Series, Franceschi et al, NeurIPS 2019\n\n[2] TS2Vec: Towards Universal Representation of Time Series, Yue et al, AAAI 2022"}, "questions": {"value": "1. Why were T-Loss, TS2Vec, and other established baselines from this literature not considered in the experiments?\n2. Could you extend the experiments to include more datasets or tasks to validate the method’s generalizability further?\n3. How are the task and contrastive losses optimized together? Is it done jointly, alternately, or through another strategy? Clarifying this would improve reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5cCxGUkjh6", "forum": "XOnde44C8d", "replyto": "XOnde44C8d", "signatures": ["ICLR.cc/2026/Conference/Submission20306/Reviewer_che7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20306/Reviewer_che7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760867939906, "cdate": 1760867939906, "tmdate": 1762933772651, "mdate": 1762933772651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We thank the reviewers for their valuable feedback. We have decided to withdraw the submission after careful consideration."}}, "id": "3fzEh2t987", "forum": "XOnde44C8d", "replyto": "XOnde44C8d", "signatures": ["ICLR.cc/2026/Conference/Submission20306/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20306/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763582804874, "cdate": 1763582804874, "tmdate": 1763582804874, "mdate": 1763582804874, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1 This paper is well written and organized. \n2 Extensive experiments are conducted."}, "weaknesses": {"value": "1.\tThe authors' motivation to shift \"from data augmentation to structured contrastive learning\" relies primarily on empirical observations (such as ECG similarity decline), but lacks formal theoretical analysis or addresses unresolved gaps in the literature. For example, \"laissez-faire representation learning\" is described as the root cause, but the concept is neither formalized nor quantified, nor is its universality across different tasks verified. Therefore, the motivation section lacks rigorous theoretical support and quantitative evidence.\n2.\tThe article's contributions may be exaggerated. The authors repeatedly emphasize \"interpretable latent space\" and \"glass box transformation,\" but fail to provide interpretability assessment metrics or experiments. Claiming that \"enhanced interpretability\" requires specific interpretability metrics or user-friendly visualization experiments, rather than simply relying on t-SNE clustering diagrams.\n3.\tThe comparison only includes SimCLR and MoCo-style contrastive learning and data augmentation, but does not compare recent structured/decoupling methods. The authors claim to preserve both semantic clustering and individual sample identity, yet they do not provide any cross-domain evaluations, such as cross-subject, cross-sensor, or cross-dataset experiments, to substantiate this claim.\n4.\tIn the ECG experiment, the authors claimed that data augmentation “actually degraded” (Fig. 3), but did not explain the augmentation parameter space and training strategy, which may have hyperparameter mismatch or undertraining problems.\n5.\tEq.4 loss appears incompatible with standard contrastive learning objectives, lacking a derivation of its relationship with existing objectives. Compared to labeled contrastive losses (such as InfoNCE), its structure is clearly unstable, lacking a temperature term or probabilistic meaning, and prone to gradient explosion/vanishing.\n6.\tThe paper assumes f(x) = [f_inv, f_var, f_free], but does not explicitly explain how these three parts are allocated in terms of dimensions or how independent optimization is achieved through gradient isolation. \"Semantic decoupling\" may only be a formal division, lacking a true guarantee of separability."}, "questions": {"value": "1.\tThe definition of the \"rotation consistency\" metric is vague and does not explain how to judge \"prediction consistency\"？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HfVr8Zo9rT", "forum": "XOnde44C8d", "replyto": "XOnde44C8d", "signatures": ["ICLR.cc/2026/Conference/Submission20306/Reviewer_BBkR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20306/Reviewer_BBkR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739078520, "cdate": 1761739078520, "tmdate": 1762933772394, "mdate": 1762933772394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigated the shift-invariancy in signals for deep learning. The authors propose an approach, SCL, to partition the latent space into semantic groups and apply contrastive learning inside these groups."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The shift-invariancy is an important problem for most of the temporal signals. Investigating this problem in self-supervised learning for temporal signals is novel."}, "weaknesses": {"value": "The paper lacks a thorough literature review. The references are minimal and do not include key related work. In particular, the authors should compare their approach with prior studies such as [1], which explore similar problems in supervised learning.\n\nThe proposed method also shows limited novelty. Related works like [2] have already introduced the idea of separating latent spaces or embedding representations to capture invariant and variant factors. The paper does not clearly state how its contribution differs from or extends such prior approaches. Clarifying the conceptual or technical distinction would significantly strengthen the paper.\n\nThe experimental evaluation is narrow. The results are based only on ECG data, which makes it difficult to assess the generality of the method. Since the framework is meant to handle temporal signals, I recommend including at least 3–4 additional modalities (e.g., EEG, PPG, EMG) to provide a more comprehensive validation.\n\n[1] Shifting the Paradigm: A Diffeomorphism Between Time Series Data Manifolds for Achieving Shift-Invariancy in Deep Learning. ICLR 2025.\n[2] What Should Not Be Contrastive in Contrastive Learning. ICLR 2021."}, "questions": {"value": "1) How does your method conceptually differ from [1] and [2]?\n\n2) What aspects of your latent space design are novel compared to the multi-space representation strategy in [2]?\n\n3) What are the main challenges that prevent extending the experiments to other temporal signals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W8qAkNTy6o", "forum": "XOnde44C8d", "replyto": "XOnde44C8d", "signatures": ["ICLR.cc/2026/Conference/Submission20306/Reviewer_MbiW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20306/Reviewer_MbiW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833806888, "cdate": 1761833806888, "tmdate": 1762933772102, "mdate": 1762933772102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}