{"id": "G4wxGLr55b", "number": 13258, "cdate": 1758215753066, "mdate": 1763012062715, "content": {"title": "VideoScore2: Think before You Score in Generative Video Evaluation", "abstract": "Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present **VideoScore2**, a *multi-dimensional*, *interpretable*, and *human-aligned* framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset **VideoFeedback2** containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling.", "tldr": "We present VideoScore2, a model able to evaluate generative videos with long-CoT thinking process.", "keywords": ["Video Quality Evaluation", "Reward Modeling", "Multimodal Large Language Models", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/bfd5acf712bdf711500fdcfc5386becc311337b6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents VIDEOSCORE2, a multi-dimensional and interpretable framework for evaluating AI-generated videos. It aims to address the gaps in existing video evaluation models, which typically produce opaque single scores without offering detailed rationales. VIDEOSCORE2 evaluates videos along three critical dimensions: visual quality, text-to-video alignment, and physical consistency. The framework employs a two-stage training pipeline, which involves supervised fine-tuning (SFT) followed by reinforcement learning with Group Relative Policy Optimization (GRPO). The model is trained on a large-scale dataset, VIDEOFEEDBACK2, containing over 27K human-annotated videos across these dimensions. VIDEOSCORE2 achieves superior performance compared to existing methods on both in-domain and out-of-domain benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Multi-dimensional Evaluation: VIDEOSCORE2 provides comprehensive evaluations across three key dimensions: visual quality, text alignment, and physical consistency, which offers a richer and more detailed understanding of video quality.\n2. Human-aligned Evaluation: The framework is specifically designed to align with human preferences and reasoning, making its evaluations interpretable and human-like. This is crucial for improving AI-generated video models.\n3. Large-scale Dataset: The paper present VIDEOFEEDBACK2, including 27,168 human-annotated videos with both scores and reasoning traces across three dimensions.\n4. Interpretable Outputs: The model's \"thinking-before-scoring\" design provides detailed rationales for each evaluation, which helps in understanding the reasoning behind the scores."}, "weaknesses": {"value": "1. Concerns about Model Generalization: One of the notable concerns with VIDEOSCORE2 arises from its performance on out-of-domain (OOD) benchmarks, as shown in Table 6. Despite achieving impressive results on in-domain data, its performance on OOD benchmarks does not consistently outperform existing models, which suggests that there are areas for improvement in its generalization ability.\n2. Concerns About Dataset Annotation Quality: A notable concern in the paper relates to the quality of the dataset annotations, particularly in terms of PLCC which are relatively low, 60.37 on its VIDEOSCORE-BENCH-V2. In high-quality datasets for visual quality or evaluation tasks, PLCC values over 0.8 are often achievable.\n3. Concerns about the Necessity of CoT: The results in Table 7 suggest that CoT leads to a slight decrease in performance on certain benchmarks. This raises a concern about the actual necessity of CoT in the training pipeline, especially considering that single SFT training already yields relatively high performance on in-domain tasks.\n4. Insufficient Model Comparisons: Models like VBench, VQ-Insight and Q-Eval-Score, which are designed for multi-dimensional video quality assessment, are not thoroughly compared against VIDEOSCORE2 on a wide range of datasets.\n5. In the paper, VIDEOSCORE2 demonstrates its performance on out-of-domain (OOD) benchmarks, but the evaluation is primarily based on single-dimensional scores rather than showing the impact of the three-dimensional analysis proposed by the model.\n6. Lack of Performance Evaluation for the Interpretability: The paper emphasizes the interpretability of VIDEOSCORE2, particularly its \"thinking-before-scoring\" design, which provides detailed rationales for each evaluation. However, there is no clear performance evaluation of how the interpretability aspect influences the model’s overall effectiveness."}, "questions": {"value": "1. VQ-Insight also uses a similar two-stage approach involving SFT (supervised fine-tuning) and GRPO (Group Relative Policy Optimization) for reinforcement learning. How does VIDEOSCORE2 differ from VQ-Insight in terms of methodology and contributions?\n2. In Table 7, the inclusion of Chain-of-Thought (CoT) reasoning results in a slight decrease in performance in certain benchmarks. Could you clarify why CoT reasoning is necessary for the model when single SFT training already achieves strong performance?\n3. Is the score for each video the result of a single annotator or was it derived from multiple annotators per video? For each video, was it annotated by 15 different people？\n4. What is the inter-annotator SRCC for the scores assigned to the videos?\n5. Can you provide the SRCC or PLCC scores for VIDEOSCORE2 on other well-established datasets, such as GenAI-Bench, FETV, LGVQ?\n6. Given that the paper focuses on a multi-dimensional evaluation approach, can you provide more results tested with multi-dimensional scoring across OOD benchmarks?\n7. What metrics or methods were used to assess the quality of the rationales themselves?\n8. Has the interpretability of the model (i.e., the ability to generate detailed reasoning behind the scores) been evaluated separately?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I73yWsR9N1", "forum": "G4wxGLr55b", "replyto": "G4wxGLr55b", "signatures": ["ICLR.cc/2026/Conference/Submission13258/Reviewer_d6Ao"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13258/Reviewer_d6Ao"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761036021918, "cdate": 1761036021918, "tmdate": 1762923936805, "mdate": 1762923936805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "bQ2Jubjy90", "forum": "G4wxGLr55b", "replyto": "G4wxGLr55b", "signatures": ["ICLR.cc/2026/Conference/Submission13258/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13258/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763012062049, "cdate": 1763012062049, "tmdate": 1763012062049, "mdate": 1763012062049, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces VideoScore2, a multi-dimensional, interpretable, and human-aligned framework designed to explicitly evaluate visual quality, text-to-video alignment, and physical/common-sense consistency, while also generating detailed chain-of-thought rationales. Significant progress has been made with extensive annotations from both humans and closed-source models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Both the presentation and the visuals are clear and well-defined.\n\n2. The experiments are comprehensive and well-rounded.\n\n3. The results of most experiments appear promising."}, "weaknesses": {"value": "1. Regarding Figure 5, most of the SFT data is concentrated around a score of 3, which suggests that for videos of similar quality, the model might tend to assign a middle-range score. Could you further explore the impact of this concentration of scores on the results and whether there are potential ways to mitigate this effect?\n\n2. The training pipeline of VideoScore2 and the scoring dimensions in CoT bear resemblance to UnifiedReward-Think [1]. Could you highlight the innovations and advantages of VideoScore2, particularly in its methodological design and scoring dimensions, when compared to previous works? We also suggest incorporating a comparison with UnifiedReward-Think to better demonstrate the relative performance of VideoScore2 on the same tasks.\n\n3. Figure 6 shows that the GRPO of VideoScore2 has an accuracy ranging from 40% to 50%, which still leaves room for improvement. Given the reward design, it appears that the model is currently only able to score accurately in a single dimension. Could this be further optimized to improve multi-dimensional scoring accuracy?\n\n4. In Figure 7, compared to the random baseline, the improvement with VideoScore2 seems quite limited. For instance, in Lavie-base, the improvement is just 0.22 points, and in VideoCrafter1, it is only 0.6 points. Could you analyze the reasons for this relatively modest improvement and explore possible avenues for further enhancement?\n\n5. Is it possible to apply VideoScore2 in a reinforcement learning setup with existing video generation models, using VideoScore2 as a reward signal, to validate the reliability and effectiveness of the scoring system?\n\n6. Is there any potential inconsistency between the content generated by CoT and the conclusions drawn? If so, how might this issue be addressed to ensure the coherence and consistency of the model's outputs?\n\n[1] Unified multimodal chain-of-thought reward model through reinforcement fine-tuning. NeurIPS 2025."}, "questions": {"value": "Please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hp8UTWbuMz", "forum": "G4wxGLr55b", "replyto": "G4wxGLr55b", "signatures": ["ICLR.cc/2026/Conference/Submission13258/Reviewer_iotF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13258/Reviewer_iotF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761131658374, "cdate": 1761131658374, "tmdate": 1762923936498, "mdate": 1762923936498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VIDEOSCORE2, a framework for evaluating text-to-video (T2V) generative models. The work has two main components: 1) A new, large-scale dataset, VIDEOFEEDBACK2, which contains 27k human-annotated videos with multi-dimensional scores (Visual Quality, Text Alignment, Physical Consistency) and detailed chain-of-thought style rationales. 2) A two-stage training pipeline for the VIDEOSCORE2 model, which first undergoes supervised fine-tuning (SFT) on this dataset to learn the structured output format, followed by reinforcement learning using Group Relative Policy Optimization (GRPO) to better align with human preferences. The authors demonstrate that their model achieves superior performance on both their in-domain benchmark and several out-of-domain (OOD) benchmarks compared to existing video evaluators. A key feature of VIDEOSCORE2 is its interpretability, providing detailed reasoning for its scores."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- High-Quality, Large-Scale Dataset: The most significant strength is the VIDEOFEEDBACK2 dataset. Its scale (27k videos, 81k scores), multi-dimensional nature, and inclusion of detailed rationales make it a state-of-the-art resource that will likely foster significant progress in the field of generative video evaluation.\n\n- Emphasis on Interpretability: The \"thinking-before-scoring\" design, which generates explicit chain-of-thought rationales, is a major step forward from opaque single-score evaluators. This is a critical feature for building trust in automated systems and for providing actionable feedback to model developers (as demonstrated by the Best-of-N sampling experiment).\n\n- Strong Empirical Performance: The paper demonstrates convincingly that VIDEOSCORE2 achieves superior performance on a wide range of in-domain and OOD benchmarks. The strong generalization is a key result, suggesting that the training methodology and rich dataset are effective.\n\n- Rigorous Experimental Protocol: The authors have conducted a thorough evaluation, including comparisons with numerous recent and powerful baselines, and have performed insightful ablation studies that justify their key design choices (e.g., the importance of rationale data for SFT)."}, "weaknesses": {"value": "- Limited Methodological Novelty: The central weakness of the paper is the perceived lack of novelty in the VIDEOSCORE2 training methodology. The pipeline is presented as SFT followed by the application of an existing RL algorithm (GRPO), using an existing RL framework (Video-R1). While this is a strong engineering effort that yields great results, it does not propose a new learning algorithm or a fundamental new technique. The contribution feels more akin to \"SOTA recipe applied to a new SOTA dataset\" rather than a novel method in itself. The paper would be strengthened if the authors could better articulate what is fundamentally new about their training approach beyond the data it is trained on.\n\n- The \"Cold-Start\" Framing: The paper describes the SFT phase as a \"cold-start\" for RL. While technically true, this is the standard and necessary procedure for nearly all modern RLHF/RLAIF pipelines. Framing it as a distinct design choice may overstate its novelty. The key insight from the ablation study is that SFT with rationale data is better than without, which is an important but expected finding.\n\n- Dependence on Expensive Annotation: The success of the model is heavily dependent on the large-scale, high-cost human annotation process used to create VIDEOFEEDBACK2. While the authors have done an excellent job with this, the paper could benefit from a discussion on the scalability of this approach. Are there insights from this work on how to reduce this annotation burden in the future (e.g., via model-in-the-loop annotation, semi-supervised methods)?"}, "questions": {"value": "- Could you please clarify the primary methodological innovation of the VIDEOSCORE2 training pipeline beyond the application of existing SFT and RL (GRPO) techniques to your new dataset? Is there a specific adaptation to GRPO or the SFT process that is novel and crucial for handling multi-dimensional, rationale-augmented video data?\n\n- The paper argues that prior models \"are all trained via supervised fine-tuning... limiting their generalization ability\" (lines 063-064). However, your method also begins with a comprehensive SFT stage. The ablation in Table 7 (\"RL w/o SFT\") shows that starting from scratch with RL is far worse. This suggests SFT is crucial for your model as well. Could you refine your argument? Is the key not SFT vs. RL, but rather that RL on top of SFT is what unlocks generalization, and if so, what is the intuition for why this is the case?\n\n- Given that the core contribution appears to be the high-quality dataset, have you considered positioning the paper as a \"Dataset and Benchmark\" paper rather than a pure methods paper? The work seems to fit that track exceptionally well and would be judged on the immense value of the dataset itself, mitigating concerns about methodological novelty.\n\n- The dataset curation involves an \"LLM semi-blind scoring and alignment pipeline\" to expand annotator comments into detailed rationales. Could you provide more detail on this? How much of the final rationale is LLM-generated versus human-written? How do you ensure the LLM-expanded rationale faithfully represents the human's original, more concise intent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jsJ1H6ZwBp", "forum": "G4wxGLr55b", "replyto": "G4wxGLr55b", "signatures": ["ICLR.cc/2026/Conference/Submission13258/Reviewer_1n4P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13258/Reviewer_1n4P"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553720623, "cdate": 1761553720623, "tmdate": 1762923936031, "mdate": 1762923936031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VideoScore2, a multi-dimensional, interpretable, and human-aligned evaluation framework for AI-generated videos. VideoScore2 generates structured judgments across three axes, including visual quality, text-to-video alignment, and physical/common-sense consistency along with explicit chain-of-thought (CoT) rationales before scoring. A large-scale dataset, VideoFeedback2, is built with 27,168 videos, 81,504 human annotations, and rationales from 22 text-to-video models, supporting both supervised fine-tuning and reinforcement learning via Group Relative Policy Optimization. Experiments on both in-domain and out-of-domain benchmarks show that VideoScore2 outperforms existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe “think-before-you-score” mechanism introduces transparent, human-like reasoning to video evaluation, bridging the gap between numerical scores and qualitative understanding.\n\n2.\tThe VideoFeedback2 dataset is lsystematically constructed, and contains reasoning traces that support both supervised fine-tuning and reinforcement learning."}, "weaknesses": {"value": "1. For the VideoScore2 method, it only performs GRPO-based reinforcement learning fine-tuning on VideoFeedback2. However, many GRPO-based quality evaluation methods have recently appeared, such as Q-Insight, VisualQuality-R1, VQ-Insight, and VQAThinker. These methods have all proposed effective improvements in the design of rewards for quality evaluation. The authors did not discuss the differences between their method and existing ones, nor whether it is superior to them.\n\n2. In the related work section, there is an obvious citation error. In Lines 121–123: “More recent approaches—VideoReward (Liu et al., 2025), UnifiedReward (Wang et al., 2025c), and Q-Insight (Li et al., 2025)—introduce multi-dimensional scoring, yet are limited to numeric ratings without explanatory reasoning.” Q-Insight is a reasoning-based video quality assessment model, not a multi-dimensional scoring model. This is an obvious mistake, and this error also appears in Table 1.\n\n3. As mentioned in above questions, the authors should provide an overview of the current landscape of generated video evaluation datasets and reasoning-based quality assessment methods in the Related Work section. They should systematically compare VideoFeedback2 with related datasets in terms of the number of videos, prompt construction, evaluation dimensions, number of annotators, and scoring accuracy to clearly demonstrate its advantages. \n\n4. In Lines 178–179, there is an empty pair of parentheses with no content. The authors should check and correct this issue.\n\n5. In Section 3.2 ANNOTATION, the paper states that 15 annotators were invited to participate in the annotation work. Was each video annotated by all 15 annotators?\n\n6. In Section 3.3 SFT DATA PROCESSING, under Rationale Elicitation, the paper states: “(i) if the difference ≤ 1, keep the human score; (ii) if the difference = 2, average the two; (iii) if any dimension differs ≥ 3, the whole entry is re-scored, up to three times.” What is the basis for this design, especially the rule “if the difference = 2, average the two”? Why should we average the human score and the Claude-4-Sonnet score? Should we trust the human scores or the model scores?\n\n7. The dataset’s CoT-like rationales are generated by Claude-4-Sonnet. How is their accuracy ensured? Was any human verification performed?\n\n8. In Section 4.2 BENCHMARKS, the authors selected T2VQA-DB as a pairwise preference dataset. However, T2VQA-DB contains single-video scores rather than pairwise comparisons. It is therefore more suitable for point-score evaluation.\n\n9. In Section 4.3 BASELINE MODELS, none of the baseline methods were fine-tuned on the VideoFeedback2 dataset. Therefore, directly comparing them with VideoScore2 is unfair. Many methods, such as Q-Align, were originally designed for natural images and lack the capability to evaluate AIGC videos. The authors should select some strong baselines and fine-tune them on the same dataset as VideoScore2 to validate its effectiveness.\n\n10. I hold a different view regarding the metrics Accuracy and Relaxed Accuracy. For evaluating continuous scores, SRCC would be more appropriate, as it directly measures the consistency between the ranks of model’s scores and the ground-truth scores , which is more aligned with real-world use cases. For evaluating prediction precision, RMSE would be a better choice. Accuracy and Relaxed Accuracy are merely relaxed classification indicators and do not sufficiently reflect the capability of a quality evaluation model."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Av9XWm9lDR", "forum": "G4wxGLr55b", "replyto": "G4wxGLr55b", "signatures": ["ICLR.cc/2026/Conference/Submission13258/Reviewer_D8sZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13258/Reviewer_D8sZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799983479, "cdate": 1761799983479, "tmdate": 1762923935580, "mdate": 1762923935580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}