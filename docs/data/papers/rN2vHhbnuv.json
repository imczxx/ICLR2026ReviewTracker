{"id": "rN2vHhbnuv", "number": 22354, "cdate": 1758329961576, "mdate": 1759896870879, "content": {"title": "Prequential Evidence Pruning: Information-Theoretic Edge Selection for Ordering-Based Causal Discovery", "abstract": "Ordering-based causal discovery reduces structure learning to parent selection under a candidate order, yet its pruning stage remains the primary bottleneck: widely used procedures rely on marginal, additivity-constrained tests and tuned thresholds, which fail to capture non-additive interactions and compromise reproducibility. We introduce *Prequential Evidence Pruning (PEP)*, a framework that reframes pruning as a local cost-benefit analysis grounded in information theory. For each candidate edge, PEP computes a prequential (out-of-fold) log-evidence gain by evaluating the child's predictive density in the context of its current co-parents, and retains the edge only when this gain exceeds a computed Minimum Description Length (MDL) code-length penalty that adapts to sample size, the number of admissible parents, and the set size. Theoretically, the population target of the evidence gain equals conditional mutual information (CMI); the statistic is stable under bounded log-loss regret of the predictive component; and prequential scoring yields finite-sample concentration. Empirically, instantiating PEP with a pre-trained tabular model that provides calibrated, zero-shot predictive densities yields consistent improvements across diverse ordering backbones and datasets, including stress tests under misspecification. PEP thus replaces fragile heuristics with a principled, auditable rule, elevating the pruning stage of ordering-based discovery from marginal testing to context-aware evidence maximization.", "tldr": "PEP is a plug‑in pruning module for ordering‑based causal discovery: it keeps an edge only if its out‑of‑sample (prequential) log‑likelihood gain exceeds a computed MDL gate, yielding consistent gains across multiple backbones.", "keywords": ["Causal Discovery", "Ordering-based Methods", "Information Theory", "Minimum Description Length", "Conditional Mutual Information", "Prequential Scoring", "Context-aware Pruning"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/631a9b676ec5e8bec3fa0d140429d0abd3d2a254.pdf", "supplementary_material": "/attachment/a95ae73044f1bb40af0e925c3cd20008b9e4784e.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Prequential Evidence Pruning (PEP), a framework to improve ordering-based causal discovery. It replaces traditional pruning heuristics with a principled cost-benefit analysis, where the out-of-sample predictive evidence for an edge is weighed against a computable MDL complexity penalty. The method is shown to be a highly effective \"plug-and-play\" module that consistently improves the performance of ordering-based algorithms on small-scale data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s key strength lies in its reframing of the pruning problem. The information-theoretic foundation connects the evidence metric to Conditional Mutual Information, and the resulting method addresses the known brittleness of marginal, assumption-heavy pruning techniques"}, "weaknesses": {"value": "The paper's impact is severely limited by its failure to address the critical challenge of scalability, which is a primary focus of the causal discovery community.\n1. The primary weakness is that this work provides an enhancement to a paradigm (ordering-based search) that is fundamentally constrained by exponential time complexity. The current research frontier in causal discovery is focused on overcoming this exact limitation through scalable, gradient-based methods that reframe the problem for continuous optimization. By focusing on a search-based paradigm, this work feels out of step with the direction the field is heading to solve large-scale problems.\n2.  It is a local improvement that does not resolve the global bottleneck of the search-based approach. The paper's own analysis confirms a pruning cost that is quadratic in the number of candidate parents, and the exponential complexity of the broader search remains the limiting factor. This restricts the method's practical applicability to the large-scale datasets where new causal discovery methods are most critically needed.\n3. The experiments are confined to small graphs with 10 to 20 nodes. A significant contribution in causal discovery must demonstrate its relevance to larger, more challenging problems. There is no evidence or discussion of how this method would perform on graphs with 100 or 200 nodes. Without a comparison showing that a PEP-enhanced method can compete with leading gradient-based methods in terms of accuracy and time efficiency at a larger scale, the paper's claims of broad utility are unsubstantiated. It perfects a method within a specific niche without challenging the very paradigms developed to overcome that niche's limitations."}, "questions": {"value": "1. Figure 5 shows that PEP's significant performance gains are realized almost exclusively when using the powerful, pre-trained TabPFN model, while its performance with standard learners like XGBoost is far less compelling. How can you disentangle the contribution of the PEP framework itself from the exceptional zero-shot performance of its predictive engine? Does this reliance on a large foundation model limit the practical accessibility of your method?\n2. The experiments demonstrate strong performance on graphs with up to 20 nodes. However, the field is increasingly focused on scalable, gradient-based methods for larger problems. Given that PEP operates within the computationally expensive ordering-based paradigm, how do you justify its relevance for real-world applications where the number of variables is often in the hundreds or thousands? Can you provide results on larger scale data and real-world data?\n3. Given the computational constraints, what do you see as the primary use case for PEP? Is it best suited for small, high-stakes problems where principled, auditable decisions are critical, or do you envision a path to making it viable for larger-scale exploratory causal analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lF2CSmRASr", "forum": "rN2vHhbnuv", "replyto": "rN2vHhbnuv", "signatures": ["ICLR.cc/2026/Conference/Submission22354/Reviewer_E4Pd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22354/Reviewer_E4Pd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760513703076, "cdate": 1760513703076, "tmdate": 1762942182178, "mdate": 1762942182178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Prequential Evidence Pruning (PEP) as a plug-and-play technique to improve existing methods for topological ordering-based causal discovery. Briefly, PEP takes a topological order of a causal graph and iteratively refines it by pruning edges that provide an information gain smaller than a Minimum Description Length (MDL)-based threshold. Experiments illustrate that incorporating PEP into other methods leads to significant improvements with respect to standard metrics for causal discovery (SHD, SID, and F1).\n\nAlthough the work provides an interesting perspective on a specific class of causal discovery algorithms, it employs a significantly heavy language that hinders my attempts to clearly understand the proposed method.  For example, it states that PEP captures “synergistic and non-additive interactions”, and that it differs from other methods by an “evidence semantics” through a score that “concentrates under cross-fitting” - with the meaning of these expressions remaining elusive throughout the manuscript."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Algorithm 1 provides a clear description of PEP.\n\n2. Experimental results highlight PEP’s effectiveness."}, "weaknesses": {"value": "My main concern with the work regards the excessive use of hand-waved expressions and formulas that are hard to understand. On top of the ones presented in the Summary section, I also have the following remarks. \n\n1. Expectation operators do not specify with respect to which distribution the expectations are being computed. \n\n2. Corollary 2 uses the term $P_{j}$, which is only introduced later in the text. This is also true for $\\text{Pred}_{\\pi}$.\n\n3. Also, it is unclear what the authors mean by “marginal additivity-constrained methods\" - and how PEP can circumvent this presumed issue. \n\n4. It is correspondingly confusing what the authors mean by “the price of order-aware combinatorics”. Could the authors elaborate on this?\n\n5. Proposition 1 is used to support the claim that “the statistic remains well-behaved with imperfect predictors”. However, both the meanings of “statistic” (perhaps the authors are referring to $\\delta$?) and the fact that it is well-behaved under imperfect predictors (which predictors?) are not clearly represented. (This is also connected to (1); expectation operators are not clearly described).\n\n6. Corollary 1 talks about small regrets; how small, and how near is $\\delta$ to $0$? \n\n7. Figure 4 is also difficult to parse: how is each axis measured and what is the baseline for the $\\Delta \\text{Area}$ calculations?\n\n8. In Figure 7, how is “linearity” measured?"}, "questions": {"value": "Please refer to the questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q3a8hMIddE", "forum": "rN2vHhbnuv", "replyto": "rN2vHhbnuv", "signatures": ["ICLR.cc/2026/Conference/Submission22354/Reviewer_zpst"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22354/Reviewer_zpst"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760795869733, "cdate": 1760795869733, "tmdate": 1762942181787, "mdate": 1762942181787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key bottleneck in \"Ordering-based Causal Discovery\" methods—the pruning stage—by proposing a novel, information-theoretic framework called \"Prequential Evidence Pruning\" (PEP). The authors point out that existing pruning methods (like CAM pruning) heavily rely on marginal tests, additivity assumptions, and manually-tuned thresholds. This causes them to fail in capturing non-additive interactions (such as synergies) and compromises reproducibility.The PEP framework reframes the pruning problem as a local, information-theoretic cost-benefit analysis. For each candidate edge $i \\rightarrow j$, the \"benefit\" is quantified by a \"Prequential Log-Evidence Gain,\" which is the improvement in the prequential (i.e., out-of-fold) predictive log-likelihood for the child $X_j$ when conditioning on $X_i$ in addition to its co-parents $S \\setminus \\{i\\}$. The population target of this gain is equal to the Conditional Mutual Information (CMI). The \"cost\" is an adaptive code-length penalty computed according to the Minimum Description Length (MDL) principle, which adjusts to sample size, the number of admissible parents, and the current parent set size. An edge is retained only when this benefit (evidence gain) exceeds the cost (MDL penalty)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper accurately identifies a core pain point in ordering-based causal discovery methods: the pruning stage. The comparison in Figure 1 between the \"Keyhole view\" (marginal) and the \"Panoramic view\" (context-aware) very intuitively illustrates the limitations of existing methods.\n\nThe misspecification stress test in Figure 4 is very comprehensive. PEP's advantages are clearly validated, especially in the Post-Nonlinear (PNL) setting and in the robustness to functional form test (Figure 7), which directly confirms the paper's hypothesis.\n\nThe experiment using a random topological order in Table 2 effectively isolates the performance of the pruning stage. This demonstrates that PEP's advantage is not just \"piggybacking\" on a strong ordering algorithm, but stems from the superiority of its own local decision rule."}, "weaknesses": {"value": "Although the paper presents the use of TabPFN as an advantage (zero-shot, well-calibrated), this is also its main weakness. The results in Figure 5 show that when PEP is paired with RF or XGBoost, its performance has no significant advantage over CAM pruning, and is even worse in some cases. This strongly suggests that the practical performance of the PEP framework is highly dependent on a powerful and well-calibrated density estimator like TabPFN. This weakens the paper's claim of being \"model-class agnostic\". In domains where TabPFN is not applicable or performs poorly (e.g., high-dimensional data, non-tabular data), the effectiveness of the PEP framework would be highly questionable.\n\nAll synthetic data experiments are limited to $d=10$ nodes (Sachs also has only $d=11$). This is a very small scale. Algorithm 1 implements greedy backward pruning. The computational analysis in Appendix H.1 shows that PEP's cost is on the order of $O(Knm_j^2\\overline{\\alpha})$, where $m_j$ is the number of candidate parents. This is in the same complexity class as CAM pruning's $O(B s^2 n m_j^2)$ (both are quadratic in $m_j$). When the graph density increases, or under the stress test of a random order (as in Table 2), $m_j$ could approach $O(d)$. This implies the pruning cost is at least $O(d^2)$ (per node), leading to a total complexity of $O(d^3)$ or even $O(d^4)$. The paper only reports runtime for $d=10$ (Table H.1), where its runtime is already noticeably higher than CAM pruning.\n\nA core argument of the paper is replacing \"manually-tuned thresholds\". However, its MDL gate (Eq. 3) includes a fixed overhead $\\kappa$. In Appendix G, the authors state that $\\lambda=1$ and $\\kappa=25$ is \"calibrated once for the entire study\". This feels like just swapping one \"manually-tuned $\\alpha$\" for another \"manually-tuned $\\kappa$\". Although the sensitivity analysis in Figure 6 shows that the computed MDL gate (marked with $\\star$) lies within a flat, high-performance plateau, this analysis does not explore how the position of this $\\star$ mark, and consequently the final SHD/SID, would be affected if $\\kappa$ took different values."}, "questions": {"value": "Given the quadratic dependency on $m_j$ (number of candidate parents) in the complexity analysis and the limitation of experiments to $d=10$, could the authors provide an experiment on scaling with the number of nodes $d$? \n\nThe results in Figure 5 show that RF/XGB perform poorly , and the authors speculate this is because TabPFN provides \"high-fidelity calibrated densities\". This speculation is very reasonable. It is suggested that the authors add an experiment: when using XGBoost or RF as the predictor, add an extra \"post-hoc calibration\" step (e.g., Isotonic Regression or Platt Scaling) before feeding the outputs into the PEP framework.\n\nAlgorithm 1 uses Greedy Backward Elimination. What is the reason for choosing this strategy? At the beginning of backward elimination, the context $S$ used to evaluate $\\delta_{i\\rightarrow j}$ contains many irrelevant variables, which could be computationally expensive and interfere with the detection of true synergies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zwoqnZtUyW", "forum": "rN2vHhbnuv", "replyto": "rN2vHhbnuv", "signatures": ["ICLR.cc/2026/Conference/Submission22354/Reviewer_Mmos"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22354/Reviewer_Mmos"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761180927269, "cdate": 1761180927269, "tmdate": 1762942181493, "mdate": 1762942181493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}