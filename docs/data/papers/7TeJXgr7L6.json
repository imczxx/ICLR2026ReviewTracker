{"id": "7TeJXgr7L6", "number": 23201, "cdate": 1758340847445, "mdate": 1759896827101, "content": {"title": "NorMuon: Making Muon more efficient and scalable", "abstract": "The choice of optimizer significantly impacts the training efficiency and computational costs of large language models (LLMs). Recently, the Muon optimizer has demonstrated promising results by orthogonalizing parameter updates, improving optimization geometry through better conditioning.\nDespite Muon’s emergence as a candidate successor to Adam, the potential for jointly leveraging their strengths—has not been systematically explored.\nIn this work, we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an optimizer that synergistically combines orthogonalization with neuron-level adaptive learning rates. Our analysis reveals that while Muon effectively reduces condition numbers, the resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate the optimization process. NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron and applying row-wise normalization after orthogonalization, ensuring balanced parameter utilization while preserving Muon's conditioning benefits. To enable practical deployment at scale, we develop an efficient distributed implementation under the FSDP2 framework that strategically distributes orthogonalization computations across devices.\nExperiments across multiple model scales demonstrate that NorMuon consistently outperforms both Adam and Muon, achieving 21.74\\% better training efficiency than Adam and 11.31\\% improvement over Muon on 1.1B pretraining setting, while maintaining a comparable memory footprint to Muon. Our findings suggest that orthogonalization and adaptive learning rates are complementary rather than competing approaches, opening new avenues for optimizer design in large-scale deep learning.", "tldr": "", "keywords": ["Optimizer", "Muon", "Training efficiency", "large language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b658b3f2b30239af6d668162031de9f73e6e6ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes NorMuon, an optimizer that augments Muon with neuron-wise adaptive normalization to balance uneven update magnitudes after orthogonalization. It also introduces an efficient FSDP2-based distributed implementation. Experiments on models up to 5.4B parameters show faster convergence and higher efficiency than AdamW, Muon, and Dion, with minimal overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper clearly identifies a real limitation of Muon (uneven neuron updates) and addresses it with a simple, elegant normalization scheme.\n* The distributed FSDP2 implementation is well-designed, maintaining scalability with low communication cost."}, "weaknesses": {"value": "* NorMuon mainly combines Muon’s orthogonalization with Adam-mini–style normalization; the algorithmic change is minimal and lacks new theoretical insight.\n* In Section 4.1.1, all model sizes are trained on the same number of tokens rather than following scaling-law-based (e.g., chinchilla law) token allocation.\n* The distributed results are limited to 16 GPUs; communication efficiency and load balancing on 64–256 GPU clusters remain untested.\n* The method’s robustness to $\\beta_1$, $beta_2$ and learning rate scaling is not explored."}, "questions": {"value": "* The paper only provides visualization from a single layer at one checkpoint in Figure 1; could the authors include additional visualizations across different layers or training stages for completeness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EA2pCS6NMw", "forum": "7TeJXgr7L6", "replyto": "7TeJXgr7L6", "signatures": ["ICLR.cc/2026/Conference/Submission23201/Reviewer_eCSL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23201/Reviewer_eCSL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894410280, "cdate": 1761894410280, "tmdate": 1762942558573, "mdate": 1762942558573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes NorMuon, applying row-wise normalization to muon after orthogonalization, to ensure balanced parameter utilization while preserving Muon’s conditioning benefits. The idea is reasonable, and the experiment demonstrates a potentially promising result."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is reasonable.\n2. The optimizer design and implementation have considered memory and distributed optimization, which is good for practical applications."}, "weaknesses": {"value": "1. There are several points to clarify further, including whether there is implicit LR tuning bias and some details. See questions."}, "questions": {"value": "1. Could the authors further ensure there is no implicit LR tuning bias? As the 0.2 in line 10 of the algorithm is likely an empirical value, how much effort does it cost to get this value? This actually affects the effective learning rate.\n    What's more, the Beta value is also different from Adam/Muon. The Beta tuning is also a source of implicit LR tuning bias. How much tuning does it take, or is there a theoretical explanation for the value? What if the authors put the same tuning effort into Muon? Would NorMuon still be superior?\n\n2. The authors say, \"we apply orthogonalization to the concatenated QKV matrix rather than separately for a fair comparison\". Then how are those matrices concatenated? As NorMuon is using row-wise normalization, this detail should be provided.\n\n3. With lines 9 and 10 in Algorithm 1, there would be a double scaling. Isn't $ \\sqrt{mn} /  ||\\widehat{O_t}||_F$ very close to 1? (It feels like the difference is only caused by epsilon.) For \"match Adam’s RMS norm\", it is row-wise or matrix-wise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UhEEWKpOhY", "forum": "7TeJXgr7L6", "replyto": "7TeJXgr7L6", "signatures": ["ICLR.cc/2026/Conference/Submission23201/Reviewer_1s2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23201/Reviewer_1s2d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970507946, "cdate": 1761970507946, "tmdate": 1762942557893, "mdate": 1762942557893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NorMuon, an optimizer that augments Muon’s matrix-level orthogonalized updates with neuron-wise adaptive scaling via an EMA of per-row squared update norms, applied after orthogonalization and followed by an RMS re-scaling of the step size. The method aims to preserve Muon’s improved conditioning while reducing variance across neuron update magnitudes. A distributed FSDP2 implementation is described that assigns orthogonalization work across ranks and performs shard-local row normalization. Experiments on 1.1B and 5.4B-parameter LLM pretraining (50B tokens) and on Modded-NanoGPT (124M/350M) report faster convergence than AdamW, Muon, and Dion, with small wall-clock overhead and modest memory cost. The idea is simple and practically appealing; however, novelty over Muon + blockwise adaptivity is incremental, and several design choices (e.g., post-orthogonalization scaling) and evaluation details need tighter justification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear diagnosis and motivation: figures show Muon improves conditioning but leaves high variance in per-neuron update norms; NorMuon directly targets this gap with a lightweight row-wise statistic and normalization.\n- Simple, scalable design: the added state is $v_t\\in\\mathbb{R}^m$ (per row), keeping memory close to Muon; the distributed algorithm avoids fully replicated orthogonalization and keeps step-time overhead small in reported runs.\n- Breadth of experiments and ablations: results at multiple scales (124M→5.4B), efficiency curves vs AdamW/Muon/Dion, and ablations on (i) coordinate- vs neuron-level adaptation (Muon+Adam), (ii) normalization before vs after orthogonalization, and (iii) restricting to $m>n$ matrices."}, "weaknesses": {"value": "- Limited novelty/positioning: the method is essentially “Muon + blockwise RMS adaptivity after orthogonalization,” which is close in spirit to Adam-mini/Adafactor-style reductions and concurrent Adamuon; theoretical justification is light beyond qualitative diagnostics.\n- Evaluation fairness and metrics: efficiency is primarily reported as “% fewer steps to reach Adam’s final loss,” with different base LRs/betas/weight-decay policies across optimizers; the paper lacks hyperparameter sweeps per baseline, robustness/stability metrics, and downstream evaluations beyond validation loss at equal tokens.\n- Distributed section clarity: Algorithm 2 elides details on communication overlap, determinism, and failure cases; the claimed 33–50% extra communication but ~3% step-time increase needs per-layer profiling, and the approach seems tied to row-wise sharding (how general is this across FSDP configs?)."}, "questions": {"value": "- About the post-orthogonalization scaling: Eq. (10) uses $\\hat{\\eta}=0.2\\,\\eta\\sqrt{mn}/\\lVert\\hat{O}_t\\rVert_F$. Why $0.2$? Is this analytically derived from RMS-matching to Adam or tuned empirically? Please provide sensitivity curves and failure modes when varying this constant.\n- About the second-moment estimator: in Eq. (7) $v_t=\\beta_2 v_{t-1}+(1-\\beta_2)\\mathrm{meancols}(O_t\\odot O_t)$, do you apply bias-correction (e.g., $\\hat v_t$) in early steps? If not, why is it unnecessary here? Also clarify whether “meancols” (per-row mean over columns) is used for all 2D tensors, and list precisely which parameter classes (QKV concat, MLP up/down, embeddings, layer norms, biases) receive NorMuon vs a fallback optimizer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Iy0zAQAWCx", "forum": "7TeJXgr7L6", "replyto": "7TeJXgr7L6", "signatures": ["ICLR.cc/2026/Conference/Submission23201/Reviewer_nzFS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23201/Reviewer_nzFS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986294657, "cdate": 1761986294657, "tmdate": 1762942557180, "mdate": 1762942557180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NorMuon, an optimizer that improves upon the recently proposed Muon optimizer for training large language models. The authors identify that while Muon effectively reduces condition numbers through orthogonalization, it produces highly non-uniform per-neuron update norms that cause imbalanced optimization. NorMuon addresses this by adding neuron-wise adaptive learning rates through lightweight second-order momentum statistics and row-wise normalization applied after orthogonalization. The authors develop an efficient distributed implementation that minimizes computational overhead while strategically distributing orthogonalization across devices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposed a simple yet effective Solution. NorMuon's design is elegantly simple, adding neuron-wise normalization after orthogonalization, and making it easy to understand and implement. \n\n2. Comprehensive experimental validation. The experimental evaluation is thorough and convincing, such as multiple scales tested (124M, 350M, 1.1B, 5.4B parameters), consistent improvements across all scales, comparisons against strong baselines (Adam, Muon, Dion). \n\n3. Rigorous ablation studies. The ablation experiments (Section 4.1.3 and Appendix A.1) systematically validate design choices: Neuron-wise vs. coordinate-wise adaptive rates, Normalization positioning (before vs. after orthogonalization)."}, "weaknesses": {"value": "1. Limited Scope of Experiments: No downstream task evaluation. All results are pretraining validation loss; no evaluation on actual downstream tasks. \n\n2. Limited Scope of Experiments: Single dataset per scale. 1.1B/5.4B models only tested on SlimPajama; 124M/350M only on FineWeb. \n\n3. I think validation loss is not a great metric for LLM training. A smaller validation loss usually does not mean a better performance. The authors can provide the performance of downstream tasks and the performance of fine-tuning or RL fine-tuning tasks. \n\n4. The motivation is not very clear to me. I'm not very clear why a more uniform per-neuron norm can achieve a better performance."}, "questions": {"value": "1. In my opinion, optimizer is very sensitive to the selection of hyperparameters, I would like to ask whether the authors fully tuned the hyperparameters of the baseline methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Zmlbe9j3h", "forum": "7TeJXgr7L6", "replyto": "7TeJXgr7L6", "signatures": ["ICLR.cc/2026/Conference/Submission23201/Reviewer_Ratf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23201/Reviewer_Ratf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106458850, "cdate": 1762106458850, "tmdate": 1762942556792, "mdate": 1762942556792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}