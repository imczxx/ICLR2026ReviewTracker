{"id": "7X30AxQnIM", "number": 5802, "cdate": 1757936167345, "mdate": 1763618613460, "content": {"title": "In-situ Adaptation for LLM-based Link Prediction: A Dynamic Cognition Paradigm for Temporal Knowledge Graphs", "abstract": "Knowledge graphs continuously evolve, making the prediction of future links a crucial but challenging task. Current methods, whether based on dynamic graph neural networks or static retrieval-augmented generation (RAG) for Large Language Models (LLMs), struggle with generalization. They often fail to capture the real-time, characteristic evolution of the graph during the testing phase, leading to degraded performance from distribution shifts. To address this, we propose a new training-free paradigm, termed Dynamic Cognition (DyCo), which posits that effective link prediction hinges on an agent's ability to continuously perceive graph evolution and adapt its strategies in-situ.\nInspired by this, we introduce a novel framework DyCo-LLM, which enables an LLM to perform live adaptation for temporal link prediction. At its core is a dynamic context engine that tailors the LLM's prompts on the fly. This engine features an adaptive multi-path recall and scoring mechanism that adjusts its parameters based on the evolving node- and graph-level features. Furthermore, the framework incorporates a dynamic few-shot learner that generates corrective reasoning examples from prediction failures, allowing the LLM to learn from its mistakes in real-time without retraining. Experimental results on two large-scale dynamic knowledge graphs demonstrate that our approach achieves state-of-the-art performance in the link prediction task. Ablations verify that each recall path is indispensable, and balanced weights are critical to fuse structural–semantic signals and history–self similarity. In addition, the reflective few-shot routine provides consistent gains. The source code is available at https://anonymous.4open.science/r/13htrueiwbgjkdsb/.", "tldr": "", "keywords": ["In-situ Test-time Adaptation", "Temporal Knowledge Graphs", "Few-shot QA"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e3844df813ae6af8a02d434e7e7911a5cd52ac7f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DyCo-LLM, a training-free framework for temporal knowledge graph (TKG) forecasting/ future link prediction on TKG. The method creates LLM prompts on the fly.\nIt comes with an adaptive multi-path recall and scoring mechanism that adjusts its parameters based on evolving node and graph-level features.\nExperiments are conducted on the DTGB benchmark (NeurIPS 2024) using datasets such as ICEWS1819 and GDELT. The paper reports strong results under AUC and Average Precision metrics."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The general idea of test-time, training-free adaptation for LLM reasoning is interesting and could inspire future research on dynamic or streaming environments.\n* The system architecture is clearly presented and visually well explained.\n* Writing quality and presentation are strong."}, "weaknesses": {"value": "1. Mismatch: Future Link prediction/ Forecasting for Temporal Knowledge Graphs vs dynamic text-attributed graphs\n* The paper claims to be a contribution to temporal knowledge graph forecasting (Future Link prediction for temporal knowledge graphs)\n* But: The actual setup follows the DTGB benchmark, which models dynamic text-attributed graphs, mostly single-relation graphs without explicit relation reasoning. \n* DyCo-LLM does not model or even use relation types (besides the final prompt), which are central to TKG reasoning.\n* The paper ignores the complete body of work on TKG Forecasting.\n* In my opinion, for these reasons, the work is conceptually misaligned with the TKG domain it claims to advance.\n\n2. Evaluation Setup Incompatible with TKG Standards\n* The paper uses DTGB’s [7] evaluation protocol. This frames link prediction as a binary edge classification task and reports AUC and Average Precision (AP). While this is consistent with DTGB, it diverges fundamentally from how temporal knowledge graph forecasting is evaluated in the literature. For reference, see e.g. [1]\n* In TKG forecasting, the model is expected to assign plausibility scores to candidate entities for a query \n(s,r,?,t), and performance is assessed by how highly the correct entity ranks among all possible candidates. Metrics such as (time aware filtered) Mean Reciprocal Rank (MRR) and Hits@K therefore measure whether the model understands relational and temporal structure well enough to identify the true target among all options.\n* By contrast, the binary setup used here only requires the model to distinguish the correct entity from one (mostly randomly chosen) negative. This makes the task much easier and does not test fine-grained ranking ability or temporal reasoning.\nThe near-perfect AUC/AP scores reported in Table 1 confirm that the evaluation is not meaningfull.\n* As a consequence, the reported results cannot be compared to prior TKG forecasting work (e.g., RE-NET, RE-GCN, TLogic, TIRGN, CognTKE, Recurrency Baseline), all of which follow the ranking-based setup.\n* Using AUC and AP also invalidates claims of “state-of-the-art” performance within the TKG forecasting domain.\n\n3. Inappropriate and Incomplete Baselines \n* The baselines (TGAT, DyRep, JODIE, ICL, AnRe, GAD) are not TKG forecasting methods but for single-relational graphs.\n* No comparison is made to actual TKG baselines, making it impossible to judge whether DyCo-LLM advances the field.\n* I recommend comparing to (at least!) the following: RE-GCN [2], TLogic [5], TIRGN [3], CognTKE [4], Recurrency Baseline [6].\n* Datasets: Typically, for TKG forecasting, the datasets ICEWS14,ICEWS18,GDELT,WIKI,YAGO (see [1]) or potentially from TGB 2.0 benchmark are used. I strongly recommend to include them in your experiments.\n\n4. Conceptual and Modeling Issues\n* It is not clear, why you propose the self-similarity score between head and tail entities. An example: for the quadruple [Obama, visits, France, 2014-01], Obama is not at all semantically similar to France. This is true for many (most?) quadruples.\nFurther, semantic similarity is computed without conditioning on the relation.\n* Temporal recall similarly ignores relation type (retrieves recent nodes globally).\n* Together these suggest that the method has been adapted from a general dynamic-graph context without careful consideration of what makes TKG reasoning distinctive.\n* The so-called “hyperparameters” are actually runtime-computed variables (based on some predefined heuristics and dataset stats), not true hyperparameters.\n\n5. Missing Related Work\n* The related work section does not contain most (or any) key contributions in TKG forecasting, including RE-NET, RE-GCN, TLogic, TIRGN, CognTKE, Recurrency Baseline, and many more.\n* This again gives the feeling that the authors do not know the TKG domain, and the paper is a Temporal single-relational instead of TKG paper.\n\n6. Overstated Claims and Framing\n* The paper repeatedly claims to “unify dynamic graph learning and LLM memory research” and achieve “state-of-the-art” results\n* But, the method mainly combines heuristic adaptation rules with existing pretrained components. The “Dynamic Cognition” framing is very rhetorical, the method does not demonstrate new cognitive or algorithmic capabilities.\n\n7. Potential Test-Set Leakage via Pretrained LLM\n* The method uses a pretrained LLM (Qwen3-8B) on datasets containing real-world events from e.g. 2018–2019 (ICEWS1819).\n* Since the LLM was trained on data likely covering the same period, it is likely that many test events were already seen during pretraining.\n* This leads to a serious data leakage risk: The model might recall facts directly rather than infer them.\n* No steps are described to mitigate or verify against such leakage.\n\n## Summary\n\nIn summary, while DyCo-LLM presents an interesting idea of test-time adaptation for LLMs on dynamic graphs, the paper’s evaluation framework, task formulation, and baselines are misaligned with temporal knowledge graph forecasting standards.\nBy adopting the DTGB binary-classification setup instead of the established ranking-based MRR/Hits@K evaluation, the work does not actually test TKG reasoning.\nCombined with conceptual issues, missing related work, and potential test-set leakage from pretrained LLMs, the submission lacks the rigor and comparability required for acceptance at ICLR.\n\n## References\n\n[1] Gastinger, J., Sztyler, T., Sharma, L., Schuelke, A., & Stuckenschmidt, H. (2023, September). Comparing apples and oranges? on the evaluation of methods for temporal knowledge graph forecasting. In Joint European conference on machine learning and knowledge discovery in databases (pp. 533-549). Cham: Springer Nature Switzerland.\n\n[2] Zixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng Guo, Huawei Shen, Yuanzhuo Wang, and Xueqi Cheng. Temporal knowledge graph reasoning based on evolutional representation learning. In The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2021b.\n\n[3] Yujia Li, Shiliang Sun, and Jing Zhao. TiRGN: Time-guided recurrent graph network with local- global historical patterns for temporal knowledge graph reasoning. In Proceedings of the 31st International Joint Conference on Artificial Intelligence (IJCAI), pp. 2152–2158, 2022a.\n\n[4] Wei Chen, Yuting Wu, Shuhan Wu, Zhiyu Zhang, Mengqi Liao, Youfang Lin, and Huaiyu Wan. Cogntke: A cognitive temporal knowledge extrapolation framework. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 14815–14823, 2025.\n\n[5] Yushan Liu, Yunpu Ma, Marcel Hildebrandt, Mitchell Joblin, and Volker Tresp. TLogic: Temporal logical rules for explainable link forecasting on temporal knowledge graphs. In 36th Conference on Artificial Intelligence (AAAI), pp. 4120–4127, 2022\n\n[6] Julia Gastinger, Christian Meilicke, Federico Errica, Timo Sztyler, Anett Schuelke, and Heiner Stuckenschmidt. History repeats itself: A baseline for temporal knowledge graph forecasting. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJ- CAI), 2024b\n\n[7] Zhang, J., Chen, J., Yang, M., Feng, A., Liang, S., Shao, J., & Ying, R. (2024). DTGB: A comprehensive benchmark for dynamic text-attributed graphs. Advances in Neural Information Processing Systems, 37, 91405-91429."}, "questions": {"value": "1. How would DyCo-LLM perform under ranking-based metrics (time-aware filtered MRR, Hits@K) in a true TKG forecasting setup?\n\n2. How is the relation type (r) besides the prompt to the LLM, if at all?\n\n3. Why was DTGB chosen as the main benchmark given its task mismatch with TKG forecasting?\n\n4. What measures did you take to prevent test-set leakage from pretrained LLMs on datasets covering 2018–2019 events?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "88EpZ9YKDq", "forum": "7X30AxQnIM", "replyto": "7X30AxQnIM", "signatures": ["ICLR.cc/2026/Conference/Submission5802/Reviewer_UWpm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5802/Reviewer_UWpm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760604432856, "cdate": 1760604432856, "tmdate": 1762918270861, "mdate": 1762918270861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "8CGyMX4i9M", "forum": "7X30AxQnIM", "replyto": "7X30AxQnIM", "signatures": ["ICLR.cc/2026/Conference/Submission5802/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5802/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763618612426, "cdate": 1763618612426, "tmdate": 1763618612426, "mdate": 1763618612426, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce DyCo-LLM, a training-free framework enabling LLMs to perform real-time, in-situ adaptation for TKG link prediction. DyCo-LLM continuously adjusts its inference behavior through a closed-loop system comprising adaptive runtime diagnostics, multi-path recall (structural, semantic, and temporal), dynamic score fusion, and self-diagnosis reasoning that converts prediction errors into corrective few-shot examples. Experiments on two TKG dataset ICEWS18–19 and GDELT demonstrate that the proposed method can outperform dynamic GNNs and LLM methods."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **good presentation**: the paper presentation is clear and easy to follow. The diagrams and tables are also well-presented\n- **query time adaption**: the authors proposed a novel method for query-level adaptation via runtime diagnosis, achieving in-situ adaptation without updating weights. \n- **code provided**: code is provided with the submission for reproducibility."}, "weaknesses": {"value": "The main limitation of this work is its empirical evaluation, the following three aspects are limited. \n\n- **lack of dataset diversity**: the authors only benchmarked on two TKG datasets and on these datasets: ICEWS1819 and GDELT, most methods have achieved > 95% performance based on the AP and AUC-ROC, further showing that the datasets are too easy for evaluation and more diverse benchmark is needed. \n- **evaluation metric should be a ranking metric**. Evaluation on TKGs are often evaluated with a ranking metric such as MRR or Hits@K. These metrics are commonly used in prior TKG literature and often significantly more challenging than the AP / AUC-ROC metric where they treat the evaluation as a binary classification. You can see MRR or Hits@K metric in TLogic[1], \tRecurrency Baseline[2] and RE-NET[3]. All these paper also evaluated on 4-5 datasets. \n- **lack of TKG baselines**. As the method is proposed for temporal knowledge graph, it should be compared directly with SOTA TKG methods. In the paper however, the authors focused on Dynamic GNN and LLM methods only but overlooking the TKG methods. TKG methods such as TLogic and RE-Net was cited but not compared. \n\nDue to the above issues with the empirical experiments, the paper didn't provide me with sufficient evidence to see the actual performance of DyCo-LLM. My suggestions are as follows:\n\n- **test on more datasets**. Testing on at least 4-5 datasets can strengthen the conclusion from the paper, you can find datasets from the references below. Recently, TGB 2.0[4] was proposed to incorporate large scale and challenging TKG datasets as well, would be interesting to evaluate the scalability of the proposed method as well. \n\n- **evaluation metric should be a ranking metric**. Either use MRR or Hits@K for better and more robust evaluation where the true edge is ranked against multiple negatives\n\n- **adding TKG baselines**. Add baselines designed specifically for TKG such as TLogic, RE-NET and Recurrency Baseline. \n\n[1] Liu Y, Ma Y, Hildebrandt M, Joblin M, Tresp V. Tlogic: Temporal logical rules for explainable link forecasting on temporal knowledge graphs. InProceedings of the AAAI conference on artificial intelligence 2022 Jun 28 (Vol. 36, No. 4, pp. 4120-4127).\n\n[2] Gastinger J, Meilicke C, Errica F, Sztyler T, Schuelke A, Stuckenschmidt H. History repeats itself: A baseline for temporal knowledge graph forecasting. arXiv preprint arXiv:2404.16726. 2024 Apr 25.\n\n[3] Jin W, Qu M, Jin X, Ren X. Recurrent event network: Autoregressive structure inference over temporal knowledge graphs. arXiv preprint arXiv:1904.05530. 2019 Apr 11.\n\n[4] Gastinger J, Huang S, Galkin M, Loghmani E, Parviz A, Poursafaei F, Danovitch J, Rossi E, Koutis I, Stuckenschmidt H, Rabbany R. Tgb 2.0: A benchmark for learning on temporal knowledge graphs and heterogeneous graphs. Advances in neural information processing system"}, "questions": {"value": "- line 159, $k_{str}$, $k_{sem}$, $k_{time}$, $\\alpha$ and $\\beta$ notations were never introduced there and only explained much later in line 172 and later\n- how would semantic recall work on standard temporal graphs without text-features? \n- what does this sentence mean? \"fixed windows or hierarchical paging cannot expand/update with event accumulation, long-context recall amplifies noise\" line 52"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No Ethical concerns"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hpfYyHWkm3", "forum": "7X30AxQnIM", "replyto": "7X30AxQnIM", "signatures": ["ICLR.cc/2026/Conference/Submission5802/Reviewer_ULjv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5802/Reviewer_ULjv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761223548380, "cdate": 1761223548380, "tmdate": 1762918270080, "mdate": 1762918270080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free procedure to perform link-prediction forecasting on temporal knowledge graphs (TKGs). The overall idea is to have an LLM answer a query based on 5 parameters that depend on the characteristic of the TKG. These parameters inform other modules, which compute several more heuristics eventually conditioning the answer of the LLM. The evaluation is performed on two TKG forecasting benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "We must acknowledge the merit of the paper in informing the decision process of heuristics that are inspired by the temporality of the task as well as the structural characteristics of the graph."}, "weaknesses": {"value": "It is my belief that this paper has large margins of improvement, starting  from how the paper has been written. The text is extremely verbose, relying on the use of adjectives that are often unnecessary or inadequate as it often happens when writing a paper using LLMs. The authors acknowledge the use of an LLM to aid the writing, but it feels as if the paper had been heavily written with it, which is not wrong per se but in this specific case carries a number of problems with it. Besides being hard to follow, despite the simplicity of the approach, it is unclear what it is that the authors strive to address (lines 53-54). The introduction looks like a related work section with the addition of a description of the proposed system that is completely obscure to the reader at that point in time. The use of words as “in-situ” is inconsistently used: sometimes they refer to the actual meaning, other times (line 221) their use makes little sense. It is unclear why Figure 1 is so important for the understanding of the proposed methodology, since ICL does not seem to appear explicitly in other parts of the text after Section 2. Images are unclear and possibly some symbols are broken (there is a “?” which overlaps with node indices, which makes me think there should have been another symbol there). Subscripts are used inconsistently, for example $k_{str}$ and $k_{struc}$. The organization of the paper could be improved, it is made of too many titled paragraphs which do not help with the flow and the Adaptive Parameters section, which is the first step of the pipeline, is explained at the very end of Section 3. Overall, in terms of clarity of presentation, the paper is insufficient for a top-tier conference and needs major improvements. I would suggest that the authors simplify the text by reducing the verbosity and improving consistent use of certain terms, in addition to explaining more clearly the specific problem being addressed by your solution and why it is important. Clear intuitions should be also added while explaining the methodology.\n\nIn terms of experiments, I disagree that the standard metrics are AP and AUC-ROC. Most reproducible works on TKG forecasting use MRR and Hits@10, especially on the two datasets the authors benchmark their methods against. I would suggest that the authors report MRR and Hits@10 scores instead. If the authors disagree with my interpretation, I would appreciate it if they could provide references where AP and AUC-ROC are used, and I will provide alternative references instead to support my argument.\n\nThe evaluation is limited, especially considering that the approach proposed by the authors is training-free. Hyper-parameter tuning of the $\\gamma$ terms is not discussed in the main paper, whereas it is important to understand how the authors chose these terms. Right now, we cannot know whether these terms were chosen to maximize test performances, which would be a severe violation of ML evaluation practices.\n\nOverall, I think the paper would benefit by a clearer description of the evaluation process as well as additional experiments as in most TKG forecasting works."}, "questions": {"value": "- Could you explain in detail how you performed hyper-parameter tuning of terms like the $\\gamma_{struc}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CzfwNd2QhH", "forum": "7X30AxQnIM", "replyto": "7X30AxQnIM", "signatures": ["ICLR.cc/2026/Conference/Submission5802/Reviewer_HGgg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5802/Reviewer_HGgg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643247566, "cdate": 1761643247566, "tmdate": 1762918269620, "mdate": 1762918269620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DyCo-LLM, a training-free and test-time adaptive framework for temporal knowledge graph (TKG) link prediction. The key idea is to treat the LLM as a self-regulating cognitive agent capable of diagnosing each query’s local context and adapting its reasoning dynamically. A dynamic context engine adjusts recall, scoring, and prompting in real time through four modules. Experiments on ICEWS and GDELT show that DyCo-LLM achieves state-of-the-art results among both parameter-trained and LLM-based baselines, demonstrating inductive generalization without any retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a novel, training-free paradigm for temporal reasoning, which is both conceptually interesting and practically relevant for dynamic environments.\n- The closed-loop adaptive design is well motivated.\n- Writing quality and methodological descriptions are generally clear and well structured."}, "weaknesses": {"value": "- The authors mention using Qwen3-8B as the backbone, but it remains unclear whether the same model size and configuration are used for all LLM-based baselines, which could affect the fairness of the comparison.\n- How sensitive is DyCo-LLM to the base LLM’s size and context window?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2rY0BS4cQQ", "forum": "7X30AxQnIM", "replyto": "7X30AxQnIM", "signatures": ["ICLR.cc/2026/Conference/Submission5802/Reviewer_w2S8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5802/Reviewer_w2S8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986065235, "cdate": 1761986065235, "tmdate": 1762918269090, "mdate": 1762918269090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DyCo-LLM, a method that enables an LLM to perform temporal link prediction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tAddresses an important and timely problem in temporal dynamic graphs."}, "weaknesses": {"value": "1.\tThe proposed link-prediction approach does not align well with the paper’s stated research objectives."}, "questions": {"value": "1.\tThe problem formulation is unclear. The introduction claims to tackle two key aspects of dynamic graphs, i.e. continual accumulation of events and shifting relevance of historical information, but the method focuses narrowly on temporal link prediction and does not clearly propose techniques that handle continual data accumulation or time-varying relevance.\n2.\tThe paper assumes a preconstructed temporal knowledge graph. In practice, building high-quality temporal KGs from domain text (e.g., biomedicine or finance) is a major challenge. Given the stated goal of in-situ adaptation for dynamic graphs, the work should clarify its scope or include methods for dynamic data management and KG construction/maintenance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "K0JGsBEGkN", "forum": "7X30AxQnIM", "replyto": "7X30AxQnIM", "signatures": ["ICLR.cc/2026/Conference/Submission5802/Reviewer_zdCN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5802/Reviewer_zdCN"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011472298, "cdate": 1762011472298, "tmdate": 1762918268677, "mdate": 1762918268677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}