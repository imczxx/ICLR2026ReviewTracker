{"id": "mOSRwj2iIL", "number": 881, "cdate": 1756821817794, "mdate": 1763301558597, "content": {"title": "Learning a Stable Reservoir from a Single Trajectory via Persistent Loops and Markov Flow", "abstract": "We study whether embedding global topology and local transport into a fixed reservoir can improve phase tracking and prediction. From a single delay‑embedded trajectory, we build a recurrent operator in two parts: (i) long‑lived $H_1$ classes from persistent cohomology are converted to circular coordinates whose average phase velocities instantiate stable $2\\times2$ rotation blocks, and (ii) short‑horizon transition counts over a coarse partition define a Markov model whose action is lifted back to neuron space through sparse, stochastic pooling and lifting maps. A convex blend of these topological and flow components is scaled by power iteration to a preset operator‑norm bound, yielding a leaky ESN with a straightforward echo‑state guarantee; only a ridge‑regularized linear readout is trained. The resulting reservoir is fixed, interpretable, and analyzable: its internal oscillators reflect the attractor’s dominant loops, while its couplings align with observed local transport. In experiments on chaotic systems and real‑world series, the method is data‑efficient and maintains the computational profile of standard ESNs, while delivering improved phase tracking and competitive—often superior—multistep forecasts relative to tuned random reservoirs of the same size. Overall, the framework offers a principled alternative to sampling‑based wiring by learning the reservoir once from data.", "tldr": "Learn the ESN reservoir itself from a single trajectory by fusing persistent-loop topology with short-horizon Markov flow; build W as rotation blocks + lifted transitions, scale it for ESP—no BPTT needed.", "keywords": ["Reservoir Computing", "Echo State Networks", "Persistent Cohomology", "Circular Coordinates", "Markov Transition Modeling", "Topological Data Analysis", "Delay Embedding", "Spectral Scaling", "Echo-State Property", "Chaotic Time Series Forecasting"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e60ce77201ff0d1de143f358b855476665dc9d29.pdf", "supplementary_material": "/attachment/50cc1809c083276577dbd0f41cdc3a700fc43062.zip"}, "replies": [{"content": {"summary": {"value": "## Summary\n\nThis paper introduces the Persistent Homology Reservoir (PHR), a novel framework for designing the recurrent operator (W) of an Echo State Network (ESN). Different from random fixed weights, PHR learns the reservoir structure offline from a single observed trajectory by explicitly embedding the system's global topology and local dynamics.\n\nThe proposed reservoir $W$ is constructed as a convex blend of two main components: Topological Operator which aims to capture long-term recurrent dynamics and Flow Operator which captures short-term local transport. The state space is partitioned by k-means, and a Markov transition matrix is estimated based on observed transitions. This coarse dynamic is lifted back to the reservoir space via stochastic pooling and lifting maps.\n\nExperiments are conducted on several chaotic systems and real-world time series forecasting demonstrate that PHR consistently outperforms various baselines, particularly in long-horizon forecasting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "## Strengths\n- This paper makes a substantive advance in reservoir computing. Instead of heuristic random initialization, the authors propose a structured, data-driven design from system geometry and dynamics. It integrates topological data analysis (TDA) for global oscillatory structure with operator-theoretic methods for local flow, with theoretical support and stronger empirical results.\n- PHR demonstrates superior empirical performance over several strong baselines and cover a comprehensive sets of benchmarks. The improvements in Valid Prediction Time on chaotic systems are particularly attractive, showing better emulation of the underlying autonomous dynamics rollouts.\n- The evaluation is rigorous and comprehensive with a detailed ablation study effectively isolates the value of the topology and flow components and clearly demonstrates the contributions of each component."}, "weaknesses": {"value": "## Weakness\n- The primary weakness is the extreme density of the presentation and the high complexity of the methodology. The construction relies heavily on advanced concepts from algebraic topology (e.g., persistent cohomology, cocycles, Rips skeletons, discrete harmonic extension, Dirichlet energy). This presents a substantial barrier to entry for the broader machine learning community. The paper is mathematically dense and assumes a high level of familiarity with TDA.\n\n- While the training (readout fitting) remains efficient, the offline construction of W involves computationally expensive steps, notably the O(n²) cost for the distance matrix and the subsequent persistent homology computations. Although subsampling is used to mitigate this, the construction phase is significantly heavier than the near-instantaneous initialization of a standard ESN."}, "questions": {"value": "## Questions \n- Could the authors provide a more intuitive explanation of how the harmonic extension produces meaningful circular coordinates for a non-expert reader? Furthermore, while the ablation study shows PH-derived coordinates outperform than PCA, could you elaborate on why this approach is necessary and what specific advantages it offers over simpler geometric estimations? I suggest include these explantions in the camera-ready version, which would greatly improve the presenation and readibility of the paper.\n\n- How sensitive is the performance of PHR to the choice of delay embedding parameters and the number of clusters? Are the dominant H1 classes and the subsequent Markov flow robustly identified across different embeddings and partitions?\n\n- Lemma 3.1 indicates that the fidelity of the flow channel depends on the pool-lift calibration defect $AB−I_Q$. How close to the identity is $AB$ in practice with the current stochastic construction of $A$ and $B$, and how does the sparsity affect this defect?\n\nI am not an expert in reservoir computing. If the authors can address the concerns above, I am willing to increase my score accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lTJUHBhWpC", "forum": "mOSRwj2iIL", "replyto": "mOSRwj2iIL", "signatures": ["ICLR.cc/2026/Conference/Submission881/Reviewer_SASq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission881/Reviewer_SASq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781261689, "cdate": 1761781261689, "tmdate": 1762915635257, "mdate": 1762915635257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary\n\nThe manuscript describes a novel form of reservoir computing\nwhich learns the structure of a reservoir from the data to be\npredicted. It is applied to chaotic time-series prediction problems.\nThe main idea is construct the dynamics of the reservoir such that\ntwo characteristics are mixed:\nFirst, the occurrence of oscillatory limit cycles. And second,\nstochastic transitions between them.\nA linear readout is finally trained on the reservoir activity\nto perform prediction.\nThe power of the presented approach is demonstrated on a number\nof established benchmark problems (chaotic attractors) as well\nas on real-world benchmark tests. The presented approach in most\ncases provides smallest root mean square error compared to the other\napproaches.\n\nSoundness\n\nThe manuscripts appears to be written with much care and mathematical\nderivations are presented in detail in the appendix. Since the work\nlies outside of my core expertise, I could not check the proofs in\ndetail. The empirical evidence, that the presented algorithm outperforms\na large number of alternatives is a convincing and potentially impactful\nresult.\n\nPresentation\n\nThe presentation is surely gauged to the expert reader who is familiar\nwith the employed mathematical tools. For such a reader, I believe, the\nmanuscript provides the important steps to reproduce the work. Also the\nauthors strive to move detailed proofs to appendices, thus improving\nreadability for readers outside their core community.\nThe main ideas are also presented verbally and one gets an idea of the\napproach even if not from the core field.\nSome of the text is, however, still very dense.\n\n\n\nContribution\n\nThe main contribution of this work is to propose an algorithm to construct\na reservoir from direct observations of the data, rather than using, for example,\nrandomly coupled networks as a reservoir.\nA main result is that on the one hand, the authors are able to guarantee\nstability criteria (echo state property) and on the other hand they\ndemonstrate competitive performance of the approach, in parts by far exceeding\nwhat alternative methods achieve. This is a remarkable improvement."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "see \"contribution\" above"}, "weaknesses": {"value": "The presentation in the main text would in my opinion benefit from\na less technical presentation to reach a larger readership, providing\nexplanations in less technical terms, that may not be known to many\nparticipants of the conference.\nHowever, I don't consider myself an expert in this very field, so\nI would weigh the assessment by more expert reviewers higher."}, "questions": {"value": "The method of construction of the reservoir, by implementing 2 x 2 rotations,\nseems to be gauged towards time-series problems that contain period orbits.\n\nAlso it seems that the algorithm is best suited for time-series prediction,\nas its aim appears to be a faithful reconstruction of the observed\ntime series.\nDid the authors think about other applications, such as classification?\n(for example language, spoken digit classification). I don't propose to\nextend the experiments, but rather would like the authors to comment,\nwhether they expect advantages in such applications as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NEBCWi2YG4", "forum": "mOSRwj2iIL", "replyto": "mOSRwj2iIL", "signatures": ["ICLR.cc/2026/Conference/Submission881/Reviewer_huCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission881/Reviewer_huCn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904830257, "cdate": 1761904830257, "tmdate": 1762915635074, "mdate": 1762915635074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Persistent Homology Reservoir, a new framework for constructing a stable and interpretable recurrent operator for Echo State Networks (ESNs). Instead of using random reservoir weights, the authors learn a fixed reservoir directly from one observed trajectory by combining two components: (1) Persistent cohomology-derived circular coordinates that define internal oscillators capturing long-lived topological loops, and (2) A lifted Markov operator that encodes local flow transitions between coarse partitions of the embedded data. A power-iteration scaling step ensures the Echo State Property by bounding the operator norm. The resulting reservoir is fixed, stable, and interpretable, with a simple ridge regression readout.\n\nExperiments on chaotic and real-world time series show improved forecasting accuracy and longer stable horizons compared to standard and structured ESNs. The theoretical section is detailed, including proofs of contraction and eigenpair persistence, and the method is backed by thorough ablations.\n\nHowever, the paper is very difficult to follow: derivations are overly dense, notation is heavy, and key intuitions are buried in long proofs and algorithm boxes. Although the framework is looks promising, the conceptual message is obscured by excessive formalism and long mathematical detours.\nEmpirical studies, while strong, focus mostly on low-dimensional chaotic systems; scalability and generalization to modern high-dimensional ML benchmarks remain unclear.\n\nOverall, the idea of learning a reservoir from topological and flow structure is novel and interesting, but the presentation lacks accessibility and practical clarity. The paper would benefit from a simpler exposition, clearer ablations, and discussion of computational limits."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a novel, principled way to learn a stable reservoir directly from data by combining persistent-homology-based oscillators with a lifted Markov flow operator. It provides a stability guarantee (Echo State Property) through explicit norm scaling, and the resulting reservoir is interpretable, with internal modes corresponding to data-driven loops and flows.\nExperiments on several chaotic and real-world series show competitive or superior performance to standard ESNs, supported by ablations. Overall, it offers an innovative and theoretically grounded direction that connects topology, dynamics, and reservoir computing."}, "weaknesses": {"value": "I personally don't like the theoretical burnen in this paper. Without proper demonstration just figure 1 (a sketch for the model), the rest are all theory which creates trouble to verify in the short peorid of time."}, "questions": {"value": "1. Can you validate on more complexed senorio like 2D kolmogorov flow?\n2. How's the scalablility of this framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "s6gDRF3XEt", "forum": "mOSRwj2iIL", "replyto": "mOSRwj2iIL", "signatures": ["ICLR.cc/2026/Conference/Submission881/Reviewer_vtns"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission881/Reviewer_vtns"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916577601, "cdate": 1761916577601, "tmdate": 1762915634959, "mdate": 1762915634959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors proposes PHR (Persistent Homology Reservoir): instead of drawing an ESN reservoir at random and scaling it, they learn a fixed reservoir once from a single delay-embedded trajectory by combining (1) a topology-driven rotation operator built from persistent cohomology and (2) a lifted Markov flow operator built from short-horizon transition counts, then (3) power-scale the blend to get an explicit echo-state/stability guarantee. Only the linear readout is trained."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ Technical novelty: The persistent-homology-informed reservoir and lifted Markov flow operator are both novel and concretely implemented. \n\n+ Empirical support: the method is consistent across synthetic and real datasets."}, "weaknesses": {"value": "- Clarity / completeness: Some parts (auto-tuning details, how unused reservoir units are filled, PH parameter sensitivity) need more explicit description for reproducibility.\n\n- Scope: Evaluation sticks to ESN-style comparisons; missing baselines from Koopman or DMD families limits broader impact.\n\n- Robustness analysis: Still somewhat heuristic; PH and k-means steps might be brittle for noisy or non-stationary data.\n\n- Presentation: I'd suggest moving much math to the appendix and include Figures summarizing the method and motivation. Current Fig1 is very hard to read."}, "questions": {"value": "- “W_top is formed … and randomly permuted to distribute the oscillator pairs across the reservoir; remaining coordinates receive decaying radii.” what is the rule for filling the rest of the reservoir when you have, say, X units but only Y loops. Can you give a concrete example to help undertsand it.\n\n- Line 1080 \"The noise channel breaks degeneracies and complements the basis without compromising\nstability, as its contribution is explicitly budgeted by ξ and then squashed by the global\nscaling to ρ⋆.\" can you illustrate this part?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "owbeEkMJGm", "forum": "mOSRwj2iIL", "replyto": "mOSRwj2iIL", "signatures": ["ICLR.cc/2026/Conference/Submission881/Reviewer_nfqs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission881/Reviewer_nfqs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066168322, "cdate": 1762066168322, "tmdate": 1762915634706, "mdate": 1762915634706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}