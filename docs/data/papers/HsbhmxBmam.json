{"id": "HsbhmxBmam", "number": 1228, "cdate": 1756866380349, "mdate": 1759898220313, "content": {"title": "RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance", "abstract": "Diffusion large language models (dLLMs) have shown great potential in large-scale language modeling, and there is an increasing interest in further improving the capacity to solve complex problems by guiding the reasoning process step by step. Common practice for autoregressive language models typically learns a process reward model with dense annotation for each intermediate step. However, this is challenging for dLLMs where the generation is in an any-order fashion and intermediate states are partially masked sentences. To this end, in this paper, we propose reward-free guidance (RFG), a principled method for guiding the reasoning trajectory of dLLMs without explicit process reward. The key idea of RFG is to parameterize the process reward by log-likelihood ratios of the enhanced and reference dLLMs, where the enhanced model can be easily obtained by any off-the-shelf dLLM that has been post-trained with reinforcement learning (RL) or supervised fine-tuning (SFT). We provide theoretical justification that RFG induces the reward-guided sampling distribution with no additional reward. We conduct comprehensive experiments on four challenging mathematical reasoning and code generation benchmarks using a diverse suite of dLLMs enhanced with various post-training methods. RFG consistently yields significant improvements across all tasks and model types, achieving accuracy gains of up to 9.2%. These findings establish RFG as a general training-free framework that scales test-time reasoning without reliance on external reward models.", "tldr": "", "keywords": ["diffusion large language model", "LLM reasoning", "test-time scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7ba047e6cd9b1cdfde9036870a6f59db55352ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the techniques of test time scaling for diffusion language models under the guidance of process rewards. To parameterize process rewards, this paper proposes using log-likelihood ratio between enhanced and referenced diffusion language model, which results in a form aligns with classifier-free guidance. The effect is validated on several representative benchmarks on math reasoning and coding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents a simple method to enhance sampling results of diffusion language models, which is theoretically identical to classifier-free guidance. \n2. The method shows improvement on some representative benchmarks of math and coding and different models. This confirms the generality and effectiveness of the proposed method.\n3. It is interesting to see the close connection between implicit reward modeling through likelihood ratio and classifier-free guidance. This contribution may open possibility for future research on the alignment of diffusion models."}, "weaknesses": {"value": "1. The method requires to maintain two diffusion language models and relies on tuning a hyperparameter, which is not very convenient.\n2. The paper is not self-contained enough in using likelihood ratio as the rewards. It is confused for readers less familiar about process rewards.\n3. The performance gains is relatively minor and is only compared to a weak ensemble baseline."}, "questions": {"value": "1. Given the alignment of the introduced method and classifier-free guidance, is it possible to achieve similar effect as the paper show using the log-likelihood ratio from a single diffusion lm w and w/o prompt?\n2. With the implicit reward modeling shown, is is possible to provide process reward to help reinforcement learning of diffusion models? \n3. What is the best of 2 in your main resutls (Tab. 1 and Tab. 2), either selecting with oracle or likelihood/confidence-based scoring. I am curious about the gap between Bo2 and RFG."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LIKxag9bUK", "forum": "HsbhmxBmam", "replyto": "HsbhmxBmam", "signatures": ["ICLR.cc/2026/Conference/Submission1228/Reviewer_JGzp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1228/Reviewer_JGzp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657326358, "cdate": 1761657326358, "tmdate": 1762915713644, "mdate": 1762915713644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address a key challenge in diffusion large language models (dLLMs): unlike autoregressive (AR) LLMs, dLLMs generate text in an \"any-order\" fashion, resulting in intermediate states of partially masked sentences that make it difficult to train explicit process reward models (PRMs) for step-wise reasoning guidance. To solve this, the authors propose Reward-Free Guidance (RFG), a test-time framework that avoids explicit PRM training by parameterizing rewards as the log-likelihood ratio between two dLLMs: a \"policy model\" ($p_{\\theta}$, typically a post-trained dLLM via RL or SFT, e.g., d1-LLaDA, DiffuCoder) and a \"reference model\" ($p_\\text{ref}$, a base pre-trained dLLM, e.g., LLaDA-Base).\n\nIn experiments, RFG is evaluated on mathematical reasoning (GSM8K, MATH-500) and code generation (HumanEval, MBPP) across multiple dLLM families (LLaDA, Dream). \nThe authors report consistent accuracy gains (up to 9.2% for DiffuCoder on HumanEval) over two baselines: the original post-trained $p_{\\theta}$ and a naive ensemble of $p_{\\theta}$ and $p_\\text{ref}$.\n\nOverall, the work identifies a relevant problem for dLLMs but lacks sufficient technical depth and novelty. Its performance gains heavily rely on the pre-optimized quality of $p_{\\theta}$ (rather than RFG’s intrinsic guidance mechanism) and fails to address the core challenge of dLLM-specific intermediate state reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper targets a high-priority gap in dLLM inference—masked intermediate states (e.g., \"[MASK] + 2 = 5\") that invalidate AR-LLM process reward model (PRM) methods. Such a challenge directly limits dLLMs’ practical reasoning scalability, making the work’s focus timely and impactful.\n\n2. The introduced training-free guidance framework bypasses explicit PRM training. This avoids the \"reward annotation bottleneck\" of RL-based dLLM methods and delivers consistent accuracy gains (up to 9.2% for DiffuCoder on HumanEval) across diverse tasks."}, "weaknesses": {"value": "1. Lacks dLLM-specific innovation—essentially repurposes Classifier-Free Guidance by swapping conditional/unconditional models. RFG’s log-likelihood ratio guidance mirrors classifier-free guidance, a generic diffusion technique, with no dLLM-specific adaptations (e.g., mask-order-aware weighting). The \"reward-free\" claim is misleading, as it relies on  implicit RL/SFT rewards. \n\n2. Gains are ambiguously attributed (e.g., +0.5% on GSM8K for LLaDA 1.5 may stem from $p_\\theta$ quality, not RFG). Further, RFG lacks \"scaling\" behavior—performance does not improve with increased test-time computation (e.g., more sampling steps), violating the essence of test-time scaling."}, "questions": {"value": "1. How is the \"naive ensemble\" logit average computed (per-token vs. sequence-level)?\n\n2. How about the performance gains with the computation budget scaling, based on RFG?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HXyrwhsh9C", "forum": "HsbhmxBmam", "replyto": "HsbhmxBmam", "signatures": ["ICLR.cc/2026/Conference/Submission1228/Reviewer_z2d3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1228/Reviewer_z2d3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061970306, "cdate": 1762061970306, "tmdate": 1762915713532, "mdate": 1762915713532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RFG (reward-free guidance) for diffusion LLM test-time sampling to achieve better reasoning performance. They prove that trajectory-level rewards can be decomposed into per-denoising-step rewards, then use the log-likelihood ratio between an enhanced model (RL-trained or instruction-tuned) and a reference base model to guide the sampling process at test time. The method shows performance gains on math and coding tasks with qualitative examples demonstrating how RFG achieves better results, outperforming both the original enhanced models and naive ensemble baselines with matched compute."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. this paper provides a clean theoretical justification showing how trajectory-level rewards can be decomposed into step-wise process rewards through log-likelihood ratios for diffusion LLMs. it tackles the difficulty that obtaining a process reward model for diffusion LLM as dLLM will generate partially denoised sequences and it would be hard to train such PRM. \n2. This paper shows their proposed sampling method RFG achieves consistent gain across pairs of dLLMs on mathematics and coding tasks and it is also training-free."}, "weaknesses": {"value": "1. lack of ablation studies (with only one ablation presented). More analysis would strength the work on how RFG actually changes the decoding process, for example how it affects the denoising order (a key feature of dLLMs' any-order generation). The paper only tests in the setting: \"the number of generation steps is set equal to the generation length\",  but the core advantage of dLLMs is parallel multi-token generation. The paper provides no ablation on how block size/number of denoising steps affects RFG performance, and no exploration of whether decoding more than 2 tokens per step changes the effectiveness of guidance. this limits its practical value for future dLLMs. \n  \n    Since RFG requires running two full dLLM forward passes at every denoising step, which is very expensive for full-attention based dLLMs like LLaDA and Dream and the fact they only tested on sampling 1 token per denoising step makes this method less practical, if RFG improves on multi-token prediction setup that is more promising.\n\n6. The paper motivates RFG by arguing that training explicit PRMs for dLLMs is challenging due to incomplete intermediate states. However, RFG is more like a guided sampling method that reweights each denoising step, rather than a search-based approach that PRMs typically enable in autoregressive LLM contexts (e.g., beam search, tree search with PRM scoring)\n1. how would RFG compare against simply using lower temperature sampling on the enhanced model alone, as it would sharpen the probability distribution and potentially achieving similar effects to RFG's guidance by making the model more confident in its existing preferences. \n\n2. The theoretical derivation assumes the enhanced model is trained via RL/preference optimization, However, instruction-tuned models represent a major portion of their experiments (LLaDA-Instruct, Dream-Instruct) and show substantial gains. The paper lacks explanation for why SFT models work as their main theoretical contribution doesn't cover a significant portion of their empirical results."}, "questions": {"value": "see weakness above.\n1. Does RFG also work for block-diffusion sampling for models like SDAR or trado?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yItdnuoGPM", "forum": "HsbhmxBmam", "replyto": "HsbhmxBmam", "signatures": ["ICLR.cc/2026/Conference/Submission1228/Reviewer_Vyfw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1228/Reviewer_Vyfw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067619276, "cdate": 1762067619276, "tmdate": 1762915713412, "mdate": 1762915713412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RFG (reward free guidance), a novel method for guiding the generation of diffusion language models at test time. RFG as a test time method makes use of information from a reference language model to guide the diffusion process of dLLM. The paper showcases improvement over a large class of dLLMs (instructioned tuned, RL'ed) showcasing the power of the new approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides an interesting way to combine reward-free guidance, that is, not training an explicit reward for guiding test time generation for dLLM. They introduce a somewhat novel and sensible way to leverage logit information from a reference model. Their experimental result is also showing relatively solid improvements."}, "weaknesses": {"value": "I think there are a few practical constraints of the proposed method that limit its applicability, and the way ablations and comparisons are conducted might also warrant additional scrutiny and improvement. I will detail these points in the question section below."}, "questions": {"value": "=== **test time cost measurement of using a reference model** ===\n\nI think it's quite important to calibrate for the actual cost at test time when making the comparison in e.g. Table 2. The algorithm proposed need to query reference model every iteration, which is extremely costly, given that dLLM generations typically denoise for a relatively large number of steps (less than generation length though). I think this is an important axis to ablate - in additional to the strength $w$, the number of query made to the reference model is also a big factor controlling the cost of the test time algorithm. If the number of query is such that it leads to a cost much higher than alternative baselines, the gains are much less surprising.\n\nNote the Ensemble baseline essentially runs the original diffusion model multiple times, but if the reference model is of a bigger size, the cost of Ensemble is much smaller than RFG. What if we run Ensemble the same order of cost magnitude as RFG and compare the performance difference? \n\nIt is helpful to have another axis for RFG that controls the cost of the test time budget, and examine how performance fares as this varies.\n\n=== **tokenizer** ===\n\nIt seems that to apply RFG we require the same tokenizer for the dLLM and reference model because we require their logits to be additive. This might form a constraint because we'd expect tokenizers to be more flexible across model family and this will naively limit the applicability. I think it's also useful to work out a solution for such cases.\n\n=== **RFG against other baselines** ===\n\nI think other baselines are worth comparing to such as training actual outcome reward for dLLM, or making use of existing outcome reward for those tasks and use the best-of-n test time budget to see whether there is a performance difference. RFG comparison directly skips through the whole outcome reward baselines and test time algorithm, which I think are very important to compare against. Such methods are also arguably cheaper than RFG because outcome rewards need to be queried fewer times than ref model.\n\n=== **Reference model** ===\n\nRFG is quite generic and arguably applicable to a big class of ref model, but I think it's useful to ablate how the strengths of the ref model impacts generation quality and how sensitive is the performance to the $w$ coefficient."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BhIxqhXNij", "forum": "HsbhmxBmam", "replyto": "HsbhmxBmam", "signatures": ["ICLR.cc/2026/Conference/Submission1228/Reviewer_o6q9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1228/Reviewer_o6q9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762597779201, "cdate": 1762597779201, "tmdate": 1762915713222, "mdate": 1762915713222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}