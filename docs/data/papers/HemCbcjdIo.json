{"id": "HemCbcjdIo", "number": 3126, "cdate": 1757338776738, "mdate": 1759898107711, "content": {"title": "When Human Data Runs Out: Self-Supervised Reasoning via Negotiation Self-Play", "abstract": "The reasoning ability of large language models (LLMs) has canonically relied on costly expert-labeled data, a resource now nearing depletion. Existing alternatives, such as Chain-of-Thought prompting or multi-agent debate, either suffer from prompt sensitivity or assume binary correctness, limiting their applicability to open-ended reasoning tasks. This raises a fundamental challenge: how can we construct scalable supervision that drives diverse yet stable reasoning without external labels? Verbal interaction offers the most natural source of new supervision signals; among the tasks that feature such interaction between AI agents, negotiation stands out as particularly suited for reasoning enhancement. We introduce Language model Self-play via Scorable negotiation Game (LSSG), a paradigm that frames reasoning enhancement as a two-player negotiation game with continuous, outcome-based rewards. Unlike prior numerical- or annotation-based games, our formulation pioneers negotiation in the language space, providing dense, interpretable signals for stable optimization at scale. LSSG combines behavioral cloning from real dialogues with self-play refinement that balances diversity and stability, yielding sustainable reasoning improvement. Across seven benchmarks, including WinoGrande, CSQA, CB, SST2, LogiQA2, MedMCQA, and CMMLU, LSSG consistently outperforms strong baselines. These results demonstrate LSSG as a scalable and robust paradigm for long-term reasoning self-supervision in LLMs.", "tldr": "", "keywords": ["scorable negotiation game", "LLM reasoning", "self-supervised learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/944a649dff89483b68a775ebe21884e7b02b8c43.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Language model Self-play via Scorable negotiation Game (LSSG), which formulates a two-player negotiation game. Using the two player formulation, they setup a joint policy optimization problem that optimizes the policies of a \"buyer\" and a \"seller\" using price negotiation data. They then evaluate their optimized policies on general reasoning benchmarks and various different two-player settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of generating dense supervision signals from multi-agent debate/self-play seems to be fairly novel. The idea of using different regularizations for joint optimization, like emotional state and semantic diversity, also seems interesting, although insufficiently supported via experimentation."}, "weaknesses": {"value": "- **Lack of clarity on adoption to more general domains**: The work is written using the buyer-seller framing, which is nice for anchoring concepts. However, it's unclear how one should apply LSSG to more general reasoning training data. E.g., given (q,a) pairs of math questions q and answers a, how should one setup LSSG? The prompts and reward modeling setups are all tailored to negotiating the price of a given object. If one wants to frame LSSG as an approach for generalized reasoning improvement, then more details are needed. If general reasoning improvement is simply the by-product of training with this specialized buyer-seller data/framework, then it seems the claims made in the paper are a bit unsubstantiated, given the evaluation setup (weaknesses of which are described next)\n- **Unclear if this method improves more recent LLMs**: The authors experiment with Llama-2 backbones, which are considered very weak models by 2025 standards. It is unclear if the benefits of their proposed methods transfer to more capable LLMs. I am surprised at choice of base model: The Llama-3.1/3.2 and Qwen2.5/3 series are readily available, available in size ranges comparable to Llama-2, and more contemporary. I think it is important to demonstrate the staying power of proposed methodology, and as such, I find the usage of Llama-2 to be insufficient.\n- **Outdated evaluation setup for measuring general reasoning ability**: The authors use benchmarks like Winogrande, Commonsense-QA, etc. as an evaluation suite for reasoning. Like the choice of backbone models, these are surprising and outdated choices. If the goal is to evaluate LLM-based reasoning, multiple new benchmarks and evaluation settings have been introduced to stress-test smaller LLMs. I would be more convinced if evaluation was done on benchmarks like GPQA.\n- **Insufficient evaluation for target task**: If the aim of this work is to train agents capable of multi-turn/multi-agent games, then the focus of benchmarking should be on such tasks. The current paper evaluates on two such tasks. I would prefer to have more comprehensive evaluations here, such as https://arxiv.org/abs/2305.19165. In fact, https://arxiv.org/abs/2305.10142 considers a very similar seller/buyer game.\n- **Insufficient baselines**: The paper compares LSSG against a few trained baselines (which I believe would be better suited as a separate ablation experiment), vanilla prompting and CoT prompting. Given the debate-style nature of LSSG, simple baselines like multi-turn debate or self-refinement (mentioned in the introduction) implemented via prompting the base model are missing.\n- **Insufficient related work**: There has been renewed interest in self-play (e.g., https://arxiv.org/abs/2505.03335, https://arxiv.org/abs/2506.24119). This paper omits more contemporary discussion of self-play RL. Furthermore, the work omits discussion of reinforcement learning from verifiable rewards (RLVR) for improving reasoning.\n- **Missing ablations**: It seems that loss components (SemDi, EmoSt) are not sufficiently ablated. It's unclear how such terms contribute to the overall performance.\n- **Several unclear details**:  See questions section."}, "questions": {"value": "- When performing policy optimization, are you back-propogating through a single fixed LLM instance with the two different \"roles\" as user/system prompts? Or are you instantiating two separate LLMs, treating one as buyer and one as seller?\n- Related to above, if you have trained two separate policies (buyer and seller), which policy model is used for inference on standard benchmarks? How do you justify which policy to use as the standalone reasoning model? Are there ablations for performance between the two policies?\n- For evaluation, do you adopt the standard N-shot setup that many of these benchmarks employ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y2Ak1LOa9E", "forum": "HemCbcjdIo", "replyto": "HemCbcjdIo", "signatures": ["ICLR.cc/2026/Conference/Submission3126/Reviewer_5sr1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3126/Reviewer_5sr1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760647221059, "cdate": 1760647221059, "tmdate": 1762916562619, "mdate": 1762916562619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper pointed out the problem that costly expert-labeled data used for improving the reasoning abilities of LLMs is becoming scarce. The authors argue that existing alternatives, like CoT prompting or multi-agent debate, are limited by prompt sensitivity or an over-simplistic assumption of binary correctness, making them unsuitable for open-ended reasoning. To overcome this, the paper introduces Language model Self-play via Scorable negotiation Game (LSSG), a novel self-supervision framework. LSSG frames reasoning enhancement as a two-player negotiation game that unfolds entirely in the language space. The authors evaluate LSSG using LLaMA2-7B and -13B models. The results demonstrate that LSSG-trained models consistently outperform baselines for reasoning benchmarks and show superior performance and stability in two dedicated negotiation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose a novel framework, LSSG, which introduces a negotiation game setting to train LLMs, with the stated goal of enhancing their general reasoning abilities. \n\n2. The LSSG framework demonstrates strong performance, outperforming all reported baselines across a diverse set of reasoning datasets."}, "weaknesses": {"value": "1. The paper claims that skills learned via negotiation games would improve the general reasoning ability for LLM. However, the authors neither provide a detailed analysis explaining why this knowledge transfer should occur nor cite relevant papers to support this hypothesis. The link between negotiation-specific skills and general-purpose reasoning remains assumed rather than proven. \n\n2. The experimental comparison may be confounded by a lack of transparency regarding data volume. While the authors report adding 105,267 data points to the CraigslistBargain Dataset, they fail to specify the original size of this dataset as well as the total training data volume used for the baseline models. Without this information, it would not be a fair comparison for the paper to determine LSSG's strong performance. \n\n3. The \"V\" baseline in Table 1 is not an informative or fair point of comparison. To properly isolate the benefits of the LSSG method, the authors should report the performance of a LLaMA2 model finetuned on the training sets of the seven downstream tasks, or finetuned on the same dataset as LSSG."}, "questions": {"value": "1. Can the authors provide a more detailed analysis to substantiate the knowledge transfer claim? For instance, can they show which specific reasoning skills from negotiation are being applied to downstream tasks? \n\n2. The results for MedMCQA and CMMLU in Table 1 show a performance decrease between LSSG_1 and LSSG_3. This appears to contradict the claim that continued self-play yields stable improvements. Could the authors explain this? \n\n3. The experiments are based on the LLaMA2 model. Given the rapid evolution of base models, why was this model chosen over more current architectures like LLaMA3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iH0l7NE5Nv", "forum": "HemCbcjdIo", "replyto": "HemCbcjdIo", "signatures": ["ICLR.cc/2026/Conference/Submission3126/Reviewer_D3mJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3126/Reviewer_D3mJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816593091, "cdate": 1761816593091, "tmdate": 1762916562396, "mdate": 1762916562396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Language Model Self-play via Scorable Negotiation Game (LSSG), a novel framework designed to enhance the reasoning capabilities of large language models (LLMs) without relying on costly, expert-labeled data. By framing reasoning as a two-player negotiation game in the language space, LSSG generates continuous, interpretable supervision signals through self-play, balancing diversity and stability in reasoning outcomes. The framework integrates generalization-aware behavioral cloning and stability-aware self-play, along with advanced regularization techniques like semantic diversity loss and emotional stability loss, to improve robustness and generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel framework, Language Model Self-play via Scorable Negotiation Game (LSSG), which effectively models reasoning enhancement as a two-player negotiation game in the language space. This innovative approach avoids reliance on costly, expert-labeled data and provides continuous, interpretable supervision signals, addressing a critical challenge in LLM development. \n\n2. The authors evaluate LSSG across seven diverse reasoning benchmarks (e.g., WinoGrande, CSQA, SST2) and demonstrate consistent improvements over strong baselines, including Chain-of-Thought and vanilla models. The results highlight the framework's scalability and robustness across a wide range of reasoning tasks. \n\n3. The paper integrates advanced regularization techniques, such as semantic diversity loss and emotional stability loss, to ensure both diverse and stable reasoning outputs. This is particularly valuable in enhancing model performance under adversarial or complex negotiation settings."}, "weaknesses": {"value": "1. The baseline models used in the experiments are relatively outdated (LLaMA2), and both the diversity of model types and the range of model parameters are limited. Incorporating a broader set of models, including Qwen and Mistral, as well as models with different parameter counts such as 2B and 70B, would significantly strengthen the persuasiveness of the experimental results.  \n2. Although this paper presents a formalization of converting the reasoning process into a two-person, evaluative negotiation game, it lacks a comprehensive description of the modeling process and concrete examples. Additionally, while the proposed modeling approach aims to mitigate issues such as high prompt sensitivity and the assumption of binary correctness in existing methods, no experimental or theoretical evidence is provided to support these claims. Further analysis is needed to substantiate these conclusions."}, "questions": {"value": "see the comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XmPV2XW3Ib", "forum": "HemCbcjdIo", "replyto": "HemCbcjdIo", "signatures": ["ICLR.cc/2026/Conference/Submission3126/Reviewer_srmg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3126/Reviewer_srmg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977655055, "cdate": 1761977655055, "tmdate": 1762916562074, "mdate": 1762916562074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper frames reasoning for LLMs as a two-player negotiation game. A buyer and seller interact using actions like proposing a numeric price, accepting quitting, and emitting a free-form message. Rewards are defined via a transaction price ratio relative to the initial price. Training proceeds with generalization-aware behavioral cloning and stability aware self-play. The author report gains on seven QA benchmarks and better win/payoff in two negotiation environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear modular training recipe: The paper defines the training recipe clearly and intuitively with behavior cloning and self-play. I believe this follows from the idea of negotiation game fairly intuitively. \n* Breadth of the evaluation: The paper finds seven diverse WA tasks and two negotiation games with a large scale evaluation.\n* Improvement: The paper shows consistent albeit small improvement and the fact that the negotiation between the seller and buyer can be more deeply understood."}, "weaknesses": {"value": "I think this papers needs some work before getting accepted and seems unfinished. I'm looking for some clarification on the following and will increase my score if I see these clarifications:\n* Objective: One part of the paper that confused me was that the objective only is applied to the terminal, outcome-based rewards. This is gated by an accept indicator as far as I understand from the equation on page 3, lines 142-144. However, I am concerned whether this means that intermediate steps from the free-form messages receive any feedback signal from this objective. I can't really tell from 3.2 or maybe I am not quite following the self-play formulation. I could not say that the objective has dense, interpretable signals in this case if the paper only provides rewards on the outcome rather than shaping free-form messages. Could the authors clarify what their objective is in lines 142-144.\n* Ablations: I think having ablations on self-play matters a lot and experiments do not isolate which part matters. For example, there are no ablations removing (1) self-play RL while keeping the same data, (b) semantic diversity and emotional stability losses, (2) the price-ratio reward e.g. self-play with random rewards. The observed gains could stem from generic RL finetuning/reguralization. \n* Confidence Intervals: I think error bars are imperative here. Please do add them. \n* Semantic Diversity and Sentiment Losses: Why are these reguralizers necessary? I think ablation experiment is necessary for this. I think semantic diversity is potentially distortive. It rewards maximal dissimilarity since the loss is minimized at -2. This is not penalizing over-similarity as claimed. The emotional stability addition seems reasonable except using DistilBERT is odd and could have a domain mismatch with using a small external and noisy model."}, "questions": {"value": "* Notation: Could the authors describe the difference in $\\alpha$ in section 2 and the $\\alpha$ in section 3.2. In section 2, $\\alpha$ scales the rewards and I assume it has the same functionality? I wasn't sure if the notation was purposefully consistent and just want to be sure.\n* Uneven improvements: I find the difference of improvements across datasets interesting. First, most downstream tasks are multiple choice accuracy, making me wonder if these are tasks with minimal need for negotiation-style planning. There are non-monotonic improvements across LSSG iterations, suggesting instability. My intuition is that there is some overfitting to particular distributions rather than a general reasoning improvements. However, I would be curious to hear the author's thoughts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OJfEwt611e", "forum": "HemCbcjdIo", "replyto": "HemCbcjdIo", "signatures": ["ICLR.cc/2026/Conference/Submission3126/Reviewer_1KV6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3126/Reviewer_1KV6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033333945, "cdate": 1762033333945, "tmdate": 1762916561149, "mdate": 1762916561149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}