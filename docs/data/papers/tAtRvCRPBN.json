{"id": "tAtRvCRPBN", "number": 21456, "cdate": 1758317729614, "mdate": 1763661185294, "content": {"title": "Finding Better Prototypes For Interpretable Text Classifiers With LLM Optimization", "abstract": "Prototype neural networks are the most popular form of interpretable-by-design classifiers in machine learning.\nWithin this field, prototypes are typically learned as black-box vectors, and then projected onto the nearest example from the training data for visualization and inference purposes. This improves interpretability because we can understand the logic behind predictions based on the similarity between the input instance and the nearest prototype in the network. However, because these prototypes are real training instances there are at least two major issues with this approach.  Firstly, as the projected prototypes do not represent the learned ``black-box'' vectors which were optimized for accuracy, there is typically a performance drop off. Secondly, because the prototypes are real training instances, they are usually overly specific and full of spurious or irrelevant details, making them difficult to interpret readily.\nIn this study, we address this problem by using large-language models (LLMs) as a tool for optimization to find better prototypes for the network. Across a series of experiments, we find that our method produces prototypes which sacrifice less performance and are more intelligible compared to baselines which project. Previously, it was not possible to visualize a learned prototype, because methods were constrained to projection using actual training data, but our approach suggests a possible path to overcome this limitation.", "tldr": "We show that prototypes in interpretable text classifiers can be made more intelligible and accurate by using LLMs as optimization tools.", "keywords": ["Interpretable ML", "Prototypes", "Large Language Models", "Optimization", "Text Classifier"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07a3c0b1f71f8cb52f4d9fe1c81543e10e2c797f.pdf", "supplementary_material": "/attachment/5b606672c80ad28b7038bad76caa9f6dd3d974f7.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a method to improve the interpretability of prototype neural networks by improving prototypes via large language models (LLM). Existing methods project learned prototypes onto nearest training examples, leading to performance drops and overly specific representations. This paper addresses this issue by using LLMs to optimize and discover better textual prototypes. To accomplish this, latent prototype vectors (numerical representation) are first learned on a text classification task. Textual representations for each of these latent prototype vectors are then found from the training text corpus based on cosine similarity. These texts are improved via LLM to further minimize the cosine similarity in the same latent space. With these optimized prototypes, text classification accuracy is on par compared to simply projected prototypes. However, from a qualitative evaluation, prototype quality appears to be better in LLM optimized prototypes."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The idea of using LLM to improve prototypes is interesting, and I find it novel."}, "weaknesses": {"value": "First and foremost, I'm not convinced about the idea of using LLMs (plural) to improve prototypes. LLMs are black boxes with even more parameters than the algorithm that the paper is trying to interpret/visualize. I think the fundamental assumption of this paper is that LLMs are sufficiently intelligent that one can trust what LLM suggests, which I don't necessarily agree with. Moreover, I don't understand if it is necessary to employ multiple, repeated LLM inferences for training a simple text classifier.\n\nAlso, if I understood correctly, the latent prototype vectors will stay the same during the LLM optimization, which means that the LLMs will not contribute to the improvement of the text classification accuracy. If so, LLMs' role appears to be to simply tweak and wordsmith the textual representation of those prototypes. I don't know if this would necessarily lead to an \"improvement\" of interpretability.\n\nAdditionally, this paper will benefit a lot from improving the presentation. The current presentation does not elaborate on the core idea and concepts effectively, which required me multiple readings to understand what exactly was going on. Mathematical equations are also not very helpful due to insufficient rigor and details.\n\nTo be fair, I might be misunderstanding something about the scientific contribution of this work. However, in its current form, the rationale behind the use of LLMs to improve prototypes is unclear and how the optimization was implemented and conducted is also unclear. Hence, the low rating."}, "questions": {"value": "- Equation 7: Where does the LLM \\mathcal{L} play a role in this objective function? I suppose t is the output of LLM, but I'm not sure.\n- Section 4.3: I don't understand the rationale here. Especially, Figure 5: # of important concepts present in a prototype is much lower for optimized prototypes--> isn't that a bad thing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qHywzOVDLl", "forum": "tAtRvCRPBN", "replyto": "tAtRvCRPBN", "signatures": ["ICLR.cc/2026/Conference/Submission21456/Reviewer_3GPv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21456/Reviewer_3GPv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760716331388, "cdate": 1760716331388, "tmdate": 1762941788024, "mdate": 1762941788024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Dear AC and Reviewers"}, "comment": {"value": "We thank the reviewers for their thoughtful and detailed feedback. We were encouraged to see that Reviewer 6YW5 appreciated the work’s strong grounding in prototype theory, Reviewer Mibe found the motivation to address the limitations of standard projection methods important, and Reviewer 3GPv found the core paradigm of using LLMs for prototype optimization to be novel.\n\nWe recognize that our initial manuscript lacked clarity in several key areas, particularly regarding the qualitative analysis, the justification for human utility, and why Generative AI was not included as a baseline. We have uploaded a revised manuscript (changes in red) that addresses these concerns. We summarize the primary changes below:\n\n* **Qualitative Analysis & Concept Reduction:** Reviewers Mibe and 3GPv raised critical questions regarding Section 4.3—specifically, whether the loss of 43% of \"concepts\" in our optimized prototypes was a negative outcome. We realize our original explanation was insufficient. We have revised Section 4.3 to clarify that this reduction represents the successful filtering of noise rather than the loss of signal; we know this because predictive performance in the LLM-as-a-judge task was the same for both the baseline and our more concise optimized variants. Put another way, the test shows our method removes a lot of concepts that “seem important”, but actually are not, and only serve to increase user cognitive load and mislead [1, 2].\n* **Human Utility:** Reviewers 6YW5 and Mibe asked about the usefulness of our method for human users. Many excellent studies have already shown the utility of similar explanations for appropriate reliance and model improvement [4], we have expanded our discussion in Section 5 to explain how the 86% reduction in prototype size directly serves to lower cognitive load [1, 2], which the ML community agrees is useful [3]. Additional user studies are not required to verify this, but we agree it is a nice direction for future work.\n* **Expanded Baselines:** In response to Reviewer 6YW5, we have significantly expanded our baseline comparisons. Our early unreported experiments (now noted in Footnote 1) showed that LLMs were not competitive here, so we have added two additional state-of-the-art classifier language models to our evaluations—including ModernBERT (released Dec 2024)—to bolster our results. We are not focused on Generate AI here as that requires very different solutions beyond scope [5]. Note some results have changed, but all trend the same as before.\n* **Reproducibility & Details:** To address general requests for more training specifics, we have added Section 3.3 and Appendix E.\n\nWe believe these revisions directly address the main objections raised. We are confident that this work makes a substantial contribution to the prototype literature by introducing a paradigm that improves accuracy while reducing complexity, and we would be thrilled to present this work at ICLR!\n\n***\n\n[1] Miller, G. A. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review. \n\n[2] Lombrozo, T. (2007). Simplicity and probability in causal explanation. Cognitive Psychology.\n\n[3] Doshi-Velez, F. and Kim, B., 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608.\n\n[4] Chen, V., Liao, Q.V., Wortman Vaughan, J. and Bansal, G., 2023. Understanding the role of human intuition on reliance in human-AI decision-making with explanations. \n\n[5] Bodla, K.V. and Yang, H., 2025. Protocode: Prototype-Driven Interpretability for Code Generation in LLMs."}}, "id": "8ruccv6Utv", "forum": "tAtRvCRPBN", "replyto": "tAtRvCRPBN", "signatures": ["ICLR.cc/2026/Conference/Submission21456/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21456/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21456/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763659141104, "cdate": 1763659141104, "tmdate": 1763662008693, "mdate": 1763662008693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes using LLMs to directly optimize/generate prototypes that better reflect the learned representations. The experiments report more intelligible prototypes with performance comparable to projection baselines. This suggests a path to visualizing learned prototypes without relying on actual training instances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe work addresses one key limitation of standard projection-based methods.\n\n•\tThe work is well-motivated and important to ML interpretability."}, "weaknesses": {"value": "- High computational cost. This method uses LLMs as optimizers, which require multiple iterations and parallel LLM inferences per prototype and are thus computationally expensive. The paper does not provide a detailed analysis of the computational costs, particularly for cases with large numbers of classes and prototypes.\n\n- Lack of human evaluation. The qualitative analysis of the optimized prototypes relies solely on an \"LLM-as-a-judge\" framework. Without a user study or human annotations, there is no direct evidence that they are more intelligible or helpful for humans trying to follow the model’s reasoning.\n\n- Limited domain generalization. The method is validated only on text. While image extensions are suggested, the paper offers no details on how to adapt the paradigm to other domains, where producing abstract representations may be harder than generating text.\n________________________________________\nMinor typo:\n\tLine 147-148: {X}_(i=0)^N --> {X}_(i=1)^N\n\tLine 339-340: datum --> data"}, "questions": {"value": "- Lack of stagnation analysis. How does the \"optimizer\" handle local minima, where the LLM repeatedly generates candidates that show little or no improvement? Did the authors observe this in practice, and what techniques (if any) were used to escape such minima?\n\n- Choice of LLM. Why choose Meta-Llama-3-8B-Instruct as the optimizer model? Have you tried to use other LLMs? For example, would a less powerful model suffice? Would a more advanced model (e.g., GPT-5) yield even better prototypes?\n\n- Depth of concept preservation analysis. In the qualitative analysis (Section 4.3), the authors report that the optimized prototypes preserve 57% of the concepts found in the projected prototypes. Could the authors elaborate on why this is sufficient to support the claim of \"preserving most of the important concepts\"? Furthermore, has any deeper analysis been conducted into the nature of the concepts preserved versus those lost at 43%? For example, do the preserved concepts tend to be the most critical for the classification decision, while the omitted ones are more secondary or contextual?\n\n- Limited baseline comparison. The experiments mainly compare the LLM-based optimization method with the standard practice of projecting prototypes onto nearest neighbors. Why were additional text-classification baselines not evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B5lPy1ftGR", "forum": "tAtRvCRPBN", "replyto": "tAtRvCRPBN", "signatures": ["ICLR.cc/2026/Conference/Submission21456/Reviewer_Mibe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21456/Reviewer_Mibe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745282401, "cdate": 1761745282401, "tmdate": 1762941787531, "mdate": 1762941787531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on a specific form of interpretability: prototype based interpretability where the model explains its decisions by pointing to prototypes from the training data. The starting point of the paper is how the prototypes based explainability is typically done: learning the prototype vectors in the penultimate layer and then projecting them to the nearest training data point to generate the explanation. The paper points out that per the prototype theory of Rosch, prototypes should be abstract representations of the class, which is difficult to do for LLMs with large sequence lengths. The key idea of the paper is to use LLMs as optimization tools which can help build more concise, simple and general prototypes. The methodology is quite similar to the traditional work on prototype based learning where the training objective is a combination of different losses that ensure good classification accuracy and learning of distinct, grounded prototypes (Section 3.1). The paper then refines the prototype by using a LLM that summarizes the nearest neighbors of the initial prototype."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Model interpretability is indeed an important topic and lack of understandability is a large blocker is building trust in LLMs.\n2. The core idea of the paper is grounded in the prototype theory. It does indeed make sense that prototypes should not focus on spurious and overly specific features but represent more abstract concepts contained within the class."}, "weaknesses": {"value": "1. The writing of the paper can be improved to add key details. (i) What is the dataset level description and why is it needed? (ii) Instead of using multiple “meta-prompts” that consist of a random sample of nearest neighbors, why not use a single meta-prompt that uses all the neighbors? (iii) How is the number of nearest neighbors and the number of meta-prompts determined? Given a new dataset, should these be treated like a hyperparameter? If yes, what should the optimization objective be?  (iv) In line 292, what is the difference between a single LLM operating on all input data vs different LLMs operating on different sets of the training data?\n2. It is not clear how the idea would generalize to domains where its not the words like “spoof” and “technical quality” that are important, rather, its is the operators surrounding the words that have more importance, e.g., negation words like “not”. Is it possible that the prototypes will end up ignoring these small yet highly influential words? Some discussion that connects the makeup of prototypes to linguistic features would add a lot more weight to the paper’s contributions.\n3. The proposed solution seems to be restricted to simple classification based tasks (AG News, IMDB Movie Reviews, Amazon Reviews, 20 Newsgroups) and is tested on relatively simpler models like BERT and RoBERTa. The paper should discuss if, and how, the method would be extended to generation based tasks like summarization and if we expect it to work on instruction tuned LLMs like LLaMA and Qwen.\n4. It’s not clear what the prototypes add in terms of explainability for the end-user. Agreed that the prototypes learnt here are shorter, but what does that add for the end-user? The paper should provide some evaluation with humans showing that the prototypes learned here are actually helpful, e.g., perhaps they help users identify wrong classifications or remove poisonous data points."}, "questions": {"value": "1. Please see the questions in W1\n2. Why are the datasets different between Fig 3 / Table 1 and Fig 4 / Table 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "afnE8veC3k", "forum": "tAtRvCRPBN", "replyto": "tAtRvCRPBN", "signatures": ["ICLR.cc/2026/Conference/Submission21456/Reviewer_6YW5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21456/Reviewer_6YW5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023750496, "cdate": 1762023750496, "tmdate": 1762941787198, "mdate": 1762941787198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}