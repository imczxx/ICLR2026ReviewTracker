{"id": "1GkzDABbME", "number": 22359, "cdate": 1758330011012, "mdate": 1763697474231, "content": {"title": "Learning Mixtures of Linear Dynamical Systems (MoLDS) via Hybrid Tensor–EM Method", "abstract": "Linear dynamical systems (LDSs) have been powerful tools for modeling high-dimensional time-series data across many domains, including neuroscience. However, a single LDS often struggles to capture the heterogeneity of neural data, where trajectories recorded under different conditions can have variations in their dynamics. Mixtures of linear dynamical systems (MoLDS) provide a path to model these variations in temporal dynamics for different observed trajectories. \nHowever, MoLDS remains difficult to apply in complex and noisy settings, limiting its practical use in neural data analysis. Tensor-based moment methods can provide global identifiability guarantees for MoLDS, but their performance degrades under realistic noise and complexity. Commonly used expectation-maximization (EM) methods offer flexibility in fitting latent models but are highly sensitive to initialization and prone to poor local minima. Here, we propose a tensor-based moment method that provides identifiability guarantees for learning MoLDS, which can be followed by EM updates to combine the strengths of both approaches. The novelty in our approach lies in the construction of moment tensors using the input–output data, on which we then apply Simultaneous Matrix Diagonalization (SMD) to recover globally consistent estimates of mixture weights and system parameters. These estimates can then be refined through a full Kalman filter-smoother EM algorithm, with closed-form updates for all LDS parameters. We validate our framework on synthetic benchmarks and real-world datasets. On synthetic data, the proposed Tensor-EM method achieves more reliable recovery and improved robustness compared to either pure tensor or randomly initialized EM methods. \nWe then analyze neural recordings from the primate somatosensory cortex while a non-human primate performs reaches in different directions. In this setting, our method successfully models and clusters different conditions as separate subsystems, which is consistent with supervised single-LDS fits for each condition. Finally, we apply this approach to another neural dataset where monkeys perform a sequential reaching task. These results demonstrate that MoLDS provides an effective framework for modeling complex neural data in different brain regions, and that Tensor-EM is a principled and reliable approach to MoLDS learning for these applications.", "tldr": "We propose a novel approach combining tensor-based moments and EM refinement for learning mixtures of linear dynamical systems, which enables reliable and improved recovery of latent systems and is validated on synthetic and neural data.", "keywords": ["Mixture of linear dynamical systems", "Tensor-based moment method", "Expectation-Maximization", "Latent dynamical systems", "Neural data analysis"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b116995b133952068c83704fa1682e89413b903c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a two-stage pipeline for learning mixtures of linear dynamical systems (LDSs). Stage one follows the tensor-decomposition framework of Bakshi et al. to obtain consistent initial estimates under standard conditions. Stage two refines all LDS parameters via an EM procedure that uses responsibility-weighted sufficient statistics (updating dynamics, emissions, and initial states). A principled initialization for the noise covariances Q and \nR based on residual covariances is introduced. The method is applied to neural recordings from primate somatosensory cortex during reaching, illustrating the approach on a scientifically meaningful dataset."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is exceptionally well written and easy to follow.\n\nThe two-stage design is natural: tensor methods provide strong initializers with theoretical guarantees, while EM addresses the well-known limitations of pure tensor approaches by locally refining parameters.\n\nAssumptions are clearly stated and appropriate (persistent excitation; controllability/observability; component separation), with careful discussion of when they hold.\n\nA notable contribution is the explicit treatment of Q and R, which are often sidelined in prior tensor-based work; the residual-covariance initialization is principled and practical.\n\nThe overall study exemplifies well-designed statistical methodology: theory-grounded, reproducible, and attentive to the mechanics of LDS estimation (including filtering/smoothing considerations).\n\nThe application to neural data is compelling and broadens the paper’s relevance beyond synthetic studies."}, "weaknesses": {"value": "I can’t identify any obvious weaknesses, other than that it’s hard for me to evaluate data collected on primates beyond evaluating the statistical methodology, which as I’ve stated before is rock solid."}, "questions": {"value": "What further applications to the authors see for mixtures of LDS’s?  What further improvements can be made to their statistical pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "J1l4gsyikV", "forum": "1GkzDABbME", "replyto": "1GkzDABbME", "signatures": ["ICLR.cc/2026/Conference/Submission22359/Reviewer_vVHY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22359/Reviewer_vVHY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953468265, "cdate": 1761953468265, "tmdate": 1762942184264, "mdate": 1762942184264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a tensor-based moment method for learning mixtures of linear dynamical systems (MoLDS). The authors use Simultaneous Matrix Diagonalization on the moment tensors constructed from the input-output data to recover globally consistent estimates of mixture weights and system parameters, which are then refined with the Kalman smoother. The paper applies the proposed method to both synthetic and real-world neural datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Clarity: The paper is extremely well written, with clear motivation and definition of the problem, related work, and where the proposed method stands (i.e., Contributions). It also has a clear explanation of the method with appropriate equations and algorithm pseudocode, and the Figures are well presented with descriptive captions.\n\n* Evaluation: The method is tested extensively on both simulated and real-world neural datasets. The results are well presented, showing the advantage of the proposed method over the compared methods.\n\n* Significance: The paper is well motivated, setting up the existing problem of the difficulty of learning MoLDS despite its usefulness. The paper tackles this problem and demonstrates its effectiveness in analyzing neural data."}, "weaknesses": {"value": "As the authors noted, one of the limitations of the paper is that the proposed method is only evaluated on datasets that have linear dynamics. A more detailed explanation of what could happen when there’s a model mismatch, in addition to how the method could be extended to mitigate the mismatch, would be valuable."}, "questions": {"value": "I am curious about how the proposed method will perform as we scale up the size of the dataset in terms of the observations and latent dimension. In general, I would be interested in stress testing the proposed method by varying different experimental axes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7tyQhMzu00", "forum": "1GkzDABbME", "replyto": "1GkzDABbME", "signatures": ["ICLR.cc/2026/Conference/Submission22359/Reviewer_tLS4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22359/Reviewer_tLS4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986805482, "cdate": 1761986805482, "tmdate": 1762942184009, "mdate": 1762942184009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new fitting procedure for modeling heterogeneous time-series data based on Mixtures of Linear Dynamical Systems (MoLDS) by introducing a robust Tensor-EM algorithm. The authors propose combining tensor-based moment estimation with EM refinement and show that this method achieves more accurate and efficient results than alternative methods based on single LDSs (Linear Dynamical Systems) or switching LDSs. They also extend the application to neural data and exemplify the method on complex recordings of a reaching task and neural recordings from premotor cortex to demonstrate the applicability of their MoLDS framework to neuroscience."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The authors enhanced the MoLDS model to achieve improved numerical stability and robustness to noise via Simultaneous Matrix Diagonalization (SMD) compared to prior tensor methods.\n2) The authors extend the tensor initialization paradigm by integrating it with a Kalman filter-smoother EM procedure.\n3) The method, which uses LDSs, maintains representation interpretability.\n4) The authors demonstrated their results on two synthetic experiments and two real-world neural datasets."}, "weaknesses": {"value": "1) My biggest concern relates to the extension of MoLDS to neural data. Neural activity exhibits complex temporal evolution that changes over time, potentially even within a single trial. While I understand this approach enables interpretability by using a single LDS per trial, it cannot capture the temporally changing nature of brain dynamics, which is a major question in neuroscience. Hence, I wonder whether neuroscience is the right application to demonstrate the method. Alternatively, how could the method enable understanding of varying brain interactions over time? For example, could your fitting procedure be extended to more complex LDS models that change over time (e.g., SLDS or dLDS)?\n\n2) It seems you treat the \"per-direction single-LDS\" as a gold standard. I understand that in real data we lack access to the true operators, so estimation is necessary. However, I am concerned that per-direction single-LDS oversimplifies the dynamics. Given the complexity of neural data, including trial-to-trial variability due to unobserved factors even under the same hand direction, this baseline may yield biased comparisons.\n\nHave the authors examined trial-to-trial variability within a direction and its effect on your comparison? Specifically, what happens if you train both your method and the per-direction single-LDS on random subsets of trials within a direction, repeating this process with different subsets? Does performance remain stable?\n\nWith respect to that, I also think a missing comparison is to train a single LDS per trial, then perform clustering on these independently trained LDSs to verify whether they capture trial direction as you implicitly assume. After validating that LDSs cluster by direction (without forcing a single LDS on all trials from a direction), you could then compare this to your method\n\n3) I think the paper could be written more clearly, as it is currently difficult to follow. The utility and importance of certain components are unclear. For example, more background information on SMD and its contribution to stability would be helpful in the related work section. Additionally, please explain more explicitly how SMD was applied in your method. I would also suggest dedicating space to explaining and defining the moments and W in Eq. 3. Finally, in the LDS paragraph (lines 54-59), there should be discussion of other LDS models that support multiple LDSs with mixtures, like dLDS [1,2].\n\n4) The abstract is very long and difficult to follow. I suggest condensing everything from `We validate our framework on...' to the end into a single sentence, which would also save space in the main text.\n\n5) I am not sure why the method illustration appears only in Figure 3. This illustration is helpful for understanding the paper and should appear earlier\n\n5) Some smaller comments on writing and presentation issues that should be fixed: 1) Please pay attention to \\cite vs. \\citep, it seems all references appear as part of the text (no brackets), which hinders readability. 2) Hyphens are used instead of em dashes in several places. 3) In Fig. 4b, the subplots are ordered oddly (dim 4-0-1-5...). \n- Some typos and formatting issues: extra bracket in line 466, double (c)(c) in line 240, missing space before reference in line 060.\n\n[1] Mudrik, N., Chen, Y., Yezerets, E., Rozell, C. J., & Charles, A. S. (2024). Decomposed linear dynamical systems (dlds) for learning the latent components of neural dynamics. Journal of Machine Learning Research (JMLR)\n\n[2] Chen, Y., Mudrik, N., Johnsen, K. A., Alagapan, S., Charles, A. S., & Rozell, C. (2024). Probabilistic decomposed linear dynamical systems for robust discovery of latent neural dynamics. Advances in Neural Information Processing Systems (NeurIPS)"}, "questions": {"value": "1) Could you clarify how you performed the SLDS comparison? Specifically, did you take the most active operator from SLDS trained on all trials together? How did you define the number of operators? What happens if you include stronger penalization on the number or frequency of switches per trial? I would assume that with strong penalization you may be able to capture the individual dominant operator as in MoLDS.\n\n2) Regarding my first concern about non-stationarity within a trial, I am curious what happens if you split each trial into multiple periods (e.g., stimulus presentation vs. reaching period) and run your method treating these as separate trials. Do you get the same operator being active in both periods, or do they switch?\n\n3) Have you examined multi-step reconstruction instead of single-step?\n\n4) It seems you focused mainly on the dominant component from MoLDS. I am curious about the distribution of probabilities for different LDSs within a single trial. Is the probability of the second component, for example, very close to that of the first? If so, what does the second component look like? Is it also shared between trials of similar direction? (I.e., for a fixed and large K, unlike Figure 4b where you appear to modulate K).\n\n5) Could you explain the rationale for choosing a test-val-train split over cross-validation?\n\n6) Could you clarify the difference between Sections 4.1 and 4.2 in terms of which version of your method was used? For instance, in Section 4.1, did you use only SMD with randomly initialized EM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MplFEZXzER", "forum": "1GkzDABbME", "replyto": "1GkzDABbME", "signatures": ["ICLR.cc/2026/Conference/Submission22359/Reviewer_Lrdy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22359/Reviewer_Lrdy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762341977278, "cdate": 1762341977278, "tmdate": 1762942183776, "mdate": 1762942183776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a new approach towards fitting Mixture of Linear Dynamical Systems models, using tensor-based methods followed by EM refinement. The paper demonstrates the superiority of their model-fitting approach on simulated data. The paper then fits the model to two neural activity datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-The authors demonstrate a model that (to my knowledge) hasn't been used in neuroscience data before\n-The authors show convincing results of the benefit of their new tensor-EM fitting approach in simulated data (and quantitatively versus standard EM approaches on one real neural dataset)"}, "weaknesses": {"value": "The importance of MoLDS for modeling neural data (which is the focus of this paper) never got across clearly to me. It feels like the authors take that importance as a given, but this is a method that hasn't been used (to my knowledge) in neural data, so really clearly motivating the need is essential - at the moment, it's primarily a sentence on line 60-61. When are specific examples when a neuroscience researcher would want to use MoLDS when existing approaches would fail? The real data examples don't make this clear, since it seems like you could just fit LDS models to different reach directions in those scenarios (they don't show any benefit of an unsupervised approach), and it's especially unclear what to take away from the results on the PMd dataset. \n\nAdditionally, the comparisons to alternative approaches for the real datasets were lacking. There was not a comparison to the tensor approach to fitting, so it's currently not clear whether this paper's fitting approach is advantageous relative to pure tensor methods. Additionally, while the paper shows a numerical advantage versus a pure EM approach, there's no advantage shown in terms of the model analysis (e.g. within Fig 4c). So in general, the advantages of this paper's model-fitting approach aren't currently shown strongly."}, "questions": {"value": "1. It took me a very long time into the paper to really understand how MoLDS was different than an SLDS. It would help readers a lot if you more explicitly compared them early on in the methodology, since SLDS is much more common in comp neuro (which will be the primary readers of this paper).\n2. I'd clarify why the RTPM method is being used for a comparison - what past paper was this used in?\n3. It seems very odd to be doing PCA on Area 2 data prior to fitting MoLDS, when generally an advantage of LDS models is that they are directly learning the underlying latent states from high-D data.\n4. Relatedly, does the PMd experiment also use PCs or all neurons?\n5. Line 72-79 - if possible, some citations for pros/cons of approaches could be useful (although I realize sometimes these insights are from personal experience)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mi7ohFg1xs", "forum": "1GkzDABbME", "replyto": "1GkzDABbME", "signatures": ["ICLR.cc/2026/Conference/Submission22359/Reviewer_jFVZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22359/Reviewer_jFVZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762664638849, "cdate": 1762664638849, "tmdate": 1762942183542, "mdate": 1762942183542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}