{"id": "IkDz6wxeTG", "number": 15184, "cdate": 1758248706824, "mdate": 1759897322780, "content": {"title": "AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models", "abstract": "Medical image segmentation is vital for clinical diagnosis, yet current deep learning methods often demand extensive expert effort, i.e., either through annotating large training datasets or providing prompts at inference time for each new case. This paper introduces a zero-shot and automatic segmentation pipeline that combines off-the-shelf vision-language and segmentation foundation models. Given a medical image and a task definition (e.g., \"segment the optic disc in an eye fundus image\"), our method uses a grounding model to generate an initial bounding box, followed by a visual prompt boosting module that enhance the prompts, which are then processed by a promptable segmentation model to produce the final mask. To address the challenges of domain gap and result verification, we introduce a test-time adaptation framework featuring a set of learnable adaptors that align the medical inputs with foundation model representations. Its hyperparameters are optimized via Bayesian Optimization, guided by a proxy validation model without requiring ground-truth labels. Our pipeline offers an annotation-efficient and scalable solution for zero-shot medical image segmentation across diverse tasks. Our pipeline is evaluated on seven diverse medical imaging datasets and shows promising results. By proper decomposition and test-time adaptation, our fully automatic pipeline not only substantially surpasses the previously best-performing method, yielding a 69\\% relative improvement in accuracy (Dice Score from 42.53 to 71.81), but also performs competitively with weakly-prompted interactive foundation models.", "tldr": "We propose a zero-shot and fully automatic medical image segmentation method that leverages pretrained vision foundation models with test-time adaptions.", "keywords": ["Medical Image Segmentation", "Vision Language Model", "Segment Anything Model", "Test Time Adaptation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4445300eb5e4829096e4fe0642299db521954b57.pdf", "supplementary_material": "/attachment/22a22f3a2d18a86bb5bd5cbc372e716a57b8f4a2.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a prompt-guided semantic segmentation framework that adapts generic pre-trained models to medical imaging tasks without task-specific fine-tuning. The method emphasizes efficiency by combining test-time augmentations, automated hyperparameter selection via Bayesian optimization, and prompt-refinement strategies to steer predictions. The contribution targets two core challenges: (1) refining the outputs of a pre-trained model and (2) adapting that model to the target domain through an automated, data-driven procedure. The authors report competitive—and in several cases state-of-the-art—results, supported by ablations and validation strategies integrated into the pipeline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation and solid empirical support: The manuscript is generally well written and easy to follow. The identified problem is timely, and the proposed approach achieves competitive (often SOTA) performance according to the reported experiments.\n- Ingenious combination of known components: Test-time augmentation, AutoML/Bayesian optimization, and prompt refinement are combined to address complementary facets of the problem in a thoughtful way.\n- Automation and practicality: The pipeline is fully automated, including a validation mechanism and domain adaptation procedure, which enhances usability in real-world settings where fine-tuning may be impractical."}, "weaknesses": {"value": "- Limited exposition of module interactions: While each component is described, the paper would benefit from a clearer explanation—ideally a schematic—of how the modules interact end to end (prompting - refinement - TTA - Bayesian optimization). Empirical analysis of interdependencies is also missing. In particular, please assess robustness when upstream (pre-trained) elements underperform: How sensitive is the pipeline to weaker grounding models or to degradation/failures in the refinement block?\n- Variance and robustness reporting: Medical imaging results can be highly seed-dependent. Reporting mean ± standard deviation over multiple runs (e.g., 3–5 seeds) for key tables would help assess robustness. If feasible, include variance under common sources of distribution shift (e.g., scanner/site differences).\n- The efficiency claim would be stronger with clearer accounting of computational cost (e.g., TTA overhead, Bayesian optimization budget, wall-clock time and GPU hours) relative to fine-tuning baselines at matched accuracy.\n- The main contribution lies in the orchestration of established components (prompting, TTA, Bayesian optimization), while the paper’s genuinely new additions appear limited; please better isolate, justify, and quantify the novel parts beyond the integration engineering."}, "questions": {"value": "1. How do performance gains decompose across prompt refinement, TTA, and Bayesian optimization when combined versus in isolation? Any observed synergistic or antagonistic effects?\n2. How sensitive are results to prompt wording/length and the number of prompts? Is there an automated prompt search, or are prompts seeded from domain knowledge?\n3. How does the approach generalize across different pre-trained backbones or foundation models? Is there a degradation when swapping to weaker models?\n4. How does the pipeline handle cross-site or cross-scanner generalization without fine-tuning? Have you tested adaptation using only unlabeled target data from a new site?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GOIv12sK2k", "forum": "IkDz6wxeTG", "replyto": "IkDz6wxeTG", "signatures": ["ICLR.cc/2026/Conference/Submission15184/Reviewer_t9YK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15184/Reviewer_t9YK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761501071594, "cdate": 1761501071594, "tmdate": 1762925490646, "mdate": 1762925490646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an automatic zero-shot medical image segmentation pipeline named **AutoMiSeg**, which requires neither manual annotations nor human prompts during inference.  The proposed method integrates vision-language models with promptable segmentation models and introduces a novel **Test-Time Adaptation (TTA)** framework.  The core innovation lies in a set of learnable Learnable Test-time Adaptors (LTAs) that align input domains, whose parameters are automatically optimized via Bayesian Optimization, guided by a Proxy Validator that provides feedback for performance improvement."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work is likely the first to combine zero-shot segmentation based on foundation models with test-time adaptation, forming a fully automatic and training-free system.  \n\n2. The modular architecture is well-structured, achieving a stable compositional zero-shot inference process through a clear task decomposition (Grounding → Prompt Boosting → Segmentation).  \n\n3. The experiments cover 7 medical imaging datasets (including fundus, ultrasound, MRI, endoscopy, and dermoscopy), providing comprehensive and comparable results with important ablation studies included."}, "weaknesses": {"value": "1. The core technical novelty is relatively limited, as the main contribution lies in system integration and pipeline optimization, while each module is built upon existing public methods. It is recommended to open-source the full zero-shot segmentation pipeline, which would significantly enhance the paper’s community value.  \n\n2. The Bayesian Optimization process involves 100 iterations per dataset, yet the **inference time and computational cost** are not sufficiently reported or discussed.  \n\n3. The model’s robustness under **noisy inputs, modality shifts, or grounding failures** has not been evaluated, leaving the discussion on clinical deployment stability incomplete.  \n\n4. The related work comparison is insufficient. For instance, recent studies such as **TV-SAM** and other multimodal zero-shot medical segmentation methods share similar ideas and should be explicitly discussed for a fair contextualization."}, "questions": {"value": "1. Could the Bayesian Optimization process be accelerated through **few-shot warm-starting** or **meta-learning** techniques?  \n\n2. It is recommended to provide the **correlation statistics between the Proxy Score and the ground-truth Dice** to demonstrate the reliability of the proxy validation mechanism.  \n\n3. In a fully automatic segmentation scenario **without human intervention**, how can potential erroneous decisions be prevented from being directly applied in clinical workflows? The authors should further elaborate on **safety strategies** in the Discussion section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EnJAPtCszP", "forum": "IkDz6wxeTG", "replyto": "IkDz6wxeTG", "signatures": ["ICLR.cc/2026/Conference/Submission15184/Reviewer_2vfY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15184/Reviewer_2vfY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926737066, "cdate": 1761926737066, "tmdate": 1762925489469, "mdate": 1762925489469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a fully automatic segmentation pipeline AutoMiSeg for zero-shot medical image segmentation. Specifically, it first uses a large language model (ChatGPT-4o) and a vision-language grounding model (CogVLM) to generate a bounding box from a text description, then refines it with a visual prompt booster (DINOv2), and finally produces a segmentation mask with a promptable model (SAM). It also integrates a test-time adaptation framework that uses Bayesian Optimization to tune a set of Learnable Test-time Adapters (LTAs). Experimental results demonstrate that the proposed method achieves superior segmentation performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proposed method is well-motivated in being fully automatic and training-free, thereby enabling zero-shot medical image segmentation.\n2.\tThe authors introduce Learnable Test-time Adaptors (LTAs) and a surrogate validation model to evaluate the segmentation outputs, whose feedback is utilized to optimize the LTAs."}, "weaknesses": {"value": "1.\tLimited Novelty: The core technical components of the pipeline are not novel. The paper primarily combines existing techniques: a grounding model (CogVLM/Grounding DINO), a feature-based prompt booster (inspired by CoVP), and a segmenter (SAM) are connected sequentially. The novelty lies in the specific integration and application to this problem, rather than in the invention of new core algorithms.\n2.\tThe evaluation of zero-shot baselines may not be fully comprehensive. Notably, the cited SaLIP model [1] has demonstrated strong performance (e.g., 80%–95% Dice) on modalities like brain MRI and lung CT in its original publication. Its comparatively low performance on the selected datasets in this work (e.g., ~30% Dice on Kvasir and ISIC2016) suggests a potential underutilization of the model or a high sensitivity to specific image domains that warrants further investigation.\n\n[1] Sidra Aleem, Fangyijie Wang, Mayug Maniparambil, Eric Arazo, Julia Dietlmeier, Kathleen Curran, Noel EO’ Connor, and Suzanne Little. Test-time adaptation with salip: A cascade of sam and clip for zero-shot medical image segmentation. In CVPR, 2024.\n\n3.\tThe supervised models used for comparison in Table 1 (e.g., ResNet-18, ResNet-50) are not representative of state-of-the-art architectures specifically designed for medical image segmentation. \n4.\tTo more robustly validate the advantages of AutoMiSeg, it is recommended to include a wider array of medical imaging modalities (e.g. CT) and anatomical regions. This would provide a more complete and convincing demonstration of the pipeline's generalizability and superior performance across the full spectrum of medical imaging tasks.\n5.\tInadequate Justification for the TTA Design: While the proposed Learnable Test-time Adapters (LTAs) are central to the method's performance, the paper provides limited justification for their specific design. There is a lack of comparative experiments with established Test-Time Adaptation (TTA) techniques (e.g., entropy minimization, feature alignment) to demonstrate that the chosen Bayesian Optimization over image transformations is a superior or even competitive strategy. This omission makes it difficult to assess the true contribution and novelty of the LTA module. \n6.\tFurthermore, the architectural integration of the LTAs within the pipeline remains unclear. For better clarity and reproducibility, it is strongly recommended to include a detailed structural diagram of the LTAs in Figure 2, explicitly illustrating how the transformations are applied and how the hyperparameters are optimized and fed into the different modules."}, "questions": {"value": "please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V63jHyHKEh", "forum": "IkDz6wxeTG", "replyto": "IkDz6wxeTG", "signatures": ["ICLR.cc/2026/Conference/Submission15184/Reviewer_4xZR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15184/Reviewer_4xZR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928774952, "cdate": 1761928774952, "tmdate": 1762925488855, "mdate": 1762925488855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AutoMiSeg, a novel pipeline for fully automatic, zero-shot medical image segmentation. The pipeline compositionally leverages existing foundation models: a vision-language model grounds the target from a text description, a prompt booster refines the input, and a segment-anything model generates the mask. A key contribution is the introduction of a test-time adaptation framework to address the domain gap while applying those general models on medical images, which uses Learnable Test-time Adaptors (LTAs). Their hyperparameters are optimized via Bayesian Optimization, guided by a cleverly designed proxy validator that assesses segmentation quality without ground truth."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1.\tThe method is extensively evaluated on seven diverse medical imaging datasets, where it substantially outperforms all existing zero-shot baselines and achieves competitive performance with weakly-supervised interactive models. \n2.\tThe Bayesian Optimization (BO) framework for Test-Time Adaptation is practical and efficient. It is guided by the custom proxy validator, whose score is demonstrated to have a strong correlation with the true segmentation performance (as measured by the Dice score) making the adaptation both target-aware and effective. \n3. The BO strategy is data-efficient. It requires only a minimal number of objective evaluations (on a small, unlabeled subset of the target data) to converge to a robust set of hyperparameters for the entire dataset. This low sample complexity is critical for adapting to a new imaging modality or clinical site without the need for extensive data collection or costly retraining.\n4. Instead of the conventional approach of fine-tuning the massive foundation models themselves—a process that demands substantial labeled data and immense computational resources—this work shifts the focus to bridging the domain gap at the input level. This design ensures future-proofing and seamless compatibility with model evolution. The pipeline can directly and immediately benefit from any future improvements in these general-purpose models (e.g., a more powerful SAM 3.0 or a more grounded VLM). Simply plugging in a better base model would likely boost AutoMiSeg's performance without any change to its core adaptation mechanism."}, "weaknesses": {"value": "1. A notable methodological limitation lies in the design of the Proxy Validator. The validation process relies on evaluating the segmented region in isolation by masking out the background. While this is an effective proxy in many scenarios, it overlooks a fundamental aspect of medical image interpretation: context can be critical for accurate identification. By isolating the candidate region, the validator discards this contextual information. Although the results demonstrate a strong correlation between the validator's score and the Dice coefficient, this weakness may limit the framework's robustness and accuracy for certain tasks where contextual reasoning is paramount, potentially causing optimization failures in these edge cases."}, "questions": {"value": "1. Do you have any experiment results for comparison with LoRa or other peft models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QcTLK60MSY", "forum": "IkDz6wxeTG", "replyto": "IkDz6wxeTG", "signatures": ["ICLR.cc/2026/Conference/Submission15184/Reviewer_kPfd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15184/Reviewer_kPfd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940929613, "cdate": 1761940929613, "tmdate": 1762925488509, "mdate": 1762925488509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}