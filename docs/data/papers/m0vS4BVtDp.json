{"id": "m0vS4BVtDp", "number": 18070, "cdate": 1758283480741, "mdate": 1763112473853, "content": {"title": "VIRO: Efficient and Robust Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension", "abstract": "Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural language query. To handle complex queries, recent work has focused on compositional reasoning, with advances in Large Language Models (LLMs) and Vision Language Models (VLMs) enabling the decomposition of queries into executable programs within reasoning pipelines. \nHowever, existing approaches implicitly assume the target is always present, forcing the model to output a result even when no valid referent exists. Moreover, multi-step reasoning processes often result in high computational costs, limiting their application in real-time scenarios. To address this limitation, we propose Verification-Integrated Reasoning Operators (VIRO), which integrate operator-level verification into a neuro-symbolic pipeline, enabling abstention and the explicit handling of no-target cases. Each operator performs a reasoning step and verifies its own execution, including a lightweight CLIP-based filter with minimal computational overhead, and logical verification for spatial and relational constraints. Experimental results demonstrate that our framework achieves strong robustness in no-target cases, achieving 61.1% balanced accuracy, while showing state-of-the-art accuracy on standard REC benchmarks, compared to compositional baselines. Our neuro-symbolic pipeline also shows superior computational efficiency, high reliability with a program failure rate of just 0.3%, and scalability—achieved by decoupling program generation from execution.", "tldr": "We propose Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework for REC that explicitly handles no-target scenarios and improves computational eff", "keywords": ["Referring Expression Comprehension (REC)", "Visual Grounding", "Compositional Reasoning", "Neuro-symbolic Reasoning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2f201326ee7b4cd21ce3263f595a5f9201387eab.pdf", "supplementary_material": "/attachment/42437f862992a7c0c4d98cf4970cf7dbc5608318.pdf"}, "replies": [{"content": {"summary": {"value": "- The authors propose VIRO — Verification-Integrated Reasoning Operators, a neuro-symbolic framework for Referring Expression Comprehension (REC) to solve the task of identifying an image region that corresponds to a natural-language description.\n\n- VIRO bridges symbolic logic and neural vision-language reasoning to create an efficient, interpretable, and verifiable REC system.\n\n- VIRO improves robustness (especially in “no target” scenes) and computational efficiency, key for real-time applications like robot perception and multimodal AI systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### 1. Technical Strengths\n\n- Each reasoning operator (e.g., FIND, FIND_DIRECTION) has built-in verification, enabling self-checking and early termination — a novel design in neuro-symbolic reasoning.\n\n- Unlike prior models that always predict a box, VIRO can abstain when no valid referent exists, increasing robustness and realism.\n\n- CLIP-based Uncertainty Filter: Efficiently removes false positives from open-vocabulary detectors with minimal overhead.\n\n- Logical spatial verification ensures relational consistency (“left of”, “inside”, etc.) through simple geometric checks.\n\n- Decoupled pipeline separates program generation (by LLM) from execution, allowing one query to be reused across multiple images — improving latency and scalability.\n\n- Neuro-Symbolic integration combines the interpretability and modularity of symbolic reasoning with the perception strength of neural models (OVD + CLIP + DepthAnything).\n\n- Program validation & grammar checking ensures syntactic correctness of generated programs before execution, avoiding runtime errors common in prior systems like HYDRA and NAVER.\n\n- Early-Exit mechanism stops reasoning when conditions fail (e.g., missing object), saving compute and improving responsiveness.\n\n- The modular design could extend to other tasks — e.g., visual question answering, grounded instruction following, or embodied agents.\n\n### 2. Experimental Strengths\n\n- The authors have benchmarked their proposed methods on both standard (RefCOCO/+/g) and no-target (gRefCOCO) datasets. Evaluates robustness, efficiency, and scalability. The reported results are strong. For example,  61.1% balanced accuracy on no-target cases (state-of-the-art for zero-shot). 71.9% standard REC accuracy, outperforming compositional baselines. 0.3% failure rate vs. 6–32% for competing neuro-symbolic methods. Fast execution (1.39 FPS) and reduced latency through early exits.\n\n- Clear component-level validation showing contributions of verification modules, CLIP thresholds, and early-exit design.\n\n- Quantifies trade-offs between TPR (precision) and TNR (robustness).\n\n- Scalability test (1-query–N-images) demonstrates linear scalability due to decoupled design; major advantage for multi-image tasks like robot search.\n\n- Visual examples clearly show suppression of false positives — interpretable verification behavior."}, "weaknesses": {"value": "### 1. Technical Limitations\n\n- Performance depends on external pretrained models (GroundingDINO, CLIP, DepthAnything). Errors in these propagate to reasoning steps.\n\n- Operators handle basic spatial and attribute reasoning, but not more complex temporal or abstract relations (e.g., “the man looking at the dog”).\n\n- CLIP-based uncertainty filter uses threshold calibration (even adaptive per-label), which may not generalize across domains or unseen categories.\n\n- Requires a large language model for program generation. Although decoupled, quality still depends on LLM prompt engineering and examples.\n\n- Fixed operator grammar may limit reasoning flexibility compared to free-form program generation (e.g., ViperGPT’s Python-like reasoning).\n\n- CLIP’s training biases (ImageNet-like categories) may skew verification toward frequent object types, missing rare or domain-specific terms.\n\n### 2. Experimental Limitations\n\n- Lack of Cross-Domain Evaluation: Tests limited to COCO-derived datasets; robustness on unseen domains (e.g., indoor scenes, robotics) not demonstrated.\n\n- Limited Real-World Deployment Evidence: Efficiency gains shown in simulation (FPS, latency), but no evidence of integration into real-time or robotic systems.\n\n- No Human Evaluation: While interpretability is claimed, no human studies assess clarity or usefulness of the generated reasoning traces.\n\n- Balanced Accuracy Metric Only: Uses balanced accuracy as the key metric; other useful measures (precision-recall, F1, calibration error) not explored.\n\n- Limited Comparison Scope: Although major baselines (HYDRA, NAVER, ViperGPT) are included, newer multimodal transformers (e.g., GPT-4V, Gemini) not tested."}, "questions": {"value": "See the weakness section. In addition:\n\n- How easily can the VIRO framework incorporate new reasoning operators (e.g., temporal, causal, or commonsense relations)?\n\n- Would adding such operators require retraining or re-prompting the LLM?\n\n- The CLIP-based uncertainty filter uses adaptive thresholds — how sensitive is performance to these threshold settings?\n\n- Could self-calibrating or Bayesian uncertainty estimation methods improve reliability?\n\n- How does VIRO handle ambiguous or under-specified queries (e.g., “the man near the big thing”)?\n\n- Does the LLM's program generation incorporate any self-check or consistency verification?\n\n- Why was CLIP chosen for verification instead of more specialized models like SigLIP or ImageBind?\n\n- Have you considered multi-modal confidence fusion across detectors instead of relying on CLIP similarity scores alone?\n\n- Given the reported FPS (~1.4), how feasible is VIRO for real-time applications like robotic perception or human-robot dialogue?\n\n- What are the computational bottlenecks (e.g., detector inference, CLIP verification, or symbolic execution)?\n\n- The paper emphasizes decoupling of program generation from execution. Are there cases where query-specific context (e.g., image content) should influence program generation dynamically?\n\n- Could fully static programs miss context-dependent reasoning patterns?\n\n- How does VIRO scale when handling long, compositional, or nested referring expressions (e.g., “the second man from the left holding a red cup behind the woman”)?\n\n- Are there limits on program depth or recursion?\n\n- Since the Open-Vocabulary Detector (OVD) may hallucinate unseen objects, could VIRO’s verification be misled by dataset bias or object frequency imbalance?\n\n- Have you observed systematic failures across specific object types (e.g., small, occluded, or rare objects)?\n\n- How much does the LLM’s choice (e.g., GPT-4 vs. Qwen2.5 vs. LLaMA) affect program structure, syntax errors, or reasoning efficiency?\n\n- Balanced Accuracy is used as the main metric — did you also assess calibration, abstention accuracy, or false discovery rate for no-target handling?\n\n- How does VIRO’s abstention threshold affect trade-offs between precision and recall?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fngL3xKkTS", "forum": "m0vS4BVtDp", "replyto": "m0vS4BVtDp", "signatures": ["ICLR.cc/2026/Conference/Submission18070/Reviewer_RR4e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18070/Reviewer_RR4e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760944076636, "cdate": 1760944076636, "tmdate": 1762927855712, "mdate": 1762927855712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We have decided to withdraw this submission from consideration. We sincerely appreciate the reviewers’ constructive feedback and their efforts in evaluating our work. Their comments have provided valuable insights that will greatly help us strengthen this work in future revisions."}}, "id": "9qGAC47TOC", "forum": "m0vS4BVtDp", "replyto": "m0vS4BVtDp", "signatures": ["ICLR.cc/2026/Conference/Submission18070/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18070/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763112472921, "cdate": 1763112472921, "tmdate": 1763112472921, "mdate": 1763112472921, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to add an extra filtering step to the output object list of referring expression compression (REC), to make sure no mismatches are returned. It claims to outperform existing REC methods with a large margin on the gRefCoCo no-target dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The presentation is clear and easy to follow."}, "weaknesses": {"value": "1. The added extra filtering step on the (image crop, caption) pair by CLIP, is kind of trivial. Moreover, the proposed method is based on a manually written program, which is a step backward from the trending end-to-end approaches.\n2. The reported performance of baselines is very low on gRefCOCO no-target. For example, NAVER has an N-acc (no-target accuracy) of 3.4%. I find this result dubious, since in the code of NAVER, the authors explicitly checked the results returned by the logic reasoner. If there is no match, it will return an empty list:\nhttps://github.com/ControlNet/NAVER/blob/4b0ae074701e57882a8b712bac09e15a73f005bc/naver/agent/logic_reasoning/logic_reasoner.py#L59 \\\nI don't see why it would achieve such a low N-acc."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rS8ggW4Gdy", "forum": "m0vS4BVtDp", "replyto": "m0vS4BVtDp", "signatures": ["ICLR.cc/2026/Conference/Submission18070/Reviewer_mXRX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18070/Reviewer_mXRX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007305944, "cdate": 1762007305944, "tmdate": 1762927855019, "mdate": 1762927855019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VIRO, which is a neuro-symbolic method for referring expression comprehension that can clearly tell when the target object does not exist. It turns the input sentence into a short program made of step-by-step checks. If any check fails, the system stops early and outputs “no target” instead of forcing a wrong bounding box."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The designed each operator verifies its own result and can abort the pipeline when evidence is insufficient. \nThe method requires no task-specific fine-tuning or new datasets, lowering engineering overhead and easing deployment."}, "weaknesses": {"value": "This paper is of limited novelty and motivation. Program-based visual question answering have been studied in prior program-execution works[1,2]. The paper positions verification as a first-class element but does not clearly establish a more fundamental new method. \n\nThe methodological contribution is limited. The approach mainly adds operator-level verification and early-exit checks to an not novel program pipeline. There is no new learning/training objective, or architecture novelty but more as engineering refinements. \n\nThe overall system relies on open-vocabulary detectors and other fundational models, so their errors and biases propagate through the pipeline which is severe for OOD cases(for fundamental models) as the system is training-free. Also the evaluation is centered on RefCOCO and gRefCOCO. \n\n[1] ViperGPT: Visual Inference via Python Execution for Reasoning [2] Visual Programming: Compositional visual reasoning without training"}, "questions": {"value": "What specific capability does VIRO have more than previous methods which could not achieve?\nCould the authors please provide evaluations or evidence on more diverse benchmarks to validate generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "osPXYl58pD", "forum": "m0vS4BVtDp", "replyto": "m0vS4BVtDp", "signatures": ["ICLR.cc/2026/Conference/Submission18070/Reviewer_8mAU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18070/Reviewer_8mAU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008342144, "cdate": 1762008342144, "tmdate": 1762927854560, "mdate": 1762927854560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We have decided to withdraw this submission from consideration. We sincerely appreciate the reviewers’ constructive feedback and their efforts in evaluating our work. Their comments have provided valuable insights that will greatly help us strengthen this work in future revisions."}}, "id": "i0yxroFqic", "forum": "m0vS4BVtDp", "replyto": "m0vS4BVtDp", "signatures": ["ICLR.cc/2026/Conference/Submission18070/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18070/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18070/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763112399785, "cdate": 1763112399785, "tmdate": 1763112399785, "mdate": 1763112399785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a neurosymbolic approach that addresses the problem of referring expression comprehension (REC) with compositional prompts and to enable efficient processing of cases wherein a referring target is not present in the image (which conventional approaches still allocate compute towards trying to solve and have high-confidence false positive rates). The authors use a few-shot prompted LLM to process an input REC-phrase into a symbolic program (for which operators are defined by authors), then a lightweight CLIP-based uncertainty filtering to identify cases wherein a REC phrase object is not present, and existing task-specific models for each defined symbolic operation. Experiments on REC benchmarks show the proposed method reduces false positive rates and error rates compared to existing compositional methods and baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach is useful to reduce false positive rates and unnecessary compute on REC cases wherein the phrase is not present in the image. \n2. Experiments show beneficial results of the method in reducing false positive rates and improving true positive rates compared to existing program-based compositional reasoning baselines. The method is also more robust and has a lesser failure rate than existing compositional reasoning baselines.\n3. Ablations and additional analysis on early stopping, qualitative cases and detection thresholds provide useful results regarding the method."}, "weaknesses": {"value": "1. The method appears to be less scalable and restricted to only REC than existing compositional reasoning methods such as ViperGPT and HYDRA which can work beyond REC tasks and for general vocabulary. Specifically, the method's dependence on predefined bank of K common categories (L210) represented in CLIP, make it unclear how it can be utilized for out-of-vocab settings. Would it not exhibit failure rates in these cases compared to open-domain methods such as ViperGPT?\n\n2. The method's primary novelty appears to be in usage of CLIP-based uncertainty filtering and designing a set of symbolic operators specific to REC. While this does address the high false-positive rates, I don't see why the same cannot be done for ViperGPT as an additional step and to have additional checks exclusive to REC in their python programs API. The novelty in this regard appears low and also constrained to a specific setting in REC."}, "questions": {"value": "Please see weaknesses above.\nAdditionally:\n1) Is the LLM used the same as in ViperGPT for fair comparison? Ideally same LLM should be used for fair comparison so that performance improvements and error rates are not due to underlying LLM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q8P9Dz9pkt", "forum": "m0vS4BVtDp", "replyto": "m0vS4BVtDp", "signatures": ["ICLR.cc/2026/Conference/Submission18070/Reviewer_BrL1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18070/Reviewer_BrL1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062661495, "cdate": 1762062661495, "tmdate": 1762927853938, "mdate": 1762927853938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}