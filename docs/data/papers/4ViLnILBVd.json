{"id": "4ViLnILBVd", "number": 1060, "cdate": 1756832018486, "mdate": 1759898230397, "content": {"title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration", "abstract": "Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents-a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs)  have emerged as promising autonomous agents for collaborative task solving. \nHowever, existing collaboration frameworks for LLMs overlook their reasoning potential for $\\textit{dynamic intent inference}$, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. \nTo bridge this gap, we propose $\\textit{\\textbf{CoBel-World}}$, a novel framework that equips LLM agents with a $\\textit{\\textbf{co}llaborative \\textbf{bel}ief world}$-an internal representation jointly modeling the physical environment and collaborators' mental states. \nCoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH),  CoBel-World significantly reduces communication costs by $\\textbf{22-60\\\\%}$ and improves task completion efficiency by $\\textbf{4-28\\\\%}$ compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.", "tldr": "We propose a novel framework that introduces belief modeling for embodied mutli-agent collaboration to improve collaboration efficiency and  reduce communication costs.", "keywords": ["multi-agent collaboration", "LLM", "Embodied AI"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cce49fddce22d3571a8dd4d2b76cd0a768c2a10b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the Collaborative Belief World framework, equips LLM agents with an internal representation that jointly models the physical environment and collaborators’ mental states. The paper introduces Symbolic Belief Representation for belief representation and Bayesian Belief Collaboration for belief update. Experiments on two embodied benchmarks show the proposed method reduces communication costs and improves task performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The introduction of a symbolic belief representation language is novel and well-motivated.\n\n- The experimental results are good and convincing with the newly introduced communication token cost metric.\n\n- Detailed prompt report and code for reproduction."}, "weaknesses": {"value": "- The presentation could use some improvements.\n   - Better to put Symbolic Belief Language Definition in the main paper rather than the Appendix, as it's the grounding of the work. Deferring it makes the method hard to follow.\n  - Several typos in the manuscript, could use another round of proofreading. E.g., l132-l133 in the contribution\n- The method seems to have a strong assumption of homogeneous agents' cooperation, which does not hold for human-agent cooperation.\n  - Coming up with agreed Belief Rules (building common ground) itself is a hard problem in decentralized multi-agent cooperation; current implementation seems to be assuming agents of the same architecture (the belief rule construction phase may not be followed perfectly by other agents of other methods, including humans)\n  - There's no experiment with heterogeneous agents or human-agent cooperation."}, "questions": {"value": "- How are the ablation experiments of CoBel-World (No SBR) implemented?\n\n- Since the proposed method introduces additional llm call overhead, how's the efficiency tradeoff regarding llm tokens used?\n\n- Missing discussion of recent work about Belief Modeling in multi-agent systems [1][2][3]\n\n[1] COMBO: Compositional World Models for Embodied Multi-Agent Cooperation\n[2] Neural Amortized Inference for Nested Multi-agent Reasoning\n[3] Too many cooks: Coordinating multi-agent collaboration through inverse planning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gz6d92Vooq", "forum": "4ViLnILBVd", "replyto": "4ViLnILBVd", "signatures": ["ICLR.cc/2026/Conference/Submission1060/Reviewer_YUZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1060/Reviewer_YUZ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879769294, "cdate": 1761879769294, "tmdate": 1762915666703, "mdate": 1762915666703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel framework, CoBel-World (Collaborative Belief World), to improve decentralized LLM agents collaboration using belief modeling - specifically to produce consistent plans and reduce communication costs. \n\nThey create a symbolic belief language to represent mental state, current plan, and collaborator intents for each agent. They also describe first-order belief (i.e. I believe agent B believes..). They perform zero-shot Bayesian belief updates with theory-of-mind reasoning based on observations and communication. Then, the belief prediction for future intents and states is done based on the prior beliefs. The communication only happens when there is a misalignment after the belief update. \n\nThe authors use transport rates for TDW-MAT, and average steps to complete all tasks for C-WAH for efficiency calculation. For communication, they use the average number of takes generated by all agents for communication on average.\n\nThey deploy Qwen3-32B and ChatGPT-4o models as agents. They compare against traditional (MHP, RHP) as well as other LLM-based frameworks (CoELA, CaPo) as baselines on the TDW-MAT and C-WAH benchmarks. The CoBel-world framework shows significantly reduced communication costs and improved completion efficiency, especially with ChatGPT-4o. They also perform ablations by removing each component - symbolic belief representation, and the bayesian belief collaboration, and show that while both are important, bayesian belief collaboration is essential for high performance for CoBel-world. They also experiment with more agents to show that framework can scale."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. They apply a useful idea from traditional multi-agent RL (belief networks/modeling) to zero-shot MLLM agents and show improved performance and costs on benchmarks.\n2. They use actual embodied benchmarks instead of grid-like or text-only worlds, which is closer to the real-world.\n3. Lack of task-specific fine-tuning allows the framework to potentially generalize and scale to any environment."}, "weaknesses": {"value": "1. The novelty of the approach is very limited. While the approach works better than baselines, the ideas of belief modeling are not uncommon in multi-agent RL. This is just application of the idea in the LLM-agent setting.\n2. The authors do not cite or compare against very easy-to-find, relevant, and perhaps similar works. A simple Google Search leads to many Belief Modeling + Belief Language + LLM ideas: https://openreview.net/pdf?id=TWC4gLoAxY, https://arxiv.org/pdf/2506.08292\n3. The benchmarks are very small (24 episodes and 10 episodes) and have limited tasks, which are perhaps not enough to signal a high-performance in the real-world. The benchmarks they use are typically only good to test 2 agents in collaboration, 3 at best.\n4. There are many typos in the paper, and missing letters.\n\t1. Line 132 - averafe -> average\n\t2. Line 146 - tep-by-step -> step-by-step\n\t3. Line 224 -> defination -> definition\n\t4. \"Collaborative representing progress\" -> do the authors mean process?\n\t5. Line 293 -> pontential -> potential\n\t6. Line 501 - Baesian -> Bayesian\n\t7. Line 505 - reproduct -> reproduce"}, "questions": {"value": "1. Can the authors clarify the differences against the recent works, and any other that incorporate belief modeling with LLMs?\n2. How can we get a better signal as to whether this approach applies generally to the real-world? Have the authors considered PARTNR: https://ai.meta.com/research/publications/partnr-a-benchmark-for-planning-and-reasoning-in-embodied-multi-agent-tasks/?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CdzpofDyPm", "forum": "4ViLnILBVd", "replyto": "4ViLnILBVd", "signatures": ["ICLR.cc/2026/Conference/Submission1060/Reviewer_xCs1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1060/Reviewer_xCs1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943214127, "cdate": 1761943214127, "tmdate": 1762915666555, "mdate": 1762915666555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CoBel-World, a framework that augments LLM agents with an explicit \"collaborative belief world.\" A symbolic belief language encodes task knowledge and agent mental states. At the same time, a Bayesian-style update loop, with prediction and measurement steps executed via LLM prompting, maintains and refines those beliefs during execution. On two embodied multi-agent benchmarks, compared with rule-based planners and two recent LLM collaboration baselines, CoBel-World reduces communication tokens by 22–60% and yields 4–28% better task-completion efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. By explicitly distinguishing zero- and first-order beliefs, the authors provide a concrete representation that is easier to inspect than pure free text.\n\n2. The Bayesian filter implemented through LLM prompting is a creative use of reasoning capability that decides whether to speak or act.\n\n3. On both TDW-MAT and C-WAH, CoBel-World achieves the best transport rate / step count while cutting token usage substantially. In Figure 3 the qualitative trajectory shows how belief prediction avoids room-duplication and unnecessary chat."}, "weaknesses": {"value": "1. The “Bayesian” update is performed by textual prompting rather than probabilistic computation, and the normalization and sampling policies are not well-defined.\n\n2. Recent work on ToM reasoning and belief-driven LLM collaboration/debate is not cited or contrasted, such as [1][2]. Differences in representation and update procedure should be spelled out.\n\n3. All results appear to be single-run numbers. No variance or statistical tests are provided. Given the inherent stochasticity of LLM outputs (temperature 0.7), this casts doubt on the reported average gains of 4%.\n\n4. The method may trade fewer tokens between agents for more tokens issued to the LLM internally (belief prediction, misalignment detection). There is no report of total LLM calls or wall-clock time, so efficiency is not well-supported.\n\n5. There are too many grammatical mistakes and typos that affect readability. For example, in the caption for Figure 2: \"All agent\" should be \"All agents\", \"collaborative reasoning progress\" should be \"collaborative reasoning process\", \"to analysis\" should be \"to analyze\", \"structured format\" should be \"a structured format\", \"each agent construct a initial belief\" should be \"each agent constructs an initial belief\", \"each agent update\" should be \"each agent updates\", and \"adaptive collaborative decision\" should be \"an adaptive collaborative decision\".\n\n[1] AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling. Zhang et al, 2025.\n\n[2] From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium. Yi et al, 2025."}, "questions": {"value": "1. How many internal LLM calls (update + prediction + alignment checks) are issued per environment step compared to CoELA/CaPo? Please report average total tokens (internal + external) and time.\n\n2. Can you conduct some failure analysis to understand when the framework will fail?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aDXv7w46uU", "forum": "4ViLnILBVd", "replyto": "4ViLnILBVd", "signatures": ["ICLR.cc/2026/Conference/Submission1060/Reviewer_MLjZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1060/Reviewer_MLjZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762232731412, "cdate": 1762232731412, "tmdate": 1762915666326, "mdate": 1762915666326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}