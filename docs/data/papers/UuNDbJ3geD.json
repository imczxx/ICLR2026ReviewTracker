{"id": "UuNDbJ3geD", "number": 10185, "cdate": 1758163320006, "mdate": 1759897668517, "content": {"title": "Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment", "abstract": "Large Language Models (LLMs) have recently demonstrated impressive performance in natural language processing due to their strong generalization and sequence modeling capabilities. However, their direct application to time series forecasting remains challenging due to two fundamental issues: the inherent heterogeneity of temporal patterns and the modality gap between continuous numerical signals and discrete language representations. In this work, we propose TALON (Temporal-heterogeneity And Language-Oriented Network), a unified framework that enhances LLM-based forecasting by modeling temporal heterogeneity and enforcing semantic alignment. Specifically, we design a Heterogeneous Temporal Encoder that partitions multivariate time series into structurally coherent segments, enabling localized expert modeling across diverse temporal patterns. To bridge the modality gap, we introduce a Semantic Alignment Module that aligns temporal features with LLM-compatible representations, enabling effective integration of time series into language-based models while eliminating the need for handcrafted prompts during inference. Extensive experiments on seven real-world benchmarks demonstrate that TALON achieves superior performance across all datasets, with average MSE improvements of up to 11% over recent state-of-the-art methods, while maintaining higher efficiency. These results underscore the effectiveness of incorporating both pattern-aware and semantic-aware designs when adapting LLMs for time series forecasting. The code is available at: https://anonymous.4open.science/r/TALON-BB00.", "tldr": "We propose TALON, an LLM-based forecasting model that bridges the modality gap between time series and language by combining temporal heterogeneity modeling with semantic alignment, achieving state-of-the-art accuracy and strong generalization.", "keywords": ["Time Series Forecasting", "Large Language Models", "Semantic Alignment"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/082d49f81180612cda2a1b3a44f07ec23598713e.pdf", "supplementary_material": "/attachment/f58ad8fba119b8784b41fc3200ed8f4df04e54f0.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes TALON, an LLM-based forecasting method, addressing temporal heterogeneity and the modality gap between continuous signals and discrete language. TALON introduces (1) a Heterogeneous Temporal Encoder that quantifies patch-level complexity and routes tokens to Linear/CNN/LSTM experts; (2) a Semantic Alignment Module that builds token-adaptive prompts from interpretable statistics and temporal context, then performs token-level contrastive alignment between time-series features and language embeddings; TALON has been evaluated across seven benchmarks under both one-for-all, one-for-one protocols, and zero-shot setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Prompt-free inference with token-level alignment: Avoids handcrafted prompts and reduces input redundancy while semantically grounding features via contrastive alignment. \n2. The architecture HTE explicitly models heterogeneity with adaptive routing and complementary experts, supported by balanced-load regularization and strong ablations. \n3. TALON has competitive accuracy with a compact head and frozen LLM backbone (~1.7M params; fast inference)."}, "weaknesses": {"value": "1. Some baseline results, such as TIME-LLM, are lower than those reported in the original paper, and TALON shows no clear advantage compared to the original results. Certain related works or baselines, such as TEMPO [1] and [2], are also missing.\n2. Token-adaptive prompts are not used during inference; the paper could further explore whether reintroducing lightweight textual cues would help in out-of-distribution settings. Context(relevant to time series)-aided time series forecasting would make the use of LLMs more motivated.\n3. The ablations are insufficient, consider replacing the LLM component with simpler architectures to better demonstrate the necessity of using LLMs.\n\n\n[1] TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. \n\n[2] From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection"}, "questions": {"value": "Have you considered using text related to time series to assist with time series forecasting? Some related works have provided such textual and time series data [1] [2] [3].\n\n[1] Time-MMD: Multi-Domain Multimodal Dataset for Time Series Analysis \n\n[2] From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection\n\n[3] Intervention-Aware Forecasting: Breaking Historical Limits from a System Perspective"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kRL4SUVHuA", "forum": "UuNDbJ3geD", "replyto": "UuNDbJ3geD", "signatures": ["ICLR.cc/2026/Conference/Submission10185/Reviewer_Vv7p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10185/Reviewer_Vv7p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761418977475, "cdate": 1761418977475, "tmdate": 1762921552061, "mdate": 1762921552061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TALON, aimed at adapting large-scale language models for time series forecasting tasks. The paper addresses two challenges: temporal heterogeneity in time series and the modal disparity between continuous signals and discrete language. TALON comprises three main modules: the Heterogeneous Time Encoder for local pattern modeling, the Semantic Alignment Module that embeds temporal features into linguistic semantic space via contrastive learning, and finally, an autoregressive prediction layer achieved through a frozen LLM and a Lightweight Forecasting Head. Systematic experiments across seven public benchmark datasets demonstrate that TALON achieves an average MSE improvement of approximately 10–20% over existing SOTA methods. It also exhibits strong performance in zero-shot transfer learning, efficiency, and ablation studies. Overall, the paper aligns with current research frontiers integrating LLMs with temporal data, featuring a framework with notable innovation and comprehensive experimental coverage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear research motivation and structural design: The paper clearly articulates the structural and modal differences between time series tasks and language modeling, systematically addressing this issue through the HTE and SAM modules. The framework exhibits logical coherence with focused innovations.\n\n2. Rational and interpretable module design: The HTE module employs local statistical features for expert dynamic routing, integrating linear, CNN, and LSTM experts to capture multi-scale temporal patterns. The SAM module introduces token-level alignment to enhance semantic consistency between temporal features and linguistic representations.\n\n3. Comprehensive experimental coverage: The authors validate the model across seven standard datasets—including the ETT series, Weather, Electricity, and Traffic—and incorporate three prediction settings: one-for-all, one-for-one, and zero-shot. This demonstrates TALON's potential for generalization and stability."}, "weaknesses": {"value": "1. Lack of Innovation and Differentiation: Although TALON conceptually integrates heterogeneous modeling with semantic alignment, its implementation primarily relies on existing MoE routing mechanisms and contrastive learning frameworks. The three features—trend, fluctuation, and autocorrelation—in the HTE module are relatively conventional, lacking exploration of more complex patterns such as frequency domain and long-range dependencies. We recommend the authors further elaborate on the theoretical rationale for these feature selections and validate their validity through feature importance or replacement experiments.\n\n2. Shallow Semantic Alignment Mechanism: The SAM module performs cosine similarity alignment solely at the lexical token level, disregarding temporal context or cross-segment structural relationships. This alignment process fails to capture cross-layer feature consistency or global temporal constraints, undermining its claim of “semantically consistent prediction.” Consequently, the context modeling capabilities of pre-trained LLMs may not be directly applicable to modeling temporal relationships.\n\n3. Semantic alignment may lead to excessive assimilation of temporal information: The paper's alignment mechanism minimizes the angular distance between temporal representations and prompt representations at the token level, then directly drives predictions using the “aligned representations” to drive the frozen LLM + linear head during inference. This introduces a structural risk: when the alignment loss accounts for a high proportion or training is imbalanced, the temporal branch representation may become overly close to the prompt representation. Consequently, independent temporal structure and statistical features may be lost, and the model may primarily rely on “semantic prompts” for prediction rather than utilizing pre-aligned temporal information. The current paper does not provide diagnostics or controls to rule out such “semantics-dominated, temporally weakened” scenarios.\n\n4. Insufficient justification for LLM freezing strategy: Authors fully freeze the LLM, training only the encoder and prediction head, without analyzing whether lighter approaches like adapter/prefix tuning were attempted. If LLMs cannot dynamically adapt to numerical patterns, the conclusion that “LLMs can be directly applied to time series inference” appears overly optimistic. A parameter efficiency comparison is recommended to clarify the benefits and trade-offs of the freezing strategy."}, "questions": {"value": "1. On Innovation and Feature Selection in HTE: Could the authors further explain the rationale behind choosing trend, fluctuation, and autocorrelation as the three primary descriptors in the HTE module? Have alternative or learned features—such as frequency-domain representations, spectral energy, or Transformer-based contextual embeddings—been evaluated for modeling complex or long-range dependencies? Additionally, would the authors consider adding a feature importance analysis or feature replacement study to validate the necessity of these hand-crafted statistical indicators?\n\n2. On the Scope of Semantic Alignment in SAM: The SAM module aligns representations through token-level cosine similarity, which may overlook cross-temporal dependencies or higher-order contextual relationships. Could the authors clarify whether they attempted any context-aware alignment strategies? Furthermore, how do they ensure that global temporal consistency is maintained when alignment is only enforced locally at the token level?\n\n3. On Potential Information Assimilation during Alignment: The contrastive alignment loss may drive the temporal encoder representations to become overly similar to semantic prompt embeddings, potentially leading to the loss of temporal structure. Have the authors conducted diagnostic experiments to confirm that the model still leverages temporal information after alignment—for example, by testing performance when prompts are shuffled or masked, or by measuring mutual information or representational similarity before and after alignment? Could they discuss mechanisms or regularization strategies that mitigate over-assimilation?\n\n4. On the LLM Freezing Strategy and Adaptation Capability: The paper reports that the LLM backbone is fully frozen during training. Could the authors elaborate on whether lighter adaptation methods—such as adapter tuning, prefix tuning, or LoRA—were explored? How does freezing the LLM affect its ability to model numerical patterns, and have the authors compared parameter efficiency or inference latency under different tuning strategies? Providing such comparisons would clarify the trade-offs between efficiency and adaptability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UXamNtr6xL", "forum": "UuNDbJ3geD", "replyto": "UuNDbJ3geD", "signatures": ["ICLR.cc/2026/Conference/Submission10185/Reviewer_rsLS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10185/Reviewer_rsLS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447524557, "cdate": 1761447524557, "tmdate": 1762921551671, "mdate": 1762921551671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TALON (Temporal-heterogeneity and Language-Oriented Network)—a new architecture that bridges large language models (LLMs) and time series forecasting! It tackles two fundamental challenges of temporal data: pattern heterogeneity and the modality gap between continuous signals and discrete language representations. TALON combines a Heterogeneous Temporal Encoder (HTE) for adaptive pattern extraction, a Semantic Alignment Module (SAM) for embedding-level cross-modal alignment, and a Lightweight Forecasting Head (LFH) that performs autoregressive prediction through a frozen LLM backbone.The paper contributes to the growing body of work exploring foundation models for structured data, providing clear architectural design, solid empirical validation, and insightful analyses of expert routing and cross-modal representation behavior! This research advances the understanding of how language priors can be leveraged to strengthen forecasting under distributional shifts and heterogeneous temporal regimes!"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive Temporal Modeling: By combining heterogeneous experts in the HTE module, TALON effectively captures diverse temporal behaviors including trends, local fluctuations, and long-range dependencies. This multi-expert design enhances its ability to adapt to nonstationary and complex temporal patterns.\n\n2. Semantic–Numerical Integration: The Semantic Alignment Module (SAM) bridges numerical time-series representations with linguistic semantics through contrastive learning. This integration allows the model to leverage pretrained LLMs for high-level reasoning while preserving low-level temporal structure, improving interpretability and generalization.\n\n3. Empirical Validation Across Benchmarks: Extensive experiments on seven benchmark datasets demonstrate consistent improvements (approximately 10–20% MSE reduction) over strong baselines in both multi-horizon and zero-shot forecasting settings. These results verify TALON’s robustness and transferability across domains and temporal resolutions."}, "weaknesses": {"value": "1. Information Loss in Cross-Modal Alignment: During cross-modal contrastive alignment, temporal embeddings may suffer from information loss as the optimization drives them excessively close to the semantic embeddings. Consequently, the model’s final representations could become dominated by textual prompts, diminishing the independent contribution of the HTE module and weakening the preservation of intrinsic temporal dynamics. The paper lacks diagnostic experiments or regularization strategies that demonstrate the balance between alignment strength and temporal integrity.\n\n2. Lack of Consistency Verification in Expert Routing: The HTE module determines expert selection using only three local statistical indicators—trend intensity, local fluctuation, and autocorrelation—but the paper does not examine whether this routing remains consistent under minor perturbations. For example, when applying slight noise to similar time series segments, it is unclear whether the same experts would be selected. Without such consistency verification, the robustness and interpretability of the heterogeneous expert routing mechanism remain uncertain.\n\n3. Unanalyzed Contribution of Prompt Components: In the adaptive prompt construction process, the three components—routing prompts, temporal context, and complexity statistics—are combined without assessing their respective contributions. The absence of ablation or attribution analysis makes it difficult to determine which component primarily facilitates cross-modal alignment and whether textual or numerical features dominate semantic influence."}, "questions": {"value": "1. On Temporal–Semantic Information Balance: How do the authors ensure that temporal embeddings retain sufficient information during cross-modal alignment? Have they measured representation diversity or mutual information before and after alignment, or applied constraints to prevent the temporal encoder from over-assimilating to the semantic space?\n\n2. On Expert Routing Robustness: Have the authors evaluated whether HTE’s expert selection remains consistent for highly similar or noise-perturbed sequences? Could they provide quantitative or visual analyses verifying that routing decisions are stable and interpretable under small data perturbations?\n\n3. On Component-wise Prompt Effectiveness: sCould the authors conduct ablation or attribution studies to quantify the relative contribution of routing prompts, temporal context, and statistical descriptors in the adaptive prompt design? This would clarify how textual versus numerical information contributes to cross-modal alignment and prediction accuracy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AdstvDps1Q", "forum": "UuNDbJ3geD", "replyto": "UuNDbJ3geD", "signatures": ["ICLR.cc/2026/Conference/Submission10185/Reviewer_G2zG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10185/Reviewer_G2zG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738024323, "cdate": 1761738024323, "tmdate": 1762921550958, "mdate": 1762921550958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework called TALON (Temporal-heterogeneity And Language-Oriented Network), which addresses the challenges of temporal heterogeneity and the modality gap when applying large language models (LLMs) to time series forecasting tasks. The paper demonstrates the effectiveness of TALON through baseline comparison experiments, validates the contribution of each component via ablation studies, and shows that TALON maintains high efficiency with minimal overhead during the inference stage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I agree that there is currently a significant gap between LLMs and time series forecasting. Existing LLM-based methods—including reprogramming approaches such as Time-LLM and prompt-based approaches such as LLMTime—indeed have certain limitations. The paper identifies two reasonable underlying causes, namely temporal heterogeneity and modality differences, which are accurately and clearly articulated in the introduction.\n2. The proposed method adopts a training–inference decoupled design, functioning as a plug-and-play module placed in front of a frozen LLM. It supports one-time training that generalizes across all time series benchmarks, and the experiments validate its effectiveness. This design is generalizable and computationally efficient, reducing the overall resource cost.\n3. It is interesting to see this work uses contrastive learning to align time series representations with text representations instead of brutally train the LLM on time series data and hoping it can automatically learn the new tokens."}, "weaknesses": {"value": "1. (major) Choice of mode quantization features: The MoE-like architectural design of this paper is impressive. However, the selection of time series statistical features—trend strength, local variation, and autocorrelation—appears somewhat simplistic. These statistics may not be sufficient to uniquely or accurately characterize the properties of a time series patch. It remains unclear whether incorporating additional or more informative statistical features could provide a more precise representation of patches and potentially improve the final prediction performance.\n2. (major) It is still unclear that if the contrastive trained time series token can be directly applied in the next-token-prediction-based LLMs. Further, since the time series tokens are only corasely aligned with some statistical features, the granularity of its semantic should be very coarse and it is not sure if these tokens still hold their previous time series details. \n3. (major) Concern regarding ablation studies and hyperparameter sensitivity analysis: \n    \n    In the ablation study, the authors show that removing individual modules or any one of the three experts (Linear, CNN, LSTM) leads to a decline in predictive performance. However, the degree of degradation is very small—on the order of 0.01 in MSE and 0.001 in MAE. Such minimal performance differences somewhat weaken the justification for the model’s highly complex architectural design.\n    \n    Furthermore, The loss function combines multiple components and includes two manually defined hyperparameters, α and β, which are not learnable. Although the authors perform a sensitivity analysis, but as shown in Figure 7, the MSE exceeds 0.390 under most combinations of α and β, and the magnitude of this variation is even larger than that observed in some groups of the ablation study (see Tables 5 and 6). Hence, the results do not convincingly support the claim that “TALON is robust to α and β.”\n    \n    If the authors consider such fluctuations in acceptable in the sensitivity analysis, it would further weaken the argument for the necessity of the model’s complex modules, since their removal only slightly affects performance within tolerable bounds. If the authors consider an MSE change of 0.01 to be meaningful in the ablation study, then this level of variation would cast doubt on the conclusion that “TALON is robust to α and β.”\n    \n4. (minor) Lack of analysis on training cost: \n    \n    While the paper discusses the efficiency of TALON during inference, it does not analyze the training cost in depth. The SAM module’s patch-wise dynamic prompt generation and contrastive learning introduce substantial computational overhead. The authors should include metrics such as training time and GPU memory consumption to provide a more comprehensive evaluation of TALON’s efficiency."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "36oRiFCtTm", "forum": "UuNDbJ3geD", "replyto": "UuNDbJ3geD", "signatures": ["ICLR.cc/2026/Conference/Submission10185/Reviewer_QPnX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10185/Reviewer_QPnX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895653320, "cdate": 1761895653320, "tmdate": 1762921550165, "mdate": 1762921550165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}