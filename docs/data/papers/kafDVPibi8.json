{"id": "kafDVPibi8", "number": 10183, "cdate": 1758163302331, "mdate": 1759897668758, "content": {"title": "SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing", "abstract": "With the rapid advancement of text-to-image (T2I) models, ensuring their safety has become increasingly critical. Existing safety approaches can be categorized into training-time and inference-time methods. While inference-time methods are widely adopted due to their cost-effectiveness, they often suffer from limitations such as over-refusal and imbalance between safety and utility. To address these challenges, we propose a multi-round safety editing framework that functions as a model-agnostic, plug-and-play module, enabling efficient safety alignment for any text-to-image model. Central to this framework is MR-SafeEdit, a multi-round image–text interleaved dataset specifically constructed for safety editing in text-to-image generation. We introduce a post-hoc safety editing paradigm that mirrors the human cognitive process of identifying and refining unsafe content. To instantiate this paradigm, we develop SafeEditor, a unified MLLM capable of multi-round safety editing on generated images. Experimental results show that SafeEditor surpasses prior safety approaches by reducing over-refusal while achieving a more favorable safety–utility balance.", "tldr": "", "keywords": ["Text-to-Image Generation", "Multimodal Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dfffbae3bcba6bfa83d846ff1e544deb9c1adf38.pdf", "supplementary_material": "/attachment/88c14b8031ac7943c0f80465e6c9629c59e62a4f.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces SafeEditor, an MLLM-based framework for safe image generation. The key idea is to prevent the production of unsafe images in text-to-image pipelines by using a custom-trained MLLM that performs multi-round editing until the result is safe. SafeEditor is trained on a dedicated dataset built for this purpose, consisting of interleaved images and textual explanations in a multi-round format. The paper compares the method with existing approaches based on prompt filtering, prompt modification, and output filtering. It also includes ablation studies on the safety–utility trade-off and an analysis of the effect of multiple editing rounds."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper clearly explains the problem setup and is well written and easy to follow.\n- I appreciate the effort invested in building the MR-SafeEdit benchmark, which could represent a valuable contribution to the community. Its construction progressively increases the safety of output images, making it potentially useful for advanced training strategies."}, "weaknesses": {"value": "My main concern with the manuscript lies in the motivation and design of the proposed approach.\n\n1. Safety methods for text-to-image generation can generally be divided into two categories: methods for hosted models and methods for open-source models. Hosted-model approaches assume that users have access only through an API and include strategies such as prompt filtering and image analysis. When users have direct access to the model, these techniques become ineffective, as they can be easily disabled. In such cases, alignment methods at the weight level are preferred, as correctly mentioned in Section 2. Based on its functioning, SafeEditor falls within the category of hosted-model methods. When dealing with hosted models, we assume that a centralized server runs the text-to-image system and serves users through inference requests. Each inference incurs a computational cost for the host. In the case of SafeEditor, the model may need to be executed multiple times (up to four rounds) before producing an acceptable image. Consequently, SafeEditor involves significantly higher inference costs compared to approaches like GuardT2I or Latent Guard. Despite this, the paper does not discuss the computational or economic implications of this design choice, which I consider a major limitation that should be explicitly addressed.\n2. Related to the previous point, I find the motivation for addressing safety in text-to-image models through this approach unclear. If the model is hosted and a user attempts to generate an image containing banned or harmful content, the natural response would be to notify or restrict the user rather than repeatedly generate alternative images that consume significant GPU resources while still resembling the original request. It is worth noting that other approaches, such as Safe Latent Diffusion [1], achieve safety without introducing additional computational overhead.\n3. The evaluation and comparison with baselines appear suboptimal. In Table 1, the paper reports only the refusal rate, which alone provides little insight into whether the method is functioning as intended. Disabling all safety mechanisms would trivially result in perfect metrics, yet this would be meaningless. It is necessary to actually generate the images and assess both their safety and their alignment with the user’s request. Moreover, input filtering methods rely on a threshold to classify prompts as safe or unsafe. Instead of reporting absolute values, it would be more informative to present performance curves as a function of this threshold. Finally, I believe that LlamaGuard2 is a missing baseline in the prompt-filtering comparison, as it is a widely used reference in this context.\n4. Finally, unless I am missing something, the results do not appear particularly compelling. The absolute performance is quite similar to that of certain baselines, such as GuardT2I in Table 1 and SAFREE in Table 2, which achieve comparable outcomes with significantly lower computational requirements.\n\nAs a minor note, the related work section is rather limited and discusses only a small number of methods. I suggest expanding it to include approaches for both closed-source and open-source text-to-image models (for example, concept-forgetting techniques), and to dedicate a subsection to the use of MLLMs for harmful concept detection. This is only a suggestion, but any restructuring that provides a clearer and more complete overview of the state of the art would be beneficial.\n\n[1] Safe Latent Diffusion, CVPR 2023"}, "questions": {"value": "1. How is the latency and computational cost of SafeEditor compared to baselines?\n2. Why is generating intermediate images progressively useful rather than interrupting the generation when a malicious intent is detected?\n3. How does the model perform compared to prompt/output filtering strategies in terms of image safety and quality?\n4. How do baselines perform if we variate the classification threshold for safe/unsafe?\n5. How do LLaMAGuard2 perform on the same task?\n6. Is it possible to justify the low performance gap with baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OkQAcvgUPd", "forum": "kafDVPibi8", "replyto": "kafDVPibi8", "signatures": ["ICLR.cc/2026/Conference/Submission10183/Reviewer_nZA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10183/Reviewer_nZA5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809554763, "cdate": 1761809554763, "tmdate": 1762921551562, "mdate": 1762921551562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the safety alignment of Text-to-Image (T2I) models, focusing on the prevalent challenges of over-refusal and the trade-off between safety and utility. The authors propose a novel post-hoc editing framework named SafeEditor, at the core of which is a unified Multimodal Large Language Model (MLLM) capable of performing multi-round, iterative edits on unsafe images until they meet safety requirements. To train this model, the paper introduces MR-SafeEdit, a large-scale, multi-round, image-text interleaved safety editing dataset. Experimental results demonstrate that this approach surpasses existing methods in significantly reducing over-refusal rates and achieving a more favorable safety-utility balance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed \"post-hoc safety editing\" is a highly innovative and practical paradigm. It mimics the human cognitive process of identifying and refining unsafe content, directly addressing the \"one-size-fits-all\" and over-refusal issues of existing filtering or prompt modification methods, which is critical for improving user experience.\n\n2. The construction of the MR-SafeEdit dataset is a major contribution of this work. This large-scale dataset, comprising 27,253 multi-round editing instances, provides the community with a valuable resource that can drive future research in more fine-grained and context-aware safety alignment.\n\n3.  The paper's experimental design is exceptionally thorough. The authors conducted a comprehensive comparison of SafeEditor against two major classes of baselines (filter-based and prompt-modification methods) across multiple standard datasets. The evaluation metrics cover multiple dimensions, including over-refusal, safety, and utility (e.g., CLIP score, LPIPS score), robustly demonstrating the proposed method's effectiveness and superiority. Furthermore, extensive ablation studies offer deep insights into the contribution of each component of the model."}, "weaknesses": {"value": "1. While the multi-round iterative editing paradigm is effective, it may introduce significant inference latency and computational overhead compared to single-pass filtering methods. The paper fails to provide an analysis of inference time or computational cost, which is crucial for the method's practical deployment. A discussion on the efficiency-performance trade-off is recommended.\n\n2. The synthesis pipeline for the MR-SafeEdit dataset relies on GPT-4o. This dependency on a powerful, closed-source model raises concerns about the reproducibility of the dataset creation process and the potential for inheriting the \"teacher model's\" latent biases. Exploring the feasibility of using open-source models for similar dataset construction would enhance the work's value.\n\n3. As the authors acknowledge, the definition of \"safety\" in this work is primarily confined to six common categories (e.g., sexual, violence, hate). The model's effectiveness on more complex, culturally sensitive, or subtle forms of harmful content (e.g., nuanced misinformation, politically sensitive topics) remains unverified.\n\n4. The paper claims the method edits images with minimal utility loss, validated by metrics like the CLIP score. However, multi-round iterative editing still poses a risk of deviating from the user's original, subtle intent, especially on complex or artistic prompts. Relying solely on automated metrics may not fully capture these semantic nuances. Incorporating human evaluation to assess the preservation of user intent would make the conclusions more persuasive."}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2dtjGoW1Hd", "forum": "kafDVPibi8", "replyto": "kafDVPibi8", "signatures": ["ICLR.cc/2026/Conference/Submission10183/Reviewer_apfP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10183/Reviewer_apfP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008309241, "cdate": 1762008309241, "tmdate": 1762921550931, "mdate": 1762921550931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SafeEditor, a novel framework for enhancing the safety of T2I models. The core idea is to move away from traditional filtering (which often leads to over-refusal) and pre-hoc prompt modification (which can compromise user intent) towards a post-hoc safety editing paradigm."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Pros:\n1. They propose a multi-round, post-hoc editing process where an unsafe generated image is iteratively refined until it meets safety standards, rather than being outright rejected.\n2. To train a model for this task, they constructed a large-scale, multi-round, image-text interleaved dataset.\n3. The experiments are comprehensive and the results are compelling."}, "weaknesses": {"value": "Cons:\n1. The technical contribution is unclear. It seems this paper only proposes a pipeline and directly adopts the exiting editing methods. This paper does not propose any new editing methods for T2I safety.\n2. The multi-round, iterative nature of SafeEditor (generate -> evaluate & edit -> potentially repeat) could introduce significant latency. For a real-time user-facing application, this could be a major bottleneck. The paper lacks any discussion or measurement of inference speed, which is a critical factor for practical deployment.\n3. The paper shows that CLIP scores are largely preserved, but multiple rounds of editing could still cause the final image to drift from the user's original, nuanced intent. The framework seems optimized for clear safety violations, but its behavior on prompts with complex or subjective meanings (e.g., historical art depicting violence, subtle satire) is not explored. A discussion of these \"semantic failure modes\" would be beneficial.\n4. The data generation pipeline uses a \"refined prompt\" to generate the next image in the sequence. However, the SafeEditor model itself seems to perform direct image-to-image editing mediated by textual thought. Is the refined prompt from the dataset used in any way during the training or inference of SafeEditor, or is it purely an artifact of the data creation process?\n5. The paper rightly celebrates the extremely low false positive (over-refusal) rate. What is the corresponding false negative rate (i.e., the rate at which SafeEditor fails to edit an unsafe image and accepts it)? Understanding this trade-off is crucial for a complete picture of its safety performance.\n6. Safety is often subjective. How does SafeEditor behave on borderline cases that might be considered unsafe by some but acceptable by others? Does the multi-round process allow for a \"softening\" of content rather than a complete removal, and how is that decision made?\n7. The editing process stops when the model outputs \"text only\" (the \"accept\" decision). In practice, is there a hard limit on the number of rounds to prevent infinite loops or excessive latency on particularly difficult-to-edit images? The dataset statistics show up to four rounds; was this a natural limit or an imposed one?"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "El4YULckjC", "forum": "kafDVPibi8", "replyto": "kafDVPibi8", "signatures": ["ICLR.cc/2026/Conference/Submission10183/Reviewer_kZVF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10183/Reviewer_kZVF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142879824, "cdate": 1762142879824, "tmdate": 1762921550130, "mdate": 1762921550130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-round safety editing framework that mimics the human cognitive process of identifying and refining unsafe content. It includes a synthesis dataset MR-SafeEdit generated by GPT-4o and SD 3.5 and an MLLM SafeEditor trained on the MR-SafeEdit. The experiments show that SafeEditor could reduce the over-refusal rate."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Important topic.\n- Introduce human cognitive process into the identification and refining of unsafe content."}, "weaknesses": {"value": "- This framework highly relies on GPT-4o-generated supervision signals without validating their reliability. All components of the training data MR-SAFEEDIT are generated by GPT-4o (thought, judgement, refined prompt) and SD 3.5 (re-generated images). That basically means the paper implicitly assumes that GPT-4o is accurate and consistent in both semantic understanding and safety classification of images. However, there is no quantitative or human-validated evidence provided to support this assumption, i.e., how do you prove the GPT-4o is reliable on this task? Moreover, the authors also annotate the safety labels of generated images using GPT-4o, which is also unreliable.\n\n- Based on the training process, SafeEditor is basically mimicking GPT-4o's behavior. Given that GPT-4o already performs image safety reasoning and refined prompt generation, the same refinement workflow (GPT-4o -> refined prompt -> SD re-generation) can be directly executed at inference time. Therefore, it remains unclear why SafeEditor is needed in addition to GPT-4o rather than simply using GPT-4o + SD. If GPT-4o is reliable, SafeEditor is redundant; if GPT-4o is unreliable, SafeEditor inherits and amplifies those errors.\n\n- This framework may silently substitute the outputs, which raises transparency and trust concerns. SafeEditor produces modified images that may differ semantically from original model outputs. The user receives the modified results without being informed that the content was altered. This implicit substitution of generated images may compromise user understanding of model behavior and introduce potential issues regarding transparency and trust between the user and the model provider.\n\n- Several experimental design choices are confusing and require clarification. For example, SafeEditor is trained and deployed with SD 3.5 latents, but Section 5.2.2 appears to evaluate images generated by SD 1.4, whereas Section 5.2.1 uses SD 3.5. This inconsistency makes it difficult to interpret the utility evaluation and raises concerns regarding the fairness of the comparison. In addition, several evaluation metrics (e.g., high-level safety ratio, HP score, UIA score, CLIP score, and LPIPS score) require clearer definitions and justification for their relevance in this context.\n\n- The additional computational cost introduced by SafeEditor is not reported. The method involves multiple stages, which likely incur significantly higher training and inference costs compared to baselines. However, the paper does not provide any measurement or discussion of computational overhead. Without such analysis, it is difficult to assess the practical feasibility and deployment value of the proposed framework, especially in comparison to simpler baselines.\n\n- I recommend that the authors spend more time on improving the writing quality and fixing the typos."}, "questions": {"value": "- How do you prove the GPT-4o is reliable on this task?\n- Why is SafeEditor needed, since GPT-4o + SD can do the job?\n- What is the extra computational cost?\n- How do you address the transparency issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5PfEYPadlG", "forum": "kafDVPibi8", "replyto": "kafDVPibi8", "signatures": ["ICLR.cc/2026/Conference/Submission10183/Reviewer_39Bq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10183/Reviewer_39Bq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762444677687, "cdate": 1762444677687, "tmdate": 1762921549452, "mdate": 1762921549452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}