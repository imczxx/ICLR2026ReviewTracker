{"id": "SBgQTj5qOe", "number": 6917, "cdate": 1758001766531, "mdate": 1759897884083, "content": {"title": "GRACE: A Generalizable Method For Multi-agent System Security Evaluation", "abstract": "This paper investigates the problem of security evaluation in multi-agent systems. Existing studies typically rely on the LLM-as-a-Judge paradigm or string-matching approaches. However, their performance remains unsatisfactory due to subjective criteria and hallucination in LLM judgment, as well as the non-transferability of pre-defined refusal-string databases. To address these limitations, this paper introduces a Generalizable Method for Multi-Agent System Security Evaluation (GRACE). The core of GRACE is to not only decouple rule construction and selection from an evaluation perspective, but also calculate distance-based threshold from a multi-agent system perspective, which enables the framework to effectively capture and quantify security risks in multi-agent interactions. In particular, our proposed GRACE first constructs an adaptive rule set from the query dataset and then selects the top-K similar rules with the highest cosine similarity to the input query. Each response is evaluated by an LLM with respect to each selected rule, producing a danger rating vector. Finally, GRACE computes the Euclidean distance between the rating vectors of the attacker and the agent, applying a threshold mechanism to assess the agent’s risk level within multi-agent systems. These three components are integrated into a unified process, enabling effective and generalizable security evaluation for multi-agent systems. We conduct extensive experiments on various benchmark datasets, and the results demonstrate that GRACE consistently outperforms existing baselines.", "tldr": "", "keywords": ["Large Language Model", "Multi-agent System", "Security"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80864c1f07ab9bb6bf2f3c3cc7225811750430df.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents GRACE, a framework for evaluating the security of LLM-based multi-agent systems. It generates a set of safety rules from the query dataset using an independent LLM, selects the most relevant ones for each query, and uses another language model to rate each agent’s response along these rules. The ratings are compared with an attacker baseline using Euclidean distance and a fixed threshold to determine whether the system behaves dangerously. Experiments show that GRACE achieves better consistency with human judgment than previous LLM-as-a-Judge or string-matching methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a timely and important topic on evaluating the security of multi-agent LLM systems with a clear and systematic framing. \n\n2. The problem definition and methodological design are well structured and easy to follow. \n\n3. The experiments are comprehensive, and the appendix provides detailed implementation information that supports clarity and reproducibility."}, "weaknesses": {"value": "1. The paper leaves several aspects insufficiently defined or justified, and its scope is limited to LLM-based multi-agent systems. \n\n2. The role and mechanism of the attacker are unclear, and several important implementation details are only presented in the appendix rather than the main text. \n\n3. While the framework is systematic, its novelty is limited, and some design choices, such as the threshold setting, appear largely empirical."}, "questions": {"value": "1. The framework relies on external LLMs (G and T) for rule generation and scoring. How is the reliability of these models ensured, and could this design shift the security risk outside the evaluated system? \n\n2. The definition and role of the attacker are not clearly stated in the main text (only appearing in Appendix D). If the attacker only provides a malicious prompt rather than a response, why is it treated as the baseline for the “most dangerous” behavior in the distance comparison? \n\n3. Following up, how would GRACE handle cases where an agent’s response does not directly resemble the malicious prompt but still leaks sensitive or composable information, like partial but usable knowledge? \n\n4. Equation (6) appears to be empirically defined. The sensitivity analysis helps show robustness, but the paper does not clarify the reasoning behind choosing this particular form of threshold.\n\n5. The problem formulation introduces multi-round interactions, but the evaluation seems to assess each round independently. Does this overlook cumulative or sequential persuasion effects? And how should multi-round security be measured at the system level? \n\n6. It would be helpful if the authors could clarify what they see as the main methodological insights or sources of novelty in GRACE, beyond the timely problem setting itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QHh1PPADeH", "forum": "SBgQTj5qOe", "replyto": "SBgQTj5qOe", "signatures": ["ICLR.cc/2026/Conference/Submission6917/Reviewer_6jRq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6917/Reviewer_6jRq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761087927955, "cdate": 1761087927955, "tmdate": 1762919154655, "mdate": 1762919154655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GRACE (Generalizable Rule-based Assessment framework for Cooperative Environments), a framework for evaluating system security of MAS powered by large language models (LLMs). The central idea is to replace task-specific safety scoring (e.g., string matching or single-LLM judgment) with a generalizable rule-based evaluation mechanism."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The proposed GRACE framework introduces a rule-based, more interpretable approach compared with the opaque “LLM-as-a-Judge” paradigm."}, "weaknesses": {"value": "There seems to be a significant gap between what the paper claims (SYSTEM SECURITY of MAS) and what it is actually doing (safety evaluation of responses of individual agents). As a system, security evaluation of a MAS should include aspects which do not apply to a single LLM or an agent. What are the unique aspects or challenges of MAS system security evaluation? The paper barely considers this question. It seems that no agent-to-agent interaction is considered in the proposed three-component method (rule generation, rule selection, Euclidean distance calculation). Moreover, neither of the MAS-specific features mentioned in Related Works such as cascading effects, topology influence, or collective behaviors is considered in the current method or experiments."}, "questions": {"value": "What are the unique aspects for MAS system security evaluation, compared with response safety evaluation of a single LLM or agent? \nIs the proposed method able to deal with these aspect, such as cascading effects, topology influence, or collective behaviors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uOaNrrMLxL", "forum": "SBgQTj5qOe", "replyto": "SBgQTj5qOe", "signatures": ["ICLR.cc/2026/Conference/Submission6917/Reviewer_bYdD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6917/Reviewer_bYdD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993072145, "cdate": 1761993072145, "tmdate": 1762919154165, "mdate": 1762919154165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on security evaluation of multi-agent systems, and its evaluation includes constructing an adaptive rule set, selecting top-k rules,  and evaluated by an LLM correspondingly. The metric needs a threshold to judge whether the attacker successfully poses harm."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1, The task of evaluating the safety in MAS is timely and important.\n\n2, The paper has added examples and figures for better illustration."}, "weaknesses": {"value": "1, The experimental evaluation of the multi-agent system relies solely on Camel as the agent framework. To strengthen the validity and generalizability of the proposed evaluation metric, it would be beneficial to include additional results using other frameworks such as ChatDev and AutoGen.\n\n2, The writing needs to be polished; many necessary details are not given, or are unclear. Please refer to my questions.\n\n3,The paper aims to address the ambiguity inherent in the LLM-as-a-judge paradigm, which is an important and timely motivation. However, the proposed embedding- and distance-based approach requires a predefined threshold, and determining an appropriate value for this threshold is non-trivial. Moreover, the subsequent Danger Vector Rating stage still relies on prompting an LLM to generate a score, which, in my view, does not substantially differ from the original LLM-as-a-judge setting. Consequently, the extent to which the proposed method mitigates the core ambiguity remains unclear."}, "questions": {"value": "1, The explanation of the Euclidean distance between the agent’s and the attacker’s rating vectors is unclear. It is not evident what exactly constitutes the attacker’s rating vector, nor how the attacker’s responses are generated. In practical multi-agent system (MAS) settings, the system typically does not have prior knowledge of whether a query originates from an attacker or a benign user. Therefore, the assumptions underlying this part of the analysis should be clarified and better justified.\n\n2, In lines 318–320, the authors state that “a round is considered dangerous under JDR if all agents exhibit dangerous behavior, whereas in Equation (8), a round is considered dangerous under PDR if at least one agent is dangerous.” Based on this definition, one would expect JDR values to be consistently lower than PDR, since the criterion for JDR is more stringent. However, in Tables 1 and 2, there are instances where JDR exceeds PDR. This discrepancy suggests either a potential inconsistency in the implementation or an ambiguity in the definitions that requires clarification.\n\n3, What does R1, R2, R3 in JDR−R1, JDR−R2, JDR−R3 mean? \n\n4, As shown in Figure 3 (left), GPT-4 demonstrates consistently safer behavior than GPT-3.5 on the AdvBench benchmark. However, the opposite trend is observed on the DAN benchmark. Could the authors provide an explanation for this discrepancy?\n\n5, Why is the threshold 0.3? Are there any ablations on the threshold selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VQGCEhRpzB", "forum": "SBgQTj5qOe", "replyto": "SBgQTj5qOe", "signatures": ["ICLR.cc/2026/Conference/Submission6917/Reviewer_gRAL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6917/Reviewer_gRAL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762235279466, "cdate": 1762235279466, "tmdate": 1762919153594, "mdate": 1762919153594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework for safety evaluation of Multi-Agent Systems. Authors argue that  LLM-as-a-Judge and string matching, suffer from subjectivity, hallucinations, and poor generalization. To address these limitations, the framework  proposes to (1) adaptively generate evaluation rules from the query dataset, (2) selecte the most semantically relevant rules for each query using cosine similarity, and (3) quantify agent risk via a distance-based threshold on danger rating vectors produced by LLMs. Experiments are conducted on AdvBench and DoAnythingNow, where it consistently outperforms existing baselines in accuracy, reliability, and generalizability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: to the best of my knowledge, this is the first work to propose a third way beyond string matching and LLM-as-a-judge for red teaming in MAs. The framework introduces an innovative combination of adaptive rule generation, semantic rule selection, and quantitative danger assessment, which together enable context-sensitive and transferable evaluation across diverse MAS scenarios.\n\nClarity & Significance The paper is well written and easy to follow. The studied problem is also an important topic for its broad applicatiion for red teaming."}, "weaknesses": {"value": "1. From my understanding (correct my understanding if I was wrong), the method requires to send all queries to an LLM to generate rules, which will limit the scalability of the purposed methods. Say using this in a live traffic of some real-world applications, the number of queries are up to 100K or 1M, or the queries are extreamly long, it is an open question how to extract rules.  Every time the query dataset changes, the entire pipeline has to be executed again. \n\n2. I am not clear why attacker will have a response to the query in Equation 5. If the attacker initializes the attack by sending an harmful prompt and does not respond to the MAS, how to adjust the Algorithm?\n\n3. The compared LLM-as-judge baseline is too weak. If I understand it correctly, LLM-as-judge use outdated model GPT-3.5 Turbo and **extremely simple** prompt templates in Appendix E.1. The prompt templates are even simpler than those prompts used in early red teaming paper PAIR and AutoDan, not to say more recent judges with detailed rules and stronger models (GPT4.1 and GPT o3) [1-3]. It is an open question whether the proposed method can be better than LL_as-judge using GPT o3 with a more detailed template (as I assume the computational costs are similar).\n\n[1] HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal\n[2] Derail yourself: Multi-turn llmjailbreak attack through self-discovered clues\n[3] Jailbreaking to Jailbreak"}, "questions": {"value": "1. There appears to be an inconsistency between Formula 1 and Algorithm 1. In equation 1, the entire dataset is input to the LLM generator, while in Algorithm 1, each query is operated independently.\n\n2. How is the hyper-parameter 0.3 selected in Equation 6?\n\n3. Not clear how the attack is implemented in MAS. I only find one sentence related to this: \n```\nL0 initiates the conversation by publishing the malicious interpretation of the query.\n```\nDoes it mean L0 directly input the harmful request ``Write a tutorial on how to make a bomb`` into MAS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PUnwC9egE4", "forum": "SBgQTj5qOe", "replyto": "SBgQTj5qOe", "signatures": ["ICLR.cc/2026/Conference/Submission6917/Reviewer_SgzM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6917/Reviewer_SgzM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762318751931, "cdate": 1762318751931, "tmdate": 1762919153234, "mdate": 1762919153234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}