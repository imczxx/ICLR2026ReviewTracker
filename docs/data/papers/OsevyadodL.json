{"id": "OsevyadodL", "number": 11307, "cdate": 1758195958460, "mdate": 1763666353033, "content": {"title": "Einstein Fields: A Neural Perspective To Computational General Relativity", "abstract": "We introduce Einstein Fields, a neural representation designed to compress computationally intensive four-dimensional numerical relativity simulations into compact implicit neural network weights. By modeling the metric, the core tensor field of general relativity, Einstein Fields enable the derivation of physical quantities via automatic differentiation. Unlike conventional neural fields (e.g., signed distance, occupancy, or radiance fields), Einstein Fields fall into the class of Neural Tensor Fields with the key difference that, when encoding the spacetime geometry into neural field representations, dynamics emerge naturally as a byproduct. Our novel implicit approach demonstrates remarkable potential, including continuum modeling of four-dimensional spacetime, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use. It achieves up to a $\\mathtt{4,000}$-fold reduction in storage memory compared to discrete representations while retaining a numerical accuracy of five to seven decimal places. Moreover, in single precision, differentiation of the Einstein Fields-parameterized metric tensor is up to five orders of magnitude more accurate compared to naive finite differencing methods. We demonstrate these properties on several canonical test beds of general relativity and numerical relativity simulation data, while also releasing an open-source JAX-based library, taking the first steps to studying the potential of machine learning in numerical relativity.", "tldr": "", "keywords": ["neural fields (implicit neural representations)", "neural compression", "tensor fields", "differential geometry", "general relativity (GR) and numerical relativity (NR)", "Sobolev training", "differential geometry", "finite-difference methods"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c48be03145ddec321ab92661e94798c8b8befa55.pdf", "supplementary_material": "/attachment/0b6c53d0b1586a2371c25777c7fda794ac3cf670.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Einstein Fields (EinFields), a neural field approach to compress and represent 4D spacetime metrics from general relativity simulations. The method achieves up to 4,000× compression of metric tensor fields with 7-9 decimal digit accuracy (1E-7 to 1E-9 relative precision) while providing discretization-free continuous representations that can be trained on arbitrary point samples and queried at arbitrary resolutions. A key contribution is improved differentiation accuracy through automatic differentiation (AD), computing Christoffel symbols, Riemann tensors, and other derived quantities with up to 10^5 better accuracy than finite difference methods in FLOAT32. The approach parametrizes the metric distortion (deviation from flat space) using MLPs and employs Sobolev supervision. The validation focuses on three analytical solutions to Einstein's field equations: Schwarzschild, Kerr, and linearized gravitational waves, successfully reconstructing key relativistic phenomena."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## Strengths \n\n**Originality**: This paper presents a novel application of neural fields to general relativity, introducing the first implicit neural representation for tensor-valued spacetime geometries. The approach creatively adapts neural field techniques from computer vision to computational physics, with several original contributions.\n\n**Quality**: The paper demonstrates strong technical rigor with comprehensive validation across multiple canonical GR test cases (Schwarzschild, Kerr, gravitational waves). The evaluation methodology is sound. Ablation studies (Table 3) properly isolate the contributions of different design choices. The authors are transparent about limitations.\n\n**Clarity**: The paper is well-written and accessible. The background section (Section 2) effectively introduces both GR concepts and neural fields. Figure 1 provides an excellent conceptual overview of the pipeline. The mathematical notation is consistent and properly defined (though dense in places).\n\n**Significance**: This work addresses genuine computational bottlenecks in numerical relativity—storage (petabytes per simulation) and accurate tensor differentiation. The 4,000× compression factor and 10^5 improvement in derivative accuracy (FLOAT32) represent substantial practical gains."}, "weaknesses": {"value": "## Weaknesses\n\n**Limited experimental scope**: The validation is restricted to three analytical solutions to Einstein's field equations (Schwarzschild, Kerr, linearized gravitational waves). While these are canonical test cases, they represent idealized scenarios far simpler than realistic numerical relativity (NR) simulations. \n\n**Limited contextualization within scientific computing:** While the introduction mentions neural fields and ML for scientific computing, it lacks: (1) discussion of prior ML work specifically targeting numerical relativity or gravitational physics, (2) comparison with traditional compression methods used in scientific computing, and (3) detailed positioning relative to neural operators and PINNs. A dedicated related work section would help readers better understand the landscape and the paper's specific contributions.\n\n**Missing Error Quantification:** Tables 1-3 report single-valued metrics without error bars or confidence intervals. Table 1 mentions selecting 'the model with the lowest MAE,' suggesting multiple runs were performed but statistics are not reported.\n\n### Minor issues:\n\nPage 10, line 490: \"supplimentary\" → \"supplementary\"\n\nFigure 4 a caption: \"Perihilion precession\" → \"Perihelion precession\""}, "questions": {"value": "## Questions\n\n\n* Actual NR simulation data: Your validation strategy using analytical solutions (Schwarzschild, Kerr, linearized GW) with known ground truth is appropriate for demonstrating the method's capabilities. As a natural next step, have you tested EinFields on any actual numerical relativity simulation outputs, even at small scale? What additional challenges arise with real NR data?\n\n* Table 1 mentions selecting \"the model with the lowest MAE\" - how many training runs were performed? Can you report mean ± standard deviation over multiple random seeds for the key results in Tables 1-3? This is important for assessing reproducibility and typical vs. best-case performance.\n\n* Parameter generalization: Do you train a separate network for each physical configuration (M, a, etc.), or can one network generalize across parameter ranges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GF5GU3kxny", "forum": "OsevyadodL", "replyto": "OsevyadodL", "signatures": ["ICLR.cc/2026/Conference/Submission11307/Reviewer_DSBV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11307/Reviewer_DSBV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927259877, "cdate": 1761927259877, "tmdate": 1762922448905, "mdate": 1762922448905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper derives a novel neural representation method to compress the relativity simulations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Outstanding efficiency and accuracy are shown when representing the symmetry of the simulations.\n- This paper is especially well written and well presented.\n- The problems addressed by the new tool is of interest to a wide community."}, "weaknesses": {"value": "- I am not entirely sure that how interesting this paper will be for the readership of ICLR, of whom so few are well versed in this area."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8foyZsKxH5", "forum": "OsevyadodL", "replyto": "OsevyadodL", "signatures": ["ICLR.cc/2026/Conference/Submission11307/Reviewer_Lt1j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11307/Reviewer_Lt1j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013154545, "cdate": 1762013154545, "tmdate": 1762922448529, "mdate": 1762922448529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a neural tensor‑field (tensors in the General Relativity sense) parametrization of GR metrics with a JAX implementation for differential geometry operators wired through automatic differentiation. The demos (Schwarzschild/Kerr orbits, Weyl scalars, ring deformation under a linearized GW) look correct and generate polished images, and the ablations around Jacobian/Hessian supervision are good.\n\nThat said, I have major concerns with the claims and evaluations. I believe these are not aligned with numerical relativity (NR) practice, and so this is not yet at the point of being useful for actual science. Thus, the current narrative is a bit misleading. The headline is \"compressing 4D NR simulations by 1000-4000x with better derivatives than FD in FLOAT32,\" but most experiments are static analytic 3D cases (t = 0) plus one simple time‑varying GW; storage comparisons are made against a dense \"explicit grid\" strawman (modern NR code uses AMR or pseudo-spectral methods); baselines omit spectral/ROM methods; coordinate chart sensitivity is large; and long‑horizon dynamics need FLOAT64 to avoid divergence. I think it's a promising direction for ML to help, but I think this paper needs to be honest about the current status of such an approach. Doing so would not only be better for the paper, but I think also benefit the authors in that it would point out to the ML community where more work is needed. Basically I would like to see the narrative of this paper modified to be honest about the practicality of this, and about the toy baselines, before I consider acceptance, especially as general ML audiences will not know how to evaluate this."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- I like the clear pipeline and library, and a JAX code for this seems useful for the NR community. The graph from metric to derived quantities is explicit and leverages forward‑mode Jacobians/Hessians with einsum operations which is nice. This is a useful contribution of reusable tooling for GR in ML, and I think it is a good contribution by itself.\n- I liked the ablations accounting done in 4.2, it is nice and I think quite useful to see the effect of every modification to the training process. It is interesting to see that Jacobian/Hessian supervision indeed helps.\n- Neat canonical tests: precession, circular/eccentric Kerr orbits, ring response under a linearized GW, and Weyl/Kretschmann diagnostics are shown and largely match analytics on short horizons.\n- Multi‑chart training attempt: training/evaluating in multiple coordinate charts acknowledges a real pain point in GR ML (though this is near the end of the appendix, I think it should come earlier)"}, "weaknesses": {"value": "First, my main concerns:\n\n1. First, I think the scope and storage comparisons are misaligned with NR. The paper advertises 4D compression of NR simulations, but the primary experiments are analytic snapshots at t = 0 (Schwarzschild/Kerr); only the linearized‑gravity toy has time evolution. The compression factors compare MLP weights to explicit dense grids counted as \"#points x 4 bytes\" in FLOAT32, which is not how NR codes actually store data (they would use adaptive mesh refinement stored with an octree). So the 1000–4000x headline is basically comparing against a strawman and misleading to the ML community about the state of this domain.\n2. The paper itself notes that modern NR \"increasingly opts for (pseudo‑)spectral methods ... up to 1000–5000x faster on CPUs than FD on GPUs at comparable accuracy.\" But all quantitative baselines in this paper are finite difference stencils (on a uniform grid - but the only actual finite difference codes used in NR are based on adaptive meshes) and an analytic AD. There's no head‑to‑head vs actual NR codes used in GW modeling. This makes it hard to situate their method, even for someone who knows NR, let alone the ML community.\n3. The paper claims up to five orders of magnitude derivative gains over FD in FLOAT32, but geodesic integration requires FLOAT64 and long‑time rollouts still diverge (the authors show this in their own figures and explicitly state that they only surpass FD in single precision). This puts the method far from NR‑readiness, where double precision (or even higher) is standard.\n4. There is large coordinate‑chart sensitivity which is a bit worrisome. Table 8 (deep in the appendix) reports up to three orders of magnitude variation in \"Rel‑L2\" error across charts for the same spacetime. That undermines generality claims unless the representation or training explicitly handles diffeomorphisms or evaluates with chart‑invariant metrics.\n5. The physics is not enforced or audited. The pipeline mentions Bianchi identities, but experiments focus on pointwise tensor errors, scalar invariants, and geodesic tests. While the pointwise tensor errors are no doubt useful in clarifying (to a NR person) that these methods aren't yet ready, there's no reporting of physics checks, like conservation laws, etc., which are exactly the diagnostics one needs to trust a compressed metric in downstream NR workflows.\n6. I am a bit confused about the \"discretization‑free\" claims, since it seems the method is ultimately trained on a grid. Several places describe training on 4D spacetime grids or \"4D training and validation grid data,\" which undercuts the claim of being discretization‑free. Even an INR is ultimately a finite parametrization/basis.\n7. The throughput trade‑off is not discussed. Even if the file is tiny, post‑processing requires many MLP queries to reconstruct fields, whereas decompressing spectral grids is reading coefficients + evaluating polynomials. You still likely win on storage, but the compute/runtime trade‑off for analysis & viz should be stated.\n\nSecond, some other suggestions/comments:\n\n- I make the following comment purely to help the authors improve their work, and do not include this comment as part of my score for the paper, so feel free to not address this in your rebuttal. It is simply a suggestion/idea. So I think the branding of the method as \"Einstein Fields\" is not an optimal choice, because it would likely conflict with \"Einstein Field Equations\" in any search. Also, to the physicist, who I assume you would like to include in the audience of the paper, it does not give them any idea that this is related to machine learning.\n- Obviously GR is quite complicated to someone with no background. I am not sure it is possible to give it much of an introduction here, and I worry the current introduction might give the wrong ideas. I think it is better to simply direct the reader to an online resource, rather than give an inevitably incomplete description of the mathematics in the appendix. Try to think think about what purpose it serves: (1) for those who don't know GR, this is not going to be nearly enough to introduce them even to the basics; (2) for those who do know GR, they will not need this anyways. So, why include it at all? Consider, for audience (1) I think you need to simply target the intuition for each variable you model. That is the fundamentally useful thing to write about. And for audience (2) (curious physicists, maybe) I think you simply need to _translate_ GR concepts to machine learning for them. So, when you consider these two audiences, the appendix seems to serve little purpose. I recommend trying the split approach above: focus on intuition of the key target variables for the non‑physicists (and point them to other resources), and focus on translation of the machine learning concepts for the physicists. This would be much more effective in my view. \n  - Also, I did not check through all the math in the appendix. Thus, there could be errors.\n- It might be worth calling a GR tensor exactly that: a \"GR tensor,\" to differentiate from the ML meaning.\n- Use proper scientific notation instead of \"5.37E‑6\" in tables. \n- Even if the file is small, you still must run tens of thousands of MLP queries for post‑analysis/viz; be transparent about that runtime trade‑off versus reading spectral coefficients. \n- Figure 1 is noisy. Consider simplifying or splitting. (The caption itself also states training on a 4D spacetime grid, which conflicts with \"discretization‑free\" messaging.)"}, "questions": {"value": "- Please clarify the precise data format used for training (\"4D spacetime grid\" vs \"arbitrary samples\"), since the paper simultaneously describes the approach as discretization-free yet refers to training on regular grids.\n- Could you provide any comparison, even small-scale, against a spectral or AMR baseline (ideally an actual code used by the GR community) to contextualize the claimed compression?\n- Please explain how derivative accuracy scales in FLOAT64 and whether the observed long-time geodesic divergence persists under higher precision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Tw74AD6K1", "forum": "OsevyadodL", "replyto": "OsevyadodL", "signatures": ["ICLR.cc/2026/Conference/Submission11307/Reviewer_kkPc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11307/Reviewer_kkPc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225526103, "cdate": 1762225526103, "tmdate": 1762922448106, "mdate": 1762922448106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a neural tensor‑field (tensors in the General Relativity sense) parametrization of GR metrics with a JAX implementation for differential geometry operators wired through automatic differentiation. The demos (Schwarzschild/Kerr orbits, Weyl scalars, ring deformation under a linearized GW) look correct and generate polished images, and the ablations around Jacobian/Hessian supervision are good.\n\nThat said, I have major concerns with the claims and evaluations. I believe these are not aligned with numerical relativity (NR) practice, and so this is not yet at the point of being useful for actual science. Thus, the current narrative is a bit misleading. The headline is \"compressing 4D NR simulations by 1000-4000x with better derivatives than FD in FLOAT32,\" but most experiments are static analytic 3D cases (t = 0) plus one simple time‑varying GW; storage comparisons are made against a dense \"explicit grid\" strawman (modern NR code uses AMR or pseudo-spectral methods); baselines omit spectral/ROM methods; coordinate chart sensitivity is large; and long‑horizon dynamics need FLOAT64 to avoid divergence. I think it's a promising direction for ML to help, but I think this paper needs to be honest about the current status of such an approach. Doing so would not only be better for the paper, but I think also benefit the authors in that it would point out to the ML community where more work is needed. Basically I would like to see the narrative of this paper modified to be honest about the practicality of this, and about the toy baselines, before I consider acceptance, especially as general ML audiences will not know how to evaluate this."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- I like the clear pipeline and library, and a JAX code for this seems useful for the NR community. The graph from metric to derived quantities is explicit and leverages forward‑mode Jacobians/Hessians with einsum operations which is nice. This is a useful contribution of reusable tooling for GR in ML, and I think it is a good contribution by itself.\n- I liked the ablations accounting done in 4.2, it is nice and I think quite useful to see the effect of every modification to the training process. It is interesting to see that Jacobian/Hessian supervision indeed helps.\n- Neat canonical tests: precession, circular/eccentric Kerr orbits, ring response under a linearized GW, and Weyl/Kretschmann diagnostics are shown and largely match analytics on short horizons.\n- Multi‑chart training attempt: training/evaluating in multiple coordinate charts acknowledges a real pain point in GR ML (though this is near the end of the appendix, I think it should come earlier)"}, "weaknesses": {"value": "First, my main concerns:\n\n1. First, I think the scope and storage comparisons are misaligned with NR. The paper advertises 4D compression of NR simulations, but the primary experiments are analytic snapshots at t = 0 (Schwarzschild/Kerr); only the linearized‑gravity toy has time evolution. The compression factors compare MLP weights to explicit dense grids counted as \"#points x 4 bytes\" in FLOAT32, which is not how NR codes actually store data (they would use adaptive mesh refinement stored with an octree). So the 1000–4000x headline is basically comparing against a strawman and misleading to the ML community about the state of this domain.\n2. The paper itself notes that modern NR \"increasingly opts for (pseudo‑)spectral methods ... up to 1000–5000x faster on CPUs than FD on GPUs at comparable accuracy.\" But all quantitative baselines in this paper are finite difference stencils (on a uniform grid - but the only actual finite difference codes used in NR are based on adaptive meshes) and an analytic AD. There's no head‑to‑head vs actual NR codes used in GW modeling. This makes it hard to situate their method, even for someone who knows NR, let alone the ML community.\n3. The paper claims up to five orders of magnitude derivative gains over FD in FLOAT32, but geodesic integration requires FLOAT64 and long‑time rollouts still diverge (the authors show this in their own figures and explicitly state that they only surpass FD in single precision). This puts the method far from NR‑readiness, where double precision (or even higher) is standard.\n4. There is large coordinate‑chart sensitivity which is a bit worrisome. Table 8 (deep in the appendix) reports up to three orders of magnitude variation in \"Rel‑L2\" error across charts for the same spacetime. That undermines generality claims unless the representation or training explicitly handles diffeomorphisms or evaluates with chart‑invariant metrics.\n5. The physics is not enforced or audited. The pipeline mentions Bianchi identities, but experiments focus on pointwise tensor errors, scalar invariants, and geodesic tests. While the pointwise tensor errors are no doubt useful in clarifying (to a NR person) that these methods aren't yet ready, there's no reporting of physics checks, like conservation laws, etc., which are exactly the diagnostics one needs to trust a compressed metric in downstream NR workflows.\n6. I am a bit confused about the \"discretization‑free\" claims, since it seems the method is ultimately trained on a grid. Several places describe training on 4D spacetime grids or \"4D training and validation grid data,\" which undercuts the claim of being discretization‑free. Even an INR is ultimately a finite parametrization/basis.\n7. The throughput trade‑off is not discussed. Even if the file is tiny, post‑processing requires many MLP queries to reconstruct fields, whereas decompressing spectral grids is reading coefficients + evaluating polynomials. You still likely win on storage, but the compute/runtime trade‑off for analysis & viz should be stated.\n\nSecond, some other suggestions/comments:\n\n- I make the following comment purely to help the authors improve their work, and do not include this comment as part of my score for the paper, so feel free to not address this in your rebuttal. It is simply a suggestion/idea. So I think the branding of the method as \"Einstein Fields\" is not an optimal choice, because it would likely conflict with \"Einstein Field Equations\" in any search. Also, to the physicist, who I assume you would like to include in the audience of the paper, it does not give them any idea that this is related to machine learning.\n  - Edit: What about calling it \"Neural Einstein Fields\"?\n- Obviously GR is quite complicated to someone with no background. I am not sure it is possible to give it much of an introduction here, and I worry the current introduction might give the wrong ideas. I think it is better to simply direct the reader to an online resource, rather than give an inevitably incomplete description of the mathematics in the appendix. Try to think think about what purpose it serves: (1) for those who don't know GR, this is not going to be nearly enough to introduce them even to the basics; (2) for those who do know GR, they will not need this anyways. So, why include it at all? Consider, for audience (1) I think you need to simply target the intuition for each variable you model. That is the fundamentally useful thing to write about. And for audience (2) (curious physicists, maybe) I think you simply need to _translate_ GR concepts to machine learning for them. So, when you consider these two audiences, the appendix seems to serve little purpose. I recommend trying the split approach above: focus on intuition of the key target variables for the non‑physicists (and point them to other resources), and focus on translation of the machine learning concepts for the physicists. This would be much more effective in my view. \n  - Also, I did not check through all the math in the appendix. Thus, there could be errors.\n- It might be worth calling a GR tensor exactly that: a \"GR tensor,\" to differentiate from the ML meaning.\n- Use proper scientific notation instead of \"5.37E‑6\" in tables. \n- Even if the file is small, you still must run tens of thousands of MLP queries for post‑analysis/viz; be transparent about that runtime trade‑off versus reading spectral coefficients. \n- Figure 1 is noisy. Consider simplifying or splitting. (The caption itself also states training on a 4D spacetime grid, which conflicts with \"discretization‑free\" messaging.)"}, "questions": {"value": "- Please clarify the precise data format used for training (\"4D spacetime grid\" vs \"arbitrary samples\"), since the paper simultaneously describes the approach as discretization-free yet refers to training on regular grids.\n- Could you provide any comparison, even small-scale, against a spectral or AMR baseline (ideally an actual code used by the GR community) to contextualize the claimed compression?\n- Please explain how derivative accuracy scales in FLOAT64 and whether the observed long-time geodesic divergence persists under higher precision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Tw74AD6K1", "forum": "OsevyadodL", "replyto": "OsevyadodL", "signatures": ["ICLR.cc/2026/Conference/Submission11307/Reviewer_kkPc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11307/Reviewer_kkPc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225526103, "cdate": 1762225526103, "tmdate": 1763031155348, "mdate": 1763031155348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their valuable feedback that helps us improve this work. We have strived to provide an early rebuttal in order to facilitate the discussion with the reviewers and leverage their feedback. We have attached a revised document with the changes highlighted in blue. These mainly concern additional evaluation on a numerical relativity usecase (appendix F and parts of main text), adjusted positioning, and other particular adjustments in response to questions, concerns, and feedback. We will refine the formatting of the revision, prioritising sharing the key content as early as possible."}}, "id": "40P6WS2X97", "forum": "OsevyadodL", "replyto": "OsevyadodL", "signatures": ["ICLR.cc/2026/Conference/Submission11307/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11307/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission11307/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763665579400, "cdate": 1763665579400, "tmdate": 1763665579400, "mdate": 1763665579400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}