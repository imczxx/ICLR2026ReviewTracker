{"id": "pNzgwJqLg1", "number": 23084, "cdate": 1758339362381, "mdate": 1759896833239, "content": {"title": "MIP-Bench: Can LLMs Implicitly Personalize Responses Using Long-Term Memory?", "abstract": "Implicit Personalization (IP) is the task of tailoring responses to individual users by implicitly inferring their personal context. Prior studies in IP typically infer context from a single prompt. However, as interactions accumulate, users expect Memory-driven Implicit Personalization (MIP), where models implicitly leverage contexts from users' long-term interaction histories to provide more helpful responses. MIP introduces two unique challenges: (i) identifying sparse yet relevant personal contexts within extensive historical interactions, and (ii) understanding how varying personal contexts influence preferences among plausible answers to differentiate responses between multiple users. To navigate these challenges, we introduce MIP-Bench, the first benchmark to evaluate MIP in large language models (LLMs). Our experiments reveal that recent LLMs struggle with MIP, primarily due to difficulties in identifying and retrieving relevant personal context from memory. Furthermore, our new distribution-level evaluation framework shows that even models with strong instance-level performance often fail to differentiate responses across users, defaulting to generic or overly broad outputs rather than personalized ones.", "tldr": "We introduce MIP-Bench, the first benchmark to evaluate memory-driven implicit personalization in large language models (LLMs).", "keywords": ["large language models", "personalization", "long-term memory"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9fd8b0c9d7c85820c3a21bc94b8bc13a99caf53.pdf", "supplementary_material": "/attachment/9baa3161a1c44ff01d217932d8b794c00bcff000.zip"}, "replies": [{"content": {"summary": {"value": "In order to maximize their benefits for each user, LLM systems should recall relevant historical interactions with that user, and infer the optimal response based on this context.  While there has been some work in this direction, it has largely been focused on explicit personalization (i.e., adding persona information to the context) and/or short interaction timelines.  However, this explicit information will often not be available in or sufficient to inform many situations, and memory in personalization should stretch across many interactions over time.  To measure our progress in long-term personalization without explicit persona prompting, the authors of this work propose MIP-Bench, where MIP stands for Memory-driven Implicit Personalization.  MIP-Bench consists of 379 synthetic users with over 74,000 conversation sessions and 514 personalization graphs spanning different domains. Each graph links user histories to rubric-based preference vectors, and two novel metrics, Instance Personalization Score (IPS) and Distribution Personalization Score (DPS), are proposed to quantify user-level accuracy and cross-user differentiation.  Experiments reveal that current LLMs struggle with implicit personalization, mainly because retrieving the right personal context remains a bottleneck."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work is in an area of high interest to the community, as LLM personalization is a widely held goal with lots of headroom for improvement.  In my opinion the biggest strength of this work is the idea of grounding examples in personalization graphs and personalization paths, which hold the potential to enable detailed analysis and interpretation of a model’s personalization performance."}, "weaknesses": {"value": "While I appreciate the goals of this work, I find that it has some key weaknesses in terms of the construction, analysis, and validation of the proposed benchmark dataset:\n\n- What role does the distinction between declarative vs. non-declarative memory play in this paper, beyond some references in related work?  It is not clear why this is a useful lens.\n- I find that the paper lacks enough examples or clear explanation to understand what exactly is in the dataset.  I count 3 query examples in the entire paper including appendix (malaria/flu example, and then the two in the Qualitative Analysis section).  Since the potential contribution here is a dataset, I think much more illustration and analysis of what’s in the dataset is needed, including at least one full end to end example of one data point.  Otherwise, how am I supposed to judge if these tasks are interesting, realistic, challenging, etc.?\n- This problem setup feels pretty unusual/contrived, which has several knock-on effects.  First, IPS and DPS are not general metrics that can be adopted in most personalization setups; instead, they are specific to this formulation of binary rubric outcomes.  Second, given that I am not familiar with other work with a similar formulation, I struggled to understand the problem setup after one read of Sections 3 and 4.  In 3.1, I had to read it multiple times and circle back later to totally get it.  Interlacing the notation with some complete worked example might be helpful here.  Also, in line 183, it says “an identical question Q is posed to M ≤ N users”.  Is the question posed “to” users, or “by” users?  I found line 241 to be quite jarring, where it says that “MIP-Bench contains 514 Personalization Graphs with a total of 3,285 personalization paths.”  After reading this, I was left wondering what a personalization graph/path is, as those terms had not been used yet.  I also do not understand the paragraph beginning at line 292.  Once again, describing this along with a worked example would be useful.  \n- More analysis and illustration of IPS and DPS are needed to validate that they measure something useful and are helpful in making important performance distinctions.  The paper does not tell the reader what the practical difference is between, say, IPS of 0.56 and 0.66, nor does it calibrate DPS against human judgments or task difficulty, so it is unclear whether reported gains reflect meaningful personalization or, e.g., noise coming from rubric phrasing.\n- Experiments do not use SoTA LLM models which makes me worry that this benchmark could already be saturated.  \n- Lack of discussion of confounding between generating with GPT4o and then evaluating it on the dataset.\n- Human validation is limited in scope and depth. While the paper mentions reviewer checks in expert domains, there is no presentation of annotation error rates, inter-annotator agreement on rubric applicability or scoring, or checks of WildChat insertions for semantic compatibility with the synthetic profiles. Absent rigorous human validation, spurious correlations introduced by the generation pipeline may bias scores without reflecting genuine personalization performance. \n- Isn’t the “retrieval bottleneck” finding trivial given the experiment setup?\n- No explanation of retrieval methods\n- Based on the qualitative example given around lease breaking, there may be some examples where the model has the relevant memory capabilities, but may want to not give specific feedback because of safety, liability, etc.  This seems very possible for this example, and makes me worry how many other examples could have similar confounding issues (once again hard to tell because very little information is given about the contents)."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iAWIOVOkYf", "forum": "pNzgwJqLg1", "replyto": "pNzgwJqLg1", "signatures": ["ICLR.cc/2026/Conference/Submission23084/Reviewer_iNms"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23084/Reviewer_iNms"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761133186768, "cdate": 1761133186768, "tmdate": 1762942505130, "mdate": 1762942505130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a realistic personalization benchmark, where models have to retrieve and reason through past interaction histories to generate a personalized answer. They showed that a graph based retrieval method beats long context and rag methods for personalization with memories."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Using interaction history as memory bank and text personalization is the most natural and realistic setting, which is something that I haven't seen from previous papers.\n\n2. The design of the graph based retrieval is also a compelling and sensible method for tackling personalization problems with long memory."}, "weaknesses": {"value": "1. My major concern is that it seems that this benchmark is highly correlated with rag benchmarks, as the authors written in 5.1 I am wondering if the correlation are so high that the benchmark is simply evaluating if the model can retrieve relevant context rather than personalization. I would like to see if sth like a covariant matrix about this.\n\n2. User profiles are synthetically generated. It would be good to have real users and real interaction histories. Currently everything is synthetic.\n\n3. Benchmark looks pretty easy: it seems that the last generation's frontier model can already have decent enough results, such as gpt 4o, gemini 1.5, and claude 3.5. One would wonder what might be the resutls for gpt-5, gemini 2.5, and claude 4.5. A good benchmark should be challenging enough to provide signals for people to develop new methods and algorithms. But the benchmark looks like saturated already."}, "questions": {"value": "1. Can you show the correlation between some rag benchmark and your benchmark? i.e if a model performs well on rag then it performs good on your benchmark.\n\n2. What are the performances of gpt-5, gemini 2.5, and claude 4.5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zo4v6M5yxt", "forum": "pNzgwJqLg1", "replyto": "pNzgwJqLg1", "signatures": ["ICLR.cc/2026/Conference/Submission23084/Reviewer_b9jW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23084/Reviewer_b9jW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988323237, "cdate": 1761988323237, "tmdate": 1762942504569, "mdate": 1762942504569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MIP-Bench, a benchmark and evaluation protocol for memory-driven implicit personalization (MIP). It builds synthetic users with long interaction histories and measures both instance-level targeting (IPS) and distribution-level differentiation (DPS). Results suggest retrieval, not generation, is the main bottleneck; a Personalization Graph retrieval scheme outperforms long-context and vanilla RAG. Strong idea with clear metrics; main concerns are synthetic-data bias, rubric oversimplification, metric incentives, and potential construction–evaluation entanglement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The personalization-graph idea plus incompatibility constraints to assemble user histories is transparent and reproducible; they even inject real chat snippets to avoid fully synthetic staleness.\nDefining both IPS (instance-level personalization score) and DPS (distribution-level personalization score) is neat: IPS checks “did you personalize for this user,” DPS checks “did you differentiate across users”. this directly penalizes catch-all answers."}, "weaknesses": {"value": "A big part of the user histories and “decisive” signals is LLM-generated. That risks baking in the creator-model’s priors into the evaluation.\nThe intro argues memory helps attribution/editing, but there’s no metric like “did the model actually ground to the retrieved memory?”, so one stated motivation isn’t fully closed.\nDPS rewards differentiation from the population; a model that sprinkles idiosyncratic outputs might look better, even if it's not more accurately personalized."}, "questions": {"value": "Did you try a baseline that explicitly maximizes DPS by making user-id-conditioned random choices? How much DPS can you get without real personalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iUcnAMiyaD", "forum": "pNzgwJqLg1", "replyto": "pNzgwJqLg1", "signatures": ["ICLR.cc/2026/Conference/Submission23084/Reviewer_36E8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23084/Reviewer_36E8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058195783, "cdate": 1762058195783, "tmdate": 1762942503961, "mdate": 1762942503961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MIP-Bench, a benchmark for testing whether LLMs can implicitly personalize responses by leverage relevant details from long-term user history, rather than relying on explicit persona prompts. It consists of 514 questions across an average of 6.4 users, each having ~196 sessions per user. The scoring is based on two metrics: IPS (correct personalization per user) and DPS (whether the model outputs different answers for different users). Experimental results show that existing LLMs struggle mainly because they fail to retrieve relevant prior from memory."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clearly identifies and isolates an important problem: personalization from memory. The evaluation metrics also identifies the importance of both correctness dimension (IPS) and differentiation dimension (DPS).\n2. Dataset spans both casual and expert domains (legal, medical).\n3. Shows empirical evidence that retrieval is the key failure point, which is useful takeaway for future methods."}, "weaknesses": {"value": "1. Writing is very hard to follow. For example, the description in Section 3.1 is not clear whatsoever, an example could make this a lot better.\n\n2. A central design choice in MIP is the use of rubrics to convert free-form model outputs into one-hot preference vectors. This implicitly defines personalization as discrete answer selection, so it seems like the collection of rubrics practically defines a discrete clustering of all plausible correct answers. \n\nBecause of this (rubrics are defined at the query level with facts selection), multiple users can actually share the same ground-truth label even when their historical contexts differ substantially. In such cases, users differ, but their y-labels do not. This means personalization signal is bottlenecked by rubric granularity. If the rubric partitions the answer space coarsely or in a meaningless way, then DPS might not be sensitive to meaningful distinctions in how a model reasons about or contextualizes a response. A model may successfully retrieve and incorporate user-specific information, yet still receive low DPS.\n\nOverall it seems like the benchmark now entangles “personalization” with answer-class selection. This limits the benchmark’s ability to capture richer or more subtle forms of personalization."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z87jf8GVMp", "forum": "pNzgwJqLg1", "replyto": "pNzgwJqLg1", "signatures": ["ICLR.cc/2026/Conference/Submission23084/Reviewer_QeUK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23084/Reviewer_QeUK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236977378, "cdate": 1762236977378, "tmdate": 1762942503645, "mdate": 1762942503645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}