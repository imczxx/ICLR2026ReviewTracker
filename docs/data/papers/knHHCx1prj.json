{"id": "knHHCx1prj", "number": 16553, "cdate": 1758265921623, "mdate": 1763025189053, "content": {"title": "Recurrent Deep Differentiable Logic Gate Networks", "abstract": "While differentiable logic gates have shown promise in feedforward networks, their application to sequential modeling remains unexplored. \n    This paper presents the first implementation of Recurrent Deep Differentiable Logic Gate Networks (RDDLGN), combining Boolean operations with recurrent architectures for sequence-to-sequence learning.\n    Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and 30.9\\% accuracy during training, approaching GRU performance (5.41 BLEU) and graceful degradation (4.39 BLEU) during inference. \n    This work establishes recurrent logic-based neural computation as viable, opening research directions for  FPGA acceleration in sequential modeling and other recursive network architectures.", "tldr": "", "keywords": ["Recurrent Neural Networks", "Differentiable Logic Gates", "Machine Translation", "Sequence-to-Sequence Learning", "Hardware Acceleration", "Efficient Deep Learning", "FPGA", "Edge device"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7208d77fa6b308847eff692b0b26c17ba9ccafbf.pdf", "supplementary_material": "/attachment/4126399a3812449f19e8ee0a802a5c087b801949.zip"}, "replies": [{"content": {"summary": {"value": "Deep Differentiable Logic Gate Networks (DDLGNs) are extended by recurrency, and then an encoder-decoder model is build based on that.\n\nThe model is tested on the WMT 2014 English-to-German translation task."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Interesting novel architecture.\n* Good motivation.\n* Shifted monolingual prediction result is interesting.\n* Some good ablations in the appendix."}, "weaknesses": {"value": "* The experiments seems flawed. The paper reports about 5 BLEU on WMT’14 English-German translation. Usual numbers on this task are about 30-35 BLEU. So this is completely broken?\n* Sequence lengths are way too short for reasonable realistic experiments.\n* Models are way too small.\n* Contradiction on vanishing gradients (see comments below).\n* Contradiction on long-sequence handling (see comments below).\n* Unclear parts."}, "questions": {"value": "I don't understand the BLEU scores. Is that serious? Is that correct? Do you maybe measure sth different? They are far away from usual number that you would expect, which are in the range of 30-35 BLEU. They are so far off that this is either measured incorrectly, or measured sth differently, or totally broken. No matter what it is, it basically makes the whole work meaningless.\n\nWhy no cross attention, as it would be common for encoder-decoder models?\n\n\"The data is tokenized at the word level using regex-based splitting, with a shared\n16,000-token vocabulary for English and German.\" - I don't understand what this means. Do you use BPE or SPM or sth like that?\n\nThe models are obviously way too small.\n\nIf the goal is an efficient model, you should show how it performs when you give it the same amount of compute as a normal-sized well-performing baseline (e.g. some reasonable-sized Transformer). Does it perform better then?\n\n\"sequence length of 16 tokens\" - this just makes it a toy task. Increase it to at least 50-100 or so to make it actually interesting and relevant.\n\nContradiction on vanishing gradients:\nSec 5.5: \"confirming the absence of vanishing or exploding gradients\"\nSec 6: \"Additionally, the architecture suffers from vanishing gradient problems\"\n\nContradiction on long-sequence handling:\nThe shifted-token task is used to argue that RDDLGNs have superior long-range memorization capabilities (Fig 4).\nThe tokenizer ablation shows the model's performance collapses as sequence length increases (Table 2)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RyuMahllyF", "forum": "knHHCx1prj", "replyto": "knHHCx1prj", "signatures": ["ICLR.cc/2026/Conference/Submission16553/Reviewer_3duw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16553/Reviewer_3duw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855123228, "cdate": 1761855123228, "tmdate": 1762926634414, "mdate": 1762926634414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Recurrent Deep Differentiable Logic Gate Networks (RDDLGN)—a recurrent seq2seq model whose layers are mixtures of relaxed Boolean operators and which can be “collapsed” after training into a purely logical (binary) network intended for efficient inference (e.g., on FPGAs). On WMT’14 En→De, the uncollapsed model reports 5.00 BLEU / 30.9% token accuracy, and the collapsed variant reports 4.39 BLEU / 27.7%, roughly between vanilla RNN and GRU baselines under the authors’ settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper extends DDLGNs to sequence modeling with an encoder–decoder built from logic layers.\n\nThe memory evaluation is interesting. The shifted-copy task shows RDDLGN maintains high accuracy at larger temporal offsets than RNN/GRU,"}, "weaknesses": {"value": "Baselines are quite small and non-standard for WMT14, 5-ish BLEU feel like garbage and not convincing it's actually a practical setup."}, "questions": {"value": "I'd suggest the author why using such MT setup."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YIqXQ93HW5", "forum": "knHHCx1prj", "replyto": "knHHCx1prj", "signatures": ["ICLR.cc/2026/Conference/Submission16553/Reviewer_dcov"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16553/Reviewer_dcov"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183970978, "cdate": 1762183970978, "tmdate": 1762926633705, "mdate": 1762926633705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This review is written by the AC after reading the 2 reviews with 0 scores. Since there is one major, likely fatal issue raised by both the reviewers, it is appropriate to focus on that instead of an independent emergency reviewer. The issue is that the bleu score reported in this paper are around 5 (as opposed to 20-40), if the reviewer's interpretation is correct, this would makes any comparisons in the paper meaningless irrespective of what method was proposed, or what the exact setting was. The authors did highlight these bleu scores in the abstract, showing this result is key to the paper as opposed to a side experiment.\n\nTo provide some specific references, On the EN-DE newstest task, phrased based system from 2014 had ~20 bleu score:\nhttps://web.archive.org/web/20140625052707/http://matrix.statmt.org/\n transformer reported 28 in 2017.\nTable 25-26 of the \"Findings of WMT 2014\" https://aclanthology.org/W14-3302.pdf for the medical tasks also reported ~20 bleu scores.\n\n~5 bleu score was reported in tables 2 and 3 of this submission, which is probably around a word dictionary look up.\n\nThe authors should respond to this in the response period."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "not evaluated"}, "weaknesses": {"value": "not evaluated"}, "questions": {"value": "Can the authors explain why their range of BLEU scores are out of normal, see summary for specific pointers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QAQ8BLolfn", "forum": "knHHCx1prj", "replyto": "knHHCx1prj", "signatures": ["ICLR.cc/2026/Conference/Submission16553/Reviewer_U2Mo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16553/Reviewer_U2Mo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762735763572, "cdate": 1762735763572, "tmdate": 1762926633331, "mdate": 1762926633331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}