{"id": "B4SCegRJOA", "number": 16775, "cdate": 1758268558356, "mdate": 1759897220064, "content": {"title": "Predictive CVaR Q-learning", "abstract": "We propose a sample-efficient Q-learning algorithm for reinforcement learning with the Conditional Value-at-Risk (CVaR) objective. Our algorithm is built upon predictive tail value function, a novel formulation of risk-sensitive action value, that admits a recursive structure as in the conventional risk-neutral Bellman equation. This structure enables the Q-learning algorithm to utilize the entire set of sample trajectories rather than relying only on worst-case outcomes, enhancing the sample efficiency. We further derive a Bellman optimality equation and a policy improvement theorem, which provide theoretical foundations of our algorithm and remedy inconsistencies that have existed in the literature. Empirical results demonstrate that our method consistently improves CVaR performance while maintaining stable and interpretable learning dynamics.", "tldr": "We introduce a new Bellman equation tailored for the CVaR objective, and develop an efficient Q-learning algorithm accompanied by a policy improvement theorem.", "keywords": ["CVaR optmization", "Risk-sensitive RL", "Q-learning", "Bellman equation", "Policy improvemen"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31204c95d9e9ec1289ceeeea26b1c4ba881d4d40.pdf", "supplementary_material": "/attachment/de2fb9b7682ad8f94e4e7353937d24927f91a9f1.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a novel CVaR recursive structure based on predictive tail value functions and predictive tail probability functions. This recursive structure, along with a risk-sensitivity exploration strategy, leads to superior empirical performance when compared to the usual CVaR recursive structure."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed CVaR recursive structure based on predictive tail value functions and predictive tail probability functions is innovative and clever. The theoretical work done to support this recursive structure is sound (albeit with some typos; see weaknesses). The empirical results are encouraging."}, "weaknesses": {"value": "Overall, I would consider this paper to be sound work. However, I do have some concerns that would need to be addressed prior to publication. \n\nMy biggest concern is that the work performed does not validate the authors’ claims. In particular, the authors mention numerous times that the primary benefit of their proposed method is sample efficiency, yet there is no theoretical work done in support of this claim, and the empirical results do not provide any evidence of such claims. The closest measure of sample efficiency that can be made from the empirical results is in Figures 2/3b), however in Figure 2b), none of the methods converge to the same solution so a proper evaluation of sample efficiency cannot be made. In Figure 3b), there is no statistical difference between the methods. \n\nMoreover, I am not convinced (i.e., it was not rigorously argued by the authors) that value-based CVaR methods have the same sample efficiency issues that policy-gradient methods do. In particular, the claim that value-based methods lead to significant sample inefficiency in lines 165-166 is not proved by the authors, nor do they provide a citation to back up these claims. In fact, a key piece of theoretical work that would greatly enhance the paper is some result that shows why the regular CVaR decomposition (Equation 2) is sample inefficient.\n\nAnother concern is that the empirical analysis lacks focus. In particular, in addition to the concerns mentioned above, the use of function approximation for the simple experiments considered in this paper seems unnecessary. I would argue that if the authors want to include the work related to function approximation, they need to provide a compelling experiment that makes proper use of it. Furthermore, it seems odd that for the simple experiments included in the paper, that neither method can find the optimal solution. \n\nIt is also not clear whether the shown gain in performance is entirely due to the risk-sensitivity exploration strategy. In particular, was the same exploration strategy used with the baseline algorithm as well? If so, then why even include it in the first place when making the comparison (i.e., would it not be a cleaner comparison without the exploration strategy)? If not, then this is not a proper evaluation of the proposed algorithm's performance, and an ablation study would be needed.\n\nOverall, although I see a lot of merit in the work performed by the authors, the current draft of the paper makes it seem like a ‘forced’ adaptation of prior work done in the policy-gradient domain into the value-based domain, rather than a purposeful, adequately-motivated endeavour. \n\nAccordingly, in order to increase my score, the authors would need to: 1a) provide theoretical and/or empirical results that support their claims related to sample efficiency, or 1b) remove the claims of sample efficiency and find a more compelling narrative for the paper. 2) The authors would also need to address my concerns related to the empirical analysis.\n\n**Minor Comments:**\n- The introduction is unfocused and hard to read. In particular, the constant switching between policy-gradient and value-based methods is hard to follow. Overall, I do not see a reason to mention policy-gradient methods at all in this paper. \n- Line 31: I would argue that CVaR has a lot of tractability issues (which is why it is such a difficult objective to optimize) and that the primary reason that it is valued is because it is a coherent risk measure.\n- Lines 46-51: The discussion in this paragraph completely ignores the notion of dynamic risk measures, which would need to be mentioned to make a proper argument.\n- Section 2 would greatly benefit by having more citations related to the methods that the authors are building upon.\n- Appendix C needs equation numbers.\n- Lemma 2 is filled with several copy/paste errors from Lemma 1 (e.g. c) is not needed in this proof)"}, "questions": {"value": "Lines 183-184: can the authors expand on why the choice of $\\eta$ is arbitrary? This seems counterintuitive to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a5SMQSX8DI", "forum": "B4SCegRJOA", "replyto": "B4SCegRJOA", "signatures": ["ICLR.cc/2026/Conference/Submission16775/Reviewer_wLib"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16775/Reviewer_wLib"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594053715, "cdate": 1761594053715, "tmdate": 1762926818071, "mdate": 1762926818071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel CVaR recursive structure based on predictive tail value functions and predictive tail probability functions. This recursive structure, along with a risk-sensitivity exploration strategy, leads to superior empirical performance when compared to the usual CVaR recursive structure."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed CVaR recursive structure based on predictive tail value functions and predictive tail probability functions is innovative and clever. The theoretical work done to support this recursive structure is sound (albeit with some typos; see weaknesses). The empirical results are encouraging."}, "weaknesses": {"value": "Overall, I would consider this paper to be sound work. However, I do have some concerns that would need to be addressed prior to publication. \n\nMy biggest concern is that the work performed does not validate the authors’ claims. In particular, the authors mention numerous times that the primary benefit of their proposed method is sample efficiency, yet there is no theoretical work done in support of this claim, and the empirical results do not provide any evidence of such claims. The closest measure of sample efficiency that can be made from the empirical results is in Figures 2/3b), however in Figure 2b), none of the methods converge to the same solution so a proper evaluation of sample efficiency cannot be made. In Figure 3b), there is no statistical difference between the methods. \n\nMoreover, I am not convinced (i.e., it was not rigorously argued by the authors) that value-based CVaR methods have the same sample efficiency issues that policy-gradient methods do. In particular, the claim that value-based methods lead to significant sample inefficiency in lines 165-166 is not proved by the authors, nor do they provide a citation to back up these claims. In fact, a key piece of theoretical work that would greatly enhance the paper is some result that shows why the regular CVaR decomposition (Equation 2) is sample inefficient.\n\nAnother concern is that the empirical analysis lacks focus. In particular, in addition to the concerns mentioned above, the use of function approximation for the simple experiments considered in this paper seems unnecessary. I would argue that if the authors want to include the work related to function approximation, they need to provide a compelling experiment that makes proper use of it. Furthermore, it seems odd that for the simple experiments included in the paper, that neither method can find the optimal solution. \n\nIt is also not clear whether the shown gain in performance is entirely due to the risk-sensitivity exploration strategy. In particular, was the same exploration strategy used with the baseline algorithm as well? If so, then why even include it in the first place when making the comparison (i.e., would it not be a cleaner comparison without the exploration strategy)? If not, then this is not a proper evaluation of the proposed algorithm's performance, and an ablation study would be needed.\n\nOverall, although I see a lot of merit in the work performed by the authors, the current draft of the paper makes it seem like a ‘forced’ adaptation of prior work done in the policy-gradient domain into the value-based domain, rather than a purposeful, adequately-motivated endeavour. \n\nAccordingly, in order to increase my score, the authors would need to: 1a) provide theoretical and/or empirical results that support their claims related to sample efficiency, or 1b) remove the claims of sample efficiency and find a more compelling narrative for the paper. 2) The authors would also need to address my concerns related to the empirical analysis.\n\n**Minor Comments:**\n- The introduction is unfocused and hard to read. In particular, the constant switching between policy-gradient and value-based methods is hard to follow. Overall, I do not see a reason to mention policy-gradient methods at all in this paper. \n- Line 31: I would argue that CVaR has a lot of tractability issues (which is why it is such a difficult objective to optimize) and that the primary reason that it is valued is because it is a coherent risk measure.\n- Lines 46-51: The discussion in this paragraph completely ignores the notion of dynamic risk measures, which would need to be mentioned to make a proper argument.\n- Section 2 would greatly benefit by having more citations related to the methods that the authors are building upon.\n- Appendix C needs equation numbers.\n- Lemma 2 is filled with several copy/paste errors from Lemma 1 (e.g. c) is not needed in this proof)"}, "questions": {"value": "Lines 183-184: can the authors expand on why the choice of $\\eta$ is arbitrary? This seems counterintuitive to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a5SMQSX8DI", "forum": "B4SCegRJOA", "replyto": "B4SCegRJOA", "signatures": ["ICLR.cc/2026/Conference/Submission16775/Reviewer_wLib"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16775/Reviewer_wLib"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594053715, "cdate": 1761594053715, "tmdate": 1763044638855, "mdate": 1763044638855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenges in risk-sensitive reinforcement learning (RL) using the Conditional Value-at-Risk (CVaR) objective, which focuses on optimizing the expected return in the worst-case quantile of the return distribution (e.g., for safety-critical applications like autonomous driving or finance). Standard CVaR RL methods are sample-inefficient due to two key issues: (1) noisy policy evaluation from treating CVaR as a non-decomposable, terminal objective, which delays learning signals and hinders temporal credit assignment; and (2) \"blindness to success,\" where the agent ignores high-return trajectories outside the risk tail, leading to premature convergence to overly conservative, suboptimal policies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposed CVaR recursive structure based on predictive tail value functions and predictive tail probability functions is innovative and clever. \n2. The theoretical work done to support this recursive structure is sound (albeit with some typos; see weaknesses). \n3. The empirical results are encouraging."}, "weaknesses": {"value": "Overall I would consider this paper to be sound work. However, I do have some concerns that would need to be addressed prior to publication. \n\nMy biggest concern is that the work performed does not validate the authors’ claims. In particular, the authors mention numerous times that the primary benefit of their proposed method is sample efficiency, yet there is no theoretical work done in support of this claim, and the empirical results provided do not provide any evidence of such claims. The closest measure of sample efficiency that can be made from the empirical results is in Figures 2-(b) and  3-(b), however in Figure 2-(b), none of the methods converge to the same solution so a proper evaluation of sample efficiency cannot be made. In Figure 3-(b), there is no statistical difference between the methods. \n\nMoreover, I am not convinced (i.e., it was not rigorously argued by the authors) that value-based CVaR methods have sample efficiency issues in the same way that policy gradient methods do. In particular, the claim that value-based methods lead to significant sample inefficiency in lines 165-166 is not proved by the authors, nor do they provide a citation to back up these claims. In fact, a key piece of theoretical work that would greatly enhance the paper is some results that show why the regular CVaR decomposition (Equation 2) is sample inefficient.\n\nAnother concern is that the empirical analysis lacks focus. In particular, in addition to the concerns mentioned above, the use of function approximation for the simple experiments considered in this paper seems unnecessary. I would argue that if the authors want to include the work related to function approximation, they need to provide a compelling experiment that makes proper use of it. Furthermore, it seems odd that for the simple experiments included in the paper, neither method can find the optimal solution. \n\nIt is also not clear whether the shown gain in performance is due to the risk-sensitivity exploration strategy. In particular, was the same exploration strategy used with the baseline algorithm as well? If so, then why even include it in the first place when making the comparison (i.e., would it not be a cleaner comparison without the exploration strategy)? If not, then this is not a proper evaluation of the proposed algorithm's performance.\n\nOverall, although I see a lot of merit in the work performed by the authors, the current draft of the paper makes it seem like a ‘forced’ adaptation of prior work done in the policy-gradient domain into the value-based domain, rather than a purposeful, adequately-motivated endeavor. \n\nAccordingly, in order to increase my score, the authors would need to: 1a) provide theoretical and/or empirical results that support their claims related to sample efficiency, or 1b) remove the claims of sample efficiency and find a more compelling narrative for the paper. The authors would also need to address my concerns related to the empirical analysis."}, "questions": {"value": "Lines 183-184: can the authors expand on why the choice of $\\eta$ is arbitrary? This seems counterintuitive to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bOUmVPGrRH", "forum": "B4SCegRJOA", "replyto": "B4SCegRJOA", "signatures": ["ICLR.cc/2026/Conference/Submission16775/Reviewer_Dyty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16775/Reviewer_Dyty"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704931519, "cdate": 1761704931519, "tmdate": 1762926817692, "mdate": 1762926817692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenges in risk-sensitive reinforcement learning (RL) using the Conditional Value-at-Risk (CVaR) objective, which focuses on optimizing the expected return in the worst-case quantile of the return distribution (e.g., for safety-critical applications like autonomous driving or finance). Standard CVaR RL methods are sample-inefficient due to two key issues: (1) noisy policy evaluation from treating CVaR as a non-decomposable, terminal objective, which delays learning signals and hinders temporal credit assignment; and (2) \"blindness to success,\" where the agent ignores high-return trajectories outside the risk tail, leading to premature convergence to overly conservative, suboptimal policies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposed CVaR recursive structure based on predictive tail value functions and predictive tail probability functions is innovative and clever. \n2. The theoretical work done to support this recursive structure is sound (albeit with some typos; see weaknesses). \n3. The empirical results are encouraging.\n\n1. The introduction of predictive tail value functions that satisfy a Bellman recursion is conceptually appealing and represents a reasonable extension to value-based CVaR methods.\n2. The paper offers analytical works supporting the proposed methods.\n3. The proposed methods achieved a strong performance"}, "weaknesses": {"value": "1) The scope of the empirical analysis performed is limited only to small-scale, tabular settings. Although the results show feasibility, they don't convincingly demonstrate the claimed sample efficiency advantages or scalability to meaningful problem domains. The paper would benefit from having more experiments performed.\n\n2) The proposed exploration strategy is not fully tested. An ablation isolating its contribution would clarify its role in performance gains. Moreover, the paper could benefit by better motivating why this exploration strategy is needed to begin with.\n\n3) The paper does not provide strong evidence of sample efficiency claims made. The experiments do not provide strong evidence that the authors' method is more sample efficient. Adding even a simple convergence rate analysis or additional experiments that show the gained sample efficiency would make the empirical claims more convincing."}, "questions": {"value": "Does the exploration strategy apply equally to baseline methods in your comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bOUmVPGrRH", "forum": "B4SCegRJOA", "replyto": "B4SCegRJOA", "signatures": ["ICLR.cc/2026/Conference/Submission16775/Reviewer_Dyty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16775/Reviewer_Dyty"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704931519, "cdate": 1761704931519, "tmdate": 1763067078289, "mdate": 1763067078289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a sample-efficient Q-learning algorithm (PCVaR-Q) to optimize the Conditional Value-at-Risk (CVaR) target. Its core contribution is two key innovations: First, the \"predictive tail value function\" is proposed, which constructs a novel recursive structure suitable for CVaR targets, similar to traditional risks. The neutral Bellman equation aims to solve the problem of noise strategy evaluation caused by the indecomposition of the target. Second, introduce a \"two-way exploration\" strategy, which explores the risk sensitivity of action space and intelligent bodies at the same time, so as to alleviate the \"blindness to success\" phenomenon."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core innovations lie in the proposal of the predictive tail value function $f^\\chi$ and the predictive tail probability function $g^\\chi$ for policy kenel $\\chi$. Based on the newly defined tail functions, Theorem 1 & 2 gives the Bellman equation and the Bellman optimality equation. \n2. Combined with the newly developed tail value function and probability function, the proposed \"two-way randomized exploration\" approach explicitly solves the known problem of \"blindness to success\" in CVaR learning, and encourages intelligent bodies to explore strategies with different risk preferences. This is achieved by samplingys around the risk budget $\\eta$.\n3. The experimental results (Figures 2 and 3) strongly support the author's argument. Compared with the CVaR-Q baseline, PCVaR-Q shows higher stability and lower variance during the training process."}, "weaknesses": {"value": "1. The entire theoretical framework (especially Theorem 1 & 2) depends on Assumption 1, which states that the distribution of the residual return $R_{t:T}$ has no probability mass. However, many standard reinforcement learning environments (including discrete rewards or deterministic rewards) would violate this assumption. Moreover, I found that the experiements environment considered (such as the. Sequential decision tree setting) clearly violates Assumption 1. The author did not discuss the impact of the violation of thies assumption.\n2. TThe pre-training step is crucial for success, as all models that did not undergo pre-training (Figure 4a-d) failed to learn the optimal path. This seems to weaken the argument that the algorithm has a robust exploration strategy and \"sample-efficient,\" as exploration seems to fail in cold-start case.\n3. Currently, the experiments in the main text (Section 5) have failed to clearly disentangle the contributions of the three main contributions—(1) the new Bellman equation, (2) pre-training, and (3) two-way random exploration—to the performance improvement."}, "questions": {"value": "1. For the augmented state $(s, y, a)$, how is the \"table-based function approximator\" implemented? Considering that $y$ is a continuous variable, is discretization based on the grid $H$ used?\n2. Without using pre-training, to what extent can your proposed new Bellman equation (Eq. 5) and bidirectional exploration itself solve the issues of \"blindness\" and learning instability?\n\n\n---\nTypos\n\n1. Line 369, \"through risk-nuetral Q-learning.\": nuetral -> neutral.\n1. Line 441, \"experimental environment and and the distinct policies\": there are two \"and\" here.\n2. Line 472, \"a novel novel CVaR Q-learning framework\": double \"novel\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ECQYGwS6CG", "forum": "B4SCegRJOA", "replyto": "B4SCegRJOA", "signatures": ["ICLR.cc/2026/Conference/Submission16775/Reviewer_9ktc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16775/Reviewer_9ktc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912426675, "cdate": 1761912426675, "tmdate": 1762926817328, "mdate": 1762926817328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Predictive CVaR Q-Learning (PCVaR-Q), which makes CVaR optimization TD-learnable by defining predictive tail value and predictive tail probability functions that satisfy a new Bellman recursion. This enables step-wise CVaR learning and supports a proven policy-improvement guarantee."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear theoretical innovation: a Bellman-consistent, value-based formulation for CVaR.\n- Strong proofs and solid connection to policy improvement.\n- Well-written and conceptually clear."}, "weaknesses": {"value": "- Limited empirical scope: Experiments are small-scale and tabular; results demonstrate feasibility but not scalability.\n- Comparison gaps: The paper could benchmark against more recent risk-sensitive or distributional RL algorithms (e.g., D4PG, IQN with tail weighting).\n- Exploration heuristic: The “risk-level exploration” scheme is sensible but empirically underexplored."}, "questions": {"value": "1. How sensitive is learning stability to the sampling of risk levels during exploration?\n2. Can the predictive-tail recursion extend naturally to actor–critic or deep function-approximation settings?\n3. Does the algorithm handle non-stationary return distributions or stochastic environments robustly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1IXPVJ524N", "forum": "B4SCegRJOA", "replyto": "B4SCegRJOA", "signatures": ["ICLR.cc/2026/Conference/Submission16775/Reviewer_fM76"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16775/Reviewer_fM76"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012793553, "cdate": 1762012793553, "tmdate": 1762926816642, "mdate": 1762926816642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method for RL with a Conditional Value-at-Risk objective. To address the challenges posed by nonlinearity and non-decomposability, the authors introduce predictive value/probability functions and develop a new RL algorithm based on them."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation and mathematical development are clearly presented. While the derivations seem fairly standard, they provide a clear motivation for the proposed algorithm."}, "weaknesses": {"value": "The paper's contribution requires further justification.\n\n1. After formulating objective (1), one could straightforwardly apply an actor–critic approach to optimize $ \\mathbb{E}^{\\chi,\\eta}[-(\\eta - R_{1:T})^{+}]$ (e.g., REINFORCE). It is not evident that the proposed method is superior to such actor-critic methods. While it may be true that only trajectories with non-zero effective reward are informative for actor-critic, the same limitation appears to affect the proposed algorithm: since g models the tail probability, when only a small subset of trajectories has non-zero effective reward, the estimate of g is likely to be noisy.\n\n2. The experiments are limited to toy settings and do not include comparisons against actor-critic methods or stronger baselines. A more comprehensive comparison would also address the weakness noted above."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0LoPJkPJYm", "forum": "B4SCegRJOA", "replyto": "B4SCegRJOA", "signatures": ["ICLR.cc/2026/Conference/Submission16775/Reviewer_FfEG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16775/Reviewer_FfEG"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16775/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763334676024, "cdate": 1763334676024, "tmdate": 1763334676024, "mdate": 1763334676024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method for RL with a Conditional Value-at-Risk objective. To address the challenges posed by nonlinearity and non-decomposability, the authors introduce predictive value/probability functions and develop a new RL algorithm based on them."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation and mathematical development are clearly presented. While the derivations seem fairly standard, they provide a clear motivation for the proposed algorithm."}, "weaknesses": {"value": "The paper's contribution requires further justification.\n\n1. After formulating objective (1), one could straightforwardly apply an actor–critic approach to optimize $ \\mathbb{E}^{\\chi,\\eta}[-(\\eta - R_{1:T})^{+}]$ (e.g., REINFORCE). It is not evident that the proposed method is superior to such actor-critic methods. While it may be true that only trajectories with non-zero effective reward are informative for actor-critic, the same limitation appears to affect the proposed algorithm: since g models the tail probability, when only a small subset of trajectories has non-zero effective reward, the estimate of g is likely to be noisy.\n\n2. The experiments are limited to toy settings and do not include comparisons against actor-critic methods or stronger baselines. A more comprehensive comparison would also address the weakness noted above."}, "questions": {"value": "See above.\n\n3. In the proposed algorithm, the function g is trained using squared-error loss. Since g models a tail probability, would using log loss be more appropriate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0LoPJkPJYm", "forum": "B4SCegRJOA", "replyto": "B4SCegRJOA", "signatures": ["ICLR.cc/2026/Conference/Submission16775/Reviewer_FfEG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16775/Reviewer_FfEG"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16775/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763334676024, "cdate": 1763334676024, "tmdate": 1763343181989, "mdate": 1763343181989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}