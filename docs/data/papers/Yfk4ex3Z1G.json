{"id": "Yfk4ex3Z1G", "number": 9565, "cdate": 1758127855680, "mdate": 1759897711918, "content": {"title": "Noise-Adaptive Diffusion Sampling for Inverse Problems Without Task-Specific Tuning", "abstract": "Diffusion models (DMs) have recently shown remarkable performance on inverse problems (IPs). Optimization-based methods can fast solve IPs using DMs as powerful regularizers, but it is susceptible to local minima and noise overfitting. Although DMs can provide strong priors for Bayesian approaches, enforcing measurement consistency during the denoising process leads to manifold infeasibility issues. We propose Noise-space Hamiltonian Monte Carlo (N-HMC), a posterior sampling method that treats reverse diffusion as a deterministic mapping from initial noise to clean images. N-HMC enables comprehensive exploration of the solution space, avoiding local optima. By moving inference entirely into the initial-noise space, N-HMC keeps proposals on the learned data manifold. We provide a comprehensive theoretical analysis of our approach and extend the framework to a noise-adaptive variant (NA-NHMC) that effectively handles IPs with unknown noise type and level. Extensive experiments across four linear and three nonlinear inverse problems demonstrate that NA-NHMC achieves superior reconstruction quality with robust performance across different hyperparameters and initializations, significantly outperforming recent state-of-the-art methods. Code will be made available on GitHub upon publication.", "tldr": "", "keywords": ["Diffusion models", "Inverse problems", "Generative model", "Bayesian inference"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/78e7b8a032eb84911ef81899db5a00607e085bc1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Noise-space Hamiltonian Monte Carlo (N-HMC), which uses HMC to search for a good initial noise for solving inverse problems. The authors additionally propose a noise adaptive version of N-HMC, which adjusts the algorithm to work with unknown noise levels."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The performance seems to be good, outperforming several widely established baselines.\n\n2. The method of using HMC for searching better noise initialization is new."}, "weaknesses": {"value": "1. *What* N-HMC is solving is unclear. Is this doing posterior sampling? The mathematical statement should be precisely provided. Currently, the derivation starts with (7), which is ad-hoc. *Where* is the posterior scores used?\n\n2. One of the motivations for this method is that the performance is inherently free form hyperparameter tuning, which does not seem to be the case. As the method is based on HMC, there are actually *more* hyperparameters that one can adjust, including how you would define the burn-in period. Reading the appendix, I am not convinced that the method requires less efforts for hparam tuning. It actually seems to require more effort, as opposed to methods such as DPS where one can just choose a step size.\n\n3. In the experiments, two more metrics should be reported. PSNR/SSIM/LPIPS are all distortion metrics, and reporting them all does not give a more informed picture. 1) Report the FID values (perception metric) with more than 1k, 2) Report the computational cost. The computational cost is reported in the appendix, but it should be more accessible. 90 seconds is relatively slow, which is another drawback of the method.\n\n4. The equality for $\\nabla_{x_T} \\log p(y|x_T)$ is, at best, an approximation. This holds across the entirety of the derivations.\n\n5. The noise-adaptive part is confusing. $\\sigma_y$ is undefined in the main text. It starts by stating that they model the noise variance with an inverse-gamma prior, which is arbitrary. How this leads to Alg. 3, is again, ambiguous. How is $m$ set?\n\n6. Following 5, even for unknown noise levels, methods such as DPS are fine off with choosing a static step size (e.g. 1.0), which works well across all noise levels. If the authors were to truly argue that the noise adaptive part is important, then the experiments should be conducted on real-world degradations that are off the inverse crime setting."}, "questions": {"value": "1. The authors assume that $\\sigma_y$ follows an inverse-gamma prior, but later admits that they use an uniformative (i.e. uniform) prior. Any clarification on this?\n\n2. Why is phase retrieval branded as a *multimodal IP*? All inverse problems are inherently multimodal, and this may confuse the readers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EXhNg6Nv9J", "forum": "Yfk4ex3Z1G", "replyto": "Yfk4ex3Z1G", "signatures": ["ICLR.cc/2026/Conference/Submission9565/Reviewer_eggp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9565/Reviewer_eggp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760492025076, "cdate": 1760492025076, "tmdate": 1762921119585, "mdate": 1762921119585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To avoid local minima or noise overfitting problem in inverse problem solving with diffusion model, the paper proposed to search a good initial noise by Hamiltonian Monte Carlo (HMC), which leads to the following sampling through an ODE staying on the data manifold. For this, the proposed method repeat updating initial noise with sampling only with 2 denoising steps. The paper also introduces noise adaptive sampling which provides robustness on various measurement noises."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents motivation and methods clearly.\n- The paper considers various measurement noise including impulse and speckle noises, which increases the effectiveness of the proposed method in real world.\n- Extensive experiments support the effectiveness of the proposed method and gives sufficient analysis on its behavior."}, "weaknesses": {"value": "- The major difference from DAPS is twofolds: the paper uses HMC instead of Langevin dynamics, and the search space is changed from image to noise space. However, both changes seems to introduce additional computational cost, which results in slower sampling.\n- Missing related work [1] that update the initial noise with data fidelity gradient after sampling.\n- The performance reported in Table 1 has a huge gap from the original paper. For example, DAPS for Phase Retrieval originally achieves 30.63dB of PSNR with the same setting, but it is 18.52dB in this paper.\n\n\nReferences\n\n[1] Diffusion Image Prior, ICCV 2025"}, "questions": {"value": "- Could authors explain the reason of large gap of performance between the original baseline paper and this paper?\n- Could authors provide runtime comparison by setting the same number of function evaluation? Or Could authors provide performance comparison by setting the same computational budget? \n- Is there any reason for empty boxes for ReSample in Figure 10 - 16?\n- What if we use the GAN or consistency model instead of diffusion model? The reviewer cannot find a strong reason to use the diffusion model from the algorithm 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x7zGYuKHkB", "forum": "Yfk4ex3Z1G", "replyto": "Yfk4ex3Z1G", "signatures": ["ICLR.cc/2026/Conference/Submission9565/Reviewer_kd9g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9565/Reviewer_kd9g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700179175, "cdate": 1761700179175, "tmdate": 1762921119135, "mdate": 1762921119135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a noise-adaptive method for solving inverse problems using diffusion models as priors. The main goal of the paper is to develop methods that adapt to the manifold structure of data, hence obtaining better performance for inverse problems under noisy scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses an important problem, that is the noise sensitivity and intractability of general methods used for solving inverse problems via diffusion priors."}, "weaknesses": {"value": "The paper has a few issues that needs to be addressed comprehensively (see questions for details):\n\n- the mathematical justification is thin and derivations are unclear\n- experimental results are missing some baselines proposed to address the same issues, notably noise sensitivity of inverse problem solvers.\n- the method is inherently expensive as DDIM mapping should be autodiffed - the paper avoids this by using a few steps, which is not very principled."}, "questions": {"value": "- Why is the method called noise-space sampling? This is a bit confusing.\n\n- In general, there is a lot of mentions of \"manifold\" but I found this non-rigorous. There's really no geometric insight in any of these comments. For example, the authors mention \"manifold feasibility problem\" as the main motivation of their work, but this is not defined or explained. Is there any theoretical result regarding this?\n\n- Please clarify the equation in line 211. How does first equality work? Since $\\mathcal{D}(x_T) \\approx x_0$, I don't understand why the first equality in line 211 would work.\n\n-  Please provide a remark after Proposition 1 that explains and clarifies the result.\n\n- The paper misses a reference for comparison, which also addresses the noise-robustness issue of standard solvers by adopting a second-order view:\n\n> *Boys, B., Girolami, M., Pidstrigach, J., Reich, S., Mosca, A., & Akyildiz, O. D. Tweedie Moment Projected Diffusions for Inverse Problems. Transactions on Machine Learning Research, 2024.*\n\nPlease add this benchmark to your comparisons in your experiments. \n\n- To see the introduced bias of the method in a simple setting, the paper would benefit from a simple experiment, see Figure 1 of the paper cited above. Please consider adding this.\n\nstyle comment: I do not think using bold text in such frequency is appropriate -- in fact, I think standard academic writing only allows italics for emphasizing - no bolds please."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CPyBQd3Qzs", "forum": "Yfk4ex3Z1G", "replyto": "Yfk4ex3Z1G", "signatures": ["ICLR.cc/2026/Conference/Submission9565/Reviewer_4fZg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9565/Reviewer_4fZg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976067226, "cdate": 1761976067226, "tmdate": 1762921118750, "mdate": 1762921118750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Noise-space Hamiltonian Monte Carlo (N-HMC) sampler, using a pretrained diffusion prior to solve general inverse problems. N-HMC directly samples from $p(x_T|y)$ with a few-step diffusion rollout estimating $\\hat x_0^*$. While reminiscent of DMPlug, it formulates recovery as sampling rather than optimization, naturally accommodating measurement noise. The authors establish theoretical guarantees for N-HMC, showing its robustness to measurement noise under mild assumptions. Based upon N-HMC, NA-NHMC is proposed to adapt to the possibly unknown measurement noise level without hyperparameter tuning. Experiments on natural images show consistent gains over prior methods, with especially strong performance in noisy inverse-problem settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is overall well-written. It categorizes and clearly explains the strengths and weaknesses of existing methods, especially highlighting how and why existing methods are sensitive to measurement noise and rely on extensive hyperparameter tuning.\n- The proposed N-HMC sampler is well justified by Proposition 1 that indicates its robustness to measurement noises.\n- NA-NHMC extends N-HMC to a blind inverse problem setting where the noise level is unknown. NA-NHMC coincides with N-HMC with known noise level under inverse-gamma prior assumption of the noise level, which is demonstrated both in theory and in practice.\n- Experimental results show clear advantage of NA-NHMC over existing diffusion posterior samplers on image restoration tasks, especially with varying measurement noise levels."}, "weaknesses": {"value": "- I found no major weaknesses in this paper, but I believe some justifications are needed. See questions below."}, "questions": {"value": "- The proposed N-HMC performs $p(x_T|y)$ sampling in the noisy space with the help of a few-step sampler that estimates $\\hat x_0$. Similar sampling strategy is discussed in [1], which also samples in the noisy space but follows an noise annealing scheme as ReSample and DAPS. Can the authors comment on the differences between these methods? In particular, what are the pros and cons of sampling $p(x_T|y)$ vs. sampling $p(x_t|y)$ with an annealing noise schedule? Also, it seems an empirical comparison against SITCOM is necessary as it reported better results than DMPlug and DAPS in the considered experimental setups.\n- What is the reason of choosing inverse-gamma prior for $\\sigma_y$? Is it a specific trick to derive proposition 2?\n\n[1] Alkhouri et al. \"SITCOM: Step-wise Triple-Consistent Diffusion Sampling for Inverse Problems\", ICML 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F9fcTOmUTz", "forum": "Yfk4ex3Z1G", "replyto": "Yfk4ex3Z1G", "signatures": ["ICLR.cc/2026/Conference/Submission9565/Reviewer_iaDC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9565/Reviewer_iaDC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981169890, "cdate": 1761981169890, "tmdate": 1762921118449, "mdate": 1762921118449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}