{"id": "863AbI0D7D", "number": 21038, "cdate": 1758313109374, "mdate": 1759896945803, "content": {"title": "CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems", "abstract": "As large language models (LLMs) evolve into autonomous agents capable of collaborative reasoning and task execution, multi-agent LLM systems have emerged as a powerful paradigm for solving complex problems. However, these systems pose new challenges for copyright protection, particularly when sensitive or copyrighted content is inadvertently recalled through inter-agent communication and reasoning. Existing protection techniques primarily focus on detecting content in final outputs, overlooking the richer, more revealing reasoning processes within the agents themselves. In this paper, we introduce CoTGuard, a novel framework for copyright protection that leverages trigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically, we can activate specific CoT segments and monitor intermediate reasoning steps for unauthorized content reproduction by embedding specific trigger queries into agent prompts. This approach enables fine-grained, interpretable detection of copyright violations in collaborative agent scenarios. We evaluate CoTGuard on various benchmarks in extensive experiments and show that it effectively uncovers content leakage with minimal interference to task performance. Our findings suggest that reasoning-level monitoring offers a promising direction for safeguarding intellectual property in LLM-based agent systems.", "tldr": "CoTGuard detects copyright violations in multi-agent LLMs by embedding trigger queries and monitoring intermediate Chain-of-Thought steps, enabling fine-grained, low-interference content leakage detection.", "keywords": ["Multi-agent Systems; Large Language Models; Copyright Protection; Chain-of-Thought"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/567f9fbdfb88f8cc56f68259edaedb9e38a04d42.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CoTGuard, a framework aimed at the intellectual property (IP) protection of intermediate reasoning steps (i.e., Chain-of-Thought, or CoT) in multi-agent LLM systems, which the authors argue are overlooked by existing output-focused methods . CoTGuard is not a prevention tool, but rather a “trigger-based detection” (watermarking) framework. It works by injecting triggers into agent prompts to embed specific, detectable stylistic “artifacts” into the CoT reasoning trace, without impacting the final answer’s correctness . A detection function, based on semantic and structural similarity, is then used to identify these watermarks in external reasoning logs. Experiments demonstrate that this method achieves a high Leakage Detection Rate (LDR) while maintaining task accuracy, and its robustness is also evaluated against adaptive attacks like paraphrasing and prompt rewriting ."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel problem**: The paper introduces a novel and important research problem: the IP protection of the reasoning process (CoT) itself, rather than just the model’s training data or final output. This is a forward-looking concern as agentic systems and their reasoning traces become more valuable.\n- **Sound evaluating framework**: The paper proposes an end-to-end framework, detailing both the watermark injection mechanism (Trigger-CoT Prompt Construction) and the detection algorithm within a multi-agent LLM setup.\n- **Adaptive attack consideration**: A significant strength is the proactive evaluation against adaptive attacks in Section 5.5 (while some details are missing). The authors test two distinct and realistic attack strategies: 1) “Post-Processing Output” (paraphrasing the stolen CoT log) and 2) “Rewriting Prompt” (using an adversarial prompt to suppress the watermark generation). This provides a much more credible assessment of the method’s robustness.\n- **Minimal interference with task performance**: The experiments (Table 1) demonstrate that CoTGuard has a minimal impact on Task Accuracy (TA), performing similarly to the Vanilla baseline and significantly better than the “Output Perturbation” method. This is a critical requirement for any practical watermarking system.\n- **Detailed analysis**: The paper includes a useful ablation study (Section 5.4) that isolates the contributions of the trigger patterns and the task-specific design . Furthermore, the appendix provides concrete examples of injection strategies (Table 6), enhancing the paper’s transparency."}, "weaknesses": {"value": "- **Misleading Framing and Missing Threat Model**: This is the most significant weakness. The term “Copyright Protection” is a misnomer. The framework does not protect (i.e., prevent) leakage; it is a detection and watermarking framework for tracing the unauthorized use of reasoning IP. This framing is confusing and misrepresents the paper’s core contribution. The paper fails to formalize its threat model. It vaguely refers to “unauthorized reuse”  but does not clearly define the attacker’s capabilities (e.g., log-only access vs. query access?), goals, or the precise definition of “protection”. Figure 1 only illustrates the injection process. The core detection mechanism is not visualized. A more effective figure would illustrate the full threat model, showing how the detector assigns.\n- **Critically Missing Evaluation Metrics (False Positives)**: The paper’s evaluation of the detector is insufficient. It relies on “Leakage Detection Rate” (LDR), which appears to be the True Positive Rate (TPR). However, the paper completely fails to report the False Positive Rate (FPR)—the frequency at which a clean, non-watermarked CoT is incorrectly flagged as “stolen.” This omission is especially alarming given the “Vanilla” (unprotected) baseline in Table 2 shows an extremely high LDR (e.g., 57.3% on GSM8K). Does this imply a baseline FPR of 57.3%? If so, the detector is practically unusable. The authors must provide standard metrics like F1-Score or an ROC curve to validate the detector’s performance.\n- **Potential Conflict with User Intent (Task Conflict)**: The framework’s assumption is that the trigger does not “disrupt the core logic or final answer”. This ignores a crucial conflict: what if the injected trigger (e.g., “Explain like a patient teacher”) directly contradicts an explicit user instruction (e.g., “Be concise”)? This would degrade the quality and utility of the CoT, even if the final answer is correct. The paper acknowledges the trade-off between “trigger strength and detectability”  but does not address this direct conflict scenario.\n- **Vulnerability to Prompt Rewriting Attacks**: The paper’s own data (Table 4/8) highlights a significant vulnerability. The “Rewriting Prompt” attack causes a severe drop in LDR (e.g., from 93.5% to 76.2% on FOLIO). This demonstrates that if an attacker suspects the mechanism (stylistic injection), they can effectively neutralize the watermark by using adversarial prompts to suppress its generation.\n- **Missing Experimental Details**: While the two adaptive attacks are considered, current manuscript lacks details of how they are actually performed. The paper conceptually describes the attacks (e.g., “rephrasing or restructuring”)  but provides no implementation details. How was the “post-processing” performed? Was another LLM used for paraphrasing? How were the “rewriting prompts” systematically generated?"}, "questions": {"value": "- **On the Generality of the Method**: The paper consistently emphasizes “multi-agent systems” . Is this setting a necessary condition for CoTGuard? Or could the framework be applied as a general CoT watermark for single LLMs? Is the multi-agent aspect merely a propagation vector for the watermark, or is it fundamental to the method’s design?\n- **On Potential Task Conflict**: Following on from Weakness #3, how does the system behave when an explicit user prompt (e.g., “Provide a one-sentence, direct answer”) is in direct conflict with an injected trigger (e.g., “Explain your reasoning in detail like a teacher”)?\n- **On Evaluation Metrics (FPR)**: Can the authors please clarify the high LDR for the “Vanilla” baseline in Table 2? Does this value represent the system’s False Positive Rate? If not, what is the FPR of the detector?\n- **On Framing**: Given that the method is for detection and tracing, not prevention, would the authors consider reframing the paper? For example, as “A Watermarking Framework for IP Tracing of LLM-Agent Reasoning Trajectories,” which would more accurately reflect the contribution.\n- **On Trigger Scalability**: The method relies on “task-specific” triggers. How much domain expertise and manual effort is required to design a new, effective (i.e., subtle yet robust) trigger for a novel domain like legal or medical reasoning? Can this design process be automated?\nMinors:\n- **Formatting**: Several elements are overwide and break the standard ICLR layout. This includes Figure 1, several tables (Table 1 , Table 2 , Table 3 , Table 4/8 ), and text (e.g., Line 170 ).\n- **Citation Style**: The paper consistently misuses citet formatting in citep contexts. For example, in the introduction, references are formatted as “…GPT-4 Achiam et al. (2023), Genimi Team et al. (2023)…”  when they should be parenthetical, e.g., “…(Achiam et al., 2023; Team et al., 2023)…”. This formatting error occurs throughout the manuscript."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PfSKm3q0eH", "forum": "863AbI0D7D", "replyto": "863AbI0D7D", "signatures": ["ICLR.cc/2026/Conference/Submission21038/Reviewer_sUCZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21038/Reviewer_sUCZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551420796, "cdate": 1761551420796, "tmdate": 1762940617141, "mdate": 1762940617141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method called \"cotguard\", which consists of a protocol for injecting certain stylistic triggers into LLM prompts. these triggers change the style of the subsequent reasoning chains, ideally without changing the final results too much. The authors show that they can detect these stylistic changes given the intermediate steps, which means that their method can be used to uncover \"content leakage with minimal interference\". The authors test this with gpt-4o, gpt-3.5, and a model referred to only as claude 3 (this is ambiguous).\n\nWhile the concept is interesting, the paper has flaws regarding framing, methodology, and clarity."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper calls the method a guard, though it is most easily understood as a watermarking procedure (lines 244, Section 4, Section 3.2 refer to the method as \"trigger-based watermarking\").\n\n- The idea of watermarking intermediate traces is interesting and is part of a large set of new issues around controlling reasoning models.\n\n- Choice of evaluation datasets is reasonable for Chain-of-thought, though not \"full\" reasoning models such as DeepSeek or similar commercial models."}, "weaknesses": {"value": "This paper has issues around framing, methodology, and clarity that I think can only be resolved in a future submission.\n\n## Inaccurate/Overclaiming\n\nW1. The abstract, introduction, and title of this paper suggest that it is a method for copyright protection. Rather, it is a detection method centered around embedding watermarks in chain of thought traces. This is provenance tracking or model fingerprinting, not a protection method.\n\nW2. The paper is initially unclear about which aspects of copyright they consider. Phrases like \"unauthorized content reproduction\" and \"copyright violations in collaborative agent scenarios\", or 62-65 are unclear. Many works related to LLMs and copyright consider the use of copyrighted material in training. However this paper presents a method for provenance tracking, which could later be used as evidence that copyright has been violated by someone using model outputs. However, this leads to the next issue:\n\nW3. **It is not established that model outputs, especially intermediate steps, can be copyrighted at all!** Assuming US law (most of the related work considers copyright in the US jursidiction), it is not established that a copyright can be established over model outputs. Direct human authorship and creativity is needed (see https://www.congress.gov/crs-product/LSB10922 for further references). Model terms of service can forbid distributing outputs, but this is a separate issue from copyright. **In short, copyright is a very specific legal term and is not appropriate for the protections that this work proposes.**\n\nW4. 2.1 describes multiagent systems, and the tasks are referred to as multi-agent throughout the paper. However Appendix 2.1 shows that the implementation is neither standard chain of thought nor an actual multi-agent system. Instead, a single LLM is used to chain responses to \"simulate collaborative reasoning\" (894-896).\n\nIn addition to these issues, there are many technical flaws:\n\n## Technical Flaws\n\nW5. While the introduction cites DeepSeek R1, this model uses only standard CoT prompting (\"Think step by step\", Appendix B). This is in contrast to the typical usage of reasoning models in the field, which mean systems like DeepSeek R1 or the open model S1, which are trained with long intermediate reasoning steps, rather than 0-shot prompting. This is even more concerning when you examine the models used: GPT-3.5-turbo, GPT-4o, and \"Claude 3\". None of those models are reasoning models in the same sense as DeepSeek - the authors should make this clear.\n\nW6. **Claude 3 is not a full model name, there are at least 3 models that can use that name:** https://www.anthropic.com/news/claude-3-family. None of them are reasoning models in the same sense as DeepSeek\n\nW7. Algorithm 1 is proposed to show trigger detection. Lines 293-295 state: \"The system evaluates various factors, including syntax, structure, and semantic alignment, using editing distance, tree comparison, or embedding-based similarity methods.\" However, the authors do not explicity use any of these methods except for embedding similarity. Algorithm 4 describes this process again, stating \"Embedding or edit-based\" for the similarity function. Finally, line 901 states that the actual method is Sentence-BERT, without mention of \"edit-based\" or \"tree comparison\". **Since the trigger detection is described differently in 3 different parts of the paper, including claims of methods that were not implemented, the evaluation seems extremely deceptive.**\n\nW8. 282-283 simply state that a diagram \"could show\" the triggering. This seems more like a editorial note rather than part of the paper, unless there are missing figures.\n> Visualization: A diagram could show how reasoning steps are shared between agents, illustrating the trigger pattern’s propagation across the multi-agent system.\n\nW9. LIne 199 describes \"Theorem 1\", but this simply states that there exists a detection function. **There is no proof given, and lines 210-214 again describe a detection process that is inconsistent with the rest of the paper.** The phrase \"we infer with high confidence\" appears, but there is no further theory or justification for this claim.\n\n**Additionally there are many formatting issues such as citation usage and tables that run into the margins.**"}, "questions": {"value": "- Can you provide a paired example of the GMS-8K outputs, so that we can understand your method? Including the full prompt and reasoning chain for both the \"guarded\" and unguarded approach. \n\n- Which claude model did you use? Do you have any experiments on full reasoning models? They are available using the same APIs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eF4xWsrHVN", "forum": "863AbI0D7D", "replyto": "863AbI0D7D", "signatures": ["ICLR.cc/2026/Conference/Submission21038/Reviewer_GjhA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21038/Reviewer_GjhA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884258176, "cdate": 1761884258176, "tmdate": 1762940616810, "mdate": 1762940616810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Multi-agent LLM systems emerges as a means to solve complex problems. Compared to single-agent LLMs, these systems can introduce additional threats, particularly regarding sensitive or copyrighted content that may reproduce during interactions between multiple agents. This paper identifies the challenge of detecting copyrighted content in multi-agent LLM systems that use Chain-of-Thought (CoT) reasoning, while minimizing utility loss. The authors propose a trigger-based copyright protection framework designed to monitor and detect sensitive or copyrighted content within the intermediate reasoning steps of multi-agent interactions. Specifically, the approach enhances CoT prompts with pre-defined trigger patterns to guide reasoning without affecting the final outputs, so that any trigger-based leakage during intermediate steps can be detected effectively."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies a novel and important challenge: copyright protection in multi-agent LLM systems that employ CoT reasoning.\n\n- The proposed trigger-based mechanism is inspiring, as it guides CoT reasoning using task-specific trigger patterns, so that it is easier to monitor and detect copyrighted content leakage during multi-agent interactions.\n\n- Experimental results show that CoTGuard achieves obviously better defense performance while minimizing degradation in task performance compared to baselines."}, "weaknesses": {"value": "- There lacks en evidence or analysis in this paper to demonstrate that the influence of trigger patterns remain while propagating through multiple agents and CoT reasoning steps.\n\n- Section 4.3 requires more explanation and clarification. For example: How are the “Syntax, Semantics, and Embedding-Based Detection” methods actually evaluated? Which embeddings are used, and how is the embedding-based similarity calculated? How is $\\hat{r}_i$ parsed for candidate trigger patterns?\n\n- The impact of the detection threshold on performance is unclear. A hyperparameter analysis would be helpful.\n\n- The paper does not cite recent related works.\n\n- Comparisons with stronger reasoning models (released in 2025) would strengthen the evaluation.\n\n- There is some inconsistency in the presentation: the paper sometimes suggests that CoT reasoning is part of the proposed framework for copyright protection, while in other places it seems to describe CoT as just the scenario in multi-agent LLM systems. This can be clarified for better readability."}, "questions": {"value": "- How do you ensure that the trigger pattern appears in all agents’ outputs, including at different positions such as reasoning steps, formulations, and summary conclusions?\n\n- How do you ensure that the influence of trigger patterns does not diminish (i.e., watermarking fade) after multiple rounds of CoT reasoning and multiple interactions between agents? Could this make detection harder?\n\n- The traces contain other text alongside the triggers; how is similarity measured? Would longer traces lower the effectiveness of detection against the trigger patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "68XeehO7Cr", "forum": "863AbI0D7D", "replyto": "863AbI0D7D", "signatures": ["ICLR.cc/2026/Conference/Submission21038/Reviewer_Yjng"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21038/Reviewer_Yjng"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983109360, "cdate": 1761983109360, "tmdate": 1762940616524, "mdate": 1762940616524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoTGuard, a novel framework designed to protect copyright in multi-agent LLM systems. The authors point out a limitation in existing copyright protection approaches: most only monitor final model outputs, while overlooking the potential leakage of sensitive or copyrighted information during intermediate reasoning steps.\n\nTo tackle this issue, CoTGuard embeds trigger-based watermarks directly into the agents' chain-of-thought (CoT) reasoning traces. These triggers are subtle linguistic cues inserted into prompts, which then propagate across the multi-agent reasoning chain. Later, by analyzing semantic and structural similarities, the system can detect unauthorized content reuse by matching reasoning traces against known trigger patterns.\n\nThe work makes three main contributions: it frames a new research challenge—copyright protection at the reasoning level in multi-agent LLM systems; introduces a trigger-CoT mechanism that plants watermark-like signals into reasoning paths and employs query-based detection to trace content leakage; and experimentally shows that CoTGuard reaches high detection accuracy with only minimal impact on task performance across several reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper breaks new ground by tackling a largely overlooked issue: copyright leakage within the reasoning processes of multi-agent LLM systems. This is a forward-looking contribution, especially as such systems become more common in real-world applications.\n\n2.The idea of using Chain-of-Thought as a watermarking medium is particularly interesting. It cleverly merges model interpretability with copyright protection into one cohesive framework.\n\n3.Unlike many perturbation-based defense methods, CoTGuard manages to detect leakage effectively without sacrificing task accuracy. This balance between security and performance makes it highly suitable for practical use."}, "weaknesses": {"value": "1.\tThe proposed method lacks strong innovation.   The idea of using trigger-based patterns within Chain-of-Thought reasoning is interesting but not conceptually new or technically groundbreaking.\n\n2.\tThe paper does not provide enough detail about the trigger detection process.   Key aspects such as similarity metrics, detection thresholds, and robustness analysis are missing, making the method hard to reproduce and evaluate.\n\n3.\tAlthough the paper claims to focus on multi-agent LLM systems, the experiments are seemingly conducted on single models without real agent interactions.   The framework’s effectiveness in genuine multi-LLM settings is therefore unproven."}, "questions": {"value": "Could the authors provide more concrete experiments demonstrating real multi-agent interactions and clarify the implementation details of the trigger detection algorithm, including similarity metrics and robustness evaluation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lTn9huxOQr", "forum": "863AbI0D7D", "replyto": "863AbI0D7D", "signatures": ["ICLR.cc/2026/Conference/Submission21038/Reviewer_5NxD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21038/Reviewer_5NxD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986789190, "cdate": 1761986789190, "tmdate": 1762940616148, "mdate": 1762940616148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}