{"id": "Rs8qiI2GDb", "number": 3410, "cdate": 1757419760217, "mdate": 1763006312491, "content": {"title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models", "abstract": "The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel **Granular-GRPO** ($\\text{G}^2$RPO) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage-Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-domain evaluations, demonstrate that our $\\text{G}^2$RPO significantly outperforms existing flow-based GRPO baselines, highlighting its effectiveness and robustness.", "tldr": "Granular-GRPO, a novel flow-based GRPO framework for precise and comprehensive reward assessments.", "keywords": ["Text-to-Image", "GRPO", "Reinforcement Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/206e54b9d0f64a04a294cb87d968f81cc04ac862.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes **G²RPO (Granular-GRPO)**, an online reinforcement learning framework designed to improve reward precision and alignment fidelity for flow-based generative models.\nThe authors identify two key weaknesses in existing Flow-GRPO and Dance-GRPO methods:\n1. **Sparse rewards** resulting from uniformly distributed credit assignment across SDE steps; and\n2. **Single-granularity evaluation**, where each sampling direction is tied to a fixed number of denoising steps.\n\n####\nTo address these, the authors introduce two major innovations:\n**Singular Stochastic Sampling :** \nconfines stochasticity to a single timestep, establishing a strong correlation between injected noise and its reward signal.\n**Multi-Granularity Advantage Integration (MGAI):** \naggregates advantage estimates from multiple denoising granularities to obtain a comprehensive evaluation of sampling directions.\nExperiments on the Flux.1-dev flow model demonstrate consistent gains across multiple human preference reward models (HPS-v2.1, CLIP Score, PickScore, ImageReward, Unified Reward)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Innovative Reward Structuring:** The combination of Singular Stochastic Sampling and Multi-Granularity Integration effectively addresses reward sparsity and limited evaluation coverage, both being crucial bottlenecks in RL-based alignment of flow models.\n\n**Methodologically Coherent:** The algorithm is well-motivated, mathematically consistent (Eqs. 8–12), and the training process (Algorithm 1) is clearly documented.\n\n**Empirically Strong:** Extensive quantitative and qualitative results (Tabs. 1–2, Figs. 4–6) demonstrate superior alignment with human preferences and improved visual fidelity.\n\n**Reproducibility:** The paper provides full implementation details, hyperparameters, and training setup, aligning with ICLR reproducibility guidelines.\n\n**Clarity and Presentation:** The manuscript is well-written, structured, and easy to follow, with clear figure illustrations and balanced technical depth."}, "weaknesses": {"value": "### (a) Method generality not fully validated.\nThe chosen granularity set Λ = {1,2,3} appears taskspecific, without analysis of its optimality or adaptability across different architectures.\n\n**Suggestion:** Include cross-task comparisons with varied granularity configurations (e.g., Λ ={1,4}, Λ = {2,3,4}) to clarify the general applicability and sensitivity of Λ.\n\n### (b) Incomplete methodological justification.\nThe choice of candidate SDE step set M = {16,15,...,9} lacks motivation—why are later timesteps excluded? Similarly, MGAI’s advantage fusion (Eq. 11) adopts an unweighted summation without comparing against alternative strategies (e.g., weighted averaging or attention-based fusion).\n\n**Suggestion:** Provide ablations for different SDE step ranges (e.g., early vs. middle timesteps) and include comparisons among fusion mechanisms to validate the rationale for the current design.\n\n### (c) Missing efficiency and sensitivity analysis.\nWhile the paper states that G²RPO converges faster (“one-fourth of iterations”), it omits quantitative comparisons on computational cost (training time, inference latency)； The sensitivity of the stochastic noise parameter η= 0.7 is also unexplored.\n\n**Suggestion:** Add explicit measurements of training time per iteration and per-sample generation latency under identical GPU settings, and conduct sensitivity analysis for different η values (e.g.,0.5, 0.7, 0.9)."}, "questions": {"value": "a. How sensitive is performance to the choice of the SDE step subset M? Would using multiple stochastic steps harm or help training stability?\n\nb. What is the computational overhead (in time or memory) introduced by the MGAI module?\n\nc. Could G²RPO be extended to diffusionbased models or text-to-video generation with minor modifications?\n\nd. How does the method perform under real human preference models rather than automatic reward surrogates?\n\ne. Why is there no direct, controlled comparison between G²RPO and Flow-GRPO under identical experimental conditions (e.g., the same HPSv2 dataset, Flux.1-dev backbone, and reward models)? Such an evaluation would more clearly highlight G²RPO’s improvements over its most relevant baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JJR9nIQped", "forum": "Rs8qiI2GDb", "replyto": "Rs8qiI2GDb", "signatures": ["ICLR.cc/2026/Conference/Submission3410/Reviewer_6ymD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3410/Reviewer_6ymD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546758278, "cdate": 1761546758278, "tmdate": 1762916711203, "mdate": 1762916711203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "After careful consideration, we have decided to revise the manuscript entirely. I sincerely thank the reviewers and the AC PC for their efforts."}}, "id": "2TArmAski1", "forum": "Rs8qiI2GDb", "replyto": "Rs8qiI2GDb", "signatures": ["ICLR.cc/2026/Conference/Submission3410/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3410/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763006311374, "cdate": 1763006311374, "tmdate": 1763006311374, "mdate": 1763006311374, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors introduce $G^2$RPO, a method that attributes the reward of generated samples to a single stochastic step made during the generation. Moving away from the original Flow-GRPO method which turns the whole flow ODE into a SDE, $G^2$RPO keeps ODE sampling everywhere but a single step which allows the policy to pinpoint exactly where the stochasticity comes from, allowing for more fine-grained \"credit assignment\". Moreover, after the SDE step, they denoise the groups with different levels of denoising granularities which gives further signal to the policy. \n\nQuantitatively, their approach yields competitive in-domain and out-of-domain scores against the baselines. Qualitatively, their samples adhere very well to the prompt in addition to being visually more appealing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is sound and makes sense and the results strongly motivate the approach."}, "weaknesses": {"value": "- See my question about Eq. 9 down below but theoretical justification for it is needed."}, "questions": {"value": "- I am not very familiar with the ODE to SDE formulation but given that you intertwine the two during generation, does that still guarantee that you're learning and sampling from the desired data distribution? It would be good to clarify that in the paper.\n- I might be missing something about the justification for Eq. 9 but why is the sum only conducted on the subset of the SDE timesteps? Disregarding the MULTI-GRANULARITY ADVANTAGE INTEGRATION, my understanding is that you go through all SDE timesteps in $k\\in M$ and for each $k$ you unroll your trajectories using ODE sampling until you get to the stochastic step $k$ after which you sample $G$ trajectories that get completed with ODE again. So you end up with $G\\times M$ trajectories, in that case in Eq. 9, there should be another sum over all the timesteps as well.\n- typo in line 14 in Algorithm 1, I think it should be $t<k$.\n- MGAI is ablated but I would also like to see the effect of $M$ on the performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "TxlIGFQJVP", "forum": "Rs8qiI2GDb", "replyto": "Rs8qiI2GDb", "signatures": ["ICLR.cc/2026/Conference/Submission3410/Reviewer_gZPr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3410/Reviewer_gZPr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937055334, "cdate": 1761937055334, "tmdate": 1762916710969, "mdate": 1762916710969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Granular-GRPO, a framework to fine-tune flow models with reinforcement learning. It introduces two techniques to improve training: 1. it introduces one stochastic sampling step to the flow model inference; 2. it chooses different discretization step sizes when collecting data. It conducts empirical studies to demonstrate the performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. the paper is overall well-written with illustrative figures;\n2. the experiments demonstrate both in-domain and out-of-domain improvement"}, "weaknesses": {"value": "1. the singular stochastic sampling strategy (introducing a single stochastic sampling step) seems to be a special case of the MixGRPO in Li et al. 2025. They insert a window of stochastic sampling steps when collecting data. When the window size decreases to one, MixGRPO reduces to the singular stochastic sampling strategy.\n2. it is not clear why the multi-granularity advantage integration is beneficial. Intuitively, the training and testing discretization steps should be consistent to ensure better performance.\n3. The numbers reported in Table 1 only shows marginal improvement compared to baseline methods."}, "questions": {"value": "Could the author make a detailed comparison to MixGRPO and illustrate why the multi-granularity advantage integration may help?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HTv3Ih71Wi", "forum": "Rs8qiI2GDb", "replyto": "Rs8qiI2GDb", "signatures": ["ICLR.cc/2026/Conference/Submission3410/Reviewer_UsSM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3410/Reviewer_UsSM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948782047, "cdate": 1761948782047, "tmdate": 1762916710769, "mdate": 1762916710769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper looks at the problem of text to image generation and uses a GRPO variant to fine-tune a large model. The authors discuss a couple of major issues faced by prior works that use RL to fine-tune these models and propose solutions for the same. First is that methods employing SDE to generate the flow at every time step suffer from proper credit assignment. Cumulative errors along the trajectory can lead to large number of poor samples that can affect GRPO training and it is difficult to attribute the reward to the correct step in the generation process. The authors propose a mixed approach with SDE only at one step and ODE on the other steps to correctly evaluate the different generation directions. Secondly, to improve the granularity in denoised images, they propose using a multi-granular advantage integration i.e. generating for different granularities. They evaluate their method against a couple of baselines on a text-to-image dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The issue of proper credit assignment is well identified. Performing SDE at every timestep in the denoising process can lead to most images being poor with poor rewards. \n\n(2) Moreover the assignment of the credit now goes to the correct step. Assigning the same reward to all steps made it unclear which step in the denoising led to the incorrect direction and which step led to the correct one."}, "weaknesses": {"value": "(1) The discussion on granularity is unclear. Why is it needed? Are other issues being injected into the denoising process by jumping over time steps?\n\n(2) Mix-GRPO is mentioned in the related work as a method that also mixes SDE-ODE. How is the proposed method different from Mix-GRPO?\n\n(3) Experiments are limited. There is one dataset with one base model and two baselines. Is the base model finetuned over a dataset (through SFT) or is it directly the pretrained model?\n\n(4) In the results, the improvement over the baselines seems to be small. If the baselines suffer from improper credit assignment, the results should indicate that. Are the improvements statistically independent?\n\n(5) Flow-GRPO is not tested against."}, "questions": {"value": "(1) What do you mean by in-domain and out-of-domain? These are different metrics and not the text distributions? Can a different metric really be called an “out-of-domain” metric?\n\n(2) What do DanceGRPO and MixGRPO do and how is your method different from them?\n\n(3) In related work, why is multi-turn RL mentioned? There is no discussion about multi-turn anywhere else in the paper.\n\n(4) You mention that PPO is sensitive to reward model inaccuracies, since GRPO is also using the same reward, why isn't it sensitive to the same?\n\n(5) Performing only ODE converges to the sample from the desired distribution. So does sampling using SDE. Does mixing the two still lead to a sample from the desired distribution or does the mixing break the theoretical guarantees?\n\n(6) In the algorithm, what about $t<k$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O36aVAkTr2", "forum": "Rs8qiI2GDb", "replyto": "Rs8qiI2GDb", "signatures": ["ICLR.cc/2026/Conference/Submission3410/Reviewer_xvs4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3410/Reviewer_xvs4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978465496, "cdate": 1761978465496, "tmdate": 1762916710550, "mdate": 1762916710550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}