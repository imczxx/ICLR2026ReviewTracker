{"id": "BV3bLKaJ3Y", "number": 22681, "cdate": 1758334406661, "mdate": 1759896852695, "content": {"title": "Post-training Large Language Models for Diverse High-Quality Responses", "abstract": "Reinforcement learning has emerged as a popular method for post-training large language models (LLMs). While improving the model's performance on downstream tasks, it often reduces the model's output diversity, leading to narrow, canonical responses. Existing methods to enhance diversity are limited, either by operating at inference time or by focusing on lexical differences. We propose a novel training method named DQO (Diversity Quality Optimization) based on determinantal point processes (DPPs) to jointly optimize LLMs for quality and semantic diversity. Our approach samples and embeds a group of responses for each prompt, then uses the determinant of a kernel-based similarity matrix to measure diversity as the volume spanned by the embeddings of these responses. Experiments across instruction-following, summarization, story generation, and reasoning tasks demonstrate that our method substantially improves semantic diversity without sacrificing model quality.", "tldr": "", "keywords": ["Large language models", "Diversity", "Reinforcement learning", "Post-training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5a78f5fb861b7c1da7a6e875420ac7ec5dd7a5c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DQO, a principled framework for post-training LLMs to optimize both diversity and quality of responses. Key contributions include: a flexible method compatible with existing RL algorithms like PPO and GRPO; a DPP-based formulation that ensures semantic diversity beyond lexical variation; and experimental demonstration of an effective quality-diversity trade-off across multiple tasks while maintaining utility and coherence."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a post-training reinforcement learning method from the perspective of diversity in content generated by LLMs, which simultaneously ensures generation quality and diverse semantic capabilities. Its research perspective and solution approach are innovative.\n\n2. The paper presents a clear dual objective function for optimizing LLMs to ensure both quality and diversity, while also addressing two challenging issues in optimizing this function and providing solutions. The overall methodological approach is well-structured.\n\n3. The paper covers a comprehensive range of experimental perspectives, including performance in two dimensions of generation, the impact of increasing diversity on performance, and performance variations under different task settings, conducting thorough experiments."}, "weaknesses": {"value": "1. The paper explains in the Introduction how existing reinforcement learning methods improve the diversity of generation at the lexical level but lacks an analysis comparing them with methods like instruction tuning. Instruction tuning uses the same training data template to enable LLMs to learn different tasks, naturally enhancing the model's ability to generate diverse downstream tasks or stylistic language. Since this paper primarily discusses the issue of generation diversity, although it focuses mainly on reinforcement learning, I believe it is still necessary to explore the necessity or fundamental differences compared to methods like instruction tuning.\n\n2. The paper proposes a training method based on determinantal point processes (DPPs) to enhance the generation quality and diversity of LLMs but lacks motivation for adopting DPPs. DPPs have been used in recommendation systems to simultaneously ensure the relevance and diversity of recommended results. Although the authors provide a definition of DPPs in Section 2.3, I still believe the authors need to more clearly explain the principle of how DPPs can improve both aspects simultaneously and the motivation for using them to train LLMs in the Introduction.\n\n3. Some of the figures in the paper lack necessary data labels, dimension identifiers, and summary titles (e.g., Figure 3), requiring readers to read extensive text to understand the content of the figures. Some tables (e.g., Table 1 and Table 2) also lack necessary highlights to allow readers to quickly draw performance conclusions.\n\n4. The paper draws conclusions by comparing the DQO method with baseline methods on general language model tasks, showing that DQO can simultaneously ensure performance in both quality and diversity. However, it lacks a detailed analysis of the differences between the methods. For instance, why do GRPO-likelihood (on the GSM8K task) and GRPO-entropy (on the Dolly task) still perform better in diversity than DQO? Since DPO incorporates considerations for diversity compared to the baseline, why is the improvement in diversity performance not significant for DPO, while there is an improvement in generation quality?"}, "questions": {"value": "See questions in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5w7v1AW5dL", "forum": "BV3bLKaJ3Y", "replyto": "BV3bLKaJ3Y", "signatures": ["ICLR.cc/2026/Conference/Submission22681/Reviewer_RQPB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22681/Reviewer_RQPB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761290724989, "cdate": 1761290724989, "tmdate": 1762942333536, "mdate": 1762942333536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses output diversity degradation in large language models (LLMs) following post-training with reinforcement learning methods like PPO and GRPO. The authors propose Diversity Quality Optimization (DQO), a novel training framework that jointly optimizes for response quality and semantic diversity. DQO introduces a Determinantal Point Process (DPP)-based diversity objective in the RL training loss. For each prompt, the method samples a set of responses, maps them into a semantic embedding space using a pretrained encoder, and computes a kernel-based similarity matrix. The diversity score corresponds to the determinant of this matrix. To address computational challenges, the authors introduce two key modifications: (i) adding an identity matrix for regularization, and (ii) employing leave-one-out (LOO) gradient estimators to reduce variance. The authors evaluate DQO across diverse tasks, including instruction-following, summarization, story writing and reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important and timely problem in LLM post-training, where training with reinforcement learning often reduces the diversity of generated output.\n- The proposed method can be easily integrated with existing RL algorithms, showing potential for broad applicability in post-training methods.\n- The experimental evaluation covers multiple task categories and shows consistent improvements in diversity metrics while maintaining comparable quality."}, "weaknesses": {"value": "While the paper is clearly written and the method is well-justified, it could be further strengthened through additional experimental analyses.\n\n- As acknowledged by the authors, the proposed approach relies heavily on the quality of the pretrained embedding model used to compute semantic similarity. Evaluating DQO with different embedding encoders would strengthen the empirical analysis and clarify its generalizability. How does performance vary with different embedding models (e.g., larger models, task-specific embeddings)? Are certain embedding models more suitable for particular task types?\n- While the LOO estimator is introduced to mitigate variance, the paper lacks ablation study to isolate its effect on performance or stability.\n- Experiments focus only 2 models (Qwen2.5-MATH-1.5B for reasoning, Llama3.2-1B for other tasks). No exploration of different model families. It's unclear whether findings generalize to larger models. How does model capacity affect the quality-diversity trade-off?\n- The paper fixes the reward model (Skywork-Reward-V2) across all tasks. How sensitive is DQO to reward model quality or choice?"}, "questions": {"value": "- How is the quality score evaluated for generation tasks ssuch as Dolly, summarization and story-writing?\n- What is the k value in the main experiment (Table 1)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xq2F6lEny5", "forum": "BV3bLKaJ3Y", "replyto": "BV3bLKaJ3Y", "signatures": ["ICLR.cc/2026/Conference/Submission22681/Reviewer_BeWa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22681/Reviewer_BeWa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959094874, "cdate": 1761959094874, "tmdate": 1762942333243, "mdate": 1762942333243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors have proposed proposes DQO (diversity quality optimization), a training method that uses DPPs to encourage llms to generate semantically diverse responses while maintaining quality. They do this by defining diversity as the determinant of a kernel matrix constructed from response embeddings. This diversity measure can be incorporated into standard RL objectives like PPO or GRPO as a simple regularizer term in the loss function."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written with good intuitive explanations.\n2. This paper addresses an important problem in RL-finetuning research, i.e. diversity collapse. If the limitations (see below) were addressed, this could be useful for the community."}, "weaknesses": {"value": "1. I strongly feel that the reward hacking failure with outcome rewards (in appendix E) should be on the main submission. This isn't a minor edge case. Such outcome rewards are standard for mathematical reasoning or coding tasks. Given that the attempted fix using prompt-response similarity also failed, this issue severely limits the method's applicability and should be acknowledged in the main experiments.\n2. Claims about \"semantic diversity\" are never validated with human judges, and even for LLM-as-judge there's only one model used. I feel that the authors should take a subset of generations and atleast show some correlation between human responses and their LLM-as-judge.\n3. Figure 4 shows pass@n curves but the differences are often small and lack error bars. Also (minor), please increase the text size for fig 4 and 5. \n4. What happens when you don't set β=α? The derivation in Eq. 5 requires this, but it's a strong constraint. This constraint means you can't independently tune:\na. How much you penalize deviation from the reference policy (β)\nb. How much you reward diversity (α)\nIf β controls exploration vs. exploitation in the standard RL sense, and α controls quality vs. diversity tradeoff, being forced to set them equal is quite restrictive. In practice, you might want β=0.01 for conservative updates but α=1.0 for strong diversity. So why was such a constraint added?"}, "questions": {"value": "1. What happens with 7B or 13B models? Does the diversity-quality tradeoff change? \n2. How expensive is computing k embeddings and determinants per update?\n3. Can you run a human study (even small-scale) where annotators judge semantic diversity? This is essential for validating the core claim of this paper.\n4. How sensitive is DQO to the underlying embedding model?\n5. Does improved diversity during training lead to better out-of-distribution generalization or just different in-distribution behavior? Maybe choose an OOD summary task for this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cNQT9D1DLV", "forum": "BV3bLKaJ3Y", "replyto": "BV3bLKaJ3Y", "signatures": ["ICLR.cc/2026/Conference/Submission22681/Reviewer_Fy8p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22681/Reviewer_Fy8p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972361265, "cdate": 1761972361265, "tmdate": 1762942333062, "mdate": 1762942333062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a post-training method called DQO (Diversity Quality Optimization) that is designed to jointly optimize LLMs for quality and semantic diversity. Using Determinantal Point Processes, the authors gather the embeddings of multiple responses from each prompt and measure diversity as the volume spanned by these embeddings by taking the determinant of their pairwise similarity matrix. This diversity measure is added to the objective function for RL post-training. DQO maintains a higher quality-diversity Pareto frontier, matches the baseline for pass@1 and scales better than the baseline for pass@n >1."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Well written \n\n- Clear presentation \n\n- Very good motivation and an effective solution to sustaining diversity \n\n- The determinantal point process is a explained clearly and can have a lot of potential in other pipelines that use frozen embeddings"}, "weaknesses": {"value": "## Minor \n\n- The intro reads more like related works. Could be shaped to flow better. \n\n- Incredibly small fonts in the axis labels\n\n- Missing citation semantic entropy [1]\n\n## Major \n\n- **Matrix via similarity Kernel Bottleneck**: There is a k by k matrix to find the determinant of. Is it cubic complexity? Is there a way to mitigate that? How much more is the overhead? A computational cost comparison is pertinent.\n\n- **Reward Model reliance**: The paper is introduced as a general post-training solution but it still requires a good frozen embedding model and a good reward model. Can this method even be reasonably applied to the vast space of situations where those are not available? \n\n- **Embedding Model**: It is necessary to see that the results hold for different embeddings models and embedding approaches. For example something like the Matryoshka representations [2] would be really helpful in supporting the generalizability of this method.\n\n\n[1] Kuhn, Lorenz, Yarin Gal, and Sebastian Farquhar. \"Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.\" arXiv preprint arXiv:2302.09664 (2023).\n\n[2] Kusupati, Aditya, et al. \"Matryoshka representation learning.\" Advances in Neural Information Processing Systems 35 (2022): 30233-30249."}, "questions": {"value": "Q1. Can you explain how equation 4 and 5 follow from 3?\n\nQ2. Can you get more results with different embedding or reward models? (embeddings models most importantly).\n\nQ3. Can you provide a description and analysis of the computational cost of the DPPs"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d5gLdrUW56", "forum": "BV3bLKaJ3Y", "replyto": "BV3bLKaJ3Y", "signatures": ["ICLR.cc/2026/Conference/Submission22681/Reviewer_J8Sx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22681/Reviewer_J8Sx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992963975, "cdate": 1761992963975, "tmdate": 1762942332875, "mdate": 1762942332875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}