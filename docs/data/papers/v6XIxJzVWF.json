{"id": "v6XIxJzVWF", "number": 7076, "cdate": 1758006784696, "mdate": 1759897873920, "content": {"title": "ParaShield: Parameter-Level Directional Defense for Federated Backdoor Robustness", "abstract": "Heterogeneous federated learning improves the stealthiness of backdoor attacks, presenting substantial challenges for existing defense methods to simultaneously ensure effectiveness and robustness. However, divergent optimization objectives lead to pronounced parameter-level differences between the benign heterogeneous clients and those infected with backdoor attacks. To address this issue, we introduce Parameter-level Directional Defense, termed ParaShield, which leverages Neural Influence Factors (NIF) to dynamically and rapidly capture the critical parameters. ParaShield enables the identification of parameters that are essential for maintaining model performance within the benign client updates. On this basis, we further calculate the Cosine Similarity of Critical Parameters (CPCS) and\nthe Sign Consistency of Critical Parameters (CPSC) to quantify directional alignment across client updates. Specifically, we initially filter out malicious model updates by analyzing the directional information of the critical parameters. Subsequently, we leverage the Mahalanobis distance in the 2D feature space formed by CPCS and CPSC to identify malicious updates deviating from the normal distribution, achieving robust aggregation. To comprehensively evaluate the robustness of ParaShield, we also construct the Projected Directional Backdoor Attack (PDBA), a stealthy backdoor attack that effectively examines defense mechanisms under realistic conditions. Extensive experiments conducted on various challenging Non-IID scenarios demonstrate the effectiveness of ParaShield.", "tldr": "", "keywords": ["Federated learning", "backdoor", "parameter directional"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5633e8577238052548bf6f447b16e948150aa9d6.pdf", "supplementary_material": "/attachment/022474de2bab57c856e817bd4db095fe4bd829d7.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ParaShield, a parameter-level directional defense method for heterogeneous federated learning. It identifies critical parameters through a Neural Influence Factor (NIF), filters malicious updates using cosine and sign consistency measures (CPCS/CPSC), and employs Adaptive Weighted Aggregation (AWA) with whitening and Mahalanobis distance for robust model aggregation. The authors also propose a stealthy Projected Directional Backdoor Attack (PDBA) to evaluate defense strength. Experiments on CIFAR-10/100 show ParaShield achieves the lowest ASR and highest robustness among tested defenses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-motivated framework**. The paper addresses heterogeneity-induced difficulties in distinguishing benign and backdoored updates.\n- **Comprehensive evaluation**. The paper includes multiple backdoor types, Non-IID settings, and ablation studies.\n- **New attack**. The paper adds value for testing robustness under realistic threat models."}, "weaknesses": {"value": "- No comparison with clustering- or influence-based defenses such as DeepSight [1] or FLAME [2], which are relevant for aggregation-level defenses.\n- Limited methodological novelty: The main difference from AlignIns is the adaptive weighting (AWA). The large performance gain lacks clear interpretability or theoretical justification.\n- Lack of interpretability: The mechanism behind the improvement from CPCS/CPSC + whitening is not analyzed or visualized.\n- No runtime/memory analysis: The computational cost of each module (CPE, CPF, AWA) is not reported.\n- Scalability not demonstrated: Only small CNN backbones and simple FL setups are tested. No evidence on larger models (e.g., ViT) or real-world-scale scenarios.\n- Source code not released, limiting reproducibility and community validation.\n\n[1] Rieger, Phillip, et al. \"DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection.\"\\\n[2] Nguyen, Thien Duc, et al. \"{FLAME}: Taming backdoors in federated learning.\" 31st USENIX Security Symposium (USENIX Security 22). 2022."}, "questions": {"value": "- Could you provide comparisons with clustering- or influence-based defenses such as DeepSight or FLAME, to better position ParaShield among aggregation-level defense methods?\n- Could you explain why the adaptive weighting (AWA) module leads to such a large performance gain over AlignIns, given the architectural similarity between the two frameworks?\n- Could you provide runtime and memory analyses for each module (CPE, CPF, AWA) to better understand the computational overhead of ParaShield?\n- Do you plan to release the source code, or could you share implementation details to ensure reproducibility and independent validation of your results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I3yyUB3eLj", "forum": "v6XIxJzVWF", "replyto": "v6XIxJzVWF", "signatures": ["ICLR.cc/2026/Conference/Submission7076/Reviewer_Vyao"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7076/Reviewer_Vyao"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760686286824, "cdate": 1760686286824, "tmdate": 1762919261564, "mdate": 1762919261564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ParaShield, a parameter-level directional defense framework for robust federated learning against backdoor attacks. It dynamically identifies critical parameters via Neural Influence Factor (NIF), measures directional alignment among clients using Cosine Similarity (CPCS) and Sign Consistency (CPSC), and performs Adaptive Weighted Aggregation (AWA) based on Mahalanobis distance to filter and downweight malicious updates. Additionally, a stealthy Projected Directional Backdoor Attack (PDBA) is designed to evaluate defense robustness. Experiments show that ParaShield detects and neutralizes backdoors under heterogeneous settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Targeted method: The combination of CPCS, CPSC, and Mahalanobis distance allows dynamic and stable defense across heterogeneous settings.\n2. Comprehensive evaluation: The introduction of PDBA provides a realistic and stealthy benchmark to test defense effectiveness rigorously."}, "weaknesses": {"value": "1. From a parameter-wise perspective, enhancing the robustness of federated learning has already been explored — for example, by existing methods such as FDCR. Therefore, the authors need to further clarify the innovation of their approach compared to these similar methods.\n\n2. Computational overhead: Calculating NIF, CPCS, CPSC, and Mahalanobis distance for each client increases server-side computation and communication costs. Complexity analysis is necessary.\n\n3. Experiments on larger client scales should be included to validate the scalability."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ENYgRKxCD8", "forum": "v6XIxJzVWF", "replyto": "v6XIxJzVWF", "signatures": ["ICLR.cc/2026/Conference/Submission7076/Reviewer_WaCs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7076/Reviewer_WaCs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761417292114, "cdate": 1761417292114, "tmdate": 1762919261142, "mdate": 1762919261142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a defense mechanism, ParaShield, against backdoor attacks in a heterogeneous federated learning setting. Due to the heterogeneity of data distribution among clients, the parameter updates from clients are different, resulting in the difficulty to identify parameter anomalies of malicious updates. ParaShield dynamically identifies critical parameters using NIF and evaluates updates based on CPCS and CPSC of these parameters. ParaShield then applies an AWA strategy that uses whitening transformation and Mahalanobis distance in a 2D feature space to detect and downweight malicious updates. To rigorously test its robustness, the authors also propose PDBA, a stealthy backdoor attack. Extensive experiments on CIFAR-10 and CIFAR-100 under Non-IID settings show that ParaShield outperforms existing SOTA defenses by effectively mitigating backdoor threats while maintaining high accuracy on benign tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Experimentally demonstrates the critical parameters exhibit significantly different values in benign heterogeneous updates compared to backdoor updates, and demonstrate that NIF cosine similarity among malicious clients shows higher values than benign counterparts.\n\nThe work specifically focuses on federated learning under non-IID data conditions, which is a major practical challenge, and where many existing defenses fail.\n\nInstead of treating the entire model update as a monolithic block, ParaShield innovatively operates at the parameter level.\n\nThe paper includes a clear ablation study that validates the contribution of each core module."}, "weaknesses": {"value": "The paper's focus on \"heterogeneous federated learning\" requires clarification. The term is used interchangeably with non-IID data distributions, yet \"non-IID\" is the more precise and established term in the field. Adopting this specific terminology would improve conceptual clarity and better align the work with standard literature.\n\nThe paper insufficiently elaborates on essential concepts. For instance, the authors claim their PDBA attack achieves \"dual stealth\" (Lines 94-95) but fail to explicitly specify the two dimensions in which stealth is achieved, leaving a critical aspect of their contribution unclear.\n\nParaShield identifies \"critical parameters\" via NIF, yet the paper insufficiently argues for NIF's universality as a metric to identify critical parameters. This definition's sensitivity to model architecture, datasets, and data distribution presents a potential vulnerability: an attacker who understands and circumvents the NIF mechanism (e.g., by implanting backdoors into parameters NIF deems non-critical) could render the defense framework ineffective.\n\nThe paper's experimental validation is limited to two datasets (CIFAR-10/100) and a single model family (ResNet-9/18). Furthermore, most baseline attacks and defenses are outdated, with only two recent exceptions published during the past 3 years. Broader evaluation on larger-scale datasets (e.g., ImageNet, CelebA), diverse model architectures (e.g., VGG, ViT and other ResNet models), and sota comparative methods would substantiate the claims more convincingly.\n\nThe paper's evaluation of Non-IID scenarios may not adequately address extreme heterogeneity. In such cases, the inherent divergence among benign client updates could potentially mask the subtle directional anomalies introduced by backdoor attacks. The defense's effectiveness under these most challenging real-world conditions remains unverified.\n\nThe paper ignores the additional computational and communication costs introduced by ParaShield. For large-scale models and a large number of clients, the fine-grained, parameter-level analysis could introduce significant overhead, impacting the efficiency of federated learning, but this is not quantified or discussed in the paper."}, "questions": {"value": "How is \"heterogeneous setting\" precisely defined in this work? Is the heterogeneity specifically in data distribution (i.e., non-IID data), data modality, or other factors? Is this terminology and its specific definition formally established in prior literature?\n\nGiven ParaShield's core reliance on identifying critical parameters, what evidence or theoretical insight demonstrates its generalizability and effectiveness across diverse model architectures, datasets, and varying data distributions?\n\nBeyond the presented experiments, is ParaShield effective against a wider range of modern model architectures (e.g., Vision Transformers), larger-scale datasets (e.g., ImageNet), and does it outperform a broader suite of state-of-the-art attacks and defenses?\n\nThe paper tests down to a Dirichlet concentration parameter of \\alpha=0.1. Would ParaShield remain effective under more extreme non-IID conditions (e.g., \\alpha << 0.1), where high benign client divergence could mask backdoor signals?\n\nWhat is the computational and communication overhead introduced by ParaShield's components (NIF calculation, CPF, AWA), and how does this scale with model size and the number of clients?\n\nThis work involves several key hyperparameters (e.g., CPE ratio \\rho, AWA weight \\beta, etc.). How sensitive is the performance to these settings across different experimental conditions? Are there guidelines for efficient tuning in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "V1j6t3qg32", "forum": "v6XIxJzVWF", "replyto": "v6XIxJzVWF", "signatures": ["ICLR.cc/2026/Conference/Submission7076/Reviewer_t6jY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7076/Reviewer_t6jY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603873323, "cdate": 1761603873323, "tmdate": 1762919260784, "mdate": 1762919260784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new server-side backdoor defense method for federated learning, called ParaShield. ParaShield first identifies critical parameters by jointly considering their absolute update magnitudes and min–max normalized scores. Based on these identified parameters, it computes two directional metrics, cosine similarity and majority sign alignment, to detect anomalous updates, which are then filtered out before aggregation. To further stabilize model aggregation under heterogeneous data, ParaShield employs an adaptive weighted aggregation that adjusts the contribution of each client update. The authors have conducted reasonable experiments to empirically evaluate the performance of ParaShield."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work addresses the problem of backdoor defense in federated learning, which is a timely and important research topic.\n\n2. The paper is well-written and easy to follow.\n\n3. The proposed method, ParaShield, is technically sound, and its effectiveness is demonstrated through reasonable empirical evaluations. \n\n4. The proposed neural influence factor is interesting, and results (e.g., in Figure 1) do show that it successfully captures the difference between malicious updates and benign updates.\n\n5. Compared with existing approaches, it consistently achieves higher main task accuracy and robust accuracy, while maintaining a lower attack success rate."}, "weaknesses": {"value": "1. The proposed defense method is evaluated solely through empirical experiments; a theoretical analysis of ParaShield’s robustness would be valuable, if feasible.\n\n2. The experimental evaluation is limited to a small-scale and single-modality dataset. Extending the experiments to larger-scale datasets and other modalities (e.g., text) could substantially strengthen the evaluation section.\n\n3. The ablation study is conducted only on a single dataset and under a single attack scenario. Incorporating additional datasets and attack settings would provide a more comprehensive understanding of the contribution of each proposed component.\n\n4. Lack of detailed defense and attack models."}, "questions": {"value": "1. What is the specified value of the important parameter $\\tau$? It is not clearly stated in the experimental settings section. An ablation study on the choice of $\\tau$ is also missing.\n\n2. The paper shows that the critical parameter extraction step can successfully identify malicious parameters, which consistently exhibit smaller values. In that case, why not adopt a pruning-based approach to directly remove these malicious parameters?\n\n3. What is the time complexity of the proposed method compared to state-of-the-art approaches?\n\n4. Is the critical parameter–based filtering process entirely borrowed from AlignIns? To the best of my knowledge, it appears quite similar to the approach used in AlignIns. If so, a clear acknowledgment and citation should be provided, for example, near line 183.\n\n5. Since the attack model is missed, I suppose the potential attacker can poison an arbitrary number of clients and turn them into malicious clients. What is the performance of  ParaShield under the cases where more than 50% of clients are poisoned?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cGSAXEDNHp", "forum": "v6XIxJzVWF", "replyto": "v6XIxJzVWF", "signatures": ["ICLR.cc/2026/Conference/Submission7076/Reviewer_NCp3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7076/Reviewer_NCp3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931467377, "cdate": 1761931467377, "tmdate": 1762919260221, "mdate": 1762919260221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}