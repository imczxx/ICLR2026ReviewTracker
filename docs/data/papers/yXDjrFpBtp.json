{"id": "yXDjrFpBtp", "number": 10177, "cdate": 1758163156105, "mdate": 1763559503187, "content": {"title": "Robust Semantic Sample Filtering for Partially View-aligned Clustering", "abstract": "Multi-view Clustering (MvC) typically assumes strict sample alignment across views.\nHowever, this assumption often fails to hold in real-world scenarios due to data acquisition or occlusions, resulting in partially view-aligned data. \nExisting methods tend to rely on prior alignment knowledge and discard unaligned samples during training, hindering their performance and practical applicability.\nTo address this, we propose a novel framework named REFINE that integrates Cross-view Semantics-based Filtering and Shared-space Contrastive Learning to robustly handle partially view-aligned data. \nOur method dynamically identifies reliable samples by aligning pseudo-labels across views and filters out noisy correspondences to improve clustering prototype initialization and cross-view consistency learning. \nMoreover, we employ a cross-view decoder to project features into a shared latent space, bridging modality gaps and facilitating more effective contrastive learning.\nExtensive experiments across five benchmark datasets under both fully aligned and partially aligned settings demonstrate that our approach achieves state-of-the-art performance, delivering superior robustness and generalization in real-world scenarios without strict alignment requirements.\nOur code has been made anonymously available at https://github.com/REFINE-REFINE/REFINE.", "tldr": "", "keywords": ["Multi-view Clustering; Partially View-aligned Clustering"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3dd77a7bdad3bfa8636afa8c4a157075131d8e15.pdf", "supplementary_material": "/attachment/02547040b6f6f178a2fd89481892d302e546c5e8.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes REFINE, a new  multi-view clustering (MVC) method. REFINE focuses on the problem of partial view alignment in MVC. Previous studies often assume that the correspondence between views is known and accurate. In contrast, this paper introduces a new and meaningful scenario, where it is unknown whether the inter-view correspondence is available or not. To address this challenge, the authors observe that the model tends to first learn clearer cross-view correspondences and then gradually adapt to more complex cross-view relationships in partially aligned data. Based on this insight, they design an effective solution. Extensive experiments demonstrate the effectiveness of the proposed method. Compared with previous works, the proposed method achieves remarkably good performance in most scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of this work is relevant and meaningful, as in real-world scenarios it is often challenging to determine whether the correspondences across multi-view datasets are accurate or contaminated by noise. From this perspective, the paper makes a valuable addition.\n\n2. The proposed method is conceptually simple yet effective. It assesses the correctness of inter-view correspondences by measuring the consistency among clustering assignments across views. Extensive experiments validate the robustness and effectiveness of this approach.\n\n3. The paper is clearly written and easy to read. Moreover, the experimental evaluation is relatively thorough and provides solid support for the proposed method."}, "weaknesses": {"value": "1. The compared baselines in the experiments are somewhat dated. It would strengthen the paper to include comparisons with more recent related methods. Although there may not be existing works that address exactly the same setting as REFINE, such comparisons would still be informative and valuable, particularly when the FP Ratio is 0.\n\n2. The authors mention that the proposed method is inspired by the sample selection strategy used in NLL. It would be beneficial to provide a brief overview or discussion of this prior strategy in the related work section to better contextualize the contribution.\n\n3. An obvious observation is that when the FP Ratio becomes large, the performance of all methods tends to decrease to some extent. This raises a concern: under such conditions, could the performance of MVC methods be inferior to clustering based on a single view?  For example, suppose the correspondence between two views (View 1 and View 2) is incorrect, and REFINE produces clustering result $a$, while performing clustering on the two views independently yields results $b$ and $c$. If $a$ < min($b$, $c$), it may suggest that the incorrect inter-view consistency leads to severe degradation, to the extent that the benefit of multi-view learning is largely lost. From this perspective, it might be helpful to include single-view clustering results as additional baselines to provide a more comprehensive comparison and better illustrate the effect of misaligned correspondences."}, "questions": {"value": "1. As the proposed approach may be the first to address the case where inter-view correspondences are unknown, it would be beneficial for the authors to clarify the experimental settings in the comparative studies. Specifically, were the compared methods evaluated under the assumption that inter-view correspondences are known? This clarification would help readers better understand the fairness and validity of the comparisons.\n\n2. From a methodological perspective, Section 3.1 emphasizes the use of Siamese encoders. In the scenario considered by REFINE, what are the advantages of using Siamese encoders compared with the more commonly used autoencoders?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dhmjmzJJti", "forum": "yXDjrFpBtp", "replyto": "yXDjrFpBtp", "signatures": ["ICLR.cc/2026/Conference/Submission10177/Reviewer_71jf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10177/Reviewer_71jf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761054904461, "cdate": 1761054904461, "tmdate": 1762921545810, "mdate": 1762921545810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces REFINE to tackle multi-view clustering with noisy correspondence. In the CSF module, it filters out possibly misaligned samples to improve the quality of cluster centers, and enforces cross-modal semantic consistency with symmetric KL divergence minimization. In the SCL module, a shared decoder for different modalities to reduce the modality gap. Combining these components, REFINE outperforms pervious SOTA methods, achieving better semantic consistency and better separated distribution. Extensive comparison and ablation results confirm the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. By employing a shared cross-view decoder to project different modalities into the same latent space, REFINE not only reduces modality gaps but also strengthens the effectiveness of the filtering mechanism.\n\n2. REFINE consistently outperforms state-of-the-art methods across five benchmark datasets and various noise levels (false positive ratio up to 80%), surpassing recent methods such as CANDY and DIVIDE.\n\n3. The paper is well written, and the proposed method is validated with a wide range of ablation studies and visualizations.\n\n4. The paper studies an practical problem for the multi-view clustering community."}, "weaknesses": {"value": "1. Although the paper achieves better performance, the improvements are relatively marginal (<2%) in the multi-view clustering community. Moreover, there is no direct evaluation of filtering module (e.g., precision/recall).\n\n2. The paper claims that the shared decoder reduces modality gaps, but does not provide comparative experiments with independent decoders.\n\n3. Minor writing and formatting issues, including improper usage of `\\citet` / `\\citep`.\n\n4. The overall novelty is limited. The network architecture is very resemble to the recent SOTA methods like CANDY.\n\n5. The experiment evaluations are not solid enough. Concretely, according to Table 1, the hyper-parameters $L_r$ and $\\gamma_{1}$ vary among different datasets, which might violate the unsupervised characteristic of multi-view clustering. Moreover, the effectiveness of the method is only verified on small scaled datasets (all less than 20,000)."}, "questions": {"value": "1. Beyond final clustering metrics, could you please report filtering precision/recall (against known ground truth correspondences) to directly validate the quality of the selected samples?\n\n2. Why shared decoder is better than independent decoders per view, especially when the embedding spaces differ substantially?\n\n3. Since the paper discusses extending to more than two views, could you comment on the performance trends as the number of views increases?\n\nPlease refer to the weaknesses to see more questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m2uq5grNwX", "forum": "yXDjrFpBtp", "replyto": "yXDjrFpBtp", "signatures": ["ICLR.cc/2026/Conference/Submission10177/Reviewer_w53J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10177/Reviewer_w53J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528481109, "cdate": 1761528481109, "tmdate": 1762921544788, "mdate": 1762921544788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the partially view-aligned clustering problem, where cross-view correspondences between samples are incomplete or noisy. The authors propose a framework integrating two core modules: cross-view semantics-based filtering and shared-space contrastive learning. Experiments on five datasets demonstrate good performance under both fully aligned and highly misaligned conditions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The proposed method demonstrates robust performance even under severe misalignment (up to 80%), showing good robust.\n\n2.Extensive ablation and sensitivity analyses clearly validate the contribution of each module to the overall performance.\n\n3.The shared latent space design effectively enhances cross-view alignment by reducing view gaps and improving feature consistency."}, "weaknesses": {"value": "1.Over-reliance on pseudo-label consistency. The filtering mechanism assumes pseudo-labels from K-means are reliable indicators of semantic correctness, which may not hold early in training. Misleading pseudo-labels could generate filtering errors.\n\n2.Discarding instead of reusing uncertain samples. All inconsistent samples are dropped, reducing data utilization under high misalignment.\n\n3.The problem of scalability to more views. Although the paper mentions that the proposed method can be naturally scaled to more than two views, the datasets chosen for the experiment still consist of only two views."}, "questions": {"value": "1.How does the filtering mechanism behave when early pseudo-labels are highly noisy?\n\n2.Can the proposed method handle more extreme cases (e.g., 90% misalignment)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Iu4rWfIVnH", "forum": "yXDjrFpBtp", "replyto": "yXDjrFpBtp", "signatures": ["ICLR.cc/2026/Conference/Submission10177/Reviewer_LVPN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10177/Reviewer_LVPN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555583537, "cdate": 1761555583537, "tmdate": 1762921543752, "mdate": 1762921543752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes REFINE, a framework for Partially View-Aligned Clustering, which combines Cross-view Semantics-based Filtering and Shared-space Contrastive Learning to improve robustness under view misalignment. It is inspired by Noisy Label Learning, which filter unreliable sample correspondences via semantic consistency. The paper is technically solid and experimentally comprehensive, but suffers from some weaknesses in clarity, novelty positioning, and mathematical justification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "REFINE tackles the practically pertinent issue of view misalignment in multi-view clustering with clear motivation, integrates the semantically based filtering and shared-space contrastive learning modules in an intuitively complementary fashion. The authors conduct comprehensive experiments across five benchmark datasets under varying false-positive ratios, supported by  ablation studies that firmly attest to the contribution of each component."}, "weaknesses": {"value": "1.The FP simulation is implemented via random shuffling. It is recommended to supplement the evaluation with a real-world misalignment dataset to verify the robustness of REFINE under naturally occurring cross-view inconsistencies.\n2.When all views are severely inconsistent, will the Semantics-based Filtering module discard too many samples, thereby reducing the effective training data and harming clustering stability? Is there any theoretical bound or empirical estimation on the filtering precision (i.e., the false discard rate of reliable samples) to ensure that the semantic filtering mechanism does not eliminate too many valid correspondences?\n3.The entropy regularization term ( L_{\\text{ent}} ) is designed to prevent cluster collapse and encourage a uniform marginal distribution across clusters. However, in real-world datasets, the true class distribution is often imbalanced. Would this term force the model to learn an overly uniform clustering structure that does not reflect the underlying data distribution?"}, "questions": {"value": "1.How does REFINE perform when only one view provides strong, informative features while other views are weak or noisy? Have related experiments been conducted to verify robustness under such conditions?\n2.in Eq. 9, why is the Student’s t-distribution preferred over the standard softmax function for pseudo-label assignment? Please clarify the theoretical or empirical rationale behind this design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FOAmC4JezV", "forum": "yXDjrFpBtp", "replyto": "yXDjrFpBtp", "signatures": ["ICLR.cc/2026/Conference/Submission10177/Reviewer_5DEh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10177/Reviewer_5DEh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622186178, "cdate": 1761622186178, "tmdate": 1762921543354, "mdate": 1762921543354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles *partially view‑aligned clustering* (PVC), where cross‑view correspondences are unknown and potentially noisy. The proposed framework, REFINE, combines (i) Cross‑view Semantics‑based Filtering—used both for Periodic Cluster Prototype Initialization (PCPI) and for *Cross‑modal Semantic Consistency Learning* (CSCL)—and (ii) a Shared‑space  Contrastive Learning* (SCL) module with spectral denoising. Architecturally, each view is encoded by Siamese encoders (online/key), query features are projected through a shared cross‑view decoder into a unified space, and view‑specific cluster heads produce soft assignments; Figure 2 (page 3) gives a clear overview and the filtering submodule on the right shows how consistent pairs are retained for prototype re‑initialization and KL‑based consistency training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Using semantic agreement to *filter* pairs is intuitive and well‑motivated by noisy label learning. The two insertion points—prototype init and consistency learning—are complementary, and the shared decoder plausibly reduces cross‑view modality gaps (Figure 2, page 3). \n\n   The evaluation spans five benchmarks and four FP settings with repeated runs. The ablation suite is unusually careful for clustering work (Table 3 and Figure 3), and the periodic re‑init analysis (Figure 3b) is useful to practitioners. \n\n   Entropy regularization against collapse, SVD‑based spectral denoising to soften supervision, and the use of Hungarian matching for class‑level alignment are appropriate for this setup (Sec. 3, pages 5–7)."}, "weaknesses": {"value": "Warm‑up assumes hard pairwise alignment in the loss. (Sec. 3.1, )\nClaims of consistent” SOTA are not fully supported by Table 2.\n Evaluation mostly on *synthetic* misalignment; limited real‑world evidence. For eg. All PVC settings are constructed by random shuffling with a fixed FP ratio (Sec. 4.1, page 7). That is useful but may not capture structured misalignment from acquisition lags, occlusions, or domain shifts. Demonstrations on genuinely misaligned multi‑modal datasets (e.g., image–text collections with partial captions or desynchronized multi‑sensor logs) would strengthen the “real‑world robustness” claim. \n\nUnder‑specified architecture and complexity details.\n   \nRisk of confirmation bias in filtering.\n  \nAlthough Table 1 lists core hyperparameters (page 7), several training choices are as in CANDY, and no standard deviations are reported. Given small absolute margins in some settings, error bars and a short note on per‑baseline tuning would increase credibility."}, "questions": {"value": "Can you ablate *disabling inter‑view contrastive* during warm‑up (i.e., only intra‑view) or replace identity‑based positives with mined high‑agreement pairs to quantify the cost/benefit of early hard alignment noise? (Sec. 3.1, Eq. (3))"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vWapAWDneT", "forum": "yXDjrFpBtp", "replyto": "yXDjrFpBtp", "signatures": ["ICLR.cc/2026/Conference/Submission10177/Reviewer_GBMU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10177/Reviewer_GBMU"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980952348, "cdate": 1761980952348, "tmdate": 1762921542898, "mdate": 1762921542898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all reviewers"}, "comment": {"value": "We sincerely thank the reviewers for their constructive feedback. In response, we have substantially revised the manuscript to address the concerns raised. Key improvements are summarized as follows:\n\n$\\textbf{1. Sample selection related work}$\n\n$\\textbf{2. Detailed model architectures}$\n\n$\\textbf{3. Comparisons with more SOTA methods}$\n\n$\\textbf{4. Filtering precision analysis}$\n\n$\\textbf{5. Robustness analysis of NLL-inspired filtering}$\n\n$\\textbf{6. Writing issues}$\n\n\nDue to space constraints, several deeper analyses and extended experiments were included in the response and supplementary material but not in the main manuscript. These include:\n\n$\\textbf{1. Experiments on larger datasets and more views scenarios}$ (i.e., Noisy MNIST, CIFAR-100, Caltech 2-5 views). \n\n$\\textbf{2. Statistical analysis}$ (i.e., experimental statistics including mean and standard deviation over 5 runs)\n\n$\\textbf{3. Additional ablation studies}$ (i.e., hard alignment vs inter-view alignment during warm-up, student’s distribution vs softmax, independent decoder vs shared decoder, single view vs multi-view)\n\n$\\textbf{4. Robustness analysis under extreme noise rates}$ (i.e., clustering performance and filtering F1 under FP 90% ).\n\n$\\textbf{5. Detailed computational complexity analysis}$\n\n\nPlease see our detailed responses below each review, and all the changes in the revised paper are high-lighted with blue font. We appreciate your feedback and welcome any additional questions."}}, "id": "htc3BErnwi", "forum": "yXDjrFpBtp", "replyto": "yXDjrFpBtp", "signatures": ["ICLR.cc/2026/Conference/Submission10177/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10177/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission10177/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763559376589, "cdate": 1763559376589, "tmdate": 1763559376589, "mdate": 1763559376589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}