{"id": "PBIHh6ibal", "number": 8895, "cdate": 1758101626298, "mdate": 1763662121197, "content": {"title": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection", "abstract": "The rapid rise of synthetic media has made deepfake detection a critical challenge for online safety and trust. Progress remains constrained by the scarcity of large, high-quality datasets. Although multimodal large language models (LLMs) exhibit strong reasoning capabilities, their performance on deepfake detection is poor, often producing explanations that are misaligned with visual evidence or hallucinatory. To address this limitation, we introduce a reasoning-annotated dataset for deepfake detection and propose Paragraph-level Relative Policy Optimization (PRPO), a reinforcement learning algorithm that aligns LLM reasoning with image content at the paragraph level. Experiments show that PRPO improves detection accuracy by a wide margin and achieves the highest reasoning score of 4.55/5.0. Ablation studies further demonstrate that PRPO significantly outperforms GRPO under test-time conditions. These results underscore the importance of grounding multimodal reasoning in visual evidence to enable more reliable and interpretable deepfake detection.", "tldr": "", "keywords": ["deepfake detection", "vision language models", "deepfake reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/445faa2c3a2477c83908ef98b3a5cbf2cc091230.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "### Method\n- The authors propose a new training dataset, an enhanced model, and a new RL algorithm for deepfake detection with MLLMs.\n    - DF-R5, a new training set, which is collected by using off-the-shelf MLLMs such as Gemini.\n\n    - An enhanced model, DX-LLaVA. The authors replace CLIP ViT with CLIP ConvNeXT to get pixel-level embeddings, enabling a finer focus on local image regions.\n\n    - A new RL algorithm, PRPO. PRPO does not require ground truth labels, but utilize Visual Consistency Reward and Prediction Consistency Reward to improve the model performance, which can be applied at test time.\n\n### Results\n- PRPO achieves the highest reasoning score of 4.55/5.0.\n- Ablation studies show the advantages of the proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This study improves deepfake detection in several aspects, including data, model architecture, and algorithms.\n- The exploration and the proposed methods are intuitive and reasonable.\n- PRPO significantly surpasses prior methods and achieves the highest reasoning score."}, "weaknesses": {"value": "### Major\n- Quality of the dataset. \n    - DF-R5 is built on off-the-shelf MLLMs such as Gemini. I'm not sure whether this pipeline can ensure stable and high-quality data annotations. Does Gemini generate hallucination or wrong annotations?\n    - Second, it seems that the upper bound of the data is limited by the used MLLMs. \n    - Last, the evaluation such as the explanation quality evaluation also relies on MLLMs (GPT-4o here). I'm wondering if this leads to overestimated performance, because MLLMs generate training data and MLLMs evaluate. It is possible that these models will prefer an answer \"similar\" to their own.\n\n- Generalization ability of DX-LLaVA.\n    - As shown in Table 2, the performance for inter-domain is not satisfactory. I didn't see further discussion on this point, so I'm not sure about the generalization ability of DX-LLaVA across domains different from the training data\n\n### Minor\n- How is the performance of the classifer trained on the CLIP ConvNeXT features? Is it better than DX-LLaVA?\n- The authors are recommended to put more examples in the main context or Appendix, to compare the quality of explanations generated by DX-LLaVA or other models. \n\n- About PRPO\n    - How do the authors separate paragraphs from the DX-LLaVA answers during training? A fixed-number of sentences is a paragraph? Or separate paragraphs based on meanings?\n    - If we use ground truth labels to introduce accuracy rewards to PROP (like GRPO), will the performance be improved?"}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oxu6RachFS", "forum": "PBIHh6ibal", "replyto": "PBIHh6ibal", "signatures": ["ICLR.cc/2026/Conference/Submission8895/Reviewer_jDoc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8895/Reviewer_jDoc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761364304758, "cdate": 1761364304758, "tmdate": 1762920651297, "mdate": 1762920651297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to improve the deepfake detection for multimodal models. To deal with limitations in existing multimodal deepfake detection, the authors introduce a new large scale dataset comprising reasoning explanations automatically generated from Gemini 2.5. The authors further modify Llava by using CLIP Convnext encoder along with auxiliary classification loss. The authors also introduce PRPO, which is a test time reinforcement learning algo using paragraph level rewards and prediction consistency reward to optimize alignment and consistency of generated reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to read and well presented.\n- PRPO in order to improve deepfake detection is interesting area and novel in this context.\n- The paper addresses a key problem and the approach in direction of explainability has significant value.\n- The authors also introduce a large scale dataset in this context, which will be useful for future exploration in this area."}, "weaknesses": {"value": "- The proposed dataset is built on the distilled reasoning from Gemini 2.5. This can severally limit the significance of the conclusion in this paper.\n- VCR relies on the semantic alignment, which could be insufficient for forensic tasks where visual consistency needs to verified at pixel level.\n- The architectural modification seems to be oversold and is not a significant modification. This could be strengthened by low level analysis or some visualisations. The claim on architecture benefits are also not well justified.\n- Its not clear how much computation overhead is at test-time with RL approach. The latency aspect is not well explained.\n- A significant drawback of current method is around overfitting the reasoning style of Gemini 2.5 which could lead to fragile explanations."}, "questions": {"value": "I would request the authors to provide inference time overhead for PRPO optimization. Please also look at the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QdcojFgTpe", "forum": "PBIHh6ibal", "replyto": "PBIHh6ibal", "signatures": ["ICLR.cc/2026/Conference/Submission8895/Reviewer_1dF7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8895/Reviewer_1dF7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784388259, "cdate": 1761784388259, "tmdate": 1762920650922, "mdate": 1762920650922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of deepfake detection and forensic reasoning using Multimodal Large Language Models (MLLMs). The core contribution is the introduction of a new reasoning-annotated dataset, DF-R5 (115k image-reasoning pairs), specifically designed for deepfake detection. Furthermore, the authors propose an enhanced MLLM architecture, DX-LLaVA, fine-tuned on DF-R5, and a novel test-time optimization algorithm, Paragraph-level Policy Optimization (PRPO), to align the generated forensic explanations with visual evidence. Experimental results demonstrate that the combined approach significantly outperforms existing detection baselines and general state-of-the-art MLLMs in both classification accuracy and reasoning quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The research tackles the complex issue of boundary ambiguity and unreliable reasoning in MLLM-based deepfake detection. The proposal of an MLLM fine-tuning method (DX-LLaVA) combined with a post-SFT optimization algorithm (PRPO) for forensic reasoning is a clear and novel contribution to the field.\n\n2. The introduced dataset, DF-R5, provides a substantial volume (115k pairs) of high-quality, fine-grained reasoning annotations crucial for training MLLMs to perform reliable deepfake forensics. This dataset significantly helps in bridging the data gap for explainable deepfake detection.\n\n3. The PRPO method demonstrates substantial and consistent performance improvements over both dedicated deepfake detection baselines (Table 4) and general SOTA MLLMs (Table 5, Table 10), validating the effectiveness of the proposed data and optimization strategy."}, "weaknesses": {"value": "Major concerns:\n\n1. Lack of clarity and details regarding the proposed DX-LLaVA architecture and its implementation. Section 3.2, which describes the method, abruptly introduces DX-LLaVA without a smooth transition from the preceding section, making the rationale behind its design unclear. Specifically, the paper needs explicitly state which parameters of the base LLaVA model are fine-tuned (e.g., full LLM weights, LoRA, or just the projection layer). Similarly, the replacement of the vision encoder with ConvNeXT is introduced suddenly. Furthermore, the embedding of certain experimental results (Tables 2 and 3) directly within the methodology section hinders the reader's ability to first grasp the complete method overview before evaluating its performance.\n\n2. Inconsistency and ambiguity in the experimental setups, which complicates the assessment of performance gains. The paper needs to clearly articulate the data split. Specifically, are the experiment results shown in Table 3 based on an inter-domain or intra-domain split. This distinction is crucial for understanding and validating the reported performance improvements.\n\n3. The ambiguity of the relationship between DX-LLaVA and PRPO, and lack of their individual contributions in the ablation analysis. While PRPO is understood to operate on the SFT-trained DX-LLaVA, the paper lacks a clear ablation study."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MaTMT3R8hF", "forum": "PBIHh6ibal", "replyto": "PBIHh6ibal", "signatures": ["ICLR.cc/2026/Conference/Submission8895/Reviewer_kPnV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8895/Reviewer_kPnV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986508174, "cdate": 1761986508174, "tmdate": 1762920650168, "mdate": 1762920650168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework to enhance both detection accuracy and interpretability of multimodal reasoning in deepfake image detection. The authors address two key gaps: the scarcity of large reasoning-annotated datasets and the misalignment between visual evidence and generated explanations by multimodal large language models (MLLMs). To tackle these, they first introduce DF-R5, a dataset of approximately 115 K images paired with high-quality reasoning annotations; second, they present DX-LLaVA, a vision-language architecture combining a CLIP ConvNeXT encoder with a language model to better capture fine-grained artifacts; and third, they develop Paragraph-level Relative Policy Optimization (PRPO), a test-time reinforcement-learning algorithm that rewards paragraph-level alignment of reasoning with visual features and internal consistency between reasoning and final decision"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The dataset contribution (DF-R5) is significant: reasoning annotations for deepfake detection are rare, and the scale (~115k) is commendable.\n* The architectural component (DX-LLaVA) shows awareness of the limitation of generic vision encoders for subtle artifact detection; replacing CLIP ViT with ConvNeXT is a reasonable design choice.\n* The RL component (PRPO) is novel in that it treats paragraphs as units for reward, rather than tokens or single outputs, and aligns reasoning with visual features as well as final output consistency."}, "weaknesses": {"value": "One concern is that the proposed model has been fine-tuned specifically on deepfake data, while the compared baselines are general-purpose MLLMs such as Gemini-2.5 and GPT-4o. This raises a fairness issue: the reported performance gains might largely reflect domain-specific adaptation rather than the effectiveness of the proposed algorithm itself."}, "questions": {"value": "A key question concerns the generality of the proposed PRPO framework. While the method is shown effective for deepfake detection, it remains unclear whether PRPO can generalize to broader vision-language reasoning tasks, such as visual question answering or visual entailment. Given that the algorithm optimizes reasoning alignment at the paragraph level, one would expect it to be applicable beyond this specific domain. I encourage the authors to clarify this point or discuss potential extensions to more general visual reasoning settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AP9sIuSZ2Z", "forum": "PBIHh6ibal", "replyto": "PBIHh6ibal", "signatures": ["ICLR.cc/2026/Conference/Submission8895/Reviewer_24DX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8895/Reviewer_24DX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020073350, "cdate": 1762020073350, "tmdate": 1762920649500, "mdate": 1762920649500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}