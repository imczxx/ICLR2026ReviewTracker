{"id": "YTn7KRcG9y", "number": 23310, "cdate": 1758342002421, "mdate": 1759896821711, "content": {"title": "Arithmetic-Mean $\\mu$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets", "abstract": "Choosing an appropriate learning rate remains a key challenge in scaling depth of modern deep networks. The classical maximal update parameterization ($\\mu$P) enforces a fixed per-layer update magnitude, which is well suited to homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in heterogeneous architectures where residual accumulation and convolutions introduce imbalance across layers. We introduce Arithmetic-Mean $\\mu$P (AM-$\\mu$P), which constrains not each individual layer but the network-wide average one-step pre-activation second moment to a constant scale. Combined with a residual-aware He fan-in initialization, scaling residual-branch weights by the number of blocks ($\\mathrm{Var}[W]=c/(K\\cdot \\mathrm{fan\\text{-}in})$), AM-$\\mu$P yields width-robust depth laws that transfer consistently across depths. We prove that, for one- and two-dimensional convolutional networks, the maximal-update learning rate satisfies $\\eta^\\star(L)\\propto L^{-3/2}$; with zero padding, boundary effects are constant-level as $N\\gg k$. For standard residual networks with general conv+MLP blocks, we establish $\\eta^\\star(L)=\\Theta(L^{-3/2})$, with $L$ the minimal depth. Empirical results across a range of depths confirm the $-3/2$ scaling law and enable zero-shot learning-rate transfer, providing a unified and practical LR principle for convolutional and deep residual networks without additional tuning overhead.", "tldr": "We prove an $L^{-3/2}$ depth–learning-rate scaling law and introduce AM-$\\mu$P, a practical rule that transfers across CNNs and ResNets without extra tuning.", "keywords": ["learning-rate scaling", "AM-muP", "muP", "hyperparameter transfer", "ResNet", "CNN"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/755f73118f209f8bcea27cbda0bdf90270204b4b.pdf", "supplementary_material": "/attachment/590d23b18ee27d805cce0c208e4e7cc812e340af.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies scaling laws for the learning rate for convolutional neural networks (CNNs) and residual networks (ResNets). The paper introduces a new condition for stability, which is the average variance of the pre-activation gradient updates, where the average is taken over the depth of the network. This condition implies a scaling for the learning rate, which scales as $L^{-3/2}$ both for CNNs and for ResNets. Experiments corroborate the findings."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Deriving scaling laws is essential to modern deep learning. The authors study realistic architectures, including for instance the effect of padding in CNNs."}, "weaknesses": {"value": "W1: there is already a strong literature on learning rate scaling for ResNets (and CNNs) that the authors do not engage with, e.g., \n\nThe Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks, Chizat and Netrapalli, NeurIPS 2024. Tthis paper is cited by the authors, but they do not compare the results, although this reference already gives formulas for ResNets.\n\nTensor Programs VI: Feature Learning in Infinite-Depth Neural Networks, Yang et al., ICLR 2024. Gives formulas for ResNets.\n\nTensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer, Yang et al., NeurIPS 2021. Mentions that the results can be extended to CNNs (see remark page 47).\n\nThe authors should explain how their work connects to the literature.\n\nW2: for ResNets, it is known (cf literature above, or also Marion et al., Scaling ResNets in the Large-depth Regime, JMLR 2025) that the residual branch should be scaled in order to avoid instability. This scaling interacts with the scaling of the learning rate, which is then different from the MLP case. In the present paper, this scaling is not considered. This questions the validity of the learning rate scaling derived in the paper (since the forward pass already leads to instabilities).\n\nW3: the condition on the average variance that is introduced in the paper relies on the idea that the scale of the updates is heterogeneous along the depth of the network. However, most of the literature precisely tries to avoid such heterogeneity, by scaling appropriately both the forward and backward pass (see again references above). So the authors would need to precisely justify why they consider the heterogeneous case. Furthermore, if the updates are heterogeneous, why don’t the authors consider a learning rate that varies depending on the layers?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rffxixKXLB", "forum": "YTn7KRcG9y", "replyto": "YTn7KRcG9y", "signatures": ["ICLR.cc/2026/Conference/Submission23310/Reviewer_YPVT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23310/Reviewer_YPVT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761138731824, "cdate": 1761138731824, "tmdate": 1762942598684, "mdate": 1762942598684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In deep learning training, the so-called \"mu parametrization\" (muP, Yang et al 2022) is a scaling of the learning rate as a function of layers' width, designed such that the variance of the first gradient descent update of preactivations, for each layer $\\ell$, $S_\\ell$, is equal to 1.\nThe paper argues that enforcing all layerwise variances to be equal to 1 is too restrictive for \"heterogeneous networks\" such as resNets and CNNs, which have layer that differ in nature (fully connected vs residual or convolutional layers).\nThe contribution of the paper is to instead enforce that **the average of all $S_\\ell$'s for the first update is equal to 1**.\nThe authors prove that the learning rate that enforces this scales like $L^{-3/2}$ for both resnets and CNNs, a result similar to existing muP results for standard MLPs. Deriving this scaling law allows for fast prototyping and learning rate tuning on small network, than transferring observations to wider and deeper networks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Deriving scaling laws has been a major progress in hyperparameter tuning these last years, allowing to test small architectures and then scale them gracefully. To my knowledge, studies on the learning rate like muP were so far limited to MLPs: extending this work to other architectures is of interest."}, "weaknesses": {"value": "The paper's writing and organization is subpar, which makes it very hard to follow. Some quantities do not seem to be defined (where is the definition of $N_{\\ell,r}$, of $\\Delta_r$?)\n- The abstract mention mathematical quantities that are not defined; while $W$ and fan_in can be inferred, $N$ and $k$ are unknown to the reader at this stage.\n- For CNNs, many quantities are not defined ()\n- I strongly suggest moving content from the related works section into the introduction.\nIndeed, the latter is hard to grasp for a non expert of the domain, as it refers to many concepts not introduced yet (residual accumulation, spatial-channel coupling, minimal effective depth, one-step preactivation update); it is only the related works section that provides the necessary information.\nFor example, at the introduction stage, the claim \"enforcing identical per layer update magnitude is overly restrictive for such heterogeneous networks\" does not seem justified; it also remains unclear why zero or circular padding affects variances (sentences such as \"visits are uniform and the recursion mirror the FC case\" L67 remain cryptic: what are visits, which recursion are the authors thinking about?).\n- The part about CNNs is especially unclear: the authors should detail, at least lightly, what are  channel heteroscedasticity, torus indexing, kernel offset set, the axial-half span, etc.\n- Still organization-wise, it would make sense to introduce strategies about weight initialization (section 2.3) *before* muP strategies (that requires mean field intialization) in section 2.2.\n\nThe paper would require a major rewrite in my opinion."}, "questions": {"value": "- The authors state that He initialization preserves the magnitude of gradients across layers: do they have a reference for that (in particular if network width varies)?\n- L205 the authors claim that controlling the average of $S_\\ell$'s yields a control over all $S_\\ell$, but this does not seem true, as the smallest $S_\\ell$ can approach 0 arbitrarily.\n- Can the authors define the spatial index set formally? since the paper is focused on convolutions, this would be very useful. What's the kernel offset set? What is the kernel axial span?\n- What do the authors mean by $N_\\ell = N_{\\ell - 1}$ \"within a block\", followed by \"$N_\\ell$ is allowed to vary with $\\ell$\" ?\n- The paper refers to \"homogeneous cases\" as, I believe, cases where all variances per layer are roughly equal. In the literature, homogenous networks are rather relu network, that present some equivariance properties to weight and bias scaling.\n\nMinor remarks: \n- All equations should be numbered to facilitate discussion\n- The paper uses \"Jelassi et al (Jelassi 2023) show that\"; this is incorrect. One should use the \\citet command of natbib, to produce content that reads: \"Jelassi et al (2023) show that\". There are several occurrences of this issue.\n- the paper uses both fan_in, $n_in$ and $n$ to refer to the number of inputs to the layer in equations; I suggest using systematically $n_in$"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4JW7wD5wZG", "forum": "YTn7KRcG9y", "replyto": "YTn7KRcG9y", "signatures": ["ICLR.cc/2026/Conference/Submission23310/Reviewer_QUSQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23310/Reviewer_QUSQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925160044, "cdate": 1761925160044, "tmdate": 1762942598430, "mdate": 1762942598430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the scaling of the optimal learning rate $\\\\eta^*$ with network depth $L$ for convolutional and residual networks. The main result is the scaling law  \n\n$$\\\\eta^* \\\\propto L^{-3/2}.$$\n\n(appearing earlier for MLPs in Jelassi et al., 2023). This law is established theoretically in theorems 1-3 and verified by a series of experiments on CIFAR 10."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a reasonable and important point of scaling the learning rate of gradient descent with network depth. The obtained scaling law, while known previously for MLPs, is established in the paper for the more practical and modern CNN and Resnet architectures. The scaling law is carefully verified experimentally, and the results agree well with the theoretical predictions. The paper is definitely sound and has a practical value.  \n\nThe paper is on the whole well written (albeit in a relatively informal style, see weaknesses below). Several appendices are provided, covering the proofs of the theorems, additional discussion of the proposed AM $\\\\mu P$ regime and additional experiments."}, "weaknesses": {"value": "**Limited novelty / significance**\n\nMy impression is that the main contribution of the paper is an extension of the previously known $L^{-3/2}$ optimal depth scaling of learning rates (Jelassi et al., 2023) to CNNs and Resnets (in Theorems 1-3). As the authors themselves point out, the scaling was known for MLPs. The present work takes into account the special aspects of CNNs and Resnets (filter size, number of channels, etc.), but it seems that the overall scaling and the main logic of the derivation are the same, and so the extension is relatively incremental - at least, I don't see important differences clearly articulated, or why this extension should be considered a challenging problem.\n\nThe paper emphasizes what it calls a new Arithmetic Mean $\\\\mu P$ regime, but I don't see how it is importantly different from the alternatives, and I don't even quite see how it is effectively implemented in the paper. While the paper discusses in detail the (more or less standard) layer-dependent initialization, it doesn't say anything about a specific layer-dependent learning rate schedule, which I would expect to characterize a new learning regime. It appears that this AM $\\\\mu P$ regime is merely used in Theorems 1-2 to justify the choice of optimal learning rate by averaging the variance of updates across the layers. However, this variance in the proofs is given by a particular cubic function of the layer index $l$ (line 809). Even if we don't average, but just consider the variance at the last layer $l=L$, or in the middle of the network, $l=L/2$, this will still produce the desired $L^{-3/2}$ scaling of the optimal learning rate. So, the role of the AM $\\\\mu P$ regime in the established results is not clear to me. \n\nMoreover, the proposed AM $\\\\mu P$ regime includes non-uniform scenarios that look unfavorable. For example, with respect to the proposed $\\overline S$ characteristic, training only the output layer of the network with a huge update variance appears equivalent to uniformly training all layers. However, the former option is likely suboptimal and conflicts with the general $\\\\mu P$ logic of equidistribution of learning over the layers.           \n\n**Issues with exposition / clarity**\n\nTheorems 1-3 don't look like theorems in the proper mathematical sense. They are stated very informally, and the paper doesn't provide fully rigorous versions (even in the appendix). For example, in Theorem 1 the precise meaning of the main object, the \"*learning rate scale that preserves width-invariant training dynamics*\" is unclear. It appears from the proof that the theorem concerns the first step of the gradient descent after network initialization by independent weights; also, this step seems to be considered in a linear approximation. The precise setting can only be guessed by studying the proofs. \"*$\\\\kappa$ depends only on the activation/initialization fixed point*\" - unclear; no fixed point has been mentioned elsewhere. Theorem 1 seems to contradict Theorem 2: e.g., the former seems to claim that $\\\\eta^*$ does not depend on the channel widths $C_l$, but the latter seems to admit such dependence. \n\nThe sctructure of the paper is generally not optimal. Some points seem to be excessively repeated, while other essential information is missing. In particular, variants of the formula $\\\\eta^*(L)\\propto L^{-3/2}$ are repeated numerous times (lines 66, 73, 96, 141, 311, 349, 360, 472), but the paper never explains in simple terms the origin of the exponent $-3/2$. The difference between $\\\\propto$ and $\\\\Theta$ in lines 66 and 73 is not explained."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XjWWJWXK5W", "forum": "YTn7KRcG9y", "replyto": "YTn7KRcG9y", "signatures": ["ICLR.cc/2026/Conference/Submission23310/Reviewer_g7Ee"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23310/Reviewer_g7Ee"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762208406951, "cdate": 1762208406951, "tmdate": 1762942598169, "mdate": 1762942598169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}