{"id": "LzCBLrNoyM", "number": 411, "cdate": 1756738604429, "mdate": 1759898262367, "content": {"title": "Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions", "abstract": "Recent advances in large language model (LLM) reasoning have shown that reasoning ability can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce \\textbf{ReLIFT} (\\textbf{Re}inforcement \\textbf{L}earning \\textbf{I}nterleaved with Online \\textbf{F}ine-\\textbf{T}uning), a novel training strategy. ReLIFT employs RL for general training, but interleaves it with targeted SFT on challenging questions for which high-quality solutions are collected online. By alternating between RL and SFT, ReLIFT addresses model weaknesses as they emerge. Empirically, ReLIFT outperforms previous RLVR methods by an average of +6.7 points across a suite of six benchmarks (five math reasoning and one out-of-distribution). More importantly, ReLIFT surpasses baselines such as individual RL, individual SFT, and various hybrid approaches while reducing the required training time. These results provide compelling evidence that ReLIFT is a powerful and resource-efficient paradigm for developing capable reasoning models. The code is available at \\href{https://anonymous.4open.science/r/Learning-What-Reinforcement-Learning-Can-t-6AFF/}{here}.", "tldr": "ReLIFT, a method combining reinforcement learning and supervised fine-tuning, enhances large language model reasoning by addressing limitations of RL through interleaved training, improving performance across benchmarks with minimal data.", "keywords": ["Large Language Models; Reasoning; Reinforcement Learning; Supervised Fine-Tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f281952dfeeb427c94ddc9709dfac112954b419.pdf", "supplementary_material": "/attachment/69de0cee9ccbf3736ed9beee6be9c065b73150d3.zip"}, "replies": [{"content": {"summary": {"value": "The paper propose to maintain a cache for the hardest questions, which cannot be solved by rollouts. After the cache is full, we will add a sft stage during the rl process. Empircal results show that it can outperfrom sft then rl method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to read and well-organized.\n- The idea is simple but effective.\n- Empircal results show the effectiveness of the method."}, "weaknesses": {"value": "- What is the value of this buffer size, $M$? The paper is also missing an analysis experiment (or ablation study) for this. This is important because Figure 4 indicates that the final results seem to be highly dependent on this parameter. And it seems related to the distribution of the datasets.\n\n- The final experimental results show limited improvement over previous methods, and the performance is inconsistent across different datasets. It will be better to show the training curve when compare to luffy rather than the final results since rl is not very stable."}, "questions": {"value": "Do you think this fine-tuning approach is primarily beneficial only when:\n- The task or dataset is particularly difficult (e.g., requiring complex multi-step reasoning), or\n- The base model is relatively weak and struggles with such questions?\n\nIn other words, would the strongest model (e.g., GPT-4-level) still benefit from this approach on datasets?\n\n\nWhat will be the results if use rl w/ sft loss instead of sft in ReLIFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "USJ48YC5N6", "forum": "LzCBLrNoyM", "replyto": "LzCBLrNoyM", "signatures": ["ICLR.cc/2026/Conference/Submission411/Reviewer_tK9a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission411/Reviewer_tK9a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761012629346, "cdate": 1761012629346, "tmdate": 1762915514208, "mdate": 1762915514208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines the differences between how LLMs learn to solve reasoning problems across various difficulties when using SFT or RL. Informed by this analysis, the paper proposes selectively using demonstrations for intermittent SFT stages during online RL training when faced with the \"hardest\" problems (i.e., those for which no solution in the GRPO group is correct). The primary results show that this is, for the most part, more effective than appropriate baselines at greater compute efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a method that is grounded in a detailed and revealing analysis about the differing impact of RL and SFT. The core results show that the proposed method yields benefits in-domain (math benchmarks) and on a single out-of-domain benchmark (MMLU-Pro). The analysis of training dynamics and the ablation studies effectively reveal how the proposed method mitigates the limitations of conducting SFT or RL alone, or as distinct training stages."}, "weaknesses": {"value": "ReLIFT is considerably less effective on LLaMA-3.1-8B than on Qwen models. It would be useful to include the full set of baselines for a model outside of the Qwen model family, so as to ensure the generality of the method. Currently, only SFT or RL alone, in addition to the instruct variant, are used as baselines for the Llama model. \n\nUsing a fixed group size of 8 for all experiments means that it is unclear whether the problem difficulty classes assigned during online RL hold for a larger group size and how this impacts the effectiveness of ReLIFT and baselines. Using at least one more group size (say 16) for the core experiments would be insightful.\n\nIn practice, all demonstrations are collected offline before finetuning, which does not fit one of the core motivations for ReLIFT: only collecting demonstrations where necessary. However, collecting demonstrations on-the-fly would add to the complexity of the training pipeline and introduce latency. Discussion of this limitation would be useful.\n\n(Nit) Using the same underline for the second and third best result reduces clarity.\n\nIt is unclear whether the entropy regularisation term used in ReLIFT is also used for SFT baselines. This is an important baseline variant that is missing if I am not mistaken."}, "questions": {"value": "Have you tried the SFT baselines with entropy regularisation?\n\nIt is not clear if the stable response length for the hardest problems is related to exploration or an artefact of prior training stages. For example, it could be because the response length for these problems is already at the maximum that was seen in training prior to RL and that the demonstrations used for SFT are longer. Could you clarify how response length in the demonstration data compares to generated response lengths from the trained model?\n\nHow do the results change if the group size is doubled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sk8IQdCJ8q", "forum": "LzCBLrNoyM", "replyto": "LzCBLrNoyM", "signatures": ["ICLR.cc/2026/Conference/Submission411/Reviewer_H2Ni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission411/Reviewer_H2Ni"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761646618918, "cdate": 1761646618918, "tmdate": 1762915514085, "mdate": 1762915514085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores the complementarity between Reinforcement Learning (RL/RLVR) and Supervised Fine-Tuning (SFT) for reasoning LLMs, and proposes ReLIFT—an alternating training strategy. \n\nDuring RL, the method identifies the “hardest” problems (current rollout accuracy acc(q)=0), collects or generates high-quality CoT solutions into a buffer, and periodically performs an SFT step before returning to RL. On five math reasoning benchmarks and one OOD benchmark with Qwen2.5-Math-7B, ReLIFT improves overall accuracy over RLVR and mixed RL+SFT baselines, while producing shorter outputs and requiring less training time and fewer examples."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly defines the GRPO objective and the alternating SFT loss with entropy regularization (α), and the training curves show how reward, length, and entropy evolve over steps, supporting a mechanism of continued exploration and steady gains.\n\n2. The method is easy to follow thanks to the flow diagram (Figure 2), the difficulty stratification, and the explicit buffer trigger condition \n（Buffer_ft >= M）which together make reproduction straightforward."}, "weaknesses": {"value": "1. The OOD evaluation relies only on MMLU-Pro and the main experiments focus on math reasoning; please add code, science QA, and multi-step commonsense tasks to test adaptability under different verifiable rewards.\n\n2.  Beyond **acc(q)=0**, please evaluate thresholds based on uncertainty, length anomalies, or self-contradictions in the CoT, and formalize the adaptive SFT trigger as a gating function of reward or entropy; report learning curves under different gating hyperparameters.\n\n3. Although ReLIFT yields shorter outputs on average, please check whether necessary long reasoning is over-penalized on AIME-style problems and provide length–accuracy analyses to identify such cases."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2w0VCSAJfG", "forum": "LzCBLrNoyM", "replyto": "LzCBLrNoyM", "signatures": ["ICLR.cc/2026/Conference/Submission411/Reviewer_u4oo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission411/Reviewer_u4oo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982657399, "cdate": 1761982657399, "tmdate": 1762915513910, "mdate": 1762915513910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current form of RL for LLMs is insufficient to induce capabilities that exceed the limitations of the base model. This paper targets at how to train reasoning-focused LLMs using RL interleaved with SFT. The paper claims that RL mostly explits what the base model already knows and SFT is better at teaching new and harder behaviors. Though SFT requires a lot of demonstrations and sometimes damage the performance oneasier questions. The paper introduces ReLIFT (Reinforcement Learning Interleaved with Online Fine-Tuning), a training framework that mixes RL and SFT adaptively rather than in fixed stages. The method first do RL on a dataset and collect hardest questions at the same time. When the hard-question buffer is full, it will invoke a SFT on these hard questions, followed by another round of RL training. Experiments show that ReLIFT outperforms pure RL, pure SFT, and prior hybrid/RLVR methods across multiple math benchmarks and an OOD benchmark, achieving higher accuracy with fewer demonstrations and more concise reasoning, suggesting that targeted, online SFT on model's failures is an efficient way to expand a model’s reasoning abilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper first do analysis on training dynamics of RL vs SFT across different difficulties, showing that RL mainly preserves existing skills while SFT can unlock previously unsolved questions. After that, they propose ReLIFT, which does RL as the primary loop while streaming in SFT steps only on hard questions. The method proposed is gounded on grounded observation, which makes sense.\n* In the experiments, the paper includes careful controlled comparisons of pure RL, pure SFT, and multiple hybrid baselines (e.g., RL+SFT loss, LUFFY, ...) on the subset of OpenR1-Math-220k dataset, with Qwen2.5 model. The performance of the model trained with ReLIFT seems to beat all baselines and prior RLvR methods while using less detailed demonstrations.\n* The paper is well-structured and easy to follow. The motivation of the proposed method is illustrated clearly in Section 2 and the introduction of the method is detailed. The reproduction of the method is very likely.\n* The framework is simple, model-agnostic, and shown to generalize across different model sizes and architectures, it’s likely to influence how future reasoning LLMs combine RL and SFT"}, "weaknesses": {"value": "* The proposed method ReLIFT is built upon the assumption that high-quality CoT already exists so that smaller LLMs can benifit (it's actually a form of distillation from large LLMs like DeepSeek-R1). Although the paper also mentions that such high-quality CoT may come from human annotators in line 202, detailed discussion on \"human annotation\" is missing. Therefore, the scope of this paper seems limited to how to efficiently distill from larger LLMs to improve the performance of smaller LLMs, rather than pushing the performance boundary of exisiting LLMs.\n* All main experiments are on five math benchmarks plus a single OOD benchmark (MMLU-Pro). There’s no evaluation on code / STEM or other tasks, which can't fully support the claim that \"ReLIFT is a powerful and resource-efficient paradigm for developing capable **reasoning** models.\"\n* “Hardest” questions are defined as those with rollout accuracy acc(q)=0 given N samples. Those questions are the only ones that enter the buffer then for SFT. This binary cut can misclassify borderline questions (e.g., model answers corretly only once) and depends heavily on sampling noise. There’s no ablation on using thresholds like acc(q) ≤ p, or on how many “hardness levels” might be useful."}, "questions": {"value": "* line 300: The paper mentions that temperature is set to 0.6 in all evaluations. However, for OlympiadBench and MATH500, the paper states that \"For OlympiadBench and MATH500, we use pass@1 as the evaluation metric\". I wonder if this is a reliable evaluation on these two benchmarks, as setting temperature=0.6 indicates that the response from the trained model is not deterministic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lYcMyPzvtQ", "forum": "LzCBLrNoyM", "replyto": "LzCBLrNoyM", "signatures": ["ICLR.cc/2026/Conference/Submission411/Reviewer_z9hW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission411/Reviewer_z9hW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762298449062, "cdate": 1762298449062, "tmdate": 1762915513659, "mdate": 1762915513659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}