{"id": "UK0j7E1f4i", "number": 13409, "cdate": 1758217491710, "mdate": 1762930867893, "content": {"title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?", "abstract": "Existing benchmarks fail to capture a crucial aspect of intelligence: physical reasoning, the integrated ability to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints. To address this gap, we introduce PhyX: the first large-scale benchmark designed to assess models' capacity for physics-grounded reasoning in visual scenarios. PhyX includes 3K meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains: thermodynamics, electromagnetism, mechanics, modern physics, optics, and wave & acoustics. In our comprehensive evaluation, even state-of-the-art models struggle significantly with physical reasoning. GPT-o4-mini, Gemini-2.5-Pro, and GPT-5 achieve only 45.8%, 62.4%, and 65.2% accuracy respectively—performance gaps exceeding 10% compared to human experts. Our analysis exposes critical limitations in current models: over-reliance on memorized disciplinary knowledge, excessive dependence on mathematical formulations, and surface-level visual pattern matching rather than genuine physical understanding. We provide in-depth analysis through fine-grained statistics, detailed case studies, and multiple evaluation paradigms to thoroughly examine physical reasoning capabilities. To ensure reproducibility, we implement a compatible evaluation protocol based on widely-used toolkits such as VLMEvalKit and lmms-eval, enabling one-click evaluation.", "tldr": "", "keywords": ["LMM Evaluation", "Physical Benchmark", "Reasoning Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/11663e3de0b9f40c254399c8cc6c92a219eba44f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces a new benchmark PhyX for physical reasoning with visual components of 3k problems and the results reveal that the physical understanding ability of existing models is still lacking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The dataset collected covers a wide range of physics problems. \n- The text deredundancy step is useful in isolating the multimodal information and avoid information compensation from the text description.\n- The data collection process includes several human verification steps to maintain the quality."}, "weaknesses": {"value": "- The expert background is not well introduced. For example, whether they have different strength under the subdomains in the dataset and whether it is considered in the evaluation.\n- The findings and analysis does not provide enough insights to sharpen the understanding of the boundary of existing LLMs."}, "questions": {"value": "- The worst/medium/best performance of human expert is not explained, is this identical to min/medium/max performance among all participants, or it is related to the expert group? The context seem to indicate the latter but I am not entirely sure. \n- Are the students within the same group able to communicate or collaborate to solve the problem?\n- Are we supposed to transfer the performance of text deredundancy as the approximation to the full-text or text-minimal setting for human expert? Do you have any knowledge about this generalization?\n- In Table 3, the strongest model is able to surpass the medium expert under some settings, can you provide more details regarding this phenomenon?\n- Where are the original source of the problems, and images if introduced elsewhere?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OHzfiFdn4i", "forum": "UK0j7E1f4i", "replyto": "UK0j7E1f4i", "signatures": ["ICLR.cc/2026/Conference/Submission13409/Reviewer_7xzb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13409/Reviewer_7xzb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740474354, "cdate": 1761740474354, "tmdate": 1762924040851, "mdate": 1762924040851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "GwrSaMNzyK", "forum": "UK0j7E1f4i", "replyto": "UK0j7E1f4i", "signatures": ["ICLR.cc/2026/Conference/Submission13409/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13409/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762930157468, "cdate": 1762930157468, "tmdate": 1762930157468, "mdate": 1762930157468, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PHYX, a benchmark for physics grounded reasoning in multimodal settings. It offers 3k unique visual physics problems with both multiple choice and open ended formats, spans six domains and six reasoning types, and provides three text variants to probe text reliance. The authors evaluate many recent LLMs and MLLMs and present error analyses and takeaways about visual grounding and modality fusion. The topic is timely and the dataset could be useful to the community."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Important problem focus. Physical reasoning that integrates perception, symbolic manipulation, and real world constraints is a valuable target for evaluation.\n- Breadth of coverage across six physics domains and six reasoning categories with both MC and OE formats.\n- Three input variants to study redundancy and text dependence.\n- Integration with common eval toolkits and release plan for one click evaluation.\n- Evaluation includes both MLLMs and text only LLMs through captions, which enables cross modality comparisons.\n\nMy recommendation: reject.\n1. Methodological weaknesses in evaluation and judging reduce confidence in the reported gaps and rankings.\n2. Incremental novelty relative to recent physics reasoning benchmarks, with several claims that appear overstated."}, "weaknesses": {"value": "- Novelty claim appears overstated. Several 2025 benchmarks already target physics reasoning with images, for example PhysReason, UGPhysics, SeePhys, PhysUniBench, and others. PHYX is larger and uses some forms of de redundancy, but the claim of first large scale benchmark is not well supported.\n- Human baseline is too small and too weak (table 2 is basically empty for the human baselines). Only 15 students answered 18 questions each, with no per question overlap, no variance estimates, and seemingly only in one setting. This cannot support strong claims about a persistent 10 point human to model gap.\n- Moreover, MC accuracy for GPT-5 reaches 90.9! in test which conflicts with the headline message that \"all models struggle\", yet this is not reconciled with the human comparison.\n- Dataset scale and counts are confusing. The paper alternates between 3k and 6k questions. Table 1 says total new questions 6k yet unique questions 3k, and the text claims 3k problems.\n- Claims of realistic images (a central assertion in the paper, as shown in Fig. 3) are inconsistent with numerous examples that resemble textbook-style drawings. The paper itself later clarifies that these are not photographs. Therefore, the claim that PHYX uses realistic scenes should be moderated or better substantiated. Statements such as “We observe that the images in our dataset are highly realistic, often depicting concrete physical scenarios rather than stylized or abstract illustrations” must be supported, especially when they form part of the paper’s main claims.\n- Evaluation details are under specified. CoT prompting, temperatures, seeds, and retries are not fixed. \n- Filtering out the shortest 10 percent of questions as a quality control step is a weak proxy for difficulty. This is definitely not a \"rigorous process (that) plays a crucial role in maintaining the quality and difficulty of PHYX\". In reality, short questions can be the hardest ones!\n- The paper is generally difficult to read, with long sentences containing many redundant words and unnecessary phrases that do not contribute to the content and serve only as fillers."}, "questions": {"value": "- How was the de redundancy edit validated. Did independent annotators confirm that answers remain unchanged and that difficulty is comparable?\n- See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "H3lUqBfEtY", "forum": "UK0j7E1f4i", "replyto": "UK0j7E1f4i", "signatures": ["ICLR.cc/2026/Conference/Submission13409/Reviewer_cgQS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13409/Reviewer_cgQS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770091861, "cdate": 1761770091861, "tmdate": 1762924040423, "mdate": 1762924040423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PhyX, a benchmark designed to evaluate the physical-reasoning capabilities of MLLMs and LLMs. Itcovers six core physics domains and includes around 3 000 multimodal questions across 6 reasoning types and 25 sub-domains. Experiments show that current strong models achieve only modest accuracy on PhyX, showing a significant gap to humans. The paper also offers fine‐grained analysis from diverse perspectives, which leads to several findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed physical reasoning tasks are indeed important and crucial to model intelligence.\n\n2. The construction process of the benchmark is detailed and reasonable.\n\n3. Experiments are conducted comprehensively, which leads to several findings."}, "weaknesses": {"value": "1. From my understanding, the authors try to equate “physical reasoning” with the ability to solve challenging physics problems, suggesting that a model performing well on such tasks demonstrates strong reasoning capabilities. However, if a model performs well on gravity-related problems, can it generalize the same underlying principles to buoyancy, which essentially involves the same concept but in the opposite force direction? This kind of \"generalization\" or \"learning the core idea\" are also important aspects of physical reasoning.\n\n2. It will be interesting if the authors can compare PhyX with [1], which investigates \"physical reasoning\" from an abstract input format perspective.\n\n3. In a benchmarking paper, while I admit that finding problems is important, it may be good to see some potential ways to mitigate the found issues based on the findings.\n\n[1] Yu M, Liu L, Wu J, et al. The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding[J]. arXiv preprint arXiv:2502.08946, 2025."}, "questions": {"value": "1. Could the authors provide a clear definition of \"physical reasoning\" under the scope of this paper?\n\n2. Could you provide some potential ways (with some results) to mitigate the found issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qbMIDb9uzu", "forum": "UK0j7E1f4i", "replyto": "UK0j7E1f4i", "signatures": ["ICLR.cc/2026/Conference/Submission13409/Reviewer_i2yj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13409/Reviewer_i2yj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931011661, "cdate": 1761931011661, "tmdate": 1762924040087, "mdate": 1762924040087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}