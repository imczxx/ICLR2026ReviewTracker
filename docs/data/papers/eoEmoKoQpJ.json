{"id": "eoEmoKoQpJ", "number": 22115, "cdate": 1758326324300, "mdate": 1759896885721, "content": {"title": "Flow Matching Policy Gradients", "abstract": "Flow-based generative models, including diffusion models, excel at modeling continuous distributions in high-dimensional spaces. In this work, we introduce Flow Policy Optimization (FPO), a simple on-policy reinforcement learning algorithm that brings flow matching into the policy gradient framework. FPO casts policy optimization as maximizing an advantage-weighted ratio computed from the conditional flow matching loss, in a manner compatible with the popular PPO-clip framework. It sidesteps the need for exact likelihood computation while preserving the generative capabilities of flow-based models. Unlike prior approaches for diffusion-based reinforcement learning that bind training to a specific sampling method, FPO is agnostic to the choice of diffusion or flow integration at both training and inference time. We show that FPO can train diffusion-style policies from scratch in a variety of continuous control tasks. We find that flow-based models can capture multimodal action distributions and achieve higher performance than Gaussian policies, particularly in under-conditioned settings.", "tldr": "", "keywords": ["Flow Matching", "Policy Gradient"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b709078b2a23b51e597b0480b3883259308c4a5b.pdf", "supplementary_material": "/attachment/5bba58abf818f61fc5d503aec5c5a0d0ac8acf17.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Flow Policy Optimization (FPO), a novel on-policy reinforcement learning algorithm for training flow-based generative models, particularly diffusion models, as policies. The core innovation is reformulating the policy gradient objective by replacing exact likelihood computations with a ratio derived from the conditional flow matching (CFM) loss. Specifically, FPO uses  a surrogate loss (See Sec. 3.3, 3.4) for the standard PPO likelihood ratio, enabling integration into the PPO-clip framework. The authors demonstrate that this ratio corresponds to optimizing an advantage-weighted evidence lower bound (ELBO), making FPO theoretically grounded while computationally tractable. Importantly, FPO is agnostic to the choice of sampling method during both training and inference, unlike prior denoising MDP approaches. Experiments across GridWorld, 10 MuJoCo Playground tasks, and high-dimensional humanoid control show that FPO successfully trains flow-based policies from scratch. The method demonstrates particular advantages in under-conditioned settings where multimodal action distributions are beneficial, outperforming Gaussian policies. The paper provides both theoretical analysis connecting the CFM objective to ELBO maximization and empirical validation showing FPO achieves competitive or superior performance compared to standard PPO and DPPO baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a genuinely novel approach to combining flow matching with policy gradients. The key insight—using the CFM loss differential as a surrogate for log-likelihood ratios—is elegant and theoretically motivated. Unlike prior work (DDPO, DPPO) that treats the denoising process as an MDP, FPO directly integrates flow matching into the policy gradient framework, avoiding the artificial expansion of the horizon and observation space. The connection to ELBO optimization through existing framework is well-established, and the observation that gradient estimates remain unbiased despite upward bias in the ratio (Equation 18-20) is insightful.\n\n\nMoreover, the paper is well-written with clear motivation. Algorithm 1 provides a concise implementation overview, and the progression from standard PPO to FPO is logical. The GridWorld visualization (Figure 1) effectively demonstrates the multimodal behavior learned by FPO, showing the learned bimodal distribution at the saddle point. The humanoid control results clearly illustrate FPO's advantage in under-conditioned scenarios."}, "weaknesses": {"value": "1. **Bias analysis incomplete**: While the paper shows gradient estimates are unbiased (Eq. 20), the impact of ratio overestimation on actual policy updates is not thoroughly analyzed. How does this bias interact with PPO clipping in practice? The claim that \"clipping mechanism controls magnitude\" needs more rigorous justification—does the clip threshold need adjustment to account for systematic overestimation?\n\n2. **Missing analysis of multimodality**: While GridWorld demonstrates multimodal learning, there's no quantitative analysis. How does the learned distribution compare to the true optimal distribution at saddle points? For humanoid control, are the policies actually multimodal or just higher variance? Entropy measurements or explicit distribution visualization would strengthen these claims.\n\n3. **Reproducibility concerns**: While code release is promised, many implementation details are missing. What network architecture is used for flow models? How is the timestep encoded?"}, "questions": {"value": "1. The paper claims sampling method agnosticism, but experiments only use 10-step Euler integration. Have you validated with DDIM, DPM-Solver, or higher-order methods? Does performance change significantly?\n\n2. In Eq. 12, you decompose $r_{FPO}$ into likelihood ratio × inverse KL gap. Can you provide empirical measurements of how each term evolves during training? Does the KL gap actually decrease?\n\n3. Why is the entropy coefficient set to 0 for FPO (Table A.2) but 0.01 for Gaussian PPO? Doesn't this disadvantage exploration for FPO?\n\n4. For the humanoid under-conditioned experiments, can you provide quantitative measures of policy multimodality (e.g., entropy, number of effective modes) rather than just success rates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6ydfuQnqit", "forum": "eoEmoKoQpJ", "replyto": "eoEmoKoQpJ", "signatures": ["ICLR.cc/2026/Conference/Submission22115/Reviewer_XkTY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22115/Reviewer_XkTY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621861435, "cdate": 1761621861435, "tmdate": 1762942071098, "mdate": 1762942071098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces flow matching policy gradients, a method for optimizing a policy using flow matching.\nThe benefit of this framework is in enabling a more general class of generative policies that do not require a pre-defined policy class (e.g., unimodal gaussian/beta distribution)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The benefit of this framework is in enabling a more general class of generative policies that do not require a pre-defined policy class (e.g., unimodal gaussian/beta distribution).\n\nI see the main benefit of this work shown in the humanoid control section.\nWe know that large NNs generally converge to global optima (\"there's always some decent direction so long as we have some random noise in the system\"). It isn't clear whether continuous control exhibits similar properties or not. \n\nThe mujoco playground experiments are low degrees of freedom and might indeed suffer from local optima, however experiments on the humanoid shows PPO outperform FPO -- this suggests that maybe in the case of very large action spaces we do not suffer from sub-optimal local minima.\n\nThe interesting result is then the ability of FPO to converge to multi-modal solutions when needed. This is shown in the under-specified humanoid problem where the policy receives rewards for the full-body pose but is only conditioned on a subset of these constraints."}, "weaknesses": {"value": "I believe the work should focus more on the generative aspects of the method, as in the humanoid control effort, and less on toy problems.\nThe mujoco playground results are \"nice to have\" they show the method generally works. But the main strength of generative methods is in their ability to model a distribution."}, "questions": {"value": "Does FPO also work in a sparser setting?\nFor example in Tessler 2024 they go beyond root/hands conditioning and also show cases where the constraints are multiple frames into the future. This is a very underspecified and hard problem to solve and would be very impressive if FPO can still tackle it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1Chg5TSRD5", "forum": "eoEmoKoQpJ", "replyto": "eoEmoKoQpJ", "signatures": ["ICLR.cc/2026/Conference/Submission22115/Reviewer_awZj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22115/Reviewer_awZj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763658263, "cdate": 1761763658263, "tmdate": 1762942070791, "mdate": 1762942070791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Flow Policy Optimization (FPO), an on-policy reinforcement learning algorithm that enables training of flow-based generative models (including diffusion models) as policies within the policy gradient framework. The key innovation is replacing the intractable likelihood ratio in PPO with a surrogate ratio computed from conditional flow matching (CFM) losses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1, The motivation is clear and significant. Training flow matching models directly from rewards can greatly popularize their usage to robotics.\n\n2, The evaluation is comprehensive to show the effectiveness of the proposed method on simple robotic tasks (with simulation)."}, "weaknesses": {"value": "1, The baseline is limited. There are existing methods which use direct rewards to weight the trajectory and are agnostic to sampling methods, although most of them are applied to text-to-image generation and other generation tasks, for example, [A]. The author should also implement some of these methods on robotics tasks and conduct simple evaluation, or at least include them as related works and describe the difference.\n\n[A] Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein Regularization"}, "questions": {"value": "1, What is the main difference of your method compared to other reward-weighted methods, empirically speaking?\n\n2, Can the proposed method generalize to scopes other than robotics? Or what could be the domain-specific point?\n\nI am willing to raise my score if all my concerns are well solved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qi6UcjUDX0", "forum": "eoEmoKoQpJ", "replyto": "eoEmoKoQpJ", "signatures": ["ICLR.cc/2026/Conference/Submission22115/Reviewer_bNty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22115/Reviewer_bNty"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878862949, "cdate": 1761878862949, "tmdate": 1762942070544, "mdate": 1762942070544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They propose Flow Policy Optimization (FPO): swap PPO’s likelihood ratio with an ELBO-ratio computed from conditional flow matching (CFM) losses. This lets you train flow/diffusion policies in a PPO-style loop without evaluating exact log-likelihoods. Empirically, they beat Gaussian-PPO and a diffusion-PPO baseline on most MuJoCo Playground tasks, and show robustness on a humanoid control benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Pros:\n- Ratio-as-difference-of-CFM-losses is simple to implement and keeps GAE/GRPO compatibility\n-Clear ablations: effect of #MC samples, ω- vs u-parameterization, clipping sensitivity. Shows robustness under sparse goal conditioning in humanoid."}, "weaknesses": {"value": "Cons:\n- ELBO is not exact likelihood. The ratio decomposes into true likelihood ratio times an inverse KL-gap factor. That second term is policy-dependent and unknown, so the proxy ratio is biased w.r.t. the true PPO ratio"}, "questions": {"value": "Questions:\n- Is there any ablations for choosing different weightings?\n- while you mentioned the method is agnostic to sampler choice, do you observe empirically any difference between SDE/ODE samplers? Does it affect the ratio variance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jEPqoaFXU3", "forum": "eoEmoKoQpJ", "replyto": "eoEmoKoQpJ", "signatures": ["ICLR.cc/2026/Conference/Submission22115/Reviewer_HevE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22115/Reviewer_HevE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979370476, "cdate": 1761979370476, "tmdate": 1762942069999, "mdate": 1762942069999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}