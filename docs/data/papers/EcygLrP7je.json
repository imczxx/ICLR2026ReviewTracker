{"id": "EcygLrP7je", "number": 15128, "cdate": 1758248039125, "mdate": 1763706180805, "content": {"title": "MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search", "abstract": "Large Language Models (LLMs) are increasingly deployed across diverse applications that demand balancing multiple, often conflicting, objectives--such as helpfulness, harmlessness, or humor. Aligning outputs to user-specific preferences in such multi-objective settings typically requires fine-tuning models for each objective or preference configuration, which is computationally expensive and inflexible. We introduce MAVIS - Multi-Objective Alignment via Value-Guided Inference-Time Search - a lightweight inference-time alignment framework that enables dynamic control over LLM behavior without modifying the base model's weights. MAVIS trains a set of small value models, each corresponding to a distinct objective. At inference time, these value models are combined using user-specified weights to produce a tilting function that adjusts the base model's output distribution toward desired trade-offs. The value models are trained using a simple iterative algorithm that ensures monotonic improvement of the KL-regularized policy. We show empirically that MAVIS outperforms baselines that fine-tune per-objective models and combine them post hoc, and even approaches the performance of the idealized setting where models are fine-tuned for a user's exact preferences.", "tldr": "A fine-tuning-free algorithm for aligning LLM outputs to a combination of objectives by approximating the optimal next-token distribution during LLM decoding.", "keywords": ["Multi-Objective", "RLHF", "Large Language Models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd49f39688b1aab21f3b4d0ef7e41c9fc5647220.pdf", "supplementary_material": "/attachment/2c6797595282aadd98d4d5402a05cce5129991b9.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes MAVIS, a value-guided decoding framework that enables a plug-and-play framework for multi-objective alignment. The main technical contribution involves resembling soft policy iteration to better estimate the Q function."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- LLM multi-objective alignment is important.\n\n- The proposed method is overall reasonable."}, "weaknesses": {"value": "- The presentation of this paper requires further refinement. Certain passages are overly informal; for instance, line 431 begins with “Of course…”. Additionally, some concepts are inadequately explained—for example, ζ is not properly defined in Section 3. Figure 2 in the methodology section seems unnecessary. Overall, the paper is a little hard to follow, especially for RL non-experts.\n\n- The motivation of the work is unclear. Previous methods such as GenARM [1] and PARM [2] have already explored test-time multi-objective alignment in LLMs.\n\n- The novelty of the paper is limited. The core contribution revolves around soft policy iteration, which has been explored in prior works. No fundamentally new algorithm is introduced.\n\n- The current comparison set is narrow. The authors should include more comprehensive baselines: (1) For training-based methods: compare against MODPO, RiC, and other SOTA methods. (2) Compare against more methods of model merging. (3) Directly compare with other multi-objective decoding methods such as GenARM and PARM.\n\n- The test set contains only 100 held-out prompts, which may limit robustness. Consider using benchmarks such as AlpacaEval 2 which leverage LLM-judge frameworks to assess helpfulness and alignment more reliably.\n\n- The paper lacks quantitative analysis of the impact of guidance on generation quality, such as fluency and diversity.\n\n- Lack of more analysis: (1) the role of Top-K sampling; (2) the performance change during each iteration when training value functions; (3) the cost analysis to estimate value functions.\n\n- The authors claim that “MAVIS can be applied regardless of whether the weights of π_ref are available.” However, it is unclear how distribution adjustment can be performed without access to these weights.\n\n- The process of training value functions may require more time compared to directly training the policy. Additionally, the training stability remains uncertain, and the performance under long output sequences has not been explored.\n\nReference\n\n[1] GENARM: REWARD GUIDED GENERATION WITH AUTOREGRESSIVE REWARD MODEL FOR TEST-TIME\nALIGNMENT\n\n[2] PARM: Multiobjective test-time alignment via preference-aware autoregressive reward model."}, "questions": {"value": "See weaknessness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BPY8uw6Vhk", "forum": "EcygLrP7je", "replyto": "EcygLrP7je", "signatures": ["ICLR.cc/2026/Conference/Submission15128/Reviewer_8deh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15128/Reviewer_8deh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659654190, "cdate": 1761659654190, "tmdate": 1762925446372, "mdate": 1762925446372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Notice of Revisions to Draft"}, "comment": {"value": "We thank the reviewers for their time and comments. Here we would like to mention that since making our submission, we have obtained new results for MAVIS on the experiments using the Llama 2 7B generative model which outperform the results shown in the original draft. These improvements are primarily attributed to removing the value normalization step during decoding (with $\\beta$ being increased to compensate) and increasing the number of top-$k$ tokens considered. We have updated figures 3, 4, and 5 as well as Table 2 and all tables in Appendix F to reflect the new results. All other changes in our revised draft are pointed out in the individual rebuttals."}}, "id": "jrNXi7oAVI", "forum": "EcygLrP7je", "replyto": "EcygLrP7je", "signatures": ["ICLR.cc/2026/Conference/Submission15128/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15128/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15128/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763706267997, "cdate": 1763706267997, "tmdate": 1763706267997, "mdate": 1763706267997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper *“MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search”* tackles the problem of multi-objective alignment during decoding. Instead of the common *weighted-logit* formulation used in prior work such as MOD, the authors propose to perform **inference-time policy search** guided by a **weighted value function**. The key novelty is that the value function is *reward-based but includes a KL regularization term*, encouraging both objective satisfaction and adherence to the base model distribution. This leads to a more balanced trade-off between competing objectives without retraining or fine-tuning the LLM. Experimental results across diverse datasets, baselines, and language models show that MAVIS achieves superior Pareto fronts and improved controllability, with convincing ablation studies that isolate the contribution of each design choice."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Comprehensive empirical evaluation:** The experiments span multiple datasets, baseline methods, and backbone LLMs, clearly demonstrating that MAVIS consistently outperforms previous approaches and yields clean, interpretable Pareto fronts.  \n- **Strong ablation studies:** The ablations effectively demonstrate how each design choice—value weighting, KL-regularized reward, and inference-time policy search—contributes to the observed improvements.  \n- **Intuitive motivation:** The explanation of why MAVIS outperforms MOD is clear and intuitive, highlighting how weighting the *value function* instead of *logits* better aligns with downstream rewards.  \n- **Principled extension:** The approach refines prior decoding-time alignment methods with subtle yet meaningful changes that improve both interpretability and empirical performance."}, "weaknesses": {"value": "- **Limited theoretical novelty:** The theoretical modifications appear incremental. Earlier methods, such as Satisficing Alignment (Chehade et al., 2025), have also explored weighting value functions rather than logits.  \n- **KL-based value functions are not new:** The idea of including KL regularization in the value formulation has been previously studied, for example in (Zhou et al., 2025).  \n- **Positioning relative to MOD:** The introductory section does not sufficiently distinguish MAVIS from the closely related MOD framework (Shi et al., NeurIPS 2024). Clarifying the conceptual differences early would improve readability.  \n- **Analytical justification:** The paper would benefit from stronger theoretical grounding or complexity analysis of the mentioned bandit-style derivation. Currently, the explanation of computational infeasibility is vague.  \n\n### References\n\n- Shi, Ruizhe, et al. *\"Decoding-time language model alignment with multiple objectives.\"* NeurIPS 37 (2024): 48875–48920.  \n- Chehade, Mohamad, et al. *\"Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time.\"* arXiv preprint arXiv:2505.23729 (2025).  \n- Zhou, Jin Peng, Kaiwen Wang, Jonathan Chang, Zhaolin Gao, Nathan Kallus, Kilian Q. Weinberger, Kianté Brantley, and Wen Sun. *\"q♯: Provably Optimal Distributional RL for LLM Post-Training.\"* arXiv preprint arXiv:2502.20548 (2025)."}, "questions": {"value": "1. Why prefer this specific weighting of value functions over similar approaches such as Satisficing Alignment (Chehade et al., 2025) that also operate on value-based objectives?  \n2. How computationally demanding is learning or estimating the value function in MAVIS?  \n3. Equation (5) is described as “computationally infeasible” — could you provide an analytical interpretation or quantification of this infeasibility?  \n4. Lines 84–86 mention that MOD requires fine-tuning large language models. Does MOD actually modify the model parameters, or does it remain a purely inference-time method?  \n5. How does the performance of MAVIS change when the KL-based term is removed, i.e., when using a reward-only value function?  \n\n### References\n\n- Chehade, Mohamad, et al. *\"Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time.\"* arXiv preprint arXiv:2505.23729 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k2lzQ3sB7B", "forum": "EcygLrP7je", "replyto": "EcygLrP7je", "signatures": ["ICLR.cc/2026/Conference/Submission15128/Reviewer_vozQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15128/Reviewer_vozQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851136796, "cdate": 1761851136796, "tmdate": 1762925445933, "mdate": 1762925445933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAVIS, an inference-time alignment method that enables dynamic multi-objective control of large language models without fine-tuning. The approach trains small value models for each objective and combines them at inference time using user-specified weights to guide token selection. Experimental results on preference datasets show that MAVIS matches or exceeds the performance of baseline methods that require fine-tuning separate models for each objective, while offering greater flexibility and computational efficiency for multi-objective alignment tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a meaningful problem by enabling a base model to adapt to different user preferences at inference time without extensive retraining, which provides significant practical value for deploying language models that can flexibly satisfy diverse user requirements."}, "weaknesses": {"value": "1. A significant limitation is the insufficient motivation, as the core idea of using weighted multi-objective reward models or value functions to influence model logits during decoding for preference-aligned generation has been explored in prior work with similar approaches (e.g., PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive Reward Model). The authors should better articulate what distinguishes their motivation and approach at a fundamental level from existing methods.\n\n2. The experimental evaluation is insufficient as it lacks comparison with recent state-of-the-art baselines\n\n   [1] Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment\n\n   [2] Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EZkPUMcRGj", "forum": "EcygLrP7je", "replyto": "EcygLrP7je", "signatures": ["ICLR.cc/2026/Conference/Submission15128/Reviewer_ZbFM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15128/Reviewer_ZbFM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968149382, "cdate": 1761968149382, "tmdate": 1762925445464, "mdate": 1762925445464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MAVIS — Multi-Objective Alignment via Value-Guided Inference-Time Search — a framework for aligning large language models to multiple, possibly conflicting objectives (e.g., helpfulness, harmlessness, humor) without fine-tuning. MAVIS trains small per-objective value (Q) models that estimate token-level returns under KL-regularization and combines them at inference using user-specified weights to steer decoding. The method provides a monotonic policy-improvement guarantee, integrates with beam or tree search, and empirically outperforms multi-objective fine-tuning baselines such as MORLHF, Rewarded Soups, and MOD on HH-RLHF and Summarize-from-Feedback benchmarks. It enables dynamic preference control at test-time while remaining computationally efficient compared to retraining-based alignment.\n\nConceptual overlap with Transfer Q★:\nThe core idea value-guided, KL-regularized inference-time decoding—is closely related to Transfer Q★ (Chakraborty et al., NeurIPS 2024). While MAVIS extends this to multi-objective settings with learned per-objective value models, the conceptual novelty beyond prior value-guided decoding frameworks could be clarified."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a principled method for multi-objective control of LLM behavior without modifying model weights — a clear step beyond single-objective inference-time methods.\n2. Provides a formal derivation from KL-regularized policy optimization and proves monotonic improvement under iterative value updates, grounding the approach in RL theory.\n3. Requires training only small per-objective value models instead of full fine-tuning, enabling dynamic preference mixing at inference and easy deployment on frozen LLMs.\n4. Empirically Demonstrates superior Pareto fronts compared to MORLHF, Rewarded Soups, and MOD on HH-RLHF and Summarization Feedback datasets."}, "weaknesses": {"value": "1. The evaluation focuses on numerical Pareto-front comparisons but lacks qualitative or human preference assessments. There is no demonstration of smooth or controllable trade-offs between objectives.\n2. The conceptual overlap with Transfer Q★ (NeurIPS 2024).  Both rely on KL-regularized, value-guided inference-time alignment, yet the distinction in mechanism and novelty is not well-articulated.\n3. The method is not generalizable, as it requires to train value model for each reward objective"}, "questions": {"value": "1. Could the authors clarify the conceptual distinction from Transfer Q★ and whether MAVIS can be interpreted as its multi-objective generalization?\n2. Can you Compare Your method with --Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iS9aRgpCBP", "forum": "EcygLrP7je", "replyto": "EcygLrP7je", "signatures": ["ICLR.cc/2026/Conference/Submission15128/Reviewer_YrgS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15128/Reviewer_YrgS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980210540, "cdate": 1761980210540, "tmdate": 1762925444890, "mdate": 1762925444890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}