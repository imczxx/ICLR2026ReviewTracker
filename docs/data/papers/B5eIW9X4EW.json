{"id": "B5eIW9X4EW", "number": 21146, "cdate": 1758314262531, "mdate": 1759896939767, "content": {"title": "STT-LLM: Structural-Temporal Tokenization for Adapting LLMs to Longitudinal Profiles", "abstract": "Large Language Models have shown strong generalization across natural language tasks but remain underexplored for longitudinal biomedical profiles. In sports, biological profiles are analyzed for doping, with particular emphasis on two key challenges for longitudinal data: (i) sequence prediction for early detection of prohibited substance use, and (ii) anomaly detection for identifying doping-related deviations. We propose STT-LLM, a structural-temporal tokenization framework that adapts LLMs to longitudinal analysis without modifying the backbone architecture. STT-LLM constructs joint embeddings that capture both temporal dynamics and biological pathway-based interactions, which are then transformed into LLM-compatible tokens through the specialized structural and temporal tokenizers. We evaluate our approach on real-world longitudinal steroid datasets from athletes, where STT-LLM consistently outperforms LLM baselines. In addition, we present a case study where STT-LLM provides contextual reasoning that aligns more closely with expert assessments compared to baseline models. These results highlight the effectiveness of embedding-guided tokenization for adapting LLMs to understand longitudinal biological data.", "tldr": "", "keywords": ["Structural-Temporal Embedding", "Longitudinal Biomedical Profiles", "LLM Tokenization", "Sports Doping"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6815144f4a271035719d21ae01292c2704b85a6b.pdf", "supplementary_material": "/attachment/d25dde6204fc52213beccec8c7841a67d72e92b0.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces STT-LLM, a structural-temporal tokenization framework that adapts frozen large language models (with LoRA adapters) to longitudinal biomedical profiles by turning pathway-aware, time-dependent measurements into LLM-compatible tokens. It builds joint embeddings that capture metabolite interaction structure (via Laplacian eigenvectors) and temporal dynamics (via attention with positional encodings), then feeds these through dedicated structural and temporal tokenizers whose outputs are concatenated with the standard text prompt before LLM inference. The method targets two tasks in anti-doping analytics—sequence prediction for early detection and anomaly detection—and is evaluated on four real-world longitudinal steroid datasets (male/female and limited-sample variants). Compared to several 7–8B LLM baselines fine-tuned with their native tokenization, STT-LLM reports lower forecasting errors and higher anomaly-detection sensitivity/AUC in zero- and few-shot settings, with ablations indicating that both the structural and temporal tokenizers contribute materially. A small case study with expert-verified profiles further suggests improved contextual reasoning and faster inference under the same hardware budget."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s strengths are primarily methodological and integrative: it offers a clear, modular way to turn structured, time-varying biomedical profiles into tokens that a frozen LLM can use, combining pathway-aware structural embeddings with temporal encodings rather than forcing everything through plain text. On originality, the structural-temporal tokenization bridges graph-informed representations and sequence modeling inside an LLM framework with lightweight adapters, which is a nontrivial combination for longitudinal settings. Quality is supported by a reasonably principled construction (Laplacian-based structure, attention for time), an end-to-end pipeline that slots into off-the-shelf 7–8B models, and ablations indicating both structural and temporal components matter. Clarity is good: the data-to-token flow and how tokens are concatenated with the prompt are explained in a way that seems straightforward to reimplement. In terms of significance, demonstrating zero-/few-shot gains on real anti-doping datasets suggests practical promise for early detection and anomaly screening, and the design appears portable to other longitudinal biomedical profiles beyond the specific case study."}, "weaknesses": {"value": "The evaluation is narrow and domain-specific, making it hard to judge generality: results are limited to four anti-doping datasets with closely related measurement spaces, few seeds, and primarily LLM baselines rather than strong time-series or graph-temporal models, so it’s unclear whether the gains come from the tokenization itself or from weaker comparators; adding competitive baselines (e.g., modern TS transformers and graph-temporal forecasters) and more seeds would strengthen claims. The construction choices need deeper justification and sensitivity: how many Laplacian eigenvectors are used, how pathway graphs are defined and updated over time, how token lengths scale with visit count, and how robust performance is to noisy or missing edges; reporting these ablations alongside compute/memory overhead would clarify practicality. Finally, clinical utility remains speculative: metrics tied to early-warning lead time, false-alarm rates, and cross-lab generalization, plus external validation and clear guidelines for privacy handling of longitudinal health data, would make the case more convincing."}, "questions": {"value": "- Evaluation scope and baselines: could you broaden beyond only 7–8B LLM comparators to include strong time-series and graph-temporal baselines (e.g., purpose-built TS transformers and GNNs for irregular clinical series) and run more seeds with confidence intervals?\n\n- Design choices and sensitivity: can you detail and ablate key tokenization decisions (how many Laplacian eigenvectors; how pathway graphs are defined/validated; how token length scales with visits; handling of missing/noisy edges), and report compute/memory overhead of S/T tokenizers and LoRA under increasing sequence lengths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0xRscx8y4Y", "forum": "B5eIW9X4EW", "replyto": "B5eIW9X4EW", "signatures": ["ICLR.cc/2026/Conference/Submission21146/Reviewer_orWt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21146/Reviewer_orWt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761245815273, "cdate": 1761245815273, "tmdate": 1762941499934, "mdate": 1762941499934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes STT-LLM, a novel framework for adapting pre-trained Large Language Models (LLMs) to analyze longitudinal biomedical data, which is often numerical, irregularly sampled, and possesses complex underlying dependencies. The core contribution is a specialized tokenization strategy that processes structural and temporal information in parallel. A structural tokenizer encodes domain knowledge (e.g., metabolic pathways) from a feature interaction graph, while a temporal tokenizer captures the time-series dynamics. These specialized embeddings are then projected into the LLM's native token space, enabling a frozen LLM (fine-tuned with LoRA) to perform tasks on this data modality. The authors demonstrate their method on the specific use case of detecting doping in athletes' longitudinal steroid profiles, showing improved performance over standard LLM baselines in sequence prediction and anomaly detection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Relevant Problem: The paper tackles the important and challenging problem of adapting the powerful capabilities of LLMs to complex, structured, and numerical time-series data, a domain where these models are not natively suited.\n\nIntuitive Architecture: The proposed architecture is well-motivated and clearly presented. The idea of creating separate, specialized information streams for structural and temporal features before projecting them for the LLM is a reasonable and intuitive approach. The model diagram in Figure 1 is particularly effective."}, "weaknesses": {"value": "Fundamentally Flawed Experimental Design: The paper's central claim that its tokenization strategy is superior is not scientifically validated by its experiments. The evaluation compares STT-LLM (using an unstated LLM backbone with the proposed tokenizer) against other LLMs (e.g., Llama-3) using their native text tokenizers. This is a confounded comparison (Backbone_A + Tokenizer_STT vs. Backbone_B + Tokenizer_Text). Any observed performance difference could be due to the choice of backbone model rather than the tokenization strategy. The paper fails to perform the essential apples-to-apples comparison needed to isolate the effect of its core contribution.\n\nCritically Insufficient Baselines: The experimental evaluation lacks the necessary context to judge the method's practical value.\n\nNo Simple Baselines: The paper omits standard statistical (e.g., ARIMA) or simple ML baselines (e.g., linear models). Without these, it's impossible to know if the proposed complex LLM-based approach offers any real advantage over trivial or well-established methods.\n\nNo Specialized SOTA Baselines: For tasks like multivariate time-series forecasting and anomaly detection, there exist highly specialized and powerful models (e.g., graph-based Transformers). The paper avoids comparing against these likely superior models, justifying this by their incompatibility with \"LLM inference pipelines,\" which is not a sufficient reason to exclude them from a performance benchmark.\n\nNo Relevant Foundation Model Baselines: The comparison overlooks powerful foundation models designed for numerical tabular data that have been successfully applied to time-series, such as TabPFN. This is a highly relevant baseline that operates on numerical data directly and would provide a much stronger point of comparison.\n\nLack of Transparency and Reproducibility:\n\nThe specific LLM backbone used for the proposed STT-LLM model is never stated, making the work impossible to reproduce or fairly evaluate.\n\nThe exact text serialization format used to feed the longitudinal profiles to the baseline LLMs is not provided, preventing an assessment of whether the baselines were configured fairly.\n\nSeveral key results, including the main ablation study in Table 4 and the few-shot sequence prediction results in Table 2, are presented without error bars or standard deviations, making it impossible to assess the statistical significance of the reported gains.\n\nOverstated Claims and Limited Scope:\n\nThe performance improvements on the sequence prediction task are marginal (often <1% in RMSE) and likely not statistically or clinically significant.\n\nThe paper makes broad claims about \"longitudinal biomedical profiles,\" but the evaluation is confined to a single, niche application (doping detection). This lack of diversity in tasks and datasets fails to support the claims of general applicability."}, "questions": {"value": "Experimental Design: Could you please clarify which LLM backbone was used for the STT-LLM model? To properly validate your core contribution, would it be possible to provide results from an apples-to-apples comparison, for instance, by evaluating Llama-3 with your STT tokenizer against Llama-3 with text flattening?\n\nReproducibility: Could you provide the exact text serialization format used to present the longitudinal data to the baseline LLMs? Furthermore, could you add standard deviations or error bars to the results in Tables 2 and 4 to allow for an assessment of significance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FoPw6YwkoV", "forum": "B5eIW9X4EW", "replyto": "B5eIW9X4EW", "signatures": ["ICLR.cc/2026/Conference/Submission21146/Reviewer_KYho"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21146/Reviewer_KYho"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408682449, "cdate": 1761408682449, "tmdate": 1762941499297, "mdate": 1762941499297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The framework introduces a structural–temporal embedding pipeline combining spectral graph features from the normalized Laplacian with transformer-style attention and positional encodings, producing unified embeddings for longitudinal profiles. Two specialized tokenizers map these embeddings into the LLM's token space, enabling adaptation via LoRA while preserving the frozen backbone. The approach targets sequence prediction and anomaly detection (local sample-level and global profile-level) in anti-doping monitoring, with evaluation across four real-world steroid datasets. Results show consistent improvements over LLM baselines (Qwen-2.5, Falcon-3, Mistral, LLaMA-2/3.1, Phi-4, DeepSeek-R1) across RMSE/MAE/MAPE and detection metrics, supported by ablations demonstrating the necessity of both tokenizers and the embedding layer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly frames the core challenge - LLMs' discrete tokenization conflicts with multivariate, irregular, structurally linked longitudinal data - and positions tokenization as the adaptation mechanism rather than requiring architectural redesign. The modular combination of graph-based structural embeddings, attention-based temporal embeddings, dual tokenizers, and LoRA proves computationally efficient and compatible with general LLMs. \n\nEmpirical evaluation spans zero-shot and few-shot performance across four datasets with realistic data scarcity and irregularity, covering both prediction and detection tasks while enforcing strict domain constraints on specificity. Comprehensive ablations and hyper-parameter studies validate design choices, while the expert case study and UMAP visualization (Fig 4) provide interpretability and practical relevance."}, "weaknesses": {"value": "The baseline comparisons exclude strong non-LLM alternatives such as GNNs over metabolic graphs or purpose-built irregular time-series transformers and NeuralODE models, limiting the scope of performance claims to LLM-only comparisons. \n\nSeveral absolute metrics lack contextualization and details (Table 2) of how they are defined and computed. Local anomaly sensitivity remains low in zero-shot despite improvements (Table 3). \n\nThe counterintuitive increase in error with more shots for many models suggests that the model struggles with more structural complexity and longer temporal dependencies (Table 2).\n\nWhile the design is general, experiments focus exclusively on steroid modules; additional biomedical longitudinal domains would strengthen the claims."}, "questions": {"value": "* Can you include comparisons to at least one non-LLM baselines (GNNs over metabolic graphs, NODE)?\n* What are the units and scales of metabolites and targets? Can you provide normalization details and per-metabolite error decomposition?\n* How are the few-shot examples selected?\n* What token count do structural/temporal tokenizers add for typical profiles, and how does this scale with longer histories?\n* Have you tested transfer to other longitudinal clinical datasets without redesigning graphs? How sensitive is performance to graph misspecification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iLqMmrsjLd", "forum": "B5eIW9X4EW", "replyto": "B5eIW9X4EW", "signatures": ["ICLR.cc/2026/Conference/Submission21146/Reviewer_RyTB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21146/Reviewer_RyTB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935276714, "cdate": 1761935276714, "tmdate": 1762941498257, "mdate": 1762941498257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The framework introduces a structural–temporal embedding pipeline combining spectral graph features from the normalized Laplacian with transformer-style attention and positional encodings, producing unified embeddings for longitudinal profiles. Two specialized tokenizers map these embeddings into the LLM's token space, enabling adaptation via LoRA while preserving the frozen backbone. The approach targets sequence prediction and anomaly detection (local sample-level and global profile-level) in anti-doping monitoring, with evaluation across four real-world steroid datasets. Results show consistent improvements over LLM baselines (Qwen-2.5, Falcon-3, Mistral, LLaMA-2/3.1, Phi-4, DeepSeek-R1) across RMSE/MAE/MAPE and detection metrics, supported by ablations demonstrating the necessity of both tokenizers and the embedding layer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper clearly frames the core challenge - LLMs' discrete tokenization conflicts with multivariate, irregular, structurally linked longitudinal data - and positions tokenization as the adaptation mechanism rather than requiring architectural redesign. The modular combination of graph-based structural embeddings, attention-based temporal embeddings, dual tokenizers, and LoRA proves computationally efficient and compatible with general LLMs. \n\nEmpirical evaluation spans zero-shot and few-shot performance across four datasets with realistic data scarcity and irregularity, covering both prediction and detection tasks while enforcing strict domain constraints on specificity. Comprehensive ablations and hyper-parameter studies validate design choices, while the expert case study and UMAP visualization (Fig 4) provide interpretability and practical relevance."}, "weaknesses": {"value": "The baseline comparisons exclude strong non-LLM alternatives such as GNNs over metabolic graphs or purpose-built irregular time-series transformers and NeuralODE models, limiting the scope of performance claims to LLM-only comparisons. \n\nSeveral absolute metrics lack contextualization and details (Table 2) of how they are defined and computed. Local anomaly sensitivity remains low in zero-shot despite improvements (Table 3). \n\nThe counterintuitive increase in error with more shots for many models suggests that the model struggles with more structural complexity and longer temporal dependencies (Table 2).\n\nWhile the design is general, experiments focus exclusively on steroid modules; additional biomedical longitudinal domains would strengthen the claims."}, "questions": {"value": "* Can you include comparisons to at least one non-LLM baselines (GNNs over metabolic graphs, NODE)?\n* What are the units and scales of metabolites and targets? Can you provide normalization details and per-metabolite error decomposition?\n* How are the few-shot examples selected?\n* What token count do structural/temporal tokenizers add for typical profiles, and how does this scale with longer histories?\n* Have you tested transfer to other longitudinal clinical datasets without redesigning graphs? How sensitive is performance to graph misspecification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iLqMmrsjLd", "forum": "B5eIW9X4EW", "replyto": "B5eIW9X4EW", "signatures": ["ICLR.cc/2026/Conference/Submission21146/Reviewer_RyTB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21146/Reviewer_RyTB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935276714, "cdate": 1761935276714, "tmdate": 1763760181806, "mdate": 1763760181806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}