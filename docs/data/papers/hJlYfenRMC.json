{"id": "hJlYfenRMC", "number": 19150, "cdate": 1758293874201, "mdate": 1763119824089, "content": {"title": "MaMe: Matrix-Based Token Merging", "abstract": "We introduce MaMe, a training-free, differentiable token merging method that relies entirely on matrix operations to accelerate vision transformers. When applied to pre-trained models, MaMe doubles ViT-B@224 throughput with a mere 2\\% drop in accuracy. For training from scratch, a ViT-T model with MaMe achieves 1.94x throughput with a 1.3\\% accuracy drop. As a downsampling layer in Swin architectures, MaMe reduces FLOPs by 2.4x for Swin-S backbones, achieving 47.0\\% mIoU on ADE20K semantic segmentation. In SigLIP2-B@512 zero-shot classification, MaMe provides 1.3Ã— acceleration with negligible performance degradation (78.02 vs. 78.37). For multimodal reasoning, MaMe accelerates LLaVA-v1.5-7B inference by 36\\% on MME with minimal degradation (31.40 vs. 32.76). In video tasks, MaMe accelerates VideoMAE-L by 48.5\\% on Kinetics-400 with a 0.84\\% accuracy loss. Collectively, these results demonstrate MaMe's effectiveness in accelerating transformer-based vision and multimodal models.", "tldr": "We propose a training-free, plug-and-play, fully matrix-based, and end-to-end differentiable token merging method to accelerate the inference and training of Vision Transformers.", "keywords": ["efficient transformer; token merging; matrix operation; inference speed; training speed; off-the-shelf"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/fae8950561ef74ae1fd15809a1906969b256b4f2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the computational inefficiency of Vision Transformers by proposing MaMe, a training-free token merging method based entirely on matrix operations. The core method involves partitioning tokens into source and destination sets, computing cosine similarity matrices, applying adaptive weight pruning, and aggregating tokens while preserving unique ones. Experimental results across image classification, semantic segmentation, multimodal reasoning, and video tasks demonstrate MaMe's effectiveness in accelerating diverse transformer-based models with minimal performance degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. MaMe eliminates non-differentiable operations through matrix-based similarity computation and adaptive pruning, enabling end-to-end training compatibility.\n\n2. MaMe introduces no learnable parameters, maintaining original model complexity and allowing direct integration into pre-trained models without retraining.\n\n3. The method relies on matrix operations with O(MN) complexity per layer, avoiding costly clustering algorithms like k-means, and scales to large models and high-resolution inputs.\n\n4. I really appreciate that the evaluations are quite comprehensive across different domains."}, "weaknesses": {"value": "1. Despite several improvement, the paper seems to be incremental compared to ToMe, which greatly reduced the novelty of the proposed MaMe. Also, the baselines are quite few and out-of-date, and more recent baselines are supposed to be incorporated.\n\n2. Despite the promising O(MN) complexity, mainstream method like ToMe, ATS and so on also does not rely on cluster, so this contribution would be somewhat overclaimed form my humble perspective.\n\n3. The results are not impressive. In table 3 and table 4, the improvement is marginal and in table 6, the reduction in  evaluation time seems at the cost of declining accuracy. More experiments are supposed to be provided to further enhance the method.\n\n4. The computational cost of matrix operations (e.g., similarity calculation) is not quantified relative to baseline attention, potentially underestimating overhead for large token counts.\n\n5. The greedy token partitioning strategy and the fusion matrix lack theoretical guarantees on optimality compared to combinatorial search, and the impact of partition choices is empirically evaluated but not analytically derived.\n\n6. While alternating partition is default, the paper does not ablate other partitioning schemes."}, "questions": {"value": "1.  Would theoretical analysis strengthen the justification for the alternating partition approach and the fusion matrix?\n\n2. Why FLOPs for MaMe is not reported in table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UjqTdMTF2i", "forum": "hJlYfenRMC", "replyto": "hJlYfenRMC", "signatures": ["ICLR.cc/2026/Conference/Submission19150/Reviewer_rKrx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19150/Reviewer_rKrx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760958303455, "cdate": 1760958303455, "tmdate": 1762931165256, "mdate": 1762931165256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "MaMe might not be able to attend ICLR. It's sad. But I still think people will like MaMe in the future. MaMe is the best."}}, "id": "zCTZs7Lmo4", "forum": "hJlYfenRMC", "replyto": "hJlYfenRMC", "signatures": ["ICLR.cc/2026/Conference/Submission19150/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19150/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119822901, "cdate": 1763119822901, "tmdate": 1763119822901, "mdate": 1763119822901, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Matrix-Based Token Merging, a token merging mechanism for vision transformers and related approaches, aiming to increase throughput while being quality neutral. The authors propose bifurcating tokens into source and destination ones and construct a non parametric weight matrix that updates the destination tokens based on the cosine similarity between the two sets. They demonstrate efficiency driven results on tasks like image classification, action recognition, zero-shot metrics on standard datasets and benchmark against some other related efficient methods like ToMe."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose a unique approach to token merging -- bringing attention-like ideas between self-defined source and destination tokens to update destination tokens and supress source tokens. \n- The authors present a comprehensive literature survey, clearly illustrating the previous relevant works and their shortcomings\n- The authors have a well presented section on methodology (barring some concerns listed in the weaknesses section of my review)\n- The authors prepare and present a comprehensive set of results on various benchmarks, clearly depicting relevant metrics like throughput, accuracy, etc."}, "weaknesses": {"value": "Please note that this section includes questions too.\n\n- The results indicate sharp drops in accuracy across various benchmarks, even though it comes at a throughput increase. \n  - For instance, in Table 1, ViT-B (Deit) with MaMe improves over the baseline in terms of throughput, but at an accuracy cost, which when compared with the baseline for ViT-S (Deit), the throughput is lower at a neutral accuracy. \n  - Similarly, Table 3 indicates that even when training with the MaMe setup from scratch, the performance remains quite low as compared to the baseline variants for ViT and SWIN.\n  - Similar observation for segmentation in Table 4\n  - Why is the throughput increase so small for action recognition (K6400) in Table 5.\n\n- The methodology has some unclear things, and is not backed up by ablations well. \n  - Why is random, alternating or any other bifurcation intuitively logical for separating source and destination tokens, other than empirical evidence.\n  - It is not explained why there are two rounds of filtering and normalization, once at equation 3, and then again at equation 7.\n  - Why is equation 6 augmented with an $\\epsilon$ which might lead to an inaccurate computation of the count, why not do $\\sum_i W_{ij} > 0$.\n  - $X_{pres}$ in equation 10 is not defined earlier. It is not clear why or how this is constructed. Similarly, the \"merge\" operation in equation 11 is not well defined.\n  - Several of these design choices are not intuitively obvious and need ablations to support why these steps are necessary or contribute to the method's effectiveness.\n\nOn a overall level, the paper will benefit from a more thorough treatment of the methodology, justifying the design choices, and working on stronger accuracy results while attempting to improve throughput."}, "questions": {"value": "Please see the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hSFYcHYWrJ", "forum": "hJlYfenRMC", "replyto": "hJlYfenRMC", "signatures": ["ICLR.cc/2026/Conference/Submission19150/Reviewer_7A9g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19150/Reviewer_7A9g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489775275, "cdate": 1761489775275, "tmdate": 1762931164696, "mdate": 1762931164696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MaMe, a method to merge tokens in Vision Transformers (ViTs) to improve efficiency. The paper claims that this method brings four key innovations: (1) it is fully differentiable to allow ViTs to be trained better from scratch, (2) it uses efficient matrix operations unlike existing approaches, to achieve a larger speedup, (3) it is parameter-free, simplifying deployment and reducing complexity, and (4) it can be applied directly to pre-trained ViTs. MaMe works by dividing tokens into two sets A and B, calculating the cosine similarity with all tokens in set B for each token in set A, and merging tokens from set B using a weighted average if their cosine similarity is above a certain threshold. With experiments on image classification, applying MaMe directly to pre-trained ViTs, MaMe is shown to achieve a better speed-accuracy balance than existing token reduction approaches. When training a ViT with MaMe from scratch, the model performs worse than when applying MaMe to pretrained models. MaMe achieves mixed results on other tasks. It achieves a comparable speed-accuracy trade-off to existing token reduction methods for action recognition and several vision-language tasks, but causes large accuracy drops when applied to semantic segmentation models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. By focusing on token reduction in ViTs, the paper tackles a relevant problem. If an effective strategy is found to reduce the number of tokens processed by a ViT without causing a drop in accuracy, then the efficiency of the ViT would improve without compromising task performance. This could make the ViT model more suitable for various applications.\n\n2. The proposed method, MaMe, obtains a better speed-accuracy balance than existing token merging approaches in several settings. As shown in Tab. 1, MaMe obtains a higher accuracy and throughput than existing approaches EViT, ToMe and DiffRate across various model sizes and training strategies (DeiT & MAE). Similar improvements are obtained for zero-shot image classification in Tab. 2.\n\n3. The visualization of the progression of token reduction in Fig. 2 provides interesting insights in the effect of the token merging operations across different ViT layers. The visualization shows that MaMe is effective in merging tokens that belong to similar semantic categories, which is the intended behavior."}, "weaknesses": {"value": "1. There is no text in Sec. 4.5 (titled Ablation Study). There are several figures and tables with additional results (Fig. 3, Fig. 4, Tab. 7), but these are not referred to in the paper. As a result, it is not clear what can be seen in these figures and tables, what the significance is of these results, and what conclusions can be drawn from them. Overall, the 'empty' subsection harms the presentation and readability of the paper.\n\n2. The technical innovation of the proposed MaMe method is limited. In its core, MaMe is a slightly more elegant, matrix-based implementation of methods that merge tokens when the cosine similarity of token embeddings is above a certain threshold, such as ALGM [a] for semantic segmentation. Like MaMe, the global merging module of ALGM also splits up tokens into two sets (like ToMe [b]), then compares each token in set A with each token in set B, and merges tokens if their cosine similarity is above a certain threshold. Like MaMe, ALGM and ToMe are also parameter-free and 'plug-and-play', so these claimed key innovations (L055-L069) are not unique. Comparing ALGM and MaMe, the only key operational difference is in the way the merged token embeddings are obtained, taking a simple average in ALGM and using a weighted average and summing with 'destination tokens' in MaMe. To properly demonstrate the value of MaMe, and show that the small nuanced differences between MaMe and existing methods constitute meaningful and effective innovations, the paper could (a) explain in text how MaMe differs conceptually from existing methods like ALGM and ToMe, and (b) compare quantitatively to existing threshold-based merging methods like ALGM. \n\n3. The paper does not demonstrate the positive impact of the fact that MaMe is fully differentiable, while this is a key contribution/innovation of the paper. The claim (L057-L060) is that MaMe being fully differentiable makes training from scratch more optimal. However, comparing Tab. 3 to Tab. 1, when training a ViT from scratch with MaMe, the accuracy drop from the baseline is significantly larger than when applying MaMe directly to a pre-trained ViT. In contrast, 'non-differentiable' method ToMe [b] *performs better* when trained from scratch, as shown in their original paper. These findings all contradict the claim/hypothesis that MaMe being fully differentiable is beneficial for training from scratch. Since this is one of the key contributions of the paper, this is a considerable weakness. To improve this, the claim should either be amended or supported with other experimental results.\n\n4. The abstract (L015) claims that Swin-S with MaMe obtains 47.0 mIoU on ADE20K, but this number does not correspond with the results from Tab. 4 (35.8 mIoU and 47.0 mAcc). Thus, this claim is unjustified and should be amended.\n\n5. Applying MaMe to Swin or Iwin leads to very large drops in accuracy. This is the case for both image classification (Tab. 3) and semantic segmentation (Tab. 4). These large accuracy drops make that MaMe will not be useful in those settings, in practice. These results show that the presented approach is not generally applicable to various Transformer-based backbones, which reduces the value of the work.\n\n6. The paper does not explain why the number of parameters decreases when applying MaMe to Swin and Iwin, in Tab. 3 and Tab. 4. If I understand the paper correctly, MaMe only reduces the number of tokens that are processed by the model. How is it possible that the number of model parameters also decreases as a result? This should be explained.\n\n7. The paper does not describe how it handles batching, in case different numbers of tokens are merged for different images in a batch. For instance, in Tab. 1, MaMe uses a threshold of 0.8 (L244), which can result in a different number of tokens being merged for each image, and the experiment uses a batch size of 1024 (L350). How are images batched in case the resulting number of tokens per image is different, due to threshold-based merging? Is any padding being used, which would negate some of the effect of token reduction? This should be explained.\n\n8. Several implementation details are not provided, which harms reproducibility. For instance, the paper does not mention (a) how the *source* and *destination* token sets are obtained (L148 only mentions that the strategy can very), and (b) what the hyperparameters are (learning rate, batch size, optimizer, etc.) for training the models in Tab. 3 and Tab. 4 from scratch. It would help if the paper had a separate subsection/paragraph in which these implementation details are specified.\n\n9. Tab. 1 does not provide the FLOPs results for the proposed MaMe method, while it does provide these results for the baseline and other token reduction methods. This means that there is not a full picture of the efficiency of MaMe compared to existing work, making it difficult to fully judge its performance.\n\n[a] Norouzi et al., ALGM: Adaptive Local-then-Global Token Merging for Efficient Semantic Segmentation with Plain Vision Transformers, CVPR 2024.\n[b] Bolya et al., Token Merging: Your ViT But Faster, CVPR 2023."}, "questions": {"value": "As the paper has a considerable number of weaknesses (see above) and relatively minor strengths, I give a *reject* rating. Some of the issues could be fixed relatively easily (e.g., by having a proper ablation study and providing implementation details), but most other issues are more fundamental (e.g., the limited technical innovation of the method and the poor performance in some settings). In the 'weaknesses' section above, I have provided several suggestions to resolve these issues. Beyond the suggestions and questions posed there already, I have no additional questions at this point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dJwNJv2JB4", "forum": "hJlYfenRMC", "replyto": "hJlYfenRMC", "signatures": ["ICLR.cc/2026/Conference/Submission19150/Reviewer_qhk5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19150/Reviewer_qhk5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812413397, "cdate": 1761812413397, "tmdate": 1762931164039, "mdate": 1762931164039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel, training-free method for reducing the computational cost of Vision Transformers (ViTs) by merging similar tokens. The core innovation is a differentiable, parameter-free algorithm that uses efficient matrix operations for token fusion, addressing important limitations of existing methods like non-differentiability, added complexity, and reliance on clustering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method avoids clustering or costly combinatorial matching by using dense matrix ops, which is attractive for implementation and throughput.\n\nThe module is plug-and-play (no extra learnable parameters), can be applied to pre-trained models or used during training, and is evaluated across several tasks (image classification, segmentation, video, etc.)"}, "weaknesses": {"value": "The paper strongly emphasizes being \"fully differentiable\" as a major advantage, enabling \"seamless end-to-end training\" and learning optimal merging strategies from scratch. However, the core merging process itself, as described in the methodology, appears to be based on a fixed, pre-defined similarity threshold (tau) and a fixed partitioning strategy. It is not immediately clear what is being learned during end-to-end training if these hyperparameters are static."}, "questions": {"value": "Regarding learning, does the model learn better token representations that are more amenable to merging at a fixed tau? Or is the threshold also learned?\n\nIn the result tables (1-3), it is not always clear whether baselines were re-run under identical settings (batch size, precision, device) or whether numbers are taken from original papers. Please explicitly state for each baseline whether you: (a) ran it yourself with the same environment, (b) used reported numbers, or (c) used an implementation with matched measurement settings. To further improve reproducibility, it would be useful to provide scripts or a reproducibility table."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2Xd5aXg9BC", "forum": "hJlYfenRMC", "replyto": "hJlYfenRMC", "signatures": ["ICLR.cc/2026/Conference/Submission19150/Reviewer_1TaD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19150/Reviewer_1TaD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928075998, "cdate": 1761928075998, "tmdate": 1762931163512, "mdate": 1762931163512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}