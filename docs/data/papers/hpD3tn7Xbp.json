{"id": "hpD3tn7Xbp", "number": 6740, "cdate": 1757994096503, "mdate": 1759897897682, "content": {"title": "AlignCLIP: Self-Guided Alignment for Remote Sensing Open-Vocabulary Semantic Segmentation", "abstract": "Open-Vocabulary Semantic Segmentation (OVSS) for remote sensing imagery plays a crucial role in applications such as land cover mapping and environmental monitoring. Recently, Contrastive Language-Image Pre-training (CLIP) has advanced the *training-free* paradigm of OVSS while also inspiring its exploration in the remote sensing domain. However, directly applying CLIP to remote sensing leads to cross-modal mismatches. Prevalent methods focus on exploring attention mechanism of CLIP visual encoder or introducing vision foundation models to obtain more discriminative feature, but they often overlook the alignment between patches and textual representations. To address this issue, we propose a *training-free* framework named **AlignCLIP**. We find that, objects of the same category tend to exhibit a more compact distribution in remote sensing, this enables a single visual feature to effectively represent all objects within the category. Based on this observation, we design the *Self-Guided Alignment (SGA)* module, which leverages the most reliable image-specific visual prototypes to refine the text embeddings. To mitigate interference among irrelevant features, we further introduce the *Cluster-Constrained Enhancement (CCE)* module, which clusters semantically similar patch features, suppresses inter-cluster correlations, and updates the logits map via a constraint propagation mechanism. Experiments on eight remote sensing benchmarks demonstrate that AlignCLIP consistently outperforms state-of-the-art *training-free* OVSS methods, achieving an average gain of +2.2 mIoU and offering a robust adaptive solution for open-vocabulary semantic segmentation in remote sensing. All code will be released.", "tldr": "", "keywords": ["Vision-Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14e30ce8b0285f09fba82b555fd5dec088160fa4.pdf", "supplementary_material": "/attachment/894256184b0f5bcbf005f40314eae9150d36c06c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes AlignCLIP, a fully training-free framework for open-vocabulary semantic segmentation (OVSS) in remote sensing imagery. Observing that remote sensing objects of the same category exhibit compact intra-class feature distributions, the authors design two lightweight modules to address cross-modal mismatches between image patches and textual embeddings. The Self-Guided Alignment (SGA) module refines text embeddings using the most reliable image-specific visual prototypes, while the Cluster-Constrained Enhancement (CCE) module groups semantically similar patches, suppresses inter-cluster noise, and updates the logits map via constrained propagation. Without requiring retraining or reference sets, AlignCLIP integrates seamlessly with SegEarth-OV’s upsampling module and achieves state-of-the-art performance across eight benchmark datasets, outperforming prior training-free methods by an average of +2.2 mIoU, with strong generalization and interpretability supported by extensive ablation and qualitative analyses."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clearly defined and well matched to the proposed SGA design; the paper convincingly demonstrates how the method directly addresses the identified issue.\n2. Extensive experiments across multiple benchmarks show consistent performance improvements, supporting the technical soundness of the approach.\n3. The adaptation to remote sensing imagery is well justified; using in-image visual references is an effective and elegant idea that leverages the unique characteristics of this domain.\n4. The paper is well-organized, with a logical flow and clear explanations that make the technical ideas easy to follow."}, "weaknesses": {"value": "1. The CCE module leverages visual feature maps and self-attention matrices from vision foundation models to constrain clustering, but similar strategies have already been widely explored in prior works. Moreover, its design does not closely connect to the paper’s main motivation of alleviating cross-modal mismatches.\n2. Although the SGA module aligns well with the paper’s motivation, its individual improvement over the baseline is relatively small. The major performance gain (+2.2 mIoU) is observed only when SGA and CCE are combined, which somewhat weakens the claim of SGA’s independent effectiveness."}, "questions": {"value": "1. In Table 2, both baseline + SGA and baseline + CCE yield very limited improvements, while the combination of SGA + CCE provides a clear gain. How do the authors explain this apparent interdependence between the two modules?\n2. The paper does not provide any comparison or discussion of computational complexity or inference time. Could the authors quantify the additional cost introduced by CCE (e.g., FLOPs, runtime, or memory)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1kfg9DYWLI", "forum": "hpD3tn7Xbp", "replyto": "hpD3tn7Xbp", "signatures": ["ICLR.cc/2026/Conference/Submission6740/Reviewer_QuDJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6740/Reviewer_QuDJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761383187153, "cdate": 1761383187153, "tmdate": 1762919026192, "mdate": 1762919026192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses open-vocabulary semantic segmentation (OVSS) in remote sensing imagery with a training-free framework, AlignCLIP. The method mitigates patch–text mismatches in CLIP-based models via two modules: Self-Guided Alignment (SGA), which refines text embeddings using visual prototypes, and Cluster-Constrained Enhancement (CCE), which clusters semantically similar patch features. AlignCLIP requires no retraining or reference sets. Experiments on eight benchmarks show improved mIoU over existing training-free OVSS baselines, supported by ablation and qualitative analyses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tTwo distinct modules, SGA and CCE, creatively harness domain-specific characteristics for improved patch-text alignment and alleviate disturbance between irrelevant patches.\n2.\tResults on eight diverse remote sensing datasets present a consistent mIoU gain of +2.2% on average over the most competitive baseline (SegEarth-OV), setting new standards for training-free OVSS.\n3.\tAblation studies dissected the contributions of each module and explored sensitivity to key hyperparameters.\n4.\tSGA is demonstrated as a plug-in for existing frameworks (Table 9), adding evidence for its generality."}, "weaknesses": {"value": "1.\tThe approach is motivated by the highly compact intra-class feature distribution of remote sensing image (Line 85-88, Figure 1c), but this property is neither **empirically quantified** nor **qualified**. Additionally, there is insufficient exploration of generalization limits: will SGA/CCE degrade if intra-class variance increases (as in higher-resolution urban datasets) or with more ambiguous categories?\n2.\tWhile the paper provides clear formulations for SGA and CCE, the mathematical treatment of several key steps appears limited, and some design choices seem primarily empirically motivated rather than theoretically grounded. For instance, the claim in Lines 238–240 is not explicitly demonstrated. In particular, Section 3.3 outlines the affinity matrix masking and constraint propagation, but does not provide a detailed justification for the optimality of the binary cluster mask or a clear explanation of how propagation enhances alignment. A more in-depth discussion of how these design decisions affect intra- and inter-class confusion would further strengthen the theoretical foundation of the approach.\n3.\tEvaluations of reference-set-based methods would be beneficial, at least in the appendix. While the authors disable post-processing (e.g., denseCRF) to isolate method effects, the paper does not discuss how this choice impacts real-world applicability or whether simple post-processing could further narrow the gap with evaluated baselines (or reference-based methods).\n4.\tThe conclusion in Line 357-358 is not justified.\n5.\tNo analysis of runtime, computational cost, or efficiency is provided, which is particularly relevant for a training-free framework where clustering and affinity computation may become nontrivial for large-scale imagery.\n6.\tWhile Figure 2 gives a good high-level view of the two modules, it does not expose the detailed mechanics of cluster refinement and “Constrain” in CCE module, which leads to ambiguity.\n7.\tQualitative results are visually compelling, but the absences of original image, GT label, and standardized metric for qualitative result make the conclusions somewhat confused."}, "questions": {"value": "1.\tCan the authors provide empirically quantified or qualified demonstrations on the claim in Line 85-88? (Weakness 1).\n2.\tCan the authors provide more rigorous theoretical justification of several key steps and design choices in Section 3.2 and Section 3.3? (Weakness 2)\n3.\tRelevant experiments are appreciated. (Weakness 3 & 5)\n4.\tSome claims require further clarification, and several minor presentation issues could be improved for clarity and consistency. (Weakness 4 & 6 & 7)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KftOFZkoPO", "forum": "hpD3tn7Xbp", "replyto": "hpD3tn7Xbp", "signatures": ["ICLR.cc/2026/Conference/Submission6740/Reviewer_Cg66"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6740/Reviewer_Cg66"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468816630, "cdate": 1761468816630, "tmdate": 1762919025799, "mdate": 1762919025799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **AlignCLIP**, a **training-free** framework for **open-vocabulary semantic segmentation (OVSS)** in remote sensing imagery. The authors observe that intra-class visual features in remote sensing are more compact than those in natural images, which allows representative image prototypes to refine textual embeddings. Based on this insight, the method introduces two core modules:  \n1. **Self-Guided Alignment (SGA)** – aligns text embeddings with image-specific visual prototypes to reduce cross-modal mismatches.  \n2. **Cluster-Constrained Enhancement (CCE)** – clusters semantically similar patches and propagates logits under cluster constraints to suppress irrelevant correlations.  \n\nThe proposed framework requires no training or external reference sets and achieves consistent improvements over 12 state-of-the-art training-free OVSS baselines across eight remote sensing benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel and effective idea:** The SGA module elegantly leverages image-specific prototypes for text refinement, mitigating cross-modal mismatches in a self-guided, training-free manner.  \n- **Strong empirical results:** Consistently outperforms recent CLIP-based training-free methods across eight diverse remote sensing datasets.  \n- **Motivation well grounded:** The observation about compact intra-class feature distributions in remote sensing is insightful and aligns with the method’s design."}, "weaknesses": {"value": "- **Limited conceptual novelty:** While effective, the SGA and CCE modules are primarily heuristic improvements on existing cross-modal matching and clustering strategies.  \n- **No theoretical formulation:** The “alignment” process is empirically justified but lacks formal theoretical grounding or interpretability analysis.  \n- **Missing cross-domain validation:** The claimed generalization would be more convincing with results on non-remote-sensing datasets (e.g., ADE20K, COCO-Stuff).  \n- **Computation not discussed:** The clustering process in CCE may add inference overhead; runtime or complexity comparisons with SegEarth-OV are missing.  \n- **Writing issues:** The paper is generally clear but occasionally verbose; the notation in Eq. (2) and Sec. 3.3 could be more concise and precise."}, "questions": {"value": "1. How sensitive is the method to the backbone choice (DINO vs. SAM) in both performance and efficiency?  \n2. Can the “compact intra-class feature” assumption be quantitatively analyzed (e.g., variance comparison between RS and natural image features)?  \n3. How does the SGA perform when intra-class diversity is higher (e.g., across seasons or sensors)?  \n4. How does AlignCLIP handle abstract unseen categories (e.g., “industrial area” vs. “building”)?  \n5. Please provide inference speed or FLOPs comparison with SegEarth-OV to support the claim of efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Po8WlLTJyh", "forum": "hpD3tn7Xbp", "replyto": "hpD3tn7Xbp", "signatures": ["ICLR.cc/2026/Conference/Submission6740/Reviewer_hDE6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6740/Reviewer_hDE6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761544952811, "cdate": 1761544952811, "tmdate": 1762919025278, "mdate": 1762919025278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AlignCLIP, a training-free framework designed for Open-Vocabulary Semantic Segmentation (OVSS) in remote sensing imagery. The study is motivated by two key observations. First, directly applying CLIP to remote sensing imagery typically results in cross-modal mismatches between visual and textual features. Second, objects belonging to the same category in remote sensing images often exhibit more compact intra-class feature distributions compared to natural images, offering opportunities for improved alignment. Based on these insights, the authors propose two modules: a Self-Guided Alignment (SGA) module, which adaptively aligns textual embeddings with representative visual prototypes extracted from image patches, and a Cluster-Constrained Enhancement (CCE) module, which clusters semantically similar patches to reinforce intra-cluster consistency and improve segmentation quality. Experiments conducted on eight remote sensing benchmarks demonstrate that AlignCLIP achieves state-of-the-art performance. The authors further validate the generalization capability of AlignCLIP by integrating the proposed modules into other OVSS frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Originality:**\n\n  This paper addresses a significant and challenging task in remote sensing: open-vocabulary semantic segmentation without additional training. The proposed AlignCLIP framework demonstrates originality through its combination of image-specific visual prototypes and self-guided alignment (SGA) mechanisms. This novel strategy circumvents reliance on external datasets or training, effectively extending the cross-modal semantic alignment capabilities of CLIP-based methods to remote sensing scenarios.\n* **Quality:**\n\n  The quality of the research is solid, demonstrated by extensive experimental validation on eight standard remote sensing datasets. The quantitative comparisons indicate that AlignCLIP consistently achieves competitive performance improvements. Additionally, the ablation studies effectively analyze both individual components and hyperparameter settings, clearly highlighting the effectiveness of the proposed SGA and CCE modules.\n* **Clarity:**\n\n  In terms of clarity, the paper is generally well-organized and logically structured. Methodological details are systematically described, and figures such as Figure 2 provide intuitive visualizations that help readers quickly grasp the core concepts. The pseudo-code included in the appendix further enhances reproducibility and reader comprehension.\n* **Significance:**\n\n  Significance arises from both methodological innovation and practical applicability. AlignCLIP provides a robust and adaptable foundation module that can seamlessly integrate into existing CLIP-based methods. This adaptability substantially broadens its potential impact within the research community and practical applications."}, "weaknesses": {"value": "### Motivation\n* **Feature Distribution Comparison:**\n\n  In Figure 1, the authors suggest that objects belonging to the same category exhibit more compact feature distributions in remote sensing imagery compared to natural images. However, this conclusion warrants reconsideration. Specifically, the visualized points in the natural image example represent distinct semantic categories (\"person\" and \"clothes\"), inherently leading to greater semantic separation than two instances of \"trees\" in remote sensing imagery. Additionally, prior research, such as Figure 7 in DeCLIP [1], demonstrates that even different parts of the same object (e.g., a dog's ear vs. its body) are distinctly separated in feature space when using VFMs or VFM-enhanced CLIP.\n\n  If the authors intend to highlight that natural images typically involve finer-grained semantic components leading to more diverse feature distributions, whereas remote sensing imagery captures more homogeneous details from greater distances, it would be beneficial to explicitly state this. Otherwise, additional clarification should be provided.\n\n* **Cross-modal Mismatch Justification:**\n\n  Another primary motivation discussed in the paper is the “cross-modal mismatch” between CLIP’s visual and textual encoders. However, the manuscript lacks concrete quantitative or qualitative evidence to substantiate this claim. Additionally, the cited work [2] appears irrelevant, as it utilizes only CLIP’s textual encoder, combined with a different semantic segmentation network for visual encoding. Given that cross-modal semantic alignment is generally considered a strength of CLIP, it remains unclear whether the mismatch identified originates from CLIP itself or from the self-self attention used within the proposed framework. This ambiguity weakens the foundational motivation for the approach.\n\n### Methodology\n* **Contradictory Approach:**\n\n  The methodological description also presents confusion. If, as stated in the introduction, a cross-modal mismatch exists between CLIP’s visual and textual branches, then why does the proposed method directly leverage visual/textual attention scores to align image-specific visual prototypes? This approach appears contradictory to the stated premise and requires clarification.\n\n### Experiments\n* **Hyperparameter Sensitivity:**\n\n  As indicated in Table 5 of the appendix, the hyperparameters $\\alpha$ and $\\beta$ differ substantially across datasets. Tables 6–8 further suggest high sensitivity of performance outcomes to these hyperparameters. Although the authors' transparency and thoroughness in ablation studies are commendable, this still raises concerns regarding the generalizability and robustness of the proposed method.\n\n* **Experimental Setup Ambiguity:**\n\n  Additionally, the experimental setting described in the manuscript is ambiguous. The authors should clearly define whether AlignCLIP is proposed as a standalone approach or as a plug-and-play module. If presented as a standalone method, an ablation study replacing SegEarth-OV's upsampling module with standard upsampling methods is necessary, as this module significantly contributes to SegEarth-OV's performance. Directly incorporating this module into AlignCLIP and comparing it against SegEarth-OV as separate approaches would thus be unfair. Conversely, if AlignCLIP is intended as a plug-and-play module, the experimental section should be restructured accordingly. The appendix data (Table 9) should be moved to Table 1 in the main text to compare the accuracy of various OVSS methods before and after applying AlignCLIP.\n\n### Minor Issues\n* The term \"image-specific visual prototypes\" (line 220) appears abstract and may not accurately reflect the intended meaning. Consider using \"text-specific visual prototypes\" if this aligns better with the authors' intended concept.\n* The preliminary section (line 199) references \"Following the practice of prior works\" without providing specific citations.\n* I suggest reviewing verb tense usage, as the manuscript currently mixes tenses, e.g., \"propose\" in line 90 and \"designed\" in line 91. Adopting consistent tenses separately when describing methods and experiments could enhance clarity.\n\n[1] DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception\n\n[2] Learning transferable land cover semantics for open vocabulary interactions with remote sensing images"}, "questions": {"value": "Please refer to the weaknesses section for my concerns. My initial judgment is BR, and I will adjust my rating accordingly based on the authors' response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZNM8he8M2q", "forum": "hpD3tn7Xbp", "replyto": "hpD3tn7Xbp", "signatures": ["ICLR.cc/2026/Conference/Submission6740/Reviewer_pRf8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6740/Reviewer_pRf8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905144343, "cdate": 1761905144343, "tmdate": 1762919024742, "mdate": 1762919024742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free paradigm for open-vocabulary semantic segmentation (OVSS) in remote sensing imagery, aiming to bridge the gap between visual representations and textual semantics without requiring additional fine-tuning. The proposed framework centers on enhancing the alignment between image patches and textual features through two main modules: (1) a Self-Guided Alignment (SGA) module that refines text embeddings by leveraging self-similarity within visual-textual pairs, and (2) a Cluster-Constrained Enhancement (CCE) module that suppresses inter-cluster correlations and propagates cluster-level constraints to refine the final logits map. Experimental results across eight remote sensing datasets demonstrate that the proposed approach achieves state-of-the-art performance, outperforming both generic open-vocabulary segmentation methods and the remote sensing–specific SegEarth-OV baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The methodology is intuitive and well-motivated, combining clustering-based feature alignment with constraint propagation in a training-free setting. The idea of incorporating self-guided refinement of text embeddings is appealing, especially for data-limited remote sensing applications.\n\n2.The experimental results are impressive, showing consistent improvements over a wide range of datasets covering various remote sensing domains and annotation styles. This broad evaluation supports the generality and robustness of the proposed approach.\n\n3.The paper is clearly written and easy to follow, with well-organized figures and method descriptions. The framework is presented in a coherent and reproducible manner."}, "weaknesses": {"value": "1.While the paper claims that the approach is motivated by the observation that objects of the same category tend to exhibit a compact spatial distribution in remote sensing images, the explanation and empirical evidence for this phenomenon remain qualitative and underdeveloped. A more rigorous analysis (e.g., feature-space visualization or cluster compactness metrics) would strengthen this motivation.\n\n2.The SGA module appears conceptually similar to existing clustering-based alignment mechanisms, and the upsampling module is directly adopted from SegEarth-OV. As a result, the overall methodological novelty feels somewhat limited, especially considering the rapid progress in open-vocabulary segmentation. The authors could clarify which parts are newly designed and how they differ algorithmically or conceptually from prior work.\n\n3.In the comparative analysis, SegEarth-OV is the only baseline specifically tailored for remote sensing. Most other baselines are general-purpose open-vocabulary segmentation models trained on natural images. This imbalance in comparison may limit the fairness of the evaluation and the strength of the claimed superiority.\n\n4.The performance gains across datasets vary considerably—some datasets show large improvements, while others only marginal gains. The paper does not adequately discuss potential causes of this variability (e.g., dataset scale, domain similarity, or text-label diversity), which weakens the interpretability of the results.\n\n5.Although the approach is labeled “training-free,” there remains ambiguity regarding the extent of adaptation involved (e.g., text prompt selection, clustering hyperparameters). Clarifying whether these components require any dataset-specific tuning would make the claims more convincing."}, "questions": {"value": "1.Please elaborate on the motivation behind the compact distribution assumption. Are there empirical studies or quantitative results that support this observation? How consistent is this phenomenon across different remote sensing scenes (urban, agricultural, oceanic, etc.)?\n\n2.The novelty of the SGA and CCE modules should be discussed in greater depth, particularly in relation to existing clustering-based refinement or graph-based propagation mechanisms. It would be helpful to highlight what is unique in the proposed design and whether it introduces new theoretical insights or performance benefits beyond engineering modifications.\n\n3.To strengthen the evaluation, please consider including more remote sensing–specific baselines or fine-tuned variants of existing ones, ensuring that the comparison fairly reflects the performance within the same domain.\n\n4.The authors are encouraged to analyze the performance variability across datasets, possibly through ablation or correlation analysis between dataset characteristics (resolution, annotation type, text granularity) and performance gain.\n\n5.It would also be beneficial to include efficiency analyses, such as runtime or computational complexity, to further justify the practicality of the proposed training-free paradigm."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lSSOJ28S78", "forum": "hpD3tn7Xbp", "replyto": "hpD3tn7Xbp", "signatures": ["ICLR.cc/2026/Conference/Submission6740/Reviewer_dPaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6740/Reviewer_dPaj"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905510980, "cdate": 1761905510980, "tmdate": 1762919024335, "mdate": 1762919024335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}