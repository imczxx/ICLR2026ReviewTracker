{"id": "k3F29Z2azs", "number": 16105, "cdate": 1758260089487, "mdate": 1759897261551, "content": {"title": "Evaluating and Steering Modality Preference in Multimodal Large Language Model", "abstract": "Multimodal large language models (MLLMs) have achieved remarkable performance on complex multimodal tasks. \nHowever, it remains insufficiently explored whether they exhibit \\textit{modality preference}, a tendency to favor one modality over another when processing multimodal contexts.\nTo study this question, we introduce $MC^2$ benchmark, which constructs controlled evidence-conflict scenarios to systematically evaluate modality preference in decision-making.\nExtensive experiments reveals that all 20 tested MLLMs generally demonstrate clear modality preferences, and such preferences can serve as a useful indictor of downstream task performances of MLLMs. \nFurther analysis shows that modality preference can be controlled by instruction guidance and be captured within the latent representations of MLLMs.\nBuilt on these insights, we propose a probing and steering method based on representation engineering to explicitly control modality preference without requiring additional fine-tuning. \nThis method effectively amplifies modality preference toward a desired direction and demonstrates promising improvements across multiple downstream applications, including multimodal visual understanding and multimodal machine translation.", "tldr": "In this paper, we evaluate and induce the modality preference. Based on the findings, the proposed method demonstrates excellent performance in hallucination mitigation and multimodal machine translation.", "keywords": ["Modality Preference", "MLLM", "Representation Engineering"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a0e257dae03221ca2f01e9c5d922519a0ce23c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MC2 (Multimodal Context Conflict), a controlled benchmark designed to evaluate modality preference in multimodal large language models (MLLMs). The benchmark presents tasks where textual and visual contexts intentionally provide conflicting evidence, enabling the measurement of whether a model relies more on text or vision. The authors further propose a representation-engineering-based steering method that identifies modality preference directions in the latent space and manipulates them at inference time, allowing for training-free control of modality bias. Experiments on 20 MLLMs demonstrate measurable modality preferences and show that steering can improve performance on multimodal visual understanding and multimodal machine translation tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- **Novel diagnostic perspective:**\nThe work introduces an insightful framing of modality preference as a measurable and steerable property of MLLMs, which is conceptually fresh and highly relevant to understanding multimodal reasoning.\n\n- **Well-controlled benchmark (MC2):**\nMC2 is carefully designed to minimize confounding factors such as model knowledge and unimodal competence. The dual-version, consistency-filtered binary QA format enhances measurement reliability.\n\n- **Methodological innovation:**\nThe application of representation engineering to multimodal preference is new. The linear probing–steering framework offers an elegant, training-free way to interpret and modulate model behavior."}, "weaknesses": {"value": "1. **The MC2 benchmark is relatively small in scale.**\n   Since *Modality Preference Steering* depends on the total sample number (N), I believe that enlarging MC2 would not only enable a more comprehensive evaluation but also improve the stability of the *Modality Preference Steering* procedure itself.\n\n2. **Lack of ablation on (N) — stability analysis is missing.**\n   The paper does not analyze how the steering results vary with the number of samples used to compute the modality preference direction. An ablation or sensitivity study on (N) would provide valuable insights into the robustness of *Modality Preference Steering*.\n\n3. **Limited evaluation scope in Table 3.**\n   Table 3 only reports results on the PhD benchmark for Qwen2VL-7B and OneVision-7B. It would be informative to include results for models that exhibit strong text preference—such as LLaVA-1.6-7B—and to extend the evaluation to additional benchmarks, which is strongly encouraged."}, "questions": {"value": "1. How sensitive are the results to the number and diversity of samples used for computing (u_ℓ)?\n2. Could the authors provide qualitative attention maps or token-level analyses showing how steering changes modality reliance?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SkXUab4t6c", "forum": "k3F29Z2azs", "replyto": "k3F29Z2azs", "signatures": ["ICLR.cc/2026/Conference/Submission16105/Reviewer_YxEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16105/Reviewer_YxEk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761138442151, "cdate": 1761138442151, "tmdate": 1762926284116, "mdate": 1762926284116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how multimodal LLMs exhibit modality preference: favoring either text or vision when resolving conflicting multimodal inputs. The authors build a new benchmark, MC², to quantify such preferences under controlled text-image conflict settings. The paper finds that most models are text-biased, though larger models shift toward vision. The proposed representation-based steering method identifies latent directions corresponding to modality bias and adjusts them without finetuning, improving both controllability and performance on visual understanding and translation tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Novel perspective: The paper tackles modality preference in MLLMs from a fresh and underexplored angle, offering new insights into how models balance visual and textual information.\n2. High-quality benchmark and analysis: The proposed MC² benchmark is carefully constructed with controlled modality conflicts and strong human validation, accompanied by clear and convincing visual analyses that reveal consistent patterns across models.\n3. Good downstream performance: The representation-based steering method not only provides interpretable control of modality bias but also delivers solid improvements on multimodal translation and visual understanding tasks."}, "weaknesses": {"value": "1. Generalization of improvements. The method is tested mainly on multimodal translation and visual understanding. Whether the observed gains extend to broader multimodal reasoning, grounding, or dialogue tasks remains uncertain.\n2. Prompt sensitivity and robustness. The benchmark uses conflict-style prompts, which might behave differently depending on how each model was trained to follow instructions. This means whether some of the performance differences are come from how well a prompt fits a model’s style rather than from real differences in modality bias.\n3. Future insights for model training. The work provides limited discussion on how the findings could guide future training strategies for balanced or adaptive multimodal learning."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1OGeHe7OF8", "forum": "k3F29Z2azs", "replyto": "k3F29Z2azs", "signatures": ["ICLR.cc/2026/Conference/Submission16105/Reviewer_EBi4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16105/Reviewer_EBi4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491916933, "cdate": 1761491916933, "tmdate": 1762926283639, "mdate": 1762926283639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates **modality preference in MLLMs** (tendency to favor text/vision under conflicting multimodal inputs) and addresses limitations of existing methods (e.g., isolating modalities).\n\n## Core Benchmark & Findings\n- **MC^2 Benchmark**: 2k samples with controlled text-vision conflict scenarios (perception-level tasks, >95% single-modality accuracy to eliminate confounders) to force modality prioritization.  \n- **Key Results**: All 20 tested MLLMs show clear bias (most favor text; Qwen2.5VL/InternVL3 favor vision); larger models strengthen visual preference; modality preference correlates with downstream performance (Spearman’s \\(\\rho=0.964\\)) and is steerable via instructions.\n\n\n## Method & Contributions\n- **Training-Free Steering**: Probe latent modality preference directions and scale inject them into latent states to adjust bias.  \n- **Contributions**: 1) Introduce MC^2 for rigorous preference evaluation; 2) Reveal MLLMs’ preference is identifiable/steerable via latent representations; 3) Propose a training-free method improving multimodal understanding/translation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## 1. Well-Justified Research Motivation  \nThe paper targets a critical understudied gap in MLLMs: **modality preference under conflicting text-visual inputs**—a gap ignored by prior work that either isolates modalities or overlooks real-world clashes. This focus is impactful, as resolving it directly boosts MLLM reliability in applications like VQA. The authors further sharpen the motivation by focusing on perception-level tasks (to exclude external knowledge confounders), ensuring the problem is well-scoped.  \n\n\n## 2. Strong Originality  \n- **\\(MC^2\\) Benchmark**: Unlike existing multimodal benchmarks (which test fusion), \\(MC^2\\) is novelly designed for conflict-driven modality prioritization, with rigorous controls (e.g., >95% single-modality accuracy) to rule out comprehension errors.  \n- **Training-Free Steering**: The authors propose a creative inference-only method using representation engineering—probing latent preference directions and injecting scaled vectors—to avoid fine-tuning costs, a novel application to multimodal bias control.  \n\n\n## 3. Rigorous Experiments  \n- **Broad Evaluation**: Tests 20 MLLMs (open/closed-source), ensuring generalizability.  \n- **Quantitative Support**: Key claims are backed by hard metrics (e.g., Spearman’s \\(\\rho=0.964\\) for performance correlation; 2.68% discrepancy with human annotations).  \n- **Confounder Control**: Validates critical choices (e.g., layer selection for probing) and filters non-perception tasks, enhancing result credibility."}, "weaknesses": {"value": "## 1. \\(MC^2\\) Benchmark’s Limited Real-World Relevance Hurts Generalizability  \nWhile \\(MC^2\\) isolates modality conflict via abstract, perception-level mismatches (e.g., counting/object recognition errors), it lacks coverage of practical scenarios where modality preference matters—such as multi-turn dialogues (historical text vs. new images) or long-chain reasoning (text inferences contradicting visuals). This over-simplification means the observed preference patterns may not generalize to dynamic, context-rich real-world MLLM use cases.  \n\n\n## 2. Steering Method Lacks Adaptiveness, Reducing Practicality  \nThe training-free method requires **a priori specification of target modality preference** (text/vision) to inject vectors. It cannot enable models to autonomously prioritize modalities based on input quality or task needs—an essential capability for real applications (e.g., chatbot, general assistant). This shifts judgment burden to humans, limiting usability in low-supervision scenarios."}, "questions": {"value": "Same as the section of **Weakness**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ejXUJh0FUd", "forum": "k3F29Z2azs", "replyto": "k3F29Z2azs", "signatures": ["ICLR.cc/2026/Conference/Submission16105/Reviewer_Ncch"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16105/Reviewer_Ncch"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909930710, "cdate": 1761909930710, "tmdate": 1762926283262, "mdate": 1762926283262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates modality preferences in Multimodal Large Language Models (MLLMs) by introducing the MC² benchmark, which uses controlled evidence-conflict scenarios to systematically evaluate whether models favor vision or text modalities. The authors evaluate some MLLMs and find that most exhibit text preference, with modality preference correlating with downstream task performance. They propose a representation engineering method to steer modality preferences without fine-tuning, demonstrating improvements on visual understanding and machine translation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated research question: The investigation of modality preference in MLLMs addresses a fundamental yet understudied aspect of multimodal reasoning, with clear practical implications for model design and application.\n\n- The MC² benchmark is carefully constructed with controlled confounding factors (question comprehension, single-modality perception, internal knowledge). The semi-automated pipeline with human verification ensures data quality.\n\n- The representation engineering approach for steering modality preference is training-free, computationally efficient, and demonstrates measurable improvements on downstream tasks (e.g., +1.33 BLEU on MMT)."}, "weaknesses": {"value": "- The MC² benchmark focuses exclusively on perception-level tasks (counting, color, object recognition, etc.) requiring minimal reasoning. This limits generalizability to more complex multimodal reasoning scenarios involving inference, common sense, or abstract reasoning. The findings may not transfer to tasks requiring deeper cross-modal integration.\n\n- The benchmark relies on artificially constructed conflicting contexts, which may not reflect real-world scenarios where modalities typically provide complementary rather than contradictory information. The ecological validity of these conflict scenarios is questionable.\n\n- While the paper demonstrates the effectiveness of the proposed steering method, it lacks thorough analysis of when and why the method might fail. What are the failure modes? How does performance degrade with increased steering intensity? Are there tasks where steering is ineffective or harmful?\n\n- The method requires two inference passes (probing and steering). The computational overhead, memory requirements, and latency implications are not discussed, which is important for practical deployment."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M5ub5u1dBe", "forum": "k3F29Z2azs", "replyto": "k3F29Z2azs", "signatures": ["ICLR.cc/2026/Conference/Submission16105/Reviewer_zM6k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16105/Reviewer_zM6k"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921004496, "cdate": 1761921004496, "tmdate": 1762926282750, "mdate": 1762926282750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}