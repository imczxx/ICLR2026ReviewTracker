{"id": "1I7e7vBErc", "number": 19736, "cdate": 1758298917798, "mdate": 1763108487560, "content": {"title": "Dense and Indiscernible Object Counting in Agricultural Scenes", "abstract": "Object counting in computer vision has traditionally focused on clearly visible objects. Many real-world applications, such as crop yield estimation and fruit harvest planning in agricultural, involve dense and indiscernible object counting (DIOC). These objects are characterized by their small size, dense distribution, and visual ambiguity with surroundings, which makes traditional counting methods impractical. To facilitate research in this crucial yet unexplored challenge, we introduce DIOCblueberry, a specialized dataset that significantly surpasses existing datasets in complexity. Compared to FSC147, the most comprehensive general counting dataset, DIOCblueberry contains 1.9 times more objects per image with an average of 108 instances, while its box pixel ratio of 2.38‰ is 7.9 times smaller. State-of-the-art counting methods struggle significantly on such challenging scenarios, with high counting errors. To address these challenges, we propose MaskCount, a two-stage multi-modal method. The first stage segments objects from complex backgrounds using multi-modal features, while the second stage enhances feature robustness through contrastive loss. We also design an edge-aware patch cropping mechanism for accurate counting of dense and small objects. Extensive experiments demonstrate that MaskCount achieves substantial improvements over previous state-of-the-art methods, reducing MAE and RMSE by 25.13% and 35.17% respectively on DIOCblueberry. We will release our data, models, and code to the public.", "tldr": "", "keywords": ["Object Counting；Agricultural Counting;  Vision Language Model;  Few shot;"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a481f71d870358a28b151452f27c2516043d946c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the task of dense and indiscernible object counting (DIOC), which involves counting objects that are typically small in size and exhibit strong visual ambiguity with surroundings. To facilitate this task, a new dataset dubbed DIOCblueberry is proposed, comprising 6,265 in-field blueberry images annotated with a total of 679,030 objects. To address the challenges of DIOC, the authors propose a two-stage multi-modal counting method that follows a segment-then-count paradigm, featuring an edge-aware cropping mechanism designed for high-resolution images. Experimental results show that the proposed method performs favorably against previous counting methods on the DIOCblueberry dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces dense and indiscernible object counting, a challenging counting task with applications such as crop yield estimation and fruit harvest planning.\n* To support research on DIOC in agricultural scenes, the DIOCblueberry dataset is presented, containing 6,265 high-resolution blueberry images captured in real-world field environments.\n* A two-stage multi-modal method is proposed for DIOC, achieving state-of-the-art results on the proposed dataset."}, "weaknesses": {"value": "* The idea of segment-then-count is not new in object counting. Several prior works [1,2,3] have already leveraged segmentation to enhance counting accuracy.\n* Complex pipeline. The proposed pipeline involves multiple steps, including cropping, image mask generation (Stage 1), density map prediction (Stage 2), and stitching. This multi-step design would increase inference time and computational overhead.\n* The necessity of the proposed pipeline requires further justification. As shown in Table 3, P2PNet already surpasses most compared methods without using visual exemplars. This implies that a well-designed single-stage crowd counting model may be sufficient for DIOC. \n* Following the previous comment, Table 4 indicates that cropping technique accounts for the majority of performance gain. It is unclear whether existing counting methods could achieve comparable results when combined with cropping technique. If so, the necessity of the proposed two-stage pipeline would become questionable.\n* Limited test set size. As noted in Section 2.2, the test set only contains 35 images, which is significantly smaller than the training set (3,759 images). This limited test set is insufficient to reliably assess the robustness and generalization capability of the proposed method.\n* The proposed dataset contains only a single object class (i.e., blueberry). In this setting, the use of exemplars may be redundant and lacks clear justification. To demonstrate broader applicability beyond a specific fruit type, the dataset should ideally encompass multiple object classes for evaluation, similar to FSC-147. Additionally, the almond and tomato detection datasets contain fewer than 100 objects per image. This low object density falls far short of the \"dense and indiscernible\" regime that defines the DIOC task. As such, evaluations on these datasets do not sufficiently validate the superiority of the proposed method.\n* Missing comparisons with latest multi-modal counting methods, e.g., CountGD [4]. \n* Missing related work section.\n\n**Reference**\n\n[1] Understanding the Impact of Mistakes on Background Regions in Crowd Counting. WACV 2021.\n\n[2] Regressor-Segmenter Mutual Prompt Learning for Crowd Counting. CVPR 2024.\n\n[3] Learning to Count from Pseudo-Labeled Segmentation. WACV 2025.\n\n[4] CountGD: Multi-Modal Open-World Counting. NeurIPS 2024."}, "questions": {"value": "1. Did the authors train the compared methods on the proposed DIOCblueberry dataset? If not, the comparisons in Table 2 become unfair, as other methods are trained on the FSC-147 dataset. For a fair comparison, the authors should ensure consistent training and testing protocols. \n2. What is the testing setting of the compared methods? Did the authors apply image resizing or cropping during inference? Intuitively, input resolution would have a certain impact on the performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "M4WW7mx3um", "forum": "1I7e7vBErc", "replyto": "1I7e7vBErc", "signatures": ["ICLR.cc/2026/Conference/Submission19736/Reviewer_x6jy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19736/Reviewer_x6jy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761028016101, "cdate": 1761028016101, "tmdate": 1762931568656, "mdate": 1762931568656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "yJ36Yp61JX", "forum": "1I7e7vBErc", "replyto": "1I7e7vBErc", "signatures": ["ICLR.cc/2026/Conference/Submission19736/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19736/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763108486859, "cdate": 1763108486859, "tmdate": 1763108486859, "mdate": 1763108486859, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the task of Dense and Indiscernible Object Counting (DIOC) in agricultural settings. It focuses on the challenge of counting small, densely packed objects that are visually indistinguishable from each other. To facilitate research in this area, the paper presents the DIOCblueberry dataset, a new collection of 6,265 high-resolution images with over 679,000 point annotations of blueberries at various growth stages from two farms in China. To address the task of estimating accurate counts from these images, the paper proposes a two-stage, multi-modal counting method called MaskCount.\n\nStage 1 uses a vision-language model (CLIP) to segment the images into foreground/background. Stage 2 then estimates a density map from the masked images, using a contrastive loss to enhance feature separation. Experiments show that MaskCount significantly outperforms several state-of-the-art methods on the DIOCblueberry dataset as well as on other datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The main strength is the DIOCblueberry dataset. It is large and high-resolution, and it captures a challenging real-world counting scenario (dense, small, and camouflaged objects). New public benchmarks are critical for advancing the field, in my opinion.\n2. The proposed MaskCount method achieves state-of-the-art results on the new dataset, significantly outperforming six other methods, including those designed for general and indiscernible counting. The method also shows good performance on other agricultural datasets.\n3. The two-stage approach, particularly the idea of using a VLM to generate a background mask to simplify the scene for a dedicated counter, is logical and proven effective by the ablation studies.\n4. The paper provides comprehensive ablation studies in the main paper and appendix, which validate the contribution of each of the method's main components."}, "weaknesses": {"value": "1. The paper's central claim that the DIOC task is \"unexplored\"  is incorrect. The field of \"Indiscernible Object Counting (IOC)\" addresses the same challenges. The paper even cites IOCFormer, which makes the \"unexplored\" claim all the more baffling and undermines the paper's scholarship. In addition, the related work section is sparse and fails to position the work properly against other agricultural counting datasets or the broader IOC literature. Some papers/surveys in the agricultural domain that also address the counting problem are: \n* MinneApple: A Benchmark Dataset for Apple Detection and Segmentation\n* A survey of public datasets for computer vision tasks in precision agriculture\n* Deep-learning-based counting methods, datasets, and applications in agriculture: A review\n\n2. The number of frames in the train/test split is inconsistent, I think. The paper mentions a total of 6,265 images: 3,759 in the training set and 35 in the test set. What happened to the other images? And how exactly were the train/test sets generated? Thirty-five images for a test set also seems really small. Usually, one follows an 80/20 or 90/10 split for train/test data. \n\n3. The paper never explicitly states how the final scalar count (e.g., \"Pred: 913.35\" ) is derived from the predicted density map. The standard method is to compute the integral (sum) of the map. More importantly, it is not specified if this same extraction method was applied to the density maps produced by the baseline methods (CounTR, LOCA, SSD, etc.) for a fair comparison. This is a crucial detail for verifying the validity of the results in Table 2.\n\n4. The dataset comparison in Table 1 is not complete, in my opinion. It should not be used as a key argument to claim that the proposed dataset is more complex than any other dataset in the literature. This is a too-strong claim that is hard to prove. Instead, in my opinion, it would suffice to highlight the advantages of the presented dataset without having to be the best or most complex.\n\n5. The paper never explicitly states how the final scalar count (e.g., \"Pred: 913.35\" ) is derived from the predicted density map. The standard method is to compute the integral (sum) of the map. More importantly, it is not specified if this same extraction method was applied to the density maps produced by the baseline methods (CounTR, LOCA, SSD, etc.) for a fair comparison. This is a crucial detail for verifying the validity of the results in Table 2.\n\n6. The process for generating the ground truth density maps is mentioned in the appendix, stating a Gaussian kernel is used, with the size being adaptive if boxes are available (3 per image ) and \"fixed\" otherwise. How this \"fixed\" kernel size is chosen and its impact on the ground truth (and thus the MAE/RMSE metrics) is not discussed."}, "questions": {"value": "1. The \"edge-aware patch cropping mechanism\"  is presented as a key design, but it appears to be a standard overlap-and-stitch tiling method, which is a common technique for processing high-resolution images, no? Or am I missing something here?\n\n2. How is the final count extracted from the density map? Is it by summing all pixel values? Crucially, was this same extraction procedure applied to the predicted density maps of all baseline methods in Table 2  for a fair comparison? \n\n3. How were the baseline methods trained? \n\n4. Can you please clarify the train/test split? Are the 35 test set images from the same two farms as the training data, or do they come from a different, unseen location or distribution? Without this, it's impossible to assess the generalization capability you are testing.\n\n5. For the ground truth density maps, how was the \"fixed kernel\" size determined for images without exemplar boxes? How sensitive is the model's performance to this choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HhVpknrmxP", "forum": "1I7e7vBErc", "replyto": "1I7e7vBErc", "signatures": ["ICLR.cc/2026/Conference/Submission19736/Reviewer_Ejr4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19736/Reviewer_Ejr4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767775357, "cdate": 1761767775357, "tmdate": 1762931568079, "mdate": 1762931568079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a task termed Dense and Indiscernible Object Counting (DIOC) in agricultural scenes and contributes i) a new real world DIOC dataset (DICOblueberry) and ii) MaskCount, a two stage multimodal counting pipeline that first derives a CLIP guided background mask from text prompts, and then estimates density map with masked image, further enhanced with exemplar based cross attention, contrastive regularization, and an edge aware patch cropping scheme. On the proposed test set, MaskCount reports MAE = 38.34 and RMSE = 55.32, and ablations credit improvements to the mask, cropped image patches, and contrastive components."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Reasonable Motivation**: This paper presents a clear motivation and a valuable dataset. The statistics convincingly show that the dataset differs from popular counting datasets.\n2. **Good Results**: The proposed MaskCount substantially reduces MAE and RMSE compared to several recent counting baselines on DIOCblueberry, and also reports performance gains on two other agricultural datasets."}, "weaknesses": {"value": "1. **Weak Technical Motivation**: The proposed method seems weakly connected to the proposed task. The manuscript does not show how MaskCount can address dense and indiscernible object counting. Furthermore, it states that SOTA methods struggle on DIOC, but does not explain why they underperform. \n2. **Limited Technical Contribution**: Compared with prior “segment then count” pipelines. The overall recipe, foreground/background suppression followed by counting, has been explored in several counting approaches such as GeCo[1]. Furthermore, the claimed edge aware patch cropping method seems to resemble the overlap-tile proposed by U-Net. \n3. **Misaligned Experimental Setup**: Experimental setup seems strange and misaligned with the central claim. The core claim is to resolve DIOC in agriculture, yet the main comparison is dominated by class-agnostic counting methods, which focus on zero/few-shot generalization counting. Thus, the results are not convincing to back up the DIOC claim, missing comparisons with category-specific counting approaches (e.g., crowd counting approaches have already dense and indiscernible datasets like ShanghaiTech) and with plant counting methods, as they focus on agricultural scenes. \n4. **Insufficient analysis of experimental results**: For instance, Table 7 shows that adding InfoNCE degrades performance, yet no mechanism is offered for this strange and non-intuitive result. \n\n[1] Pelhan, Jer, et al. \"A novel unified architecture for low-shot counting by detection and segmentation.\" Advances in Neural Information Processing Systems 37 (2024): 66260-66282."}, "questions": {"value": "1. Method and experiment specification leave critical details unexplained. For example, in stage 1, how positive/negative patches are constructed from the cosine similarity map (thresholds? top k? hard/soft labels?)\n2. Which parameters are trainable beyond the frozen CLIP backbone? \n3. In stage 2, how the negative background patches are sampled, and how the match module is designed? \n4. In section 4, the reproducibility of baseline training is not documented, as it only gives hyper parameters for MaskCount. \n5. What is the training strategy of baselines, or whether they are retrained on DIOC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VbPQArU7Sv", "forum": "1I7e7vBErc", "replyto": "1I7e7vBErc", "signatures": ["ICLR.cc/2026/Conference/Submission19736/Reviewer_pvMD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19736/Reviewer_pvMD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827431737, "cdate": 1761827431737, "tmdate": 1762931567613, "mdate": 1762931567613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his paper introduces a new computer vision task: Dense and Indiscernible Object Counting (DIOC), focusing on scenarios common in agriculture where objects are small, densely packed, and visually ambiguous with their background. To facilitate research in this area, the authors present a new, challenging dataset named DIOCblueberry, which contains over 6,200 high-resolution images with extensive annotations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper successfully defines and motivates the DIOC task, a challenging problem with significant practical importance, particularly for smart agriculture (e.g., yield estimation). This work addresses a clear gap in existing object counting research, which has largely focused on more discernible objects.\n\nThe introduction of the DIOCblueberry dataset is a major contribution. The authors provide a detailed description of the data collection and annotation process (involving ~1,700 human hours), highlighting its complexity compared to existing benchmarks like FSC147."}, "weaknesses": {"value": "The paper focuses exclusively on counting metrics (MAE and RMSE). However, since the method generates density maps, it implicitly performs localization. Given that some baselines like CLTR[1] and FIDTM[2] are localization-based, including a brief analysis of localization performance (e.g., using point-based F1-score or precision/recall) would provide a more complete picture of the model's capabilities and the dataset's challenges.\n\nThe first stage relies on a large vision-language model (Qwen2.5-VL-72B) to generate background prompts. How robust is the model to variations or potential failures in this prompt generation step? While Appendix A.3 provides an ablation on prompt choice, a discussion in the main paper regarding the consistency and potential limitations of this automated prompt engineering would be beneficial.\n\n[1] An End-to-End Transformer Model for Crowd Localization. ECCV 22. \n[2] Focal Inverse Distance Transform Maps for Crowd Localization. IEEE TMM."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1mzwuB7EpF", "forum": "1I7e7vBErc", "replyto": "1I7e7vBErc", "signatures": ["ICLR.cc/2026/Conference/Submission19736/Reviewer_s7Pq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19736/Reviewer_s7Pq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960536407, "cdate": 1761960536407, "tmdate": 1762931567143, "mdate": 1762931567143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}