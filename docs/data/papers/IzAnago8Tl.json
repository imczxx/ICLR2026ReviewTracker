{"id": "IzAnago8Tl", "number": 4949, "cdate": 1757818072427, "mdate": 1759898003443, "content": {"title": "SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities", "abstract": "Mixture-of-Experts (MoE) architectures have become a key approach for scaling large language models, with growing interest in extending them to multimodal tasks. Existing methods to build multimodal MoE models either incur high training costs or suffer from degraded language capabilities when adapting pretrained models. To address this, we propose Soft Modality-Aware Routing (SMAR), a novel regularization technique that uses Kullback–Leibler divergence to control routing probability distributions across modalities, encouraging expert specialization without modifying model architecture or heavily relying on textual data. Experiments on visual instruction tuning show that SMAR preserves language ability at 86.6% retention with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance. Our approach offers a practical and efficient solution to balance modality differentiation and language capabilities in multimodal MoE models.", "tldr": "", "keywords": ["Mixture of Experts (MoE)", "Large Multimodal Models (LMMs)"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e6e0a0805ccd80ff9fc1bce65c5f5facd1fcd99.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Soft Modality-Aware Routing (SMAR), a regularization technique for Mixture-of-Experts (MoE) based multimodal large language models (MLLMs) to preserve language capabilities during multimodal training. The key contributions include: (1) introducing Modality Routing Distribution (MRD) as a novel metric to characterize routing patterns across different modalities, (2) designing a tolerance band-based loss function using Kullback-Leibler divergence to control expert specialization, and (3) demonstrating language capability retention of 86.6% with only 2.5% pure text data, compared to 81.6% for the baseline without auxiliary loss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel perspective on analyzing MoE routing behavior through MRD, which provides valuable insights into modality-specific expert specialization. The tolerance band approach for controlling expert differentiation is creative and theoretically motivated. The core methodology is clearly explained with appropriate mathematical notation. The MRD concept is well-motivated and the SMAR loss formulation is understandable. Addressing language capability degradation in multimodal training is an important problem. The proposed approach offers a potential solution without requiring architectural modifications or extensive pure text data."}, "weaknesses": {"value": "**Misleading Performance Claims**: The paper's main performance claims are problematic. The authors compare their MoE-based method against dense models (LLaVA-1.5) to claim improvements of 5.4%, 7.0%, 7.4%, etc., which is fundamentally unfair due to architectural differences. When compared against fair baselines with the same architecture, the multimodal improvements are marginal (typically <2%) and sometimes negative (e.g., VQAv2: 82.5→82.4).\n\n**Questionable Baseline Quality**: The baseline model shows anomalously poor performance on MBPP (10.4 vs. 49.0 for base Mixtral), suggesting potential training issues. This 70%+ degradation is inconsistent with other language metrics (85-94% retention) and raises questions about experimental validity. The dramatic \"improvement\" from SMAR (10.4→28.4) appears more like fixing a broken baseline than algorithmic innovation.\n\n**Incomplete Experimental Reporting**: Table 5 (ablation study) only reports language capabilities, omitting multimodal metrics entirely\n\n**Statistical Issues**:\n  - The paper contains a basic reporting error: it claims results on six benchmarks but lists only five improvement percentages. For example, Section 4.3 (\"RESULTS\") states: \"Specifically, on SQAI, MME, MMBench, MM‑Vet, VQAT, and VQAv2, our model achieves performance gains of 5.4%, 7.0%, 7.4%, 12.8%, and 3.0%, respectively, over LLaVA‑1.5‑13B,\" which is inconsistent.\n  - No significance testing or confidence intervals provided\n  - Results may not be statistically meaningful given small effect sizes AND inconsistent patterns across benchmarks. Table 1 shows that SMAR's effects are highly inconsistent across different benchmarks, with both improvements and degradations:\n\nBaseline vs Baseline w/ SMAR:\n\n  VQAv2:    82.5 → 82.4  (-0.1, decline)\n\n  GQA:      62.2 → 62.4  (+0.2, minimal gain)\n\n  VizWiz:   53.7 → 55.1  (+1.4, modest gain)\n\n  SQAI:     74.6 → 75.5  (+0.9, modest gain)\n\n  VQAT:     69.6 → 69.2  (-0.4, decline)\n\n  POPE:     86.8 → 86.6  (-0.2, decline)\n\n  MME:      1634.7 → 1638.8 (+4.1, minimal gain)\n\nThis mixed pattern of gains and losses suggests that:\n  1. The method lacks consistent effectiveness across different task types\n  2. Improvements may be within noise levels rather than systematic algorithmic gains\n  3. The overall \"improvement\" narrative is misleading when individual benchmarks show contradictory trends\n  4. Cherry-picking favorable comparisons (e.g., emphasizing gains vs LLaVA-1.5 while downplaying mixed results vs fair baselines) may be masking the true performance profile\n\nThe lack of a coherent improvement pattern across related benchmarks (e.g., VQAv2 declining while other VQA tasks improve slightly) raises questions about whether SMAR provides genuine algorithmic benefits or merely redistributes performance across different evaluation scenarios.\n\n**Trade-off Analysis Missing**: The method exhibits clear language-multimodal performance trade-offs (evident in Table 3), but the paper fails to acknowledge or analyze this limitation. This omission is misleading for practitioners who need to understand the full implications of the approach."}, "questions": {"value": "1. **Baseline Validity**: Can you explain why the baseline MBPP performance (10.4) is so dramatically lower than the base Mixtral performance (49.0) in Table 2, while other language metrics show much smaller degradations? What specific training issues might have caused this anomaly?\n2. **Fair Comparison**: Why compare against dense models (LLaVA-1.5) rather than focusing on improvements over fair baselines with identical architectures? Could you provide a more balanced discussion of the actual gains when architectural advantages are controlled?\n3. **Complete Ablation Results**: Table 5 is missing multimodal performance metrics. Can you provide the complete ablation results showing how modality-aware bias and load-balancing loss affect both language and multimodal capabilities?\n4. **Trade-off Quantification**: The Table 3 results suggest language improvements come at multimodal performance costs. Can you quantify this trade-off and provide guidance on optimal operating points for different application scenarios?\n5. **Threshold Selection**: The optimal thresholds [1.5, 2.0] were chosen based on \"best overall language score,\" but the selection criteria are unclear. How was \"overall\" defined, and why weren't other metrics considered in the selection?\n\nWhile the core idea has merit, the experimental validation contains significant flaws that undermine the paper's claims. The work would benefit from more rigorous experimental design, complete reporting of results including trade-offs, and honest discussion of limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fg1EN5tDoi", "forum": "IzAnago8Tl", "replyto": "IzAnago8Tl", "signatures": ["ICLR.cc/2026/Conference/Submission4949/Reviewer_EVSU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4949/Reviewer_EVSU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761357886750, "cdate": 1761357886750, "tmdate": 1762917787878, "mdate": 1762917787878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Soft Modality-Aware Routing (SMAR), a KL-divergence based loss that shapes modality routing distributions of MoE experts to explicitly control modality specialization without architecture changes, trying to preserve language ability while maintaining strong multimodal performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear and practically meaningful motivation: The paper pinpoints a real, widespread issue in LVLMs—when integrating visual capabilities via multimodal data, the underlying language competence of the LLM is degraded; addressing this is crucial for real-world deployment of LVLMs.\n2.\tDiagnostic “metric” for modality-wise routing: The authors propose a novel MRD “metric” to evaluate routing probability distributions across modalities, providing a useful lens for analyzing routing strategies in MoE-based multimodal models.\n3.\tSoft, controllable modality-aware routing: The authors introduce trainable per-modality biases and a tolerance-banded symmetric-KL constraint between vision/text routing distributions, enabling controlled expert specialization without hard partitions while maintaining load balance and task performance.\n4.\tIn certain specific areas—such as code generation—the proposed improvements yield substantial gains in model performance.\n5.\tThe authors conduct extensive visualizations of expert activations and routing characteristics, which are valuable for analyzing internal signal flow in sparse multimodal large models."}, "weaknesses": {"value": "1.\tInsufficient experimentation: Although the authors claim improvements on multiple metrics, the experimental section does not provide sufficient evidence. For example, in Table 1 across nine multimodal datasets, the proposed method is best on only four; on text datasets, it is best on only 4/8. In Table 2, across six datasets, only two are best. While the mean reportedly improves from 81.6% to 86.6%, if MBPP is excluded, the mean gain drops from 5% to 0.4%. If the improvements hold primarily on domain-specific datasets, the evaluation and claims should focus on that class of datasets.\n2.\tMissing ablations: For the tolerance band (Table 4), the ablation varies only the band’s location, not its width, and does not explore values to the right of [1.5, 2.0]. For the modality-specific bias ablation (Table 5), an experiment without the modality-specific bias but retaining the Load-Balancing Loss is missing; thus, the gains in the final row cannot be ruled out as stemming from the Load-Balancing Loss.\n3.\tImprecise terminology: The mentioned “MRD distance” defined via symmetric KL divergence does not satisfy the axioms of a mathematical metric (triangle inequality). The wording should be revised."}, "questions": {"value": "1.\tIn Appendix C, authors state that encouraging MRD toward a target value has clear advantages over using constraints of +∞ or 0. However, the benefits of a tolerance band are not discussed. What advantages does a tolerance band offer compared with supervising toward a single exact value?\n2.\tPrior work suggests that the roles of visual and language tokens differ across layers in LVLMs. Is it feasible to use different hyperparameters d across layers to constrain the MRD distance—or even make d adaptive?\n3.\tIn Table 5, why is [1.0, 1.5] used as the range for d? This differs from the other experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XS8r1lqihi", "forum": "IzAnago8Tl", "replyto": "IzAnago8Tl", "signatures": ["ICLR.cc/2026/Conference/Submission4949/Reviewer_JyHr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4949/Reviewer_JyHr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665617637, "cdate": 1761665617637, "tmdate": 1762917787376, "mdate": 1762917787376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces the Soft Modality-Aware Routing Strategy, which controls the routing of experts across modalities. The author showed that the language capabilities are better preserved while maintaining good multimodal performance. Overall, the writing and organization of the paper are good."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper studies and proposes a metric to quantify the routing strategies in MoE-based multimodal models.\n2) The paper proposes SMAR to control expert modality differentiation.\n3) The experiment and ablation are well designed with detailed analysis to study SMAR."}, "weaknesses": {"value": "1) The SMAR does not generally improve multimodal capabilities, although it retains language capabilities better. Why?\n2) There is a lack of comparative analysis with works mentioned in the intro and related works sections to show how this work challenges SOTA.\n3) The abstract mentioned that 2.5% pure text was used, while the conclusion mentioned \"without additional pure text\"?"}, "questions": {"value": "1) Line 416, why only layer 13 almost all text token, but not the other layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eECH1k3nwF", "forum": "IzAnago8Tl", "replyto": "IzAnago8Tl", "signatures": ["ICLR.cc/2026/Conference/Submission4949/Reviewer_FkpW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4949/Reviewer_FkpW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934595598, "cdate": 1761934595598, "tmdate": 1762917787020, "mdate": 1762917787020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Soft Modality-Aware Routing (SMAR), a novel regularization technique for Mixture-of-Experts (MoE)-based multimodal large language models (MLLMs). The method addresses the challenge of maintaining strong language capabilities while adapting pretrained MoE models to multimodal tasks. The SMAR method uses Kullback-Leibler (KL) divergence to control routing probabilities across different modalities, encouraging expert specialization without heavily relying on textual data. The approach demonstrates strong multimodal performance and achieves 86.6% retention of language capabilities with only 2.5% pure text, outperforming baselines. The paper provides experiments and ablation studies to validate the proposed method on several multimodal benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposal introduces a unique method, SMAR, to manage expert specialization in MoE-based multimodal models. It creatively uses KL divergence to control routing across modalities, enhancing language capability retention without requiring architectural changes. The idea of soft modality-aware routing is innovative and addresses the crucial challenge of balancing modality differentiation with language performance.\n- The experimental setup is comprehensive, and the paper uses relevant and established benchmarks to evaluate the proposed method. The ablation studies and comparison with baselines provide a solid understanding of the method’s performance in both multimodal and language tasks. The authors also present clear details of the model architecture and training strategy, ensuring reproducibility.\n- The paper is well-structured and clearly written. The method is explained in detail, and the experimental results are presented in a clear manner with sufficient visual aids (e.g., tables and figures). The paper avoids unnecessary jargon and communicates complex ideas in an accessible way."}, "weaknesses": {"value": "- The paper uses VITA and MoE-LLaVA as the base models in the experiments. They are now considered outdated. The validity of the proposed method on these older models might not be representative of its potential on more advanced architectures.\n- The paper compares SMAR with load-balancing loss, but it lacks sufficient details about how load-balancing loss is applied in the experiments. There are no descriptions of the training parameters, settings, or any explanation of why this method is being used as a baseline.\n- The benchmarks used in this paper are outdated, both in terms of pure-text and multimodal evaluations. The current benchmarks primarily focus on simple question answering (QA) tasks and lack a comprehensive assessment of reasoning abilities. Reasoning is crucial for evaluating the true strength of a model’s language capabilities, especially when tackling more complex tasks, which is essential for validating the core claim of this paper. Similarly, newer multimodal benchmarks incorporate more challenging tasks, such as multi-image inputs and video understanding, which better assess the model’s visual capabilities. Updated multimodal benchmarks, with their increased visual challenges, would provide a more thorough examination of how the proposed method impacts the model’s visual reasoning abilities.\n- The proposed method focuses on a specific configuration of the MoE model (e.g., Mixtral). However, it does not discuss how well SMAR would scale to even larger models or more complex multimodal tasks. Given the rapid development of MoE-based architectures, it is crucial to understand how this method might behave in larger-scale models."}, "questions": {"value": "- Could you elaborate on the specific training parameters and settings used with load-balancing loss?\n- How would SMAR perform on more recent pure-text reasoning benchmarks that focus on more complex reasoning tasks?\n- How would SMAR scale when applied to larger and updated models or more complex multimodal tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yr4DWFOhJe", "forum": "IzAnago8Tl", "replyto": "IzAnago8Tl", "signatures": ["ICLR.cc/2026/Conference/Submission4949/Reviewer_UCfG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4949/Reviewer_UCfG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051597788, "cdate": 1762051597788, "tmdate": 1762917786608, "mdate": 1762917786608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}