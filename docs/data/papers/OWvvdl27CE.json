{"id": "OWvvdl27CE", "number": 9725, "cdate": 1758136706228, "mdate": 1759897702108, "content": {"title": "Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering", "abstract": "Uncertainty Quantification (UQ) research has primarily focused on closed-book factual question answering (QA), while contextual QA remains unexplored, despite its importance in real-world applications. In this work, we focus on UQ for the contextual QA task and propose a theoretically grounded approach to quantify \\emph{epistemic uncertainty}. We begin by introducing a task-agnostic, token-level uncertainty measure defined as the cross-entropy between the predictive distribution of the given model and the unknown true distribution. By decomposing this measure, we isolate the epistemic component and approximate the true distribution by a perfectly prompted, idealized model. We then derive an upper bound for epistemic uncertainty and show that it can be interpreted as semantic feature gaps in the given model’s hidden representations relative to the ideal model. We further apply this generic framework to the contextual QA task and hypothesize that three features approximate this gap: \\emph{context-reliance} (using the provided context rather than parametric knowledge), \\emph{context comprehension} (extracting relevant information from context), and \\emph{honesty} (avoiding intentional lies). Using a top-down interpretability approach, we extract these features by using only a small number of labeled samples and ensemble them to form a robust uncertainty score. Experiments on multiple QA benchmarks in both in-distribution and out-of-distribution settings show that our method substantially outperforms state-of-the-art unsupervised (sampling-free and sampling-based) and supervised UQ methods, achieving up to a 13-point PRR improvement while incurring a negligible inference overhead.", "tldr": "", "keywords": ["Uncertainty Quantification", "LLMs", "RAG", "Contextual QA", "Hallucinations"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a907cb443d27a28eca05e6c5041f0643aa2c2037.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose an uncertainty quantification method derived from an approximation of the upper bound for epistemic uncertainty.  They comprehensively justify the theoretical basis for this method and demonstrate that it empirically works effectively with three different models on three datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Strong theoretical justification for the validity of this uncertainty quantification method, which is uncommon for many uncertainty quantification methods\n2. Strong experimental results, particularly when compared to the computational complexity of baselines\n3. Generally, many UQ methods quantify total uncertainty, rather than epistemic uncertainty; this broadens potential applications for this UQ method"}, "weaknesses": {"value": "1. Some assumptions are not well-justified, in particular that there exists an optimal token string that would result in an ideal model. This is more or less solely supported by previous work that suggests appending a prompt can be considered to be fine-tuning. While the empirical results suggest this assumption holds true enough for this method to work, I feel that stating that this is a close approximation to a fine-tuned model (lines 179-180) or that it is the “ideal” model (line 190) may need more evidence or more qualifiers.\n2. There may be more to add for related work, specifically in the domain of uncertainty decomposition (for instance, https://arxiv.org/abs/2311.08718 and https://arxiv.org/pdf/2402.10189)—I believe this would provide better context for this work."}, "questions": {"value": "Is there a way to organize Table 1 so it’s easier to tell which baselines require multiple samples and which require supervised data? This would make comparisons easier."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pYrC9HNegi", "forum": "OWvvdl27CE", "replyto": "OWvvdl27CE", "signatures": ["ICLR.cc/2026/Conference/Submission9725/Reviewer_WhVN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9725/Reviewer_WhVN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939819660, "cdate": 1761939819660, "tmdate": 1762921223930, "mdate": 1762921223930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an information-theoretic metric to quantify epistemic uncertainty, i.e., cross-entropy between the true (ideal) predictive distribution and the given model's distribution --adapted from [Schweighofer et al. (2024)]. It operationalizes it via interpretability tools. Starting from the information-theoretic formulation, the authors derive an upper-bound of the epistemic uncertainty formulated as the difference between the ideal and given model's internal states. These can be represented as decomposed vectors (linear representation hypothesis); furthermore, the ideal and given model's coefficients in the linear representation can be interpreted as the deviation of the ideal from the given model (namely Feature-Gaps). In practice, for contextual QA, three features are selected: Context reliance, Context comprehension, and Honesty. Vectors for these features are extracted using interpretability techniques.  \n\nThe proposed method based on interpretability tools is sensible, efficient, and shows competitive performance. However details about the feature extraction/approximation are missing and there are some weak points in the experimental setup (see below)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Using interpretability insights to estimate uncertainty is novel. \n- The proposed approach is efficient at test time; although it requires training it necessitates only few labelled data points. It does require tuning some hyper-parameters (e.g., layers to use), features, prompts."}, "weaknesses": {"value": "- Missing experiments on RAG contexts, larger models, and p(true) approach. Significance tests on results as sometimes these are quite close values.\n- Lack of details about the training of the $w_i$ weights."}, "questions": {"value": "- Experimental setup:\n\n\t- The authors should clarify what sort of input is used for HotPotQA, it is common practice for this dataset to be considered an open-book QA task where external retrieval systems are used to retrieve passages for context (RAG). If this is not the case, and passages available in the dataset are used, then all evaluation tasks are instances of Reading Comprehension (RC). In this case the evaluation does not cover the case of real RAG scenarios. How would the approach work on the case of noisy and misleading real retrieval?\n\t\n\t- The evaluation focuses on 3 models of similar size (different families). It would also make a clearer empirical statement of the method's performance if the authors evaluate on larger models, e.g., Qwen2.5-32B.\n\t\n\t- Out-of-distribution experiments (Section 5.3, Figure 3) need to incorporate a non-supervised baseline, e.g., the cheaper unsupervised Perplexity, in order to better visualize how the compared out-of-distribution methods (Feature-Gaps and SAPLMA) compare to this simple approach. Also, by looking at values in Figure 3, we can see that these are PRR (better to add this in the caption), the authors should also report the AUROCs.\n\n\t- Significance values should be reported for results on Table 1.\n\t\n\t- An strong widely used approach p(true) is mentioned in the paper though not included in experiments.\n\n- \"Feature Extraction\" in Section 4.2 could be better explained (add missing details). Line 281,  \"access to a set of T labelled samples\" I guess this is with gold answer. Line 282, \"standard instruction\", what would be standard instruction in the case \"look at the context\" and \"se your own knowledge\"? How is $\\alpha_i$ in Eq. 7 (and Line 304) computed? Is $\\alpha_i v_i$ a decomposition of $v^l$? Is in the end $\\alpha_i$ assumed to be 1 in Eq. 7? More details about the training of the procedure to obtain the $w_i$ weights are needed (line 305). What is the connection of the $w_i$ weights and the ideal model representations ($w_i$'s are trained to predict correct/incorrect)? What is the relation of training these weights and the ranking formulation in Line 134?\n\n- Related work. The method by [Laura Perez-Beltrachini and Mirella Lapata] trains the so called passage-utility that is predicting correct/incorrect answer. How does this differentiates from the training $w_i$s on the task of predicting correct/incorrect answer? Beyond the different amount of training data required."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d9E6HG0TfV", "forum": "OWvvdl27CE", "replyto": "OWvvdl27CE", "signatures": ["ICLR.cc/2026/Conference/Submission9725/Reviewer_id5u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9725/Reviewer_id5u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948745838, "cdate": 1761948745838, "tmdate": 1762921222301, "mdate": 1762921222301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new supervised method for uncertainty quantification for LLMs. The features are constructed as the difference between hidden states of LLM prompted in the standard way and the LLM which was prompted to be “liar” or “do not look at the context”. \nThe authors claim that constructing features in this way results in better generalization than standard hidden states or features built from attention maps (such as Lookback lens).\nThe model is basically a linear regression on top of the hidden states. \nImportant contribution of the paper is that the authors show that the epistemic uncertainty can be bounded from above by such a linear expression. \n\nOne of the main problems of the paper is that the authors claim that they quantify only epistemic uncertainty. However, the tasks where they test their methods assume that both types of uncertainty would be useful, so it seems that there is no need for such disentanglement.  \n\nAnother problem is that the method needs sampling of the answer multiple times, which introduces large computational overhead (basically up to 3-4 times). These is very close to sampling-based methods that need as little as 5 samples to show improvements over the simple baselines. Sampling alone should carry some additional information that help to improve the performance of hallucination detection. So, the substantial part of the improvement might come from sampling rather than other parts of the method. \n\nThe set of baselines is good, however, there are few methods that explore the idea of the difference between “good” and “bad” answers of LLM.\nVazhentsev, Artem, et al. \"Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models.\" Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 2025.\nComparison to this baseline looks important for me\n\nThe authors claim that the method works well with small amount of training data, which is good. However, the analysis in the case when the amount of data is bigger is not conducted. It would interesting to see wheather there are improvements when the number of train instances is relatively large 5000+. \nNote that the lack of the improvements doex not deteriorate the method, but the lack of such analysis is a notable drawback of this work. This analysis is important for me.\n\nPlease, also note that in the experiment with generalization you compare your method with SAPLMA. However, the authors of Lookback lens showed that SAMPLA in general is not very good at generalization. It would be reasonable to compare your method to Lookback lens, as the main point of their paper was better generalization than SAPLMA. This analysis is moderately important, but nice to have.\n\nOverall, I think the work is good and open for the discussion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.\tInteresting method that has better performance than other supervised baselines.\n2.\tImportant contribution of the paper is that the authors show that the epistemic uncertainty can be bounded from above by such a linear expression."}, "weaknesses": {"value": "1.\tNeed additional analysis with more training data\n2.\tComparison to another baseline\n3.\tThe method is computationally expensive, because it needs to sample 3-4 times\n4.\tThe improvements might come not from the “epistemic nature” of uncertainty, but just because of sampling.\n5.\tThe authors show that benefits of their method that disentangles epistemic uncertainty on tasks that need both epistemic and aleatoric uncertainty."}, "questions": {"value": "1.\tInteresting method that has better performance than other supervised baselines.\n2.\tImportant contribution of the paper is that the authors show that the epistemic uncertainty can be bounded from above by such a linear expression."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bk0Ew1QCkI", "forum": "OWvvdl27CE", "replyto": "OWvvdl27CE", "signatures": ["ICLR.cc/2026/Conference/Submission9725/Reviewer_Eyf6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9725/Reviewer_Eyf6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160631734, "cdate": 1762160631734, "tmdate": 1762921220923, "mdate": 1762921220923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}