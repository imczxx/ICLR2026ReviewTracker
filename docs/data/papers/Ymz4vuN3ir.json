{"id": "Ymz4vuN3ir", "number": 10722, "cdate": 1758180384617, "mdate": 1759897633380, "content": {"title": "Causal-CoT: Causal Chain-of-Thought for Validated Reasoning", "abstract": "Chain-of-Thought (CoT) prompting enables large language models (LLMs) to expose intermediate reasoning, but the resulting rationales are often unfaithful—skipping premises, confusing relations, or relying on unsupported leaps. We propose Causal-CoT, a framework that integrates causal graph construction, augmentation, and verification into the CoT paradigm. Causal-CoT operates through a three-stage pipeline: (1) DAG-guided CoT constructs an initial causal graph from the problem context; (2) Reflection and Augmentation enriches the graph by adding plausible mediators and contextual variables; and (3) Causal Verification estimates conditional probabilities via prompting and applies do-calculus to compute causal effects. This structured approach transforms linear reasoning into graph-based inference, enabling more faithful and interpretable reasoning. Experiments across seven benchmarks in mathematics, commonsense, and causal reasoning show that Causal-CoT improves reasoning fidelity, mitigates shortcut behaviors, and achieves more stable performance compared to standard CoT. Moreover, Causal-CoT significantly enhances both reasoning fidelity and answer accuracy, effectively suppresses “jump-to-answer” shortcuts, and strikes a favorable balance between accuracy and computational cost.", "tldr": "We present Causal-CoT, a framework that integrates causal graph construction, completion, and verification into CoT.", "keywords": ["Causal Reasoing; Chain-of-Thought; Large Language Models"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/685fdb3d35293c6d9f776f44180c7186a437f9be.pdf", "supplementary_material": "/attachment/9e48ba52a5cff6e0192f2ffee25266e7481636e8.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Causal-CoT, a causality-enhanced CoT prompting technique. Causal-CoT has 3 stages. It first constructs the initial causal graph and adds potential mediators; finally, it performs causal verification through intervention on the causal graph. The authors conduct extensive experiments on several LLMs and benchmarks, demonstrating the superior performance of Causal-CoT over standard CoT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Figure 1 is well drawn; however, I would suggest that the authors incorporate more explicit causal factors into this graph (e.g., mark the confounder variable).\n\n2. Authors conduct lots of experiments on diverse models.\n\n3. The radar graph is well drawn."}, "weaknesses": {"value": "1. First, I believe the motivation needs to be supplemented. Specifically, authors could elaborate on the idea of why they believe causality is a good way to enhance the soundness of thinking steps. Because there may be other ways to do this.  \n\n2. Second, I suggest that authors add some background on the causality. For example, the basic definition of Structural Causal Models (SCMs) and the causal intervention.\n\n3.  It seems there are no guarantees on the correctness of the generating causal graph. Since the LLMs could have hallucination problems, I suggest that authors discuss the potential outcomes of an incorrect initial causal graph.\n\n4. The authors employ the fixed threshold $\\tau = 0$, to judge the existence of causal relationship. However, I believe a more scientific way is to employ the significance test rather than a fixed threshold. Besides, authors could discuss the outcomes of different thresholds.\n\n5. Most baselines are variants of Causal-CoT; maybe authors could incorporate other prompt strategies like Tree-of-Thoughts (ToT) [1].\n\n6. Since LLMs' causal reasoning is an active area and this work is inspired by causality, I would like to encourage authors to discuss more causal reasoning benchmarks, including (but not limited to): CausalProbe-2024 [2] and e-care [3].\n\n> [1] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., & Narasimhan, K. (2023). Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36, 11809-11822.\n\n> [2] Chi, H., Li, H., Yang, W., Liu, F., Lan, L., Ren, X., ... & Han, B. (2024). Unveiling causal reasoning in large language models: Reality or mirage?. Advances in Neural Information Processing Systems, 37, 96640-96670.\n\n> [3] Li Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin. e-care: a new dataset for exploring explainable causal reasoning. In ACL, 2022."}, "questions": {"value": "1. According to Figure 1 (b) and Figure 4, it seems that Causal-CoT causes performance degradation in some closed-source models. Can authors explain this phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MEYg43mb3S", "forum": "Ymz4vuN3ir", "replyto": "Ymz4vuN3ir", "signatures": ["ICLR.cc/2026/Conference/Submission10722/Reviewer_2Pwu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10722/Reviewer_2Pwu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760707124307, "cdate": 1760707124307, "tmdate": 1762921951246, "mdate": 1762921951246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new reasoning framework for LLMs that integrates causal inference into the traditional CoT approach. Causal-CoT introduces a three-stage process: (1) DAG-guided CoT, which maps reasoning into a directed acyclic causal graph; (2) Reflection and Augmentation, which refines this graph by adding plausible mediators and confounders through internal prompting or external retrieval; and (3) Causal Verification, which estimates causal effects using LLM-derived conditional probabilities and applies do-calculus for validation. Experiments on seven reasoning benchmarks (mathematics, commonsense, and causal inference) show that Causal-CoT improves reasoning faithfulness, reduces shortcut behaviors, and provides more stable accuracy across diverse LLMs while maintaining efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a novel combination of CoT prompting and causal inference via DAGs. While prior work improves CoT through verification or self-reflection, none have embedded formal do-calculus reasoning into LLM prompting.\n- Unlike prior “linear” CoT methods, Causal-CoT translates reasoning into a graph-structured process, allowing explicit identification of mediators, confounders, and colliders. This is an original problem formulation that bridges symbolic causal modeling and natural language reasoning.\n- The work’s method of mapping verbal likelihoods (“likely,” “unlikely,” etc.) to calibrated probability distributions and using them for causal effect estimation is creative and technically innovative."}, "weaknesses": {"value": "- The use of single verbal likelihood prompts mapped to fixed Beta distributions oversimplifies causal inference. This ignores uncertainty propagation and variance in model responses.\n- The IR-augmented variants (Causal-CoT-KG, -RAG) are tested but not well integrated into the causal verification process — retrieval results are fused post hoc without quantitative quality control of external evidence.\n- Although the paper claims improved “reasoning fidelity,” most evaluations rely on final-answer accuracy (Table 2). There is limited analysis of reasoning-chain correctness, such as step-wise causal link validation or graph quality metrics (e.g., precision/recall of DAG edges).\n- All benchmarks are text-based reasoning datasets; none test robustness under unseen causal shifts or domain transfer.\n- The authors briefly mention performance drops in certain datasets but provide no causal analysis of why graph structuring hurts performance in open-ended reasoning tasks."}, "questions": {"value": "- The paper mentions using verbal likelihoods (e.g., “likely,” “possible,” “very unlikely”) mapped to fixed Beta distributions (Section 3.1, Eq. 6–10). How were these Beta parameters chosen?\n- How sensitive are the causal effect results to the chosen threshold?\n- How does the framework handle ambiguous causal directions? Many natural language statements can be interpreted in either direction (e.g., “Smoke indicates fire” vs. “Fire causes smoke”). How does the DAG-guided CoT resolve such ambiguities — via language priors, causal markers, or external knowledge graphs?\n- How is “reasoning fidelity” quantitatively defined? The text claims Causal-CoT improves “reasoning fidelity” and “faithfulness,” but the reported metrics focus on answer accuracy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VEnQ29CcRT", "forum": "Ymz4vuN3ir", "replyto": "Ymz4vuN3ir", "signatures": ["ICLR.cc/2026/Conference/Submission10722/Reviewer_JwAb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10722/Reviewer_JwAb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988421047, "cdate": 1761988421047, "tmdate": 1762921950855, "mdate": 1762921950855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new reasoning framework for LLMs that integrates causal inference into the traditional CoT approach. Causal-CoT introduces a three-stage process:  **DAG-guided CoT**, which maps reasoning into a directed acyclic causal graph. **Reflection and Augmentation**, which refines this graph by adding plausible mediators and confounders through internal prompting or external retrieval. **And Causal Verification**, estimates causal effects using LLM-derived conditional probabilities and applies do-calculus for validation. The experiments on seven reasoning benchmarks (e.g., mathematics, commonsense) show that the method improves reasoning faithfulness, reduces shortcut behaviors, and provides more stable accuracy across diverse LLMs while maintaining efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper introduces a novel combination of CoT prompting and causal inference via DAGs. While prior work improves CoT through verification or self-reflection, they donot embed formal do-calculus reasoning into LLM prompting.\n- This paper translates reasoning into a graph-structured process, allowing explicit identification of mediators, confounders, and colliders. This is an original problem formulation that bridges symbolic causal modeling and natural language reasoning.\n- The method of mapping verbal likelihoods (“likely,” “unlikely,” etc.) to calibrated probability distributions and using them for causal effect estimation, in my view, is creative and technically innovative."}, "weaknesses": {"value": "- The use of single verbal likelihood prompts mapped to fixed Beta distributions oversimplifies causal inference. This may ignore uncertainty propagation and variance in model responses.\n\n- The IR-augmented variants (Causal-CoT-KG, -RAG) are tested but not well integrated into the causal verification process. It seems retrieval results are fused post hoc without quantitative quality control of external evidence.\n\n- Although the paper emphasizes improvements in “reasoning fidelity,” most of the evaluation still seems to rely on final-answer accuracy (in Table 2). There’s limited discussion around how accurate the intermediate reasoning actually is, like whether the causal steps check out, or if the graph structures are evaluated with metrics such as edge-level precision or recall.\n\n- Most benchmarks are text-based reasoning datasets. It appears that none of the benchmarks test robustness under unseen causal shifts or domain transfer.\n\n- Authors briefly mention performance drops in certain datasets. I hope to see a causal analysis of why graph structuring hurts performance in open-ended reasoning tasks."}, "questions": {"value": "- The paper mentions using verbal likelihoods (e.g., “likely,” “possible,” “very unlikely”) mapped to fixed Beta distributions (Section 3.1, Eq. 6–10).  I wonder:\n  - How were these Beta parameters chosen?\n  - and how sensitive are the causal effect results to the chosen threshold?\n  - How does the framework handle ambiguous causal directions? Many natural language statements can be interpreted in either direction (e.g., “Smoke indicates fire” vs. “Fire causes smoke”). How the DAG-guided CoT resolve such ambiguities, via language priors, causal markers, or external knowledge graphs.\n- How the “reasoning fidelity” quantitatively defined. The text claims Causal-CoT improves “reasoning fidelity” and “faithfulness,” but the reported metrics focus on answer accuracy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VEnQ29CcRT", "forum": "Ymz4vuN3ir", "replyto": "Ymz4vuN3ir", "signatures": ["ICLR.cc/2026/Conference/Submission10722/Reviewer_JwAb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10722/Reviewer_JwAb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988421047, "cdate": 1761988421047, "tmdate": 1763641735201, "mdate": 1763641735201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Causal-CoT, a framework that structures CoT reasoning using causal graphs. It consists of: (1) DAG construction from problem statements, (2) augmentation with mediators/confounders, and (3) causal verification using do-calculus and LLM-derived conditional probabilities. Experiments span 7 benchmarks and 9 language models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel and principled integration of causal inference with LLM reasoning.\n\n2. Clear three-stage framework with formalization and Algorithm 1.\n\n3. Broad evaluations across variety of reasoning---math, commonsense, and causal reasoning tasks.\n\n4. Thorough error analysis separating reasoning errors vs knowledge gaps."}, "weaknesses": {"value": "1. Overall average improvement over CoT is small (+1.1pp). and retrieval variants often perform worse.\n\n2. Probability estimation (verbal likelihood to Beta distribution) is not theoretically justified; no sensitivity analysis.\n\n3. No human evaluation of DAG correctness or reasoning fidelity. Nor simple correctness/rule-automated evaluation.\n\n4. Increased complexity and token cost; runtime not reported.\n\n5. Some design choices (task reformulation, temperature settings) lack ablations/may confound evaluations.\n    - Task reformulation: multiple-choice questions are reformulated into binary causal judgments (premise + hypothesis), which can change the task structure. \n\n    - Temperatures: 0.7 for Stages I–II and 0.3 for Stage III, without ablations. \n\n    - No statistical significance reporting accompanies the deltas in Table 2."}, "questions": {"value": "1. How sensitive are results to Beta parameters and temperature choices?\n\n2. Any human assessment of DAG accuracy or causal correctness?\n\n3. Why do retrieval-based variants frequently underperform?\n\n4. Could you provide more rigorous justifications for your design choices? (particularly on task reformation)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZlhDYqbUH8", "forum": "Ymz4vuN3ir", "replyto": "Ymz4vuN3ir", "signatures": ["ICLR.cc/2026/Conference/Submission10722/Reviewer_zKwU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10722/Reviewer_zKwU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989171092, "cdate": 1761989171092, "tmdate": 1762921950565, "mdate": 1762921950565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}