{"id": "d2pUyiXwcm", "number": 4617, "cdate": 1757728800717, "mdate": 1759898023250, "content": {"title": "Physics-Informed Inference Time Scaling for Solving High-Dimensional Partial Differential Equations", "abstract": "Solving high-dimensional partial differential equations (PDEs) is a critical challenge where modern data-driven solvers often lack reliability and rigorous error guarantees. We introduce Simulation-Calibrated Scientific Machine Learning (SCaSML), a framework that systematically improves pre-trained PDE solvers at inference time without any retraining. Our core idea is to derive a new PDE, which we term the Law of Defect, that precisely governs the error of a given surrogate model. Because this defect PDE retains the structure of the original problem, we can solve it efficiently with traditional stochastic simulators, yielding a targeted correction to the initial machine-learned solution. We prove that SCaSML achieves a faster convergence rate, with a final error bounded by the product of the surrogate and simulation errors. On challenging PDEs up to 160 dimensions, SCaSML reduces the error of various surrogate models, including PINNs and Gaussian Processes, by 20-80%. SCaSML provides a principled method to fuse the speed of machine learning with the rigor of numerical simulation, enhancing the trustworthiness of Al for scientific discovery.", "tldr": "We introduce SCaSML, a framework that improves pre-trained PDE solvers at inference time without retraining by deriving and efficiently solving a new PDE that governs the model's error, provably accelerating convergence.", "keywords": ["AI for Science", "Inference-time Scaling", "Deep learning", "Curse of dimensionality"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bb4a01475984bf67303fc49bb9c524b1c11114f3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Inference-time scaling is a characteristic phenomenon observed in large language models in which allocating additional computation resources at inference time yields an improvement of output quality of LLMs. The paper is the first attempt to bring the idea into physics-informed emulators to solve high-dimensional PDEs. The key assumption to realize the idea is the semi-linearity of parabolic PDEs, and this makes it possible to derive a PDE, called Law of Defect, that governs the residual between the ground-truth solution and surrogate solution for an original PDE. This finding enables the application of standard solvers such as multilevel Picard solver without requiring any additional retraining and fine-turning, and the residual is also utilized to correct the error of the output of a trained surrogate model, which is the proposed inference pipeline to solve the PDEs. The proposed framework is evaluated on a variety of scenarios with various dimensions that control the difficulty of the task and compared to a couple of standard solvers and plain data-driven surrogate models."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- **Intriguing approach:** The proposed approach is disparate from the other approaches boosting the performance in the post-training phase, especially that requires additional training. The question raised could serve as a new class of tasks in the scientific discovery. \n\n- **Performance gain:** The improvement in the prediction accuracy over plain surrogate models is significant and impressive."}, "weaknesses": {"value": "- **Practical implication:** Table1 of the main experiments show that surrogate models already outperform the MLP solver in most of the scenarios. It is not very clear to me what practical scenario necessitates an additional computational budget to boost the performance of the trained data-driven models, especially when the performance of the data-driven model is already superior to that of the solvers.\n\n- **Assumption for experiments:** The hyperparameter configuration for the MLP used in the experiment is not stated exhaustively and it makes it hard to access the fairness of the experiments. \n\n- **Mathematical notations for theoretical results:** I see the authors make a tremendous effort to solidify the theoretical ground of the proposed framework. However many mathematical notations are not introduced satisfactorily and I could not access the correctness of the theoretical result. Followings are some of the points which I saw were necessary to be introduced properly when I went through Appendix C. In addition, some of the symbols and notations are apparently coming from the previous sections. I suggest the section dedicated for the notations should be placed at the beginning of the appendix.\n  - Topology and metric on the extend real numbers.\n  - The definition of $\\mathcal{B}(\\mathbb{R}^{d+1})$.\n  - It is not clear that for which mathematical objects the assumptions 5 and 6 are made.\n  - The definition of the terminal function (I speculate that it is a function describing terminal conditions for PDEs)\n  - $\\breve{F}$ in Definition8. Apparently this is different from one introduced in Fact 1 in the main text. \n  - I still do not understand the assertion in Remark 8."}, "questions": {"value": "- What does training points $m$ mean? How does $m$ training points translate to $m$ additional Monte Carlo paths? \n- How and/or along which metric is the computational budget given for MLP in the main experiments ensured to be comparable to that taken for surrogate models and the SCaSML?\n- How long/how many training epochs/what change in hyperparameters is additionally necessary for MLP and SR models to reach the comparable performance of SCaSML reported in Table 1? While theoretical analysis on the variance and computational cost is given, I believe this observation and/or potential contrast could strengthen the significance of the contribution of the paper. \n- In what practical scenario do the authors expect the proposed framework play a crucial role?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CvA9EnQnLJ", "forum": "d2pUyiXwcm", "replyto": "d2pUyiXwcm", "signatures": ["ICLR.cc/2026/Conference/Submission4617/Reviewer_op81"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4617/Reviewer_op81"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761316441488, "cdate": 1761316441488, "tmdate": 1762917470577, "mdate": 1762917470577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses inference-time scaling for machine-learning (ML) solvers for high-dimensional partial differential equations (PDEs). Concretely, the idea is to train a physics-informed neural network (PINN) or Gaussian process (or any other PDE solver that yields a continuously differentiable solution), derive the defect correction PDE which describes the error of the approximation, and solve this error PDE with a Feynman-Kac-based solver at inference time to correct the PINN prediction.\nThis chaining of solvers improves the accuracy order: the final error is the product of the individual errors. \nThe resulting algorithm is benchmarked on a range of PDEs in up to 160 dimensions, including a Hamilton--Jacobi--Bellman problem.\n\n\n**Summary of my recommendation:**\nI appreciate the idea of inference-time corrections to PINN predictions, and I definitely consider it promising. However, as is, I recommend rejecting this work because the defect correction approach is not as novel as the submission states (details below) and because the submission's experiments have some gaps that need to be filled before publication. I believe that if a future version of this manuscript embedded itself better into the literature of numerical solvers and if the experimental gaps were filled, it would be stronger, and I would probably have given a different score. But I believe that these changes are beyond the scope of a revision during the rebuttal period, which is why I give a \"reject\" score."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Even though the idea of chaining two (or more) solvers, like the submission does with PINNs/GPs and Feynman--Kac simulations, is not new, its application to training-vs-inference stages in physics-informed ML is novel. Inference-time scaling had a significant impact on LLMs (though I'm not an expert on LLMs), and I wouldn't be surprised if it became standard in future versions of physics-informed ML solvers as well. \nFurthermore, I appreciate the mathematical rigour of the analysis and the fact that the experiments cover a range of problems, including linear and nonlinear ones, as well as truly high-dimensional problems (dim $\\gg$ 100)."}, "weaknesses": {"value": "Despite the submission's strengths, I think three weaknesses dominate, which is why I ultimately recommend rejecting this work: \n\n1. What the paper calls the \"law of total defect\" (Section 2.1, Definition 1) is a defect correction approach, and defect correction methods have been studied in numerical analysis for a long time; for example, see:\n\n    > Stetter, Hans J. \"The defect correction principle and discretization methods.\" Numerische Mathematik 29.4 (1978): 425-443.\n\n    > Dutt, Alok, Leslie Greengard, and Vladimir Rokhlin. \"Spectral deferred correction methods for ordinary differential equations.\" BIT Numerical Mathematics 40.2 (2000): 241-266.\n\n    and many others. The submission suggests that the \"law of total defect\" is a central contribution of this work (e.g. in lines 017f, 063f, or 073f), which is misleading because describing the approximation error of a PDE via the defect equation is a standard approach in numerical analysis. Furthermore, it's known that the approximation order of defect correction approaches is the product of the individual orders, which raises questions about the novelty of the submission's theoretical analysis in Theorem 3. In summary, while the application of defect correction techniques to training vs. inference stages of ML-based PDE solvers is new, the general construction is not. For a future version of this manuscript, I would recommend positioning this work more clearly as a ML-application of existing defect correction techniques (adjusting the claims and the related work accordingly).\n\n2. The distinction between training and inference time of ML-based PDE solvers is a bit arbitrary in the current setup: both the PINN and the Monte-Carlo solver contribute to solving the PDE, and there is no reason to delay the Monte-Carlo solver to inference time (unless I have missed something). I imagine that the separation of training and inference would become clearer for parameter estimation problems, where external data enters the loss function, but parameter estimation is not studied in the current version of the submission. I believe that a discussion of inference-time scaling with ML-based PDE solvers requires a more thorough distinction of training and inference time.\n\n3. The numerical results, while broad, are not as conclusive as they could be. On the one hand, the accuracy differences between the methods are small, but standard deviations are not reported. From the current results, it is unclear whether the improvement is statistically significant. On the other hand, I think that the accuracy improvements would be more convincing if they were normalised by budget in the sense that all three \"competitors\" (PINN/GP, MC, PINN/GP+MC) should use similar compute (either function evaluations or runtime/memory). For example, in Table 1, the baselines are slightly less accurate but much faster, and I don't know whether running the baselines for longer would make them more accurate.\n\n\n4. (Minor) Finally, and I consider this to be a smaller weakness than the three above, I wonder whether the volume of the paper (almost 40 pages) is a bit too high for an ICLR paper, and not necessarily proportional to the amount of contributions of this work. I understand it is common practice to defer proofs and extra results to the appendices, but it is currently almost impossible to read the main paper without extensive reference to the supplement. That said, while I think this point is worth mentioning, I consider it less important than the three above."}, "questions": {"value": "The following questions don't affect my score, but I think including their answers in a revision would improve the submission.\n\n- Line 038f suggests that ML-based solvers don't suffer from the curse of dimensionality like traditional solvers do. This statement needs to be supported more strongly.\n- Lines 061f: PINNs are not surrogate models. A surrogate model is an approximate model used to replace an expensive simulation. PINNs are (at their core) PDE solvers. I suggest replacing \"surrogate model\" with \"ML-based PDE solvers\".\n- Experiment in Section 3.1: what's the boundary condition? There are boundary collocation points, but the problem statement does not mention boundary conditions.\n- Line 126: How costly is it to compute the full Laplacian in 160 dimensions? \n- Lines 365f: Defect correction methods are sometimes unreliable if the residual is only approximate. How problematic is it to use Hutchinson's estimator for evaluating Laplacians here? For reference, Lines 425f suggest that inaccurate Laplacians are problematic for the experiment in Section 3.4"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hJOHuFAHsc", "forum": "d2pUyiXwcm", "replyto": "d2pUyiXwcm", "signatures": ["ICLR.cc/2026/Conference/Submission4617/Reviewer_Wcpd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4617/Reviewer_Wcpd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757071059, "cdate": 1761757071059, "tmdate": 1762917470067, "mdate": 1762917470067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to estimate and correct the error predicted by a surrogate model when solving a PDE equation. \n\nThe method applies to semi-linear parabolic PDE equations and is based on recent results on Multilevel Picard (MLP) solvers.  \n\nThe paper describes the approach, and in the Annex provides extensive details of the approach. \n\nThe paper evaluates the approach on a few tasks: linear convection-diffusion equation, viscous Burger equation, high-dimensional Hamilton-Jacobi-Bellman equation, and diffusion-reaction equation. \n\nIt would be nice to highlight the computational or sample complexity of the method (there is a paragraph in 2.3, maybe a clarification of the impact on the specific cases in the annex would be nice).\n\nThe paper mentions in the first paragraph the use of the method in various applications:\n\n1. Schroedinger equation in quantum many-body systems, \n2. nonlinear Black–Scholes equations in finance, and \n3. the Hamilton–Jacobi–Bellman equation \n\nwhile there are experiments for the last, it would be nice to understand if the application to the other two cases requires some modification to the approach. \n\nIt is not completely clear what is observed. From eq. (2), the forcing term f(r,y) is used to define dht error. It is not clear what is the connection between the error (or residual) and the defect. We have access to the prediction $\\hat u(r,y)$, what is also observed?"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper presents a post-prediction method that improves the accuracy by solving a PDE, that describes the \"defect\", which is then computed by solving the \"law of defect\" or the error PDE."}, "weaknesses": {"value": "Only clarification.\n\nIt would be nice if the authors would release the code, since it would be difficult to build on top of this work."}, "questions": {"value": "Mentioned before:\n\n1. Clarify the applicability to other problems. \n2. Give an intuitive explanation of what the expectation operator $\\Phi$ of Eq. (8) does\n3. possibly a complexity (sample, computation) analysis (probably what is described in line 264 and 1662, a summary of the results would be nice)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AgrX3rgL8x", "forum": "d2pUyiXwcm", "replyto": "d2pUyiXwcm", "signatures": ["ICLR.cc/2026/Conference/Submission4617/Reviewer_Kghx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4617/Reviewer_Kghx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923771909, "cdate": 1761923771909, "tmdate": 1762917469808, "mdate": 1762917469808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to start from an initial solution obtained by neural operators and then solve the residue as a new PDE solving problem. This paper uses two variants of MCMC to solve the new PDE. Theoreical results are provided to show the proposed method has a faster convergence rate. The paper conducted experiments on multiple datasets and achieved the best accuracy among all the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. By formulating the residual part as another PDE, the paper further improves the PINN and GP based surrogates without training any new networks.\n2. The paper theoretically shows the proposed method has a faster convergence rate.\n3. The proposed method is compared on multiple datasets and achieves the best accuracy among all the baselines."}, "weaknesses": {"value": "1. The idea of correcting the initial solution is not new. The paper chooses to use the existing MCMC variants (quadrature MLP and full-history MLP) for the correction, which are also not new. \n2. It is not clear to me how the paper stands out from the existing works, where correction is applied to the initial solution for improvement.\n3. The experiments are not convincing for that the baselines (PINN) is not tuned enough. A fixed architecture, a fixed optimization strategy, and a fixed number of points are used. To show the effectiveness of the proposed method, the authors can consider starting with a stronger initial solution. More layers and more collocation points will lead to more runtime, and it is interesting to see how that part scales with the complexity compared with the proposed method. For example, for SR, the runtime is about 1 - 2 seconds, while for the proposed method, the runtime is nearly 10 times. I am curious under the similar budget, what will make the proposed method stand out."}, "questions": {"value": "1. Is there any prior work that uses MCMC, deep learning, or traditional numerical solvers for correcting the initial solutions? If there exists some, the authors should discuss and possibly compare with them.\n2. Can the authors justify the improvement drop in Fig. 3?\n3. Have the authors tried variants of PINN and what are the errors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "plVi0TxaRl", "forum": "d2pUyiXwcm", "replyto": "d2pUyiXwcm", "signatures": ["ICLR.cc/2026/Conference/Submission4617/Reviewer_RnQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4617/Reviewer_RnQk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185633939, "cdate": 1762185633939, "tmdate": 1762917469461, "mdate": 1762917469461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}