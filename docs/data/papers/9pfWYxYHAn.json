{"id": "9pfWYxYHAn", "number": 10374, "cdate": 1758168712803, "mdate": 1759897655288, "content": {"title": "GAKD: Generative Adversarial Knowledge Distillation For Large Language Models", "abstract": "Current white-box knowledge distillation (KD) methods for large language models (LLMs) often rely on distribution distance metrics, such as forward or reverse Kullback–Leibler Divergence (KLD), as optimization objectives. However, KLD objective only provides token-wise feedback during knowledge distillation, lacking long-range, sequence-level signals and leading to poor distribution alignment between the teacher and student models. To address this, we propose the Generative Adversarial Knowledge Distillation (GAKD) framework, which adopts a minimax adversarial strategy. Specifically, GAKD trains: (1) a generator (student) to align with the teacher's distribution via a combination of sequence-level adversarial loss and reverse KLD loss, and (2) a discriminator to distinguish whether per-token logits are from the teacher or student. By jointly minimizing the token-level reverse KLD and sequence-level adversarial losses, GAKD enables the student model to more effectively align with the teacher’s distribution, leading to improved performance. Furthermore, we provide a mathematical proof of the feasibility of optimizing reverse KLD loss on teacher-generated sequences, establishing the theoretical soundness of GAKD. Experimental results on the instruction-following tasks, conducted on the Qwen-3 model families (with parameters ranging from 0.6B to 8B), demonstrate that utilizing the sequence-level signals, GAKD generates more accurate responses than the SOTA baselines, especially in the long-text generation scenario. Our code can be found in https://anonymous.4open.science/r/GAKD-8753/.", "tldr": "This paper introduces GAKD, a novel adversarial knowledge distillation method that leverages adversarial training and reverse KL divergence to enhance student model alignment with large language model teachers.", "keywords": ["knowledge distillation", "LLM"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/771425001c373ae6f9f0eb9d50e7f23429f4da7f.pdf", "supplementary_material": "/attachment/3f44b1b3d3a11682db32336c97d02c83729f6fff.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel knowledge distillation technique for large language models by integrating an adversarial training pipeline with a discriminator to verify the outputs of the teacher and student models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The knowledge distillation task is highly significant, especially at the current stage where large language models are too big and difficult to deploy on small or personalized devices.\n3. Theoretical proofs provide a rigorous foundation for the validity and consistency of the proposed methods."}, "weaknesses": {"value": "1. The concept of adversarial training has shown greater effectiveness in image domains, where data is continuous and such training can substantially enhance image quality. In contrast, for text generation tasks, where data is discrete, adversarial training tends to be less effective. This indicates that incorporating a GAN loss in addition to the KD loss does not yield significant improvements. Experimental results further demonstrate that the proposed method consistently performs on par with, or below, existing approaches.\n\n2. In the experiments, it would be more informative to evaluate the performance when using a larger teacher model."}, "questions": {"value": "1. Since the KD loss already aligns the student’s output distribution with that of the teacher in discrete space, the additional GAN loss essentially optimizes in the same direction. This raises the question of how the GAN loss can further contribute to performance improvement in this task.\n2. Does any part of our paper explicitly address the challenge of \"lacking signals for long-range consistency or higher-order dependencies across the sequence\" (as you mentioned in introduction section)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9dSgRX3ajk", "forum": "9pfWYxYHAn", "replyto": "9pfWYxYHAn", "signatures": ["ICLR.cc/2026/Conference/Submission10374/Reviewer_iCmr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10374/Reviewer_iCmr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625552861, "cdate": 1761625552861, "tmdate": 1762921695897, "mdate": 1762921695897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose GAN-style knowledge distillation, where they train a generator and a discriminator. The generator (student) is encouraged to generate logits that cannot be distinguished from the teacher logits with the discriminator. Results suggest the proposed method is better than existing divergence-based distillation methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is reasonable: the authors adopt relativistic GAN, which helps them avoid mode collapse issues.\n2. The authors propose importance sampling to reduce sampling cost with RKL, which is a nice touch."}, "weaknesses": {"value": "1. The improvement seems somewhat inconsistent, specifically with smaller models. Results with different methods are generally close.\n2. ROUGE-L and BLEU scores may not be the best evaluation method. Given how unstable these metrics are, I recommend the authors try some model-based metrics.\n3. Ablation on different GAN variants suggest that the effect of mode collapse may not be as big as the authors claim."}, "questions": {"value": "How efficient is the proposed method in terms of distillation, compared to other methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FOeSwLHpZL", "forum": "9pfWYxYHAn", "replyto": "9pfWYxYHAn", "signatures": ["ICLR.cc/2026/Conference/Submission10374/Reviewer_UAfz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10374/Reviewer_UAfz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714160879, "cdate": 1761714160879, "tmdate": 1762921695297, "mdate": 1762921695297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework to address a key limitation in existing knowledge distillation (KD) methods. Traditional techniques like Kullback-Leibler Divergence (KLD) only provide \"token-wise\" feedback, lacking a global, \"sequence-level\" signal.\n\nGAKD solves this by adopting a Generative Adversarial (GAN) strategy. It trains the student model (the \"generator\") using a joint objective: 1) a reverse KLD loss for local, token-level alignment, and 2) a sequence-level adversarial loss from a \"discriminator\" that learns to distinguish between teacher and student outputs. This dual-feedback mechanism enables the student to better match the teacher's overall distribution, showing superior performance, especially in long-text generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Framework: It proposes the GAKD framework, innovatively combining token-wise KLD loss with a sequence-level adversarial (GAN) loss.\n\n2. Superior Long-Text Performance: This dual-feedback mechanism effectively solves the problem of missing global signals, leading to better alignment and performance, especially in long-text generation."}, "weaknesses": {"value": "1. The motivation \"KLD objective only provides token-wise feedback during knowledge distillation, lacking long-range, sequence-level signals and leading to poor distribution alignment between the teacher and student models\" is questionable. In MiniLLM[1] and GKD[2], the long-range information is already considered by on-policy optimization. For example, MiniLLM directly minimizes the **sequence-level** reverse KLD, and the final gradient also includes a term targeting the long-range information. Although the empirical experiments in the paper show that GAKD outperforms the baselines, more experiments are needed to justify whether the improvement is achieved by considering the long-range signals.\n\n2. More details about the discriminator are needed. For example, how large the model is and how much additional computation cost it would introduce during KD."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FucwBTLGfu", "forum": "9pfWYxYHAn", "replyto": "9pfWYxYHAn", "signatures": ["ICLR.cc/2026/Conference/Submission10374/Reviewer_HUfL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10374/Reviewer_HUfL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903253258, "cdate": 1761903253258, "tmdate": 1762921694590, "mdate": 1762921694590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Generative Adversarial Knowledge Distillation (GAKD), a white-box KD framework for LLMs. GAKD reframes distillation as a minimax adversarial game , training a Student (Generator) to create logits that a Discriminator cannot distinguish from the Teacher's. The Student's objective is a composite loss combining a sequence-level adversarial loss, a token-level Reverse KLD loss, and a standard Negative Log-Likelihood loss . The paper includes a proof (Corollary 1) to support optimizing the Reverse KLD using teacher-generated sequences via importance sampling. Experiments on Qwen-3 models report that GAKD outperforms baselines , especially in long-text generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper is well-motivated, addressing a significant  limitation of traditional knowledge distillation: standard token-level objectives (e.g., token-wise KL) lack the long-range, sequence-level signals required for generating coherent, long-form text.\n\nS2. GAKD demonstrates some empirical advantage over the specific baselines chosen for comparison. The method consistently outperforms off-policy token-level objectives (SFT, Supervised KD) and the MiniLLM framework on the reported instruction-following tasks (On-policy is not compared tho)."}, "weaknesses": {"value": "W1. Flawed Motivation and Missing Baselines: The paper's core motivation rests on the claim that on-policy sampling (from the student) is a \"drawback\" that \"adversely affect[s]\" performance. This assertion is outdated and mischaracterizes the current state of KD literature. A large body of recent works (e.g., GKD (https://arxiv.org/pdf/2306.13649), SKD (https://arxiv.org/abs/2410.11325)) is built specifically on on-policy sampling, proposing various methods to stabilize it (sft student model or interleaved sampling). Authors failed to compare to those approaches and justify their claims.\n\nW2. Unjustified Adversarial Complexity: The discriminator ($D_{\\phi}$) is functionally a learnable reward model that provides a sequence-level score. The paper fails to justify why this signal must be delivered via a complex, unstable adversarial minimax game. A simpler baseline would be to train the discriminator as a static reward model and add its score directly to the loss (GKD (https://arxiv.org/pdf/2306.13649) already has this study). By not comparing against this simpler alternative, the paper fails to prove that the adversarial component provides any benefit.\n\nW3. Critically Confounded Ablation: The paper's central claim is that its adversarial sequence-level approach is superior to token-level methods. However, the GAKD loss function introduces two new signals at once: a sequence-level RKLD (via importance sampling) and a sequence-level adversarial loss. The experiments never disentangle these two effects. A critical ablation with the adversarial weight ($\\beta$) set to zero is missing. We cannot know if the performance gain comes from the sequence-level RKLD term or the (unjustified) adversarial loss.\n\nW4. Lack of Stability Analysis: The paper's own ablations (Table 2, Figure 3) show that performance is highly sensitive to the $\\alpha$ and $\\beta$ hyperparameters. For example, dollyeval needs low $\\beta$ while SNI needs relatively higher $\\beta$. The paper lacks a rigorous analysis of training stability, hyperparameter sensitivity, or potential mode collapse, all of which are well-known and critical failure modes for GANs. The method doesn't seem to generalize to unseen tasks.\n\nW5: The sequence-level importance weight $w(x,y) = \\prod_{t=1}^{T} \\frac{q_{\\theta}(y_t | \\cdot)}{p(y_t | \\cdot)}$ is a product of many ratios. For any non-trivial sequence length $T$, this product is numerically unstable and prone to vanishing (if $q_{\\theta} < p$) or exploding (if $q_{\\theta} > p$). The paper fails to adequately discuss how this instability is managed (e.g., log-space computation, gradient clipping) to ensure stable training."}, "questions": {"value": "1. What is the computation overhead of this approach compared to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ONcs0UKFcg", "forum": "9pfWYxYHAn", "replyto": "9pfWYxYHAn", "signatures": ["ICLR.cc/2026/Conference/Submission10374/Reviewer_GjnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10374/Reviewer_GjnC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762302005714, "cdate": 1762302005714, "tmdate": 1762921694170, "mdate": 1762921694170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}