{"id": "6whdepL0v1", "number": 13673, "cdate": 1758220721340, "mdate": 1759897420561, "content": {"title": "Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory", "abstract": "Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest such as harmfulness, bias, or other properties.\nIn this paper, we use information-theoretic analysis to show that a task requiring CoT is a necessary, but not sufficient, condition for CoT monitorability.\nWe identify two sources of approximation errors that may undermine the performance of CoT monitors in practice: _information gap_ which intuitively measures the extent to which the monitor can extract the information available in CoT, and _elicitation error_ which measures the extent to which the monitor approximates the optimal monitoring function.\nWe further demonstrate that CoT monitorability can be systematically improved through targeted training objectives.\nTo this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs.\nIn a coding honeypot environment, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking even when the task reward is imperfectly specified.", "tldr": "CoT monitorability can be improved via training with specific objectives", "keywords": ["chain-of-thought monitoring", "mutual information", "AI Safety", "reasoning", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec4b4734131287e6100801f307595c6d03e23556.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- This paper provides an information-theoretic analysis of CoT monitoring, converting this analysis into a practical mitigation that can help improve monitorability during training\n- The conditions in which CoT can provide a performance uplift are formalized. It is shown that strictly positive uplift in CoT monitorability requires additional positive mutual information between the CoT and the output, beyond what the prompt provides\n- Next, the paper considers approximate CoT monitors, and states that the errors in an approximate monitor come from: (i) information gap - how well the monitor can predict the outputs of the model given the prompt and CoT; and (ii) elicitation error - how well the monitor can approximate the true output monitoring function.\n- A solution that bears on 'information gap' is proposed, one that train the model to produce more informative CoTs. Specifically, an oracle-based reward, and a label-free reward term are porposed. The label free reward is based on an estimate of the conditional mutual information in the models CoT.\n- Their mitigations are tested in an MBPP setup with a hackable task reward, while also training against a CoT monitor. Here, the main finding is that using the mitigation reward terms can prevent reward hacking in this setting."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The contributed information-theoretic framework for CoT monitorability adds a novel and potentially useful and clarifying perspective to the field. Formalizing the importance of mutual information between CoT and outputs is useful for thinking about the problem. Similarly, concretely and formally identifying two distinct sources of CoT monitoring errors is useful for designing mitigations/improvements.\n- The analyses have led to a novel and practical mitigation in the form of the proposed mutual information reward. In the provided experiments, this mitigation shows promising signs of life, greatly reducing MBPP reward hacking rates when training against a CoT monitor.\n- The writing is very clear throughout.\n- Overall, this theory and experiments could be significant and influential in driving future important research along similar directions. I am excited to see future work further investigating the efficacy of the mutual information reward, and other similar rewards that could improve the monitorability of the model's CoT."}, "weaknesses": {"value": "**Limited experiments.**\n\nThe current set of experiments feel very limited in breadth and depth. This makes it hard for me to gauge how promising the MI reward truly is. I will outline additional experiments I would like to see below. Ideally, this paper would add a more thorough set of empirical results.\n\n**Theory yields somewhat expected takeaways.**\n\nWhile the provided theory is well executed and presented, the main takeaways from this theory are perhaps not overly surprising or insightful. Indeed Section 3.2 findings have been shown empirically by [Emmons et al (2025)](https://arxiv.org/abs/2507.05246).\n\nThe takeaways I am referring to are: (1)  \"task requiring a CoT is a necessary, but not sufficient, condition for CoT\nmonitorability\", and (2) the sources of error are information gap and elicitation error.\n\nNevertheless, I still maintain it is valuable to highlight and fomralize these, as done in the paper.\n\n**Confusions regarding CoT being necessary for a task.**\n\nIn contribution 1, line 045, you state “...use it to show that a task requiring a CoT is a necessary, but not sufficient, condition for CoT monitorability”. Here, “a task requiring CoT” implies it is a task where the use of CoT is required to get good task performance, as in Emmons et al (2025). However, your analysis is based on the MI between CoT and output, and does not seem to consider the extent to which CoT is necessary for obtaining high task reward. It seems to me that the MI between CoT and output can still be high, even when CoT is not strictly necessary for the task? (i.e., even when the model can maximize task reward without the use of CoT). It seems important to make this distinction clear throughout the paper.\n\nLikewise, it would be useful to emphasize whether the MBPP experiment setting is one where use of CoT is necessary to obtain high task reward.\n\n**Unclear how previously empirically observed CoT monitoring failure modes relate to the provided theory.**\n\nIn general, the paper does not seem to consider CoT monitoring failure modes that have been highlighted in the literature. For example, [Arnav et al (2025)](https://arxiv.org/abs/2505.23575) show \"CoT traces can also contain misleading rationalizations that deceive the monitor\". I would be interested to hear from the authors how their theory accounts for empirical failure modes like this, and whether this deserves some more discussion in the paper."}, "questions": {"value": "**Possible additional experiments.**\n\nThere are additional experiments that could clarify my understanding of the promise of the MI reward:\n\nI would like to see more experiments that directly show the “monitor accuracy” metric being higher after training against R_MI, versus baselines. Currently, the experiments in Figure 2 doesn’t seem to directly do this. For example, you could consider training runs where there is no R_monitor, and the reward is just R_task + R_MI. This could help properly disentangle improvements in monitorability from the reward hack metric. It could also be useful to present somewhere monitor accuracies for hack and non-hack solutions seperately\n \nIn general, I would probably prefer to see some theory moved into the appendix and more depth of analysis of the experimental results in the main paper.\n\nI would be particularly interested to know whether CoT is necessary for the Qwen models used on the current coding tasks. It would be useful to include results in an experimental setting where CoT is necessary - in these settings the MI reward could hurt task performance and have a higher \"alignment tax\"?\n\nIt could be nice to have experiments with reasoning models, though not crucial.\n\nIt could be nice to have experiments in a setting where the monitor looks at both the CoT and the outputs.\n\nIn general, results in more environments would be informative regarding the generality of your empirical findings. At least some discussion of the simple-equations environment results in the main body could be useful.\n\n**Other.**\n\nThe transition from the identified sources of error in the theory to the practical mitigations used could perhaps be better justified. Corollary 3.14 identifies policy-as-monitor as a solution to information gap, however the paper instead proposes the MI reward. Additionally, there is not much discussion regarding why the paper decides to focus on the information gap error in its experiments, and not the elicitation error? I think what is done in the paper is mostly very reasonable, but could perhaps be better justified and tied back more precisely to the prior theory."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QPdPXOJnzU", "forum": "6whdepL0v1", "replyto": "6whdepL0v1", "signatures": ["ICLR.cc/2026/Conference/Submission13673/Reviewer_pTo5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13673/Reviewer_pTo5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945486776, "cdate": 1761945486776, "tmdate": 1762924237640, "mdate": 1762924237640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1) The paper’s entire theoretical framework makes a set of assumptions which, IIUC, **rule out the value of CoT monitoring a-priori**. In particular, sections 3.1-3.2 assume that:\n\nA) Z (CoT) can only influence Y (attribute being monitored) via O (the model’s action).\n\nB) The monitor is bayes-optimal.\n\nBut in this setting, it is *theoretically impossible* for the CoT to provide information to the monitor beyond that contained in the model’s action. Why bother monitoring the CoT at all, rather than the action?\n\nFurthermore, the optimal CoT in terms of monitorability would be one which contains all of the information in O. The model could just *repeat O as its CoT*, and this would provide the maximum possible uplift as defined in Definition 3.7.\n\nThe paper attempts to improve the realism of its setup with subsection 3.3, which introduces “approximate CoT monitors”, accounting for monitors which may have limited capacity and computation. It then decomposes monitor error into two components: the “information gap” measuring how well the monitor’s internal model q(O | X,Z) matches the agent’s true policy, and elicitation error, measuring how well the monitor can approximate the true output monitoring function.\n\nBut in implicitly treating these terms as independent, the paper ends up in the same position as before: it is *theoretically impossible* for Z to provide information about Y to the monitor beyond that contained in O. Why monitor the CoT at all? The paper even seems to state this as a result: Corollary 3.14 shows that using the agent’s own policy to predict its own outputs achieves zero information gap. In other words, according to the paper’s assumptions, monitoring the agent’s actual actions is optimal; monitoring the CoT can’t possibly add value over just monitoring actions!\n\n2) The paper attempts to draw connections between its results and existing empirical work in “Do CoTs that matter for the task also help monitoring?”, claiming its theoretical analysis shows “that the necessity of CoT reasoning for task completion is a necessary precondition for the monitorability of any attribute Y of the final output.” But proposition 3.11 only shows that positive uplift requires positive *mutual information* between O and Z, not that Z is necessary for O. In particular, consider the following scenario:\n\nA) First, the LLM performs the task and decides on its output O internally in a single forward pass, without any necessary CoT.\n\nB) Second, the LLM outputs O as its CoT.\n\nC) Finally, the LLM outputs O as its final output.\n\nThe LLM’s CoT and its final output are perfectly correlated, and will therefore have the maximum possible mutual information. But the CoT had no necessity for the task whatsoever.\n\n3) The empirical results, while interesting and suggestive of a trend, only evaluate two models on a single environment. Evaluating additional environments would provide more confidence in the robustness of the results.\n\n4) The monitor for the RL experiments only observes the CoT. The experiments should also compare to a monitor which observed the actions, and a monitor which observes both actions and CoT, to show whether the CoT is providing value above simple action monitoring.\n\nIn particular, it seems like the current reward function would incentivize CoTs to just explicitly state the action that they’re going to take; but then why not just monitor actions?"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1) The paper’s empirical results are interesting, and Figure 2 does a good job at communicating the value of the mutual information reward in the given environment.\n\n2) The discussion of steganography in Appendix A, and the empirical identification of examples, is interesting (although it also highlights the limitations of the paper’s theoretical justifications for optimizing MI)."}, "weaknesses": {"value": "1) The paper’s entire theoretical framework makes a set of assumptions which, IIUC, **rule out the value of CoT monitoring a-priori**. In particular, sections 3.1-3.2 assume that:\n\nA) Z (CoT) can only influence Y (attribute being monitored) via O (the model’s action).\n\nB) The monitor is bayes-optimal.\n\nBut in this setting, it is *theoretically impossible* for the CoT to provide information to the monitor beyond that contained in the model’s action. Why bother monitoring the CoT at all, rather than the action?\n\nFurthermore, the optimal CoT in terms of monitorability would be one which contains all of the information in O. The model could just *repeat O as its CoT*, and this would provide the maximum possible uplift as defined in Definition 3.7.\n\nThe paper attempts to improve the realism of its setup with subsection 3.3, which introduces “approximate CoT monitors”, accounting for monitors which may have limited capacity and computation. It then decomposes monitor error into two components: the “information gap” measuring how well the monitor’s internal model q(O | X,Z) matches the agent’s true policy, and elicitation error, measuring how well the monitor can approximate the true output monitoring function.\n\nBut in implicitly treating these terms as independent, the paper ends up in the same position as before: it is *theoretically impossible* for Z to provide information about Y to the monitor beyond that contained in O. Why monitor the CoT at all? The paper even seems to state this as a result: Corollary 3.14 shows that using the agent’s own policy to predict its own outputs achieves zero information gap. In other words, according to the paper’s assumptions, monitoring the agent’s actual actions is optimal; monitoring the CoT can’t possibly add value over just monitoring actions.\n\n2) The paper attempts to draw connections between its results and existing empirical work in “Do CoTs that matter for the task also help monitoring?”, claiming its theoretical analysis shows “that the necessity of CoT reasoning for task completion is a necessary precondition for the monitorability of any attribute Y of the final output.” But proposition 3.11 only shows that positive uplift requires positive *mutual information* between O and Z, not that Z is *necessary* for O. In particular, consider the following scenario:\n\nA) First, the LLM performs the task and decides on its output O internally in a single forward pass, without any necessary CoT.\n\nB) Second, the LLM outputs O as its CoT.\n\nC) Finally, the LLM outputs O as its final output.\n\nThe LLM’s CoT and its final output are perfectly correlated, and will therefore have the maximum possible mutual information. But the CoT had no necessity for the task whatsoever.\n\n3) The empirical results, while interesting and suggestive of a trend, only evaluate two models on a single environment. Evaluating additional environments would provide more confidence in the robustness of the results.\n\n4) The monitor for the RL experiments only observes the CoT. The experiments should also compare to a monitor which observed the actions, and a monitor which observes both actions and CoT, to show whether the CoT is providing value above simple action monitoring.\n\nIn particular, it seems like the current reward function would incentivize CoTs to just explicitly state the action that they’re going to take; but then why not just monitor actions?"}, "questions": {"value": "1) What are potential reasons one might want to monitor CoT, rather than just monitoring actions? Could you account for these theoretically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oad3TKZEur", "forum": "6whdepL0v1", "replyto": "6whdepL0v1", "signatures": ["ICLR.cc/2026/Conference/Submission13673/Reviewer_m7dV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13673/Reviewer_m7dV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948367886, "cdate": 1761948367886, "tmdate": 1762924237321, "mdate": 1762924237321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the “monitorability” of chain-of-thought (CoT) reasoning (i.e., how well we can predict if attributes of interest $Y$ like hacking/malicious intent/bias are present/factors in a model’s response $O$ to an input $X$ by examining the LLM’s reasoning trace $Z$). The main contribution of this paper is to ground this analysis in an information theoretic framework, and uses this grounding to make the (intuitive) argument that monitoring is possible only when the CoT $Z$ carries information that is relevant to the attribute $Y$, and that tasks requiring an informative CoT (high $I(O;Z|X)$) provide a necessary (but not sufficient) condition for this monitorability. From an applied perspective, the authors propose a label-free auxiliary reward that maximizes the mutual information between the output and the CoT to encourage models to produce informative CoTs (and thereby, improve the necessary but not sufficient condition for monitorability)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The scope and direction of the paper is interesting and timely: understanding how to reliably monitor LLM reasoning is a critical, unsolved problem.\n- The analysis leads to a practical label-free reward that appears to lead to modest improvements in monitorability (+ resistance to hacking when paired with monitoring reward + task reward).\n- A precise language for discussing monitorability (and formalizing its necessary conditions) can be valuable for the community (however, the setup is arguably a bit long-winded in getting to the main conclusions — and not much space is left for the experiments)."}, "weaknesses": {"value": "- The theoretical argument is nicely done, but it’s unclear how much new insight it offers (e.g., it is intuitively obvious that a CoT must contain information relevant to the output + attribute of interest to be useful for monitoring). \n- The paper proves that maximizing I(O, Z | X) is not sufficient, but this is not reflected in the proposed reward. What stops the model from gaming this reward by simply outputting the final answer in the thought?\n- It’s not that obvious how well the MI reward would generalize to other tasks — the abstract and introduction motivate broad concepts like “harmfulness” and “bias” yet the experiments focus on fairly specific forms of hacking.  It’s unclear if maximizing I(O, Z | X) would be effective for more complex attributes (and if a model cannot learn to easily obfuscate the parts relevant to Y but keep parts relevant to O)."}, "questions": {"value": "- Could the authors elaborate on what prevents a model from maximizing $R_{\\text{MI}}$ by producing a CoT $Z$ that is simply a copy of the output $O$? Is the <think>/<code> XML structure reward (mentioned briefly in Sec 5.1) required to prevent this?\n- The MI estimator seems to be a critical, non-trivial component. How sensitive is the model to the choice of a and b? How necessary is it to normalizing by length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jM7CAFGcXC", "forum": "6whdepL0v1", "replyto": "6whdepL0v1", "signatures": ["ICLR.cc/2026/Conference/Submission13673/Reviewer_NPUK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13673/Reviewer_NPUK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141986580, "cdate": 1762141986580, "tmdate": 1762924236971, "mdate": 1762924236971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}