{"id": "reOA4r0FGL", "number": 17790, "cdate": 1758280563327, "mdate": 1759897153712, "content": {"title": "A Generalized Information Bottleneck Theory of Deep Learning", "abstract": "The Information Bottleneck (IB) principle offers a compelling theoretical framework to understand how neural networks (NNs) learn. However, its practical utility has been constrained by unresolved theoretical ambiguities and significant challenges in accurate estimation.\nIn this paper, we present a \\textit{Generalized Information Bottleneck (GIB)} framework that reformulates the original IB principle through the lens of synergy, i.e., the information obtainable only through joint processing of features. We provide theoretical and empirical evidence demonstrating that synergistic functions achieve superior generalization compared to their non-synergistic counterparts. Building on these foundations we re-formulate the IB using a computable definition of synergy based on the average interaction information (II) of each feature with those remaining. We demonstrate that the original IB objective is upper bounded by our GIB in the case of perfect estimation, ensuring compatibility with existing IB theory while addressing its limitations. \nOur experimental results demonstrate that GIB consistently exhibits compression phases across a wide range of architectures (including those with \\textit{ReLU} activations where the standard IB fails), while yielding interpretable dynamics in both CNNs and Transformers and aligning more closely with our understanding of adversarial robustness.", "tldr": "", "keywords": ["Information Bottleneck", "Mutual Information", "Generalization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13f151e4b6058bf13b764e03f590b6b6ff160177.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a Generalized Information Bottleneck (GIB) framework that reformulates the original Information Bottleneck principle through the lens of synergy - information obtainable only through joint processing of features. The authors claim that synergistic functions achieve superior generalization and present a computable definition of synergy based on average interaction information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. GIB demonstrates consistency in exhibiting compression phases across diverse architectures and activation functions.\n2. The complexity term's correlation with adversarial vulnerability provides a quantitative tool for analyzing model robustness."}, "weaknesses": {"value": "1. The computational complexity of the GIB approach is prohibitive for real-world applications, requiring 2N+1 mutual information calculations for N features.\n\n2. Mutual information estimation via binning is known to be inaccurate (Saxe,et.al,2019) yet all major conclusions rely on this flawed estimation method.\n\n3. No comparison with other state-of-the-art generalization techniques (NIB [1] , HSIC [2], DIB [3], Gate-IB [4]) or information-theoretic approaches.\n\n4.  Incorporates the concept of synergy lacks sufficient novelty, previous work already uses more accurate way (matrix-based Rényi's mutual information estimation techniques [5] [6]) to estimate the synergy. Building on these foundations, several established information bottleneck variants  (DIB) [3], Gate-IB [4], and Multi-view Information Bottleneck [7] have already incorporated synergistic principles into their formulations.\n\n[1] Kolchinsky, et al. \"Nonlinear information bottleneck.\" Entropy 21.12 (2019): 1181.\n\n[2] Wang, Zifeng, et al. \"Revisiting hilbert-schmidt information bottleneck for adversarial robustness.\" Advances in Neural Information Processing Systems 34 (2021): 586-597.\n\n[3] Yu, Xi, et al. \"Deep deterministic information bottleneck with matrix-based entropy functional.\" ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021.\n\n[4] Alesiani, Francesco, et al. \"Gated information bottleneck for generalization in sequential environments.\" Knowledge and Information Systems 65.2 (2023): 683-705.\n\n[5] Yu, Shujian, et al. \"Multivariate Extension of Matrix-Based Rényi's $\\alpha $ α-Order Entropy Functional.\" IEEE transactions on pattern analysis and machine intelligence 42.11 (2019): 2960-2966.\n\n[6] Yu, Shujian, et al. \"Measuring dependence with matrix-based entropy functional.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 12. 2021.\n\n[7] Zhang, Qi, et al. \"Multi-view information bottleneck without variational approximation.\" ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022."}, "questions": {"value": "1. The paper demonstrates that GIB exhibits compression phases where standard IB fails, but does this translate to measurable improvements in test accuracy or other standard generalization metrics? Can you provide a quantitative analysis?\n\n2. The PCA preprocessing used for CIFAR-10 experiments potentially discards important synergistic relationships. Have you validated that the reduced representation preserves the synergistic interactions you aim to measure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1uqZrJMRf1", "forum": "reOA4r0FGL", "replyto": "reOA4r0FGL", "signatures": ["ICLR.cc/2026/Conference/Submission17790/Reviewer_ZKFn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17790/Reviewer_ZKFn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531402648, "cdate": 1761531402648, "tmdate": 1762927633287, "mdate": 1762927633287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work extends the traditional Information Bottleneck (IB) principle to account for synergistic information. This extension results in the introduction of the Generalized IB (GIB), which is claimed to be more capable of capturing compression phases in Deep Neural Networks (DNNs), while additionally being less prone to the problem of infinite MI values in deterministic DNNs. The authors conduct extensive experiments revealing compression phases under numerous setups, in which traditional IB fails to detect compression. The study supports these findings with both theoretical justification and informal explanations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The experimental results are extensive, covering settings from toy problems to vision and language models. The resulting plots (at least for the binning estimator) clearly indicate a compression phase, even under conditions where IB compression does not typically occur (e.g., with ReLU activations). Overall, from my point of view, these findings are highly interesting."}, "weaknesses": {"value": "**Methodological problems:**\n\nThe work is currently divided into two distinct parts: a theoretical component, which uses information theory to introduce the GIB and establish its properties (e.g., non-vacuousness and its connection to the original IB); and an experimental component, which relies on binning for mutual information (MI) estimation (or on a loss comparison in Appendix F).\n\nFor these parts to form a coherent whole, the MI estimates **must** be tight, or at least reflect the correct dynamics (i.e., the true MI increases if and only if the estimated value increases). However, based on established literature, I believe this connection is tenuous at best. This fundamental weakness calls the information-theoretic explanation of the observed phenomena into serious question, unless the authors can rigorously demonstrate the validity of their estimators.\n\nHere are detailed explanations of my concerns:\n1. **Binning estimator** is the most crude approach to MI estimation, which is notoriously problematic and misaligned with the actual MI in various setups.\n    - This estimator is essentially the rectangle method applied to the differential entropy estimation (Theorem 8.3.1 in Cover & Thomas, 1991). Even under the infinite-samples assumption, this method is cursed with the dimensionality: given the integrated function is Lipschitz, the local approximation error can grow as $O(\\sqrt{d})$; however, since we need $O(const^d)$ evaluations, the global error grows exponentially with $d$.\n    - The problem worsens if we consider finite-sample regimes (Geiger, 2021):\n       - If binning is too coarse and all samples fall into one bin, one has $\\hat{H}(X) = \\log (\\text{bin size})$, while true $H(X)$ can vary from $-\\infty$ to $\\hat{H}(X)$.\n       - If binning is too fine, one has $\\hat{H}(X) = \\log(\\text{num. of samples}) - const$, while $H(X)$ can range from $-\\infty$ to $\\infty$. Moreover, in high dimensions, this is a very likely scenario for non-compressed data, as the probability to hit the same bin twice decays as $O(const^{-d})$.\n       - Adaptive bin size selection has limited success in dealing with the issues above (Geiger, 2021).\n    - Experimental evaluation confirms extremely poor performance of histogram-based MI estimators [1]\n    - The resulting estimates **and** corresponding dynamics depend severely on the binning strategy. It has been shown changing the bin size or performing non-uniform binning can create or disappear the compression phase ([2] and Goldfeld, 2018; Saxe, 2018; Geiger, 2021).\n2. **Loss Comparison**, while being more accurate, is only tight when the conditional probability is Gaussian (for MSE) or Laplace (for MAE); please, refer to Theorem 8.6.6 in (Cover & Thomas, 1991) for a general idea. This approximation is decent for low-dimensional cases. However, in higher dimensions, especially under the manifold hypothesis, the gap between estimated and real values of MI can be arbitrary high (since actual $H(Y \\mid X)$ might $\\to -\\infty$ for any fixed MSE).\n3. **Difference in the results**. I consider the plots in the main text and in the Appendix F to be quite different. Combined with the reasoning above, this makes me suspect that observed compression may arise from MI estimation artifacts in *both* cases.\n\n[1] Czyz et al. \"Beyond Normal: On the Evaluation of Mutual Information Estimators\". Proc. of NeurIPS 2023.\n\n[2] Noshad et al. \"Scalable Mutual Information Estimation using Dependence Graphs\". Proc. of ICASSP 2019.\n\n**Theoretical Problems:**\n\nSome steps in theoretical derivations are hard to follow.\n1. Since $Q(Z,Y)$ is defined as a *distribution* (line 284), the notation $I(\\mathcal{X};Q(Z,Y))$ is incorrect: MI is defined between random variables, not distributions. I kindly ask to clarify this part. If, however, $Q$ is instead defined as a PMI function applied to $Z$ and $Y$, I do not understand the notation $Q(Z,Y) = Z = Y$: in this case, $Q$ is 1-dimensional, while $Z$ is of arbitrary dimensionality.\n2. While Theorem 3 seems fine to me (with an exception for $Q(Z,Y) = Z = Y$, which I am unable to follow at the moment), the converse result in lines 788--792 is rather informal and, probably, incorrect. There are many cases in Information Theory when $\\infty - \\infty \\neq 0$.\n\n    For instance, consider $I(X;Y;Z) = I(X;Y) - I(X;Y \\mid Z)$ for $X = Y \\sim U[-1;1]$ and $Z = {X > 0} \\sim Bern(1/2)$. Here, both $I(X;Y)$ and $I(X;Y \\mid Z)$ are infinite. But one can also write $I(X;Y;Z) = I(Z;Y) - I(Z;Y \\mid X) = 1 \\\\; \\text{bit} - 0 \\\\; \\text{bit} = 1 \\\\; \\text{bit}$.\n\n    Moreover, by selecting $X = Y \\sim \\mathcal{N}(0, I_2)$ and $Z = X_1$, one gets $I(X;Y;Z) = I(X;Y) - I(X;Y \\mid Z) = \\infty - \\infty$, but also $I(X;Y;Z) = I(Z;Y) - I(Z;Y \\mid X) = \\infty - 0 = \\infty$.\n\n**Minor issues:**\n1. Time travel evidence in line 175: (Rosas et al., 20019).\n\n**Conclusion:**\n\nWhile I am both fascinated and intrigued by the experimental results, my concerns in regards with the methodology are strong and thus keep me from advocating for publishing this manuscript.\n\nCurrently, there appears a huge gap between the theory and practice. Additionally, there are some major inconsistencies in the experimental results and a certain level of informality in the theoretical derivations.\n\nHowever, **I am more than willing to raise my score** above the acceptance treshold if the following happens during the rebuttal or revision: either the authors provide solid, rigorous and convincing arguments proving that the histogram-based estimator truly captures dynamics of MI, or the authors derive a separate (non-information-theory-based) theoretical explanation of the compression phenomenon from the perspective of the binning estimator."}, "questions": {"value": "1. Please, clarify, what is $Q$, and how it is possible that $Q(Z,Y) = Z = Y$.\n2. I kindly ask you to reinterpret the infinities in GIB given my remarks from the weaknesses section. Does one of the key claims (finiteness of GIB under reasonable settings) still hold?\n3. Since there is no supplementary code attached, I also kindly ask to provide strict definitions/algorithms for both MI estimators. Please, also provide corresponding experimental protocols."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lLczTL6YSq", "forum": "reOA4r0FGL", "replyto": "reOA4r0FGL", "signatures": ["ICLR.cc/2026/Conference/Submission17790/Reviewer_TtzU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17790/Reviewer_TtzU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956622708, "cdate": 1761956622708, "tmdate": 1762927632900, "mdate": 1762927632900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors present the Generalized Information Bottleneck (GIB), a successor to the traditional Information Bottleneck (IB). Unlike IB, GIB is designed to overcome key limitations, such as producing vacuous infinite values in deterministic neural networks and lacking pronounced compression phases with non-tanh activations. Through extensive experiments, the authors demonstrate that GIB consistently reveals strong compression in DNNs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "GIB presents an original and appealing solution to IB's limitations. The authors strongly motivate its use through both theoretical analysis and comprehensive experiments. By connecting GIB to the established concept of synergy, they also provide an intuitive interpretation for the novel compression term they introduce."}, "weaknesses": {"value": "1. In this work, the binning technique for mutual information (MI) estimation is utilized for the majority of the experiments. While the results appear consistent with the authors' intuition, the validity of these estimates is questionable. The histogram estimator is known to be among the least accurate non-parametric methods, even in low-dimensional settings [R1]. Furthermore, Goldfeld et al. (2019) have demonstrated that binning-based estimates can exhibit dynamics contrary to the true MI.\n2. In Appendix F, the authors employ an alternative MI estimation method. From my perspective, the results differ significantly from the corresponding plots in the main text, which raises concerns about the robustness of the findings.\n3. The definition and motivation for introducing $Q(Z,Y)$ are unclear to me. Its role in the formulation is not well-justified.\n4. While I follow the proof of Theorem 3, the converse result (lines 787-793) is problematic. The derivation relies on an informal manipulation of the expression $\\infty - \\infty$, which is not mathematically rigorous to say the least.\n\n[R1] Czyz et al., Beyond Normal: On the Evaluation of Mutual Information Estimators, NeurIPS 2023"}, "questions": {"value": "1. Can you, please, elaborate on the introduction of $Q$?\n2. How do you explain the significant discrepancies between the MI dynamics obtained with the binning estimator and the loss-based estimator (shown in Appendix F)? Extending this observation, is it possible that using a more accurate MI estimator would reveal an absence of compression phases?\n3. Can you provide a more formal derivation for the converse of Theorem 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SCEFRsoN8w", "forum": "reOA4r0FGL", "replyto": "reOA4r0FGL", "signatures": ["ICLR.cc/2026/Conference/Submission17790/Reviewer_py7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17790/Reviewer_py7F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987775476, "cdate": 1761987775476, "tmdate": 1762927632332, "mdate": 1762927632332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Generalized Information Bottleneck (GIB) grounded in synergy—information that is only available through joint processing of features—to address well-known limitations of the classic Information Bottleneck (IB) in deep learning. Concretely, the authors (i) argue that synergistic functions generalize better than non-synergistic ones, with supporting theory and synthetic/empirical evidence; (ii) define a computable feature-wise synergy measure via average interaction information (II) and introduce a PMI-reweighted distribution \\(Q(Z,Y)\\) to emphasize correct predictions; and (iii) derive a Lagrangian objective (GIB) that promotes collective feature interactions while penalizing reliance on individual features. They further show that, under perfect estimation, the original IB objective is upper bounded by GIB, thereby ensuring compatibility with IB. Empirically, GIB exhibits robust compression phases across architectures and activations (including ReLU where standard IB often fails), yields interpretable training dynamics for CNNs/Transformers, and aligns with adversarial robustness trends."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality.**\n1. Recasts IB through the lens of feature synergy, operationalized via an II-based decomposition and a PMI reweighting that focuses the prediction term on correct outputs. This framing is distinct from prior multivariate IB/PID lines by directly optimizing for synergy rather than redundancy or total MI alone. \n\n2.  Provides a principled rationale (theory $\\to$ synthetic $\\to$ CIFAR-10) that more synergistic functions have smaller MI with noise and inputs, yielding tighter generalization—connecting synergy to smoothness/Lipschitz arguments in known generalization bounds. \n\n**Significance.**\nResolves practical roadblocks of IB (e.g., infinite/constant complexity in deterministic ReLU nets) and offers a unifying diagnostic that matches observed generalization/robustness behavior—potentially impactful for theory, interpretability, and robustness."}, "weaknesses": {"value": "1. **Estimator dependence / computational load.** Although the paper justifies histogram binning for tractability and shows an alternative loss-based MI estimate (Appendix F), the synergy computation (feature-wise II with PMI reweighting) may be costly or sensitive to preprocessing (e.g., Kernel PCA to 50 dims on inputs for ResNets). More discussion on estimator bias/variance and runtime scaling would strengthen claims of practicality. \n\n\n2. **Bounding assumptions.** The clean upper-bound result (IB $\\le$ GIB) assumes perfect training accuracy/deterministic predictors; while standard in IB analyses, it would be valuable to extend the comparison to the realistic imperfect-estimation regime and quantify deviation."}, "questions": {"value": "1. How sensitive are GIB’s information-plane conclusions to the MI estimator (binning vs. kNN/KDE/variational) and to dimensionality reduction choices (e.g., number/type of components in the ResNet experiments)? Can the authors report variability/error bars over estimators?\n\n2. What is the time/memory cost of computing the feature-wise synergy decomposition per epoch on CIFAR-10/BERT, and how does it scale with feature dimension? Any approximations (e.g., random feature subsets) that preserve the qualitative planes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8jgWtNTmSq", "forum": "reOA4r0FGL", "replyto": "reOA4r0FGL", "signatures": ["ICLR.cc/2026/Conference/Submission17790/Reviewer_nzFV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17790/Reviewer_nzFV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138181003, "cdate": 1762138181003, "tmdate": 1762927631749, "mdate": 1762927631749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}