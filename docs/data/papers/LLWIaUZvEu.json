{"id": "LLWIaUZvEu", "number": 24842, "cdate": 1758360971665, "mdate": 1759896745904, "content": {"title": "VGPO: Fine-Tuning Speech Autoregressive Diffusion Models with Value Guided Policy Optimization", "abstract": "Autoregressive diffusion models (ARDMs), which generate continuous latent sequences, have recently achieved state-of-the-art zero-shot text-to-speech (TTS) performance. However, fine-tuning these models with reinforcement learning (RL) to directly optimize user-defined reward functions remains an open challenge. In this work, we propose Value-Guided Policy Optimization (VGPO), an actor-critic RL algorithm tailored to ARDMs. We train a causal value model to predict expected future rewards and update the ARDM using gradients from this value model. To validate VGPO, we fine-tune the recently introduced DiTAR model and evaluate it on two tasks: improving F0 variance to enhance expressiveness; and optimizing text log-probability to improve the model's robustness to challenging long text. VGPO can achieve significant improvement in zero-shot TTS expressiveness and robustness, while maintaining naturalness and speaker similarity.", "tldr": "", "keywords": ["text-to-speech", "speech synthesis", "diffusion model", "continuous-valued language model", "reward optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a7a4176ff60351f9891f12be1851b2220d0c3ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Value-Guided Policy Optimization (VGPO), an online, actor-critic reinforcement learning algorithm designed to fine-tune speech autoregressive diffusion models (ARDMs). The authors provide a theoretical derivation, based on KL-regularized reward maximization, showing that the optimal policy's score is the sum of the reference model's score and the gradient of a \"soft value function.\" Based on this, VGPO trains a value model (critic) to predict future rewards and updates the ARDM (actor) policy to match this theoretically optimal, value-guided score. The method is evaluated on two TTS tasks: improving F0 variance (expressiveness) and enhancing robustness to difficult, repetitive text. To combat error accumulation, the authors also introduce an Adversarial Distribution Matching (ADM) regularizer."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a novel and significant problem: how to apply reinforcement fine-tuning (RFT) to the SOTA class of autoregressive diffusion models for speech. This is a non-trivial challenge, and a robust solution would be a valuable contribution.\n\nThe derivation in Section 3.1, which connects the KL-RFT objective to a concrete update rule for the diffusion score (Corollary 1), is clean and provides a theoretical basis for the algorithm.\n\nThe method shows clear empirical gains on two distinct and relevant tasks. VGPO outperforms baselines like Best-of-K and an ARDM-DPO variant in both objective and subjective metrics. The identification of error accumulation as a key problem is also a good insight."}, "weaknesses": {"value": "Significant Gap Between Theory and Practice: The entire theoretical derivation (Sec 3.1) is based on optimizing a soft value function (Def 2), which requires the unstable Exp-MSE loss (Eq 12). However, the paper states in Sections 4.2 and 4.3 that the experiments actually use the standard MSE loss (Eq 13), which estimates the \"hard\" value (expected reward). This is a major disconnect. The theoretical guarantees of Corollary 1, which are the paper's main theoretical anchor, do not apply to the algorithm that was actually implemented and tested. This gap is not adequately justified or analyzed.\n\nOmission of Computational Cost: VGPO is an online, on-policy algorithm, which requires new rollouts from the policy at every training iteration. This is in sharp contrast to the offline DPO baseline. Online RFT is known to be extremely computationally expensive and sample-inefficient. The paper provides no discussion whatsoever on the computational cost (e.g., training time, GPU hours, sample-efficiency) of VGPO versus its baselines.\n\nInsufficient Baseline Comparison: The comparison to ARDM-DPO (Liu et al., 2025b) is a key result, but the implementation details are too sparse. DPO requires preference pairs (chosen, rejected). The paper does not explain how these pairs were generated for the DPO baseline (e.g., were they sampled and ranked by the reward function?). Without this, the comparison is not reproducible and it's difficult to assess whether the DPO baseline was implemented in its strongest form."}, "questions": {"value": "For both Task A (F0V) and Task B (NLL), how exactly were the preference pairs for the DPO baseline generated?\nFor example, did you sample two candidates from the base model and then use the reward function (F0V or NLL) to label them as 'chosen' and 'rejected'?\nWhat β (beta) values were swept for the DPO baseline, and how was the final β (e.g., β=200 in Task A) selected? This is crucial for ensuring a fair comparison.\n\nThe Adversarial Distribution Matching (ADM) regularizer was shown to be critical in Task B for mitigating speaker similarity degradation (Table 3, models B8/B9). This is a very interesting and valuable finding. However, its application seems specific to Task B.\nWas ADM also applied to Task A? If so, what was its effect?\nIf ADM was not used for Task A, why was it unnecessary? Did Task A not suffer from the same \"error accumulation\" and distribution drift?\nDoes this suggest that the standard KL regularization in VGPO (Eq. 15) is generally insufficient for complex tasks or long training runs? When should a practitioner using VGPO choose to enable ADM?\n\nIn Section 4.3, you state that \"Group Reward Normalization\" was \"necessary to train the value model\" for Task B. This seems like a critical, non-obvious implementation detail.\nCould you elaborate on what failed when this normalization was not used? Did the value loss not converge, or did it converge to a useless \"average\" value?\nWas this normalization also applied in Task A, or was it specific to the CTC likelihood reward in Task B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jc0C4g7f2I", "forum": "LLWIaUZvEu", "replyto": "LLWIaUZvEu", "signatures": ["ICLR.cc/2026/Conference/Submission24842/Reviewer_jkmb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24842/Reviewer_jkmb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623073383, "cdate": 1761623073383, "tmdate": 1762943216540, "mdate": 1762943216540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VGPO (Value-Guided Policy Optimization), an actor–critic reinforcement fine-tuning (RFT) algorithm tailored to Autoregressive Diffusion Models (ARDMs) for speech/TTS. A causal value model predicts expected future rewards given partial trajectories; the actor (ARDM) is updated using the value gradient, which the authors argue yields an optimal update that equals the reference model’s prediction plus the value gradient (under a MaxEnt-RL/energy-guidance view). The method is evaluated by fine-tuning DiTAR on two targets: (A) increasing F0 variance to enhance expressiveness; (B) improving text log-probability for robustness on long/hard text. An additional Adversarial Distribution Matching (ADM) regularizer replaces conventional KL to mitigate drift. Experiments report improved expressiveness/robustness while preserving naturalness and speaker similarity"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper takes a relatively unexplored direction by applying reinforcement learning—specifically an actor–critic framework—to autoregressive diffusion models. This adaptation is nontrivial and provides a promising pathway toward controllable fine-tuning in continuous latent ar models.\n\n2.  The paper is well-written and easy to follow. The theoretical formulation is consistent and mathematically coherent."}, "weaknesses": {"value": "The current experiments and motivations are too narrowly scoped to the text-to-speech (TTS) setting, focusing mainly on F0 variance (expressiveness) and robustness to long text. While these are valid and practically meaningful objectives, they are very specific to TTS and do not convincingly demonstrate that the proposed VGPO framework offers a general contribution to the broader class of autoregressive diffusion models.\n\nFor an ICLR-level paper, one would expect more general insights or cross-domain validation. For instance, showing that the same value-guided optimization can improve the ar diffusion model on image generation [1] \n\nAt present, the work feels too task-specific, and its novelty risks being interpreted as a domain-engineering improvement more appropriate for a venue such as Interspeech, rather than ICLR. Expanding the experimental scope to include at least one non-speech autoregressive diffusion task would greatly improve the paper’s impact and positioning.\n\n[1] Li T, Tian Y, Li H, et al. Autoregressive image generation without vector quantization[J]. Advances in Neural Information Processing Systems, 2024, 37: 56424-56445."}, "questions": {"value": "It is well known that directly using GRPO to optimize a specific metric can improve that metric’s performance — but likewise, the generated quality in other dimensions may degrade. Such optimization may lead to reward hacking. Does VGPO fall into a similar risk?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ADx5mcfjeD", "forum": "LLWIaUZvEu", "replyto": "LLWIaUZvEu", "signatures": ["ICLR.cc/2026/Conference/Submission24842/Reviewer_vauG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24842/Reviewer_vauG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978970751, "cdate": 1761978970751, "tmdate": 1762943216319, "mdate": 1762943216319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Diffusion models use a continuous latent space, and using RL to directly optimize user-defined rewards remains an open challenge. Instead of saying PPO/DPO are unstable, the paper shows that KL alone cannot fully correct error accumulation during reinforcement fine-tuning. \nThis paper proposes VGPO (Value-Guided Policy Optimization), a reinforcement learning method designed to overcome these issues in diffusion-based TTS models. VGPO adds a value function inside the diffusion model and uses its value-gradient to guide each generation step. The paper also introduces Adversarial Distribution Matching (ADM) as a new regularization method, making the training process more stable than using only KL divergence.\nExperimental results show that VGPO improves both expressiveness and robustness compared to DPO. Overall, the paper shows that VGPO helps reduce the instability of reinforcement learning in diffusion-based models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper shows that reinforcement learning can be applied to diffusion models and presents a policy gradient method suitable for their continuous and non-differentiable nature. It demonstrates that gradients can be passed through the value function, making fine-grained optimization possible. The value function is also well formulated for continuous diffusion processes. They further show that the reward function using F0 variance and ASR likelihood converges stably during training."}, "weaknesses": {"value": "[1] The paper does not describe key settings for the discriminator, such as learning rate, update frequency, or stabilization methods.\nBecause of this, the stability of ADM training and reproducibility remain uncertain.\n[2] To answer the question “Does the improvement come only from reward tuning?”,\nthe paper should compare results without value guidance or without ADM.\nThis would show how much each component contributes to the overall performance.\n[3] Even though the rewards and quantitative metrics improved, it remains unclear how the generated speech is perceptually better, since only a small-scale CMOS test was conducted and no qualitative analysis or listening examples were discussed."}, "questions": {"value": "[1] Can you provide more details about the ADM discriminator training, such as learning rate, update frequency, or any stabilization methods used? This information would make the training process clearer and easier to reproduce.\n[2] Did you run experiments without value guidance or without ADM? Showing these results would help understand how much each part contributes to the final performance.\n[3] How is the generated speech perceptually better? Since only a small-scale CMOS test was shown, some listening examples or qualitative analysis would make the perceptual claim more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "17BPNxUk47", "forum": "LLWIaUZvEu", "replyto": "LLWIaUZvEu", "signatures": ["ICLR.cc/2026/Conference/Submission24842/Reviewer_BGr3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24842/Reviewer_BGr3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762091742133, "cdate": 1762091742133, "tmdate": 1762943216046, "mdate": 1762943216046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a value-guided policy optimization method for autoregressive diffusion models and evaluates it on two different objectives for a speech synthesis model, namely F0 variance for improved naturalness and text probability optimization for addressing issues regressive models have with long text. A soft value function, which predicts the future reward given partial trajectory information, is learned from a reference model and a target reward function (which is an approach related to classifier guidance, except that the reward here is a soft value instead of a hard classification value)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper proposes a novel method for RL training for autoregressive diffusion models, which allows one to incorporate specific reward functions to improve certain characteristics of the generated output. The proposed method is justified theoretically and supported by proofs (although I did not review the proofs myself)."}, "weaknesses": {"value": "The objective metrics measured in the experiments are unrelated to one of the experiments (F0 variability/naturalness), although that is partially covered by the subjective metrics. The authors present F0V as being \"the higher the better\", but that is not objectively true: speech with very high F0V is perceived as very unnatural. The subjective metrics do not seem to present a convincing case for VRPO vs. DPO, given that there is basically no significant change in all scores for both experiments.\n\nI would have liked to see an ablation study showing the effect of each decision in designing the proposed method, instead of just baselines with DPO and best-of-K sampling."}, "questions": {"value": "Why was the model only evaluated on TTS? It would have been interesting to see the effect of the proposed optimization strategy in other domains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZwXO9uxbfI", "forum": "LLWIaUZvEu", "replyto": "LLWIaUZvEu", "signatures": ["ICLR.cc/2026/Conference/Submission24842/Reviewer_1DeF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24842/Reviewer_1DeF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762194290795, "cdate": 1762194290795, "tmdate": 1762943215033, "mdate": 1762943215033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}