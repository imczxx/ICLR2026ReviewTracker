{"id": "dHN0Pkxz0V", "number": 13329, "cdate": 1758216620067, "mdate": 1759897444588, "content": {"title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning", "abstract": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI.\n$\\infty$-THOR provides:\n(1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories;\n(2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents’ long-context reasoning ability; and\n(3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction.\nExperimental results and analyses highlight the challenges posed by our benchmark and provide insights into training strategies and model behaviors under long-horizon conditions.\nOur work provides a foundation for the next generation of embodied AI systems capable of robust, long-term reasoning and planning.", "tldr": "", "keywords": ["Embodied AI", "Long-horizon task", "Long-context reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7818b67dbcb94ee6ae427bb71a35fea9815c34e0.pdf", "supplementary_material": "/attachment/a24ef6cb4894d6eac0311ffe2fab94d65da736aa.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents a new framework for generating, training, and evaluating long-horizon embodied reasoning tasks. It introduces the Needle(s) in the Embodied Haystack (NiEH) benchmark, which tests agents’ ability to recall and reason over multiple scattered clues across hundreds of environment steps. The framework supports scalable trajectory synthesis using AI2-THOR and integrates architectural techniques—such as interleaved Goal-State-Action modeling, context extension, and Context Parallelism, to enhance long-context reasoning in embodied agents. Extensive experiments reveal the severe performance degradation (“memory cliff”) that occurs as context length increases and analyze how different extension methods mitigate this effect."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel benchmark and task design: NiEH introduces an original embodied QA setting requiring multi-clue, multi-step reasoning over long trajectories, a valuable contribution to evaluating memory and reasoning.\n2. Scalable trajectory generation pipeline: The framework builds a reproducible, large-scale dataset based on AI2-THOR, enabling consistent generation of trajectories with hundreds of steps and millions of tokens.\n3. Systematic empirical analysis: The experiments provide comprehensive comparisons of context-extension strategies and clearly expose the “memory cliff” phenomenon in existing models.\n4. Technical depth: The architectural adaptations (Goal-State-Action modeling, Context Parallelism) are conceptually well-motivated and technically detailed."}, "weaknesses": {"value": "1. Dataset construction methodology: The long trajectories appear to be concatenations of short demonstrations; it remains unclear how object states and interaction continuity are maintained. Why not synthesize single coherent long trajectories directly?\n2. Benchmark novelty and scope: The NiEH benchmark closely resembles long-video understanding settings; the distinction between this work and prior long-video benchmarks should be made clearer beyond simply being “embodied.”\n3. Limited model evaluation: Most experiments rely on off-the-shelf VLMs with minimal low-level VLA validation. This weakens claims about the framework’s utility for training or improving embodied agents.\n4. Visualization clarity: Figure 4 heatmap presentation is visually rich but lacks clear quantitative comparisons, statistical significance, and explicit axis labeling. More intuitive ablations would improve interpretability.\n5. Benchmark discrimination: The results highlight engineering aspects of long-context handling but do not clearly show whether the proposed benchmark can distinguish different models’ reasoning abilities in a consistent way.\n6. Missing supplementary materials: Several details are deferred to the appendix, but no appendix appears in the submission, limiting reproducibility and clarity."}, "questions": {"value": "The paper provides a valuable benchmark and framework for studying long-context reasoning in embodied AI, with strong engineering and analytic depth. However, the novelty relative to existing long-video settings, limited evaluation coverage, and missing appendices reduce its impact. Strengthening empirical breadth and clarifying benchmark construction would elevate it to a solid accept. Please refer to the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s6BxDnHJLj", "forum": "dHN0Pkxz0V", "replyto": "dHN0Pkxz0V", "signatures": ["ICLR.cc/2026/Conference/Submission13329/Reviewer_rSXS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13329/Reviewer_rSXS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636880247, "cdate": 1761636880247, "tmdate": 1762923989924, "mdate": 1762923989924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method for generating long multi-goal trajectories in the AI2-THOR environment, along with question-answer pairs whose answers depend on either a single (NiaH) or multiple (NsiaH) past events. This setup requires long-term memory capabilities and enables the evaluation of vision–language question answering over extended sequences.\nThe generated data is used for the evaluation of VLA / VLM models in long context settings.\n\nIn addition, the paper discusses an online interaction VLA design / implementation for such long context settings.\n\nThe paper presents empirical evidence showing that several current VLMs struggle in long-context settings where the context length exceeds that used during training / FT.\nFurthermore, in another set of experiments, the proposed online VLA design was fine-tuned on the proposed dataset under several configurations and the models were evaluated in an \"interactive\" setup."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The long context and multi goal trajectory generation method and question-answer pairs generation method are novel and could contribute to the VLA/VLM community.\n- The empirical results reflect the poor performance of current methods in long-context settings."}, "weaknesses": {"value": "`W1`: Overall, the paper is not easy to follow, and the presentation lacks clarity in explaining what was actually done. Significant effort is required to understand precisely the main contributions and methodology. The writing would benefit from a more direct, transparent exposition of the key ideas and experimental details.\n\n\n\n\n`W2`: Ultimately, the results in Figure 4 suggest that current architectures (incl. long context solutions) struggle with contexts longer than the training / FT context length. This observation is not particularly surprising.\n\n\n\n`W3`: The experimental design of the interactive (online) experiment setup may be flawed. More information is needed to determine with certainty, see `Q5, Q6` below for questions. This could impact the validity of the corresponding conclusions.\n\nMy concern is that the context is reset after each sub-goal based on the data (states and actions) of the trajectory from the dataset (generated by the same planner used in the training set), rather than the states and actions produced through the online interaction. In such a case, it is possible that performance are maintained due to overfitting the training set (which was used for the fine-tuning). It is also unclear whether states and actions from previous tasks are relevant for the success on the current task.\n\nFurthermore, such a setting is not a true online evaluation, but rather evaluates each task independently until first failure.\n\n\n\n`W4`: line 269: the acronym \"PDDL\" was never introduced. Please also provide a reference.\n\n`W5`: \n> \"$\\infty$-THOR enables the creation of unlimited trajectories ***with arbitrarily long***, and provides\" (line 163)\n\nThis sentence is truncated?\n\n\n`W6`: The paper claims\n> We show that interleaved Goal-State-Action modeling ...  is the most practical approach for this class of problems (lines 77-78)\n\nThis claim is not supported by the evidence presented in the paper. To show that an approach is \"the most practical\", one must defined precisely what \"practical\" means and compare to all relevant existing baselines.\n\n\n`W7`: The term \"embodied AI\" is much broader than language-vision based models, and includes non-language models as well (e.g., deep RL). The paper proposes a framework aimed specifically at language-vision driven methods. The scope of the discussion should be clearer and more accurate.\n\nThe claim \n> \"We present empirical results and analyses, providing insights to the current capabilities and\nlimitations ***of embodied AI systems*** on long-horizon tasks.\" \n\nsuggests a wider scope than what the paper includes. The scope of the claim should be adjusted accordingly.\n\n\n\n`W8`:\n> ... lack the dynamic interactivity and memory needed for long-horizon embodied tasks involving continuous vision-language-action sequence (line 303)\n\nThis statement lacks supporting evidence (either provide empirical evidence or refer to prior works that include such supporting evidence).\n\n\n\n`W9`: \n> Moreover, many state-of-the-art models are only accessible via proprietary APIs, making them impractical for real-time, controllable embodied settings and managing long-term memory states. (line 305)\n\nThe fact that a model is proprietary does not mean that it lacks capabilities. It is unclear what argument this statement aims to support.\n\n\n`W10`: The plots in Figure 5 are too small. In addition, information about the shaded area is missing. \n\n\n`W11`: Figure 4 is missing axes labels and a color bar. In addition, tick labels are too small.\n\n\n\n\n`W12`: The setup in Section 5.1 is not clear enough. Are the models fixed? Were they fine-tuned? What data exactly was used for the evaluation? test set only? all dataset splits?\n\n\n`W13`: \n> Our experiments demonstrate that exposure to longer contexts during training significantly improves model performance (line 482)\n\nThis observation is inline with existing literature and is not surprising or new.\n\n\n`W14`: The literature review pertaining to the method discussed in Section 4 is insufficient. Specifically, regarding the \"goal-state-action\" design, the idea of concatenating tokens of various modalities along the temporal axis was studied in many prior works, see [1][2][3] for example.\n\nThis \"goal-state-action\" approach can not be considered as novel. \n\nUltimately, in this context, the paper explores fine-tuning such methods to longer context lengths (e.g. 200K+), and shows that long-context performance improve with long-context specific training, which is not surprising.\n\n\n\n[1] Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., ... & De Freitas, N. (2022). A generalist agent. arXiv preprint arXiv:2205.06175.\n\n[2] Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., ... & Finn, C. (2024). Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246.\n\n[3] Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Wahid, A., ... & Florence, P. (2023). Palm-e: An embodied multimodal language model.\n\n\n\n`W15`: Based on the information in the paper alone, it is unclear how the online interactive evaluation is performed. The corresponding information from the appendix should be included in the main text, or at least a short description of it."}, "questions": {"value": "`Q1`: The term “reasoning” has gained widespread use in recent years, yet there remains no formal definition or consensus on its meaning. What exactly do *you* mean by \"reasoning\"? Please be precise. This term was used extensively throughout the paper, and its meaning seems to differ based on the context.\n\n\n`Q2`: \n> ... a new framework for generation, training, and evaluation of long-horizon embodied tasks (line 39)\n\n\"training / evaluation of [...] tasks\", what does it mean to train a task? The paper does not describe training of tasks.\n\n \n`Q3`: Why is the supplementary material in a separate file?\n\n\n`Q4`: \n> We release a large-scale trajectory dataset and an interactive evaluation environment ... (line 96)\n\nWhere is this interactive evaluation environment described?\n\n\n`Q5`: In the online interactive evaluation, it is stated that \n> After each plan, the context is reset to include the GT actions and states from the completed portion of the trajectory, ... (line 871)\n\nwhat does \"GT actions and states\" mean in this context? pre-generated actions and states generated with the planner or the states and actions produced during the online interaction with the environment? are there actions and states that are not GT?\n\nIf the meaning of GT here is pre-generated planner actions, why do you use these? do you also reset the environment state accordingly? is there a discrepancy between the environment state and the context?\n\nAlso, in such a case, the model is effectively evaluated on each sub-goal independently, while terminating upon the first failure, as the context at each sub-task is being reset to a trajectory prefix generated by the (same) planner, used in the training set.\n\n\n`Q6`: Is there a dependency between sub-goal/tasks? i.e., is it necessary to use information from previous tasks (through the states and actions in the context) to successfully perform the current task (goal)?\n\n\n`Q7`: Regarding the trajectories generation process: How do you set up the initial environment state? how do you determine the configuration of the elements in the scene (placements, object types, number of objects, etc)? Given an initial setup, how many possible combinations for sub-goals are there? \n\nI am concerned about the overall diversity of the dataset and how it affects the results and conclusions. Based on the information in the paper alone, it is impossible to get a sense of the true diversity of the dataset (I assume that this is in large implied by the AI2-THOR environment). Can you provide further information in this regard?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tjSci1DvX8", "forum": "dHN0Pkxz0V", "replyto": "dHN0Pkxz0V", "signatures": ["ICLR.cc/2026/Conference/Submission13329/Reviewer_fHk7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13329/Reviewer_fHk7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638120528, "cdate": 1761638120528, "tmdate": 1762923989440, "mdate": 1762923989440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new framework aimed at advancing research on long-horizon embodied tasks. The framework provides: (1) infrastructure for generating and evaluating arbitrarily long, reproducible agent trajectories; (2) a benchmark evaluating agents ability to reason over temporally distant multimodal cues; and (3) a dataset with tasks of hundreds of steps and ground-truth actions. The authors explore architectural approaches and training methods to allow agents to handle long contexts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation for this work is very solid and timely, as current models for embodied planning and decision making are struggling with long context, often confined to short terms tasks without the ability to perform long horizon optimization. Although the task of recalling details in long horizon action sequences is not directly aiming at the core of the planning problem, it also points at a capability in the right direction.\n\n2. The interleaved Goal–State–Action modeling idea is interesting."}, "weaknesses": {"value": "1. The model claims to explore \"ARCHITECTURES FOR LONG-HORIZON VISION-LANGUAGE-ACTION MODELS\". However, there is no explicit evaluation of the core task for VLAs: planning. Instead, authors only evaluate the model on the Needles in the Embodied Haystack task of long horizon question answering. \n\n2. The performance gains on this single task does not fully justify the complex architecture changes made. Perhaps one way to more concretely justify the modeling is by experimenting on other tasks more relevant to Embodied Agents, like actual task planning, or established embodiment benchmarks like VSI-Bench, EAI, etc.\n\n3. Assuming that the architecture has its merits, I think this paper has the potential to be a good work, it just needs more time to test different hypothesis to get some more solid results."}, "questions": {"value": "1. Are there any quantitative metrics for the Needles in the Embodied Haystack task, and why are they not reported in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "niwbg6HEdg", "forum": "dHN0Pkxz0V", "replyto": "dHN0Pkxz0V", "signatures": ["ICLR.cc/2026/Conference/Submission13329/Reviewer_fpYK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13329/Reviewer_fpYK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964750671, "cdate": 1761964750671, "tmdate": 1762923988958, "mdate": 1762923988958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles long-horizon embodied reasoning agents remembering and acting on events hundreds of steps apart.\nIntroducesTHOR, built on AI2-THOR, for generating ultra-long interactive tasks.\nAdds Needle(s) in the Embodied Haystack (NiEH) QA tasks where clues are hidden across 600–900+ steps.\nProposes interleaved Goal–State–Action modeling combining vision, language, and action into one token sequence.\nExtends transformer context with RoPE scaling, YaRN, and LongRoPE for 100k+ tokens.\nUses Context Parallelism via Ring Attention to train on massive sequences efficiently.\nImplements a 7B LLaVA-based agent fine-tuned for long-term reasoning.\nFindings: long-memory modeling becomes feasible, but coherence over hundreds of steps remains weak.\nOverall: a strong framework and benchmark pushing embodied AI toward true long-term reasoning and planning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strong problem focus - long-horizon memory in embodied AI, timely and hard.\nTHOR is scalable and open  can synthesize endless, ultra-long trajectories with full action traces.\nNiEH benchmark is unique tests recall of scattered clues across hundreds of steps, bridging vision, language, and long-term reasoning.\nTask design is clever enforces early–late dependencies, no shortcuts.\nInterleaved Goal–State–Action model clean, unified architecture, handles temporal context elegantly.\nRigorous experiments real ablations on RoPE scaling, YaRN, context length; solid quantitative insight.\nContext Parallelism practical and technically hard; shows long-context training is possible, not just theoretical.\nFindings are balanced improvement with long context, still breaks past 512k tokens; honest about limits.\nGreat clarity and visuals figures explain results at a glance.\nOpen release and reproducibility detailed setup, environment, data planned for release.\nBroader impact connects symbolic planning to physical control; builds a base for next-gen long-term embodied agents."}, "weaknesses": {"value": "Relies only on context extension for memory. No exploration of retrieval or hierarchical memory; limits scalability beyond 512k tokens.\n\nNo direct baseline against modular vision–language–action models. Claim of superiority for interleaved modeling not empirically proven.\n\nLacks external dynamics: all environment changes are agent-driven. No tests of memory for unobserved or changing scenes. Models fail beyond 0.5M tokens, multi-evidence QA accuracy drops sharply, and long runs often collapse. Needs clearer absolute metrics.\n\nStatistical rigor missing. No variance, confidence intervals, or multiple seeds reported; unclear if differences are significant. 7B model fine-tuned on 130k tokens with 8×H100s; not practical for most labs. Inference cost not discussed. Planning and manipulation models evaluated separately; no unified control pipeline yet."}, "questions": {"value": "How is the goal given during interaction? Is the final instruction known from the start and repeated each step, or revealed later? Does the agent get any subgoal hints along the way? Clarifying this would help interpret its planning and autonomy.\n\nDid you try other memory idea like retrieval, recurrent state, or hierarchical summaries? Since all RoPE methods fail past 512k tokens, do you think future work should shift toward explicit or learned memory rather than just longer contexts?\n\nAny comparison to a non-interleaved setup, e.g. separate vision + policy modules like ALFRED or PaLM-E? Even a small-scale test would show if the interleaved model truly helps.\n\nWhat actually breaks beyond 0.5M tokens—GPU memory, instability, or degraded attention? Any thoughts on mixing retrieval or staged training to go further?\n\nMulti-Evidence Reasoning: which question types fail most—temporal, counting, or spatial? Do models usually pick one wrong clue or combine clues incorrectly? A short error breakdown would be great.\n\nIf using a larger model (13B–70B), would memory or training cost be the main blocker? Do you expect bigger models to meaningfully extend reasoning, or still hit the same context wall?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nrLNfIiEqR", "forum": "dHN0Pkxz0V", "replyto": "dHN0Pkxz0V", "signatures": ["ICLR.cc/2026/Conference/Submission13329/Reviewer_xoC5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13329/Reviewer_xoC5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985264186, "cdate": 1761985264186, "tmdate": 1762923988476, "mdate": 1762923988476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}