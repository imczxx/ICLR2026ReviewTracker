{"id": "ADeeoMY4Dn", "number": 21717, "cdate": 1758320883218, "mdate": 1759896907138, "content": {"title": "Why Transformers Succeed and Fail at Compositional Generalization: Composition Equivalence and Module Coverage", "abstract": "Compositional generalization—the ability to train on some combinations of modules and then generalize to unseen module combinations—is an important form of out-of-distribution generalization. A large body of work has evaluated this form of reasoning in transformer-based models, but the underlying mechanisms of success and failure remain poorly understood. We systematically evaluate compositional generalization in transformer-based models, and we identify two factors that play important roles in determining performance: ***composition equivalence*** and ***module coverage***. We show that the apparent performance of direct models (trained only on final outputs) can be entirely due to exploiting composition equivalences—different sequences of modules that reduce to identical end-to-end functions. When benchmarks eliminate these equivalences, the performance of these models drops to *near zero*, showing their inability to generalize to compositions of known modules that produce novel end-to-end functions.  We discuss two key failure modes of step-by-step learning (trained on intermediate outputs). We show that composition equivalences encourage shortcut learning in step-by-step models, and these models fail to generalize when specific modules always appear at certain positions or in fixed combinations in the training set. These findings provide new insights into the conditions under which atomic modules that constitute a compositional task can be correctly learned by a model class for a specific train-test distribution.", "tldr": "We study how composition equivalence and module coverage explain compositional generalization successes/failures in transformers, showing direct models achieve superficially high benchmark performance with equivalences", "keywords": ["compositional generalization", "out-of-distribution generalization", "systematic generalization", "compositionality", "identifiability", "transformers"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6fab0c2d31daae1f6da4edeb662a2b53ffcc9764.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors evaluate Transformers trained from scratch on a series of sequence learning tasks that require compositional generalization. They propose two theoretical criteria (composition equivalence and module coverage) that they suggest explain generalization in different variants (step-by-step vs. direct output and uniform vs. diverse sampling). In particular, this provides a systematic explanation for generalization patterns that at first seem idiosyncratic and provides important context for prior findings of successful compositional generalization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "I thought this was an interesting paper. It is well-written and easy to follow. The theoretical setup is clear, well-motivated, and interesting. Similarly, I think the theoretical definition of composition equivalence is really valuable and I appreciated the effort to identify fundamental theoretical principles that predict compositional generalization. Overall, I think that this is a thought-provoking paper that dives deeply into disentangling different factors contributing to compositional generalization (or the lack thereof) and will be valuable to the field. Below are a few things I liked in particular:\n- I thought the introduction compactly provided a great motivation for the investigation.\n- Figure 1 was very helpful.\n- I think your identification of the potential for shortcut learning is very helpful to keep in mind; it provides evidence in a simple setting for something that machine learning researchers have articulated as a worry in a naturalistic setting as well: just because models output their reasoning step-by-step doesn't actually mean that they *use* that reasoning.\n- Figure 5 is very insightful and provides important validation for your theory (and indeed it would be very valuable to go a little further in this evaluation, see below)."}, "weaknesses": {"value": "As noted, I think this is a very interesting paper. However, I think it would benefit from a closer investigation/validation of the theoretical principles it proposes:\n- In particular, I think some of the explanations on how the proposed principles explain the observed generalization patterns could have been more specific (see section 1 under Questions).\n- I think your proposed principles should allow you to predict the specific subsets of the test data for which models should generalize or not generalize (in my mind, this would be particularly interesting on the diverse dataset without the identity mapping). I think this would provide stronger evidence for your proposed mechanism and would also nicely extend your investigation in Figure 5. In particular, this would provide insight into whether the same relationship between the number of equivalences and accuracy holds for the diverse dataset, which would be an important step towards scaling these principles towards realistic data. (See section 2 under Questions.)\n- Module coverage currently lacks a formal theoretical definition. While I think your investigation in Section 4 is very interesting, I think having such a formal definition would be very helpful in providing a basis for future extensions of your investigations (much like your formal definition of composition equivalence is very helpful in this direction). (See section 3 under Questions.)\n\nOverall, I want to emphasize that I really appreciate the approach in this manuscript: I see its primary contribution as a systematic investigation of principles of compositional generalization, which I think is very valuable. In light of this primary contribution, I think it would be useful to further solidify this systematic investigation. While I think the manuscript in its current form would be a worthy addition to ICLR, addressing the weaknesses above would substantially improve the paper and cause me to further increase my score."}, "questions": {"value": "**1 Clarifications about composition equivalence**\n- In l. 330-340, I think it would be valuable to make clear that the six bijections you sampled don't allow for any other composition equivalence (since there would be bijections for which this wouldn't be true, e.g. if two of the bijections were each others' inverses). I think that is what is implicit in your statement that there are no other composition equivalences, but I don't think that's a trivial statement. I imagine that it's just super unlikely under random sampling and this is very intuitive for, say, concatenations of two functions. However, it is less intuitive to me how the probability of two different concatenations of functions being equivalent scales with the number of times $k$ that you're concatenating them. I imagine for $k=2$ it's close to zero, for $k\\to\\infty$ it converges to $1$. Is it still close to zero for e.g. $k=6$? My sense is probably yes, since there are 26! possible bijections but only $6^6$ possible concatenations of the six bijections you sampled and 6^6 is vanishingly smaller than 26!. But it would be helpful to expand a bit on this point, e.g. through a formal argument or by brute-force checking the specific bijections you sampled. (Apologies for the long bullet point for a relatively minor point.)\n- Similarly, the statement in l.325-327 is not obvious to me. Does random sampling necessarily create train-test splits consisting of composition equivalences/what is the likelihood of creating it?\n- Why does identity-based composition equivalence exist only among sequence lengths with fixed $k$? Isn't $f_1\\circ f_2\\circ \\text{Id}$ equivalent to $f_1\\circ f_2$? I understand that that does not qualify as composition equivalence according to your definition (which requires the same $k$ for $F$ and $F'$), but why does the two functions would still be identical, so why is that unable to support compositional generalization?\n\n**2 Evidence for the role of composition equivalence**\n- Could you split the test dataset in the diverse setting without identity into test data where the training data contains equivalent functions and where it doesn't? Your theory would predict that for data where the training data contains no equivalent functions, accuracy should be zero, correct? Similarly, would this allow you to create a plot where you plot accuracy as a function of data points in the training set with an equivalent function?\n\n**3 Module coverage**\n- Could you speak to how you would think about a formal definition for module coverage?\n- Even better, do you have any empirically testable predictions for which specific permutations the models should and should not generalize to? (Expanding on your observation in l. 458-461.)\n\n**Other questions/suggestions**\n- I think having an example input-output pair somewhere in the main text would have been helpful during task explanation (though your explanation was still pretty easy to follow). It would also allow you to visually depict \n- I don't understand why your mapping always outputs a sequence of the same length $m$. filter, e.g. should decrease the sequence length, no?\n- You may find it interesting to relate shortcut learning to the literature on faithfulness (e.g. https://arxiv.org/abs/2307.13702), which attempts to measure whether language models actually rely on their step-by-step reasoning. My understanding is that shortcut learning provides an instance where reasoning is not faithful.\n- While I don't think this paper requires evaluation on real-world benchmarks (and indeed benefits from the toy setting), do you have any thoughts on how composition equivalence and module coverage relate to real-world settings and perhaps some observed empirical phenomena?\n- Do you have any thoughts on the minimal setting where researchers could investigate your results? I.e. how small could we make the set of possible tokens, possible tasks, etc. while still capturing the most important aspects of your insights? I think this would be interesting to potentially ground future theoretical investigations.\n\n**Final note**\n\nI realize that this is a pretty long set of questions, so I want to emphasize that I do not expect the authors to necessarily address all of my questions (especially as some of my suggestions would involve new empirical experiments, which may not be feasible during rebuttal). My questions are intended to provide some specifics on how I think the authors could further concretize their theoretical principles and the evidence for the role they play in compositional generalization. Indeed, I think the many questions this submissions raises shows that it is already quite a thought-provoking paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0YEd4DmjNN", "forum": "ADeeoMY4Dn", "replyto": "ADeeoMY4Dn", "signatures": ["ICLR.cc/2026/Conference/Submission21717/Reviewer_tN2B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21717/Reviewer_tN2B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939810889, "cdate": 1761939810889, "tmdate": 1762941902070, "mdate": 1762941902070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why transformer models succeed or fail at task-based compositional generalization by looking into the ability to combine known modules (e.g., sort, filter) into novel sequences. The authors identify two key data-centric factors: composition equivalence and module coverage, showing that compositional generalization in transformers is heavily influenced by the data-generating process, not just model architecture, and calls for more careful benchmark design to avoid evaluating shortcut learning instead of true reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces and formalizes composition equivalence and module coverage as key factors influencing compositional generalization, shifting the focus from model-centric to data-centric explanations.\n\n2. The paper uses controlled synthetic benchmarks (uniform vs. diverse functions) and systematic train-test splits (within-k vs. cross-k) to cleanly isolate the impact of each factor."}, "weaknesses": {"value": "1. The failure results from using step supervision seem confusing and lacks more in-depth discussion. There could be multiple factors: Does the model still choose to ignore the intermediate result even in training when such supervision is given, or does it happen at test time? How often is composition equivalence still maintained even in step level supervision? There is no experiments isolating the factors. \n\n2. Experiments are conducted only with small GPT-2-style models. It remains unclear whether the findings generalize to larger, more capable transformers or real-world tasks.\n\n3. The shortcut learning problem introduced in this paper does not seem to be specific to Transformers. I can imagine it happens for other model architectures as well since it is more of a data problem, so the contribution is not very clear."}, "questions": {"value": "Do you expect these findings to hold at scale? Would larger models learn more robust compositional reasoning, or just more sophisticated shortcuts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tipeWe1YdT", "forum": "ADeeoMY4Dn", "replyto": "ADeeoMY4Dn", "signatures": ["ICLR.cc/2026/Conference/Submission21717/Reviewer_ya87"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21717/Reviewer_ya87"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997220320, "cdate": 1761997220320, "tmdate": 1762941901741, "mdate": 1762941901741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the ability of transformers to generalise compositionally by studying the effects of two properties of a dataset on the model's learning and inductive biases. The two properties are composition equivalence -- a property where multiple sequences of computations can lead to the same outcome, meaning that the correct sequence becomes unidentifiable, and module coverage which describes the fact that all modules (or computations) are needed in different orders and positions in the sequence in the training data. The paper shows that if composition equivalence is present in the data or if module coverage is absent then the network will learn shortcuts where computations by the model do not correspond to meaningful (ground truth) manipulations of the data. The work proposes step-by-step modules which also output intermediate steps towards solving a problem as a potential way to mitigate such shortcut learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## Originality\nThe work studies and interesting setting for LLMs and considers who important properties of a dataset. There is a wealth of work at the moment aimed at just understanding how transformers perform tasks, however the paper does a good job of justifying why the tasks considered in this work are representative and useful.\n\n## Quality\nThe hypothesis of the work is clearly stated and the experimental design does seem to isolate the effects of the stated properties of a dataset that should impact prior learning. Clearly, Direct (Relative) does fail in Figure 2 without the identity tokens for example. Results are interpreted fairly overall.\n\n## Clarity\nThe paper is well written and figures are clear. Captions are also detailed which aids in the clarity of the work. The paper is structured well given the among of content which is covered and so it remains understandable and each section does support the next.\n\n## Significance\nUnderstanding transformers, and in particular their points of failure, is an important line of work given their widespread adoption. The topic of compositional generalisation in transformers in particular is important as this is one of the primary inspirations for the models as language is compositional. Moreover much has been said recently on the emergent reasoning abilities of the models. If shortcut learning is indeed a problem for these models, then this has implicates for a number of related topics like neural scaling laws and these emergent reasoning abilities from scale."}, "weaknesses": {"value": "## Clarity\nThe paper is quite dense and I find that it remains quite high-level on some of the topics. Some conclusions are also drawn or treated as obvious where I think there are not. The first paragraph of page 7 is particularly problematic for me as much of what is said there is not shown directly but rather implied from the model performance. It is also not explicitly said how the training and validation sets are constructed for this conclusion about removing identity modules and so I am not convinced by the comparison either. Similarly the statement \"Cross-$k$ failures occur ...  with fixed $k$\" on lines 334 and 335 should also be supported by more direct evidence.\n\n## Quality\nMy main concern on quality is that the need for additional (and difficult to obtain) supervision is not discussed at all when discussing the step-by-step model. While I appreciate that this work is isolating a phenomenon and is mainly concerned with model behaviour, proposing additional supervision as if it is a good solution (without some consideration for the applicability of the approach) is a bit premature. I will also highlight that my above point on the need to support the conclusions more directly from the results is a quality concern, however I list this under clarity as I think some writing edits could improve upon this.\n\n## Significance\nSince the experimental design is one of the primary strengths of the work, the clarity concerns limit my ability to assess significance fully. This is mainly in relation to Ramesh et al. Lines 126 and 127 seem to imply that Ramesh et al. has already considered task order and coverage in this setup and that composition equivalence is the primary contribution of this work. So why not focus more on that? Similarly on lines 339 and 340, why not focus on the clear distinction from Ramesh et al. and the difference with interleaved modules? Overall I would have appreciated a more detailed and clear demonstration of the findings, even at the expense of breadth. \n\nFinally I will note that module coverage has been considered theoretically and it may be helpful to consider this prior work:\n- Schug, Simon, et al. \"Discovering modular solutions that generalize compositionally.\" The Twelfth International Conference on Learning Representations."}, "questions": {"value": "How does the paper presented here explicitly build of Ramesh et al. \n\nHow would future work aim to practically incorporate the step-by-step modules given the need for additional, detailed supervision.\n\nI would be open to increasing my score and advocating acceptance if these questions are answered and indeed I have missed something fundamental."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NB63dB7byo", "forum": "ADeeoMY4Dn", "replyto": "ADeeoMY4Dn", "signatures": ["ICLR.cc/2026/Conference/Submission21717/Reviewer_8EDx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21717/Reviewer_8EDx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999631418, "cdate": 1761999631418, "tmdate": 1762941901364, "mdate": 1762941901364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes an empirical study of when transformers succeed/fail at task-based compositional generalization, introducing two dataset-side factors: composition equivalence (distinct module sequences that induce the same end-to-end mapping) and module coverage (how uniformly modules appear across positions/contexts). The presence of these factors is controlled by considering two types of datasets: one where sequences are generated by uniform random token sampling (\"uniform\"), and one where the used composition rules are diverse string ops, inducing some composition equivalence (\"diverse\"). The uniform data acts as the control dataset, as:\n(i) it induces bijections; namely, the only way to have composition equivalence in it is by adding \"identity tasks\" (essentially, task tokens that don't do anything); \n(ii) and module coverage is easily controllable by enumerating token permutations and changing the train-test split.\nThe authors compare direct vs. step-by-step training under within-k and cross-k regimes, reporting large performance swings that they attribute to these two factors."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The topic is timely and relevant, as the paper proposes a deeper analysis of LLM behavior grounded on compositional generalization, which can be seen as a form of human alignment.\n- The notions of composition equivalence and module coverage are well formulated and relevant to the study of compositional generalization.\n- The experiments are well designed and substantiate the conclusions drawn by the authors to some extent."}, "weaknesses": {"value": "- The experiments rely entirely on controlled, synthetic benchmarks. While this design enables clear isolation of factors, it is unclear how the observed mechanisms would extend to real-world settings, where additional phenomena may come into play. The degree to which these findings generalize beyond the constructed tasks is unclear.\n- Several important ideas are described only verbally, without accompanying formalism. For instance, the discussion around lines 329-335 on identity-based composition equivalence could benefit from simple propositions that explicitly formalize the claims. For example, proving that identity-induced equivalence is the only possible form in the uniform benchmark; and that the two cases of zero compositional generalization (within-k without identity, and cross-k regardless of identity) follow directly from this property. Such additions would clarify the logical structure of the argument and strengthen its contribution.\n- While some conclusions drawn from the experiments are well substantiated by the empirical results (especially when such behavior is predicted from theoretical analysis, such as the one described in the point above), others seem a bit handwavy/too strong to draw from the experiments. Seeing examples of failure/success in those cases would have been helpful, but still wouldn't be enough, as, for instance, in isolating the effect of module coverage, we need to know that ALL failures come from lack of module coverage. This is not clear at the moment. I might be missing something, but even if that is the case, the authors have to do a better job at discussing these sufficiency/necessity conditions, and formalizing them mathematically as possible.\n\nOther examples of the above weakness:\n- Lines 88–105: The claim that \"exact and approximate equivalence in the diverse benchmark causes non-zero performance in direct models\" feels overstated. Here, direct models supposedly perform better only because specific string-operation functions (e.g., mode, filter, sort) share invariances that let them superficially appear compositional. The argument attributes improved accuracy entirely to these functional properties without empirically isolating their effect. For example, it is unclear what percentage of all samples consists of examples with such invariances.\n- Lines 378-385, and again in lines 462-465: The claim is that compositional equivalence encourages shortcuts in step-by-step models. Again, what percentage of the successes in predicting the final token comes from shortcuts?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JJtwedEKiO", "forum": "ADeeoMY4Dn", "replyto": "ADeeoMY4Dn", "signatures": ["ICLR.cc/2026/Conference/Submission21717/Reviewer_Mq38"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21717/Reviewer_Mq38"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011430402, "cdate": 1762011430402, "tmdate": 1762941901135, "mdate": 1762941901135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}