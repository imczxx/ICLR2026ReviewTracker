{"id": "QZmKyAy1VK", "number": 15063, "cdate": 1758247338630, "mdate": 1759897332197, "content": {"title": "Code2Bench: Scaling Source and Rigor for Dynamic Benchmark Construction", "abstract": "The evaluation of code-generating Large Language Models (LLMs) is fundamentally constrained by two intertwined challenges: a reliance on static, easily contaminated problem sources and the use of superficial, low-rigor testing. This paper introduces a new benchmark construction philosophy, Dual Scaling, designed to systematically address both limitations. Our approach involves continuously\nscaling the source of problems from dynamic, real-world code repositories and systematically scaling the rigor of tests via automated, high-coverage Property-Based Testing (PBT). We instantiate this philosophy in CODE2BENCH, an end-to-end framework that leverages Scope Graph analysis for principled dependency classification and a 100% branch coverage quality gate to ensure test suite integrity.\nUsing this framework, we construct CODE2BENCH-2509, a new benchmark suite with native instances in both Python and Java. Our extensive evaluation of 10 state-of-the-art LLMs on CODE2BENCH-2509, powered by a novel \"diagnostic\nfingerprint\" visualization, yields three key insights: (1) models exhibit a fundamental performance gap, excelling at API application (Weakly Self-Contained tasks) but struggling with algorithmic synthesis (Self-Contained tasks); (2) a model’s\nperformance is profoundly shaped by the target language’s ecosystem, a nuance we are the first to systematically quantify; and (3) our rigorous, scaled testing is critical in uncovering an \"illusion of correctness\" prevalent in simpler benchmarks. Our work presents a robust, scalable, and diagnostic paradigm for the next generation of LLM evaluation in software engineering. The code, data, and results are available at https://code2bench.github.io/.", "tldr": "We propose CODE2BENCH, an automated pipeline for dynamically constructing rigorous code benchmarks from recent real-world GitHub repositories to combat data contamination", "keywords": ["Dynamic Benchmark Construction", "LLM Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c38016f7a147677af6780460243410a07036d650.pdf", "supplementary_material": "/attachment/d070cc2a256bcf1b76dd933f6d93fb41c16db47c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel benchmark construction paradigm termed \"Dual Scaling\", which addresses two critical challenges in evaluating code generation LLMs: the reliance on static, easily contaminated problem sources and the use of superficial testing methods. The authors present the Code2Bench framework, which dynamically acquires problems from real-world code repositories (Scaling the Source) and integrates property-based testing (PBT) with a 100% branch coverage quality gate (Scaling the Rigor) to build the Code2Bench-2509 benchmark. Empirical evaluations demonstrate that this benchmark effectively uncovers performance gaps between models in algorithmic synthesis (SC tasks) and API application (WSC tasks), while quantifying the influence of language ecosystems on model behavior."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The 100% stringent branch coverage gate and large PBT-generated suites substantially reduce false positives and expose \"near-perfect\" failures that many benchmarks miss.\n- The outcome spectrum and “diagnostic fingerprints” provide more granular failure analysis (SyntaxErr/RuntimeErr/LogicErr vs. partial pass bands), illuminating the algorithmic synthesis vs. API-application divide and the role of language typing in error suppression.\n- WSC-Python spans >35 libraries; SC-Java demonstrates multi-language extensibility. Tasks show higher cyclomatic complexity and test volume than legacy benchmarks."}, "weaknesses": {"value": "- Some of the figures in the paper are not very clear or visually polished. For example, in Figure 2, there is noticeable overlap between text elements and between text and icons, which affects readability. Improving the clarity and layout of the figures would make the presentation more professional and easier to interpret.\n- In the Related Work section, the authors assert that existing live benchmarks rely on narrow or specific data sources. However, Code2Bench is also curated from specific GitHub repositories without disclosing the selection criteria, repository sampling strategy, or inclusion/exclusion policies. This lack of transparency makes it difficult to assess source diversity, potential sampling bias, and contamination risk. Moreover, prior benchmarks such as DomainEval also use GitHub repositories to collect domain-specific tasks. The novelty of \"Scaling the Source\" appears limited.\n- Evaluation scope focused on functional correctness: Important dimensions like performance/efficiency, readability/style, security, robustness to invalid inputs, and documentation/test generation are not directly evaluated.\n- Project-Dependent problems are mostly discarded in this pipeline. Although LSC is discussed as future work, the current evaluation does not yet include multi-function or multi-file context tasks that matter in industrial settings (e.g. I/O, resource handling, exceptions, and protocols)."}, "questions": {"value": "- It is unclear what criteria were used to assess the testing rigor for all benchmarks presented in Figure 2. Could the authors clarify how testing rigor is defined and measured?\n- Why is the Java track limited to SC only? Where is WSC-Java?\n- The authors state that each benchmark task includes approximately 500 test cases. Given that these test cases are selected after PBT generation and a 100% branch-coverage gate, does the pipeline need to generate and evaluate hundreds or even thousands of candidate inputs per task before filtering? If so, what are the actual computational costs (time and resources) per task? Similarly, for the evaluation stage, running ~500 test cases per task can significantly increase runtime and resource usage. Could the authors provide quantitative measurements of the end-to-end evaluation time per model and per task, and discuss any mechanisms (e.g., batching, caching, reduced-cost modes, seed control) used to ensure scalability across many models and large task suites? Guidance on lighter-weight modes that preserve diagnostic value would also be useful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S79H83ekKX", "forum": "QZmKyAy1VK", "replyto": "QZmKyAy1VK", "signatures": ["ICLR.cc/2026/Conference/Submission15063/Reviewer_9nHm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15063/Reviewer_9nHm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606030307, "cdate": 1761606030307, "tmdate": 1762925383343, "mdate": 1762925383343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CODE2BENCH, a benchmark construction framework for evaluating code-generating LLMs. The framework addresses data contamination through temporal filtering (extracting functions from GitHub commits after model knowledge cutoffs) and improves test rigor through Property-Based Testing with 100% branch coverage. Tasks are classified into Self-Contained (SC, pure algorithmic reasoning) and Weakly Self-Contained (WSC, API usage) using Scope Graph analysis. The authors construct CODE2BENCH-2509 with Python and Java tasks and evaluate 10 LLMs, reporting three insights: SC vs WSC performance gap, language ecosystem impact, and \"illusion of correctness.\"\n\nThis paper shows good engineering efforts but unclear novelty contribution. Two critical flaws need to be addressed: (1) No validation of benchmark value. The paper lacks direct comparison with existing benchmarks (HumanEval, MBPP, BigCodeBench) on the same models, making it impossible to assess whether CODE2BENCH provides unique insights or simply evaluates differently. (2) No novelty contribution in analysis. the three \"key insights\" are either expected results (SC/WSC gap), intuitive observations already explored (language ecosystem), or incremental findings without baseline comparison (illusion of correctness)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Important problem: Addresses real limitations in LLM code evaluation, which are data contamination and superficial testing.\n\n2. Solid engineering: Scope Graph analysis for dependency classification is technically sound. Property-Based Testing with 100% branch coverage demonstrates rigor. The framework automates benchmark construction.\n\n3. release code, data, and results."}, "weaknesses": {"value": "1. Lack of Direct Comparison with Existing Benchmarks\nFor a benchmark paper, it is crucial to demonstrate how the new benchmark compares with existing ones when evaluating the same models. The paper only shows Table 1 comparing characteristics (Dynamic, Rigorous Test, etc.) but lacks direct performance comparison with these baselines. Without evaluating the same 10 models on existing benchmarks, it's impossible to determine whether CODE2BENCH provides unique insights, whether the lower pass rates reflect higher quality or different task distribution, or whether the three \"key insights\" could be revealed by existing benchmarks. This comparison is essential to establish the benchmark's value.\n\n2. Missing Details of Validating Benchmark Construction Method\nThe paper proposes Dual Scaling with temporal filtering, Scope Graph classification, and PBT with 100% coverage, but provides less details to validate these components. What happens with 80% coverage instead of 100%? Does temporal filtering applied to HumanEval produce similar contamination resistance?  An ablation study may be a good way to explore these points.\n\n3. LLM Analysis Lacks Technical Contribution and Novelty\nThe paper presents three \"key insights\" as major contributions: (1) SC vs WSC performance gap. it is an expected result since BigCodeBench already focuses on API tasks and it's well-known these are different skills; (2) language ecosystem impact, which is also intuitive and already explored in HumanEval-X; (3) illusion of correctness. EvalPlus already demonstrated this, and without comparison to EvalPlus, the 6.94% figure lacks context. The analysis is descriptive rather than prescriptive, providing no actionable insights for improving models or evaluation. Without showing these insights are unique to CODE2BENCH or impossible to obtain from existing benchmarks, the analysis appears to justify the benchmark circularly rather than contribute genuine discoveries.\n\n4. Limited Validation of Practical Utility\nThe paper doesn't show whether performance on CODE2BENCH correlates with real-world coding capabilities (e.g., SWE-bench)."}, "questions": {"value": "Do you have any existing comparison data (even partial) showing how the same models perform on CODE2BENCH vs existing benchmarks? Can you clarify why this comparison was not included in the paper?\n\nCan you clarify how your three key insights differ from findings in BigCodeBench (WSC tasks), HumanEval-X (multi-language), and EvalPlus (test insufficiency)?\n\nDo you plan to validate CODE2BENCH's value through comparison with existing benchmarks? What would be your approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LPsC3i604U", "forum": "QZmKyAy1VK", "replyto": "QZmKyAy1VK", "signatures": ["ICLR.cc/2026/Conference/Submission15063/Reviewer_dNZT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15063/Reviewer_dNZT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981256241, "cdate": 1761981256241, "tmdate": 1762925382732, "mdate": 1762925382732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dual Scaling, a benchmark construction philosophy in CODE2BENCH framework to scaling from dynamic, real-world code repository and generating rigor test with 100% coverage. Using this method, the authors further build CODE2BENCH-2509, with 411 Python instances and 249 Java instances. The paper conducted comprehensive experiments on closed source models and open source models, and the result suggests a performance gap between API application tasks and algorithm synthesis tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark designs rigorous and strong test cases. It not only accounts for edge cases but also ensures complete test coverage, substantially outperforming other benchmarks that rely on sparse test examples, which may lead to incorrectly judged “pass” cases.\n\n2. The paper provides a carefully designed implementation in both Python and Java, addressing not only translation between languages but also their distinct type systems and library ecosystems. This enables meaningful cross-language comparison and reveals how LLM performance depends on the target language’s constraints.\n\n3. The authors effectively decouple API-calling ability from algorithmic implementation ability. The experiment suggests that models perform better at API usage than at algorithmic reasoning. This insight offers a valuable lens for future work on improving model reasoning.\n\n4. The authors also emphasize clarity and unambiguity when generating instructions, which contributes to the benchmark’s reliability and reproducibility."}, "weaknesses": {"value": "1. Although CODE2BENCH draws its source data from real repositories, the benchmark tasks remain function-level and isolated. This design simplifies testing but does not capture cross-function or module-level dependencies, which are prevalent in real-world software engineering. As such, the benchmark evaluates isolated reasoning rather than full software generation ability or collaborative code development.\n\n2. As mentioned by the authors, real-world code often includes numerous defensive branches and error-handling structures. The current test generation strategy struggles to fully cover these fragmented control flows, which are often filtered out because they fail the 100% coverage requirements. While this improves test rigor, it also excludes many defensive programming constructs that are significant in real-world software development.\n\n3. The filtering process relies on a fixed list of *allowed libraries* to define “Weakly Self-Contained” tasks. This helps maintain consistency but may also limit domain diversity, since tasks from less common libraries or specialized fields are excluded. As a result, limiting the allowed libraries may constrain the representativeness of the benchmark and may introduce unexpected bias."}, "questions": {"value": "1. The paper mentions differences between Java and Python fingerprints (Figure 3) as LLM's coding ability intertwined with their target language's ecosystems. Could the authors clarify whether this refers to differences between interpreted and compiled languages, or to other ecosystem-level factors?\n\n2. The framework requires generating hundreds of PBT-based test cases per function and enforcing 100% coverage. Could the authors quantify the computational and time costs of this process?\n\n3. A typo: Last line in page 4: perturbation techniqueZhao -> missing a blank"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U8Xr0XRylQ", "forum": "QZmKyAy1VK", "replyto": "QZmKyAy1VK", "signatures": ["ICLR.cc/2026/Conference/Submission15063/Reviewer_Esu7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15063/Reviewer_Esu7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998442923, "cdate": 1761998442923, "tmdate": 1762925382253, "mdate": 1762925382253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}