{"id": "mzLOnTb3WH", "number": 19377, "cdate": 1758295797926, "mdate": 1763700218907, "content": {"title": "WIMLE: Uncertainty‑Aware World Models with IMLE for Sample‑Efficient Continuous Control", "abstract": "Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across 40 continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over 50\\% relative to the strongest competitor, and on HumanoidBench it solves 8 of 14 tasks (versus 4 for BRO and 5 for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.", "tldr": "Uncertainty aware IMLE world models can outperform strong model-based and model-free baselines in terms of both asymptotic performance and sample efficiency.", "keywords": ["Reinforcement Learning", "Model-based RL"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3282f0f9aab4a36dd0bd59bc0296fd6bf8fabe46.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes WIMLE, a Model-Based Reinforcement Learning (MBRL) approach that constructs an uncertainty-aware world model by integrating the IMLE generative model. The method aims to address compound errors and the issue of overconfident predictions prevalent in existing MBRL algorithms. The authors evaluate WIMLE on several vector-state benchmarks, including DMC, HumanoidBench, and MyoSuite. The results indicate that WIMLE achieves significant improvements in both sample efficiency and asymptotic performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper successfully extends and integrates IMLE (a mode-covering generative model proven effective in low-data regimes) into the MBRL framework for world model learning.\n- The method effectively integrates uncertainty estimates from model predictions into the Reinforcement Learning objective function, which helps mitigate policy misalignment caused by model bias and overconfident predictions.\n- WIMLE demonstrates substantial improvements over strong model-free and model-based baselines across multiple continuous control tasks, particularly showcasing outstanding performance on the challenging Humanoid-run task."}, "weaknesses": {"value": "- Limitation in Observation Space: The experimental evaluation is limited to vector-state (low-dimensional) environments. Many state-of-the-art MBRL algorithms have demonstrated strong performance in high-dimensional (pixel/image) state spaces. The paper currently lacks verification of WIMLE’s performance in these crucial high-dimensional settings.\n- Potential for Unfair Comparison: The WIMLE algorithm uses a mixture of real and imaginary data for policy training. In contrast, the original paper implementations of comparable baselines (e.g., DreamerV3, TDMPC2) typically rely solely on imaginary trajectories. This difference in data usage introduces a potential source of unfairness in the comparison and requires clarification from the authors."}, "questions": {"value": "1.  As noted in Weakness 1, could the experimental scope be extended to image-state environments (e.g., a subset of DMC tasks using raw pixel observations or the Atari 100k benchmark) to evaluate WIMLE’s effectiveness in such contexts?\n2.  As noted in Weakness 2, given that the comparison algorithms (DreamerV3, TDMPC2) primarily use model-generated trajectories for policy training in their original implementations, please clarify whether this variable was unified in the current comparative experiments. If not, to ensure a fair comparison, could the authors provide an ablation experiment where the WIMLE policy is trained exclusively on model-generated trajectories (i.e., without real environment data)?\n3. WIMLE uses uncertainty-based weighted updates within the Actor-Critic architecture. Is the weighting factor applied only to the loss update of the Value Network, or is it applied to the loss updates of both the Policy Network and the Value Network?\n4. It is recommended that the results presented in Figure 2 be converted into a table format. The current graphical representation is poor for clearly depicting the performance (e.g., time efficiency) of lower-performing algorithms like DreamerV3.\n5. Code and Implementation Details: To ensure research reproducibility, can the authors publicly release the complete experimental code and model implementation upon acceptance of the paper? This is critical for the community to verify the claimed performance improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kzOgLulVFB", "forum": "mzLOnTb3WH", "replyto": "mzLOnTb3WH", "signatures": ["ICLR.cc/2026/Conference/Submission19377/Reviewer_z1Wb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19377/Reviewer_z1Wb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760864195596, "cdate": 1760864195596, "tmdate": 1762931304788, "mdate": 1762931304788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We appreciate the time and effort taken by all the reviewers to provide constructive feedback on our paper, and we are pleased to receive unanimous appreciation for our proposed method and results. In particular, the reviewers remarked, \"**Timely and relative contribution** - addresses two active challenges in model-based RL\" (R1), \"**clean and general mechanism**... simple yet effective\" (R2), \"**experiments are convincing**: the paper compares WIMLE across several hard continuous RL benchmarks\" (R3), and \"WIMLE demonstrates **substantial improvements** over strong model-free and model-based baselines\" (R4).\n\nBelow is a brief summary of our responses to selected questions, with full details in the individual responses.\n\n### Q1: Does WIMLE remain stable at long horizons? (R2)\n\nA1: Yes, WIMLE remains stable at long horizons (e.g., H=40) without rollout schedulers (Figure 11, Appendix), as the weighting naturally down-weights unreliable long-term predictions.\n\n### Q2: How does efficiency compare to Transformers/Diffusion? (R1)\n\nA2: WIMLE is significantly faster in rollout throughput (~10x vs Transformers, ~20x vs Diffusion) while achieving state-of-the-art results in continuous control.\n\n### Q3: Does WIMLE capture reward multimodality? (R1)\n\nA3: Yes, WIMLE models the joint distribution of next states and rewards, allowing diverse outcomes for both from a single latent variable.\n\n### Q4: How does uncertainty weighting improve stability? (R3)\n\nA4: By inversely weighting synthetic data based on predictive variance, WIMLE performs a risk-weighted regression to mitigate unreliable predictions.\n\n### Q5: How does WIMLE handle unimodal dynamics? (R1)\n\nA5: IMLE is a mode-covering estimator, allowing it to capture diverse outcomes in multimodal dynamics while concentrating on a single mode in deterministic settings.\n\n### Q6: Can WIMLE extend to visual tasks? (R4)\n\nA6: Yes, WIMLE's uncertainty mechanism is orthogonal to input modality.\n\n### Q7: Can WIMLE be used for planning? (R1)\n\nA7: Yes, WIMLE can serve as a drop-in stochastic world model for planners. Its uncertainty estimates can be used to penalize high-risk trajectories or adapt planning horizons.\n\n### Q8: Do long rollouts saturate batches with low-weight data? (R2)\n\nA8: No, batches remain balanced because they contain real transitions ($w=1$) and early rollout steps with high weights. Uncertainty grows with horizon, so only later steps are down-weighted, preserving a strong learning signal.\n\n### Q9: How does WIMLE perform on pure imagination? (R4)\n\nA9: WIMLE performs comparably when trained on pure imagination (Figure 12, Appendix), confirming the high quality of the generated data and the effectiveness of our uncertainty weighting.\n\n### Q10: How does WIMLE compare to model-free baselines in total efficiency? (R1)\n\nA10: WIMLE is significantly more efficient in total time-to-solution (Figure 14, Appendix) when accounting for data collection costs, as its sample efficiency outweighs the higher per-update compute cost."}}, "id": "FLoeibWZBP", "forum": "mzLOnTb3WH", "replyto": "mzLOnTb3WH", "signatures": ["ICLR.cc/2026/Conference/Submission19377/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19377/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19377/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763699717878, "cdate": 1763699717878, "tmdate": 1763699717878, "mdate": 1763699717878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed to leverage Implicit Maximum Likelihood Estimation (IMLE) as base architecture for model-based reinforcement learning.\n\nWhile the use of IMLE is to some extent incremental, leveraging its uncertainty estimates within TD learning is interesting.\nOverall the empirical results of this paper are encouraging: on most tasks the performance generally modest, however on several harder tasks (Humanoid Run, H1-slide and Myo-key-turn-hard), the proposed algorithm is roughly two to three times more sample efficient."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Weighting Bellman backups by model uncertainty is an interesting idea.\n\n* The experiments are convincing: the paper compares WIMLE across several hard continuous RL benchmarks against what is (to my knowledge) the state-of-the-art algorithms (model-free and model-based)"}, "weaknesses": {"value": "**The usage of uncertainty is not well grounded**\n* For instance, equation 12 does not distinguish between the aleatoric and epistemic uncertainty. This can lead to low credit assignment in states with high stochasticity, rather than states with large model error. I believe this is not observed in the experiments as most of these Mujoco tasks are mostly deterministic.\n* In addition, rewarding states based on their epistemic uncertainty (“intrinsic rewards”) is known to accelerate learning in sparse reward tasks. A discussion/study on how WIMLE works even in light of this contradiction would improve the paper.\n* Aside from very high-level intuition about penalizing states where model accuracy is low, there is no grounding for why this may work. Building some theoretical intuition (even if not at the strictest rigorousness) would improve the paper.\n\n**Other**\n\n* Minor: adding vision tasks would make the contribution of this paper more impactful."}, "questions": {"value": "* What UTD values did you use when comparing the model-free methods (e.g. BRO)? Did you try to ablate their performance and sample efficiency when increasing UTD?\n* How does WIMLE compares to model-free methods in terms of wall-clock time? It would be great to discuss/demonstrate empirically the tradeoff between performance and wall-clock time."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "emYmo7ythi", "forum": "mzLOnTb3WH", "replyto": "mzLOnTb3WH", "signatures": ["ICLR.cc/2026/Conference/Submission19377/Reviewer_8Fgq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19377/Reviewer_8Fgq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761301563552, "cdate": 1761301563552, "tmdate": 1762931304336, "mdate": 1762931304336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes WIMLE, a model-based reinforcement learning (MBRL) framework that combines IMLE with uncertainty-aware weighting of rollouts to downweight uncertain counterfactuals in a dyna-style framework. The method trains an ensemble of IMLE world models to capture multi-modal dynamics and computes predictive uncertainty by aggregating variance across ensemble members and latent samples. Synthetic rollouts are then weighted by uncertainty to downweight uncertain transitions in the critic update, reducing bias from unreliable predictions. Using SAC with distributional and quantile Q-learning, WIMLE achieves improved sample efficiency and asymptotic performance on 40 continuous-control tasks from DMC, MyoSuite, and HumanoidBench, outperforming strong model-free and model-based baselines and notably solving tasks in the HumanoidBenchmark that comparable methods could not."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is the first to extend Implicit Maximum Likelihood Estimation (IMLE) to the conditional, stochastic setting of model-based reinforcement learning. This adaptation allows the world model to represent multi-modal transitions, mitigating the regression-to-the-mean problem common in Gaussian dynamics models like Model-Based Policy Optimization (MBPO). This improvement is shown through ablations wherein they compare their method with and without IMLE.\n\n- The world model choice of IMLE is simple to swap in place of prior methods and relatively straightforward making the wall-clock (Figure 2) and performance gains (Figure 4) attractive alongside the relative simplicity of implementation.\n\n- WIMLE introduces a clean and general mechanism to handle model uncertainty once IMLE is introduced and is comparable to prior words like MBPO: ensemble variance aggregated over latent samples is used to compute per-transition confidence weights. These weights directly scale the critic loss, reducing the influence of unreliable synthetic transitions. This simple yet effective weighting stabilizes learning, mitigates autoregressive errors across a few short horizon lengths.\n\n- The authors evaluate on 40 continuous-control tasks spanning three suites (DMC, MyoSuite, and HumanoidBench), which together cover locomotion and dexterous manipulation at different difficulty levels. WIMLE achieves strong sample efficiency improvements and solves more HumanoidBench tasks (8) than competing baselines (4 and 5 for Bro and SimbaV2, respectively). The diversity of evaluated settings and consistent advantage over both model-free and model-based methods lend credibility to the reported improvements.\n\n- The paper is well-organized, and the visualizations are effective in communicating the author's desired goals. Figures 5 and 6 clearly illustrate the practical effects of uncertainty weighting and IMLE-based multi-modality on performance and confidence calibration. The authors’ explanations are intuitive and technically sound, making the concepts introduced in this work accessible to a broader RL audience and relatively easy to implement."}, "weaknesses": {"value": "- Comparisons to other uncertainty-aware methods such as InfoProp [1] and Macura [2] are mostly missing, even though these are conceptually the most similar baselines which is noted frequently by the authors. There is a note in the model-bias section of the paper saying that InfoProp fails in high dimensional complex domains like DMC humanoid-run as well as a subfigure (Figure 1)(a)), but InfoProp was not deployed in DMC in the original paper (just Gym) and likewise with Macura. Was any hyperparameter tuning done to show these methods don't work? Furthermore, the Ant and Humanoid tasks in OpenAI Gym are both complex, high-dimensional tasks, and their state and action spaces are larger than many in DMC other than dog. Both Macura and InfoProp outperform prior work on those tasks, contradicting this paper’s claim that such methods fail to scale to high-dimensional domains. Even without direct evaluation against other uncertainty-based methods, it is at least clear that WIMLE is competitive or better than SOTA model-based and model-free methods. However the relative performance when compared to InfoProp and Macura is not a fair comparison when neither was tested in DMC, Myosuite, etc and leaves out an important comparison to prior similar works.\n\n- Following on the prior point, the paper does not include any results on OpenAI Gym, making it difficult to assess cross-domain generalization, which has been shown to be a challenge for Dyna-style methods. Prior works [3] and [4] show that performance often fails to transfer between Gym and DMC for Dyna-style methods (e.g., MBPO, ALM), so the omission of deployment in this benchmark weakens the claim of broad generality across benchmarks despite including 40 tasks across 3 benchmarks.  \n\n- The performance attribution between WIMLE’s IMLE weighting and its upgraded SAC backbone is unclear. The authors use a distributional, quantile-based version of SAC without explaining why, but then go on to compare against SAC without these additional modifications. As a result, some of the observed gains might come from the introduction of these modifications to SAC, but be attributed to WIMLE. It seems necessary to at minimum include further ablations showing performance after the removal of the model beyond what is shown in Figure 5(a) for humanoid-run for the model-free case. \n\n- Reported rollout horizons are limited to H = 8, whereas MBPO and related methods demonstrate stability at horizons of 15–25 (when combined with heuristic or algorithmic iteratively rollout length selection) and then show performance collapse for horizons of 500 in MBPO (See Figure 3 (b) in the MBPO paper). The claim that uncertainty-aware weighting mitigates long-horizon bias is therefore untested at scales where such bias typically becomes problematic enough to cause performance collapse.  \n\n- As mentioned previously, several design choices are under-explained, including the decision to use quantile Q-learning and distributional critics. These components can independently improve stability and learning dynamics, but the paper does not disentangle their contributions from those of WIMLE’s IMLE-based uncertainty weighting. \n\n- Unless I missed it, the wall-clock comparison in Figure 2 lacks key details such as the benchmark environment and codebase implementations (e.g., JAX, PyTorch, TensorFlow) used for each baseline. Without these specifics, it is difficult to assess whether the timing advantage reflects algorithmic efficiency or implementation-level differences.\n\n[1] Frauenknecht, B., Subhasish, D., Solowjow, F., & Trimpe, S. (2025). On rollouts in model-based reinforcement learning. In Proceedings of the 2025 International Conference on Learning Representations (ICLR 2025). arXiv preprint arXiv:2501.16918. Available at https://arxiv.org/abs/2501.16918\n\n[2] Frauenknecht, B., Eisele, A., Subhasish, D., Solowjow, F., & Trimpe, S. (2024). Trust the model where it trusts itself: Model-based actor-critic with uncertainty-aware rollout adaption. In Proceedings of the 41st International Conference on Machine Learning (ICML 2024) (Vol. 235, pp. 13973–14005). Available at https://arxiv.org/abs/2405.19014\n\n[3] Voelcker, C. A., Hussing, M., & Eaton, E. (2024). Can we hop in general? A discussion of benchmark selection and design using the Hopper environment. In Finding the Frame: An RLC Workshop for Examining Conceptual Frameworks (RLC 2024). arXiv preprint arXiv:2410.08870. Available at https://arxiv.org/abs/2410.08870\n\n[4] Barkley, B., & Fridovich-Keil, D. (2025). Stealing that free lunch: Exposing the limits of Dyna-style reinforcement learning. In Proceedings of the 42nd International Conference on Machine Learning (Vol. 267, pp. 2978–3002). Proceedings of Machine Learning Research. Available at https://proceedings.mlr.press/v267/barkley25a.html"}, "questions": {"value": "1. How does WIMLE perform on standard Gym benchmarks? And as a follow on, why were InfoProp and Macura-style adaptive rollout methods excluded from direct comparison, given their conceptual similarity in uncertainty modulation? I saw the note about InfoProp not performing in humanoid-run, but as I mentioned in the weaknesses section, the explanation doesn't seem sufficient with the added context of Ant and Humanoid from Gym.\n2. Have you tested rollout horizons beyond H = 8? Would uncertainty weighting still stabilize longer synthetic rollouts, as MBPO demonstrated for horizons up to 25 or more? Are there asymptotic max performance gains to be had with longer rollouts due to the down weighting? Or does the training dataset get saturated with bad/down weighted synthetic data if you opt for progressively longer rollouts and that leads to degraded performance since entire batches are down weighted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JQdHn7PqUr", "forum": "mzLOnTb3WH", "replyto": "mzLOnTb3WH", "signatures": ["ICLR.cc/2026/Conference/Submission19377/Reviewer_GhD7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19377/Reviewer_GhD7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761344806279, "cdate": 1761344806279, "tmdate": 1762931303626, "mdate": 1762931303626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WIMLE, a model-based reinforcement learning (MBRL) algorithm that uses Implicit Maximum Likelihood Estimation (IMLE) from Li & Malik 2018 to learn multi-modal, uncertainty-aware world models for continuous control. WIMLE leverages conditional IMLE to generate stochastic transitions without iterative sampling, enabling fast rollouts while avoiding mode collapse typical of unimodal Gaussian models. An ensemble of IMLE world models provides predictive uncertainty estimates, which are then incorporated into the RL objective through an uncertainty-weighted TD loss. Empirically, WIMLE is evaluated across 40 tasks from different benchmark suites, demonstrating substantial gains in sample efficiency and strong asymptotic performance relative to baselines. The paper positions WIMLE as a practical alternative to diffusion- or transformer-based world models for online RL, claiming competitive performance with reduced inference cost."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. **Timely and relative contribution** - addresses two active challenges in model-based RL - multi-modal dynamics modeling and uncertainty calibration through an elegant adaptation of IMLE.\n2. **Strong experimental coverage** - Evaluation spans three modern continuous-control suites and 40 tasks with statistically rigorous reporting from Agarwal et al., 2021\n3. **Conceptual generality** - the proposed uncertainty-aware weighting is algorithm-agnostic and could integrate with diverse RL backbones.\n4. **Empirical clarity** - Ablations (Fig 5-6) isolate the impact of uncertainty weighting and IMLE-based multimodality, demonstrating that multi-modal model predictions both improve calibration and stability at longer horizons."}, "weaknesses": {"value": "1. **Writing and organization** - Section 3 (Method) is dense and at times difficult to follow. The algorithmic code could be clarified by referencing a baseline (e.g., MBPO) and then enumerating modifications. Sections 3.2-3.3 overlap conceptually and might be merged for concision. Figure placement and scaling should be revised - several are too small to read; figure text should roughly match main text size.\n2. **Missing multimodal context** - While the paper emphasizes multimodal dynamics, it remains unclear how well IMLE captures multimodality relative to more contemporary models, e.g., transformer-based world models (Micheli et al., 2023; Robine et al., 2023) or diffusion models (Ajay et al., 2023). A pedagogical analysis (similar to Figure 3, Chi et al., 2024) illustrating where multimodality matters would strengthen the argument.\n3. **Missing baselines** - Although MR.Q and PPO are mentioned as baselines, their results appear absent from key plots. Including them, especially PPO for wall-time comparison, would provide a clearer model-free reference.\n4. **Insufficient exploration of uncertainty weighting** - the uncertainty metric $w=\\dfrac{1}{\\sigma + 1}$ is intuitive but somewhat ad hoc. Alternative formulations (e.g., softmax or learned calibration of confidence) would help validate robustness and necessity.\n5. **Presentation and formatting** - The manuscript has several typographic and formatting inconsistencies. Figures (especially 3, 5, and 6) and algorithm boxes should be cleaned up for readability. The main algorithm should be self-contained enough to implement directly.\n6. **Scope of multimodality** - the paper restricts multimodal modeling to transition dynamics. Given that the reward and value functions can also be multimodal, extending the IMLE framework to those components could be a natural next step."}, "questions": {"value": "1. You convincingly argue that dynamics are multi-modal; why not extend the IMLE-based formulation to multi-modal rewards or even policy/value functions?\n2. If a non-multimodal model achieves the same asymptotic performance as WIMLE, does that imply that the underlying task is not truly multimodal? How do you detect multimodality in practice?\n3. Comparison to diffusion or transformer world models - since you position WIMLE as a more efficient alternative, could you report asymptotic and wall-time comparisons against diffusion-based and transformer-based world models?\n4. Why only update the critic uncertainty-weighted (Eq. 13)? Would uncertainty-aware plicy gradients offer additional stability?\n5. Figure 2 suggests favorable throughput. Can you provide wall-time comparisons against the baselines in Figure 4? Effectively swap out the x-axis for time.\n6. Could WIMLE be used as a plug-in world model within planning-based algorithms (e.g., TD-MPC2 or MPPI)? How would uncertainty weighting interact with such planners?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nx1E3brAET", "forum": "mzLOnTb3WH", "replyto": "mzLOnTb3WH", "signatures": ["ICLR.cc/2026/Conference/Submission19377/Reviewer_FqDz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19377/Reviewer_FqDz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983760338, "cdate": 1761983760338, "tmdate": 1762931303003, "mdate": 1762931303003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}