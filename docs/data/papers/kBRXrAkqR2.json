{"id": "kBRXrAkqR2", "number": 23537, "cdate": 1758345118464, "mdate": 1759896809797, "content": {"title": "SplitQuant: Efficient Low-Bit Quantization for Diffusion Transformers via In-Channel Dimension Splitting", "abstract": "Diffusion models currently dominate the field of image generation. However, generating high-resolution images requires larger-scale diffusion models that consume substantial computational resources and memory during inference. While post-training quantization offers a promising solution to reduce computational costs and memory usage through low-precision representations, existing approaches face significant challenges when applied to diffusion models. Unlike large language models that are memory-bound, \\textbf{Di}ffusion \\textbf{T}ransformers (DiT) are compute-intensive during inference. Consequently, current methods that rely on additional parameters to recover the performance of extremely low-bit quantized models achieve minimal acceleration benefits, as they introduce non-negligible computational overhead.\nTo address these challenges, we propose \\textbf{SplitQuant}, a novel approach that reduces additional computational overhead while improving low-bit quantization performance by strategically splitting the in-channel dimension of linear layers and activations. Recognizing that diffusion transformer architectures differ fundamentally from large language models, we develop a specialized optimization pipeline tailored specifically for diffusion models, which significantly enhances the generation quality of low-bit quantized models. Additionally, we implement custom-optimized CUDA kernels for SplitQuant that render the preprocessing overhead from additional parameters and quantization processes negligible, achieving single-operator performance comparable to W$4$A$4$ QGeMM across various tensor shapes.\nExtensive experiments on FLUX.1 and PixArt-$\\Sigma$ demonstrate SplitQuant's effectiveness in both image generation scenarios. Our method achieves $2.7\\times$ acceleration on linear layer operators across different shapes, with SplitQuant kernels delivering performance that approaches Int4 QGeMM acceleration. The code is available at this \\href{https://anonymous.4open.science/status/SplitQuant-23537iclrAnonymous}{anonymous link}.", "tldr": "", "keywords": ["Diffusion models", "quantization", "diffusion transformers", "low-bit quantization", "image generation", "video generation", "model acceleration", "memory optimization", "post-training quantization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cfe00433a2e1e543e18178c15e41ad46ddfaf567.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SplitQuant, a post-training quantization method for Diffusion Transformers. It splits the in-channel dimension of weights and activations into multiple slices that share transformation parameters, effectively reducing additional computation by about a factor of two while maintaining 4-bit quantization quality. The authors also introduce adaptive strategies to mitigate timestep-induced error amplification and text-guided gradient conflicts. Experiments on FLUX.1 and PixArt-Σ demonstrate near-lossless W4A4 quantization and up to 2.7× operator-level speedup."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a practical and well-engineered method that improves quantization efficiency for diffusion transformers. The slicing and parameter-sharing design is simple, effective, and compatible with existing PTQ frameworks. The adaptive optimization strategies are thoughtfully tailored to diffusion-specific issues, and the implementation appears technically sound."}, "weaknesses": {"value": "**1. Limited Applicability** The method is evaluated solely on diffusion transformers, with no evidence of generalization to other architectures such as large language models, vision transformers, or CNN-based diffusion systems. The approach appears tied to DiT-specific modules like timestep and AdaLN, limiting its broader relevance.\n\n**2. Small-Scale Experiments** The evaluation scope is narrow. Only FLUX.1 and PixArt-Σ are tested, while Wan2.1 is mentioned without concrete results. The datasets and calibration samples are small, and the baselines are limited to SVDQuant. Without comparisons to FlatQuant, PTQ4DiT, or other strong baselines, the reported improvements lack sufficient context.\n\n**3. Missing End-to-End Evaluation** The reported acceleration results are restricted to operator-level measurements. The paper does not provide end-to-end image generation times, GPU memory analysis, or hardware-specific benchmarks. Without such evidence, the claimed efficiency benefits remain unsubstantiated in practical use cases.\n\n**4. Overclaim** Some claims in the paper appear overstated relative to the presented evidence. The title emphasizes “Efficient Low-Bit Quantization,” yet all experiments focus on W4A4 settings, with no results below 4 bits. Similarly, while the abstract claims “near-lossless” quality, Table 1 shows noticeable degradation on several metrics. This gap between claim and data weakens the paper’s credibility.\n\n**5. Presentation Issue** Figure 1 appears to lack embedded fonts, causing rendering problems in some PDF viewers. This affects readability of the main workflow illustration and should be corrected for proper presentation."}, "questions": {"value": "As above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qipFAaijUu", "forum": "kBRXrAkqR2", "replyto": "kBRXrAkqR2", "signatures": ["ICLR.cc/2026/Conference/Submission23537/Reviewer_U8dJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23537/Reviewer_U8dJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760700732332, "cdate": 1760700732332, "tmdate": 1762942702473, "mdate": 1762942702473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SplitQuant, a novel framework for efficiently quantizing large language models (LLMs) into low-bit formats while preserving accuracy. Instead of quantizing full weight matrices directly, the method splits them into smaller submatrices, quantizes each part independently, and then stitches them back together using learned scaling factors. This approach reduces quantization error, lowers memory footprint, and speeds up inference. Experiments on GPT and LLaMA models show that SplitQuant achieves better performance than existing methods like GPTQ and AWQ under the same bit-width settings. The method is also hardware-friendly and supports parallelizable computation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n\n1: Lower Quantization Error via Split-and-Stitch Strategy\nThe matrix-splitting approach effectively reduces local quantization error, maintaining accuracy even at 3–4 bit precision.\n\n2: Efficient and Hardware-Compatible\nThe approach avoids complex operations like reconstruction or per-token calibration, making it suitable for GPU/TPU deployment.\n\n3: Strong Experimental Validation\nExtensive benchmarks on language modeling and question-answering tasks show consistent improvements over leading baselines."}, "weaknesses": {"value": "Weaknesses\n\n1: Additional Splitting Hyperparameters\nThe choice of split size introduces a new hyperparameter that may affect accuracy and adds tuning complexity.\n\n2: Limited Analysis on Extremely Low-Bit (e.g., 2-bit) or Mixed Precision\nThe method mainly focuses on 3–4 bit quantization, leaving extreme compression cases less explored.\n\n3: No Theoretical Guarantee on Optimality\nWhile effective empirically, the method lacks a theoretical foundation for why the split strategy minimizes quantization error globally."}, "questions": {"value": "1: Scalability to Multimodal Models\nCan SplitQuant be extended to multi-modal or vision-language models like LLaVA or GPT-4V, where activations vary across modalities?\n\n2: Optimal Split Strategy\nHow sensitive is the model performance to how weight matrices are split (uniform vs. learned splits)? Is there an optimal rule?\n\n3: Latency vs. Accuracy Trade-off\nDoes the splitting and stitching process introduce additional latency during inference, and how does this compare with fast-decode methods like SmoothQuant or TensorRT optimizations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GVGQWeEOdz", "forum": "kBRXrAkqR2", "replyto": "kBRXrAkqR2", "signatures": ["ICLR.cc/2026/Conference/Submission23537/Reviewer_9mzz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23537/Reviewer_9mzz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533440835, "cdate": 1761533440835, "tmdate": 1762942702181, "mdate": 1762942702181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SplitQuant, a novel post-training quantization method designed to efficiently run powerful Diffusion Transformer (DiT) models in low-precision (4-bit) formats. The core innovation is a strategy that splits the input channel dimensions of weights and activations into smaller slices, applies shared fine-grained transformations to mitigate outlier-induced errors, and uses custom CUDA kernels to minimize computational overhead. The authors also identify and address two DiT-specific optimization challenges—error amplification by timestep modules and gradient conflicts from text conditioning—via an adaptive learning rate schedule and a grouped isolation strategy, ultimately achieving near-lossless W4A4 quantization with significant speedups on models like FLUX.1 and PixArt-Σ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive Technical Contribution: The solution is multi-faceted and robust. It's not just the splitting idea; it includes a specialized optimization pipeline (addressing timestep and text-guided interference) and custom kernel implementation, which are crucial for achieving both high quality and practical speedup.\n\n2. Empirical Validation: The experiments are extensive, using state-of-the-art models (FLUX.1, PixArt-Σ) and multiple datasets (MJHQ-30K, sDCI). The results convincingly demonstrate superior or comparable performance to the strong baseline (SVDQuant) across multiple metrics (FID, IR, LPIPS, PSNR), supporting the \"near-lossless\" claim."}, "weaknesses": {"value": "1. Indirect Evidence of Speedup: While the paper claims a 2.7x acceleration and provides a theoretical FLOPs reduction derivation, it lacks a direct, end-to-end inference latency or throughput comparison against the baselines (especially SVDQuant) on the same hardware. The performance claims rely heavily on single-operator kernel benchmarks and theoretical analysis.\n\n2. Limited Scope of Models: The evaluation is focused exclusively on image generation models (FLUX.1, PixArt-Σ). Given the rising importance of video generation DiTs (like the mentioned Wan2.1) and their even higher computational demands, demonstrating the method's efficacy on a video generation task would significantly strengthen the paper's impact.\n\n3. Clarity on Overhead: The claim of \"negligible\" overhead from the additional parameters and transformations could be more precisely quantified. A breakdown of the total parameter increase and its memory footprint compared to the baseline quantized model would make this claim more concrete."}, "questions": {"value": "In Fig.1, there are so many small dots, and I don't know what they mean."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s27Lk7dASy", "forum": "kBRXrAkqR2", "replyto": "kBRXrAkqR2", "signatures": ["ICLR.cc/2026/Conference/Submission23537/Reviewer_o1WF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23537/Reviewer_o1WF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797625739, "cdate": 1761797625739, "tmdate": 1762942701911, "mdate": 1762942701911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SplitQuant, which improves upon FlatQuant by splitting input and weight tensors along the inner dimension, and optimizing separate transformation matrices for each of the tensor slices. Such splitting operation reduces the computation in the transformation by a factor of $\\sqrt{K}$, where $K$ is the number of splits. The paper also studied diffusion-specific stabilization techniques for diffusion, namely layer-wise adaptive learning rate and gradient separation. The paper evaluated the generation quality after quantization with two popular text-to-image models (FLUX.1-schnell and PixArt-$\\Sigma$) and on two popular text-to-image datasets (MJHQ and sDCI)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The proposed method achieves generation quality comparable to the BF16/FP16 high precision models under the W4A4 settings."}, "weaknesses": {"value": "* The Wan 2.1 experiments mentioned in the Abstract cannot be found in the main text or the released code.\n\n* The design and evaluation details of the CUDA kernel mentioned in the Abstract and Conclusion sections could not be found in the main text. Therefore, it's unclear if this is a significant technical contribution on top of FlatQuant, or whether the claimed theoretical computational reductions can be practically achieved.\n\n* Following the previous point and according to the paper's computation analysis in A.2, the compute intensity of the transformation appears to be pretty low for modern GPUs: $2bsn_1n_2(n_1+n_2) / \\sqrt{K} / (bsn_1n_2) = 2(n_1+n_2) \\sqrt{K} \\approx$ around 100, meaning that the transformation is likely to be memory bound. In such case, a careful CUDA kernel benchmark or analysis would be very important to demonstrate the real-world advantage of the computation reduction relative to FlatQuant.\n\n* Even if the theoretical speedup can be achieved in real-world kernel benchmark, it's also important to demonstrate the end-to-end speedup, as the proposed method would be less meaningful if the FlatQuant overhead is already small (e.g., a two-fold speedup of a 5% overhead would still only be a 2% end-to-end speedup, which might be less attractive for the additional complexity of the method).\n\n* The quantization error analysis of the slicing operation is missing. As the method essentially degenerates to channel-wise scaling as the number of slices reaches the maximum (i.e., the number of channels), it's a natural question to investigate how fast the error grows (i.e., how fast the benefit of FlatQuant vanishes) as the number of slices increases. The proposed method might not be as meaningful if the quantization error grows quickly even with a very small number of slices.\n\n* Equation (4) appears to be confusing: If each $X_i$ is of shape $n \\times (k/s)$ and each $W_i$ is of shape $m \\times (k/s)$, then each of the item in the concatenation operation $Q (X_i P_i) Q( P_i^{-1} W_i^T)$ should be of shape $n \\times m$, and therefore concatenation along any dimension would cause a shape mismatch to $XW^T$ which is also of shape $n \\times m$.\n\n* Section 3.3.2 is confusing: In Equations (4) (6) (7), the optimization goal of the quantization process is to minimize the layer-wise L2 norm of the output before and after quantization, which does not involve noise reconstruction or text alignment. Is this section describing another stage of the training process? If so, can the detailed optimization goals be described in math expressions (e.g., in the form of $\\text{argmin}_\\theta L(\\theta)$, with $L$ and $\\theta$ clearly defined?)\n\n* The implementation details and ablation study of the adaptive learning rate (Ln 311-319) are missing."}, "questions": {"value": "Please address the questions raised in the Weaknesses part. I'd be willing to check the released codebase if authors could kindly point out the file path and line number of any missing details mentioned in the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JowotdzSnH", "forum": "kBRXrAkqR2", "replyto": "kBRXrAkqR2", "signatures": ["ICLR.cc/2026/Conference/Submission23537/Reviewer_LXf6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23537/Reviewer_LXf6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998784114, "cdate": 1761998784114, "tmdate": 1762942701594, "mdate": 1762942701594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}