{"id": "nESyz4PvJL", "number": 21394, "cdate": 1758317106239, "mdate": 1759896924300, "content": {"title": "VLMgineer: Vision-Language Models as Robotic Toolsmiths", "abstract": "Tool design and use reflect the ability to understand and manipulate the physical world through creativity, planning, and foresight. As such, it is often regarded as a measurable indicator of cognitive intelligence across biological species. While much of today’s research on robotics intelligence focuses on generating better control strategies, inventing smarter tools offers a complementary form of physical intelligence: moving the problem-solving onus into the tool’s geometry so that control becomes simpler. This motivates us to ask: can today’s foundation models offer useful priors to automatically invent—and effectively wield—such tools? We present VLMgineer, the first fully automatic framework designs tools and actions from scratch by harnessing the creativity of Vision–Language Models (VLMs) together with evolutionary search. We evaluate VLMgineer on a diverse benchmark of everyday manipulation scenarios that demand creative tool design and use. Across this suite, VLMgineer consistently discovers tools and policies that solve tasks more effectively and innovatively, transforming challenging robotics problems into straightforward executions. It also consistently outperforms VLM-generated designs from human specifications and existing human-crafted tools for everyday tasks. We further demonstrate that VLMgineer’s automatically designed tools and action policies transfer seamlessly to real-world task execution on a physical robot. To facilitate future research on automated tool invention, we will release our benchmark and code. Project Website: https://vlmgineer.github.io/.", "tldr": "We introduce VLMgineer, a novel VLM-driven evolutionary framework that automatically co-design tools and actions to solve robotics task.", "keywords": ["robotic manipulation", "robotic tool use", "vision language models"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abc48bc978dc63b7e6bf0b26db2fd701570367b1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents VLMgineer, a VLM-aided evolutionary algorithm that jointly optimizes tool design and action strategy to solve challenging manipulation tasks beyond the capabilities of conventional robot morphologies. It is validated that the inherent physical creativity of vision-language models, when paired with iterative evaluation and refinement, can yield interesting tool-action pairs that outperform human-designed counterparts. The paper also introduces a comprehensive simulation benchmark, RoboToolBench, for evaluating tool design performance, which could facilitate further investigation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper addresses an important research problem, i.e. free-form evolutionary optimization of tool design to enhance the capabilities of robotic systems in challenging scenarios. An innovative VLM-based approach is presented that combines the physical creativity of VLMs with evolutionary computation to yield high-performing tool-action pairs. \n\n2.The paper is well-written and easy to follow. The abundant illustrations and qualitative results greatly help with readers’ understanding. \n\n3.Full prompts and implementation details are provided to ensure reproducibility. The authors also provide an actionable procedure for transferring the co-optimized solutions onto physical robot arms, increasing real-world practicality."}, "weaknesses": {"value": "1.In the proposed method, the VLM is prompted to directly generate discrete action waypoints, instead of using reinforcement learning. From my understanding, this would more or less rely on the privileged geometric information provided in environment source codes (such as object positions and orientations). The authors could discuss in more detail the specific information provided regarding environment setup, and what is the implication and generalizability of such control strategies for less structured, real-world settings where only images and textual instructions are available. \n\n2.The tool use setting considered in this work is somewhat simplified, where the tool is attached to the robot arm and fixed all along. Since in this case the robot morphology and tool could be considered as a whole, “augmented” morphology, the authors are suggested to discuss what are the unique challenges to be solved for this particular problem setup, compared with previous morphological design studies that also leverage large models."}, "questions": {"value": "1.Could the authors describe how much does the proposed approach rely on the spatial reasoning capabilities of VLMs, and how much on the privileged geometric information, in order to directly generate action waypoints? \n\n2.In many cases expert knowledge could still serve as useful informative priors. I wonder whether the authors have tried utilizing the human-specified or RLBench tools as an initial population and further evolve them with VLMgineer. Would this yield additional performance gains, or instead impose human biases that constrain more creative search?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wHYApKBJL6", "forum": "nESyz4PvJL", "replyto": "nESyz4PvJL", "signatures": ["ICLR.cc/2026/Conference/Submission21394/Reviewer_5kyU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21394/Reviewer_5kyU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725376097, "cdate": 1761725376097, "tmdate": 1762941742156, "mdate": 1762941742156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VLMGINEER, an autonomous framework that uses vision–language models (VLMs) together with evolutionary search to co-design both robotic tools and action policies from scratch. Unlike prior work that assumes a fixed toolset or relies on human-specified design parameters, VLMGINEER automatically generates tool geometries (in URDF) and corresponding action waypoints, iteratively refining them through simulation-based evaluation. To benchmark this setting, the authors propose ROBOTTOOLBENCH, a suite of 12 manipulation tasks that are infeasible using a standard gripper. Experiments show that VLMGINEER consistently outperforms human-designed tools, VLM-generated tools from human prompts, and RLBench baselines, with strong improvements in both success rates and reward metrics. The framework also demonstrates successful sim-to-real transfer on a physical Franka robot."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel use of inductive in-context crossover and mutation:**  \n  The paper introduces an interesting concept of using VLMs to perform crossover and mutation entirely in-context without predefined operators. This is a creative and promising perspective that differentiates the approach from traditional evolutionary robotics.\n\n- **Clear demonstration of tool-design capability:**  \n  The proposed framework is able to zero-shot generate both tool geometry and corresponding action strategies, showcasing VLM-based physical reasoning beyond conventional policy learning.\n\n- **Benchmark contribution:**  \n  ROBOTOOLBENCH could serve as a useful benchmark for future research on tool co-design and automated physical reasoning."}, "weaknesses": {"value": "- **Citation formatting reduces readability.**  \n  The manuscript embeds citations directly in narrative form, which disrupts flow and clarity. Using standard citation commands (e.g., `\\citep{}`) would improve readability and maintain academic consistency.\n\n- **Missing key related work: *RobotSmith*.**  \n  The paper does not reference *RobotSmith: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skills*, one of the closest prior works on tool co-design. Including this would strengthen the literature coverage and contextualize the contribution.\n\n- **Overstated novelty claims.**  \n  The claim that “VLMGINEER leverages the surprising physical creativity of VLMs...” suggests first-mover novelty. However, works such as *Evolution 6.0: Evolving Robotic Capabilities Through Generative Design* have similarly explored creative, generative physical design via LLMs. The authors should clarify what is genuinely new here and frame contributions accordingly.\n\n- **Limited evaluation on long-horizon tool-use tasks.**  \n  ROBOTOOLBENCH primarily evaluates single-stage manipulation tasks. Including long-horizon, multi-step tool-use scenarios with meaningful constraints would better demonstrate reasoning depth and practical utility.\n\n- **Ambiguity in Table 1 metrics.**  \n  It is unclear whether reported values represent normalized reward, success rate, or another metric. Clearly defining the evaluation metric and explaining its interpretation would improve transparency.\n\n- **Lack of manufacturability considerations.**  \n  Although real-world results are included, manufacturability constraints are not integrated during tool generation. Some designs (e.g., for *ScoreGoal* and *BringCube*) appear unnecessarily complex compared to human-designed solutions. Incorporating manufacturability into the optimization process would improve real-world relevance.\n\n- **Baseline design comparison could be expanded.**  \n  Current baselines rely primarily on human-prompted tool designs, which may introduce expertise-dependent bias and variance. Including recent LLM-based baselines—such as *RobotSmith* or simple LLM-generated tool-and-action plans without evolutionary refinement—would provide a stronger and clearer comparison, better contextualizing the advantages of the proposed method."}, "questions": {"value": "- **Quantitative benefit of joint tool–action sampling.**  \n  Have you evaluated whether jointly sampling tool geometry and actions in a single VLM pass outperforms a sequential approach (i.e., generating tools first, then actions with the VLM)? Providing such a comparison would help validate this core design decision.\n\n- **Justification for action representation.**  \n  Given that VLMs often struggle with spatial grounding and precise low-level control, what motivated the choice of directly predicting waypoint sequences instead of using motion primitives or parameterized action abstractions? Please clarify this design choice or include ablation experiments.\n\n- **Separating tool-quality vs. action-quality effects.**  \n  Since execution relies on VLM-generated waypoints, how do you distinguish failure cases caused by suboptimal tool design from those stemming from imperfect action generation? Disentangling these factors is important for interpreting the contribution of the evolutionary pipeline.\n\n- **Clarity in Figure 6.**  \n  The meaning of the light blue and light orange shaded regions is not obvious from the figure or its caption. Please provide a clearer explanation of what these regions represent.\n\n- **Evolutionary search vs. simpler iterative refinement.**  \n  Have you compared your evolutionary algorithm against a simpler iterative prompting baseline (e.g., repeatedly asking the VLM to refine the tool design without population-based evolution)? Such a comparison would help isolate the specific advantages of evolutionary search."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "33PV5sMd5q", "forum": "nESyz4PvJL", "replyto": "nESyz4PvJL", "signatures": ["ICLR.cc/2026/Conference/Submission21394/Reviewer_gsFY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21394/Reviewer_gsFY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859870779, "cdate": 1761859870779, "tmdate": 1762941741895, "mdate": 1762941741895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VLMGINEER, a system that uses a VLM to invent new tools for robots. It uses an \"evolutionary search\" process where the VLM generates, tests, and improves tool designs and the robot actions needed to use them. The main idea is to create tools that make the robot's task feasible or easier. The paper also introduces ROBOTOOLBENCH, a new 12-task simulation benchmark for this problem. Results show this method performs better than a standard robot gripper and tool designs specified by human experts. The authors also successfully transferred three of the simulated designs to a real robot."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper studies the challenging problem of automatic tool-action co-design. This is a significant and creative step beyond typical VLM-based planning, which generally assumes a set of pre-existing tools. This task requires a high degree of physical common-sense reasoning, and the paper demonstrates that VLMs can provide useful priors for this.\n\n- The proposed method is well-explained. It effectively uses the VLM's semantic and physical knowledge to guide the search, successfully automating the process that would typically require significant hand-engineering.\n\n- The ROBOTOOLBENCH test suite is a valuable contribution. Its 12 tasks are well-design and could be useful for future research."}, "weaknesses": {"value": "- The framework's reliance on thousands of simulation-based evaluations for its evolutionary loop (e.g., 8000 samples for the ablation ) makes the full design process impractical for direct real-world deployment. The paper demonstrates sim-to-real transfer on only 3 of the 12 tasks. More critically, these transfers rely on open-loop execution of discrete waypoints, which is known to be brittle and highly sensitive to any real-world physics or calibration error. This remains a major gap between the simulated success and a real-world \"robotic toolsmith.\"\n- The set of baselines is a significant weakness. The primary comparisons are against \"Human Prompts\" and existing \"RLBench Tools\". While outperforming the \"Human Prompts\" baseline is interesting, it mainly proves that evolutionary search is superior to a single-shot (human + VLM) guess. The related work (Section 2) extensively discusses prior computational co-design methods, including those using RL , differentiable simulation , and model-based optimization. Liu et al. (2023) is even cited as inspiration for a task. However, none of these state-of-the-art computational methods are included as quantitative baselines. Without them, it is impossible to situate VLMGINEER's performance relative to the actual prior art in computational tool design.\n- The \"Human Prompts\" baseline, which is central to the paper's claim of outperforming human specifications, is not a rigorous study. The appendix (A.1.1) reveals this was a \"case study\" with only three participants: one \"LLM expert,\" one \"robotics expert,\" and one \"layperson\". This sample size is far too small to draw any generalizable or statistically meaningful conclusions."}, "questions": {"value": "- The sim-to-real transfer was open-loop. How robust are these policies to minor perturbations in the real-world setup (e.g., a 1cm shift in object position)?\n- The action representation is an $N \\times 7$ array of waypoints. How is $N$ (the number of waypoints) determined? Is it a fixed hyperparameter per task, or does the VLM decide on the number of waypoints? This seems like a critical parameter that would heavily influence the performance.\n- For example, how often does the VLM produce an invalid URDF, a non-manipulable tool, or a design that violates a constraint? How does the pipeline handle such errors?\n- Number of trials for each human subject, what information is presented, what training the participants received, and how the participants were recruited?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6Zh2mZTvbS", "forum": "nESyz4PvJL", "replyto": "nESyz4PvJL", "signatures": ["ICLR.cc/2026/Conference/Submission21394/Reviewer_UWYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21394/Reviewer_UWYg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981639471, "cdate": 1761981639471, "tmdate": 1762941741608, "mdate": 1762941741608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a fully automatic framework, VLMgineer, that designs both tools and corresponding actions from scratch by drawing on the generative capabilities of Vision–Language Models (VLMs) together with evolutionary search. The system jointly discovers tool geometries and manipulation strategies to solve tool-use tasks, and it is evaluated on a custom benchmark (RoboToolBench) of everyday manipulation scenarios where creative tool design is considered necessary. While the idea of co-optimizing tool design and use is conceptually interesting, the practical benefits and generalizability of the approach remain unclear. The experimental setup lacks strong baselines, and the model relies too heavily on task-specific and highly detailed inputs, which limits its applicability and generalization to broader scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Using evolutionary strategies to generate tools, rather than a pure VLM solution, is a strength that introduces an iterative refinement process, helping to generate diverse and more functional designs.   \n\n- RoboToolBench provides a set of distinct environments tailored for evaluating the performance of tool-use strategies.  \n\n- The method yields more effective designs and improved performance for the given task compared to human-prompted designs."}, "weaknesses": {"value": "- The tools are designed as permanent end-effector attachments (or new grippers), rather than as distinct, portable tools that would be grasped by the robot. This design choice fundamentally limits the system’s generalizability and applicability to real-world scenarios where a robot must use a variety of pre-existing, non-integrated tools.  \n\n- The overall evaluation lacks sufficient diversity and strength in its baselines. The comparison to the default two-fingered Franka Panda gripper is weak, as these tasks are explicitly designed to be unsolvable without custom tools. While the work includes a comparison to RLBench tools on four tasks, it omits comparisons against recent, more sophisticated computational co-design or tool-learning baselines (e.g., one of the studies reported in the Related Work section), which would provide a more robust validation of VLMgineer's effectiveness. The comparison against a small-sample group of three human designers is also subjective.  \n\n- The VLM requires low-level and environment-specific information, including raw environment code, task description, and explicit system instructions as input (Figure 2). This reliance on low-level, internal data severely limits the method's generalizability and reusability, as a truly general-purpose system should ideally operate with high-level, natural inputs like an environment image and a simple task goal.  \n\n- The authors claim to have empirically observed their action sampling strategy outperforms other forms of feedback (line 212); however, no quantitative analysis or representative baseline is provided in the paper to substantiate this critical statement.  \n\n- The authors claim that optimizing both the tool’s design and its manipulation strategy is a more practical solution than using already existing tools in the environment for a general-purpose robot (line 111). However, this assertion is questionable regarding real-world practicality, scalability, and generalization. Continuously generating and 3D printing new tools for each task is costly and time-consuming (line 437), limiting its applicability. An approach focused on learning when to use existing tools and when to generate/form new ones  would contribute more significantly to generalizable robotics and open-world applications.  \n\n- For clarity and reproducibility, the paper should include an example of the generated action sequences (the 7 waypoints) and a discussion of their corresponding usage and limitations.  \n\n- It would be better to present the algorithm in the main paper rather than in the Appendix."}, "questions": {"value": "In addition to the issues presented in the weakness section: \n\n- The paper compares VLMgineer’s co-designed tool and action against the original, pre-existing tools from RLBench. Since the RLBench tools have established, fixed designs and VLMGINEER can co-optimize both tool and action, is this a fair comparison? Specifically, the robot cannot improve the pre-established tools in the RLBench environment setup, which limits the optimization space compared to VLMGINEER's full co-design approach. Could the authors clarify the intent behind this specific comparison?  \n\n- The VLMgineer framework takes \"System Instructions\" as input, alongside the raw environment code and task description. For clarity and to better assess the generalizability of the VLM prompt, what exactly do these system instructions entail? Given that the model already leverages a Vision-Language Model, wouldn't it be more generalizable if the inputs were ideally limited to high-level, natural inputs like the environment image and a simple task goal to maximize generalizability and reusability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MTAbjAgkEI", "forum": "nESyz4PvJL", "replyto": "nESyz4PvJL", "signatures": ["ICLR.cc/2026/Conference/Submission21394/Reviewer_Cg95"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21394/Reviewer_Cg95"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986996268, "cdate": 1761986996268, "tmdate": 1762941741343, "mdate": 1762941741343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}