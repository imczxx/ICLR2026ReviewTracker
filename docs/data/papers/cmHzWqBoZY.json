{"id": "cmHzWqBoZY", "number": 15856, "cdate": 1758256162970, "mdate": 1763712660616, "content": {"title": "Entropy Meets Importance: A Unified Head Importance–Entropy Score for Stable and Efficient Transformer Pruning", "abstract": "Transformer-based models have achieved remarkable performance in NLP tasks. However, their structural characteristics—multiple layers and attention heads—introduce efficiency challenges in inference and deployment. To address these challenges, various pruning methods have recently been proposed. Notably, gradient-based methods using Head Importance Scores (HIS) have gained traction for interpretability, efficiency, and ability to identify redundant heads. However, HIS alone has limitations as it captures only the gradient-driven contribution, overlooking the diversity of attention patterns. To overcome these limitations, we introduce a novel pruning criterion, **HIES (Head Importance-Entropy Score)**, which integrates head importance scores with attention entropy, providing complementary evidence on per-head contribution. Empirically, HIES‐based pruning yields up to 15.2\\% improvement in model quality and $2.04\\times$ improvement in stability over HIS‐only methods, enabling substantial model compression without sacrificing either accuracy or stability. Code will be released upon publication.", "tldr": "", "keywords": ["Transformer Architecture", "Attention Head Pruning", "Model Stability", "Attention Entropy"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77c597d3ac85fdf6b61cfb7ca9e2147a40b3a85c.pdf", "supplementary_material": "/attachment/8698fba4fe49deda53d5c07e1a73e6e7a973df2e.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Head Importance–Entropy Score (HIES), a criterion for pruning attention heads. HIES combines the traditional Head Importance Score (HIS) with attention entropy. Compared with HIS-only pruning, HIES maintains higher accuracy and stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper effectively leverages prior findings that attention entropy correlates with model stability, extending this concept to the context of head pruning. By combining entropy with gradient-based head importance, the authors provide a simple but well-grounded way to prune attention heads without sacrificing robustness.\n2. The theoretical section motivates HIES as a principled balance between task relevance and stability. Head Importance (HIS) measures how much each head contributes to minimizing loss, while Attention Entropy (AE) captures the stability and generalization of attention patterns. The authors show that the two quantities are largely uncorrelated, and combining them provides a more reliable criterion for pruning than either alone.\n3. The empirical results demonstrate gains over HIS-based pruning."}, "weaknesses": {"value": "1. The paper does not report standard deviations or variance across random seeds, leaving it unclear how robust the reported results are. Given that pruning outcomes can vary substantially across runs, reporting variance would help assess reliability.\n2. While the paper includes comparisons with several pruning baselines, it does not evaluate against existing head-pruning methods such as https://aclanthology.org/P19-1580/ and https://aclanthology.org/2021.tacl-1.86/, etc. Comparison with these approaches would provide a clearer picture of HIES's relative contribution."}, "questions": {"value": "Is $\\alpha$ chosen task-specifically? How many runs or random seeds are used per task, and are the reported numbers averages or single-run results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hwcBIGU025", "forum": "cmHzWqBoZY", "replyto": "cmHzWqBoZY", "signatures": ["ICLR.cc/2026/Conference/Submission15856/Reviewer_FG9H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15856/Reviewer_FG9H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867707106, "cdate": 1761867707106, "tmdate": 1762926078000, "mdate": 1762926078000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified criterion for pruning Transformer attention heads, called the Head Importance–Entropy Score. The authors first point out the limitation of existing gradient-based methods, such as the Head Importance Score (HIS), these approaches focus solely on a head’s contribution to the loss, while ignoring the diversity or concentration of its attention patterns. As a result, when the pruning sparsity becomes high, the model’s performance drops sharply. The authors observe that HIS may mistakenly prune heads with highly focused, low-entropy attention, since such heads often receive lower HIS scores, while retaining diffuse, high-entropy heads. This leads to the loss of critical information on key tokens, thereby reducing the model’s performance and stability. To address this issue, HIES combines HIS (representing loss-based importance) with Attention Entropy (AE). This combination is designed to retain attention heads that are both important to the loss and highly focused in their attention patterns. Extensive experiments across various models and tasks demonstrate that HIES significantly outperforms HIS, LLM-Pruner, and SliceGPT in both accuracy and stability, especially under high pruning ratios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The motivation of the paper is very strong. The authors provide an in-depth diagnosis of the limitations of existing methods, particularly HIS. The analysis presented in Figure 1 is highly compelling, showing that HIS fails to distinguish between heads that focus on key information and those that exhibit diffuse attention over non-informative tokens. The proposed method is not arbitrary. It is grounded in a clear and principled assumption that a good attention head should exhibit both high loss contribution and high attention concentration. Furthermore, the authors provide a detailed theoretical analysis to justify why HIS and AE are complementary in capturing these two aspects of head importance. The experimental evaluation is thorough and comprehensive. In terms of diversity, it covers NLP, vision, and multimodal models. For baselines, it includes strong state-of-the-art methods such as LLM-Pruner and SliceGPT. Across all these settings, HIES consistently and significantly outperforms all baselines under high pruning ratios, particularly in terms of accuracy and stability."}, "weaknesses": {"value": "1. Appendix D.5’s sensitivity analysis shows that the choice of the hyperparameter α is crucial for performance, for example, the worst α performs very poorly on QNLI, MNLI, and RTE. Should I therefore understand that this increases the method’s deployment overhead, since users may need to carefully tune α for each new task or model?\n\n2. The paper does not explicitly discuss the additional computational overhead of HIES compared to HIS. Based on my understanding, computing Attention Entropy requires collecting and processing attention matrices during forward propagation, which could introduce non-negligible overhead when the calibration dataset is large.\n\n3. HIES combines gradient-based HIS with activation-pattern-based AE. Have the authors considered other non-gradient importance metrics, such as using only the activation L2-norm, and combining it with AE? From the paper’s formulation, AE appears to be a rather general and complementary measure, which could potentially be integrated with other types of importance signals to further evaluate effectiveness."}, "questions": {"value": "1. Appendix D.5 shows that the optimal value of α varies greatly across different tasks. Does this imply that applying the proposed method requires an expensive tuning process for each new task or model? Could the authors develop a unified framework to standardize or automatically adapt the hyperparameter settings?\n\n2. Compared with the baseline HIS, how much additional computation time or resources does calculating HIES, particularly the AE component, require? Could the authors provide a detailed ablation study and a visualization to illustrate this overhead more clearly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xE2Tgpq9zH", "forum": "cmHzWqBoZY", "replyto": "cmHzWqBoZY", "signatures": ["ICLR.cc/2026/Conference/Submission15856/Reviewer_CUbc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15856/Reviewer_CUbc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869029341, "cdate": 1761869029341, "tmdate": 1762926077502, "mdate": 1762926077502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HIES (Head importance-entropy score), a pruning criterion combining the gradient-based head importance score with attention entropy. The method aims to improve pruning stability by accouting for both the contribution and distributional diversity of each attention head. Experimental results across BERT, LLaMA, ViT and LLaVA show notable gains in both accuracy (about +15%) and stability (about x2) under high pruning ratio."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Theoretical formulation includes risk decomposition, orthogonality proof, and stability analysis.\n\n(2) Integrates entropy to differentiate between concentrated and diffuse heads.\n\n(3) Significant stability and performance gains at aggressive pruning ratios.\n\n(4) Can be easily integrated into existing head pruning frameworks."}, "weaknesses": {"value": "(1) Experiments rely on BERT and GLUE tasks without comparison to modern LLM distillation methods. While existing benchmarks remain meaningful, deeper experiments on modern QA benchmarks are needed to verify whether the same stability and accuracy improvement hold across newer large-scale reasoning (for example) tasks.\n\n(2) Lack of training cost or runtime analysis, despite \"Efficiency\" being a claimed contribution. While the paper emphasizes computational efficiency, no empirical analysis (e.g., wall-clock time, FLOPs, or memory cost) is provided. Including such runtime cost analysis would significantly strengthen the claim of efficiency and help validate the practical impact of the proposed pruning criterion.\n\n(3) In certain tasks (e.g., SST-2 with 30% pruning), random pruning achieves comparable results to the proposed HIES method. In my humble opinion, it could be helpful to the readers by explaining why this occurs."}, "questions": {"value": "Please refer to the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2TCkSK5Rwj", "forum": "cmHzWqBoZY", "replyto": "cmHzWqBoZY", "signatures": ["ICLR.cc/2026/Conference/Submission15856/Reviewer_x54u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15856/Reviewer_x54u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980967870, "cdate": 1761980967870, "tmdate": 1762926076919, "mdate": 1762926076919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HIES (Head Importance–Entropy Score), a unified criterion for Transformer attention head pruning. HIES integrates gradient-based head importance (HIS) with attention entropy (AE) to achieve stable and efficient compression. The paper starts with the motivation that HIS overlooked the diversity of attention patterns. To complement this, HIES also captures the dispersion of each head’s contribution. Experimental results across diverse models and benchmarks demonstrate that HIES is an efficient and generalizable method to prune large Transformer models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Combining both gradient-based importance and attention distribution is an intuitive approach. HIES preserves the practical advantages of HIS, computing importance without extra training, while achieving advanced performance.\n* Experiments cover multiple Transformer architectures (BERT, ViT, Llama-2, Llava) and show consistent gains across diverse datasets and tasks."}, "weaknesses": {"value": "(Please note that I could not check all the details of the theoretical derivations. I will defer to other reviewers for a more thorough evaluation of those parts.)\n\n* Regarding Lemma 3, the assumptions appear quite strong and may not always hold in practice. Since this lemma is central to the theoretical contribution, an empirical examination would strengthen the paper. In addition, it is unclear whether the u and v used in Lemma 3 correspond to those used in Appendix D.1, orthogonality analysis.\n* Hyperparameter $\\alpha$ (i.e., mixing ratio) tends to show the best performance near the extremes (0.0 or 1.0) (Figure 11). Moreover, the “Sharp drop” phenomenon seems to persist even for low $\\alpha$ values that emphasize AE. This behavior raises concerns about the claimed robustness and stability.\n* The paper claims robustness to input distribution shifts, but the presented experiments do not seem to demonstrate this aspect sufficiently.\n* Although the SliceGPT paper includes experiments on decoder-only models, the results in Figure 4 lack comparisons with other pruning methods. Including such baselines would make the evaluation more convincing."}, "questions": {"value": "* The Introduction suggests that HIES can address layer-specific adaptation. Could the authors discuss how the proposed criterion achieves this adaptivity?\n* The term “Inference-time” stability is somewhat ambiguous. While I understand stability with respect to pruning ratios or parameter sensitivity, the phrase “inference-time” might imply runtime robustness, which seems to be a different concept.\n* The paper argues that HIS magnitude and attention patterns are not necessarily correlated, which is reasonable. However, it would be helpful to provide further intuition on why this discrepancy occurs. Is it a task-specific phenomenon or an inherent property of gradient-based importance?\n* (minor) The phrase “Eq. equation” appears multiple times throughout the paper and should be corrected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cubKXSRPnV", "forum": "cmHzWqBoZY", "replyto": "cmHzWqBoZY", "signatures": ["ICLR.cc/2026/Conference/Submission15856/Reviewer_pQ3d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15856/Reviewer_pQ3d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989686762, "cdate": 1761989686762, "tmdate": 1762926076327, "mdate": 1762926076327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all reviewers for their constructive feedback. Our work is a novel attention head pruning approach, HIES, which combines gradient-based head importance with attention entropy to achieve higher accuracy, greater stability, and substantial model compression compared to baseline methods.  \n\n &nbsp;\n&nbsp;\n    \n\nOverall, our work has been recognized for its \n- **Strong motivation** (CUbc)\n- **Intuitive and novel proposed method** (pQ3d, x54u, CUbc, FG9H)\n- **Easily integrable with existing head pruning frameworks** (x54u)\n- **Theoretical formulation including risk decomposition, orthogonality proof, and stability analysis** (x54u, CUbc, FG9H)\n- **Consistent performance across extensive experiments involving diverse architectures and tasks** (pQ3d, x54u, CUbc, FG9H)\n\n&nbsp;\n\nWe provide detailed responses to each reviewer's questions and comments in their respective Official Comment sections."}}, "id": "rUeqA4DFWI", "forum": "cmHzWqBoZY", "replyto": "cmHzWqBoZY", "signatures": ["ICLR.cc/2026/Conference/Submission15856/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15856/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15856/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763712908361, "cdate": 1763712908361, "tmdate": 1763712908361, "mdate": 1763712908361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}