{"id": "cmHkNCtvb5", "number": 8560, "cdate": 1758090960249, "mdate": 1763111612849, "content": {"title": "Poivre: Self-Refining Visual Pointing with Reinforcement Learning", "abstract": "Visual pointing, which aims to localize a target by predicting its coordinates on an image, has emerged as an important problem in the realm of vision–language models (VLMs). Despite its broad applicability, recent benchmarks show that current VLMs still fall far behind human performance on this task. A key limitation is that VLMs are typically required to complete the pointing task in a single step, akin to asking humans to point at an object without seeing their own fingers. To address this issue, we propose a simple yet effective self-refining procedure: *Point, Visualize, then Refine* (Poivre). This procedure enables a VLM to first mark its estimated point, then iteratively refine the coordinates if necessary. Inspired by advances of reasoning models in the natural language domain, we employ reinforcement learning (RL) to incentivize this self-refining ability. For the RL training, we design a neat process reward that is not only empirically effective but also grounded in appealing theoretical properties. Our trained model, *Poivre-7B*, sets a new state of the art on Point-Bench, outperforming both proprietary models such as Gemini-2.5-Pro and strong open-source models such as Molmo-72B by over 3%. To support future research, we release our training and inference code, dataset, and the Poivre-7B checkpoint.", "tldr": "", "keywords": ["Visual pointing", "Reinforcement learning", "Vision-language models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abbe140914f647b4b2898ce11c2dd9e46063c53c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Poivre (Point, Visualize, then Refine), a self-refining framework for visual pointing tasks using reinforcement learning (RL). Instead of predicting target coordinates in a single step, Poivre enables a Vision-Language Model (VLM) to iteratively visualize its prediction on the image and refine the coordinates through multiple rounds. The key novelty lies in the PBRS-inspired process reward, which encourages improvement across refinement steps rather than focusing solely on the final outcomes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper identifies an important gap in current VLMs: the lack of self-refinement capability. The proposed “Point → Visualize → Refine” loop is simple yet conceptually appealing, like human correction behavior in pointing tasks. \n2.\tThe pipeline and training process (rollout sampling, reward computation) are clearly described, and the paper includes implementation details that make the work easy to replicate. \n3.\tThe adaptation of PBRS to the visual refinement setting is neat and theoretically grounded. It encourages incremental improvement across iterations, which aligns naturally with the proposed iterative reasoning loop."}, "weaknesses": {"value": "1.\tWhile the results are promising, the technical contribution remains incremental. The core ideas, visualizing model predictions and refining them iteratively, are intuitive, and the PBRS-based reward formulation is largely a straightforward adaptation of an established concept (Ng et al., 1999). The work demonstrates solid empirical gains but offers limited novelty.\n\n2.\tFrom Table 3, the improvements across multiple refinement iterations are marginal , which are all within 0.5%. This suggests that after training, the model’s first prediction during testing is already very accurate, making further refinement less impactful. This raises the question of whether the iterative training truly contributes to better refinement behavior, or if the model simply learns to predict correctly in the first step. It would be helpful for the authors to analyze the accuracy gap between the initial prediction and the first refinement during training, to clarify whether the model indeed benefits from iterative supervision.\n\n3.\tA similar multi-round refinement mechanism could be implemented in a general supervised setting, where the model iteratively predicts and updates based on the distance between the predicted point and the ground truth. Would such a setup achieve comparable effects without using GRPO? In other words, what is the fundamental difference between performing iterative supervised updates and adopting GRPO in this framework? The paper would benefit from clarifying why reinforcement learning is necessary here, and what unique advantage GRPO provides over conventional multi-step supervised training.\n\n4.\tThe paper lacks detailed visualization or failure case analysis. It would be useful to show when refinement helps or hurts, and whether the model can meaningfully interpret the marker in complex scenes."}, "questions": {"value": "1.\tHave you tried any ablation where the model receives coordinate input directly (without visualization)? \n2.\tDoes the model ever over-correct (move away from the correct target) in refinement steps? \n3.\tCould a supervised multi-step training baseline achieve similar improvements without RL? \n4.\tHow does the model behave when the initial pointing is already accurate, does refinement still occur?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s7szEA8DC8", "forum": "cmHkNCtvb5", "replyto": "cmHkNCtvb5", "signatures": ["ICLR.cc/2026/Conference/Submission8560/Reviewer_cVvG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8560/Reviewer_cVvG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893523192, "cdate": 1761893523192, "tmdate": 1762920416239, "mdate": 1762920416239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present poivre, a VLM model capable of pointing to images given natural language queries. The model is trained with reinforcement learning, in order to encourage it to refine its predictions. Rather than assigning rewards only for the final state, the authors propose summing the rewards for how much the model improves its prediction at every step; they show this is equivalent to a formulation that emphasizes the first and last prediction the most. Through experiments, they show state-of-the-art results on pointing benchmarks, show that the model's predictions improve with an additional refinement step, and show the model's ability to extrapolate beyond its training regime to a longer horizon."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* the paper is well-motivated and clearly written. the figures are helpful towards understanding the method and showcasing qualitative results. \n* the composition of ideas seems original, however I am unable to assess this with high confidence as I am not an expert on the literature of visual pointing methods.\n* the comparison to single turn RL is interesting! Even with one inference call, the multi-turn objective yields better performance."}, "weaknesses": {"value": "* T is set to 2 during training, which somewhat undercuts the message that the model is refining its responses. It would be very interesting to see how this dynamic plays out over longer sequences, of T=3 and greater.\n* As an extension to the above note, it is difficult to access the utility of the PBRS term with just T=2.\n* The \"exploration\" experiments only try T=3 and cite this as evidence of exploration. Please expand the analysis to include longer horizons, T=4, 5, etc, so we can draw more conclusive evidence of extrapolation.\n* It would be interesting to compare to additional self-baselines that rely less on visual reasoning and more on text reasoning; for example, just displaying the input image in the original prompt, and asking the model to make text-based prediction outputs, instead of overlaying the prediction onto the original image, as shown currently in Figure 2. This would help tease apart whether **visual** self-feedback is crucial.\n* There are sparse citations to several papers in the field related to sampling-based approaches, such as PIVOT (Nasiriany et al.), MOKA (Liu et al.), and Set-of-Mark Prompting (Yang et al.)"}, "questions": {"value": "Please see the points raised in the \"weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qCKbPvtdt6", "forum": "cmHkNCtvb5", "replyto": "cmHkNCtvb5", "signatures": ["ICLR.cc/2026/Conference/Submission8560/Reviewer_VPKH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8560/Reviewer_VPKH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987364903, "cdate": 1761987364903, "tmdate": 1762920415820, "mdate": 1762920415820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Poivre, a reinforcement learning (RL) framework that enables vision-language models (VLMs) to iteratively refine their pointing predictions through a Point → Visualize → Refine (Poivre) procedure. Rather than predicting a pointing coordinate in one shot, the model visually marks its initial prediction on the image and refines it over multiple rounds. To incentivize this self-refinement, the authors design a potential-based reward shaping (PBRS) mechanism — a “process reward” that encourages progressive improvement across refinement steps, rather than optimizing only for the final outcome.\nThe resulting model, Poivre-7B, achieves state-of-the-art performance on Point-Bench (67.5% success rate, +2.7% over the best prior open model Molmo-72B) and generalizes to robotics benchmarks (where2place) without domain-specific fine-tuning. The paper includes ablations comparing outcome vs. process rewards, extrapolation to unseen refinement rounds, and a case study illustrating iterative correction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear conceptual motivation: Identifies the one-shot limitation in visual pointing and proposes a self-refining paradigm inspired by human feedback and test-time scaling.\n\n- Principled reward design: The PBRS-inspired process reward is elegant, grounded in RL theory (potential shaping), and practically effective (+1.3% improvement).\n\n- Strong empirical results: New state-of-the-art on Point-Bench, with additional validation on robotics transfer and extrapolation beyond training rounds."}, "weaknesses": {"value": "- Limited conceptual novelty. The idea of self-refinement and iterative improvement is well explored in text reasoning (test-time scaling, visual CoT). The contribution lies mainly in applying this paradigm to visual pointing with straightforward reward shaping, rather than introducing fundamentally new algorithms or representations.\n- Modest absolute gains. The improvement (≈2–3%) over strong baselines like VisionReasoner-7B or Molmo-72B, while consistent, is relatively small given the added RL complexity and compute cost (~$2000 training run).\n- Reward design tuning unclear. The Gaussian-shaped outcome and PBRS coefficients (σ, γ) are selected heuristically; there’s no sensitivity or stability analysis.\n- No human evaluation or qualitative failure analysis. The case studies are anecdotal and do not quantify where refinement helps or fails (e.g., cluttered vs. simple scenes).\n- Dependence on GRPO. The method’s improvements might rely heavily on the GRPO setup from prior RL works (DeepSeek-R1, VisionReasoner), rather than intrinsic advantages of Poivre itself.\n- No runtime or inference cost discussion. Iterative refinement increases inference time linearly with the number of turns; this tradeoff is not analyzed quantitatively."}, "questions": {"value": "- How sensitive are results to the discount factor (γ) and σ in the process reward? Could these parameters significantly alter performance?\n\n- How many refinement rounds before diminishing returns or overfitting occur (beyond T=3)?\n\n- Can Poivre be extended to spatially dense tasks (e.g., segmentation, affordance maps), not just 2D coordinate regression?\n\n- Did the authors attempt to combine supervised fine-tuning with RLHF/DPO before GRPO?\n\n- What is the inference-time latency cost (in seconds per refinement step) compared to one-shot baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No notable ethical issues. The work relies on public datasets (Pixmo, Point-Bench, RoboPoint). Potential concerns include model misuse for surveillance or dataset licensing, but these are minimal and standard for vision-language RL research."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2fEhzi0VKL", "forum": "cmHkNCtvb5", "replyto": "cmHkNCtvb5", "signatures": ["ICLR.cc/2026/Conference/Submission8560/Reviewer_qSiU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8560/Reviewer_qSiU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028541633, "cdate": 1762028541633, "tmdate": 1762920414600, "mdate": 1762920414600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Visual pointing, which aims to localize a target by predicting its coordinates on an image, has emerged as an important problem in the realm of vision–language models (VLMs). Despite its broad applicability, recent benchmarks show that current VLMs still fall far behind human performance on this task. A key limitation is that VLMs are typically required to complete the pointing task in a single step, akin to asking humans to point at an object without seeing their own fingers. To address this issue, we propose a simple yet effective self-refining procedure: Point, Visualize, then Refine (Poivre). This procedure enables a VLM to first mark its estimated point, then iteratively refine the coordinates if necessary. Inspired by advances of reasoning models in the natural language domain, we employ reinforcement learning (RL) to incentivize this self-refining ability. For the RL training, we design a neat process reward that is not only empirically effective but also grounded in appealing theoretical properties. Our trained model, Poivre-7B, sets a new state of the art on Point-Bench, outperforming both proprietary models such as Gemini-2.5-Pro and strong open-source models such as Molmo-72B by over 3%. To support future research, we release our training and inference code, dataset, and the Poivre-7B checkpoint."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1) Extensive experiments show the effectiveness of the proposed method."}, "weaknesses": {"value": "1) The writing should be improved.\n2) The motivation should be further highlighted.\n3) The paper organization is poor.\n4) Figures in the paper should be enhanced."}, "questions": {"value": "1) The writing should be improved.\n2) The motivation should be further highlighted.\n3) The paper organization is poor.\n4) Figures in the paper should be enhanced."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qqDyLP2PiI", "forum": "cmHkNCtvb5", "replyto": "cmHkNCtvb5", "signatures": ["ICLR.cc/2026/Conference/Submission8560/Reviewer_VxU2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8560/Reviewer_VxU2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152673451, "cdate": 1762152673451, "tmdate": 1762920413072, "mdate": 1762920413072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}