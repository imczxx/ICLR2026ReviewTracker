{"id": "TIOFvhliLA", "number": 13740, "cdate": 1758221836000, "mdate": 1759897416349, "content": {"title": "Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison", "abstract": "Current preference learning methods discard the rich explanations humans naturally provide when comparing examples, collapsing detailed feedback into binary signals. We introduce \\textit{Feedback Descent}, a framework that widens this information bottleneck by leveraging textual feedback to enable directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits of text artifacts such as prompts, code, and JSON. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $200{,}000$ compounds across six protein targets.", "tldr": "Feedback Descent is an inference-time framework that improves text artifacts by iteratively editing based on textual feedback.", "keywords": ["Text optimization", "continual learning", "prompt oprimization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a6bb9b623bd8bd24462489430fb03b9ff372e5c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a very interesting idea of optimizing in the text space via explicit feedback instead optimizing in the latent space. They use pairwise preferences with text to provide a direction and show that even with imperfect correlation of the feedback and the gradients, the aggregated messages across failures guides the model towards improving the result. They evaluate the model on three domains and show in SVG design iterative feedback produces improvements above and beyond direct prompting from both scratch and rubric-aware initializations. On the molecule discovery task the proposed approach outperforms RL based approaches. Overall this is a very interesting area of research direction with good results as a start."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors identify an important area of research and apply it to three different domains where they show the performance of the proposed approach is competitive. Exploiting the text space to provide feedback is an interesting idea. It can be thought of as analogous to gradient descent when the direction of text feedback aligns with the direction of gradients. Even if the latest feedback is not useful as the authors show the accumulation of past feedback and why it failed helps the model to refine the output. \nThe first proposition shows that how the proposed approach is advantageous given it does not depend on the hidden dimension unlike the other methods. Both propositions mainly highlight the dimensionality as a key bottleneck. \n\nResults on the SVG designs look impressive with the proposed approach beating baselines in most cases. In the docking example the proposed approach augments the LLM with feedback on the key properties related to the task."}, "weaknesses": {"value": "As the authors mention in the limitations section, the model will rely on strong evaluators which can be challenging in certain domains for e.g. in designing antibodies. If there is a way to combine multiple feedbacks such as use an ensemble of feedback from text and latent space and then guide the model it might make the approach robust. And second strictly following either feedback might also be limiting and there is definitely scope to be creative on \"how to use the feedback\"."}, "questions": {"value": "In the equation for the proposition 1 when the algorithm calculate the \\delta(r) for each rationale, how does it determine the positively aligned past rationales? Is there another module to compare whether the model refined the feedback directly based on the current rationale? What defines a \"useless rationale\"? Can it be that the proposed direction is not completely off but would take longer towards the refinement and if so is it considered a bad rationale?\n\nFor the second proposition, it has been shown that DPO based approaches can extrapolate to spaces (and hence for e.g. propose new outputs) that have better fitness say e.g. in the protein domain. Can such feedback be used in conjunction with DPO to reach to the better designs faster?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XwHQUX2CNS", "forum": "TIOFvhliLA", "replyto": "TIOFvhliLA", "signatures": ["ICLR.cc/2026/Conference/Submission13740/Reviewer_ckan"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13740/Reviewer_ckan"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788104058, "cdate": 1761788104058, "tmdate": 1762924278510, "mdate": 1762924278510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Feedback Descent, a general framework for inference-time optimization of text-based artifacts. Instead of learning from scalar rewards or binary preferences, it leverages pairwise comparisons augmented with textual rationales. In each iteration, a candidate is generated using prior feedback, compared to the current best, and either accepted or rejected based on preference and rationale. This loop produces directional edits in semantic space, acting like approximate gradients. The method is tested on three tasks: SVG design, prompt optimization, and molecular discovery."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The method addresses a real limitation in preference learning by incorporating rich feedback rather than collapsing supervision into binary signals. It is model-agnostic and requires no parameter updates, making it widely applicable. Experiments span distinct domains and show  improvements over well-established baselines. The use of textual rationales to guide generation is intuitive and aligns with emerging trends in LLM usage. The SVG and molecular tasks show the advantages of iterative feedback, even over strong initial prompts. The theoretical section provides useful insight, framing textual feedback as noisy but directionally useful signals that improve sample efficiency in high-dimensional tasks."}, "weaknesses": {"value": "1. The framework heavily depends on high-quality evaluators that can provide meaningful and consistent textual rationales. If the feedback is noisy, vague, or inconsistent, the system may stagnate or regress. \n\n2. The update step only keeps one best candidate per iteration, which may limit diversity and exploration. \n\n3. The paper lacks ablation studies showing how performance changes when textual feedback is removed or corrupted. \n\n4. although the method is said to be domain-general, all tasks still rely on well-defined evaluators or rubrics. In domains lacking reliable feedback sources, its applicability remains unclear."}, "questions": {"value": "1. How does the method perform when the textual feedback is noisy, irrelevant, or inconsistent? Would it still improve over time, or does it degrade quickly? \n\n2. Why did the authors choose to reset the feedback history after each accepted update? Could cumulative memory of feedback help guide long-term edits better? \n\n3. Have the authors tested multi-candidate selection instead of greedy updates? \n\n4. In creative tasks without clear evaluation rubrics, can this approach still function reliably, or does it require strong evaluators to be useful at all?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rmGXA24k1a", "forum": "TIOFvhliLA", "replyto": "TIOFvhliLA", "signatures": ["ICLR.cc/2026/Conference/Submission13740/Reviewer_jREt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13740/Reviewer_jREt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921098903, "cdate": 1761921098903, "tmdate": 1762924277759, "mdate": 1762924277759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Feedback Descent**, an inference-time optimization framework that iteratively improves text-based artifacts (SVG code, prompts, molecules) through pairwise comparisons augmented with textual rationales. Unlike standard preference learning methods that compress judgments into scalars or binary signals, Feedback Descent treats textual feedback as directional information analogous to gradients in continuous optimization.\n\nThe authors validate their approach across three diverse domains: SVG aesthetic optimization, prompt engineering on IFBench, and molecule discovery on DOCKSTRING, demonstrating competitive or superior performance compared to state-of-the-art baselines including GEPA, GRPO, REINVENT, and graph-based molecular optimizers.\n\nThe authors are refreshingly honest that textual feedback is not a literal gradient, instead framing it as a \"heuristic directional cue\" that provides higher-bandwidth supervision than scalar rewards or binary preferences. In this sense, Feedback Descent extends the TextGrad philosophy by emphasizing pairwise comparisons with accumulated feedback rather than self-reflection alone."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well-written with clear motivation and contributions.\n\n2. Algorithm 1 is simple, self-contained, and reproducible.\n\n3. Inference-time optimization with no weight updates is valuable for practitioners. (Minor note: \"SVG\" should be expanded to Scalable Vector Graphics at first mention for accessibility.)\n\n4. The framework is validated on three qualitatively different tasks:\n\n    - Visual design (SVG),\n\n    - Natural language (prompts), and\n\n    - Chemistry (molecules).\n\n5. The molecule discovery results are particularly strong.\n\n6. It has fair comparisons with matched budgets using comprehensive baselines, including recent methods (GEPA, GRPO)."}, "weaknesses": {"value": "1. The authors claim that \"a paragraph of feedback contains more Shannon information than a single scalar or bit,\" which is trivially true in terms of raw bits. However, information content does not equal actionable directional information. Several critical questions remain unaddressed.\n\n2. **Quantitative validation missing:**\nCan the authors quantify whether textual feedback actually provides gradient-aligned directions? For example, measure the correlation between feedback-suggested changes and actual objective improvement.\n\n3. **Ablations needed:**\nIf feedback is noisy, vague, or contradictory across iterations, scalar signals + random search might be more reliable. The paper needs ablations showing comparison to \"random descent\" (same loop structure but random mutations instead of feedback-guided ones). Maybe adding analysis of when feedback helps vs. when it misleads.\n\n4. **Theory–practice gap:**\nThe theoretical contribution is undermined by missing formalism. The paper assumes that if feedback provides gradient-like directions, then fast convergence follows — but there’s no natural notion of $z_t + \\eta v_t$ for SMILES strings or SVG code. The LLM generates a new $x_{t+1}$ by interpreting textual feedback — this is not vector addition. Also, the mapping $\\Phi: S \\to Z$ (text to latent space) is never defined or validated."}, "questions": {"value": "1. Can you provide empirical evidence that feedback directions are aligned with improvement directions?\n\n2. Can you provide error bars (mean $\\pm$ std) over multiple random seeds for all experiments?\n\n3. Can you run TextGrad on at least one shared task (e.g., molecules) for direct comparison?\n\n4. What is the computational cost (wall-clock time, API calls, dollars) compared to baselines? Are the gains worth the added evaluation expense?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QOsFSLR0Su", "forum": "TIOFvhliLA", "replyto": "TIOFvhliLA", "signatures": ["ICLR.cc/2026/Conference/Submission13740/Reviewer_GXdf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13740/Reviewer_GXdf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980519828, "cdate": 1761980519828, "tmdate": 1762924277322, "mdate": 1762924277322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a simple LLM-based evolutionary algorithm for optimization, where each comparison within the population is accompanied by a textual description of why it was better. They evaluate on a diverse set of tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper correctly identifies that text information contains a lot more information than binary preferences\n- Good choice of experiments and baselines, method seems effective\n- Writing generally clear and figures look nice!"}, "weaknesses": {"value": "- **Scientific question incoherent / inconsistent**: in §1-§4, the paper sets up the scientific question of the paper as \"does textual feedback provide a stronger learning signal than binary feedback\". Then, §5 seems to answer the question \"is direct feedback optimization an effective way to accomplish these tasks\". _This is not the original question!!_ In my opinion, the \"missing\" baseline in each case is the same LLM-based optimizer with the same binary feedback, but _without_ the explanatory text. As far as I can tell, the actual baseline used is a _non-iterative_ (single shot) optimizer, which is quite distinct.\n- **\"Mathiness\" in §3**: §3 felt like it should be entirely omitted and replaced with the sentence \"prior work has shown that first-order optimizers are able to explore high-dimensional spaces more effectively than zeroth-order optimizers [citations]\". I think introducing a bunch of theorems which never actually get used just makes the paper harder to read without really adding anything.\n\n  The actual theorems themselves seem vacuous: the assumptions for proposition 1 are clearly constructed in a way that omits dependence on the dimensionality (through the assumption of $\\mu$-PL), which guarantees that the desired conclusion will hold. In practical problems I imagine the value of $\\mu$ in the assumption would depend on d.\n- **Relationship to evolutionary algorithms**: the method proposed, which keeps a best candidate, proposes a variation, and keeps the variation if it improves, is an instance of an evolutionary algorithm (aka genetic algorithm). This is a well-established optimization approach so there is nothing wrong with proposing an EA, but I think this should be recognized and acknowledged in the paper.\n- **Textual feedback is not a novel insight**: the motivation for the method in lines 30-47 seems to imply that the richness of textual feedback is a novel insight. I'm not sure whether the authors intended this implication, but regardless I want to emphasis that this is _not_ a novel insight. Many people have thought about this, and the reason it is not done more is that it is in fact much more expensive to give text feedback than binary feedback (binary could be <1s, text is probably 10x as long to type out)."}, "questions": {"value": "- It was unclear in the paper who the judges are. I presume in all cases the feedback was provided by LLMs, is that correct? (with the exception of the docking scores from vina in dockstring)\n- I had some questions about the molecule design task:\n  1. As far as I can tell, the feedback from the judge is a bunch of molecular descriptors, so it doesn't actually explain _why_ one molecule was better than another, is that correct?\n  2. I did not understand the analysis of figure 3 in lines 461-466. How does the _correlation_ between score and similarity show whether the algorithm is recycling ideas from known molecules? Isn't it sufficient for there to be a _single_ non-novel molecule?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ABpOplzsLV", "forum": "TIOFvhliLA", "replyto": "TIOFvhliLA", "signatures": ["ICLR.cc/2026/Conference/Submission13740/Reviewer_MZQt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13740/Reviewer_MZQt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169511664, "cdate": 1762169511664, "tmdate": 1762924276966, "mdate": 1762924276966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}