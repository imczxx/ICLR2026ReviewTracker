{"id": "fN676M9BM7", "number": 5077, "cdate": 1757844487337, "mdate": 1759897996121, "content": {"title": "Attention Layers Add Into Low-Dimensional Residual Subspaces", "abstract": "Transformer architectures, and their attention mechanisms in particular, form the foundation of modern large language models. \nWhile transformer models are widely believed to operate in high-dimensional hidden spaces, we show that attention outputs are confined to a surprisingly low-dimensional subspace, where about 60\\% of the directions account for 99\\% of the variance--a phenomenon that is consistently observed across diverse model families and datasets, and is induced by the attention output projection matrix. Critically, we find this low-rank structure as a key factor of the prevalent dead feature problem in sparse dictionary learning, where it creates a mismatch between randomly initialized features and the intrinsic geometry of the activation space. Building on this insight, we propose a subspace-constrained training method for sparse autoencoders (SAEs), initializing feature directions into the active subspace of activations. Our approach reduces dead features from 87\\% to below 1\\% in Attention Output SAEs with 1M features, and can further extend to other sparse dictionary learning methods. Our findings provide both new insights into the geometry of attention and practical tools for improving sparse dictionary learning in large language models. Code is available at \\url{https://anonymous.4open.science/r/Language-Model-SAEs-2B1D/}.", "tldr": "We find attention outputs have a low-rank structure, identify this as the root cause of dead features, and solve it.", "keywords": ["Sparse Autoencoders", "Understanding high-level properties of models", "Transformer"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f60fa407e72170ac70092c9a66a2dec4bb5f94cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors demonstrate that the outputs of attention layers are relatively low-rank, with ~60% of dimensions accounting for 99% of variance. They use this to develop a new method for initializing SAEs which initializes the feature directions into this “active” space, rather than initializing uniformly across the entire activation space. They demonstrate that this significantly decreases the number of dead SAE features."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The claim that attention outputs are relatively low-rank is demonstrated convincingly and thoroughly.   \n2. I like how they follow up on the low-rank finding and investigate the source of this phenomenon in section 4.3.  \n3. The idea of initializing SAEs based on how active a given subspace is is novel and interesting, and I would expect it to help improve SAE training (in terms of not just dead features but also other things which the authors didn’t mention, e.g. reducing the number of training tokens needed)  \n4. The paper is mostly well-written, easy to follow, and the mathematical notation is clear (except for some of the figures)."}, "weaknesses": {"value": "1. Given that most activation spaces in LLMs aren’t low-rank, the usefulness of this technique is somewhat limited. As far as I’m aware, training SAEs on the attention output is somewhat uncommon, so if that is the only activation space where this technique is relevant, most real-world use cases would not benefit from this approach.  \n2. The authors do not study other activation spaces which *are* commonly used when training SAEs, e.g. the hidden activations in the MLP. I would really like to see many other spaces included in e.g. Figure 1\\.  \n3. The definition of the initialization method itself is somewhat unclear. The authors say (around line 340\\) that the decoder is initialized to span the active subspace, which makes sense but since there are significantly more decoder directions than dimensions in the active subspace, there are many ways that one could do this. As such, currently the central piece of the initialization method isn’t clearly explained in the main text (I only understood it by reading the pseudocode in appendix I).  \n4. Figure 7(b) is confusing, are they saying that their SAEs find up to 1M alive features in an 8B model? I would expect insanely high levels of feature splitting at that point; I think arguably having dead features is preferable to having all semantically meaningful features be arbitrarily split into many SAE features. This is making me confused about what the features here look like, and as far as I can tell the authors do not present any qualitative evidence on what type of features their SAEs learn (especially the ones with very high numbers of features).  \n5. SAEs trained with this initialization might end up missing certain features. Even though the active subspace contains the vast majority of the variance (99%), SAEs have *many* features and one would expect that roughly 1% of them should fall into the “dead” subspace. As such, I would somewhat expect that SAEs trained with this initialization method would fail to learn (some of) those features.  \n6. Many of the figures are difficult to interpret. E.g. in Figure 1, in the left subfigure and the right subfigure, it isn’t clear what model is being used.  \n7. Many of the figures in the appendices lack proper captions and are thus very difficult to interpret (e.g. Figure 15 — what model are you using, what layer etc).  \n8. The figures are clearly exported as raster images rather than vector images, and they get blurry when you zoom in. The authors really should export in a vector format instead (matplotlib makes exporting to pdf very easy).  \n9. Section 5.2 defines SVD a second time, after it was already defined in section 4.1. Arguably defining SVD is already slightly redundant for a technical audience; defining it twice is certainly redundant.  \n10. Typo in footnote 5 on page 6: “Another prominent open-source SAEs…”. Should be something like “Another prominent set of open-source SAEs…”"}, "questions": {"value": "1. ​​What does Figure 6 mean? TopK is an activation function, ASI is an initialization method, how are these comparable? I’m really confused by the labels here.  \n2. For most figures, can you clarify which models you’re using? Many of them currently do not include this.  \n3. In the initialization method, people have to choose the fraction of variance explained to use as a threshold. Do you have any advice on how to choose the right number here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IW3aC3LtyZ", "forum": "fN676M9BM7", "replyto": "fN676M9BM7", "signatures": ["ICLR.cc/2026/Conference/Submission5077/Reviewer_TjHM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5077/Reviewer_TjHM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760898836492, "cdate": 1760898836492, "tmdate": 1762917858915, "mdate": 1762917858915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the effective dimensionality of Transformer activations and finds that attention outputs occupy a low-dimensional subspace: roughly 60% of directions explain 99% of variance, whereas residual streams and MLP outputs are near full rank. The authors trace this low-rank structure mainly to the attention output projection matrix W_O. They show a strong correlation between low intrinsic dimension and dead features in SAEs. To address this, they introduce Active Subspace Initialization (ASI), which initializes SAE dictionaries in the top right-singular subspace of the target activations, sharply reducing dead features and improving reconstruction and downstream loss; the gains persist as the number of features scales, and combining ASI with SparseAdam further reduces dead features. They also show that ASI helps a sparse replacement model (Lorsa) by lowering dead parameters and improving reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper highlights a previously under-discussed phenomenon: attention outputs live in a lower-dimensional subspace (while this is not the case for MLP and residual activations).\n- The connection to dead features in SAEs, while correlational, is supported by convincing evidence.\n- The proposed initialization appears effective in practice."}, "weaknesses": {"value": "- In Figure 4 the typical effective rank for MLPs is about 0.90–0.95, but the last layer shows a drop to roughly 65%. Any insights into what might cause this behavior? Did you apply your initialization to this layer as well, given it also shows a nontrivial fraction of dead features?\n- Architectural implications of low-rank attention: If attention outputs are low rank, could one reduce per-head dimensionality while keeping d_model fixed, possibly increasing the number of heads, without losing much information? Is the observed low rank due to redundancy across heads or primarily to W_O anisotropy? A short discussion would help clarify whether narrower heads or different head counts would help or hurt.\n- The paper does not discuss whether the initialization helps interpretability. Even a brief note or example on whether feature quality or concept alignment improves would be useful, though I understand this is not the main focus."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NSrK2Dd1Aq", "forum": "fN676M9BM7", "replyto": "fN676M9BM7", "signatures": ["ICLR.cc/2026/Conference/Submission5077/Reviewer_fThX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5077/Reviewer_fThX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401956381, "cdate": 1761401956381, "tmdate": 1762917858685, "mdate": 1762917858685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the intrinsic dimensionality of attention-layer outputs, showing that these representations are highly redundant: roughly 60% of directions capture 99% of the variance. The authors propose a connection between the low-rank structure of attention outputs and the dead-feature phenomenon observed in sparse dictionary learning. To address this, they introduce a novel initialization scheme for self-attentive encoders (SAEs) that projects feature directions into the activation’s active subspace, with the aim of reducing dead features."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper develops principled methodologies for probing the intrinsic dimensionality of various network outputs (attention, MLPs, and residual streams), yielding useful diagnostics for how different components contribute to loss and representation capacity.\n\nEstablishing a link between the low-rank structure of attention outputs and the dead-feature problem is an insightful contribution; this hypothesis could deepen our understanding of representation collapse and suggest principled prevention strategies.\n\nThe proposed initialization strategy appears to alleviate the dead-feature issue for moderate token/model sizes, suggesting practical value for improving representational utilization without costly retraining."}, "weaknesses": {"value": "The biggest issue I have with the paper is that the experimental section seems to be disorganized. It is hard to understand which models, datasets, and setups were used to prepare the experiments and support the claims. To be precise:\n\nSection 4.1 studies the intrinsic dimensions of the attention outputs, MLP outputs, and residual streams. However, it is unclear which models (not just model families, but exact sizes) were used in the experiments. What does the x-axis in the “across layers” image mean? (Does x/6 mean that the value was measured after the first x×6 layers?) What parts/splits/number of tokens were used in the study?\n\nSection 4.2 studies the spectra of the layers in the model. It also reports the impact of the principal components on the model’s loss. It seems that this experiment was conducted on a different dataset than the one from Section 4.1.\n\nSection 5.1 / Figure 4: It is again unclear on which datasets the values were obtained. If the goal was to depict correlation, why not plot the dead feature proportions against the intrinsic dimension to actually measure the correlation?\n\nThe paper is compared only against the Top-K method and its variant with an additional loss (AuxK), which is not explained in the main text.\n\nIn general, the paper seems very interesting, but it requires a more rigorous and structured experimental design."}, "questions": {"value": "I would suggest the authors unify the datasets and models across all the studies in the paper,  preferably adding also a section that summarises the used models and datasets before moving to the analysis of the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LO8mmWWr9O", "forum": "fN676M9BM7", "replyto": "fN676M9BM7", "signatures": ["ICLR.cc/2026/Conference/Submission5077/Reviewer_GERd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5077/Reviewer_GERd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147991512, "cdate": 1762147991512, "tmdate": 1762917858175, "mdate": 1762917858175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate the \"dead latents/features\" problem in Sparse AutoEncoders (SAEs), under the hypothesis that the low intrinsic dimensionality of model activations in various locations may be responsible for dead features. Empirical results in specific settings show that, under singular-value decomposition, attention head outputs require fewer singular values to reconstruct up to a given precision than do residual stream activations or MLP outputs, with SAEs trained on attention head outputs also exhibiting more dead features than SAEs trained on residual-stream activations or MLP outputs. The authors leverage this insight to propose a new method, \"Active Subspace Initialization\" (ASI), which initializes SAEs from singular vectors of activations. In a specific setting, ASI shows fewer dead features relative to baselines, with additional benefits such as lower mean-squared error and lower language modeling loss given ASI-reconstructed activations."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The overall hypothesis -- i.e., that low intrinsic dimensionality may be the cause of dead features -- is an interesting one that could carry substantive implications for SAE training and mechanistic interpretability more broadly. ASI is well-motivated and shows promise as a potential/partial solution to the dead feature problem."}, "weaknesses": {"value": "The most important weaknesses of the paper are the lack of technical details regarding the problem formulation and experiments, and the severely limited range of experiments. In many cases, there is not enough technical information regarding what experiments have been performed to properly interpret results; and in those cases where experiments are clearly defined, the current results are insufficient to support most substantive claims. Key weaknesses by section are elaborated below (with missing critical information elaborated in the Questions section of the review).\n\nIntroduction:\n- The authors claim that \"low intrinsic dimensionality strongly correlates with the number of dead features\". However, there is no systematic analysis of obvious confounders such as dictionary size or the setting of K (L0) with which (top-K) SAEs are trained.\n\nBackground (sec 2):\n- Discussion of linear subspaces (sec 2.1) focuses on the ill-defined notion of \"low-rankness within self-attention mechanisms\"; but there is in fact long line of work studying the phenomenon of linear subspace representation beyond attention mechanisms -- see, e.g., [1-5]. The subsequent discussion of the \"linear representation hypothesis\" in sec 2.2 focuses on the narrow interpretation of this hypothesis regarding 1-dimensional subspaces, when higher-dimensional subspaces (as studied in [1-5, inter alia]) would be a more relevant point of comparison for the intended study of intrinsic dimensionality.\n- The authors state that \"the linear representation hypothesis... has been validated across diverse model scales, architectures, and modalities\"; but the cited works do not (to my knowledge) conduct any specific hypothesis testing. Instead, they train SAEs across such contexts, and demonstrate their utility in interpretability; but the linear representation hypothesis remains very much in contention -- see, e.g., [5, 6].\n\n[1] Mikolov, T., Yih, W. T., & Zweig, G. (2013, June). Linguistic regularities in continuous space word representations. In Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies (pp. 746-751).           \n[2] Pennington, J., Socher, R., & Manning, C. D. (2014, October). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).           \n[3] Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., & Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.           \n[4] Vargas, F., & Cotterell, R. (2020, November). Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 2902-2913).           \n[5] Makelov, A., Lange, G., Geiger, A., & Nanda, N. (2024). Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching. In The Twelfth International Conference on Learning Representations.           \n[6] Sharkey, L., Chughtai, B., Batson, J., Lindsey, J., Wu, J., Bushnaq, L., ... & McGrath, T. (2025). Open problems in mechanistic interpretability. arXiv preprint arXiv:2501.16496.\n\nSec 3:\n- The authors state that \"This formulation shows that O can be viewed as the sum of low-dimensional outputs from each head.\" However, it shows only that there exists a linear transformation from Z to O, which may be full-rank. (Note that the authors use \"rank\" and \"dimension\" interchangeably; so I am interpreting the use of \"low-dimensional\" in the context of a linear transformation to mean \"low-rank\" -- please correct this interpretation if I am wrong.)\n\nSec 4:\n- Throughout the entire paper, the authors refer to \"intrinsic dimension\", which they define here as the smallest integer k such that the ratio of summed squared singular values to total summed squared singular values exceeds a threshold. This definition of \"intrinsic dimension\" is nonstandard -- e.g., the notion of \"intrinsic dimension\" with which I am familiar, and have seen studied in the context of deep learning representations (see, e.g., sec 2.2 of [1] for an overview), is that of [2] -- and in the only citations of \"intrinsic dimension\" in the paper (Guth et al., 2023 and Staats et al., 2025), I do not see any mention of the term \"intrinsic dimension\".\n    - Beyond the lack of clarity and consistency with respect to the literature introduced by this term, it also implies that the authors' proposed re-definition of intrinsic dimension does not carry the same weight as it would elsewhere. Why should we care about this particular measurement? Introducing a new measurement that is so central to the empirical findings requires theoretical or empirical justification regarding its significance, which is not provided.\n- In sec 4, there is no systematic comparison between different layers, models, or SAE hyperparameters (e.g., dictionary size, train-time L0 (i.e., K in top-K), alpha as the auxiliary loss coefficient used to mitigate dead features, etc.).\n    - Across layers and models, there is only a single result in Figure 1 that is not adequately explained (see the relevant point in the Questions section of the review, below).\n    - Across hyperparameter settings: after consulting the full main paper and appendix, I cannot find the hyperparameter settings for the results reported in sec 4. (I see only tau in footnote 3 and some values reported in Appendix G, but it is not clear whether Appendix G applies to all experiments or only those in sec 5.) Even if these values were properly reported, it is crucial to also report results across hyperparameters to determine the extent to which the phenomena in question (i.e., the prevalence of dead features) are simply due to a specific hyperparameter setting.\n\n[1] Janapati, S., & Ji, Y. (2024). A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension. arXiv preprint arXiv:2412.06245.           \n[2] Facco, E., d’Errico, M., Rodriguez, A., & Laio, A. (2017). Estimating the intrinsic dimension of datasets by a minimal neighborhood information. Scientific reports, 7(1), 12140.\n\nSec 5:\n- As in sec 4, there is no systematic comparison across layers, models, or hyperparameters (see the Questions section of the review, below).\n- In sec 5.2 (and 5.4), the differences between AuxK and ASI seem quite small, and could easily be the result of random initialization/sampling, hyperparameter selection, or the specific model/layer/dataset considered. (The experiments below might place a substantial burden in terms of compute requirements, and if necessary could accordingly be run at a a smaller scale than the main-paper results.)\n    - Regarding random initialization: experiments should be repeated with different random seeds, with error bars shown and statistical significance reported. (Note that this may or may not be relevant for ASI, which unlike the baselines is not randomly initialized; but it might still be relevant if training examples are shuffled. I suggest shuffling and running all experiments in Figure 5 and 6 with multiple random seeds.)\n    - Regarding hyperparameter selection: as, per Appendix G.5, it appears that authors have already experimented across  batch size and learning rate. At minimum, I would suggest (a) reporting the results in these settings and indicating whether and why (not) the main-paper findings hold across these settings, and (b) additionally running experiments across dictionary size and K (and ideally alpha as well).\n    - Model/layer/dataset: only a single model (Llama-3.1-8B), layer (15), and dataset (SlimPajama) are considered. All of the claims regarding the apparent superiority of ASI relative to baselines must be validated across multiple models, layers, and datasets. (In my opinion, given the setting, experimenting across a wider range of layers seems more important than doing so with respect to datasets, and datasets more important than models; but failing to consider multiple values for *any* of these experimental considerations is a clear red flag for cherrypicking.)"}, "questions": {"value": "Sec 4:\n- What is the intended meaning and significance of the nonstandard \"intrinsic dimension\" measurement? (See the Weaknesses section of the review for discussion on related work studying \"intrinsic dimension\" -- why opt for this definition instead?)\n- What is the x-axis in the leftmost plot of Figure 1?\n- In sec 4.1, the authors state that \"We empirically verify that this sample size is sufficient to produce stable and reproducible singular spectrum analysis.\" Where is the evidence to support this statement?\n- In sec 4.2, \n    - What, precisely, is fig 2a visualizing? (Please provide a mathematical specification in terms of objects defined in the paper.) What does it mean that \"only 74.7% singular values exceed 1% of the maximum in attention outputs, versus 100.0% for MLP outputs and residual streams\", and how/why is it relevant to the hypothesis under investigation?\n    - In fig 2b, what does \"fraction loss recovered\" mean? (Again, please provide a formal specification.) Is this the train-time loss of SAEs, test-time loss, or language modeling loss of the underlying model? And what does \"recovery\" mean in this sense -- is it something to be minimized or maximized, and what would perfect recovery indicate?\n- In sec 4.3,\n    - What is \"relative value\" on the y-axis of Figure 3? And the authors state that, shown in this figure, \"We compute and visualize both quantities across a set of directions aligned with the right singular vectors of attention output\" -- what, specifically, are the two quantities and the set of directions? (Please provide formal specifications in response to both questions.)\n    - The authors state that \"Our analysis reveals that the low-rank structure of attention outputs O arises primarily from the anisotropy of W^O, which heavily compresses the output space into a lower-dimensional subspace\" -- how, precisely, is anisotropy defined here, and what is the threshold or standard of evidence for arguing \"heavily compresses\"? What is the justification for this threshold/standard, and (where) is supporting evidence provided?\n    - If I am reading the MHSA equation here correctly -- that is to say, that the dimensionality of the *union* of subspaces corresponding to each head is less than or equal to that of the *sum* -- isn't this completely trivial, as it would be guaranteed given any parameterization of attention heads?\n        - Does \"head_i\" denote the output of attention heads, or something else?\n        - Why does this matter (i.e., in what sense is it nontrivial), and how does it relate to the empirical results reported in section 4.3?\n\nSec 5:\n- Are all SAEs (in Sec 5.2-5.4) trained on outputs of attention heads, MLPs, or the residual stream? (Appendix B.2 seems to suggest that main-paper results might be from attention head outputs, but this is never explicitly states.)\n- In sec 5.2,\n    - For fig 5, what is being visualized? in particular, it seems the x-axis is being described as \"different subspace dimensions for activations with a full space dimension of 4096\" -- what does this mean?\n    - For the experiments whose results are reported in figs 5 and 6: what are the dictionary size and train-time K/L0 (for all methods), and alpha (for AuxK)?\n        - And what is L0 on the y-axis of Figure 6? Does this refer to different values of K at train- or test-time? In the former case, are different SAEs trained for each value of K; or, in the latter case, is only one SAE trained at a single value of K then tested at different values of K (which would be out of distribution)?\n        - (Note: \"AuxK\" is never defined -- I had to jump around in the document to find other uses of the term \"auxiliary\" and trace it to the preliminaries. I would state it and cite the relevant works here as well.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vRweOSHkDM", "forum": "fN676M9BM7", "replyto": "fN676M9BM7", "signatures": ["ICLR.cc/2026/Conference/Submission5077/Reviewer_e5FR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5077/Reviewer_e5FR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762224169910, "cdate": 1762224169910, "tmdate": 1762917857783, "mdate": 1762917857783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}