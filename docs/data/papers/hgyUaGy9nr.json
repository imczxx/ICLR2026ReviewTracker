{"id": "hgyUaGy9nr", "number": 2983, "cdate": 1757312055656, "mdate": 1759898115786, "content": {"title": "LAMP: An LLM-based Message Passing Architecture for Text-Rich Graphs", "abstract": "Text-rich graphs, which integrate complex structural dependencies with abundant textual information, are ubiquitous yet remain challenging for existing learning paradigms.\nAn ideal model must simultaneously satisfy **semantic fidelity** (reasoning over full raw text), **structural integrity** (faithful multi-hop propagation), and **computational scalability** (efficient handling of large neighborhoods).\nCurrent approaches inevitably compromise one of these aspects: GNN-based methods compress text into fixed embeddings, losing semantic detail; LLM-based methods serialize graphs into sequences, weakening structural reasoning; and recent \"LLM-as-GNN\" hybrids improve structural integrity but still bypass explicit reasoning on raw content. We introduce **LAMP**, an **L**LM-based **A**rchitecture for **M**essage **P**assing that overcomes this trade-off.\nLAMP reinterprets the stacking of decoders as message passing steps and adopts a dual-representation scheme: it anchors inference on each node’s raw text during each iteration while propagating compact summaries across neighbors.\nFurthermore, LAMP unifies discriminative (e.g., node classification) and generative (e.g., GraphQA) tasks under a single generative formulation, allowing end-to-end training without task-specific heads.\nExtensive experiments show that LAMP effectively unifies graph propagation and text reasoning, achieving competitive performance while offering new insights into the role of LLMs as general-purpose graph learners. *Code will be available upon publication.*", "tldr": "We propose LAMP, a framework that internalizes message passing within LLM decoder layers while keeping raw node texts accessible at every propagation layer.", "keywords": ["Text-Rich Graph; GNN; LLM;"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd1c4c82ed940088b02edc6b09ccdf0b94928347.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In the paper, the authors focus on the rich-text graphs and propose an LLM-based architecture called LAMP to process the graphs. The architecture modify an LLM to be a node encoder and concatenate the neighbors encoding representation to learn from the graph structure. The architecture can both perform reasoning on both raw node language and graph structure. The experiments show that LAMP can finish classic graph tasks such as node classification and graphQA tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The architecture is simple and easy to understand and implement. The LLM is less modified to be a node encoder.\n2. The efficiency seems good. The architecture roughly has the same time complexity level as classic GNNs."}, "weaknesses": {"value": "1. **Novelty overclaim**: though the architecture is new, the idea to make reasoning on the raw data is not novel. In the paper [1], the authors also (1) use an summary token provided by an LLM as the first node representation, (2) and also keep all the raw node text (and even the edge text) to perform reasoning on text-rich graph. They use an cross-attention mechanism to avoid cost of full-text propagation. Besided, their architecture can keep the permutation invariance strictly. Their reported results are also better than LAMP on 'history' dataset.\n2. **Architecture design**: There are some design choices that are not well justified. For example, (1) target on the graph structure learning, the permutation invariance of neighbors order is not theoretically guaranteed. (2) At the same time, although the authors emphasize the importance of the raw text information, the final generation process cannot directly access the raw text of the nodes. The text information is also compressed into the summary tokens $S$ and cannot be directly used. (It could be incorrect because the authors do not explain the generation process in detail. For example, it is not clear what is the $K$ and $V$ in the equation 3.) (3) The message passing and summarization process is task-agnostic, the task description is only used in the final generation process. Considering different tasks may require essentially different information from the graph, it is better to have task-specific message passing and summarization process.\n3. **unclear explanation of experiment results**: the experiments need further explanation. The perplexity in Table 2 cannot support the authors' claim that LAMP learn the neighbor's information becuase the perplexity is not compared to a baseline. Compared to the \"self\" results, does it show that LAMP do not really recover the neighbors' text? At the same time, only the 2-hop neighbor is not enough for some graphs. For example, for some knowledge graphs, the necessary information may be 3-hop or more away. The dataset used in the experiments are classic but somehow old. More recent datasets like ogbn-arxiv can be used to further validate the model's effectiveness. Also notice that the LLMs have get strong performance (like Qwen-2.5), which suggests that the structure information could be not so useful for these tasks. The Cross-Node shuffle show not significant performance drop, which also suggests that the structure information may not be well used.\n4. **Insufficient experiments**: I believe the text-rich graph learning is a very important direction and more complex senarios. The citation graphs (as well as the co-purchase networks) are too naive and special. These graphs are highly homophilous and also the text have enough information to finish the tasks. There is no need to operate deeper structure learning or some complex reasoning on the graph. I believe some experiments on some heterogeneous graphs or multi graphs could better validate the model's effectiveness. For example, the multi-hop reasoning on knowledge graphs can be useful testbed.\n\n[1] Yang et. al., GL-Fusion: Rethinking the Combination of Graph Neural Network and Large Language model. [link](https://arxiv.org/abs/2412.06849)"}, "questions": {"value": "See weaknesses. And:\n1. What is the exact generation process?\n2. Are there some edge-level or graph-level tasks?\n3. Where is the discussion of your LLM usage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AsOmCe7opX", "forum": "hgyUaGy9nr", "replyto": "hgyUaGy9nr", "signatures": ["ICLR.cc/2026/Conference/Submission2983/Reviewer_hkTD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2983/Reviewer_hkTD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761143415450, "cdate": 1761143415450, "tmdate": 1762916479267, "mdate": 1762916479267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose several key desiderata for their ideal text-rich graph model and introduce the LAMP model, featuring an innovative architecture: the dual-representation scheme. In each message-passing layer, LAMP’s aggregator (based on an LLM) simultaneously attends to two types of information: the full raw text of the target node and compact summary tokens from its neighboring nodes. The LLM acts as a rewriter in a different form, generating the messages to be passed—all in textual format."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It balances semantic fidelity and structural integrity.\n\n2. It unifies node classification (a discriminative task) and graph question answering (a generative task) under a single generative paradigm, enabling task-agnostic reasoning through a KV cache mechanism."}, "weaknesses": {"value": "The approach is overly naive. The authors’ method can be summarized as follows: the LLM generates a summary based on neighbor information, and this summary is then passed as the message to produce summaries for the next layer. This kind of approach has already become ubiquitous in 2024.\n\nThe experimental comparisons are insufficient; the paper lacks comparisons against more advanced models such as TAPE, LangTopo, LLaGA, and UniGraph."}, "questions": {"value": "I am genuinely curious: where do the authors believe the novelty or interesting aspect of their design originates? This seems more like a summary model than a genuinely new text-rich graph model. ::)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PLCsAbzNIQ", "forum": "hgyUaGy9nr", "replyto": "hgyUaGy9nr", "signatures": ["ICLR.cc/2026/Conference/Submission2983/Reviewer_Q53Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2983/Reviewer_Q53Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412407116, "cdate": 1761412407116, "tmdate": 1762916479064, "mdate": 1762916479064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes, LAMP, a new architecture combining GNN and LLM for graph with text attributes. This work aggregate information from neighbors for each node in decoder layer level. Therefore, it do not need to compress node text with a seperate LLM. Experiments show that LAMP works well on node classification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear illustration of LAMP architecture."}, "weaknesses": {"value": "1. Insufficient related work: [1] is missing.\n2.  The architecture is very similar to GOFA:: they both iteratively compress GNN text information into memory tokens and passing memory tokens between nodes only.\n3. Insufficient experiment. The baseline combining GNN and LLM only includes PromptGFM in the main table, and representative like LlaGA, GLEM, GOFA cited in this work are not included. Experiments only contains small node classification tasks and one GraphQA dataset. While larger datasets like ogbn-arxiv and ogbn-products, and link prediction tasks (like tasks used in GLEM and GOFA) are not included.  All these making the claims of scalabillity to number of nodes and strong performance not persuasive.\n4. The scalability experiments do not include other models as baseline.\n\n[1] One For All: Towards Training One Graph Model For All Classification Tasks. ICLR 2024"}, "questions": {"value": "1. In Table 6, Cross Node shuffle leads to nearly no performance decrease. Does this result mean the model do not use graph structure informance at all?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QnCFKQBqLI", "forum": "hgyUaGy9nr", "replyto": "hgyUaGy9nr", "signatures": ["ICLR.cc/2026/Conference/Submission2983/Reviewer_3RsJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2983/Reviewer_3RsJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990664577, "cdate": 1761990664577, "tmdate": 1762916478858, "mdate": 1762916478858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an LLM-based message passing scheme for textual graphs where each node always keeps its full text while only compressed neighbor tokens are passed. This aims to balance text fidelity, multi-hop propagation, and cost. Experiments show consistent gains over GNN and text-only baselines, and the structural tests indicate the model actually uses the graph."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The explored task of representation learning on text rich graphs without losing either semantics or structure is important; using an LLM decoder itself as the message passing engine is a natural and well motivated direction.\n2. The proposed LAMP architecture is reasonable with raw text retention, summary token exchange, and iterative updates each supporting the next one.\n3. The reported improvements seem to be strong, together with the structure perturbation tests, indicate that the method is truly exploiting graph signals."}, "weaknesses": {"value": "1. The empirical scope is still narrow for a method that targets large text-rich graphs. All five classification datasets are relatively old, small datasets, and the largest is below fifty thousand nodes. It would be good to include results on larger textual graphs like OGB datasets, to stress test the paper's claims. Right now, the experiments show the method works, but only on fairly narrow settings. \n2. Comparisons to the most recent LLM as GNN or long context compression approaches are not all there. PromptGFM is quoted but is not run in the same setup and uses GPT 4o; so it is more of a reference than a baseline. A few recent hybrid systems also propagate compressed activations or do hierarchical condensation [1,2]; those should be run in the same subgraph setting to make the advantage of LAMP fully convincing.\n3. There is no ablation study or parameter analysis in this paper. It would be better to analyze the ratio, number of summary tokens, or placement of self tokens in the input sequence, which are central to the model design. \n\n[1] Hierarchical Compression of Text-Rich Graphs via Large Language Models. 2024\n[2] OpenGraph: Towards Open Graph Foundation Models. EMNLP 2024"}, "questions": {"value": "Please see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aHztCvFFn0", "forum": "hgyUaGy9nr", "replyto": "hgyUaGy9nr", "signatures": ["ICLR.cc/2026/Conference/Submission2983/Reviewer_2kRn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2983/Reviewer_2kRn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059179548, "cdate": 1762059179548, "tmdate": 1762916478646, "mdate": 1762916478646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}