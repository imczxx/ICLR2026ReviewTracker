{"id": "hgyUaGy9nr", "number": 2983, "cdate": 1757312055656, "mdate": 1763727467861, "content": {"title": "LAMP: An LLM-based Message Passing Architecture for Text-Rich Graphs", "abstract": "Text-rich graphs, which integrate complex structural dependencies with abundant textual information, are ubiquitous yet remain challenging for existing learning paradigms.\nAn ideal model must simultaneously satisfy **semantic fidelity** (reasoning over full raw text), **structural integrity** (faithful multi-hop propagation), and **computational scalability** (efficient handling of large neighborhoods).\nCurrent approaches inevitably compromise one of these aspects: GNN-based methods compress text into fixed embeddings, losing semantic detail; LLM-based methods serialize graphs into sequences, weakening structural reasoning; and recent \"LLM-as-GNN\" hybrids improve structural integrity but still bypass explicit reasoning on raw content. We introduce **LAMP**, an **L**LM-based **A**rchitecture for **M**essage **P**assing that overcomes this trade-off.\nLAMP reinterprets the stacking of decoders as message passing steps and adopts a dual-representation scheme: it anchors inference on each node’s raw text during each iteration while propagating compact summaries across neighbors.\nFurthermore, LAMP unifies discriminative (e.g., node classification) and generative (e.g., GraphQA) tasks under a single generative formulation, allowing end-to-end training without task-specific heads.\nExtensive experiments show that LAMP effectively unifies graph propagation and text reasoning, achieving competitive performance while offering new insights into the role of LLMs as general-purpose graph learners. *Code will be available upon publication.*", "tldr": "We propose LAMP, a framework that internalizes message passing within LLM decoder layers while keeping raw node texts accessible at every propagation layer.", "keywords": ["Text-Rich Graph; GNN; LLM;"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f16a1aa248b6bf71a75c0071991d9e62008dccec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In the paper, the authors focus on the rich-text graphs and propose an LLM-based architecture called LAMP to process the graphs. The architecture modify an LLM to be a node encoder and concatenate the neighbors encoding representation to learn from the graph structure. The architecture can both perform reasoning on both raw node language and graph structure. The experiments show that LAMP can finish classic graph tasks such as node classification and graphQA tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The architecture is simple and easy to understand and implement. The LLM is less modified to be a node encoder.\n2. The efficiency seems good. The architecture roughly has the same time complexity level as classic GNNs."}, "weaknesses": {"value": "1. **Novelty overclaim**: though the architecture is new, the idea to make reasoning on the raw data is not novel. In the paper [1], the authors also (1) use an summary token provided by an LLM as the first node representation, (2) and also keep all the raw node text (and even the edge text) to perform reasoning on text-rich graph. They use an cross-attention mechanism to avoid cost of full-text propagation. Besided, their architecture can keep the permutation invariance strictly. Their reported results are also better than LAMP on 'history' dataset.\n2. **Architecture design**: There are some design choices that are not well justified. For example, (1) target on the graph structure learning, the permutation invariance of neighbors order is not theoretically guaranteed. (2) At the same time, although the authors emphasize the importance of the raw text information, the final generation process cannot directly access the raw text of the nodes. The text information is also compressed into the summary tokens $S$ and cannot be directly used. (It could be incorrect because the authors do not explain the generation process in detail. For example, it is not clear what is the $K$ and $V$ in the equation 3.) (3) The message passing and summarization process is task-agnostic, the task description is only used in the final generation process. Considering different tasks may require essentially different information from the graph, it is better to have task-specific message passing and summarization process.\n3. **unclear explanation of experiment results**: the experiments need further explanation. The perplexity in Table 2 cannot support the authors' claim that LAMP learn the neighbor's information becuase the perplexity is not compared to a baseline. Compared to the \"self\" results, does it show that LAMP do not really recover the neighbors' text? At the same time, only the 2-hop neighbor is not enough for some graphs. For example, for some knowledge graphs, the necessary information may be 3-hop or more away. The dataset used in the experiments are classic but somehow old. More recent datasets like ogbn-arxiv can be used to further validate the model's effectiveness. Also notice that the LLMs have get strong performance (like Qwen-2.5), which suggests that the structure information could be not so useful for these tasks. The Cross-Node shuffle show not significant performance drop, which also suggests that the structure information may not be well used.\n4. **Insufficient experiments**: I believe the text-rich graph learning is a very important direction and more complex senarios. The citation graphs (as well as the co-purchase networks) are too naive and special. These graphs are highly homophilous and also the text have enough information to finish the tasks. There is no need to operate deeper structure learning or some complex reasoning on the graph. I believe some experiments on some heterogeneous graphs or multi graphs could better validate the model's effectiveness. For example, the multi-hop reasoning on knowledge graphs can be useful testbed.\n\n[1] Yang et. al., GL-Fusion: Rethinking the Combination of Graph Neural Network and Large Language model. [link](https://arxiv.org/abs/2412.06849)"}, "questions": {"value": "See weaknesses. And:\n1. What is the exact generation process?\n2. Are there some edge-level or graph-level tasks?\n3. Where is the discussion of your LLM usage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AsOmCe7opX", "forum": "hgyUaGy9nr", "replyto": "hgyUaGy9nr", "signatures": ["ICLR.cc/2026/Conference/Submission2983/Reviewer_hkTD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2983/Reviewer_hkTD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761143415450, "cdate": 1761143415450, "tmdate": 1762916479267, "mdate": 1762916479267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Beginning of Reply"}, "comment": {"value": "Thank you for your valuable feedback! We believe that addressing the reviewer’s comments has enhanced the clarity and presentation of the paper's contributions, raising it to a higher standard. The reviewer's remarks are in _italics_, followed by our responses. Moreover, in the revised version of our paper, we mark all newly added or changed paragraphs in blue. Unless otherwise noted, all references to pages, equations, sections, and citations pertain to the revised version."}}, "id": "S0yWw0pNS3", "forum": "hgyUaGy9nr", "replyto": "hgyUaGy9nr", "signatures": ["ICLR.cc/2026/Conference/Submission2983/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2983/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2983/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763727995418, "cdate": 1763727995418, "tmdate": 1763727995418, "mdate": 1763727995418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose several key desiderata for their ideal text-rich graph model and introduce the LAMP model, featuring an innovative architecture: the dual-representation scheme. In each message-passing layer, LAMP’s aggregator (based on an LLM) simultaneously attends to two types of information: the full raw text of the target node and compact summary tokens from its neighboring nodes. The LLM acts as a rewriter in a different form, generating the messages to be passed—all in textual format."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It balances semantic fidelity and structural integrity.\n\n2. It unifies node classification (a discriminative task) and graph question answering (a generative task) under a single generative paradigm, enabling task-agnostic reasoning through a KV cache mechanism."}, "weaknesses": {"value": "The approach is overly naive. The authors’ method can be summarized as follows: the LLM generates a summary based on neighbor information, and this summary is then passed as the message to produce summaries for the next layer. This kind of approach has already become ubiquitous in 2024.\n\nThe experimental comparisons are insufficient; the paper lacks comparisons against more advanced models such as TAPE, LangTopo, LLaGA, and UniGraph."}, "questions": {"value": "I am genuinely curious: where do the authors believe the novelty or interesting aspect of their design originates? This seems more like a summary model than a genuinely new text-rich graph model. ::)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PLCsAbzNIQ", "forum": "hgyUaGy9nr", "replyto": "hgyUaGy9nr", "signatures": ["ICLR.cc/2026/Conference/Submission2983/Reviewer_Q53Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2983/Reviewer_Q53Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412407116, "cdate": 1761412407116, "tmdate": 1762916479064, "mdate": 1762916479064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response: Clarification on Novelty and Contributions"}, "comment": {"value": "We thank the reviewers for their constructive feedback regarding the novelty of LAMP. We realize that our initial manuscript did not fully illustrate the fundamental gap LAMP fills. To address this, we have completely rewritten the Introduction to frame our contribution and we summarize the key points below:\n\n**1. The Trilemma and the Landscape** We argue that existing paradigms for text-rich graphs face a \"Trilemma\" between 1) Semantic Fidelity, 2) Structural Integrity, and 3) Computational Scalability. Before an in-depth analysis of their limitations, we first distinguish between two popular LLM-GNN integration architectural paradigm:\n\n+ **Invasive Integration:** Apporaches such as GOFA and GL-Fusion mechanically inject auxiliary structure-learning modules (e.g., GNN-like layers) into the LLM backbone. The resulting disjointed system makes the structural aggregation decoupled from the LLM's native reasoning.\n+ **Behavioral Simulation:** Methods including PromptGFM and HiCom attempt to make the LLM itself execute message passing within its textual space. LAMP belongs to this more promising paradigm.\n\n**2. The Critical Gap: Drawbacks of Current \"Behavioral Simulation\"** We identify that prior \"Behavioral Simulation\" attempts fail to resolve the above Trilemma because they impose information bottlenecks for Semantic Fidelity at the following stages:\n\n+ **The Message Bottleneck (happens in Neighbor Transmission):** Methods relying on hard prompts (e.g., PromptGFM) compress rich neighbor context into discrete, coarse-grained natural language summaries, resulting in significant information loss.\n+ **The Anchor Node Bottleneck (happens in Message Aggregation):** More seriously, all prior methods (including both Invasive Integration models like GL-Fusion and Behavioral Simulation models like HiCom) force the LLM to operate on a compressed representation of the target node itself during the aggregation step. This limits the LLM’s most powerful asset—the target node's raw text—at the most critical moment of reasoning.\n\n**3. LAMP’s Unique Contribution** LAMP is the first architecture to dismantle these bottlenecks via raw-text-guided message passing. Unlike symmetric compression methods, LAMP employs an asymmetric dual-representation scheme:\n\n+ **Soft Prompts from Neighbors:** We use differentiable soft prompts to propagate neighbor information, mitigating the discrete Message Bottleneck.\n+ **Raw-Text from Anchor Node:** We architecturally recast the LLM decoder to use the target node’s full, uncompressed raw text as a dynamic query during aggregation.\n\nThus, LAMP enables native, semantically faithful message passing, which fundamentally distinguishs it from prior works that rely on pre-compressed embeddings or summaries. Specifically, the instantiation of GNN's message passing paradigm (Eq. 1) in LAMP is reframed by Eq. 2.\n\n\\begin{equation}\n\\mathbf{h}\\_i^{(l+1)} = \\textbf{Update}\n\\Big(\n\\mathbf{h}\\_{i}^{(l)},\n{\\textbf{Aggregate}\n(\\\\{\n\\textbf{Msg}(\\mathbf{h}\\_{j}^{(l)}):\nj \\in \\mathcal{N}(i)\n\\\\})\n}\n\\Big),\n\\tag{1}\n\\end{equation}\n\n$ \\begin{equation}\n    \\mathbf{S}\\_i^{(l+1)} = \\textbf{Decoder}\\Big(\\big[\n    \\underbrace{\\mathbf{S}\\_{j\\_1}^{(l)} \\|| \\cdots \\|\\| \\mathbf{S}\\_{j\\_m}^{(l)}}\\_{\\text{neighbors}} \n    \\|\\| \n    \\underbrace{\\mathbf{X}\\_i}\\_{\\text{raw text}} \n    \\|\\| \n    \\underbrace{\\mathbf{S}\\_i^{(l)}}\\_{\\text{self}}\n    \\big]\n    \\Big) , j_1, ..., j_m \\in \\mathcal{N}(i).\n    \\tag{2}\n\\end{equation} $"}}, "id": "ctVtuUc6CE", "forum": "hgyUaGy9nr", "replyto": "hgyUaGy9nr", "signatures": ["ICLR.cc/2026/Conference/Submission2983/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2983/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2983/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728871895, "cdate": 1763728871895, "tmdate": 1763728871895, "mdate": 1763728871895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response: Clarification on Novelty and Contributions"}, "comment": {"value": "We thank the reviewers for their constructive feedback regarding the novelty of LAMP. We realize that our initial manuscript did not fully illustrate the fundamental gap LAMP fills. To address this, we have completely rewritten the Introduction to frame our contribution and we summarize the key points below:\n\n**1. The Trilemma and the Landscape** We argue that existing paradigms for text-rich graphs face a \"Trilemma\" between 1) Semantic Fidelity, 2) Structural Integrity, and 3) Computational Scalability. Before an in-depth analysis of their limitations, we first distinguish between two popular LLM-GNN integration architectural paradigm:\n\n+ **Invasive Integration:** Apporaches such as GOFA and GL-Fusion mechanically inject auxiliary structure-learning modules (e.g., GNN-like layers) into the LLM backbone. The resulting disjointed system makes the structural aggregation decoupled from the LLM's native reasoning.\n+ **Behavioral Simulation:** Methods including PromptGFM and HiCom attempt to make the LLM itself execute message passing within its textual space. LAMP belongs to this more promising paradigm.\n\n**2. The Critical Gap: Drawbacks of Current \"Behavioral Simulation\"** We identify that prior \"Behavioral Simulation\" attempts fail to resolve the above Trilemma because they impose information bottlenecks for Semantic Fidelity at the following stages:\n\n+ **The Message Bottleneck (happens in Neighbor Transmission):** Methods relying on hard prompts (e.g., PromptGFM) compress rich neighbor context into discrete, coarse-grained natural language summaries, resulting in significant information loss.\n+ **The Anchor Node Bottleneck (happens in Message Aggregation):** More seriously, all prior methods (including both Invasive Integration models like GL-Fusion and Behavioral Simulation models like HiCom) force the LLM to operate on a compressed representation of the target node itself during the aggregation step. This limits the LLM’s most powerful asset—the target node's raw text—at the most critical moment of reasoning.\n\n**3. LAMP’s Unique Contribution** LAMP is the first architecture to dismantle these bottlenecks via raw-text-guided message passing. Unlike symmetric compression methods, LAMP employs an asymmetric dual-representation scheme:\n\n+ **Soft Prompts from Neighbors:** We use differentiable soft prompts to propagate neighbor information, mitigating the discrete Message Bottleneck.\n+ **Raw-Text from Anchor Node:** We architecturally recast the LLM decoder to use the target node’s full, uncompressed raw text as a dynamic query during aggregation.\n\nThus, LAMP enables native, semantically faithful message passing, which fundamentally distinguishs it from prior works that rely on pre-compressed embeddings or summaries. Specifically, the instantiation of GNN's message passing paradigm (Eq. 1) in LAMP is reframed by Eq. 2.\n\n$\n\\begin{equation}\n\\mathbf{h}\\_i^{(l+1)} = \\textbf{Update}\n\\Big(\n\\mathbf{h}\\_{i}^{(l)},\n{\\textbf{Aggregate}\n(\\\\{\n\\textbf{Msg}(\\mathbf{h}\\_{j}^{(l)}):\nj \\in \\mathcal{N}(i)\n\\\\})\n}\n\\Big),\n\\tag{1}\n\\end{equation}\n$\n\n$ \\begin{equation}\n    \\mathbf{S}\\_i^{(l+1)} = \\textbf{Decoder}\\Big(\\big[\n    \\underbrace{\\mathbf{S}\\_{j\\_1}^{(l)} \\|| \\cdots \\|\\| \\mathbf{S}\\_{j\\_m}^{(l)}}\\_{\\text{neighbors}} \n    \\|\\| \n    \\underbrace{\\mathbf{X}\\_i}\\_{\\text{raw text}} \n    \\|\\| \n    \\underbrace{\\mathbf{S}\\_i^{(l)}}\\_{\\text{self}}\n    \\big]\n    \\Big) , j_1, ..., j_m \\in \\mathcal{N}(i).\n    \\tag{2}\n\\end{equation} $"}}, "id": "ctVtuUc6CE", "forum": "hgyUaGy9nr", "replyto": "hgyUaGy9nr", "signatures": ["ICLR.cc/2026/Conference/Submission2983/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2983/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2983/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728871895, "cdate": 1763728871895, "tmdate": 1763732731119, "mdate": 1763732731119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes, LAMP, a new architecture combining GNN and LLM for graph with text attributes. This work aggregate information from neighbors for each node in decoder layer level. Therefore, it do not need to compress node text with a seperate LLM. Experiments show that LAMP works well on node classification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear illustration of LAMP architecture."}, "weaknesses": {"value": "1. Insufficient related work: [1] is missing.\n2.  The architecture is very similar to GOFA:: they both iteratively compress GNN text information into memory tokens and passing memory tokens between nodes only.\n3. Insufficient experiment. The baseline combining GNN and LLM only includes PromptGFM in the main table, and representative like LlaGA, GLEM, GOFA cited in this work are not included. Experiments only contains small node classification tasks and one GraphQA dataset. While larger datasets like ogbn-arxiv and ogbn-products, and link prediction tasks (like tasks used in GLEM and GOFA) are not included.  All these making the claims of scalabillity to number of nodes and strong performance not persuasive.\n4. The scalability experiments do not include other models as baseline.\n\n[1] One For All: Towards Training One Graph Model For All Classification Tasks. ICLR 2024"}, "questions": {"value": "1. In Table 6, Cross Node shuffle leads to nearly no performance decrease. Does this result mean the model do not use graph structure informance at all?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QnCFKQBqLI", "forum": "hgyUaGy9nr", "replyto": "hgyUaGy9nr", "signatures": ["ICLR.cc/2026/Conference/Submission2983/Reviewer_3RsJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2983/Reviewer_3RsJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990664577, "cdate": 1761990664577, "tmdate": 1762916478858, "mdate": 1762916478858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an LLM-based message passing scheme for textual graphs where each node always keeps its full text while only compressed neighbor tokens are passed. This aims to balance text fidelity, multi-hop propagation, and cost. Experiments show consistent gains over GNN and text-only baselines, and the structural tests indicate the model actually uses the graph."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The explored task of representation learning on text rich graphs without losing either semantics or structure is important; using an LLM decoder itself as the message passing engine is a natural and well motivated direction.\n2. The proposed LAMP architecture is reasonable with raw text retention, summary token exchange, and iterative updates each supporting the next one.\n3. The reported improvements seem to be strong, together with the structure perturbation tests, indicate that the method is truly exploiting graph signals."}, "weaknesses": {"value": "1. The empirical scope is still narrow for a method that targets large text-rich graphs. All five classification datasets are relatively old, small datasets, and the largest is below fifty thousand nodes. It would be good to include results on larger textual graphs like OGB datasets, to stress test the paper's claims. Right now, the experiments show the method works, but only on fairly narrow settings. \n2. Comparisons to the most recent LLM as GNN or long context compression approaches are not all there. PromptGFM is quoted but is not run in the same setup and uses GPT 4o; so it is more of a reference than a baseline. A few recent hybrid systems also propagate compressed activations or do hierarchical condensation [1,2]; those should be run in the same subgraph setting to make the advantage of LAMP fully convincing.\n3. There is no ablation study or parameter analysis in this paper. It would be better to analyze the ratio, number of summary tokens, or placement of self tokens in the input sequence, which are central to the model design. \n\n[1] Hierarchical Compression of Text-Rich Graphs via Large Language Models. 2024\n[2] OpenGraph: Towards Open Graph Foundation Models. EMNLP 2024"}, "questions": {"value": "Please see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aHztCvFFn0", "forum": "hgyUaGy9nr", "replyto": "hgyUaGy9nr", "signatures": ["ICLR.cc/2026/Conference/Submission2983/Reviewer_2kRn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2983/Reviewer_2kRn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059179548, "cdate": 1762059179548, "tmdate": 1762916478646, "mdate": 1762916478646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}