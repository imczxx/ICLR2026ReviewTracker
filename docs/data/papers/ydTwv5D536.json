{"id": "ydTwv5D536", "number": 9790, "cdate": 1758140623996, "mdate": 1759897695650, "content": {"title": "Pick Your Textual Gradients", "abstract": "Automated prompt optimization using textual gradients is a promising approach to improve the performance of Large Language Models (LLMs) with the guidance of natural language feedback. However, the iterative application of these gradients is notoriously unstable. We identify two primary sources of this instability: 1) gradient noise from correctly handled examples, and 2) a loss of generalization, where performance on simpler tasks declines due to over-specialization on complex cases.\nTo address this, we propose a novel framework that stabilizes the optimization process through two core mechanisms: $\\textbf{Error-Driven Refinement}$ and $\\textbf{Regularized Verification}$. First, the error-driven approach ensures a high-quality learning signal by exclusively generating textual gradients from \"picking\" incorrect model outputs, thereby mitigating the noise introduced by correctly handled examples. Second, the regularized verification step treats each resulting prompt update as a candidate, which is \"picked\" only if it passes a preservation test on a fixed holdout set of general examples, ensuring that targeted improvements do not compromise broad robustness. Experiments on several complex instruction-following and reasoning benchmarks demonstrate that our framework drastically reduces optimization instability, prevents performance degradation on general test cases, and consistently finds more robust prompts than standard iterative methods. Our work provides a principled approach to harnessing textual gradients with a high-quality learning signal and preventing specialization-induced degradation, thus enabling a more stable and effective methodology for automated prompt optimization.", "tldr": "", "keywords": ["Prompt Optimization", "Large Language Model", "Automatic Prompt Optimization", "Continual Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f56cb6d773a6741cc6f316abe1a4759cfafd55c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles why automated prompt optimization is so unstable. The authors find two main points: getting feedback on correct answers creates junk gradients, and focusing only on hard cases leads to an overly complex prompt that fails on simple tasks (overfitting).\n\nTheir solution, STEVE, is a two-step filtering process. First, it only learns from the model's failures. Second, it uses a preservation set of easier examples to verify that any new prompt candidate doesn't hurt general performance. This makes the optimization process much more stable and effective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper identifies a real, practical problem. The instability it describes is a major pain point for anyone working on automated prompting.\n2.  The two-part solution is simple and makes perfect sense. It smartly borrows established ideas like hard-negative mining and preventing catastrophic forgetting.\n3.  The results are very convincing. Testing across diverse tasks and multiple powerful LLMs shows that the method is robust and generally applicable."}, "weaknesses": {"value": "1. The set used for verification is based on what the initial prompt gets right. It does not get updated, so it might not protect the new, more complex capabilities the prompt learns during optimization.\n2.  The stability comes at a cost. STEVE requires more LLM calls per iteration to generate and verify candidates. This practical trade-off could be discussed more explicitly."}, "questions": {"value": "I was curious about the annealing strategy for the regularization parameter λ. The paper mentions it is increased at each step, making the optimization more conservative over time. Could you elaborate on the intuition behind this choice, as opposed to keeping it constant or decreasing it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FqQ79CYa3G", "forum": "ydTwv5D536", "replyto": "ydTwv5D536", "signatures": ["ICLR.cc/2026/Conference/Submission9790/Reviewer_PyXM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9790/Reviewer_PyXM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821552937, "cdate": 1761821552937, "tmdate": 1762921277986, "mdate": 1762921277986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes STEVE, a framework to stabilize textual gradient-based prompt optimization. Specifically, STEVE includes two core mechanisms: (1) Error-Driven Refinement, which ensures a high-quality signal by generating textual gradients exclusively from model failures; (2) Regularized Verification, which acts as a gate, accepting a candidate prompt only if its improvement on hard cases does not degrade performance on a fixed \"preservation set\". The authors validate its effectiveness on 10 complex instruction-following and reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of Regularized Verification is intuitive and sound. Tackling the trade-off between generalization and improvement on hard cases with a regularization objective ($Improvement - \\lambda \\cdot Regression$) is a straightforward and effective method.\n\n2. The authors validate the proposed method across a wide range of 10 tasks with three different optimizer models, validating the effectiveness."}, "weaknesses": {"value": "1. The core claims of this paper are based on weak experiments and unrigorous deduction, especially the claim of \"noisy gradients from correct examples\": \n(1) the primary evidence comes from the w/ Correct-Only ablation in Table 2. However, the observed performance drops (<2%) are extremely small and only based on an average of 3 runs, which is well within the margin of random fluctuation for LLM prompting or even prompt rephrasing [1,2]. \n(2) the result are not supportive to deduce the claim that \"producing feedback with noisy gradients that acts as a random perturbation\". This claim is unrigorous without any direct evidence.\n\n\n2. The two core mechanisms of the proposed method seems only re-brand standard components present in prior work. (1) \"Generating feedback only from error cases\" is a widely-adopted mechanism, which has been used in ProTeGi (Pryzant et al., 2023) and many of the following work. (2) The idea of using a dev set to select the best prompt is also standard, as have used in ProTeGi, [1-4] and many other work. The only new idea is the use of a trade-off regularization objective for prompt selecting.\n\n\n\n[1] Large Language Models are Human-Level Prompt Engineers\n\n[2] Are Large Language Models Good Prompt Optimizers?\n\n[3] Prompt Engineering a Prompt Engineer"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HgbuC10b3Z", "forum": "ydTwv5D536", "replyto": "ydTwv5D536", "signatures": ["ICLR.cc/2026/Conference/Submission9790/Reviewer_PA7f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9790/Reviewer_PA7f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830884381, "cdate": 1761830884381, "tmdate": 1762921277456, "mdate": 1762921277456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies instability in automated prompt optimization methods that leverage textual gradients. The paper identifies two major issues: (1) noisy gradients generated from correctly handled examples, and (2) over-specialization on difficult cases leading to degraded generalization. To address this, they propose STEVE, which focuses gradient updates on failed examples while validating each new prompt on a held-out preservation set. Experiments across 10 reasoning benchmarks (e.g., GSM8K, StrategyQA, Object Counting) demonstrate improved stability and consistent performance gains compared to existing baselines such as TextGrad and REVOLVE."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles an important and timely problem in automated prompt optimization for LLMs, focusing on the stability of iterative textual gradient methods.\n\n- The proposed approach, while simple, is conceptually clear and well-motivated: “picking” failed cases for feedback and verifying with a generalization-preservation gate is intuitive and practical.\n\n- The presentation is generally clear, and the problem formulation is rigorous."}, "weaknesses": {"value": "- The two proposed mechanisms, i.e., selecting failed cases and verifying updates on a held-out set, are conceptually related to known ideas such as active learning. While their combination is effective, the methodological contribution feels somewhat incremental relative to existing APO frameworks like TextGrad and REVOLVE.\n\n- The experiments can be extended to a more representative benchmark to demonstrate its effectiveness. It would be valuable to test STEVE on broader and more challenging domains (e.g., MATH500, code generation, open-domain dialogue, or safety-sensitive evaluation) to support claims of general robustness.\n\n- Some of the motivational findings (e.g., “noise from correct examples” or “over-specialization on hard cases”) remain mostly qualitative. More quantitative evidence (e.g., measuring gradient variance, evaluating degradation curves over iterations) would make the analysis more convincing."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LwDzA6NY76", "forum": "ydTwv5D536", "replyto": "ydTwv5D536", "signatures": ["ICLR.cc/2026/Conference/Submission9790/Reviewer_aBt9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9790/Reviewer_aBt9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085570564, "cdate": 1762085570564, "tmdate": 1762921277045, "mdate": 1762921277045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}