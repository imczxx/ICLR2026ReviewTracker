{"id": "3kzDWrohSY", "number": 11215, "cdate": 1758193666618, "mdate": 1759897600721, "content": {"title": "Warm Starts Accelerate Conditional Diffusion", "abstract": "Generative models like diffusion and flow-matching create high-fidelity samples by progressively refining noise. The refinement process is notoriously slow, often requiring hundreds of function evaluations. We introduce Warm-Start Diffusion (WSD), a method that uses a simple, deterministic model to dramatically accelerate conditional generation by providing a better starting point. Instead of starting generation from an uninformed $\\mathcal{N}(\\mathbf{0}, I)$ prior, our deterministic warm-start model predicts an informed prior $\\mathcal{N}(\\hat{\\boldsymbol{\\mu}}_C, \\text{diag}(\\hat{\\boldsymbol{\\sigma}}^2_C))$, whose moments are conditioned on the input context $C$. This warm start substantially reduces the distance the generative process must traverse, and therefore the number of diffusion steps required, particularly when the context $C$ is strongly informative. WSD is applicable to any standard diffusion or flow matching method, is orthogonal to and synergistic with other fast sampling techniques like efficient solvers, and is simple to implement. We test WSD in a variety of settings, and find that it substantially outperforms standard diffusion in the efficient sampling regime, generating realistic samples using only 4-6 function evaluations, and saturating performance with 10-12.", "tldr": "", "keywords": ["diffusion", "efficient diffusion", "conditional diffusion", "inpainting", "image inpainting", "weather forecasting", "probabilistic modelling", "probabilistic forecasting"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6cf4871a747f79f3ea1fc0c09ceb8a7b28652dda.pdf", "supplementary_material": "/attachment/b81bca72933478a3daca277152215aa3bcc91f6e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a neat trick for improving the performance and efficiency of diffusion/flow models. The main idea is to first learn a conditioning-dependent Gaussian initialization (mu, std) rather than using the global distribution of (0, 1). Using samples from this distribution as a starting point, the diffusion process can be improved. A technique is introduced to transform the diffusion process such that it can still operate on the normalized space of mapping N(0,1) -> x_0. A small ablation highlights that the mean-only version of the process does not achieve the same gains, and the affect of variance normalization is crucial. Additionally, a \"warmth-blending\" regularizer is shown to improve performance empirically."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is focused and communicates a simple, solid idea. The technique itself does not introduce significant computational overhead, as the warm-start model is set to be 10x smaller than the full diffusion model. Empirically, the effect of utilizing warm-start is a consistent improvement.\n\nThe paper is written in a clear and concise way. Figure 1 explains the intuition behind the two key ideas in the paper (using warm-start, and using the normalization transformation). Notation is clear and not overly complicated. \n\nExperiments are conducted in a fair manner, using the same architecture and codebase."}, "weaknesses": {"value": "- Experiments are only run on small datasets such as CIFAR. and CelebA. It is unclear how well the technique presented scales to more complex distributions such as Imagenet or text-conditioned models.\n- The 'inpainting via randomly mixed pixels' task is a toy setting not used in practice, and may show outsized improvement on this specific formulation. Specifically, the randomly selected pixels give a large hint as to the mean of the resulting distribution, which may be less true for other distributions such as text or class-conditioning. This paper would be strengthened by a ablation of the performance increase when various types of conditioning is used.\n- There is no related work section, which is a large issue. While the idea presented in the paper is solid, a thorough discussion of past related ideas is necessary to identify the novelty of the presented ideas.\n- The introduction setting describes the problem setting, but does not describe the proposed method. It is not necessary to describe the various conditioning possibilities in the first paragraph."}, "questions": {"value": "See above. My main ask to improve the rating of this paper is to include a thorough related work section, and secondarily to conduct an experiment of the warm-start idea to other conditioning types. Particularly, CIFAR already comes with class labels that can be used as a form of conditioning. If these points are addressed, I am willing to raise my score.\n\nAn interesting finding in this paper is that the mean-normalization aspect is less important than the variance-normalization. I wonder if there are any deeper ways to investigate this phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sB6FWVDEKt", "forum": "3kzDWrohSY", "replyto": "3kzDWrohSY", "signatures": ["ICLR.cc/2026/Conference/Submission11215/Reviewer_eWSh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11215/Reviewer_eWSh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761680211584, "cdate": 1761680211584, "tmdate": 1762922362158, "mdate": 1762922362158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed the warm-up method to accelerate the sampling speed of the diffusion model. The key motivation of this paper is that we can train a model to generate a better start point for diffusion models, thereby reducing the unnecessary timesteps. The experimental results demonstrate that the proposed method can accelerate the diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall method is reasonable since a good starting point, e.g., a gold seed, can bootstrap the diffusion model`s generation speed."}, "weaknesses": {"value": "1. The overall writing of this paper should be further improved. For example, 1) typos. See 118 lines. 2) Section 2 is too confusing. Section 2.3 should be removed from Section 2 and moved to a new section to illustrate how to train WSD in multi-task settings. \n\n2. The overall method is unconvinable. To begin with, this paper claims that WSD is model-agnostic. However, WSD needs to first generate a new dataset from its output. How can this situation be model-agnostic? Meanwhile, each diffusion model needs to fine-tune on this new dataset, thereby leading to a higher computational cost than direct model distillation. The overall pipeline contains: 1) First training WSD, 2) then fine-tuning diffusion models, which is unconvinable.\n\n3. The experimental results lack the necessary baselines. This paper has no baseline to compare against. For example, the warm start is similar to generating good seeds. Therefore, these works can serve as a baseline [1]. Then, the training cost of this paper seems to reach the distillation. How to illustrate that WSD is better than distillation? \n\n[1] Golden Noise for Diffusion Models: A Learning Framework. Zhou et al. ICCV 2025."}, "questions": {"value": "No question, please see Sec. Weaknesses part.\n\nTo sum up, this paper proposed WSD as a good starting point for diffusion models. But due to the lack of necessary baselines and the overall method's unconvincing nature, this paper is obviously below the bar for acceptance. I rate it as reject."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KRtLKiystU", "forum": "3kzDWrohSY", "replyto": "3kzDWrohSY", "signatures": ["ICLR.cc/2026/Conference/Submission11215/Reviewer_UpBA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11215/Reviewer_UpBA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722356218, "cdate": 1761722356218, "tmdate": 1762922361464, "mdate": 1762922361464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"Warm-Start Diffusion (WSD),\" a method designed to accelerate the sampling process in conditional diffusion models. Traditional diffusion models typically start sample generation from an uninformed Gaussian noise distribution, requiring a large number of function evaluations (NFEs). WSD addresses this by incorporating a simple, deterministic \"warm-start\" model that predicts the initial moments (mean and standard deviation) of the conditional data distribution, thereby providing an \"informed\" prior closer to the true data distribution. This significantly reduces the distance the generative process must traverse, leading to a substantial decrease in the required number of diffusion steps. WSD is compatible with any standard diffusion or flow matching algorithm and can be combined with other fast sampling techniques. The paper evaluates WSD on image inpainting and weather forecasting tasks, demonstrating that it generates high-quality samples with only 4-6 function evaluations and saturates performance with 10-12, significantly outperforming traditional methods. Additionally, the paper proposes a \"conditional normalisation trick\" and a \"warmth blending\" mechanism to further enhance the method's effectiveness and compatibility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Sampling efficiency improvement. WSD drastically reduces the number of function evaluations (NFEs) required to generate high-quality samples. It achieves high-quality image generation with as few as 4-6 NFEs and saturates performance around 10-12 NFEs, which is crucial for practical applications requiring fast sample generation (e.g., in autoregressive generation tasks).\n2. Modular design. WSD acts as a \"plug-and-play\" method that can be combined with any standard diffusion or flow matching algorithm without extensive re-derivation or major modifications to existing models. This makes the method flexible and broadly applicable.\n3. The normalisation trick enables WSD to seamlessly integrate with existing diffusion algorithms that assume noise sampling from a standard Gaussian distribution, avoiding complex re-implementation efforts."}, "weaknesses": {"value": "1. Gaussian prior assumption of the warm-start model. WSD's warm-start model assumes an uncorrelated Gaussian posterior for the conditional data distribution. While effective for tasks with strong conditioning information (like image inpainting or weather forecasting), a single Gaussian may be insufficient to capture complex distributions in highly multimodal or weakly conditioned settings (e.g., text-to-image generation), limiting its utility in such tasks.\n2. Task/Dataset dependency of the model. Currently, a separate warm-start model needs to be trained for each experiment and dataset. Although the paper mentions the possibility of training a single general-purpose warm-start model, this remains a limitation that adds complexity and cost to practical applications.\n3. Less pronounced gains at high NFEs. While WSD excels at low NFEs, its performance gains compared to standard flow matching are less significant in the high-NFE saturation regime. This implies that the benefits of WSD are limited when computational resources are abundant.\n4. Introduction of additional model complexity. Although introducing an additional warm-start model improves efficiency, it also increases the overall model complexity, including an extra training phase and additional parameters."}, "questions": {"value": "1. The optimization loss of the first stage. \n2. The effectiveness on other tasks (e.g., image editing) and higher resolutions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ysckMkYHBu", "forum": "3kzDWrohSY", "replyto": "3kzDWrohSY", "signatures": ["ICLR.cc/2026/Conference/Submission11215/Reviewer_AiWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11215/Reviewer_AiWQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727342344, "cdate": 1761727342344, "tmdate": 1762922360996, "mdate": 1762922360996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Warm-Start Diffusion (WSD), a method to accelerate sampling in conditional diffusion and flow-matching models. Standard diffusion starts from an uninformed noise prior, $\\mathcal{N}(0, I)$. WSD instead uses a small, separate, deterministic model ($h_{\\phi}$) to predict an *informed* prior, ${N}(\\mu_{C}, {\\sigma}_{C}^{2} ) $, \n\nbased on the conditional context $C$. By starting from this \"warm\" state, which is closer to the target data distribution, the diffusion process has a shorter distance to traverse, thereby reducing the required Number of Function Evaluations (NFE). The method uses a \"conditional normalisation trick\" to train a standard generative model $p_{\\theta}^{\\prime}$ in a normalized space, making WSD compatible with existing diffusion frameworks. Experiments on image inpainting and ERA5 weather forecasting demonstrate that WSD can generate high-fidelity samples in 4-12 NFE, substantially outperforming a standard flow-matching baseline in the low-NFE regime."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Modular and Simple:** The proposed method is straightforward to implement. It consists of two distinct components: a warm-start regression model ($h_{\\phi}$) and a generative model ($p_{\\theta}^{\\prime}$). This modularity allows any suitable Gaussian regression model to be used for $h_{\\phi}$ without retraining.\n* **Compatible and Synergistic:** WSD is orthogonal to other sampling acceleration techniques, such as efficient ODE solvers. The paper demonstrates this by combining WSD with a flow-matching model and a DPM-Solver.\n\n* **Strong Ablation Studies:** The ablations (Sec 4.2) effectively validate the design choices. The \"Mean-only\" ablation confirms that predicting the conditional standard deviation  is the critical component for low-NFE gains, not just predicting the mean (residual diffusion). The \"Feature only\" ablation confirms that the performance benefit stems from the warm-start normalization itself.\n\n* **Relevant Application:** The method is tested on tasks like autoregressive weather forecasting, where sampling efficiency is a primary practical bottleneck, making the work relevant."}, "weaknesses": {"value": "* **Limited Applicability:** The method's core assumption is that the conditional posterior $p(X_0|C)$ can be reasonably approximated by a *single, unimodal Gaussian with a diagonal covariance matrix* ($\\mathcal{N}(\\hat{\\mu}_{C}, \\text{diag}(\\hat{\\sigma}_{C}^{2}))$). This is a severe limitation. The method only works for **strongly conditional** tasks (e.g., inpainting, 6-hour weather steps) where the output is already highly constrained. The claim of being \"widely applicable\" is an overstatement. The method will fail on weakly conditional or multimodal tasks (like text-to-image), where this prior is entirely insufficient.\n\n* **Ad-Hoc Fix for High-NFE:** The paper reports that the base WSD method *underperforms* standard flow matching in the high-NFE regime. The proposed \"warmth blending\" (Sec 2.3) is an ad-hoc fix for this. This underperformance implies the learned warm-start prior is fundamentally flawed—likely too confident and simplistic (due to the diagonal $\\hat{\\sigma}_{C}$)—and acts as an incorrect constraint. The multi-task blending just papers over this structural problem.\n\n* **Added Overhead:** WSD introduces the overhead of training, storing, and running a *second* model, $h_{\\phi}$. Although the authors state $h_{\\phi}$ is small and do not count its single forward pass in the NFE, it is still an added computational step. Furthermore, this warm-start model is task-specific and must be trained from scratch for each new dataset.\n\n* **Simplistic Prior:** The reliance on a diagonal covariance matrix ignores all correlations in the data. This is a crude approximation, likely responsible for the poor high-NFE performance and limiting the method's ultimate sample quality."}, "questions": {"value": "1.  The \"warmth blending\" (Sec 2.3) was introduced because WSD performs worse than the baseline at high NFE. Does this not point to a fundamental failure of the warm-start model's assumptions? If the $\\mathcal{N}(\\hat{\\mu}_{C}, \\text{diag}(\\hat{\\sigma}_{C}^{2}))$ prior was a *good* approximation, performance should saturate *above* the baseline, not below it. Why is this behavior not interpreted as evidence that the diagonal Gaussian prior is simply incorrect?\n\n2.  In your \"Mean-only\" ablation, you show that predicting $\\sigma_{C}$ is key to low-NFE performance. This suggests the generative model $p_{\\theta}^{'}$ learns to ignore regions with low predicted variance. In Figure 2a, the $\\sigma_{C}$ image looks like a simple blur. Does $\\sigma_{C}$ learn anything more sophisticated than \"high variance for missing pixels, low variance for visible pixels\"? For example, does it capture semantic uncertainty (e.g., higher uncertainty on a mouth vs. a cheek)?\n\n3.  The method is only tested on strongly conditioned tasks. How does WSD's performance degrade as this conditioning $C$ is weakened? For the inpainting task, what happens to the FID vs. NFE curve if you provide 50% or 80% of the pixels, rather than just 5-10%? At what point does the unimodal Gaussian prior become useless and the method's advantage vanish?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lU0yi5yvm3", "forum": "3kzDWrohSY", "replyto": "3kzDWrohSY", "signatures": ["ICLR.cc/2026/Conference/Submission11215/Reviewer_AevA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11215/Reviewer_AevA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934530168, "cdate": 1761934530168, "tmdate": 1762922360606, "mdate": 1762922360606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}