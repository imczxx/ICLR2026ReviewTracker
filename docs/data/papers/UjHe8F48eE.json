{"id": "UjHe8F48eE", "number": 11344, "cdate": 1758196894766, "mdate": 1763548705511, "content": {"title": "Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone", "abstract": "Sparse Autoencoders (SAEs) are widely employed for mechanistic interpretability and model steering. Within this context, steering is by design performed by means of decoding altered SAE intermediate representations. This procedure essentially rewrites the original activations as a weighted sum of decoder features. In contrast to existing literature, we forward an encoder-centric alternative to model steering which demonstrates a stronger cross-modal performance. We introduce S&P Top-K, a retraining-free and computationally lightweight Selection and Projection framework that identifies Top-K encoder features aligned with a sensitive attribute or behavior, optionally aggregates them into a single control axis, and computes an orthogonal projection to be subsequently applied directly in the model’s native embedding space. In vision-language models, it improves fairness metrics on CelebA and FairFace by up to 3.2 times over conventional SAE usage, and in large language models, it substantially reduces aggressiveness and sycophancy in Llama-3 8B Instruct, achieving up to 3.6 times gains over masked reconstruction. These findings suggest that encoder-centric interventions provide a general, efficient, and more effective mechanism for shaping model behavior at inference time than the traditional decoder-centric use of SAEs. We make our code publicly available.", "tldr": "An encoder-focused SAE steering method which suppresses VLM biases and targeted LLM behaviors.", "keywords": ["Sparse Autoencoders", "SAE", "steering", "control", "debiasing", "VLM debiasing", "Steerable representations", "fair representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d7d6720f789690aaaad097452291bf4e15291621.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the **S&P Top-K method**, which selects salient features from Sparse Autoencoders and applies orthogonal projection in the embedding space to achieve effective debiasing and behavioral control. The approach emphasizes encoder-centric representation steering and contrasts with decoder-based reconstruction methods. The authors conduct extensive experiments across language and multimodal models, demonstrating strong performance in mitigating bias and controlling behaviors such as sycophancy, outperforming established baselines like masked reconstruction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper has several strengths:\n\n1. **Cross-model validation**: The method is evaluated on both language and multimodal models, demonstrating strong generality and robustness.\n2. **Encoder-centric paradigm**: It shifts focus from decoder reconstruction to encoder feature control, offering a novel and interpretable perspective.\n3. **Low integration cost**: Once SAEs are trained, the method requires no additional fine-tuning, making it efficient and practical for downstream use."}, "weaknesses": {"value": "1. **Methodological clarity**\n\nI find several aspects of the proposed method are insufficiently explained. For example, Section 3 lacks sufficient explanation of key steps, such as how the learned weight vector functions as a feature-importance signal, by cosine similarity or other criterion? Also, C mentions “distributional variations across sensitive attribute values” without clearly defining which distribution is being analyzed.\n\n2. **Related work comparison**\n\nThe paper is not well-situated in the context of existing studies on SAE-based feature manipulation. Recent works such as \n\n[1] Do I know this Entity? Knowledge Awareness and Hallucinations in Language Models, 2024 \n\n[2] Scaling and Evaluating Sparse Autoencoders, 2024\n\nalso use SAE-derived features to control model behavior by subtracting decoder-weight features from the representation space. The lack of direct comparison with these methods weakens the empirical positioning. \n\nMoreover, the orthogonal projection technique in S&P Top-K appears conceptually similar to the projection approach used in [1], which should be acknowledged and contrasted more explicitly.\n\n3. **Formatting and grammatical issues**\n\nThe paper contains several grammatical errors (e.g., line 113), formatting inconsistencies (line 236), and incomplete references (line 236), which detract from its overall polish. \n\n4. **Limited scope of evaluation**\n\nThe experimental coverage could be expanded to include additional model architectures such as Gemma-based SAEs and a broader set of topics related to model trustworthiness, which would strengthen the paper’s generality and impact."}, "questions": {"value": "How would the S&P Top-K method perform if applied to decoder features, and what underlying factors intuitively differentiate encoder features from decoder features in this context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KgETfs4FUZ", "forum": "UjHe8F48eE", "replyto": "UjHe8F48eE", "signatures": ["ICLR.cc/2026/Conference/Submission11344/Reviewer_TVgc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11344/Reviewer_TVgc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761100014234, "cdate": 1761100014234, "tmdate": 1762922478512, "mdate": 1762922478512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "WJtB1YVrmW", "forum": "UjHe8F48eE", "replyto": "UjHe8F48eE", "signatures": ["ICLR.cc/2026/Conference/Submission11344/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11344/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763548704458, "cdate": 1763548704458, "tmdate": 1763548704458, "mdate": 1763548704458, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes improving the inference of sparse autoencoders (SAEs) by projecting the input onto the selected encoder directions, rather than reconstructing it with the decoder. The decoder is only used for training the SAE. Experiments with the CLIP ViT-B/16 and Llama 3 8B Instruct models demonstrate the validity of the approach for steering, specifically debiasing, at inference time."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work proposes a significant paradigm shift in controlling large models with SAEs, which has the potential to spark impactful future work in this new direction.\n2. Applying the method to two domains (vision, language) demonstrates its versatility and simplicity.\n3. The paper is generally well-written and reports results appropriately."}, "weaknesses": {"value": "1. The method lacks theoretical grounding. It seems that some of the steps outlined in Section 3 are trial-and-error approaches without a clear intuition for why approach A would be more appropriate than B, or whether a particular heuristic is even necessary.\n2. Experiments are weak: \n    - a) The experimental setup is limited to a single SAE architecture and expansion rate $k$ per model.\n    - b) The baseline SAE / Masked Reconstruction can be arbitrarily weak without calibrating the steering strength - see [a] and references given there.\n    - c) Some experimental results seem inconclusive, e.g. the methods' rankings differ between Table 2 (CelebA) and Table 3 (FairFace).\n    - d) The comparison with CAV-like methods (Table 5) should be given in the main text and extended to include its more recent extensions like [b].\n3. In [c], SAEs are also used to debias CLIP on CelebA (Figure 6).\n4. The paper should include code to reproduce the results.\n\n[a] To steer or not to steer? Mechanistic error reduction with abstention for language models. ICML 2025\n\n[b] Navigating neural space: Revisiting concept activation vectors to overcome directional divergence. ICLR 2025\n\n[c] Interpreting CLIP with hierarchical sparse autoencoders. ICML 2025"}, "questions": {"value": "1. What does '(re)training-free' mean in the context of this work? It is used only twice: in the abstract and in contribution 2.\n2. More details are needed regarding the \"S&P Top-K + BendVLM\" method in the paper, beyond what's written in L273.\n3. How can results for the Orth-Proj and Orth-Cal methods be sourced from (Gerych et al., 2024)? Is the experimental setup and implementation identical?\n\n**Other feedback**:\n- The term 'Masked Reconstruction' is confusing. It cannot be found in the papers referenced next to it in L166 (Cywinski & Deja, 2025; Anthropic, 2024), and it is not conventionally used in the context of SAEs. This appears to be a new term, which makes the Abstract and Introduction unclear, particularly Figure 1.\n- Figure 1 conveys little information. It's a single example where method A leads to a better prediction than method B. On another example, it might be the opposite. Why did it happen? How do methods A and B differ? How general is this example?\n- Figure 2 is not understandable without actually reading the paper. What does interpolation do? What is the single bias axis? It overlooks the fact that the goal of the method is to remove information about the concept (orthogonalized against). Additionally, placing two figures before the Introduction is excessive. \n- The Stylist approach proposed by Smeu et al. (2025) is a major factor contributing to the method's performance, so it should be briefly described in the main text or the Appendix.\n- L145: typo in $z$ should be `\\bm{z}`\n- L158: what is $\\mathbf{Y}$?\n- L236: typo in the citation \"?\"\n- L315: \"Worst Group AUC ROC\" should be described in the paper\n- Many typos in how quotation marks are written."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3EyxUghqKo", "forum": "UjHe8F48eE", "replyto": "UjHe8F48eE", "signatures": ["ICLR.cc/2026/Conference/Submission11344/Reviewer_6tCC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11344/Reviewer_6tCC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863831477, "cdate": 1761863831477, "tmdate": 1762922477811, "mdate": 1762922477811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces S&P Top-K, an encoder-centric method for model steering (e.g. controlling model behavior). It selects the Top-K SAE encoder features (instead of a sum of the decoder weights) that are most related to a target attribute and can combine them into a single control direction. At test time, the method removes the target information by projecting the model’s embedding away from that direction.\nIt requires no retraining and adds very little computation. The approach is evaluated on vision-language fairness tasks and LLM behavior steering, and they report consistent improvements over traditional methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- New viewpoints: this paper proposes a different view that shift the focus from decoder-centric to encoder-centric SAE interventions.\n- Computationally efficient and plug-and-play: no model retraining and only a single projection operation at inference.\n- Good performance: experiments on both VLM and LLM shows effectiveness of the method."}, "weaknesses": {"value": "- Motivation & problem framing are weak. The paper does not convincingly justify why encoder-centric intervention is necessary beyond empirical gains. It does not clearly articulate why an encoder-centric intervention is necessary or what specific failure modes of existing decoder-based SAE approaches motivate the new design. \nEven if the paper aims to “rethink” the intervention mechanism, it lacks a modeling or analytical treatment of existing methods, which further limits the clarity and strength of its claimed novelty.\n\n- Lack of related works\nThe related-work section does not adequately cover prior studies, e.g.\n[1] SEMANTICS-ADAPTIVE ACTIVATION INTERVENTION FOR LLMS VIA DYNAMIC STEERING VECTORS, ICLR'2025\n[2] INTERPRETABLE STEERING OF LARGE LANGUAGE MODELS WITH FEATURE GUIDED ACTIVATION ADDITIONS, ICLR'2025 workshop\n[3] IMPROVING INSTRUCTION-FOLLOWING IN LANGUAGE MODELS THROUGH ACTIVATION STEERING, ICLR'2025\n[4] Analyzing the Generalization and Reliability of Steering Vectors, NIPS'2024\n[5] Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models, NIPS'2025\n[6] Evaluating feature steering: A case study in mitigating social biases, Anthropic\n...\n\n- Lack of theoretical grounding.\nThe method is described procedurally, but the theory behind the encoder-centric is not presented. It also lacks analysis of identifiability, feature entanglement, or projection optimality, leaving the approach without a solid conceptual foundation.\n\n- Limited or unfair Evaluation\nSince the paper discusses a general methodological paradigm, choosing only fairness scenarios for comparison is not comprehensive enough. Other aspects, such as trustworthiness, knowledge entities, social bias, etc., should also be considered. Also, comparison with related methods is necessary and required to be discussed.\n\n- Experiments concerns. \n1. Key sensitivities (SAE training data/quality, choice of k, α, and whac-a-mole effects) are not thoroughly explored;\n2. The time cost and the efficiency should be discussed;\n3. Behavioral evaluation on LLMs relies heavily on an internal “LLM-as-a-judge” protocol (potentially subjective and model-dependent) with limited human evaluation and limited reporting of statistical significance/variance."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kcZ26RKtVF", "forum": "UjHe8F48eE", "replyto": "UjHe8F48eE", "signatures": ["ICLR.cc/2026/Conference/Submission11344/Reviewer_Xg7k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11344/Reviewer_Xg7k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926407067, "cdate": 1761926407067, "tmdate": 1762922477454, "mdate": 1762922477454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits sparse autoencoders and proposes an encoder-centric framework for model steering and debiasing without relying on the decoder. Traditional SAE-based steering methods modify sparse latent activations and decode them back into the model’s embedding space, which can introduce reconstruction errors and high computational cost.\n\nThe proposed method instead directly operates on encoder representations: it selects the top-K encoder features correlated with a target attribute or behavior, optionally aggregates them into a single control axis, and applies an orthogonal projection in the embedding space to suppress or enhance specific semantic directions.\n\nThe framework is training-free and lightweight, demonstrated on vision–language models (CelebA, FairFace) for fairness debiasing and large language models (Llama-3-8B) for behavior control (reducing aggressiveness and sycophancy), showing up to 3.2× and 3.6× improvements over decoder-based masked reconstruction baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary originality lies in its proposal to rethink the standard application of Sparse Autoencoders for model steering. It shifts the paradigm from a \"decoder-centric\" approach to an \"encoder-centric\" one. This work identifies and challenges the implicit assumption that SAE semantics are primarily stored in the decoder. \n\nA key finding is that while traditional masked reconstruction significantly harms downstream task utility (wgAUC), the proposed S&P Top-K method (with interpolation) successfully preserves it .In LLM experiments, the method demonstrates substantial gains over the baseline (e.g., 3.6x improvement in sycophancy reduction) \n\nBy identifying a key weakness in the conventional \"masked reconstruction\" approach—namely, poor utility preservation and ineffective control —the paper offers a practical and more effective alternative. The proposed S&P Top-K method is computationally lightweight, training-free, and demonstrates superior performance. These findings provide a valuable contribution that could inform future \"best practices\" for applying SAEs to model steering and debiasing."}, "weaknesses": {"value": "Limited methodological novelty: The core contribution mainly reuses existing components of sparse autoencoders and linear subspace projection. The idea of encoder-only steering through orthogonal projection has conceptual overlap with prior works. To enhance originality, the authors could integrate a theoretical analysis of how sparsity contributes to the interpretability or robustness of the learned control directions, rather than merely showing empirical improvements.\n\nInsufficient interpretability and visualization: While the paper argues that sparsity leads to interpretable feature directions, it does not provide qualitative visualizations (e.g., activation heatmaps or t-SNE plots of projected embeddings). Without showing how selected Top-K features correspond to semantic concepts, it is difficult to validate interpretability claims. Providing such visualization would strongly reinforce the motivation of sparsity-driven controllability.\n\nAlgorithms are limited: Its success depends on a complex and carefully tuned process, where the interpolation method must be LP weighted and feature selectors must be trained for the data. The high degree of process dependency and extreme sensitivity to specific component selection greatly increase the difficulty of method tuning and weaken its usability"}, "questions": {"value": "Provide theoretical explanations. Why does LP bring stability while Mean/Sign fails;\n\nCan we visualize the features before and after modification to prove that certain regions have biased features?\n\nThe model results may be sensitive to the value of K and the random selection process. It is recommended to provide performance curves for different K values.\n\nThe article mentions “For each targeted behavior (aggressiveness and sycophancy), we train a logistic regression classifier on the SAE activations to distinguish between the behavior and its corresponding non-behavior counterpart (i.e., aggressive versus non-aggressive, and sycophantic versus non-sycophantic responses).” does it need to train a feature selector for each requirement"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hZtMpap0uy", "forum": "UjHe8F48eE", "replyto": "UjHe8F48eE", "signatures": ["ICLR.cc/2026/Conference/Submission11344/Reviewer_pdw3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11344/Reviewer_pdw3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997360897, "cdate": 1761997360897, "tmdate": 1762922477031, "mdate": 1762922477031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}