{"id": "iB6ACmX3tM", "number": 7964, "cdate": 1758046391461, "mdate": 1759897819486, "content": {"title": "Mechanistic Analysis of Demonstration Conflicts in In-Context Learning", "abstract": "In-context learning (ICL) enables large language models (LLMs) to perform novel tasks through few-shot demonstrations. However, demonstrations per se can naturally contain noise and conflicting examples, making this capability vulnerable. To understand how models process such conflicts, we study demonstration-dependent tasks where models must infer underlying patterns—scenarios we characterize as rule inference. We find that models suffer substantial performance degradation from single corrupted demonstrations, with corrupted rules accounting for 80\\% of wrong predictions despite appearing in only one position. This systematic misleading behavior motivates our investigation of how models process conflicting evidence internally. Using linear probes and logit lens analysis, we discover models encode both correct and incorrect rules in early layers but develop prediction confidence only in late layers, revealing a two-phase computational structure. We identify attention heads for each phase underlying the reasoning failures: Vulnerability Heads in early-to-middle layers exhibit positional attention bias with high sensitivity to corruption, while Susceptible Heads in late layers significantly reduce support for correct predictions when exposed to single corrupted evidence. Targeted ablation validates our findings, with masking identified heads improving performance by over 10\\%. Our work establishes a mechanistic foundation for understanding how LLMs process conflicting evidence during in-context rule learning.", "tldr": "", "keywords": ["In Context Learning", "Rule Inference", "Conflict Resolution", "Mechanistic Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1aa1943785af5a0154d593fcbd2e5c843c1d242.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper explores how Large Language Models (LLMs) handle conflicting demonstrations in In-Context Learning (ICL), focusing on rule inference tasks (where models need to infer underlying patterns from examples). It is found that a single \"corrupted demonstration\" leads to a significant performance drop – although the corrupted rule appears in only one position, it accounts for 80% of incorrect predictions. Through linear probes and logit lens analysis, the study reveals a two-phase computational structure: LLMs encode both correct and incorrect rules in early network layers, but form prediction confidence only in late layers. In addition, two types of attention heads that cause reasoning failures are identified: \"Vulnerability Heads\" (in early-to-middle layers, exhibiting positional attention bias and high sensitivity to \"corrupted (examples)\"); and \"Susceptible Heads\" (in late layers, reducing support for \"correct predictions\" when exposed to corrupted evidence). \"Targeted ablation\" of these attention heads can improve performance by more than 10%, establishing a mechanistic understanding of how LLMs handle conflicting evidence in \"contextual rule learning\"."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Mechanistic clarity: The study reveals the internal two-phase process of \"early rule encoding + late confidence formation\" and locates specific attention heads that cause failures, providing a practical mechanistic explanation for the \"vulnerability of ICL to conflicts\".\n2. Rigorous validation: The results are consistent across multiple LLMs (Qwen3 series, Llama-3.1), task variants (with/without interleaved \"operator induction\" tasks), and analytical tools (linear probes, logit lens, targeted ablation), enhancing the reliability of the conclusions.\n3. Practical relevance: The identified attention heads and \"ablation strategy\" provide specific improvement paths for enhancing the robustness of LLMs to \"noisy demonstrations\" in real ICL scenarios."}, "weaknesses": {"value": "1. Task limitations: Focusing on \"operator induction\" (a type of mathematical rule task) limits the exploration of how conflicts affect other ICL tasks (such as text classification and commonsense reasoning).\n2. Incomplete analysis of cross-layer causal pathways. Although the empirical synergistic effect between the two types of heads is found (shielding vulnerability heads can reduce the impact of susceptible heads), the complete computational circuit of information transmission and interaction across different layers has not been accurately characterized.\n3. Model diversity gap: Although multiple open-source LLMs are tested, the generalization to \"closed-source models (such as GPT-4)\" or \"ultra-large-scale LLMs\" is not verified.\n4. Mitigation strategies not explored: The paper proves that \"ablating problematic attention heads\" is effective, but does not propose proactive methods (such as training tricks and prompting strategies) to mitigate \"demonstration conflicts\" in practical scenarios.\n5. Insufficient fine-grained analysis: How \"model scale, pre-training objectives, or data filtering\" affect \"conflict handling\" has not been deeply analyzed.\n6. Single evaluation metric. The study mainly focuses on classification accuracy and does not explore other important dimensions such as the model's confidence calibration and uncertainty estimation when facing conflicts."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v2aML99lqh", "forum": "iB6ACmX3tM", "replyto": "iB6ACmX3tM", "signatures": ["ICLR.cc/2026/Conference/Submission7964/Reviewer_jDr5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7964/Reviewer_jDr5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761053981509, "cdate": 1761053981509, "tmdate": 1762919978448, "mdate": 1762919978448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates an insteresting phenomenon: when presented with noisy demonstrations, ICL sometimes can fail. Even though the wrong example consists of only a small proportion. The paper studies this phenomenon in a controlled setting, corrupting examples in different position, showing positional bias. It further uses probing to show that both correct rule and corrupt rule (the rule expressed by the corrupt example) are encoded in model representation. The paper then tries to identify which heads are important for propagating the corrruption from the input example to final prediction. The authors design two kinds of metrics to find two sets of heads, which the authors refer to as Vulnerability Heads and Susceptible Heads. These two sets show relatively small overlapping and one set seems to be located in earlier layers than the other. These heads show high sensitivity to corruption and causally important for making the wrong prediction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Investigated an interesting problem of interpreting the process of demonstration conflicts in ICL.\n- Experimented with five LLMs.\n- Created a controlled setting that might be useful for future study."}, "weaknesses": {"value": "The main problem is that mechanistic interpretation is very limited. The authors define two metrics that measures vulnerability to corruption and the metrics manage to highlight two sets of heads with small overlapping. One set (Vulnerability Head) is sensitive to the corruption in terms of its output and attention weights, but the direct effect (i.e. when projecting to vocabulary space) of promoting certain tokens are small (Figure 8). Their effect seems to be mediated by other downstream components (as they do have effects in causal experiments). On the other hand, the other set (Susceptible Heads) is sensitive in terms of their direct effect changes with corruption. These two sets of heads both play important roles for routing the corruption and all susceptible and vulnerable to corruption. They are similar in this respect, instead of doing very different jobs.\n\nThe paper tells us that there are some sensitive and important heads for routing corruption (also this is not a clear cut, they are just relatively more sensitive and important, other heads can also play roles). However, there’s no mechanistic interpretation about how and why the model decides to flip its prediction because of a single corruption. Yes, there are vulnerable heads, but why are they sensitive to that single corruption? or why do they choose to attend that example? There is little information about how, in some cases, single corruption outweighs many correct demonstrations. While I can understand that these questions are indeed harder and requires future work, but I find the information provided by the current paper is too limited.\n\nIn addition, the metric “Positional Vulnerability Score” seems overly intuitive and heuristic, without enough principle or theory supporting it. It combines attention weights and the norm of the change in head output, though both components make some sense intuitively, but their reliability are debatable, combining them makes it even less reliable in my perspective. Also, the pattern in Figure 6 seems to be not so strong."}, "questions": {"value": "In Sec 4.2, you mentioned that you modified ICL prompts to elicit LLMs to predict rules directly, how did you do that? I don’t find details about it in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6jkG1fgNd7", "forum": "iB6ACmX3tM", "replyto": "iB6ACmX3tM", "signatures": ["ICLR.cc/2026/Conference/Submission7964/Reviewer_vpJX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7964/Reviewer_vpJX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761494031948, "cdate": 1761494031948, "tmdate": 1762919977888, "mdate": 1762919977888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the effect of conflicting demonstrations in in-context learning (ICL) of large language models (LLMs). Using synthetic ICL tasks, they first demonstrate that with a single contradictory example, ICL ability of LLM can be substantially degraded. Then through controlled experiments and analysis, the authors show two temporal phases that cause the model to make wrong prediction. In the *conflict creation* phase, which is concentrated in the early-mid layers, model encodes both correct and corrupted rules, and in the *conflict resolution* phase, which is concentrated in the late layers, model tried to resolve the conflicting rules."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and structurally sound.\n- Although mostly synthetic, authors conduct the well-designed controlled experiments to analyze the effect of conflicting demonstrations."}, "weaknesses": {"value": "- The experiments are synthetic, and comparably easy ICL tasks. \n- The phenomenon itself is not very surprising. Although authors backed their hypotheses with concrete evidence \"one bad example can hurt model's ICL ability\", \"model encodes rules in early-mid layers\" \"model makes predictions in late layers\" are somewhat trivial.\n- Experiments on layer ablation can be an evidence to support their hypothesis, however, it hardly a mitigation strategy."}, "questions": {"value": "- Qwen 32B and Llama-3.1 8b seem to show strong resilient to the corrupted example. What might be the reason? Since the original ICL performance is not reported, it is hard to tell whether this is due to their robustness or their performances were bad even with the correct examples. \n- Is the vulnerability and susceptibility heads are consistent over different ICL tasks / rules?\n- Wouldn't ablating heads would likely to imply degradation of models' performance on general tasks? \n- Line 215 \" far from insufficient\" -> \"far from sufficient\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8oN2njhWNP", "forum": "iB6ACmX3tM", "replyto": "iB6ACmX3tM", "signatures": ["ICLR.cc/2026/Conference/Submission7964/Reviewer_bybq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7964/Reviewer_bybq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831725217, "cdate": 1761831725217, "tmdate": 1762919977451, "mdate": 1762919977451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how large language models process conflicting demonstrations during in-context rule inference. Using operator induction tasks that require genuine demonstration reliance, a single corrupted example causes substantial performance drops and drives 80% of wrong answers toward the corrupted rule despite its minority. A mechanistic analysis with linear probes and logit lens suggests a two-phase computation: early-to-middle “vulnerability heads” encode and amplify conflicted evidence with positional bias and high corruption sensitivity, while late “susceptible heads” reduce support for the correct rule under minority corruption. Targeted ablations of identified heads improve accuracy under corruption by over 10% and reduce positional bias, providing causal support for the proposed mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A principled, demonstration-reliant corruption framework isolates inter-context conflicts and motivates a two-phase (creation vs. resolution) hypothesis grounded in probes and logit lens.\n2. Single-position corruption consistently degrades performance across models and shots; 80.3% of flipped errors align with the corrupted rule, indicating systematic misinterpretation rather than random failure.\n3. Quantitative identification of early vulnerability heads (positional attention and corruption sensitivity) and late susceptible heads (logit attribution shift) provides actionable interpretability."}, "weaknesses": {"value": "1. Focus on operator induction (and an interleaved variant) limits claims about broader ICL settings, multi-step reasoning, or real-world tasks with interdependent demonstrations or unequal evidence weights.\n2. Linear probes and logit lens indicate encoding and confidence but do not establish full causal pathways; head masking is coarse and may conflate multiple circuits or side effects."}, "questions": {"value": "please refer to the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wKFygEccHB", "forum": "iB6ACmX3tM", "replyto": "iB6ACmX3tM", "signatures": ["ICLR.cc/2026/Conference/Submission7964/Reviewer_p32P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7964/Reviewer_p32P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146653019, "cdate": 1762146653019, "tmdate": 1762919977056, "mdate": 1762919977056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}