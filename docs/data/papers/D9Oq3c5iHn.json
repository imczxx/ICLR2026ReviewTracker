{"id": "D9Oq3c5iHn", "number": 22566, "cdate": 1758332934023, "mdate": 1763698887185, "content": {"title": "GALE: Gradient Activation Low-rank Extraction for Fast Memory Efficient Large Language Model Training", "abstract": "Training large language models (LLMs) is highly memory-intensive, primarily due to the storage of optimizer states. A major barrier to scaling LLMs is the memory required to store optimizer states. Current memory-saving strategies involve a trade-off: Parameter-Efficient Fine-Tuning (PEFT) methods are fast and memory-efficient but often underperform compared to full-parameter training. Conversely, gradient projection methods like GaLore enable full-parameter learning with a low memory footprint. However, they introduce a computational overhead within the optimizer step: their reliance on exact decompositions like SVD is prohibitively slow. We introduce \\textbf{Gradient Activation Low-rank Extraction (GALE)}, a method that resolves this overhead. GALE re-engineers the gradient projection pipeline. Instead of SVD, it uses a randomized sketching + QR decomposition algorithm. This approach eliminates a key computational bottleneck in the update step by accelerating the low-rank optimizer update step by up to 23$\\times$ over GaLore. This removes the overhead of gradient projection, resulting modest gains in training throughput. We present GALE in several variants, including an optimized version using mixed-precision fused kernels, which both modestly improve throughput and boost final task performance. When pre-training LLaMA models on the C4 dataset, GALE's task performance maintains that of GaLore while consistently and substantially outperforming PEFT methods. On the GLUE fine-tuning benchmark, GALE reduces the performance gap to leading PEFT techniques while removing the optimizer overhead of GaLore, thereby achieving higher training throughput than prior gradient projection methods and making full-parameter fine-tuning more computationally practical. By effectively balancing memory, performance, and computational speed, GALE sets a new practical frontier for efficient full-parameter LLM training. Code to replicate our findings can be found at \\href{https://anonymous.4open.science/r/GALE/README.md}{GitHub}.", "tldr": "GALE dramatically accelerates memory-efficient LLM training by replacing the slow SVD used in gradient projection with a fast randomized QR algorithm, achieving up to a 23x speedup in the update step without sacrificing performance.", "keywords": ["Memory-Efficient Training", "Large Language Models", "Gradient Projection", "Low-Rank Approximation", "Randomized Numerical Linear Algebra"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13e4b06e0c228c79ddcd15422ebcb136034d21ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GALE, a memory- and computation-efficient framework for full-parameter training of large language models (LLMs). Building upon GaLore (Zhao et al., 2024), GALE tackles its primary bottleneck—the computationally expensive bidirectional SVD used for gradient projection, by substituting it with a lightweight randomized sketching and one-sided QR decomposition. This reformulation preserves low-rank optimizer states while reducing the optimizer step complexity from (O(mn^2)) to (O(mn r_{os})), resulting in substantially faster updates without compromising task performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a bottleneck in memory-efficient full-parameter LLM training by targeting the computational overhead of gradient projection methods such as GaLore.\n\n2. Replacing SVD with a randomized QR-based one-sided projection is conceptually straightforward yet highly effective, achieving up to 23× faster optimizer updates while preserving task performance.\n\n3. The paper evaluates GALE under both pretraining and fine-tuning settings, provides multiple algorithmic variants (Native, Fused, Fused Approx)."}, "weaknesses": {"value": "1. The literature review omits many recent advances in parameter-efficient and memory-efficient training (e.g., DoRA, VeRA, FourierFT, Lion, 8-bit optimizers), leaving GALE’s novelty and positioning insufficiently grounded.\n\n2. Core hyperparameters such as projection rank, subspace refresh frequency, and oversampling/approximation factors lack thorough sensitivity or joint ablation analyses, making robustness claims difficult to assess.\n\n3. Downstream results diverge sharply from prior baselines (e.g., GaLore), with missing explanations for optimizer differences, possible under-training, and mislabeling of task metrics in GLUE.\n\n4. While the paper presents complexity and convergence bounds, it lacks empirical verification of key assumptions (e.g., gradient spectral decay, projection accuracy) and does not fully quantify how one-sided QR projection affects convergence and scalability in large-scale training."}, "questions": {"value": "Q1: The related work section, particularly Subsection 2.1 on low-rank training and adaptation, lacks sufficient coverage of recent state-of-the-art developments in PEFT and related low-rank adaptation strategies. The discussion stops at 2022 (e.g., LoRA, Prefix-Tuning, and (IA)³), overlooking a wave of major advances from 2023–2025 that have substantially shaped this area. Notable examples include DoRA (ICML 2024), VeRA (ICLR 2024),and FourierFT (ICML 2024). These works represent distinct methodological improvements beyond static low-rank updates and thus are essential for positioning GALE’s contribution within the broader landscape of efficient LLM optimization. Similarly, Subsections 2.2 and 2.3 could benefit from integrating more recent advances in memory-efficient optimizers (e.g., 8-bit AdamW variants, Lion, and hybrid quantization-based optimizers) and gradient projection or subspace learning methods (e.g., Adaptive Subspace Descent, Orthogonal Subspace Projection, or spectral projection approaches). I am not suggesting the authors must exhaustively list every new method, but at least the most representative and widely recognized works should be cited and contrasted to clarify how GALE differentiates itself in terms of computational trade-offs and scalability. Without this contextualization, the related work section currently undersells the novelty of GALE’s design and its position relative to contemporary efficient training literature.\n\nQ2: While GALE is built around low-rank gradient projection, the paper does not provide a clear sensitivity analysis on the choice of rank r , which is a critical hyperparameter controlling both memory footprint and model performance. Although Table 1 and Figure 2 include results for fixed ranks (e.g., 128, 256, 512), there is no systematic evaluation showing how varying r affects perplexity, convergence stability, and optimizer overhead. Since the effectiveness of randomized QR projection—and the resulting subspace quality—should strongly depend on r, it is essential to understand whether GALE’s speedup and accuracy gains remain stable under different rank regimes or if the method requires careful tuning per model scale. Could the authors provide a quantitative sensitivity study or ablation demonstrating the trade-off between projection rank, computational speed, and downstream performance, to substantiate the claimed robustness of GALE’s low-rank design?\n\n\nQ3: GALE’s training algorithm (Algorithm 1, Lines 5–17) periodically refreshes the projection basis every (k_{\\text{upd}}) steps, yet the paper does not provide a clear discussion or empirical justification for how this interval is determined or how sensitive the method’s performance is to it. Since updating the subspace too frequently could negate the intended computational savings, while updating too infrequently might cause the projection basis to drift and degrade optimization quality, this parameter likely plays a pivotal role in GALE’s stability and efficiency. Could the authors clarify the heuristic or rationale used to select (k_{\\text{upd}}), and provide an analysis (e.g., perplexity or speed versus refresh interval) demonstrating how often the subspace needs to be updated to maintain accuracy without reintroducing significant overhead?\n\nQ4: The downstream results reported in Table 2 appear to diverge substantially from those in the original *GaLore* paper. For instance, on QNLI, GaLore with (r=4) achieves 92.24% in the original publication, whereas your Table 2 reports only 66.6%, which is a dramatic drop. This discrepancy raises concerns that the pretrained model used for fine-tuning may not have converged properly. Given that GLUE fine-tuning typically requires only a few epochs (often fewer than 100), such underperformance is unlikely to stem from undertraining alone. Could the authors clarify whether the pretraining process was fully converged and whether early stopping or reduced training duration might have contributed to the performance gap? Additionally, I notice that your experiments employ AdamW, while the original *GaLore* used Adam. Since AdamW often provides smoother and more stable gradient flow in transformer-based architectures, one would expect comparable or improved downstream performance, not a significant degradation. Could the authors elaborate on this optimizer choice, and explain why GALE’s fine-tuning results, as well as those of the GaLore baseline reimplementation, are markedly lower than the established benchmarks?\n\n\nQ5: Section A.3 offers a complexity comparison between SVD and QR, but there is no theoretical discussion of the *information retention trade-off* between one-sided QR projection and the optimal two-sided SVD. Since GALE relies on randomized sketching, the bound in Theorem 1 assumes fast singular value decay. Could the authors empirically verify this assumption for typical transformer gradients (e.g., by plotting singular value spectra) to justify why low-rank QR projection maintains comparable task performance?\n\n\nQ6: The ablation studies (Figure 2 and 3) independently vary  r , ( k_{\\text{upd}} ), ( f_{\\text{os}} ), and ( f_{\\text{approx}} ), but the paper never analyzes their joint interactions. Given that oversampling and approximation jointly affect subspace quality and computational cost, a 2-D ablation or Pareto frontier analysis would better illustrate the robustness of GALE’s performance-speed trade-off. Could the authors provide such combined analyses or at least discuss observed parameter coupling effects?\n\nQ7: Table 4 reports “throughput (tokens/sec)” but shows nearly identical throughput values across methods despite large per-step optimizer speedups (up to 23×). This inconsistency suggests the optimizer step is not the dominant component of total training time. Could the authors clarify the relative time share of forward, backward, and optimizer steps, and explicitly quantify the end-to-end wall-clock improvement GALE provides over GaLore in realistic multi-GPU settings?\n\n\n\nOther questions:\n\nO1: In the abstract (Lines 13–15), you state that optimizer-state storage is the *major* barrier to scaling LLM training. While this was arguably true a few years ago, the constraint landscape has shifted with modern hardware and software: H100/H200-class GPUs (large HBM, high NVLink bandwidth) and widespread use of FSDP/ZeRO, activation checkpointing, FlashAttention-style kernels, and 8-bit/memory-factorized optimizers substantially mitigate optimizer-state pressure. At frontier scales (e.g., GPT-5-class efforts), practitioners increasingly report data-side bottlenecks (data quality/curation, mixture design, token throughput, loader IO, distributed pipeline balance) and system throughput limits (network/storage) as the dominant blockers rather than raw optimizer memory.\n\n\nO2: Line 42–43: The claim that training a 7B-parameter model requires upwards of 100 GB of GPU memory appears overstated. Under a realistic bfloat16 training setup, the model weights (~14 GB), optimizer states (two copies for the first and second moments, ~28 GB), and gradients (stored in bfloat16, ~14 GB) together total around 56 GB, well below 100 GB even before accounting for sharding or activation checkpointing. Moreover, modern frameworks (e.g., Megatron-LM, DeepSpeed-ZeRO, FSDP) compute activations in situ or recompute them on demand rather than storing them persistently, further reducing the effective memory footprint. Thus, the cited 100 GB figure likely overestimates the true per-GPU requirement by a wide margin.\n\nO3: In Line 86, the text introduces the term “LE”, but this abbreviation is never defined anywhere in the paper. It appears in the sentence where the authors transition from describing GaLore’s computational bottleneck to presenting their own approach (“LE fundamentally re-engineers the gradient projection pipeline…”). However, it is unclear whether “LE” refers to GALE (Gradient Activation Low-rank Extraction) or an intermediate concept such as “Low-rank Extraction.” This ambiguity interrupts the logical flow of Section 1 and may confuse readers encountering the method for the first time. Please clarify whether this is a typographical omission or an intended abbreviation, and if the latter, define it properly at first use to maintain clarity and consistency throughout the manuscript.\n\nO4: In Table 2, the results are reported for multiple GLUE tasks, but the column header and text refer to a unified “GLUE Score.” This terminology is inaccurate, since each task in the GLUE benchmark uses its own evaluation metric — e.g., Matthews correlation for CoLA, accuracy for MNLI/QNLI/SST-2/RTE, F1 or accuracy** for MRPC/QQP, and Pearson/Spearman correlation for STS-B. The table, as currently formatted, conflates these heterogeneous metrics into a single label, which may mislead readers into interpreting them as a normalized composite score. Please revise the table caption and accompanying text to specify the correct evaluation metric per task, or explicitly report an averaged GLUE composite score if that was the intended meaning, including the aggregation procedure used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6iaWRumnjA", "forum": "D9Oq3c5iHn", "replyto": "D9Oq3c5iHn", "signatures": ["ICLR.cc/2026/Conference/Submission22566/Reviewer_qzQS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22566/Reviewer_qzQS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761130506692, "cdate": 1761130506692, "tmdate": 1762942283033, "mdate": 1762942283033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GALE, a computationally efficient alternative to GaLore-style low-rank optimizers. Instead of performing costly full SVD updates during training, GALE employs randomized sketching followed by QR decomposition to approximate the low-rank subspace for gradient updates. This drastically reduces overhead while maintaining the expressiveness of full-rank training. Experiments on LLaMA pretraining and GLUE fine-tuning show that GALE achieves up to 23× optimizer-step speedups with negligible perfomance loss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation: The SVD cost in GaLore is a important bottleneck for high throughput training.\n- Strong empirical evidence: experiments covers pretraining, fine-tuning and plenty of ablation studies.\n- Co-design: the method is improved by algorithm-side as well as customized CUDA optimization."}, "weaknesses": {"value": "- Confusion expressions: in Line 82-85, the expression makes me feel GaLore will conduct SVD at each training iteration, which is not true, GaLore will do SVD across certain training intervals, e.g., 1000 iterations. It is a little bit exaggerate of the SVD cost.\n\n- Invalid contribution 1: as mentioned in Line 95, the first contribution is to identify the cost of SVD in GaLore, that is not true. nearly half to one year ago, the GaLore-2 and Q-GaLore already mentioned this issue and proposed corresponding solutions. \n\n- Missing important baseline: GaLore-2 already proposed an alternative randomized SVD to alleviate this issue, which should be a important baseline for this paper. Also, other works like https://arxiv.org/pdf/2406.17660 and https://arxiv.org/abs/2412.05270, propose other solutions to get rid of the cost SVD operation.\n\n- Suggestions for experiment design: (i) it's better to pre-train the models for longer time, like the settings in GaLore paper for a fair comparsion. Because some evidence shows performance matching at early training stage may not guarantee late training status. I understand this might be due to limitations in training resources, so I don’t view it as a major concern. (ii) Several GaLore follow-up works use more recent benchmarks to evaluate the fine-tuning performance like https://arxiv.org/abs/2412.06289 and more others. Results on GLUE is good but can be better."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mce0nLQjYC", "forum": "D9Oq3c5iHn", "replyto": "D9Oq3c5iHn", "signatures": ["ICLR.cc/2026/Conference/Submission22566/Reviewer_GYJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22566/Reviewer_GYJz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957694923, "cdate": 1761957694923, "tmdate": 1762942282650, "mdate": 1762942282650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GALE (gradient activation low-rank extraction) is a new method addressing the memory bottlenecks in LLM training by improving gradient projection. Similar to prior work (GaLore for example), the core idea is to avoid storing full optimizer states by projecting gradients into a low-dimensional subspace before the optimizer update. GALE specifically replaces the expensive SVD operation used in GaLore with a fast randomized sketching + QR decomposition procedure. By doing so, GALE eliminates the major computational overhead of GaLore’s approach, with up to 23x faster optimizer update steps in micro benchmark. The paper introduces three variants (GALE Native, GALE Fused, and GALE Fused Approx) to progressively speed up the update step using optimized CUDA kernels and approximations. \n\nFrom the experiments, GALE maintains the same model quality as GaLore while dramatically improving speed, leading to modest gains in training throughput. In LLaMA-1B pretraining on C4, GALE’s final perplexity remains on par with GaLore’s and far better than PEFT baselines, but with less computation overhead. In GLUE fine-tuning, GALE similarly closes much of the accuracy gap between full-model training and LoRA, removing GaLore’s slowdown so that full fine-tuning can run at speeds comparable to standard AdamW."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. GALE consistently achieves huge speedups in the optimizer step while matching or nearly matching the model performance of full training. Notably, GALE brings the training throughput back in line with a full-memory baseline, while retaining the memory-saving advantages of GaLore’s approach\n2. The paper validates GALE in many scenarios: decoder-only and encoder-decoder, pretrain & fintuning, and also experimented with combining GALE with other optimizers (Adafactor, AdamW8bit). The authors also include ablations (studying projection rank and refresh frequency) and report theoretical insights, which adds depth."}, "weaknesses": {"value": "1. While GALE is motivated by training 7B+ param models, the experiments only go up to 1B parameters. The paper cites GaLore’s success on 7B and logically GALE should replicate that with better speed. As a reviewer, I don’t doubt the method scales, but actual evidence on a >7B model would strengthen the statement.\n2. Similar to the original GaLore work, decomposition assumes access to the full gradient matrix, which however will not be the case as long as the workload scales beyond single-GPU or vanilla data parallel. This limitation greatly hinders the practical usability of the method, requiring further work such as GaLore 2 (Su et al., 2025) to unlock generalizability."}, "questions": {"value": "1. It would be great to see confirmation of GALE’s efficacy on larger models (e.g., 7B or 13B parameters) in future revisions\n2. The results show GALE narrowing the gap with LoRA/IA^3 on GLUE, but LoRA still has slight advantage on some low-resource tasks. Could the authors comment on why full-finetuning underperforms LoRA on tasks like CoLA/RTE? Is it due to overfitting or optimization difficulties when updating all weights on small data?\n3. The GALE Fused Approx version introduces an approximation by subsampling the sketch. It would be helpful if the authors could clarify the impact of this on final performance\n4. It would be great to discuss about GALE's compatibility with distributed training (especially tensor parallel and ZeRO stage 2/3 which partitions the gradient for one weight tensor)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WsfdTjWWp0", "forum": "D9Oq3c5iHn", "replyto": "D9Oq3c5iHn", "signatures": ["ICLR.cc/2026/Conference/Submission22566/Reviewer_aCW8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22566/Reviewer_aCW8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959317423, "cdate": 1761959317423, "tmdate": 1762942281836, "mdate": 1762942281836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}