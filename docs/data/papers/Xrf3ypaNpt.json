{"id": "Xrf3ypaNpt", "number": 11979, "cdate": 1758204982189, "mdate": 1759897541508, "content": {"title": "Fixed Point Explainability", "abstract": "This paper introduces a formal notion of fixed point explanations, inspired by the “why regress” principle, to assess, through recursive applications, the stability of the interplay between a model and its explainer. Fixed point explanations satisfy properties like minimality, stability, and faithfulness, revealing hidden model behaviours and explanatory weaknesses. We define convergence conditions for several classes of explainers, from feature-based to mechanistic tools like Sparse AutoEncoders, and we report quantitative and qualitative results for several datasets and models, including LLMs such as Llama-3.3-70B.", "tldr": "We introduce the notion of fixed point explanation to formally characterise and study the interplay between a model and its explainer.", "keywords": ["explainability", "formal explainability", "guaranteed explainability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8946a70f3e1ac1ec2cca73dc156336d8067d090f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces fixed point explanations; iteratively explaining explanations from a model for a given input-label pairing until convergence. This follows the idea of humans asking 'why?' to a provided explanation, attempting to develop and understanding of the answer provided. This can reveal the models inconsistent behaviours, captured in the variability between explanations, and minimal sets of features responsible for a models behaviour."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The notion of recursive explanations is interesting and of interest to the ICLR community.\n- The authors justify why the principles of recursive explanations are of importance in explainable AI (XAI).\n- The authors provide reasoning as why to why the recursive explanations satisfy the commonly used properties of faithfulness and stability, although the applications differ from the traditional methods in XAI.\n- The experiments cover a range of modalities."}, "weaknesses": {"value": "- Section 2.1.3 could benefit from added discussion on the key differences between the principles applied in traditional XAI, and in the fixed point iteration. For example, stability in an algorithm such as LIME is usually seen as perturbations in the feature space or perturbations in the sampled neighborhood when training your local model, and how does this related with the fixed point iteration definition?\n- I also think it would help readers familiar with XAI to see some simple visual examples - for example a 2-d example iterating using a feature-based explainer. It would help relate to concepts that people in the field are familiar with."}, "questions": {"value": "- As in the weaknesses section, what do the authors think are the main differences between aspects of explainers such as stability and faithfulness in the traditional XAI sense vs the fixed point iteration setting?\n- Do these aspects behave similarly to the traditional XAI defining, following areas of uncertainty in the models classification? Different behaviour can be observed depending on whether the data point you are explaining is close to the decision boundary or far. It would also be interesting to see the movement. I could also see some of these aspects being captured by a sphere which surrounds the explanations in all iterations in feature space for a feature-based explainer, i.e. if the radius of this sphere is large then there is more variability between iterations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JtVNEArSnn", "forum": "Xrf3ypaNpt", "replyto": "Xrf3ypaNpt", "signatures": ["ICLR.cc/2026/Conference/Submission11979/Reviewer_WWxa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11979/Reviewer_WWxa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765561189, "cdate": 1761765561189, "tmdate": 1762922976462, "mdate": 1762922976462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces fixed point explainability, a theoretical framework that formalizes the stability of explainability methods through recursive application. The authors argue that an explainer ε, when repeatedly applied to its own outputs with respect to a model f, should converge to a fixed point x* satisfying ε(x*; f) = x*. Such convergence is interpreted as a certificate of self-consistency between the model and its explanation. To generalize this idea, the paper defines P-fixed points, where the recursion preserves a set of properties P that maintains model's predictive invariance. Theoretical guarantees for the existence of fixed points are provided under determinism and monotonicity assumptions.\n\nThe framework is instantiated for three families of explainers. First, feature-based methods are examined to test whether their attribution maps remain stable under recursion. Second, prototype-based models are analyzed, showing that prototype interactions often lead to cyclic rather than convergent behavior. Third, the concept is extended to SAEs used in mechanistic interpretability of large language models, where recursive interventions are used to test consistency of token-level predictions. Across these domains, the experiments show that many explainers fail to reach consistent fixed points, implying that current explanation techniques may not be self-consistent with the models they are meant to interpret. The authors propose that fixed-point analysis can thus serve as a general diagnostic tool for evaluating the stability and internal validity of explanation methods."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a conceptually novel and intellectually stimulating approach to evaluating explainability methods. Its central idea, that recursion of an explainer on its own outputs can reveal internal consistency between the model and the explanation, introduces a fresh perspective to a field. The mathematical framing is clear and well motivated. The authors carefully define fixed points and property-preserving fixed points, connect them to the fixed-point theorem, and articulate how convergence may serve as an internal validity check. The unifying scope of the framework is also impressive. It brings together three interpretability directions, e.g, feature-based attribution methods, prototype-based reasoning models, and sparse autoencoder analysis in LLMs, under a single analytical lens. This breadth demonstrates that the idea is not bound to a specific architecture but reflects a general principle about model–explainer interactions. The paper is written with strong clarity and coherence, and the theoretical–philosophical motivation adds depth without overshadowing the formalism. Overall, it provides a thought-provoking conceptual foundation that could inspire a new class of diagnostics for stability and faithfulness in explainability research."}, "weaknesses": {"value": "**1.**\nThe core premise equates recursive stability with explanatory validity. This is conceptually fragile. A trivial constant explainer that outputs the same mask or rationale for every input reaches a fixed point in one step, yet it communicates no model specific information. Stability, as defined here, is therefore not sufficient for faithfulness. The paper acknowledges faithfulness informally, but never rules out such degenerate fixed points nor provides a practical test that separates meaningful convergence from vacuous convergence.\n\n\n**2.**\nThe theoretical guarantees rest on determinism and monotonicity. Many widely used explainers are stochastic or adaptive. LIME samples neighborhoods and can change across runs. SHAP often relies on sampling estimates. Under such settings, non convergence is expected behavior due to intrinsic randomness, not a failure of explanation. One could study convergence in expectation or concentration around a mean fixed point, but this is not developed. As a result the negative empirical findings risk re describing stochasticity rather than revealing lack of faithfulness.\n\n**3.**\nThe recursion requires that the explainer output can be reinjected as a valid input. This domain alignment rarely holds in practice. For saliency or GradCAM the output is a heatmap, not an input image. The paper introduces support functions to coerce reinjection, yet these choices change the induced dynamics and can dominate the result. In the appendix the authors try several masking strategies and obtain different behaviors. This sensitivity implies that the fixed point is as much a property of the chosen support as of the explainer or the model. The main text does not quantify this dependence.\n\n**4.**\nRecursive application can collapse information and force convergence for the wrong reason. If each step removes or attenuates features, the process can converge to empty or near empty representations. Such convergence is a signature of information decay, not of explanatory adequacy. The paper shows shrinkage for some feature explainers, but treats this as evidence of instability rather than as a predictable artifact of the masking operator. Without a conservation or sufficiency control, convergence is ambiguous.\n\n**5.**\nThe prototype analysis treats cycles as a pathology, yet cycles can reflect meaningful conceptual neighborhoods. A pair or small set of prototypes can be mutually nearest under the model similarity metric while still representing coherent local structure. Declaring non convergence as failure ignores this possibility. The theory anticipates cycles in a finite state system, but the empirical section does not attempt to measure whether cycles align with human concepts or with class structure.\n\n**6.**\nFor LLM SAEs the property P is defined as preservation of token level distributions under residual patching. The experiments show large intermediate drift before eventual stabilization. This demonstrates that recursion can degrade predictions in the process, which limits its utility as a diagnostic that one would actually run. Moreover, token preservation is a model internal criterion. There is no check of semantic coherence of features or alignment with human judged rationales. The claim that P fixed points certify anything beyond internal invariance is therefore unsubstantiated.\n\n**7.**\nExternal validity is not established. There is no correlation between fixed point metrics and standard faithfulness measures, such as deletion and insertion curves, or with human preference judgments, or with downstream task success. Without such links, the reader cannot infer whether fixed point behavior is predictive of explanation quality. The framework risks becoming a self referential audit of model explainer dynamics rather than a measure of usefulness.\n\n**8.**\nPractical aspects are under reported. The paper does not provide iteration count distributions, runtime, convergence failure rates, or sensitivity to seeds. For large models and SAEs, the cost of recursion may be high. Without cost and reliability analysis, it is difficult to recommend the method as a routine diagnostic."}, "questions": {"value": "**Q1.**\nHow do the authors distinguish meaningful fixed-point convergence from trivial or degenerate cases, such as constant explainers that immediately satisfy the fixed-point condition but provide no model-specific insight? Could an additional measure of information content or explanatory sufficiency help rule out such vacuous convergence?\n\n\n**Q2.**\nMany explainers, including LIME and SHAP, are inherently stochastic or adaptive. How does the proposed theory extend to such cases? Could the authors analyze convergence “in expectation” across random seeds or provide empirical evidence that approximate probabilistic fixed points correlate with explanation quality?\n\n\n**Q3.**\nSince the recursion requires reinjecting explainer outputs through a support or masking function, how sensitive are the observed dynamics to this design choice? Would an ablation across different support mappings clarify whether convergence behavior arises from the explainer itself or from the auxiliary reinjection mechanism?\n\n\n**Q4.**\nCan the authors verify that convergence is not a byproduct of information collapse? For instance, could they track feature-level entropy, attribution sparsity, or retained variance across iterations to confirm that convergence reflects stability in explanatory content rather than progressive degradation?\n\n\n**Q5.**\nIn the prototype experiments, could cycles among prototypes represent meaningful conceptual neighborhoods rather than instability? Would it be possible to evaluate whether cyclic prototype transitions align with class structure or human-interpretable clusters instead of treating them as pathological?\n\n\n**Q6.**\nFor the SAE analysis on LLMs, can the authors examine whether recursive degradation of token predictions affects the semantic coherence of identified features? Beyond internal token preservation, would a human or qualitative evaluation help determine if the eventual P-fixed points correspond to interpretable or meaningful concepts?\n\n\n**Q7.**\nHow do fixed-point metrics relate to established measures of faithfulness and usefulness? Could the authors report correlations with standard metrics such as deletion/insertion curves, sensitivity tests, or human judgment scores to substantiate external validity of the proposed diagnostic?\n\n\n**Q8.**\nWhat is the computational cost and reliability profile of recursive evaluation? Providing distributions of iteration counts, runtime, and convergence rates, as well as sensitivity to random seeds, would help clarify the feasibility of applying FPE to large-scale explainers or LLM settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "a9d2Dq2lBi", "forum": "Xrf3ypaNpt", "replyto": "Xrf3ypaNpt", "signatures": ["ICLR.cc/2026/Conference/Submission11979/Reviewer_uFZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11979/Reviewer_uFZV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945389961, "cdate": 1761945389961, "tmdate": 1762922974226, "mdate": 1762922974226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the concept of a \"fixed point explanation.\" The idea is to apply the classic mathematical concept of fixed points to explanation algorithms. \n\nTo arrive at the fixed point, the explanation algorithm is iteratively applied multiple times.\n\nA high-level motivation for fixed points explanations is given: They are linked to the concept that humans, when provided with an explanation, will continue to ask further questions, that is, they will demand explanations for an explanation (this is how I understand it).\n\nThe paper presents a mathematical exposition of the properties of fixed-point explanations, including a discussion of the desirable mathematical properties of fixed points (stability, faithfulness) and the existence of fixed points.\n\nThe paper also applies the concept of fixed-point explanations to different classes of explanation algorithms (saliency maps on CIFAR-10, SAEs for language models). \n\nQualitative examples throughout the paper illustrate the iterative process of computing fixed-point explanations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea to apply fixed-point analysis to explanation algorithms is, to the best of my knowledge, novel, and I find this idea interesting. The idea is also somewhat innovative and could be of potential interest to the community. \n\nThe mathematical exposition in the paper is sound. While most of the analysis appears to be relatively straightforward, given that the concept of fixed points is well-studied in mathematics, the overall mathematical exposition is coherent.\n\nThe concept of fixed-point analysis is applied to various explanation algorithms, including saliency maps for images and sparse autoencoders for LLMs."}, "weaknesses": {"value": "The main weakness of the paper is that it does not contain a single empirical example where the benefits of the proposed fixed-point approach for explainability are apparent. In Figure 2, the fixed-point explanation looks just like the original explanation. In Figure 3, we can see that dynamics of iterating, but what is the point of obtaining the final image and heatmap, especially since the final image is different from the original image, and also the class label is now different? What is the real-world meaning of this explanation? In Figure 4, the result of the iteration is a strange artifact. In Figure 7, again, what are exactly are we supposed to see from these qualitative examples? \n\nThere are further limitations of the empirical evaluations; for example, there is no evaluation of standard metrics like ROAR (or any other standard metric) that compares the fixed-point explanations to other explanations (Table 1 and Table 2 evaluate the explanations according to quantitative metrics, but I am missing commonly used metric to gauge the quality of the explanations). However, the main limitation of this paper is that despite the nice mathematical exposition, there is not a single qualitative example that clearly demonstrates what the real-world benefits of the proposed approach are in an application that the community cares about. For this paper to be accepted at a conference like ICLR, multiple such examples would be required. \n\nI also remain skeptical about the overall usefulness of the concept of fixed points when applied to explanations. Fixed points are a concept for functions that map from X to X. Most explanation algorithms, however, perform dimensionality reduction: They take the function and the input and map them to a simpler, lower-dimensional structure. The paper is aware of this complication and attempts to overcome it with a support function that maps the lower-dimensional explanation back to the input space so that the concept of fixed points can be applied. The choice of support function seems like a very critical step that deserves even more discussion than what is currently in the paper - whether the concept of fixed points is at all applicable to many explanation algorithms depends on whether we can find convincing support functions."}, "questions": {"value": "**Question 1:** I understand that you discuss the existence of fixed points, but what about convergence? Do you have results that the iterative algorithm is guaranteed to converge to fixed points for the different setups? Can we say anything about the required number of iterations?\n\n**Question 2:** Can you describe in intuitive terms the advantages of providing a fixed-point explanation to an end user or model developer as opposed to providing the original explanation?\n\n**Question 3:** How does the choice of support function impact the interpretation of fixed-point explanations?\n\n**Justification for final score:** This paper introduces the concept of fixed points to the analysis of explanation algorithms. After reading the paper, I am intrigued by this idea, but remain far from convinced that it will be fruitful. To be accepted at a venue like ICLR, this paper needs to go beyond proposing an interesting idea; it must make the case that the idea is, in fact, useful. Since the paper does not currently make this case, I recommend rejecting it at this point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mNhAwo0GMG", "forum": "Xrf3ypaNpt", "replyto": "Xrf3ypaNpt", "signatures": ["ICLR.cc/2026/Conference/Submission11979/Reviewer_QTvA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11979/Reviewer_QTvA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762343274092, "cdate": 1762343274092, "tmdate": 1762922973446, "mdate": 1762922973446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an effort to analyze the properties of explainability methods using a regressive framework, where explanations are recursively applied until convergence. Several case studies are provided to illustrate how such recursive applications can reveal properties like consistency and stability across different types of explainability methods.\n\n\nHowever, the paper tends to be somewhat abstract in parts, and certain subsections suffer from notational inconsistencies that make them difficult to follow. Moreover, the practical utility of this work is questionable in the current landscape, where explainability has moved towards multimodal paradigms (for example, visual decisions explained through text). The study remains confined to feature-based, saliency-driven explanation frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors present solid arguments for analyzing explanation methods through the “why regress” principle, focusing on consistency and stability as key properties.\n\nThe proposed framework is explored across both feature-based explainers such as VGG16 and transformer-based language models, showing some versatility in application."}, "weaknesses": {"value": "The work is limited to saliency or feature-attribution-based explanation methods, most of which are relatively dated (5–7 years old). While understanding their properties is intellectually useful, the motivation feels weak in today’s context. Modern vision-language models no longer rely on such feature-based explanations, raising questions about the current relevance of the work.\n\nThe description of the SAE’s application on transformer models lacks clarity. In particular, it is not well explained why the SAE was applied to the last-layer outputs (before tokenization).\n\nThe theoretical guarantees of convergence and other claimed properties are based on assumptions that may not hold for many practical XAI methods.\n\nComputational complexity is not discussed at all, despite the authors mentioning recursive applications extending up to 20 or more steps in some cases.\n\nPrototype-based explainers are not shown to satisfy consistency or stability, and the authors defer this as “future work.” This weakens the overall claim of analyzing fixed-point, regression-based explanation methods.\n\nFigure 4 is confusing. The recursive steps alter class labels, and the final “converged” explanation appears inconsequential. The visualization raises more questions than it answers."}, "questions": {"value": "The writing needs significant refinement. Several sections are incomprehensible and overloaded with notation that is neither introduced nor explained clearly.\n\nThe paper does not adequately justify the choice of support functions or how they are constructed, merely referring to them as a “common choice.”\n\nThe recursive procedure might violate the input data distribution on which the original models were trained, an issue that remains unaddressed.\n\nThe discussion on convergence guarantees under non-monotonic conditions (a notably strict requirement) is superficial, representing a major oversight.\n\nOverall, the positioning of the work is unclear. The paper does not articulate what key takeaways modern XAI methods should derive from this study to enhance their practical applicability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qhfu8meUaU", "forum": "Xrf3ypaNpt", "replyto": "Xrf3ypaNpt", "signatures": ["ICLR.cc/2026/Conference/Submission11979/Reviewer_yLGy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11979/Reviewer_yLGy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762412748536, "cdate": 1762412748536, "tmdate": 1762922973025, "mdate": 1762922973025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper's main idea is to test how stable an explanation is by recursively explaining the explanation. This is inspired by the \"why regress?\" principle. Based on my understanding, this means:\n1. You get an explanation for an input (e.g., a heatmap of important pixels).\n2. You use that explanation as the new input for the explainer.\n3. You repeat this process until the explanation stops changing.\n\nThis final, unchanging explanation is what the authors call a \"fixed point\". They argue this fixed point should be a minimal and faithful explanation. They also check if a key property (like the model's prediction staying correct) holds true during every step; if it does, it's a \"P-fixed point\". The authors use this method as a \"sanity check\" to find hidden instabilities in different XAI tools, including LIME, SHAP, and even methods for LLMs like SAEs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The concept of a \"fixed point explanation,\" based on recursively applying an explainer to its own output (the \"why regress\" principle), is an interesting new way to think about and evaluate the stability of XAI methods.\n2. The paper's ambition in applying this single framework across three very different and timely classes of explainers (feature-based, prototype-based, and mechanistic) demonstrates the potential generality of the fixed-point concept."}, "weaknesses": {"value": "1. **Unclear Core Methodology and Role of the Support Function**:  I found the paper's central definition difficult to follow. The main methodology in Section 2 defines the recursive step as $x_k = \\epsilon(x_{k-1}; f)$, which implies the explainer $\\epsilon$ outputs a new object in the same format as the original input $x$. However, for feature-based explainers like LIME or SHAP (Section 3.1), the explainer's output isn't a new image, but rather a set of feature importance or a heatmap (which the paper calls $Z$). To make this work, the paper briefly introduces a \"support function\" $s$ to map this explanation ($Z$) back to an input ($X$). This support function seems critical to the whole process, but it's absent from the main definitions (2.1, 2.2, 2.3) and only discussed in detail in the Appendix. This makes it very confusing to understand what is actually being iterated. It seems the choice of $s$ (e.g., zeroing out, blurring) is just as important as the explainer $\\epsilon$, but it's not treated as a core part of the method. Clarifying how $s$ interacts with $\\epsilon$, and whether all experiments use the same $s$, would greatly improve the paper’s clarity.\n\n2. **Conceptual gap in what the \"explanation\" is**: The paper claims the final fixed point $x*$ is a minimal, stable, and faithful explanation. However, looking at the examples (e.g., Figure 2 or Figure 7), the fixed point $x^*$ doesn't look like an \"explanation\" in the sense of why the model made its decision. Instead, it looks like a minimal input that can still trigger the same classification (e.g., the glowing outlines of the digits in Figure 7). It's not clear how this is a new type of explanation, as opposed to a different way of finding a \"sufficient input subset,\" which is a known concept. The paper's \"why regress\" justification was hard to connect to the actual process, which seems to be more about iterative feature removal than iteratively deepening an answer.\n\n3. **Mismatch Between “P-Fixed Point” Definition and Experimental Outcomes**: A major contribution seems to be the \"P-fixed point explanation\" (Definition 2.4), which acts as a \"certificate\" that a property $P$ (like correct classification) holds at every step. This is presented as a desirable outcome. However, the empirical results often show this property failing.\n    \n    (a) In Fig. 4, the classification shifts from Sneaker $\\rightarrow$ Sandal $\\rightarrow$ Sneaker during recursion.\n\n    (b) In Fig. 3, it changes from Airplane $\\rightarrow$ Bird.\n\n    If the main goal of the P-fixed point concept is to maintain such properties, then its failure undermines its practical utility. While the authors argue these failures are “insights,” it remains unclear what actionable interpretability insight is gained when the defining property of the explanation does not hold. Clarifying whether instability is expected (and why) would make the framework’s intent clearer.\n\n4. **Questionable conclusions from SAE experiments:** The paper applies its recursive method to Sparse Autoencoders (SAEs) and finds that the LLM's correctness drops to zero. The paper then claims this poses serious issues with the SAEs themselves and their ability to explain the model. I am not sure this conclusion is justified. The FPE method involves taking the SAE's reconstruction, $z_k$, and feeding it back into the SAE to get $z_{k+1}$. This recursive process seems to push the hidden activations far away from their original state (as shown by the very low Jaccard similarity and high KL divergence in Table 2). It's possible this failure isn't an \"explanatory weakness\" of the SAE, but simply an artifact of using the SAE in a way it was never trained for (i.e., on inputs that don't come from the LLM's actual hidden states)."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YsY2JMsWxx", "forum": "Xrf3ypaNpt", "replyto": "Xrf3ypaNpt", "signatures": ["ICLR.cc/2026/Conference/Submission11979/Reviewer_6LhJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11979/Reviewer_6LhJ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762567808104, "cdate": 1762567808104, "tmdate": 1762922972470, "mdate": 1762922972470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}