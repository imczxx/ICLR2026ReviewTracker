{"id": "wmiEXNEXPs", "number": 17429, "cdate": 1758275922260, "mdate": 1759897175885, "content": {"title": "ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack", "abstract": "Large language models (LLMs), despite being safety-aligned, exhibit brittle refusal behaviors that can be circumvented by simple linguistic changes.\nAs tense jailbreaking demonstrates that models refusing harmful requests often comply when rephrased in past tense, a critical generalization gap is revealed in current alignment methods whose underlying mechanisms are poorly understood.\nIn this work, we introduce Activation-Scaling Guard (ASGuard), an insightful, mechanistically-informed framework that surgically mitigates this specific vulnerability.\nFor the first step, we use circuit analysis to identify the specific attention heads causally linked to the targeted jailbreaking, the tense-changing attack.\nSecond, we train a precise, channel-wise scaling vector to recalibrate the activation of tense vulnerable heads.\nLastly, we apply it into a ``preventative fine-tuning\", forcing the model to learn a more robust refusal mechanism.\nAcross three LLMs, ASGuard effectively reduces the attack success rate of targeted jailbreaking while preserving general capabilities and minimizing over refusal, achieving a Pareto-optimal balance between safety and utility.\nOur findings underscore how adversarial suffixes suppress the propagation of the refusal-mediating direction, based on mechanistic analysis.\nFurthermore, our work showcases how a deep understanding of model internals can be leveraged to develop practical, efficient, and targeted methods for adjusting model behavior, charting a course for more reliable and interpretable AI safety.", "tldr": "", "keywords": ["Safety", "Interpretability", "Circuit", "Multi-Head Attention", "Scaling", "Jailbreak Guard"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e6b3a5d34942cb581f9cea300e0cd38c11c3bbb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies a safety flaw called tense jailbreaking, where rephrasing harmful prompts in the past tense bypasses model refusal. It proposes ASGUARD, a three-step mechanistic patch:\n(1) build circuits with edge attribution patching and integrated gradients (EAP-IG) to locate attention heads active only in successful tense jailbreaks;\n(2) learn channel-wise activation scalers to steer those heads toward safe refusals;\n(3) run preventative fine-tuning while freezing scalers so the model internalizes safety, then remove them.\nExperiments on Llama-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, and Gemma-2-9B-it show large drops in attack success with little utility loss. Linear probes reveal that several heads act as tense detectors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Combining circuit discovery with scaling and using the scalers as a temporary scaffold during fine-tuning is novel. It goes beyond standard post-hoc steering by making the intervention teachable and later removable.\n- Experiments cover three instruction-tuned models and compare against SFT, DPO, Circuit Breaker, and Representation Bending. The paper also introduces a robustness composite R-Score and reports raw metrics on OR-Bench-Toxic, OR-Bench-Hard, and MMLU. The Pareto plots are informative.\n- The paper is clearly structured and easy to follow."}, "weaknesses": {"value": "- In Table 2, all models show heavily degraded performance on OR-Bench-Hard. **It is possible that the reduction in Past-Tense ASR mainly results from an increased tendency to refuse responses**. This undermines the purpose of updating only the tense-vulnerable attention circuits. An analysis controlling for over-refusal rates would be very helpful.\n- The study focuses **solely on past-tense reformulations**. It does not show transfer to other semantic variants that commonly appear in real jailbreaks, such as future or hypothetical moods, passive voice, or non-English prompts. The work would have greater impact if it could generalize this approach to more attack types.\n- Circuit discovery with EAP-IG involves many integration steps, multiple refusal templates, and top-n pruning, which can be **computationally heavy** compared to baselines. The paper should include an analysis of EAP-IG’s cost and accuracy for readers less familiar with the method.\n- **Some baselines may be under-tuned or incomplete**. DPO uses only one epoch, while Circuit Breaker and Representation Bending use fixed configurations per model, and result counts vary. Because safety fine-tuning is sensitive to hyperparameters, stronger or more systematically tuned baselines would improve fairness.\n- The paper **lacks a reproducibility statement and open-source code**. Since ASGUARD involves complex interpretability tools and custom fine-tuning procedures, the results are difficult to verify."}, "questions": {"value": "- The SFT(30/70) model appears competitive but sacrifices robustness. This might result from catastrophic forgetting due to full fine-tuning on small datasets. Would using LoRA mitigate this issue? Since ASGUARD fine-tunes only a small subset of parameters, this may explain why it maintains overall performance.\n    \n- While the paper reports MMLU scores to assess utility retention, MMLU does not involve tense understanding. If updates to tense-related circuits affect the model’s handling of tense more broadly, how could that degradation be measured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rWhae1Qo85", "forum": "wmiEXNEXPs", "replyto": "wmiEXNEXPs", "signatures": ["ICLR.cc/2026/Conference/Submission17429/Reviewer_HMBL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17429/Reviewer_HMBL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761035079834, "cdate": 1761035079834, "tmdate": 1762927322314, "mdate": 1762927322314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors provide a new way of mitigating specific attack points of an LLM. They identify key components to the past-tense attacks, learn suppression coefficients and then finetune with the suppressed model to increase its robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The approach seems to be effective against the past tense attack\n- It is evidence for the circuit identification"}, "weaknesses": {"value": "- The technique is very attack-specific. It is unclear how effective the finetuning will be against any other attack. In that sense, the research is quite limited in my opinion. If it does not generalize, then it is a very expensive way of finetuning since one cannot just add data from other attacks but has to do all the different steps again. \n- Because of the previous point, the Pareto frontier data is a bit less relevant since the reduction in ASR is only against a single attack. I would like to ask the authors to add a range of other attacks to provide some data for the generalization of the technique\n- If it does not work well, then I believe the paper is still somewhat relevant, but it should be rephrased towards not being a defense technique but rather evidence for the usefulness of mechanistic interpretability (with a few more findings on the mechanisms, it could be very interesting)\n- The insight of this paper that attention heads only signal the tense and then the harmfulness evaluation will be later could use more experiments or arguments. To me, it feels a bit unsupported and is only drawn as a conclusion based on other work"}, "questions": {"value": "I would like to understand the finetuning step a bit more. \nAs I understand it, the coefficients disable the \"tense\" heads to signal the tense. Then the model is finetuned and the coefficients are detached. To me, it seems like you are helping the model not to be susceptible to this attack during finetuning and enabling this attack path again after it. Hence, I am a bit confused about why this works. What is your justification for this? Can you provide some evidence that this is actually the case (e.g., only using a few coefficients)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ktjYJn6Me2", "forum": "wmiEXNEXPs", "replyto": "wmiEXNEXPs", "signatures": ["ICLR.cc/2026/Conference/Submission17429/Reviewer_KbDQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17429/Reviewer_KbDQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759697541, "cdate": 1761759697541, "tmdate": 1762927321823, "mdate": 1762927321823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ASGuard, a mechanistically-informed defense against tense jailbreaking attacks in LLMs. The method: (1) uses circuit analysis to identify \"tense vulnerable\" attention heads causally linked to the attack, (2) trains channel-wise scaling vectors to suppress these heads, and (3) applies \"preventative fine-tuning\" to learn robust refusal mechanisms. Across 3 models and multiple benchmarks, ASGuard reduces tense jailbreaking ASR from 42-51% to 8-19% while maintaining better safety-utility balance than baselines like SFT, DPO, and Circuit Breakers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong mechanistic foundation & insights: The circuit analysis approach (EAP-IG) to identify vulnerability-specific heads is principled, well-studied in literature, and provides interpretable insights. The linear-probe validation (§6.1) provides mechanistic confirmation that these heads encode tense information. Furthermore, the finding that alignment is localized to specific routing paths (many vulnerable heads disappear post-ASGuard, §6.2) is an important contribution to understanding LLM safety mechanisms.\n\n2. Novel defense paradigm: The authors propose the \"Preventative fine-tuning\" (training with scaling vectors attached, then removing them), which is to my knowledge creative and shows clear improvements over naive scaling or standard fine-tuning alone.\n\n3. Good empirical results (but unsure if there are still competive to the baseline that I’ll mention below): ASGuard achieves Pareto-optimal safety-utility trade-offs across all three models tested, substantially outperforming strong baselines, including well-known methods such as circuitbreaker."}, "weaknesses": {"value": "1. Extremely Limited Scope\nThe method is designed for exactly one attack type (tense jailbreaking). There is no evidence it generalizes to: other semantic attacks (negation, translation, role-play, etc.), adversarial suffixes (GCG, AutoDAN), multi-turn jailbreaks.\nI think this is a critical weakness as the baselines that are used by authors are evaluated on different types of attacks. It further shows somehow an unfair comparison. \n\n2. Circular Methodology: the detection dataset (§3.1) and evaluation dataset both use JBB-Behaviors prompts, creating a potential unreliable evaluation. Indeed, 100 prompts are used for circuit construction and the same prompts (with different reformulations) seem to be used for evaluation, exposing the risk of overfitting to these specific prompts.\n\n3. Limited Baseline Comparisons\n\nThere are missing well-known defenses: Representation Engineering (Turner et al., 2023) is cited but not compared. Prompt-based defenses (system prompts, few-shot examples) and Input filtering or paraphrasing are not evaluated. Authors should also compare their method against RFA (Yu et al, 2024) \"refusal feature adversarial training\" and Gradient Cuff. \n\nFurthermore, there seem to be some issues with the RepBend comparison: RepBend is described as \"recent SoTA\" but it's surprising that  it doesn't consistently outperform simpler methods.  Some results show RepBend > ASGuard on specific metrics (e.g., Llama Toxic: 96.1 vs 96.4)."}, "questions": {"value": "1. :Can this framework defend against attacks other than tense jailbreaking? If not, how is it more than a specialized patch?\n\n2. What are the computational costs relative to standard fine-tuning? Can you ablate on detection dataset size? \n\n3. If you train defenses for multiple attack types, do they interfere? Can one set of scaling vectors handle multiple attacks?\n\n4.Why does preventative fine-tuning work? Can you provide formal analysis or at least deeper mechanistic explanation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qjpyYxG1kh", "forum": "wmiEXNEXPs", "replyto": "wmiEXNEXPs", "signatures": ["ICLR.cc/2026/Conference/Submission17429/Reviewer_PzPS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17429/Reviewer_PzPS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972320571, "cdate": 1761972320571, "tmdate": 1762927321228, "mdate": 1762927321228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ASGUARD, an activation-scaling method to mitigate targeted jailbreaks like tense-based attacks. It identifies vulnerable attention heads via circuit attribution, scales their activations to suppress unsafe behavior, and uses a short preventative fine-tuning phase to internalize the fix. Experiments across models show lower attack success rates and limited over-refusal while maintaining utility. The approach has demonstrated a practical and interpretable defense to the specific type of attack."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper targets a concrete jailbreak based on tense-based rephrasing and provides an interpretable fix.\n2. The activation-scaling intervention is lightweight, requiring only head-level scaling vectors rather than any weight updates.\n3. The preventative fine-tuning shows that it is feasible to transfer the scaling effect into model parameters, achieving Pareto-optimal results under this compounded method.\n4. The experiments generalize across architectures and model sizes, showing that the internal behavior is replicable and the robustness holds consistently across models."}, "weaknesses": {"value": "1. The attribution step that identifies “vulnerable heads” is largely heuristic. It would be beneficial to demonstrate that the selected heads are causally necessary for the jailbreak behavior. Additional analyses, such as head ablation or random-head controls, would make the causal interpretation more convincing.\n\n2. The method is designed specifically for tense-based jailbreaks, and it is unclear whether the same workflow would generalize to other jailbreak categories. While the paper does not overclaim, the scope still appears limited."}, "questions": {"value": "1. Have you verified whether removing or scaling different random sets of heads produces similar reductions in attack success?\n\n3. Could ASGUARD be extended to other types of jailbreaks, and if so, how would “vulnerable heads” be defined in those cases?\n\n3. How well does the preventative fine-tuning generalize to OOD data?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yrtTvQyB3I", "forum": "wmiEXNEXPs", "replyto": "wmiEXNEXPs", "signatures": ["ICLR.cc/2026/Conference/Submission17429/Reviewer_vndG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17429/Reviewer_vndG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241508363, "cdate": 1762241508363, "tmdate": 1762927320733, "mdate": 1762927320733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}