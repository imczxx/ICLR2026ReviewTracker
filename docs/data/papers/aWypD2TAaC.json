{"id": "aWypD2TAaC", "number": 15308, "cdate": 1758250142033, "mdate": 1763723887241, "content": {"title": "Towards the Generation of  Structured Scientific Vector Graphics with Large Language Models", "abstract": "We address the challenge of automatically visualizing scientific explanations. While prior work has explored large language model (LLM)-based vector graphic generation, existing approaches often overlook structural correctness, a key requirement for valid scientific diagrams. To achieve structurally correct generation, we make three key contributions. First, we introduce SSVG-Bench, a novel benchmark for evaluating the generation of Structured Scientific Vector Graphics. Unlike conventional visual similarity metrics, SSVG-Bench employs task-specific structural analysis for accurate evaluation, and it supports three vector formats: TikZ, SVG, and EPS. Second, we conduct an extensive benchmarking and analysis, revealing key findings such as the crucial role of LLM reasoning in ensuring structural validity. Third, we propose LLM-Oriented Orchestration Prompting (LOOP), a new prompting method that leverages LLMs' reasoning potential by combining familiar subtasks. Experiments demonstrate substantial improvements over existing prompting techniques, suggesting promising directions for scientific diagram generation. We will release our code and benchmark upon acceptance.", "tldr": "We introduce a new benchmark for scientific vector graphic generation, apply it to evaluate recent LLMs, and propose a novel prompting method.", "keywords": ["LLMs", "vision and language", "multimodal", "vector graphics", "image generation", "benchmarking", "prompting"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d092a134291f9833055dde3e92fd2b85fd329fc3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper fills the gap of evaluating the structural correctness of text-guided scientific figure generation, which previous work overlooks. Evaluating structural correctness is not easy to do with fully automated methods. Instead, the authors focus on a specific subset of scientific figures (plane geometry and molecular structure). They derive a benchmark of 410 text-figure pairs and implement rule-based programs that evaluate the correctness of generated outputs. The authors evaluate a range of existing fine-tuned and general-purpose models on this benchmark using three output formats (TikZ, SVG, and EPS). In addition, they introduce a new task-specific prompting method that can help improve performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is very well written and easy to understand.\n* Structural correctness is an important property for the evaluation of scientific figure generation that is often omitted from evaluations due to the difficulty of assessing it. The provided benchmark fills this gap and will be useful for future work.\n* The evaluation compares a wide range of models across three formats, which provides interesting insights.\n* The provided prompting technique may be useful for future work and applications."}, "weaknesses": {"value": "* Table 3 is an interesting approximation of potential occurrences in the training data, but a TikZ graphic doesn't necessarily have to start with `\\documentclass[tikz]`.\n* The rule-based programs seem central to the benchmark, yet no details or example snippets are provided in the paper, which would have been insightful for assessing how well they work or whether they are brittle with failure cases.\n* Although the benchmark will be very useful when released, there is very little technical novelty in the paper, and while the introduced prompting technique seems to work well and leads to additional improvements, it is hardly exciting.\n* The authors motivate their benchmark by stating that human evaluation doesn't scale well (l.129), but the creation of the benchmark still requires heavy manual curation (l.209ff), so it still doesn't scale well.\n* The performance of fine-tuned models (AutomaTikZ, TikZero) seems surprisingly low. Have the authors ensured that the provided prompts are in the format the models expect? At least the example prompts provided in the appendix are not in the correct format."}, "questions": {"value": "* In l.242, it says that parts of the output are provided to the models as input. How is this done exactly, as this is not clear from the prompt examples in the appendix? Furthermore, how is this provided for models that are fine-tuned and do not accept general-purpose prompts?\n* Why are the scores of the TikZ models in Table 4 provided in the SVG column?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pFeo9sessP", "forum": "aWypD2TAaC", "replyto": "aWypD2TAaC", "signatures": ["ICLR.cc/2026/Conference/Submission15308/Reviewer_r4TZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15308/Reviewer_r4TZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714841767, "cdate": 1761714841767, "tmdate": 1762925607121, "mdate": 1762925607121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **SSVG-Bench**, a benchmark to evaluate **structural correctness** in scientific vector-graphics generation (TikZ/SVG/EPS), provides **automatic structure-aware evaluators** for plane geometry and molecular structures, benchmarks many LLMs, and proposes **LOOP**, a prompting workflow that improves accuracy, especially in SVG."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- **Clear problem motivation:** moves beyond visual/code similarity to **structure-aware** evaluation.\n- **New benchmark & tooling:** SSVG-Bench (two tasks; three formats) with **Python scripts for structural checks**\n- **Broad, timely evaluation:** compares fine-tuned TikZ models and recent LLMs; reveals the importance of **reasoning modes**.\n- **Actionable insight on formats:** compelling evidence that **SVG > TikZ/EPS** for LLM reasoning; novel angle likely to influence future work.\n- **LOOP** (information/relationship extraction → reasoning → code) yields **consistent gains** over popular CoT variants.\n- Prompts, task setup, and evaluation logic are described in detail; many examples illustrate successes/failures."}, "weaknesses": {"value": "- **Evaluation blind spots:** plane-geometry scorer does not penalize **extraneous elements**; results may overstate correctness in cluttered outputs.\n- **Bond order ignored:** molecular task collapses bond multiplicity; risks awarding correctness to chemically different graphs.\n- **Potential data leakage:** plane-geometry items sourced from Wikipedia/SVG Commons; large web-trained LLMs may have seen near-identical diagrams/captions.\n- **Scope & scale:** overall size (1,230 items) is moderate; per-topic diversity (e.g., non-Euclidean, circuits, algorithmic flowcharts) could be broader.\n- **Ablations on LOOP:** helpful but could be deeper (e.g., remove each stage, vary decomposition granularity, measure latency/cost)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "t1qQRJjvG6", "forum": "aWypD2TAaC", "replyto": "aWypD2TAaC", "signatures": ["ICLR.cc/2026/Conference/Submission15308/Reviewer_vtgB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15308/Reviewer_vtgB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756589801, "cdate": 1761756589801, "tmdate": 1762925606737, "mdate": 1762925606737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addressed the structural correctness issue in the LLM's vector graphics generation. It provides a benchmark called SSVG-Bench consisting of two types of tasks, plane geometry task and molecular structure task, along with novel evaluating scripts to evaluate the structural correctness of the generated images. It performs a comprehensive benchmarking and analysis of existing models on the proposed benchmark, revealing the poor performance of LLM and key feature that might enhance a LLM's capability of generating correct vector graphics. Finally, it proposed LLM-Oriented Orchestration Prompting (LOOP), a method that enhances the accuracy of vector graphics generation in LLM. Experiments result shows that the proposed LOOP can improve the performance in terms of structural correctness on it's proposed benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed evaluation method is novel. Instead of using common metrics in computer vision, the author proposed using scripts to evaluate the correctness of scientific vector graphics. This makes the evaluation process more explainable and accurate.\n2. The evaluation in Table 4 is comprehensive, covering all recent SOTA models. \n3. There's a significant improvement in performance with the proposed LOOP strategy."}, "weaknesses": {"value": "1. The diversity of the proposed benchmark is greatly limited. It only contains two kinds of tasks, plane geometry task (with only 5 elements) and molecular structure task. Both tasks have a very clear and fixed path to solve, and therefore, might not be able to test the model's generalizability on all other tasks requiring structural correctness. \n2. Table 1 missed lots of recent benchmark (even those benchmark are mentioned in the text from L105-L107, and lots of them are larger than the proposed benchmark). For example, the generation evaluation suite of VGBench contains 5845 instances in total, and SVGEditBench has 1366 instances in total. Not comparing them in Table 1 is unfair. \n3. Molecular structure is an extremely specialized task. It not only evaluates the ability to generate graphics, but also evaluate the model's understanding of IUPAC name and the model's chemistry knowledge. The requirement for such specialized knowledge presents a bias so that the results will be better for models with chemistry knowledge than models without, while the chemistry knowledge is not usually related to the model's ability to generate vector graphics in general. \n4. There is no evaluation on the proposed LOOP strategy's performance on other vector graphics generation benchmark. Evaluating it on other commonly used benchmark will ensure the method's generalizability."}, "questions": {"value": "1. How to ensure the robustness of the Python script for Pattern 2? Are we using a different script for each case or using the same script for all cases?\n2. In L241, it's mentioned that \"In Pattern 1, the correct output can be uniquely determined\", Why it can be uniquely determined? Even with the given element (in black), the remaining element (in red) can still have multiple ways of expression. For example, the circle can be represented as circle using `<circle>`, or represented as curve using `<path>` in SVG."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kOK9b8GTgZ", "forum": "aWypD2TAaC", "replyto": "aWypD2TAaC", "signatures": ["ICLR.cc/2026/Conference/Submission15308/Reviewer_8knp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15308/Reviewer_8knp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875067893, "cdate": 1761875067893, "tmdate": 1762925606289, "mdate": 1762925606289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduced SSVG-Bench, a benchmark for generating structured scientific vector graphics from text, covering plane geometry and molecular structures across three vector formats (TikZ, SVG, and EPS). It applied Python-based automatic accuracy evaluation and reports results for multiple LLMs. The paper also proposed a prompting method, LOOP, which yields measurable gains on the benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear, well-structured writing with concrete, illustrative examples that make the task and setup easy to follow.\n2. The automated molecular evaluation via graph isomorphism is sensible.\n3. Broad coverage of vector formats (TikZ, SVG, EPS) and a diverse model suite, comparing both reasoning and non-reasoning LLMs.\n4. The proposed prompting method (LOOP) is simple to implement and yields consistent gains."}, "weaknesses": {"value": "W1. Insufficient technical detail for automatic plane-geometry evaluation (Pattern 1/2): missing normalization pipeline, tolerance settings, red-element extraction, alignment strategy, and failure modes. This undermines the trustworthiness and reproducibility of the reported Accuracy.\n\n\nW2. Table 4 inconsistency: AutomaTikZ and TikZero+ are trained on TikZ, yet the TikZ column is empty while SVG numbers are reported. Is this a mistake or a data-availability issue? Please clarify/correct.\n\n\nW3. Over-reliance on a single binary metric (Accuracy): no complementary automatic metrics or human evaluation, leading to an overly coarse assessment; near-misses and completely wrong outputs are both scored 0. Consider adding metrics used in related work TikZero+ (e.g., DreamSim, KID, CLIPScore, code-level CrystalBLEU and TEX Edit Distance, Mean Token Efficiency) and geometry-specific measures (e.g., how many elements are correctly covered?)\n\n\nW4. Narrow evaluation of the prompting strategy: results are shown primarily on GPT/Gemini and mostly as Accuracy; broader model coverage and multi-metric reporting are needed.\nLack of human evaluation: no user study to validate automatic metrics, resolve borderline cases, or assess readability/usability of the generated diagrams."}, "questions": {"value": "Q1. Pattern taxonomy: How exactly categorize plane-geometry items into Pattern 1 (unique solution) vs Pattern 2 (multiple valid solutions), and what is the split (%) across the 110 examples?\nQ2. Pattern 2 evaluation cost: Does Pattern 2 require case-specific code per instance? \nQ3. Prompting generality: LOOP appears to be a fixed “think” scheme—do its gains differ between reasoning and non-reasoning models? Q4. Please provide results on more models and with more metrics (as above)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xoo1JZkrtp", "forum": "aWypD2TAaC", "replyto": "aWypD2TAaC", "signatures": ["ICLR.cc/2026/Conference/Submission15308/Reviewer_Q9nD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15308/Reviewer_Q9nD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940601381, "cdate": 1761940601381, "tmdate": 1762925605912, "mdate": 1762925605912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}