{"id": "8eYOBBgP05", "number": 4707, "cdate": 1757750569957, "mdate": 1759898018734, "content": {"title": "Kairos: Towards Adaptive and Generalizable Time Series Foundation Models", "abstract": "Time series foundation models (TSFMs) have emerged as a powerful paradigm for time series analysis, driven by large-scale pretraining on diverse data corpora. However, time series inherently exhibit heterogeneous information density over time, influenced by system states and signal complexity, presenting significant modeling challenges especially in a zero-shot scenario. Current TSFMs rely on non-adaptive processing pipelines that fail to capture this dynamic nature. For example, common tokenization strategies such as fixed-size patching enforce rigid observational granularity, limiting their ability to adapt to varying information densities. Similarly, conventional positional encodings impose a uniform temporal scale, making it difficult to model diverse periodicities and trends across series. To overcome these limitations, we propose Kairos, a flexible TSFM framework that integrates a dynamic patching tokenizer and an instance-adaptive positional embedding. Kairos adaptively selects tokenization granularity and tailors positional encodings to the unique characteristics of each time series instance. Trained on a large-scale Predictability-Stratified Time Series (PreSTS) corpus comprising over 300 billion time points and adopting a multi-patch prediction strategy in the inference stage, Kairos achieves superior performance with much fewer parameters on two common zero-shot benchmarks, GIFT-Eval and the Time-Series-Library benchmark, consistently outperforming established methods across diverse tasks.", "tldr": "", "keywords": ["Time series modeling", "Time series forecasting", "Time series foundation model"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be4682720ab589d4de0425482c8adc005bdd93bd.pdf", "supplementary_material": "/attachment/118d5cf52e0ed4b28a124d04e659919014b4902f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Kairos, a new Time Series Foundation Model designed to handle heterogeneous and dynamic time series data. It introduces two key components: Mixture-of-Size Dynamic Patching, which adaptively adjusts tokenization granularity, and Instance-Adaptive Rotary Position Embedding, which customizes positional encoding for each instance. Kairos also adopts a multi-patch prediction strategy and leverages a large PreSTS corpus (300B points) for pretraining. Experiments on GIFT-Eval and TSLib show strong zero-shot forecasting performance with fewer parameters than existing models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of adaptive mechanisms, such as dynamic patching and instance-specific positional encoding, represents an impressive and well-motivated approach to address several key limitations present in existing time-series foundation models.\n\n2. The model exhibits consistently strong zero-shot forecasting performance across a diverse set of benchmarks, demonstrating its robustness and generalizability to various time-series tasks.\n\n3. Remarkably, it achieves competitive‚Äîand in some cases even superior‚Äîresults, all while using a substantially smaller number of parameters compared to existing models, highlighting its efficiency and scalability."}, "weaknesses": {"value": "1. The use of ùëÉ = {32,64,128}, k=3 and z=2 indicates that the proposed dynamic tokenization strategy is not genuinely flexible. In practice, the method behaves very similarly to a fixed tokenization with P=32, where different patches are merely assigned different weights. While the approach may appear more sophisticated at first glance, its operations are ultimately limited to combining a small set of predefined patch sizes, which constrains its adaptability and overall novelty.  A truly dynamic tokenization should produce patches of arbitrary lengths. Ideally, it should generate very long patches in smooth regions and patches whose lengths are roughly comparable to the fluctuation period in regions with rapid changes, rather than being limited to a few predefined lengths such as 32, 64, or 128.\n\n2. The function and purpose of the null expert remain unclear. Specifically, it is not evident what contributions it makes to the model‚Äôs performance, and no ablation study has been presented to quantify its effects or justify its inclusion.\n\n3. The proposed IARoPE method lacks in-depth theoretical analysis or thorough empirical investigation that would clarify why it performs effectively. As a result, the source of its improvement and the underlying intuition behind its success are insufficiently explained, leaving readers without a strong understanding of its operational principles.\n\n4. While the experiments provide a comparison between Kairos and Sundial on the GIFT-Eval benchmark, they omit a corresponding evaluation on the TSLib benchmark. This absence of broader benchmarking reduces the strength of the empirical validation and makes it harder to assess the generalizability of the proposed method across diverse datasets."}, "questions": {"value": "1. Is the null expert‚Äôs s‚Äòn initialized randomly, or is there a specific initialization strategy that influences its behavior?\n\n2. Could the FFT-based scaling used in IARoPE become unstable when applied to noisy or non-stationary signals? Additionally, have you conducted any experiments to assess its robustness in the presence of outliers or spectral distortions?\n\n3. How computationally demanding is the MoS-DP routing and fusion process in comparison to fixed-patch baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZTAmYQIxKa", "forum": "8eYOBBgP05", "replyto": "8eYOBBgP05", "signatures": ["ICLR.cc/2026/Conference/Submission4707/Reviewer_nXjk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4707/Reviewer_nXjk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761291648958, "cdate": 1761291648958, "tmdate": 1762917527876, "mdate": 1762917527876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this study, the authors investigate the tokenisation of time series in the context of forecasting. In particular, they propose (a) Mixture-of-Size Dynamic Patching (MoS-DP) to obtain patches of varying size, depending on the information density of the time series, and (b) Instance-adaptive Rotary Position Embedding (IARoPE) to adequatly encode such patches of varying size with positional information. The authors evaluate their method across two benchmarks and conduct an ablation study to assess the importance of MoS-DP and IARoPE."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper is well structured.\n2) The authors evaluate their method on two established forecasting benchmarks.\n3) The authors conduct ablation studies to provide insights on the effectiveness of the proposed components.\n4) The authors provide their code to support reproducibility."}, "weaknesses": {"value": "Related Works:\n\n1) The authors do not discuss the most recent works in the field of time series tokenisation. For instance, works on domain-specific tokenisation [1] and wavelet-based tokenisation [2] should be included to provide a representative overview of the current literature. \n\nTerminology:\n\n2) The term 'general-purpose time series foundation model (TSFM)' (see line 039) is misleading, as the proposed model is only able to perform forecasting. This is also the case for the title. A more suitable term in the context of this study would be 'foundational forecasting model', since general-purpose time series models such as MOMENT [3] are able to perform classification and regression in addition to forecasting. \n\nMethodology:\n\n3) The authors measure the information density of a time series via spectral entropy. They claim a direct relationship between spectral entropy (SE) and information density (ID), i.e. low SE indicating low ID and high SE indicating high ID, as suggested in Figure 1.b. However, I believe the relationship between ID and SE to be inverse. For example, the ID of a pure sine wave should be higher than the ID of a sine wave overlaid with white noise, which is not the case in Figure 1.b. \n4) Furthermore, the ID of a time series is strongly coupled to the sampling frequency (SF). For instance, ETTm1 and ETTh1 are recordings of the exact same underlying signal, however, ETTm1 has a higher ID since it is recorded at a higher SF. While an interesting approach, the usage of spectral entropy as a measure of information density might not be the best fit. \n5) The overview of the proposed method in Figure 2 is not self-containing and thus hard to follow for the reader. \n6) The proposed method is designed for univariate time series analysis and thus cannot model dependencies between variates. However, previous works [1] have shown that inter-variate dependencies are critical in time series analysis and should be considered during the tokenisation process.\n\nExperiments: \n\n7) The authors compare the proposed tokeniser against several baselines, however, they do not include recent works in the field of time series tokenisation. Particularly, OTIS [1] and WaveToken [2] should be included as baselines to ensure a fair comparison of the proposed method. \n8) The authors argue that the proposed multi-patch prediction strategy 'effectively mitigates the cumulative errors inherent in autoregressive precesses' (see ll. 289-299). However, in Figure 6 (appendix C) they show that predicting only the next two patches achieves best performance. In light of this, the authors seem to overstate the importance of the multi-patch prediction strategy. \n9) The authors ablate the proposed components MoS-DP and IARoPE in Section 4.3. While their work primarily focuses on how \"fixed-size patching enforce rigid observational granularity, limiting their ability to adapt to varying information densities\" (see ll 17-19), the results presented in the last two rows of Table 2 suggest otherwise. Specifically, the proposed dynamic patching tokeniser MoS-DP yields only marginal improvements of 0.68%, 0.13%, and 0.12% across short-, medium-, and long-term prediction horizons, respectively. This modest gain raises the question of whether fixed-size patching truly constitutes a limitation of current methods in the first place.\n10) The authors analyse the proposed IARoPE in Section 4.4.2 and show that Intra-Dataset shuffle performs better than the original RoPE, which seems to be counter-intuitive and requires further elaboration.\n\nDiscussion:\n\n11) The discussion of the experiments is limited. The experiments section only provides numbers without their interpretation, while the conclusion section only provides a brief summary of the study.\n12) The authors do not discuss the limitations of the study in the main manuscript. \n13) The authors do not discuss topics relevant for future work in the main manuscript.\n\n___\n[1] Turgut, √ñ. et al. \"Towards generalisable time series understanding across domains.\" arXiv preprint arXiv:2410.07299 (2024).\n\n[2] Masserano, L. et al. \"Enhancing Foundation Models for Time Series Forecasting via Wavelet-based Tokenization.\" ICML (2025).\n\n[3] Goswami, M. et al. \"MOMENT: A Family of Open Time-series Foundation Models.\" ICML (2024)."}, "questions": {"value": "1) Could the authors visualise examples of the synthetic data included in the pre-training corpus?\n2) How do the authors determine the perfect balance between real-world and synthetic data used for pre-training?\n3) What is the advantage of infusing synthetic data into the pre-training corpus? The authors state that 'PreSTS comprises real-world time series spanning multiple domains and is augmented by a synthetic dataset [...] to ensure comprehensive coverage' (see ll. 320-321). However, the authors do not explain how 'comprehensive coverage' is defined or measured in this context.\n4) Could the authors evaluate their method against multi-variate approaches such as [1]? This would enable a comparison of different tokenisation approaches that address the problem of varying temporal dependencies across domains. For instance in [1], the authors introduce domain-specific positional embeddings for variates and show that these embeddings can successfully capture temporal dependencies (see Figure 11).\n5) Have the authors experimented with any other measure than spectral entropy to measure the information density of a time series? \n___ \n[1] Turgut, √ñ. et al. \"Towards generalisable time series understanding across domains.\" arXiv preprint arXiv:2410.07299 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l48DqhLwMO", "forum": "8eYOBBgP05", "replyto": "8eYOBBgP05", "signatures": ["ICLR.cc/2026/Conference/Submission4707/Reviewer_WLGx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4707/Reviewer_WLGx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682087046, "cdate": 1761682087046, "tmdate": 1762917527302, "mdate": 1762917527302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a time series foundation model (TSFM) that introduces an adaptive input embedding / tokenization scheme, called MoS-DP. Moreover, the authors introduce a modified positional embedding, called IARoPE, based on the well-known RoPE scheme. The authors then demonstrate their network performance on common time series leaderboards, such as GIFT-Eval and TSLib."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a timely problem of tokenization in the time series domain. Most architectures try to overtake input embedding schemes from the NLP domain, e.g., patching techniques, but it is well-known in this field that this formulation might be limited and not ideal. Thus the efforts to develop new approaches is important. Dynamically combining patches to form differently-sized patches sounds reasonable.  \n\nAlso, the approach from the authors to enhance the RoPE is a good idea and especially important for their transformer backbone.\n\nI appreciate the evaluation on standard leaderboard benchmarks in this field, where their model shows competitive performance."}, "weaknesses": {"value": "The description of the MoS-DP feels **overly complicated and counter-intuitive**:\\\nThe authors mention that the aim is to \"yield the embedding for each finest patch by fusing\ninformation from multiple granularities\". However, if that is the goal, the more intuitive way would be to actually start from the fines resolution, i.e., the individual time steps and then fuse more and more information together to form larger patches eventually. It is unclear how splitting the initial coarse patch into finer patches would \"fuse information from multiple granularities\". Furthermore, the approach that the authors took also requires the user to choose the largest granularity p_S, which may not be easy as it already requires good knowledge about the time series at hand. The illustration in Figure 4 is not quite clear. It would help to overlay directly the selected size of the finest patch p_k for the regions. This way, the reader should see immediately that smaller patches are considered by the network in \"interesting\" areas, such as the ones with high fluctuation or rapid changes. Furthermore, the description of the patch fusion and the Figure 2 illustrating it feel overly complicated. In Figure 2, the information flow is from the bottom to the top. However, in the dynamic patch fusion phase, the coarse patch is broken up into smaller pieces, hence shouldn't the larger green patch be at the bottom and then the splits be towards the top? Also, the coloring scheme is confusing, both across the entire figure (inappropriate color reusing) and within the patch fusion as well (Expert 1 is not even selected for the patch on the right and Expert 1 in green is not chosen in both of the show patches, but highlighted on top).\n\nThe adjustment of the RoPE idea to be specific to the current instance sounds reasonable and the idea to use the core frequency components of the time series to do that also makes sense. However, the specific strength of the original RoPE approach was to be able to generalize to seamlessly generalize to various sequence lengths. It feels that with the IARoPE, this crucial benefit may be lost? Why is so much emphasis put on RoPE then, instead of using a custom position embedding based on the components that the authors extract from the time series?\n\nThe literature review part is very weak and not well written:\\\nSince the paper focuses so much on the input tokenization, there are several papers left unmentioned / undiscussed. For example, the PatchTST architecture, which introduces the central component of this paper, the patches, in the first place is not discussed. Other works, that also employ dynamic patching, such as the TTM, is just mentioned on passing and in a different context even. Finally, recent works that employ non-transformer-based architectures and thus don't necessarily require patching of positional encodings, such as TiRex or FlowState, are not discussed at all.\n\n[1] Nie, Yuqi, et al. \"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers.\" arXiv, 27 Nov. 2022, doi:10.48550/arXiv.2211.14730.\n[2] Ekambaram, Vijay, et al. \"Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series.\" arXiv, 8 Jan. 2024, doi:10.48550/arXiv.2401.03955.\n[3] Auer, Andreas, et al. \"TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning.\" arXiv, 29 May. 2025, doi:10.48550/arXiv.2505.23719.\n[4] Graf, Lars, et al. \"FlowState: Sampling Rate Invariant Time Series Forecasting.\" arXiv, 7 Aug. 2025, doi:10.48550/arXiv.2508.05287.\n\nKAIROS does not appear to be a zero-shot model for the GIFT-Eval:\\\nAlthough the authors claim to \"have excluded all test sets found within the GIFT-Eval benchmark\", from the list of datasets used in Appendix F.1, this seems to be not true at all. For example, Loop Seattle, M_DENSE, Solar, KDD Cup 2018, M4 Hourly, and several other datasets are clearly marked as being part of the GIFT-Eval benchmark in Table 13 of [5] and are not within the \"allowed\" Pretraining datasets listed in Table 14 of said paper. This can make a huge difference.\n\n[5] Aksu, Taha, et al. \"GIFT-Eval: A Benchmark For General Time Series Forecasting Model Evaluation.\" arXiv, 14 Oct. 2024, doi:10.48550/arXiv.2410.10393."}, "questions": {"value": "Regarding the MoS-DP: \nWhy was the approach chosen to start from the coarsest granularity and go to the finest and not from the finest to the coarsest?\\\nDoes it work to choose the p_S to be the entire time series?\\\nPlease revise Figure 2. The coloring scheme is confusing and the illustration of the MoS-DP may not be accurate.\n\nRegarding the IARoPE:\nCan the IARoPE still generalize to varying sequence lengths?\n\nRegarding GIFT-Eval:\nPlease clarify whether indeed the datasets in Table 5 have been used for pretraining and if so, that the KAIROS model is NOT a zero-shot model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l9N72pp6ly", "forum": "8eYOBBgP05", "replyto": "8eYOBBgP05", "signatures": ["ICLR.cc/2026/Conference/Submission4707/Reviewer_6umn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4707/Reviewer_6umn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903028037, "cdate": 1761903028037, "tmdate": 1762917526554, "mdate": 1762917526554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The presented study introduces Kairos, a time series foundation model that aims to robustly process time series data with varying information density. To achieve this, the work makes four contributions. First, it introduces mixture-of-size dynamic patching, which uses a mixture-of-experts mechanism to dynamically decide at which resolution patches should be extracted. Second, it introduces an instance-adaptive version of the established rotary positional embeddings. Third, it proposes the use of a multi-token prediction window during model training. Finally, it introduces a curated time series dataset with over 300 billion time points together with a quality metric to preferentially sample high-quality data. Kairos' time series forecasting ability is tested using two public benchmarks, GIFT-Eval and TSLib. It is shown to slightly outperform a range of baselines while having a smaller number of model parameters. Additionally, ablation studies demonstrate the benefit of the dynamic patching and altered position embedding mechanism, while also providing insight into their functioning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The use of a mixture-of-expert mechanism is an elegant solution to dynamically extract embeddings with variable patch size while also providing insights into the model‚Äôs functioning.\n\n- Improvements of foundation models in other domains are partially driven by the use of larger, quality-controlled datasets. As such, the curated PreSTS dataset is relevant to the scientific community.\n\n- By evaluating their method on two publicly available benchmarks the, the reported results can be transparently judged by readers.\n\n- The manuscript is clearly structured, very well written with clean mathematical notation, and nicely illustrated."}, "weaknesses": {"value": "- While it is apparent that time series from different domains exhibit patterns across diverse temporal scales, I am not fully convinced that this heterogeneity must be addressed explicitly at the feature-encoder level. In my view, a foundation model could instead aim to learn a comprehensive repertoire of feature extractors that capture fundamental time series characteristics, such as slopes, periodicities, and bursts, across multiple scales. These features could then be contextualized and combined within subsequent transformer layers. Ultimately, robustness to varying time scales might emerge from training on large and diverse datasets, rather than from architectural specialization. This mirrors the behavior of foundation models in other domains: for example, in computer vision, convolutional networks learn to represent objects at different scales and resolutions using a shared set of feature detectors, without domain-specific encoders.\n\n- Similarly, I struggle to see the need for adaptive positional embedding. In my understanding the positional embedding is primarily needed to order the tokens while the internal processing of the transformer can handle any contextualization of the processed tokens.\n\n- The proposed method is only evaluated for its time series forecasting ability. The performance in other tasks that a foundational time series model should typically solve, such as classification or imputation, is not reported.\n\n- The proposed method fails to convincingly beat several included baselines, most noticeable the Toto and Sundial models. I do not believe that parameter count is overly relevant in a foundation model setting. Additionally, it is placed behind several others on the public GIFT-Eval leaderboard (https://huggingface.co/spaces/Salesforce/GIFT-Eval), including older methods (e.g. Das et al. \"A decoder-only foundation model for time-series forecasting.\" International Conference on Machine Learning. PMLR, 2024)."}, "questions": {"value": "- While the multi-token prediction window makes intuitive sense, I believe a similar concept has been proposed by several others including work by Lim et al. (Lim, Bryan, et al. \"Temporal fusion transformers for interpretable multi-horizon time series forecasting.\" International journal of forecasting 37.4 (2021): 1748-1764) and Zhou et al. (Zhou, Haoyi, et al. \"Informer: Beyond efficient transformer for long sequence time-series forecasting.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 35. No. 12. 2021). The authors should comment on the differences to these works or adjust the text to acknowledge them.\n\n- I do not understand the distinction that the related Pathformer and ElasTST models ‚Äúare designed for dataset-specific training rather than as foundation models,‚Äù and lack ‚Äúlack cross-domain generalization capabilities.‚Äù It appears to me as if these models are trained and tested on time series data from a wide range of domains.\n\n- I was surprised to learn that authors merely use three different patch sizes, 32, 64 and 128, for their proposed dynamic patching mechanism. At such a low number of different patch sizes, it might be much simpler to train three feature extractors and fuse their embeddings, in line with the previous Pathformer and ElasTST works.\n\n- In the reproducibility statement, the authors laudably promise to make the source code and model weights publicly available without providing a link to a code repository yet. Additionally, they should clarify whether they plan to publish the PreSTS dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Cuy7UooOL", "forum": "8eYOBBgP05", "replyto": "8eYOBBgP05", "signatures": ["ICLR.cc/2026/Conference/Submission4707/Reviewer_miCT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4707/Reviewer_miCT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951998798, "cdate": 1761951998798, "tmdate": 1762917526245, "mdate": 1762917526245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}