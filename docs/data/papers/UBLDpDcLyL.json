{"id": "UBLDpDcLyL", "number": 13704, "cdate": 1758221190241, "mdate": 1759897418430, "content": {"title": "FleetAgent: Natural Language Driving Explanation and Evaluation for Vehicle Teleoperation", "abstract": "Large-scale driverless fleets rely on teleoperation to resolve rare, safety-critical edge cases that onboard autonomy cannot handle robustly. We introduce FleetAgent, a cloud-hosted multimodal large language model (MLLM) that assesses the plan and context of an autonomous vehicle (AV) to decide whether teleoperation is needed. FleetAgent consumes a compact vectorized representation of observations and planned actions rather than raw sensors, and produces a natural-language explanation and evaluation towards the traffic scenario and driving decision. A dedicated vector encoder replaces conventional text tokenizers and vision encoders, substantially reducing the number of input tokens and server memory footprint while preserving the information needed for proper functioning. We also build a dataset based on nuScenes and augment it with synthetic imperfect driving decisions and annotated explanation and evaluation labels. System-level studies indicate a maximum $625 \\times$ reduction in communication demand and a maximum $16.54 \\times$ reduction in cache size. Model-level experiments also show competitive response quality and plan-evaluation accuracy, with a 41\\% improvement in BLEU score and an 11\\% reduction in task failure rate. Because all computation runs on the cloud, the approach introduces no additional onboard burden. Together, these results outline a practical path to scalable, explainable teleoperation support for AV fleets, paving the way to another paradigm for MLLMs' application in autonomous driving.", "tldr": "", "keywords": ["Vehicle Teleoperation", "LLM", "Autonomous Driving"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/094226ec077961fccd7dd12eb445809bcf2374be.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents FLEETAGENT, a cloud-assisted vision-language framework for fleet-level autonomous vehicle teleoperation. Instead of transmitting high-bandwidth sensor data, each vehicle uploads compact vectorized representations of its environment, including detected objects, dynamic trajectories, and planned maneuvers. A specialized encoder called VECFORMER converts these vectors into embeddings, which are then processed by a large VLM to generate natural-language explanations and intervention scores—deciding when human teleoperators should intervene. To improve efficiency, the system employs a differentiable top-K context selection mechanism (based on Gumbel-Softmax) that identifies the most relevant contextual vectors for reasoning. Experiments on NU-EVAL and VECEVAL datasets show that FLEETAGENT reduces bandwidth by 625×, cuts cache cost by 16.5×, and achieves strong interpretability and safety-intervention accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Innovative problem formulation: The paper defines a new and practically relevant task—language-based teleoperation reasoning from vectorized driving data—bridging large-scale autonomy and vision-language understanding.\n\n+ Elegant and efficient system design: The combination of vectorized inputs + VECFORMER encoding + differentiable context selection enables both interpretability and communication efficiency, demonstrating real bandwidth and latency advantages.\n\n+ Strong empirical validation and practicality: The authors provide extensive experiments on multiple datasets with concrete system-level metrics (bandwidth, cache, latency) and model-level metrics (BLEU, ROUGE, intervention accuracy), making the approach convincing for real-world deployment."}, "weaknesses": {"value": "1. Limited gain in Intervention Failure Rate (IFR) and potential evaluation bias.\nAlthough FLEETAGENT demonstrates strong system-level efficiency, its improvement in Intervention Failure Rate (IFR) appears modest. On the NU-EVAL benchmark, the reported IFR is 12.12%, only a 1.51-point absolute reduction compared with the GPT-4o baseline (13.63%), and within ~2–4 points of other strong baselines (16.05%, 15.90%, 15.24%). Such a small margin may fall within statistical noise, especially since no significance test, variance analysis, or sensitivity study (e.g., threshold vs. IFR curves) is reported. Moreover, because FLEETAGENT is fine-tuned on datasets highly similar to the evaluation sets, its responses naturally match the ground-truth style and template, which can inflate n-gram metrics (BLEU/ROUGE/ChrF) without necessarily improving explanatory quality or decision usefulness. In other words, the model may learn to mimic the format rather than produce substantively better reasoning. The evaluation should therefore include format-agnostic judging—e.g., LLM-as-a-judge and human-as-a-judge with blind protocols, report inter-rater agreement and confidence intervals, and add cross-template robustness (paraphrased GT, randomized phrasing) and unseen-domain tests to ensure gains are not merely artifacts of response-format alignment.\n\n2. The paper exclusively adopts vectorized embeddings as the vehicle-to-cloud representation, omitting any version of FleetAgent trained directly on raw images. Could the authors clarify the rationale behind this design choice? Training a vision-language variant that processes raw images could leverage pretrained SOTA VLMs with stronger world knowledge transfer, richer scene semantics, and potentially lower training cost via lightweight fine-tuning instead of building a separate vector-based encoder. It would be helpful if the authors could discuss the trade-offs—e.g., bandwidth vs. accuracy, interpretability vs. scalability—and whether a hybrid approach (vector + vision) might further improve generalization and efficiency.\n\n3. Qualitative results are too few, lacking failure cases and comparisons with other methods."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BY1C1sEthC", "forum": "UBLDpDcLyL", "replyto": "UBLDpDcLyL", "signatures": ["ICLR.cc/2026/Conference/Submission13704/Reviewer_bbDM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13704/Reviewer_bbDM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854171652, "cdate": 1761854171652, "tmdate": 1762924253874, "mdate": 1762924253874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FleetAgent, a cloud-hosted multimodal LLM that consumes vectorized observations and planned trajectories to generate natural-language explanations. The approach centers on replacing tokenizers/vision encoders with a vector encoder (VecFormer) plus a differentiable top‑K context selection, and introduces a nuScenes-derived dataset with synthetic imperfect plans and NL annotations for explanation and evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is generally readable and easy to understand.\n2. This paper introduces a new dataset, NU-EVAL, which is extended from nuScenes. \n3. The architectural and training pipeline is coherent and competently specified, and the quantitative results are consistent with expected trade-offs: vector inputs reduce bandwidth while maintaining competitive explanation and plan-evaluation quality."}, "weaknesses": {"value": "1. The novelty of the architecture is limited. Designing a vector-to-LLM interface with attention-based token prioritization and Gumbel‑Softmax selection is an incremental adaptation of known components rather than a conceptually new modeling contribution.\n2. The technical significance is not significant. The core improvements are largely architectural, packaging, and efficiency engineering around vectorized inputs to a pre-existing VLM, with the selection mechanism and training stages reflecting standard practices, and without introducing a fundamentally novel learning objective or evaluation framework.\n3. The dataset contribution is not very strong. Many nuScenes-based VQA/explanation datasets already exist (e.g., NuScenes‑QA, NuInstruct, NuPrompt, NuPlanQA, DriveLM, Nu‑X), and re-deriving a plan‑explanation set on nuScenes with synthetic imperfect plans does not substantively expand coverage, realism, or scale for teleoperation safety assessment. Moreover, the nuScenes is a very small dataset.\n4. Imperfect plans are synthesized by sampling from clustered human trajectories rather than sourced from actual autonomy failure logs, calling into question whether the evaluation meaningfully reflects the edge cases where teleop decisions matter most.\n5. The paper acknowledges extensive VLM-for-driving work and enumerates multiple nuScenes/nuPlan-derived datasets for captioning, grounding, and reasoning. Given this landscape, positioning the contribution as a new paradigm is not fully convincing, as many elements, like the language explanations, plan reasoning, and nuScenes-based supervision, are already common."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nTohqnCGFO", "forum": "UBLDpDcLyL", "replyto": "UBLDpDcLyL", "signatures": ["ICLR.cc/2026/Conference/Submission13704/Reviewer_Ajwj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13704/Reviewer_Ajwj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988440734, "cdate": 1761988440734, "tmdate": 1762924253486, "mdate": 1762924253486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a cloud-based Multimodal Large Language Model framework, FLEETAGENT, designed to assess the driving plan and context of an Autonomous Vehicle (AV) to determine the necessity of remote human teleoperation. The paper tends to bridge large-scale driverless fleets and human teleoperators, and provid both evaluation and explanation to enable rapid situation awareness for remote drivers. It leverages vectorized representations involving many information to generate compact context embeddings via a custom VecFormer encoder. A new dataset designed for this new proposed task is also provided."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper targets a underexplored yet (possibly) critical issue, scalable teleoperation support to address both efficiency and explainability for fleet-level autonomous driving.\n- The proposed VecFormer encoder efficiently transforms structured driving context into embeddings suitable for LLM reasoning.\n- The dataset extends nuScenes with imperfect trajectory generation and human-verified explanation annotations, and may fill an important gap between driving VQA and plan-assessment datasets."}, "weaknesses": {"value": "- The framework focuses on explanation and plan evaluation but does not assess whether FleetAgent’s judgments lead to improved teleoperation decisions or reduced risk in real or simulated driving loops.\n- FleetAgent mainly integrates known ideas—vector embedding, transformer encoding, masked pretraining and instruction tuning—into a teleoperation pipeline. Method novelty is very limited.\n- Lines 74 said \"actual limitaitons of deploying MLLM for real-world applications..\" what are the limitations? Deployment from device to cloud?\n- Better provide more demonstrations and user study or human preference evaluation to verify whether FleetAgent’s explanations are actually useful for teleoperators.\n- Can we use this pipeline to different scenarios, apart from nuScenes-based ones."}, "questions": {"value": "Please emphasize the significance and scientific contribution of the proposed setting within the context of AI and machine learning research, rather than primarily as a telecommunication or systems-engineering study. Explain any novel methodological designs and describe whether any closed-loop evaluations or real-time validation experiments can be conducted"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "teTMUQrTcE", "forum": "UBLDpDcLyL", "replyto": "UBLDpDcLyL", "signatures": ["ICLR.cc/2026/Conference/Submission13704/Reviewer_UgRP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13704/Reviewer_UgRP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991390507, "cdate": 1761991390507, "tmdate": 1762924253172, "mdate": 1762924253172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed FleetAgent, a cloud-based Multimodal Large Language Model system designed to evaluate the driving decisions of autonomous vehicles and trigger remote human intervention when necessary. By utilizing a vectorized scene representation and a custom-built encoder called VecFormer, the system significantly reduces communication and computational overhead while generating natural language explanations and evaluations. The authors also construct the Nu-Eval dataset based on nuScenes, which includes imperfect driving behaviors and natural language annotations. Experiments demonstrate that FleetAgent outperforms existing methods in both system efficiency and task performance, offering a scalable and explainable solution for the teleoperation of large-scale autonomous vehicle fleets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The system design is highly practical and scalable.The proposed FleetAgent system places computational tasks in the cloud, adding no onboard computational burden, while the vectorized input reduces communication demands, making it highly suitable for large-scale fleet deployment.\n\n2. The authors designed VecFormer, a dedicated encoder for vectorized driving scenes that replaces traditional visual or text encoders. It effectively compresses input length while preserving key semantic information and introduces a differentiable Top-K selection mechanism to improve reasoning efficiency.\n\n3.The authors constructed the Nu-Eval dataset, which includes both real and synthetic driving trajectories, natural language explanations, and intervention scores. A multi-stage training strategy effectively enhances the model's generalization capability with limited annotated data."}, "weaknesses": {"value": "1.In Section 4.2.1, the generation of imperfect planning relies on random sampling from clustered trajectory categories.This method may not adequately capture the nuanced and context-specific errors that real-world autonomous systems make under complex or adversarial conditions. \n\n2.In Section 5.2, the token prioritization mechanism in VecFormer depends on a manually set top-K value.While the Gumbel-Softmax sampling enables differentiable selection, the choice of K is heuristic and fixed across all scenarios. This may lead to under-selection in complex scenes or over-selection in simple ones, potentially affecting both model accuracy and inference speed.\n\n3.In Section 6.1 and 6.2,the authors do not provide human-subject studies to assess whether FleetAgent’s natural-language explanations actually enhance teleoperator understanding.Also, the system-level experiments in Section 6.1 ignore network latency, bandwidth fluctuation, and other real teleoperation constraints."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4YVukD5c5U", "forum": "UBLDpDcLyL", "replyto": "UBLDpDcLyL", "signatures": ["ICLR.cc/2026/Conference/Submission13704/Reviewer_d3mg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13704/Reviewer_d3mg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152443360, "cdate": 1762152443360, "tmdate": 1762924252842, "mdate": 1762924252842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}