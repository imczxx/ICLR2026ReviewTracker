{"id": "Ty8Z51mjgd", "number": 6586, "cdate": 1757989897410, "mdate": 1759897906615, "content": {"title": "UniFy: Efficient Modeling of Non-Stationary Periodicity for Time Series Forecasting", "abstract": "Periodic structures dominate long-term temporal dependencies in real-world signals, forming the cornerstone of long-term time series forecasting (LTSF). Existing methods typically aim to capture globally stable periodicity while overlooking the fact that real-world systems often exhibit substantial waveform variations across different periodic intervals. Representing such non-stationary sequences with a fixed period can lead to underfitting or overfitting of periodic components, thereby degrading forecasting accuracy. We further identify that the fundamental reason for this phenomenon is *frequency competition*, where multiple frequencies interfere with each other and distort the learning of periodic structures. We address this with a purely linear model: **Uni**fying Competing **F**requenc**y** (UniFy). It employs a multi-round Adaptive Frequency Selector (AFS) to progressively extract frequency components into multiple subspaces, mitigating frequency competition. Each subspace is then modeled by an Independent Linear Modeler (ILM) to extract its principal component, and the predictions from all subspaces are fused through Multi-subspace Calibration (MSC) to generate the final output. UniFy enables accurate and efficient modeling of non-stationary periodicity. Extensive experiments on 12 real-world datasets demonstrate the superiority of UniFy, delivering an average 16.0% MSE improvement on both long-term and short-term forecasting tasks, as well as an average 15.5% improvement in few-shot and zero-shot scenarios. Furthermore, its purely linear architecture ensures excellent computational efficiency and scalability. The code for our experiments is anonymously available at: https://anonymous.4open.science/r/UniFy-22F2/\n.", "tldr": "", "keywords": ["Multivariate time series forecasting"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/309ce7e43543e44224dd4780f124b69932752a63.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes UniFy, a linear framework for time series forecasting that aims to model non-stationary periodicity by alleviating what the authors call frequency competition, where the dominant frequency suppresses the secondary frequency. The approach introduces three components for separating and independently modeling different frequency subspaces before recombining them. The paper's contributions include a new perspective on frequency interference in time series and a lightweight, interpretable architecture. However, while the problem of non-stationary periodicity is real, its motivation is less compelling because existing frequency-domain methods already accommodate varying periodic components rather than assuming a fixed period."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a novel approach for modeling nonstationary periodicity in time series forecasting by applying adaptive frequency selection in the frequency domain.\n2. This approach demonstrates reliable empirical performance on multiple real-world datasets, achieving significant improvements in both long-term and short-term forecasting tasks.\n3. The paper is well-thought-out and provides a clear explanation of the proposed method."}, "weaknesses": {"value": "1. This article does not explain why selecting specific frequencies or using learned masks improves time series forecasting performance. Given that the frequency domain is commonly used in signal processing, a stronger theoretical justification can be provided to explain why and how frequency selection can benefit forecasting models.\n2. Existing frequency-domain models (e.g., those in [1, 2, 3, 4]) do not assume fixed periodicity, but rather accommodate varying periodicity. This makes the motivation for frequency competition somewhat problematic. The paper claims that the problem with existing models is that dominant frequencies suppress minor frequencies, but fails to convincingly demonstrate that this competition is a significant problem in methods that already allow for varying periodicity.\n3. The related work section of this paper does not fully discuss the proposed method in relation to recent research in this area, in particular [1, 2, 3, 4]. These methods also deal with periodicity in time series, but in different ways.\n4. The discrete Fourier transform (DFT) produces both positive and negative frequencies, while existing frequency-domain methods (such as those in [1, 2, 3, 4]) typically retain only non-negative frequencies. However, this paper retains both positive and negative frequencies in its decomposition. However, the motivation for retaining negative frequencies is not clearly explained.\n5. This paper adopts a channel-independent approach, treating each variable as independent of the others. However, time series data often exhibit inter-variable dependencies (for example, in multivariate time series, variables often influence each other). Ignoring these dependencies can limit the effectiveness of the model, and the paper does not provide relevant analysis.\n\n[1] Xu, Zhijian, et al. “FITS: Modeling Time Series with 10k Parameters.” International Conference on Learning Representations (ICLR), 2024.\n\n[2] Zhang, et al. “Not All Frequencies Are Created Equal: Towards a Dynamic Fusion of Frequencies in Time-Series Forecasting.” ACM Multimedia (ACM MM), 2024.\n\n[3] Wang, et al. “FreDF: Learning to Forecast in the Frequency Domain.” International Conference on Learning Representations (ICLR), 2025.\n\n[4] Yi, et al. “Frequency-domain MLPs are More Effective Learners in Time Series Forecasting.” Conference on Neural Information Processing Systems (NeurIPS), 2023."}, "questions": {"value": "1. Can the authors explain, either theoretically or empirically, why frequency selection via learned masks can improve prediction performance?\n2. The authors claim that frequency contention is a problem in existing frequency-based methods. However, many recent methods (e.g., [1, 2, 3, 4]) already allow for dynamic, non-stationary periodicity. Can the authors explain why frequency contention remains a problem in these methods?\n3. The paper lacks a detailed comparison with existing frequency-domain methods, particularly [1, 2, 3, 4]. How does the proposed method compare to these methods in terms of dynamic frequency modeling and handling nonstationary periodicity? Could the authors discuss the key differences between them and why their method offers an improvement?\n4. What is the motivation behind retaining both positive and negative frequencies in the decomposition? Do negative frequencies provide any distinct advantage in forecasting over methods that only retain positive frequencies?\n5. The current method treats each variable independently. However, in many real-world time series, there are causal relationships between variables. Does the author's method account for this causal relationship? If not, can the authors provide relevant results along the way on how this omission affects performance, especially when forecasting multivariate data with known dependencies between variables (such as traffic and ECLs datasets)?\n\n[1] Xu, Zhijian, et al. “FITS: Modeling Time Series with 10k Parameters.” International Conference on Learning Representations (ICLR), 2024.\n\n[2] Zhang, et al. “Not All Frequencies Are Created Equal: Towards a Dynamic Fusion of Frequencies in Time-Series Forecasting.” ACM Multimedia (ACM MM), 2024.\n\n[3] Wang, et al. “FreDF: Learning to Forecast in the Frequency Domain.” International Conference on Learning Representations (ICLR), 2025.\n\n[4] Yi, et al. “Frequency-domain MLPs are More Effective Learners in Time Series Forecasting.” Conference on Neural Information Processing Systems (NeurIPS), 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DDpAR0uwWe", "forum": "Ty8Z51mjgd", "replyto": "Ty8Z51mjgd", "signatures": ["ICLR.cc/2026/Conference/Submission6586/Reviewer_dCRg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6586/Reviewer_dCRg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760683211609, "cdate": 1760683211609, "tmdate": 1762918917291, "mdate": 1762918917291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the forecasting bottleneck caused by non-stationary periodicity in multivariate time series and identifies frequency competition (where the dominant frequency suppresses secondary yet informative components, and secondary frequencies in turn interfere with the dominant one) as the root cause of underfitting/overfitting. To address this problem, it proposes the nearly purely linear UniFy. This design explicitly mitigates interference between dominant and secondary frequencies, preserving stable primary periodicity while capturing time-varying details; experiments demonstrate the effectiveness of the approach. The paper claims that \"there is a key gap in the field of TSF: existing approaches lack a principled mechanism to balance accurate modeling of the dominant component against interference from secondary components\"."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses frequency competition through an AFS->ILM->MSC pipeline, reducing interference between dominant and secondary frequencies.\n2. The proposed method uses an almost purely linear architecture with low inference/training cost and strong scalability; ILM assigns an independent linear projection to each scale to cut computation and avoid subspace interference.\n3. This paper achieves good performance on 12 real-world datasets across long/short horizons and zero/few-shot settings."}, "weaknesses": {"value": "1. This article proposes a key gap in the field of TSF: existing approaches lack a principled mechanism to balance accurate modeling of the dominant component against interference from secondary components. But I think this gap is not very reasonable because there are many methods [1-4] that explicitly separate and independently process different frequency components of time series.\n2. Many papers[1-4] and the method proposed in this article are similar (both decompose the time series into multiple frequencies and process them separately before fusing the output), but this article does not discuss it. The author needs to supplement the differences between the methods used in this article and these methods.\n3. The authors do not explicitly state the input  length in the main text; instead, they place it in a hard-to-find appendix, which reduces readers’ efficiency in accessing key information.\n\n4. Why does the AFS module use a multi round mask selection mechanism to gradually decompose the spectrum into multiple subspaces. Isn't it simpler and more effective to directly decompose a time series into different frequency components through Fourier transform？ The necessity and rationality of the author's use of AFS modules need further elaboration.\n\n\n[1] Not all frequencies are created equal: Towards a dynamic fusion of frequencies in time-series forecasting\n\n[2] TimeKAN: KAN-based Frequency Decomposition Learning Architecture for Long-term Time Series Forecasting\n\n[3] FreqMoE: Enhancing Time Series Forecasting through Frequency Decomposition Mixture of Experts\n\n[4] Frequency-domain MLPs are More Effective Learners in Time Series Forecasting"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q6q2cqu86a", "forum": "Ty8Z51mjgd", "replyto": "Ty8Z51mjgd", "signatures": ["ICLR.cc/2026/Conference/Submission6586/Reviewer_gRZs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6586/Reviewer_gRZs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760965599242, "cdate": 1760965599242, "tmdate": 1762918916963, "mdate": 1762918916963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight linear framework, UniFy, for time series forecasting with non-stationary periodicity. The authors point out that existing methods often suffer from frequency competition, where dominant frequencies suppress secondary but informative ones, leading to biased periodic modeling. To address this issue, UniFy introduces three modules: AFS (Adaptive Frequency Selector), ILM (Independent Linear Modeler), and MSC (Multi-Subspace Calibration). Experiments on multiple datasets show good performance improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The concept of frequency competition is clearly presented. The authors analyze the problem from a frequency-domain perspective and propose a quantitative metric, High-Competition Impact (HCI), which improves theoretical interpretability.\n\n2. The framework is logically designed and modular.\n\n3. The experiments cover various datasets and tasks, and the authors provide reproducible code."}, "weaknesses": {"value": "1. The paper does not clearly explain how the model identifies or distinguishes dominant and secondary frequencies, making it hard to understand how frequency competition is alleviated.\n\n2. All decomposition rounds use the same structure and loss without any orthogonality or hierarchy constraints, so it is unclear how the model ensures complementary or non-overlapping frequency components.\n\n3. The meaning and necessity of the residual update and multi-round decomposition (R rounds) are not well explained.\n\n4. The appendix shows that when the number of decomposition rounds $R=1$, the model achieves the best performance. If the first round mainly extracts dominant frequencies, this implies that dominant frequency energy contributes the most, which seems inconsistent with the paper’s claim that multi-round decomposition alleviates frequency competition."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9sVQucU3YV", "forum": "Ty8Z51mjgd", "replyto": "Ty8Z51mjgd", "signatures": ["ICLR.cc/2026/Conference/Submission6586/Reviewer_wFdR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6586/Reviewer_wFdR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761228216976, "cdate": 1761228216976, "tmdate": 1762918916635, "mdate": 1762918916635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address a critical and prevalent challenge in long-term time series forecasting (LTSF): the modeling of non-stationary periodicity. The authors begin by positing a core argument that the fundamental reason for the poor performance of existing models on such problems is \"frequency competition\"—a phenomenon where, in a shared representation space, dominant frequencies suppress weaker yet informative secondary frequencies, while these secondary components, in turn, introduce noise that interferes with the learning of the dominant ones. This competition leads models to either underfit periodic details (e.g., DLinear) or overfit and generate spurious periodic patterns (e.g., HDMixer, CycleNet)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. UniFy frames and empirically verifies the issue of frequency competition as a central challenge in non-stationary periodic time series forecasting, with clear evidence in Figures 1 and 2 and quantitative error attribution\n2. On 12 benchmarks, UniFy outperforms or matches nine state-of-the-art baselines in both long-term and short-term forecasting (Table 1, Table 2, Table 16, and Table 17), with statistically validated improvements (Appendix F, Tables 10–11). Ablation studies (Tables 5, 12–15) sharply isolate the contributions of each module.\n3. Full code, experimental details, and implementation settings are referenced (including open-source code and Appendix D). The empirical methodology—repeat runs, standard error reporting, significance—meets best practices."}, "weaknesses": {"value": "1. The methodology remains almost entirely empirical or algorithmic; there is no formal analysis of the optimality or identifiability of the mask selection scheme, no bounds on error due to frequency competition, and no guarantee that the masking-plus-linear approach cannot in some circumstances collapse or mix frequencies. The effectiveness of the mask-separation rests on empirical attribution (see Figure 6), but this only partially addresses the underlying theoretical issues. \n2. The masks $\\mathbf{m}^{(r)}$ are described as “learnable” and tied at conjugate pairs, but no details are given on initialization, regularization (e.g., entropy/spread), or how the model avoids degenerate solutions where most energy is absorbed by a single mask. \n3. While Figure 6 demonstrates that frequency components are effectively separated across different scales on the ETTh1 dataset, a corresponding analysis for the remaining datasets is lacking.\n4. While each ILM is linear, it is not entirely clear how the projections $\\mathbf{W}^{(r)}$ avoid redundancy or trivial overlap in subspace coverage if the decomposition is not sharp. Are the masks softly overlapping, or is there a formal guarantee of orthogonality or completeness?\n5. The calibration module (MSC) is a generic two-layer MLP. There is no analysis or empirical report of how its design or hyperparameters affect cross-subspace misalignment—is its benefit truly from calibration, or just additional (albeit shallow) nonlinearity?\n6. While the breadth of datasets and baselines is excellent, the experiments are limited to public benchmarks with strong periodic structure. The claims of generality for arbitrary non-stationary or non-periodic time series are unverified and may overstate the domain of effectiveness. Furthermore, the comparison remains mostly within the “linear/Transformer/patch/MLP” family of recent literature; more classical and hybrid time-frequency methods (e.g., FITS、DLinear、CycleNet、SparseTSF)"}, "questions": {"value": "1. Can the authors provide empirical or theoretical evidence on the mask initialization and regularization strategies in AFS? Specifically, how is overfitting of masks to dominant frequencies avoided, and what prevents multiple masks from redundantly focusing on the same components? If frequency responsibilities are not well-separated, is there appreciable performance degradation, and can you provide quantitative separation/overlap statistics across datasets?\n2. How does UniFy perform on datasets with weak, highly transient, or non-deterministic periodic structure? Can the authors report or discuss failure modes for cases where periodic features are only weakly present or contaminated by stochastic noise, or where the non-stationarity is truly adversarial?\n3. The attribution analysis (Figures 6, 10, 11) is visually compelling, but could the authors provide quantitative separation metrics (e.g., mask entropy, overlap, mutual information) and statistics across initializations, datasets, and mask counts, to confirm robustness of decomposition and allocation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NIw8yqDtTU", "forum": "Ty8Z51mjgd", "replyto": "Ty8Z51mjgd", "signatures": ["ICLR.cc/2026/Conference/Submission6586/Reviewer_AZ9V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6586/Reviewer_AZ9V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991557555, "cdate": 1761991557555, "tmdate": 1762918916280, "mdate": 1762918916280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}