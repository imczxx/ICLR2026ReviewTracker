{"id": "JPZoLWYo82", "number": 23964, "cdate": 1758350925075, "mdate": 1759896788851, "content": {"title": "WarriorMath: Empowering Mathematical Reasoning for Large Language Models via Expert Battles", "abstract": "Large Language Models (LLMs) excel in solving mathematical problems, yet their performance is often limited by the availability of high-quality, diverse training data. Existing methods focus on augmenting datasets through rephrasing or difficulty progression but overlook the specific failure modes of LLMs. This results in synthetic questions that the model can already solve, providing minimal performance gains. To address this, we propose WarriorMath, a defect-aware framework for mathematical problem solving that integrates both targeted data synthesis and progressive training. In the synthesis stage, we employ multiple expert LLMs in a collaborative process to generate, critique, and refine problems. Questions that base LLMs fail to solve are identified and iteratively improved through expert-level feedback, producing high-quality, defect-aware training data. In the training stage, we introduce a progressive learning framework that iteratively fine-tunes the model using increasingly challenging data tailored to its weaknesses. Experiments on six mathematical benchmarks show that WarriorMath outperforms strong baselines by 12.57% on average, setting a new state-of-the-art. Our results demonstrate the effectiveness of a defect-aware, multi-expert framework for improving mathematical ability.", "tldr": "", "keywords": ["Mathematical NLP", "Data Synthesis"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/702292ebabe15d70ae32a854e6f0e35277600387.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces WarriorMath, a novel framework designed to enhance the mathematical reasoning abilities of Large Language Models (LLMs). The core idea is to move beyond generic data augmentation and instead adopt a \"defect-aware\" approach, analogous to personalized tutoring. The framework consists of two main stages. First, a \"Defect-Aware Data Synthesis\" stage where a committee of expert LLMs generates new math problems from scratch. Crucially, only problems that the base \"student\" model fails to solve are kept, ensuring the data is targeted at the model's specific weaknesses. The expert committee then collaboratively generates and refines solutions for these problems, using an Elo rating system to select the highest-quality \"golden answer.\" Second, a \"Progressive Training\" stage where the student model is first fine-tuned on this defect-aware data (SFT) and then further improved through \"Iterative Defect Alignment,\" a DPO-based process where the model learns from its own mistakes by comparing them against the golden answers. The authors demonstrate through experiments on six math benchmarks that their method achieves state-of-the-art performance among open-source models of similar size."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Novel and Intuitive Framework:** The central concept of a \"defect-aware\" learning system is highly innovative and compelling. By distinguishing between problem \"difficulty\" and model-specific \"defects,\" the paper introduces a more targeted and efficient paradigm for data synthesis. The analogy to a human tutor identifying and correcting a student's weak points is powerful and well-executed.\n\n2.  **High-Quality Data Generation Mechanism:** The use of a multi-expert committee to generate not only problems but also high-quality solutions is a significant strength. The integration of an Elo rating system to adjudicate between different expert solutions is a sophisticated and robust method to ensure the quality and reliability of the training data, moving beyond simple majority voting.\n\n3.  **Holistic and Closed-Loop System:** WarriorMath presents a complete, end-to-end system that combines targeted data creation with a progressive training schedule. The synergy between the data synthesis stage (diagnosing defects) and the iterative alignment stage (correcting defects) forms a coherent and powerful self-improvement loop.\n\n4.  **Strong Empirical Results:** The paper presents strong experimental results, showing that models trained with WarriorMath significantly outperform existing baselines across multiple challenging math benchmarks. Achieving state-of-the-art performance among similarly-sized open-source models demonstrates the practical effectiveness of the proposed framework."}, "weaknesses": {"value": "1.  **Unclear Description of the Problem Generation Process:** There is a confusing passage in the methodology (Section 3.1.1, \"Problem Synthesis from Scratch\") that creates a contradiction. The high-level idea is that an \"examiner LLM A\" creates a problem for the \"base model\" to solve. However, the text states that the goal is for LLM A to \"pose challenging questions to LLM B.\" This is confusing because it's unclear what role \"judge LLM B\" plays at the problem creation stage. This lack of clarity makes it difficult to fully understand the core mechanism of how problems are initially generated and targeted at the base model's defects.\n\n2.  **Crucial Missing Baseline: Comparison with Expert Teachers:** The paper's central claim is that it can effectively distill knowledge from a committee of powerful expert models. However, the main results in Table 1 do not include the performance of these expert \"teacher\" models (e.g., Qwen2.5-Math-72B, DeepSeek-R1-Distill-Llama-70B) as baselines. Without this comparison, it's impossible to know if the student model has truly learned to \"surpass its teachers\" or if it is simply a less effective version of the best expert in the committee. This is a critical omission that weakens the paper's claims about knowledge aggregation and superiority.\n\n3.  **Insufficient Ablation to Justify the Elo Mechanism:** To prove the value of the complex multi-expert Elo rating system, a stronger and more direct ablation study is needed. The authors should compare WarriorMath against a simpler baseline: **\"Single-Teacher Distillation.\"** In this setup, the defect-aware problems would be generated as usual, but the answers would come from only the **single best-performing expert** in the committee, without any voting or Elo competition. If WarriorMath does not significantly outperform this simpler baseline, the necessity of the complex and computationally expensive Elo mechanism is questionable.\n\n4.  **Unfair Comparison in the Data Quality Experiment:** In the \"Data Quality\" experiment (Table 2), the paper claims to use a single solver (`Qwen2.5-Math-72B-Instruct`) for all baseline methods to ensure a fair comparison of problem generation strategies. However, WarriorMath's own training data uses \"golden answers\" derived from the **entire multi-expert Elo consensus process**, which is inherently superior to any single solver. This creates an unfair advantage, as WarriorMath's strong performance may be due to its better answer generation process, not just its better problem generation strategy. The experiment conflates these two factors.\n\n5.  **High Computational Cost and Scalability Concerns:** The data synthesis process is extremely computationally expensive. It requires multiple inferences from the base model, the entire expert committee, and a quadratic number of pairwise comparisons for the Elo rating. This high cost may limit the practical applicability of the method and raises questions about its scalability if the expert committee were to be expanded. Can you show how many GPU hours are used in your experiemnts?\n\n6.  **Coarse Definition of \"Defect\":** The framework identifies a \"defect\" based solely on whether the final answer is right or wrong. This is a very coarse signal. It cannot distinguish between a minor calculation error and a fundamental conceptual misunderstanding. This limits the \"defect-aware\" nature of the framework, as it cannot provide more fine-grained, process-based feedback to the model."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JeBFPHPtnm", "forum": "JPZoLWYo82", "replyto": "JPZoLWYo82", "signatures": ["ICLR.cc/2026/Conference/Submission23964/Reviewer_hBrm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23964/Reviewer_hBrm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569881938, "cdate": 1761569881938, "tmdate": 1762942873784, "mdate": 1762942873784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WarriorMath, a framework that train LLMs on synthetic data that is shown to be difficult (defect) for the model generated by a \"committee\" of multiple expert LLMs. This method differs from previous method in that they tilt the distribution such that only difficult samples (defects) are preserved and hence the base LLM is trained more on difficult data. The end model achieves SOTA performance in terms of pass@1 accuracy on AIME24/25, AMC23, MATH500, Minerva, and Olympiad Bench benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies issues overlooked in existing methods and suggests an intuitive solution around such problem, namely the difficulty distribution of the synthetic data from LLMs for mathematical reasoning ability.\n2. Performance: the end model's performance is strong on several challenging benchmarks."}, "weaknesses": {"value": "1. As a paper proposing a method dealing with multiple LLMs that serves different roles, this paper does not do a clean job at clarifying each role for each LLM, which causes confusion for readers. In fact, the naming scheme of different roles are all over the place: for example, in line 214, LLM B is referred to as 'judge' and A as 'examiner', but in line 238, A is referred to as 'reviewer', or the other way around (reviewer is reviewer but A is not A). I could not tell for certain which is which upon several readings even with the help of an LLM that reads this pdf and of figure 2 and this is not how a well presentation should look like. I can gain a vague grasp on the process of generating the data, but clarity is lacking. (maybe add a concluding sentence at the end of section 3.1.2)\n2. The method proposed requires multiple expert-level LLMs to train one base LLM and the authors in the experiments only used a base model of 7B and examine committee of 70B models. Does this mean that we could only improve a smaller model and the method requires multiple strong models? How do we improve a strong model? If this requires too much computing power, try using a series of similar sized committee member could be good. I would gladly raise the score of contribution once this experiment is performed.\n3. Since the model is only trained on problem sets which it failed previously, how do we guarantee that it does not forget the ones it previously answered correctly?\n\nMinor:\n1. Forgotten period at the end of line 410.\n2. Abrupt introduction of the concept of battle in line 272.\n\nTo raise my score:\nA cleaner writing and an experiment as described above"}, "questions": {"value": "Explain the member's role clearly and cleanly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "an6UieUv3W", "forum": "JPZoLWYo82", "replyto": "JPZoLWYo82", "signatures": ["ICLR.cc/2026/Conference/Submission23964/Reviewer_pJb1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23964/Reviewer_pJb1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709603129, "cdate": 1761709603129, "tmdate": 1762942873501, "mdate": 1762942873501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WarriorMath, a defect-aware data synthesis pipeline. The pipeline is aimed to improve mathematical reasoning by (i) generating data from scratch that targets the failure modes (defects) of a base model via a multi-LLM “exam committee”, and (ii) progressive training the base model with SFT done on top-rated solutions, followed by iterative alignment where the model learns from its own incorrect responses through preference optimization. The paper reports notable gains across six math benchmark and ablations on committee size and iterative rounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The proposed pipeline design is conceptually sound and practically executable.\n- Evaluations include multiple math benchmarks and show consistent improvements. Iterative alignment ablation demonstrates monotonic gains.\n- Offers a reproducible blueprint (prompts + committee + filtering + progressive alignment) that community can adapt and benifit, even with open models.\n- The paper is well-written, the pipeline design is sound, and the results are clear and impactful."}, "weaknesses": {"value": "- The synthetic data quality is directly tied to the capabilities of the models used for generation and judging. Scaling the dataset can be costly with more committee members and larger/costly models.\n- There are no abalations on controlled removal of defect filtering, Elo vs votes, or coverage selection to pinpoint what truly drives gains.\n- Use of ROUGE-based scores for overlap analysis against two corpora is not an reliable metric since it does not capture the semantic and perturbed problems. Semantic metrics and we search hit rate can act as higher quality contamination/leakage rate proxies. [1]\n\n[1]: SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NMw3JemacO", "forum": "JPZoLWYo82", "replyto": "JPZoLWYo82", "signatures": ["ICLR.cc/2026/Conference/Submission23964/Reviewer_jqqG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23964/Reviewer_jqqG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817765438, "cdate": 1761817765438, "tmdate": 1762942873159, "mdate": 1762942873159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed WarriorMath, a framework that empowers models with multi-expert LLM systems. Despite the complexity of the system, the core contribution seems to be about how to address the defects that the previous LLM systems can not detect with naive data pipelines.The authors benchmarked their methods on Qwen2.5-7B and Deepseek-R1-Qwen-Distill-7B, and showed the effectiveness of their methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and well demonstrated.\n2. The idea is unique and intersting. The combination of mathematical reasoning and defect awareness seems to give a unique perspective of LLM4math."}, "weaknesses": {"value": "1. I think the paper has not given enough evidences that the proposed method is able to address the unique challenge of defect awareness. The authors could consider to prove this point other than the benchmark results themselves, maybe a categorized benchmark.\n2. Given the complexity of the system, it is hard to evaluate the effectiveness of each components. I understand that the complexity is necessary, but I wish the authors could at least give some decomposition and ablation study to help future researchers to understand why the methodology works."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BrAGaUf62D", "forum": "JPZoLWYo82", "replyto": "JPZoLWYo82", "signatures": ["ICLR.cc/2026/Conference/Submission23964/Reviewer_iS8p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23964/Reviewer_iS8p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999330310, "cdate": 1761999330310, "tmdate": 1762942872470, "mdate": 1762942872470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}