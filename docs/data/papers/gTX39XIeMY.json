{"id": "gTX39XIeMY", "number": 22226, "cdate": 1758327995347, "mdate": 1759896879097, "content": {"title": "ARS Adaptive Reasoning Suppression for Efficient Large Reasoning Language Model", "abstract": "Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena. Existing efficient reasoning methods face the challenge of balancing reasoning quality with inference cost reduction. We propose \\textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism with progressive suppression thresholds, achieving superior efficiency compared to static suppression methods. Our extensive evaluation across mathematical reasoning benchmarks using multiple model architectures demonstrates that ARS achieves up to 53\\%, 46.1\\%, and 57.9\\% in token, latency and energy reduction, while maintaining or improving accuracy.", "tldr": "We propose Adaptive Reasoning Suppression (ARS), a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring.", "keywords": ["Large Reasoning Language Models; LRMs; Efficient Reasoning; On-Device; Energy efficient; Token Efficient"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55aa89a9fb30636f69c0b7ac9bcd0dfdbf54e2d2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Adaptive Reasoning Suppression (ARS), a training-free approach designed to improve the computational efficiency of Large Reasoning Language Models (LRMs). The main idea is to ease the overthinking phenomenon by dynamically suppresses the redundant reasoning steps. There are three coponents proposed: Multi-checkpoint certainty estimation, Progressive threshold adaptation and Dynamic suppression. The results demonstrate that ARS achieves efficiency gains while maintaining or improving accuracy."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The \"overthinking phenomenon\" is a real and important issue.\n- The core idea of being training-free is a practical strength.\n- The concept of using adaptive thresholds based on trends is more nuanced than a static, fixed threshold (like CGRS's)."}, "weaknesses": {"value": "This paper is full of holes and missing details, putting it in no shape for publication. Here are some major concerns:\n1. The method is not well explained:\n  - What exactly is the compute_entropy_confidence function?\n  - How is the adaptive_threshold determined? Is it a learned function or a heuristic?\n  - Algorithm 1 contains many undefined functions (probe_answer, compute_trend, Policy functions, etc.).\n  - The parameters ($\\alpha$, $\\beta$, $\\gamma$) in Equation 2 are not explained or tested.\n2. The experimental setup is vague:\n  - How was energy measured? How were the baselines (TALE, CGRS) run? Hyperparameters and setup are missing.\n  - There is a major contradiction in the token limit: the paper states a \"maximum token limit\" of 1200, but Table 2 shows a TPC of 1583 for a baseline. And this 1200 token limit also seems arbitrarily short for complex reasoning\n3. The evaluation is thin:\n  - Only two baselines are measured, but the introduction implies many other training-free methods exist.\n  - Testing on only 200 data points, and only on math dataset is likely not statistically significant.\n4. The theory is incomplete and somewhat meaningless:\n  - The theoretical analysis (Section 2.3) relies on undefined terms and provides only a one-sentence \"Proof Sketch\" instead of a proper proof."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UwJMo7SbfB", "forum": "gTX39XIeMY", "replyto": "gTX39XIeMY", "signatures": ["ICLR.cc/2026/Conference/Submission22226/Reviewer_fxsE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22226/Reviewer_fxsE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936538526, "cdate": 1761936538526, "tmdate": 1762942124422, "mdate": 1762942124422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Reasoning Suppression (ARS), a training-free method designed to improve the efficiency of large reasoning language models. ARS dynamically suppresses unnecessary reasoning through adaptive certainty monitoring, where the model establishes multiple checkpoints during generation, prompts for tentative answers, and evaluates their confidence scores to decide whether to continue or suppress further reasoning. Empirical analysis shows some improvements on metrics like LAT. while maintaining accuracy."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper is targeting a real-world problem of easing the overhead for long reasoning output trace."}, "weaknesses": {"value": "I am very concerned about the writing quality of the paper. On a high level. The paper lacks too many details, both in methodology and experimental results. There is also no related work, background nor detailed explanation of the methods, making it difficult to understand.\n\n\nStarting from section 2, what is “MULTI-CHECKPOINT” vs. “single checkpoint”.\n\nHow the equation 2 is designed? Why choose those configurations like “80”/”10” and design the formula in such a way. In the meantime, what is the mathematical keyword? Why counting “mathematical keywords” and “symbols” helped evaluating the confidence.\n\nHow is theorem 3 correct? Is there a proof instead of proof sketch. Even if it is correct, how does it guarantee the efficiency.\n\nPrompting the model in each generation phase adds up the efficiency overhead, do you count it into the end-to-end efficiency analysis?\n\nIt lacks the recent prompt based efficient reasoning baselines i.e. CoD.\n\nChain of Draft: Thinking Faster by Writing Less"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wGMGqnv8WN", "forum": "gTX39XIeMY", "replyto": "gTX39XIeMY", "signatures": ["ICLR.cc/2026/Conference/Submission22226/Reviewer_A2c7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22226/Reviewer_A2c7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938034867, "cdate": 1761938034867, "tmdate": 1762942123796, "mdate": 1762942123796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ARS, a training-free, truncation-based efficient reasoning method."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Efficient reasoning is an important topic under the era of LRM."}, "weaknesses": {"value": "Respectfully, this paper is not in a state suitable for submission. In terms of presentation, it lacks proper formatting and key components, such as related work and an ablation study, which are crucial for supporting the claims.\n\nRegarding novelty, the core idea of truncating reasoning traces at different intervals to assess whether the current reasoning trace is sufficient has already been explored in works such as HALT-COT, DEER, Answer Consistency, FlashThink, and likely many other CoT compression techniques. Unfortunately, none of these works are mentioned nor compared in the paper.\n\nThe experimental setup described in Section3.1 is also lacking in several critical areas, including but not limited to:\n\n1. Qwen 2.5 instruct models are not LRMs, so they are nor appropriate for this task.\n2. #1 makes the only LRM to be the DeepSeek distilled 7B one. Evaluating only one LRM is not enough.\n2. Setting the answer length to 1200 tokens is far too low to be representative. This likely explains why the DeepSeek-Distilled 7B model performs worse than the non-reasoning model of the same family.\n3. Evaluating only 200 questions from common datasets like MATH500 and GSM8K provides an overly limited perspective.\n4. There is a lack of basic experimental settings information, such as temperature, number of runs, etc.\n\nThese are just a few of the many issues with the paper, but I believe they are significant enough to warrant a strong rejection. **My sincere recommendation to the authors is to ensure the work is more complete before submitting it for peer review.** As it stands, the paper feels unfinished even from a page-limit perspective (only 5 pages, with at least one page occupied by pseudo-code and a visualization of already presented tables). As authors, we often hope for thoughtful and engaging reviewers, but part of achieving that comes from submitting work that is fully developed so that the review resource can be better distributede."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lqIfvJrAxO", "forum": "gTX39XIeMY", "replyto": "gTX39XIeMY", "signatures": ["ICLR.cc/2026/Conference/Submission22226/Reviewer_9Bwa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22226/Reviewer_9Bwa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997279762, "cdate": 1761997279762, "tmdate": 1762942123538, "mdate": 1762942123538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adaptive Reasoning Suppression (ARS), a training-free approach that applies multi-checkpoint certainty estimation to suppress the reasoning process. However, the submission is incomplete. Key sections such as Method and Experiments are only partially written, making it difficult to evaluate the paper’s contributions and merits."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper is currently incomplete, and therefore no clear strengths can be identified at this stage."}, "weaknesses": {"value": "The paper is incomplete, which makes it difficult to understand the proposed methodology, as many essential details are missing. As a result, it is challenging to identify any clear scientific merit at this stage."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EagmlUV0ID", "forum": "gTX39XIeMY", "replyto": "gTX39XIeMY", "signatures": ["ICLR.cc/2026/Conference/Submission22226/Reviewer_ZHz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22226/Reviewer_ZHz9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762300391409, "cdate": 1762300391409, "tmdate": 1762942123250, "mdate": 1762942123250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adaptive Reasoning Suppression (ARS), a training-free approach that applies multi-checkpoint certainty estimation to suppress the reasoning process. However, the submission is incomplete. Key sections such as Method and Experiments are only partially written, making it difficult to evaluate the paper’s contributions and merits."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper is currently incomplete, and therefore no clear strengths can be identified at this stage."}, "weaknesses": {"value": "The paper is incomplete, which makes it difficult to understand the proposed methodology, as many essential details are missing. As a result, it is challenging to identify any clear scientific merit at this stage.\n\nTo be specific:\n- Section 2.2 is intended to introduce the proposed Adaptive Reasoning Suppression (ARS) framework, which consists of three core components: (1) multi-checkpoint certainty estimation, (2) progressive threshold adaptation, and (3) dynamic suppression with adaptive intensity. However, only the first component is partially described in Section 2.2.1, while the other two components are not introduced at all.\n- Experiments: The Main Results section provides only a rough summary of the results in Tables 1&2 and Figures 1&2, without any substantive analysis. It is also difficult to align the narrative with specific results in the tables and figures. The case study in Section 3.3 consists of only two sentences, offering no meaningful insight.\n\nOverall, the submission is approximately five pages long and appears incomplete at this stage."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EagmlUV0ID", "forum": "gTX39XIeMY", "replyto": "gTX39XIeMY", "signatures": ["ICLR.cc/2026/Conference/Submission22226/Reviewer_ZHz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22226/Reviewer_ZHz9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762300391409, "cdate": 1762300391409, "tmdate": 1763572790203, "mdate": 1763572790203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}