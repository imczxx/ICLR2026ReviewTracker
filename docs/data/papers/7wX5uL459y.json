{"id": "7wX5uL459y", "number": 2592, "cdate": 1757158383983, "mdate": 1763629508542, "content": {"title": "DAD-SFT: Dual Attention Distillation for Lightweight UAV Vision-Language Navigation", "abstract": "In recent years, Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) has attracted increasing attention due to its broad applications in scenarios such as autonomous inspection and emergency rescue. Large-scale Vision-Language Models (VLMs) demonstrate strong cross-modal understanding and reasoning capabilities; however, their massive parameter size and computational demands hinder their deployment on resource-constrained devices. \nAlthough lightweight models facilitate efficient deployment, their performance and generalization ability remain limited.\nTo address this challenge, we propose a Dual Attention Distillation into Supervised Fine-Tuning (DAD-SFT) framework. First, Cross-Modal Attention Distillation (CAD) is employed to guide the student model in aligning its semantic focus patterns with those of a powerful teacher model, thereby enhancing its cross-modal perception ability.\nMeanwhile, we introduce a Contrastive Attention Alignment (CAA) that constructs diverse types of negative samples to strengthen the model’s discriminative capability, which in turn improves generalization under complex scenarios. \nSystematic evaluations on the CityNav benchmark demonstrate that our method consistently outperforms mainstream baselines in terms of navigation accuracy, cross-scene generalization, and deployment efficiency, showcasing strong overall performance and practical potential.\nOur code is publicly available for reproducibility.", "tldr": "We distill large vision-language models into lightweight agents via Dual Attention Distillation, achieving strong performance and even surpassing the teacher on CityNav.", "keywords": ["UAV-VLN", "Lightweight Model", "Knowledge Distillation", "Contrastive Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6bb6026a8e1e283442088768afd9abe56dd532b4.pdf", "supplementary_material": "/attachment/20f687a2abe6cdf19998bcc4c3270fe6941a0856.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a vision-language model (VLM) to tackle aerial vision-language navigation tasks.  VLMs have demonstrated strong cross-modal understanding and reasoning.  However, these capabilities scale with the model size while larger VLMs have heavier computational overhead, making them inefficient for real-time decision.  This paper addresses such limitations by distilling large VLMs into compact student models based on the guidance of cross-modal attention maps, where the student models learn to generate similar attention maps as the teacher model.  To enhance robustness, this paper additioanlly proposes a contrastive attention objective with the introduction of negative attention maps.  This paper demonstrates that jointly training with task loss, attention distillation loss and contrastive attention loss makes the student model as performant as the teacher model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well written.  Readers can easily follow the rationale and the implementation details of the proposed method.\n2. This paper demonstrates strong performance on CityNav dataset, outperfroming other baselines by a large margin.\n3. This paper presents detailed ablation study, demonstrating the efficacy of each model component\n4. While the contrastive learning idea is commonly known, combining attention-based distillation with contrastive learning seems novel."}, "weaknesses": {"value": "1. This paper considers navigation actions as texts.  However, texts  are not suitable to represent high-precision floating numbers, required for robotic navigation tasks.\n2. Following Weakness 1, this paper does not consider a strong baseline--OpenVLA [1] / SpatialVLA [2] and $\\pi_0$ [3], as the former predict discretized action tokens and the latter predicts continual action control.  These models should have similar model size (7B) as the student model used in this paper, meanwhile they have shown strong performance in robotic manipulation tasks.  They should be considered as strong baselines for navigation tasks and compared against the proposed method.\n3. This paper lacks discussion of recent studies on training-free aerial navigation policies.  See-Point-Fly [4] proposes that navigation tasks are inherent visual grounding tasks in static scenes.  Since VLMs excel at visual grounding, one can directly use VLM models to generate 2D waypoints for 3D navigation, without the need for training navigation policies.  I'd strongly encourage the authors to include discussions on these new perspectives.\n4. This paper only conducts experiments on a single simulation benchmark, which seems to be relatively small-scale.  I'd highly encourage the authors to evaluate the proposed method on a larger-scale simulation benchmark--OpenNAV[5], to showcase the robustness of the proposed method.\n\n---\n\nReference:\n\n[1] Kim, Moo Jin, et al. \"Openvla: An open-source vision-language-action model.\" arXiv preprint arXiv:2406.09246 (2024).\n\n[2] Qu, Delin, et al. \"Spatialvla: Exploring spatial representations for visual-language-action model.\" arXiv preprint arXiv:2501.15830 (2025).\n\n[3] Black, Kevin, et al. \"$\\pi_0 $: A Vision-Language-Action Flow Model for General Robot Control.\" arXiv preprint arXiv:2410.24164 (2024).\n\n[4] Hu, Chih Yao, et al. \"See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation.\" Conference on Robot Learning. PMLR, 2025.\n\n[5] Qiao, Yanyuan, et al. \"Open-nav: Exploring zero-shot vision-and-language navigation in continuous environment with open-source llms.\" 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2025."}, "questions": {"value": "1. How does DAD-SFT compare to OpenVLA, SpatialVLA and $\\pi_0$ on CityNav?\n2. Can you discuss recent studies that reformulate navigation tasks as visual grounding tasks?\n3. How does DAD-SFT work on OpenNAV?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QUERhu7klC", "forum": "7wX5uL459y", "replyto": "7wX5uL459y", "signatures": ["ICLR.cc/2026/Conference/Submission2592/Reviewer_wPUD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2592/Reviewer_wPUD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761368445817, "cdate": 1761368445817, "tmdate": 1762916293003, "mdate": 1762916293003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the deployment of large Vision-Language Models for UAV navigation on resource-constrained devices. The authors propose DAD-SFT, which distills knowledge from a 32B teacher to a 3B student model through: (1) Cross-Modal Attention Distillation (CAD) that transfers first-layer attention patterns via KL divergence, and (2) Contrastive Attention Alignment (CAA) that uses teacher attention as positive samples with four negative types (random, perturbed, adversarial, cross-instance) optimized via InfoNCE loss. On CityNav benchmark, the 3B student matches or exceeds the 32B teacher's performance (12.96% vs 11.98% SR on Test Unseen) while using 5× less memory (13.53GB vs 70.12GB) and 8× faster inference (6.52s vs 53.42s), enabling single RTX 4090 deployment. Ablations confirm that both mechanisms contribute complementarily to achieving efficient yet accurate UAV navigation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a genuine deployment challenge in robotics/UAV applications: powerful VLMs cannot run on resource-constrained devices.\n- The attention-level distillation approach is conceptually elegant. Rather than distilling outputs or deep features, the method transfers \"where to look\" patterns from the teacher. This is intuitive for navigation tasks that require spatial grounding.\n- The decision to use first-layer attention (rather than deeper layers) is supported by both prior literature and empirical ablations (Table 2), which show that shallow layers capture more transferable cross-modal correspondences."}, "weaknesses": {"value": "- The paper evaluates only on the CityNav dataset within a single simulation environment. For claims about \"cross-scene generalization\" and practical UAV deployment, this is insufficient. The \"unseen\" splits are still part of the same city-scale environment, with similar visual characteristics. No evaluation on other VLN benchmarks (R2R [A], REVERIE [B], SOON [C]), different simulation environments, or real-world data undermines the generalization claims.\n- The CoT training data (2,300 samples) is synthetically generated by the teacher model (Qwen2.5-VL-32B), filtered to remove errors >20m, and then the predicted coordinates are replaced with ground truth. This raises several concerns: (a) the student learns from the same model family it distills from, potentially amplifying biases, (b) replacing predictions with ground truth creates a mismatch between reasoning and answers, (c) only 2,300 samples seems small for training a 3B model, and (d) no analysis of data quality or potential distribution shifts is provided.\n- Both teacher (Qwen2.5-VL-32B) and student (Qwen2.5-VL-3B) share identical architectures from the same model family. This raises critical questions: Does attention distillation work across different architectures? The paper claims this is a \"unified framework,\" but provides no evidence that it generalizes beyond Qwen2.5-VL variants.\n- The paper lacks comparisons with established knowledge distillation methods (FitNet, Attention Transfer, RelationKD) or other attention distillation approaches mentioned in related work (Feng et al. 2025 [D], Elnoor et al. 2025 [E], Zhou et al. 2025 [F]). Also, there is another trend of using MLLMs/VLMs to perform VLN in a zero-shot manner (SPF [G] and OpenFly [H]), which is worth comparing. The \"Naive RL\" baseline is poorly explained (only mentioned in one sentence in Section 4.2).\n\n[A] Weihs, Luca, et al. \"Visual room rearrangement.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\n[B] Qi, Yuankai, et al. \"Reverie: Remote embodied visual referring expression in real indoor environments.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[C] Zhu, Fengda, et al. \"Soon: Scenario oriented object navigation with graph-based exploration.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[D] Ge, Yuyao, et al. \"Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning.\" arXiv preprint arXiv:2509.06461 (2025).\n\n[E] Elnoor, Mohamed, et al. \"Vi-LAD: Vision-Language Attention Distillation for Socially-Aware Robot Navigation in Dynamic Environments.\" arXiv preprint arXiv:2503.09820 (2025).\n\n[F] Zhou, Yang, et al. \"Attention distillation: A unified approach to visual characteristics transfer.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[G] Hu, Chih Yao, et al. \"See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation.\" Conference on Robot Learning. PMLR, 2025.\n\n[H] Gao, Yunpeng, et al. \"OpenFly: A Comprehensive Platform for Aerial Vision-Language Navigation.\" arXiv preprint arXiv:2502.18041 (2025)."}, "questions": {"value": "- Can you provide results on other VLN benchmarks (R2R, REVERIE, SOON), or on other UAV datasets mentioned in the related work, such as the dataset from Wang et al. 2024? Even preliminary results would help assess whether the method generalizes beyond the specific CityNav environment. If not available, can you explain why CityNav alone is sufficient to validate your claims about \"cross-scene generalization\"?\n- What is the impact of your data construction procedure? Specifically: (a) What happens if you use ground-truth reasoning from human annotations instead of teacher-generated CoT? (b) How does performance change if you don't replace predicted coordinates with ground truth? (c) Can you provide an analysis showing the teacher's reasoning quality and how the filtering affects data distribution? (d) Why are 2,300 samples sufficient for training a 3B model?\n- Can you demonstrate that DAD-SFT works when the teacher and student have different architectures (e.g., LLaMA-3.2-11B as the teacher and Qwen2.5-VL-3B as the student, or vice versa)?\n- How do you explain the 3B student achieving higher performance than the 32B teacher on Test Unseen (12.96% vs 11.98% SR)? Did you run multiple seeds and compute confidence intervals? Could this be overfitting, evaluation inconsistency, or statistical noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YAjDLysP7j", "forum": "7wX5uL459y", "replyto": "7wX5uL459y", "signatures": ["ICLR.cc/2026/Conference/Submission2592/Reviewer_3tX4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2592/Reviewer_3tX4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761526616458, "cdate": 1761526616458, "tmdate": 1762916292820, "mdate": 1762916292820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DAD-SFT, a novel framework for lightweight UAV Vision-Language Navigation (VLN) that integrates Dual Attention Distillation into Supervised Fine-Tuning. The authors propose CAD module to aligns the student’s semantic focus and CAA module to boosting discriminative ability. Experiments on CityNav show that DAD-SFT outperforms traditional and VLM-based baselines while maintaining low computational cost suitable for edge deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The combination of cross-Modal attention distillation and contrastive attention alignment module effectively guides the model's attention distribution, boosting model’s robustness and generalization in unseen environments. \n2. This article explores a solution to effectively deploy the professional capabilities of large models on edge devices, greatly optimizing memory and inference speed without sacrificing accuracy."}, "weaknesses": {"value": "1. The success rate is not significantly improved compared to the baseline in prior work and the CityNav paper (SR is 10.16 in val-seen), raising doubts about whether VLM is a suitable baseline for the CityNav task.\n2. The paper emphasizes the advantages of its lightweight model in deployment efficiency and provides an overall inference latency (6.82 seconds/step). However, this is an aggregate latency and does not provides a breakdown analysis on specific modules (perception acquisition , target inference or action planning.\n3. The representation of the Semantic Map and the usage in the model are not provided."}, "questions": {"value": "Whether the experiment is deployed on a real UAV platform?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4mLlTyfHpn", "forum": "7wX5uL459y", "replyto": "7wX5uL459y", "signatures": ["ICLR.cc/2026/Conference/Submission2592/Reviewer_VBWr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2592/Reviewer_VBWr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619737911, "cdate": 1761619737911, "tmdate": 1762916292697, "mdate": 1762916292697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of deploying large, computationally expensive Vision-Language Models (VLMs) for Unmanned Aerial Vehicle (UAV) navigation on resource-constrained devices. Lightweight models, while efficient, typically suffer from poor performance and generalization. To solve this, the authors propose a Dual Attention Distillation into Supervised Fine-Tuning (DAD-SFT) framework. This method uses Cross-Modal Attention Distillation (CAD) to train a lightweight \"student\" model to mimic the semantic focus patterns of a powerful \"teacher\" model. Simultaneously, it employs Contrastive Attention Alignment (CAA), which uses diverse negative samples to enhance the student model's discriminative ability. When evaluated on the CityNav benchmark, the proposed DAD-SFT framework reportedly outperforms baseline methods in navigation accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the critical, real-world trade-off between model performance and computational efficiency. Finding a way to run powerful VLMs on \"edge devices\" like UAVs is a significant and practical challenge for the field.\n2. The core contribution, DAD-SFT, is a novel framework that intelligently combines two distinct techniques: knowledge distillation (via CAD) and contrastive learning (via CAA).\n3. The method achieves impressive results on the CityNav benchmark, consistently outperforming a wide range of baselines, including traditional methods (Seq2Seq, CMA, MGP) and other large VLMs (LLaMA, GPT-4o).\n4. The authors provide an ablation study in Table 1 that validates their design. By comparing \"Naive SFT\" against \"CAD-only,\" \"CAA-only,\" and the full \"DAD-SFT,\" they successfully demonstrate that both components are complementary and contribute to the final model's superior performance."}, "weaknesses": {"value": "1. The proposed model is not trained on the first-person view (FPV) RGB-D observations that an actual UAV would perceive. Instead, the model is fed a \"Semantic Map\", which is a top-down view 2D image of the whole map. This departs from a strict FPV-only setting and can leak strong global priors about where the target is, unlike real FPV drones that only see local observations.\n2. The paper's core experimental setup is fundamentally mismatched with the CityNav baselines it compares against, which, in my view, invalidates the primary performance claims. The final model reports large gains over these baselines while adding a global RGB map view input. These comparisons are therefore not strictly fair to the FPV‑only baselines.\n3. All experiments stay within a single simulator/dataset. The experiments don't test DAD-SFT framework transfer to other UAV VLN settings to test its generalization.\n4. The authors repeatedly claim that the student model surpasses the teacher (Qwen2.5-VL-32B) on SR. However, this comparison is potentially misleading, as the teacher serves mainly as a distillation source rather than a comparative baseline fine-tuned using the same SFT/CoT regimen. This weakens the conclusion that the student is intrinsically stronger rather than advantaged by additional supervision tailored to the task.\n5. While there is a first-vs-last-layer attention distillation study, the work lacks the ablation of sensitivity to $λ_{attn}$, $λ_{contrast}$ and per-negative-type contribution in CAA, which are central to the claimed gains."}, "questions": {"value": "Please see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ht0sJGYq4u", "forum": "7wX5uL459y", "replyto": "7wX5uL459y", "signatures": ["ICLR.cc/2026/Conference/Submission2592/Reviewer_EEFV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2592/Reviewer_EEFV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679072811, "cdate": 1761679072811, "tmdate": 1762916292544, "mdate": 1762916292544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}