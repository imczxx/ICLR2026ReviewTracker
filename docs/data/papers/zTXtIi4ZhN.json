{"id": "zTXtIi4ZhN", "number": 9054, "cdate": 1758108769932, "mdate": 1763644335207, "content": {"title": "CycleIE: Robust Document Information Extraction through Iterative Verification and Refinement", "abstract": "In document AI, reliable analytics require converting long, noisy (often multi-document) corpora into heterogeneous structured data—e.g. tables for numerical fields, graphs for entity–relation structures, trees for hierarchies, and faithful text chunks. Yet one-pass LLM extraction often yields incomplete or inconsistent structures because it lacks explicit verification and opportunities to revise earlier choices. We present CycleIE, an iterative information extraction (IE) framework that closes the loop between reasoning and acting by coupling ReAct with Monte Carlo Tree Search (MCTS). CycleIE employs a multi-agent workflow orchestrated through ReAct and optimized via MCTS to iteratively retrieve, structure, extract, and refine extracted content under verification guidance. This design treats extraction as a search process with feedback, enabling systematic correction of omissions and inconsistencies that defeat one-pass methods, and remains orthogonal to retrieval-augmented generation (RAG) by operating directly over user-provided documents. Experiments on challenging the document-based QA benchmark demonstrate that CycleIE delivers >10% relative improvements in extraction quality over strong one-pass baselines, with the largest gains in lengthy or multi-document contexts.", "tldr": "", "keywords": ["Information Extraction", "Document Question Answering", "Iterative Reasoning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/584181c86932d542e10d429641346644dec10454.pdf", "supplementary_material": "/attachment/fec5aeb5df16c6233c05e1e9ba04a5009946c47e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CycleIE, an iterative information extraction (IE) framework designed to enhance robustness and accuracy when extracting structured data from long, complex documents. Unlike conventional one-pass approaches that directly prompt LLMs to output structured data, CycleIE iteratively verifies and refines intermediate results using a multi-agent architecture (Retriever, Extractor, Verifier, Refiner, Reasoner, Structurer). CycleIE combines the ReAct paradigm for reasoning-action interleaving with Monte Carlo Tree Search (MCTS) for strategic action planning and anomaly intervention. Through iterative verification cycles, the model identifies incomplete or inconsistent information and refines extraction accordingly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation and strong execution. The paper convincingly argues for iterative verification in document IE, a gap in current RAG-based or one-pass extraction systems. The integration of ReAct and MCTS is technically well justified and implemented in a coherent multi-agent pipeline.\n- Comprehensive experimental evaluation. Evaluation spans multiple document lengths and task types (comparison, clustering, reasoning, etc.) across the Loong dataset. The performance improvements, especially under long-context conditions, are large and consistent.\n- Thorough ablation and analysis. The authors conduct clear ablations (w/o verify, w/o extract) and show that verification contributes more to performance than extraction alone. Figure 3 (p. 8) and Table 3 illustrate this quantitatively.\n- Strong reproducibility and implementation details. The paper provides extensive appendices covering prompts, agent workflows, and backend design, making reproduction feasible. The inclusion of a runtime breakdown (Table 4, p. 18) is appreciated.\n- Solid conceptual clarity. The workflow diagram (Figure 2, p. 4) effectively visualizes agent collaboration, making the iterative refinement concept accessible and well structured."}, "weaknesses": {"value": "- Limited conceptual novelty. The framework largely combines known ideas — ReAct reasoning loops and MCTS-based planning — into the IE setting. The novelty lies in applying these together rather than proposing a fundamentally new reasoning paradigm. Compared to recent structured extraction frameworks (e.g., StructRAG 2025, GraphRAG 2024, DataMosaic 2025), CycleIE’s methodological contribution feels incremental.\n- Experimental scope limited to one dataset. All experiments use the Loong benchmark. While this dataset is large and complex, additional validation on other domains (e.g., DocVQA, ContractNLI, or scientific papers) would strengthen claims of robustness.\n- Lack of human evaluation or generalization proof. The evaluation is entirely automatic (GPT-4 judge). Some human annotation or downstream use-case study (e.g., financial or legal analytics) would make the contribution more compelling.\n- Risk of over-engineering. The six-agent architecture, while conceptually elegant, may be overcomplicated for marginal improvements on shorter contexts. The benefit–complexity balance is not deeply analyzed."}, "questions": {"value": "- Generality of the approach. Can CycleIE generalize beyond the Loong benchmark to less structured document sets (e.g., PDFs with layout noise or cross-lingual corpora)?\n- Choice of MCTS. Why is Monte Carlo Tree Search preferred over simpler policy-selection methods (e.g., reinforcement learning with learned value functions)? How sensitive are results to the number of simulations?\n- Failure cases. The case study (Figure 4) shows clear success. Could the authors also share examples where iterative refinement failed or produced contradictions?\n- Ablation on agent granularity. Would merging Verifier + Refiner into one component materially change results or efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uLM9vHudCe", "forum": "zTXtIi4ZhN", "replyto": "zTXtIi4ZhN", "signatures": ["ICLR.cc/2026/Conference/Submission9054/Reviewer_kGJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9054/Reviewer_kGJb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768011396, "cdate": 1761768011396, "tmdate": 1762920765801, "mdate": 1762920765801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel way to incorporate text extractions and verification when working with LLMs to increase task completion success when working with long documents. The model presents significant improvements on several benchmarks, notably the LLM on its own, but the accuracy remains too low to use in accuracy-critical settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper considers an important problem.\n- The approach is sound, albeit somewhat convoluted. \n- The results show significant improvement compared to LLM-only baseline. \n- Extensive experiments."}, "weaknesses": {"value": "I see two key weaknesses for this work:\n\n- I think the comparison with baselines in the paper is not at a good enough level -- yes, there are many baselines, but they have many fewer steps compared to the proposed method. This means that I don't know if the proposed method itself is strong, or just adding a deconstruction and verification step. For example, how superior are the `agents' compared to a set of prompts aimed at carrying out the same function? Another significant baseline should be the new LangExtract, The general approch is sound but the experiments do not prove the specific approch is suprior to exsiting alternatives. \n\nAnother key weakness I see in this paper is that although the improvement is significant -- the results are still not accurate enough to use in critical settings. This is very briefly mentioned in the ethics statments, but not disucssed anywhere. Who do you see adopting your method? For what use cases? Is it possiable to add a human in the loop to improve the accuracy to >95%? (e.g., https://dl.acm.org/doi/full/10.1145/3652591)"}, "questions": {"value": "- Who do you see adopting your method? For what use cases? \n\n- Are you able to compare your method to more competitive baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EhPV3Gmnso", "forum": "zTXtIi4ZhN", "replyto": "zTXtIi4ZhN", "signatures": ["ICLR.cc/2026/Conference/Submission9054/Reviewer_xqjV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9054/Reviewer_xqjV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826158989, "cdate": 1761826158989, "tmdate": 1762920765093, "mdate": 1762920765093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CycleIE, an iterative framework for document information extraction (IE) that combines Reactive Reasoning (ReAct) with Monte Carlo Tree Search (MCTS). Unlike one-pass IE methods that directly extract structured data from documents, CycleIE performs iterative verification and refinement, leveraging multi-agent collaboration (Retriever, Extractor, Verifier, Refiner, Reasoner) to improve data completeness and consistency. Experiments on the Loong benchmark demonstrate notable performance gains (10–37% improvement in LLM and EM scores) over strong baselines such as StructRAG, GraphRAG, and RQ-RAG, especially for long documents up to 250k tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear motivation and solid formulation. The paper convincingly argues that one-pass IE fails to perform self-verification and introduces a principled iterative framework with ReAct + MCTS integration.\n\nTechnically novel combination. The orchestration of six agents with verifier-driven feedback and MCTS-based action optimization is original and well-justified.\n\nComprehensive experiments. Strong empirical results on Loong across multiple tasks (spotlight, comparison, clustering, reasoning) show CycleIE’s robustness and scalability to long documents.\n\nWell-analyzed ablations. The ablation on “w/o verify” and “w/o extract” modules clearly isolates the contribution of verification and modular synergy.\n\nReadable and systematic presentation. The figures and tables are clear, and the technical flow (agents → ReAct → MCTS → experiments) is logically organized."}, "weaknesses": {"value": "Lack of theoretical or statistical significance analysis. The paper reports gains but does not include variance or significance tests across runs, which weakens the empirical rigor.\n\nLimited baselines in iterative IE space. The comparison set focuses on one-pass RAG methods; missing direct comparisons to other iterative refinement or multi-agent reasoning frameworks (e.g., Self-Refine, Graph-of-Thoughts, AutoGen variants).\n\nComputational cost and scalability. No quantitative discussion of iteration overhead, runtime, or resource efficiency; MCTS may scale poorly for very long sequences.\n\nDependence on large base model. All experiments use Qwen2-72B-Instruct; the method’s generalizability to smaller or open-weight models remains unverified.\n\nClarity on reward design. The reward terms in Eq. (3) are conceptually sound but not empirically grounded—hyperparameters (e.g., α, μ, η) are not explained or validated."}, "questions": {"value": "How sensitive is CycleIE’s performance to the choice of α and the reward function components in MCTS?\n\nCould the authors report runtime overhead or number of iterations per query to evaluate cost vs. accuracy trade-offs?\n\nHow would CycleIE perform if smaller models (e.g., Qwen1.5-7B) were used for each agent? Would iterative refinement still yield consistent improvements?\n\nSince the paper claims generality beyond financial QA, has CycleIE been tested on non-numerical document types (e.g., legal or biomedical texts)?\n\nHow are verification thresholds (e.g., completeness ≥ 3) tuned or decided? Are they fixed across datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RgqPAdGUWJ", "forum": "zTXtIi4ZhN", "replyto": "zTXtIi4ZhN", "signatures": ["ICLR.cc/2026/Conference/Submission9054/Reviewer_zMes"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9054/Reviewer_zMes"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065106633, "cdate": 1762065106633, "tmdate": 1762920764696, "mdate": 1762920764696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}