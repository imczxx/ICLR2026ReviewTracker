{"id": "YxJEMTflww", "number": 13897, "cdate": 1758224578637, "mdate": 1759897405435, "content": {"title": "Real-Time Detection of Hallucinated Entities in Long-Form Generation", "abstract": "Large language models are now routinely used in high-stakes applications where hallucinations can cause serious harm, such as medical consultations or legal advice. Existing hallucination detection methods, however, are impractical for real-world use, as they are either limited to short factual queries or require costly external verification. We present a cheap, scalable method for real-time identification of hallucinated tokens in long-form generations, and scale it effectively to 70B parameter models. Our approach targets *entity-level hallucinations* -- e.g., fabricated names, dates, citations -- rather than claim-level, thereby naturally mapping to token-level labels and enabling streaming detection. We develop an annotation methodology that leverages web search to annotate model responses with grounded labels indicating which tokens correspond to fabricated entities. This dataset enables us to train effective hallucination classifiers with simple and efficient methods such as linear probes. Evaluating across four model families, our classifiers consistently outperform baselines on long-form responses, including more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for Llama-3.3-70B), and are also an improvement in short-form question-answering settings. Moreover, despite being trained only with entity-level labels, our probes effectively detect incorrect answers in mathematical reasoning tasks, indicating generalization beyond entities. While our annotation methodology is expensive, we find that annotated responses from one model can be used to train effective classifiers on other models; accordingly, we publicly release our datasets to facilitate reuse. Overall, our work suggests a promising new approach for scalable, real-world hallucination detection.", "tldr": "", "keywords": ["hallucinations", "hallucination detection", "probing", "large language models"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7c2b2a82814f59ff23a1945ef738abf65dd6bc1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focused on entity-level hallucination detection in long-form generations. The authors introduced an annotation pipeline to automatically annotate hallucinations at the entity level. The authors then proposed to detect hallucinations by training a probing model on the LLM's internal states with a designed span-max loss. The authors conducted experiments on four datasets and with two LLMs, showing that their proposed linear probe achieves the highest performance. Additional analysis also showed the importance of training the linear probe on datasets with long-form generations, highlighting the value of their annotated dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear and easy to follow.\n2. The proposed entity-level hallucination detection dataset is valuable and can contribute to future research.\n3. The authors conducted extensive experiments and analysis in different settings."}, "weaknesses": {"value": "1. **The novelty of the annotation pipeline.** The annotation pipeline is highly similar to the one proposed in [1], while the authors did not properly cite and discuss their paper. This greatly questions the novelty of this paper as the annotation pipeline and the proposed dataset are two major contributions of this paper.\n2. **Task definition.** The term \"entity\" in the paper is not clearly defined. The authors only list some examples (e.g., named people, organizations, locations, dates, and citations). It is unclear to me what would be considered an entity and what would not, and whether some essential types of entity are overlooked. The authors should provide more details of explanation or statistics of the entity type. \n3. **Reliability of annotation.** The authors mentioned that they manually verify the quality of entity labels (Line 184). However, the assessment of the reliability of entity extraction is missing in the verification process. To me, entity extraction is less reliable and may be subjective, so it is important to report metrics like precision or recall rate to show that whether the extraction matches the definition of entity.\n4. **Baselines.** Some novel token-level hallucination detection baselines were excluded from the experiments, such as [2] and [3]. The authors should include them to strengthen their claim: \"token-level probes markedly outperform baselines\" (Line 294)\n5. **Lack of deep analysis.** This paper introduced an entity-level dataset, while the experiments only focus on performance comparison without a deeper analysis on how the performance varies across different entities or what entities are easier to be hallucinated by LLMs. Therefore, the contribution of this paper is limited. A deeper analysis could help future research develop better hallucination detection/mitigation approaches at the entity level.\n\n[1]: HalluEntity: Benchmarking and Understanding Entity-Level Hallucination Detection (2025)\n\n[2]: Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus (2023)\n\n[3]: Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models (2024)"}, "questions": {"value": "1. It would be clearer to consistently use either token-level or entity-level within the whole paper.\n2. As shown in [4], there may be sentence- and relation-level hallucinations. How does the proposed pipeline handle these types of hallucination?\n\n[4]: Fine-grained Hallucination Detection and Editing for Language Models (2024)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tPKfaxTtCo", "forum": "YxJEMTflww", "replyto": "YxJEMTflww", "signatures": ["ICLR.cc/2026/Conference/Submission13897/Reviewer_QrDD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13897/Reviewer_QrDD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760479868496, "cdate": 1760479868496, "tmdate": 1762924406274, "mdate": 1762924406274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a streaming, token‑level detector for hallucinated entities (names, dates, citations) in long‑form LLM outputs by training lightweight linear/LoRA probes on hidden states; entity labels are obtained via an LLM‑with‑web‑search pipeline, then mapped to tokens and optimized with a combined tokenwise + span‑max loss. The approach achieves strong long‑form performance and generalizes to short‑form QA and even math reasoning without explicit entities. It further shows cross‑model transfer and explores selective answering by abstaining when probe scores cross a threshold, improving conditional accuracy at the cost of attempt rate. The authors discuss KL‑regularized LoRA to retain model behavior while keeping high detection AUC and commit to release code/data upon publication"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Practical, low‑latency design: token‑level probes run in the forward pass and support real‑time detection during generation; entity focus preserves token alignment for streaming.\n\nConsistent long‑form gains: On LongFact / LongFact++ / HealthBench, linear probes ≥0.85 AUC; LoRA ≈0.89–0.93, beating perplexity/entropy/semantic‑entropy baselines.\n\nRobust generalization: Trained on long‑form, probes transfer to short‑form (TriviaQA LoRA AUC ≈0.97–0.98) and to MATH reasoning (AUC ≈0.88), suggesting the signal captures more than entity fabrication.\n\nCross‑model utility: Probes trained on one model detect hallucinations in others with small degradation; larger‑model probes supervise smaller models effectively.\n\nDeployment‑oriented analysis: KL‑regularized LoRA offers a tunable trade‑off between detection AUC and output distribution shift; authors quantify KL, win‑rate, and MMLU impacts.\n\nSelective answering demo: Monitoring probe scores to abstain boosts conditional accuracy (e.g., Llama‑3.3‑70B from 27.9% → 50.4%) illustrating actionable safety benefits."}, "weaknesses": {"value": "Label noise & ceiling: Annotation pipeline achieves ~80.6% recall on synthetic errors with ~15.8% FPR; this noise constrains both training effectiveness and evaluation confidence.\n\nRecall at practical FPR still modest: On long‑form, R@0.1 ≈0.70 (often 0.65–0.78), which may be insufficient for high‑stakes deployments where misses are costly.\n\nEntity‑centric scope: Focus on entity fabrication misses broader reasoning/relational hallucinations; evidence of MATH transfer is promising but indirect.\n\nAnnotation cost & dependence: Data creation relies on a frontier LLM with web search; while runtime is cheap, upfront labeling is expensive and may depend on proprietary tools whose behavior can drift.\n\nBehavioral side‑effects: Unregularized LoRA can materially shift output distributions (lower win‑rates/MMLU); even with KL regularization, careful calibration is required per deployment.\n\nUtility trade‑off in abstention: Selective answering significantly reduces attempt rate (e.g., 76.1% → 19.1% for Llama‑3.3‑70B at t=0.5), which may limit usefulness in production."}, "questions": {"value": "Annotation efficiency: Can you quantify the cost/time per annotated long‑form sample and explore semi‑supervised or active‑learning loops to reduce reliance on search‑augmented LLM judges?\n\nBeyond entities: How would you extend the streaming detector to claim‑ or relation‑level hallucinations (reasoning chains), while maintaining token alignment and low latency?\n\nThresholding & calibration: How should production systems set and adapt thresholds (per domain/user risk) to balance R@FPR trade‑offs, especially under distribution shift?\n\nHarm‑aware detection: Can the probe be trained to weight hallucinations by potential harm, prioritizing medically/legal‑impactful entities over inconsequential ones?\n\nRobustness of cross‑model transfer: Do probes trained on larger, more up‑to‑date models degrade when evaluating stronger future models where “supported” knowledge expands? Any plans for continual calibration?\n\nHuman evaluation: Beyond AUC/R@FPR, do you have human‑in‑the‑loop studies showing reductions in user‑visible factual errors in realistic workflows (multi‑turn, mixed modality)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eoTs2YeqxX", "forum": "YxJEMTflww", "replyto": "YxJEMTflww", "signatures": ["ICLR.cc/2026/Conference/Submission13897/Reviewer_arys"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13897/Reviewer_arys"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760648239932, "cdate": 1760648239932, "tmdate": 1762924405649, "mdate": 1762924405649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a token-level, real-time hallucination detection framework for large language models (LLMs). By framing hallucination detection as a sequence labeling task, the authors train lightweight linear and LoRA probes to identify fabricated entities in long-form generation. A new dataset, LongFact++, is introduced using web-augmented annotation with Claude-4 Sonnet. Experiments across five model families show consistent AUC gains over baselines like semantic entropy and token-level perplexity. The method generalizes to different models and even to out-of-domain reasoning tasks such as MATH. The authors also demonstrate a real-time selective-answering setup that abstains when hallucination risk is high."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It is quite convincing that framing the hallucination detection as a token-level task for streaming detection is original and practically motivated.\n- empirical results seem effective; The proposed probes outperform semantic entropy and uncertainty-based baselines with clear quantitative evidence."}, "weaknesses": {"value": "- **Questionable conference fit**: The paper is more application-oriented and empirical than methodologically innovative, making it feel more aligned with NLP venues (ACL, EMNLP, NAACL) rather than ICLR, which prioritizes fundamental learning representations or optimization insights.\n- **limited novelty in the methodology**: The core technique—linear or LoRA probes trained on hidden states—is not conceptually new; the contribution lies mainly in data engineering and evaluation. Thus, the review was concerned that constructing an efficient training dataset alone truly gives sufficient information to handle hallucination detection.\n- **Annotation pipeline**: The web-based LLM annotation method is expensive, noisy, and partially unverifiable. Reported human agreement (84%) and false positive rates (~15%) suggest quality issues that may affect probe robustness.\n- **Evaluation depth**: The study lacks qualitative analysis of failure cases and does not compare against strong retrieval-augmented verification systems under similar latency constraints.\n- **Overstated generalization**: While probes show some transfer to reasoning tasks, it’s unclear whether this reflects genuine factuality awareness or spurious correlations in hidden representations."}, "questions": {"value": "Good experimental rigor and motivation, but lacks theoretical depth and venue alignment. \nThe work would be a stronger fit for an NLP-focused conference emphasizing factuality and evaluation rather than ICLR’s learning-theoretic orientation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CP0jy1WhBy", "forum": "YxJEMTflww", "replyto": "YxJEMTflww", "signatures": ["ICLR.cc/2026/Conference/Submission13897/Reviewer_tiwT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13897/Reviewer_tiwT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566546312, "cdate": 1761566546312, "tmdate": 1762924404997, "mdate": 1762924404997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of the paper is to detect hallucinations in long form generations. The motivation of the paper is that previous methods are mostly limited to short form generations and require manual verification. The paper scales to larger generations by considering entity level hallucinations, that is, hallucinations in names, citations etc, rather than focusing on claim level hallucinations. This focus allows the paper to map the hallucination detection task to individual tokens. The key idea is to first obtain a training dataset by performing a web search and annotate the model’s responses for correctness. Then paper then trains linear probes in a manner similar to prior works like Azaria and Mitchell and Orgad et al. The probes can also be accompanied by LoRA adapters. Interestingly, the paper shows that the method can be generalized across domains to math problems as well."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Extension of hallucination detection to long generations is an important problem. Much of prior work indeed seems to focus on short statements with atomic claims.\n2. The experimental analysis is well-thought though. The ablations in Section 5, e.g., checking cross model and cross task transfer are quite interesting.\n3. The idea of early abstention, that is, halting generating when the first hallucination is detected has potentially large practical benefits."}, "weaknesses": {"value": "1. Currently, the paper lacks strong baselines. The paper correctly points out that prior work on long form hallucination detection has to rely on external fact checking for individual claims. However, the paper itself breaks the problem down into checking hallucinations in entities and hence checks for individual hallucinations. Potentially, one could break the whole generation down into sentences and run it through hidden-state based classifiers like those trained by Orgad et al. It would be great to compare with such baselines and compare the performance and the cost.\n2. The paper seems to oversell the contribution a little bit. True that by relying on entity level hallucinations, the paper can perform detection in a streaming manner. However, it just means that the paper solves a very different problem, that is, entity hallucination as opposed to claim hallucination. For instance, some claims do not have to involve entities, e.g., logical and mathematical claims. So it would be nice to tone down the statements a little bit or provide justification of why the paper thinks it is solving the same problem.\n3. The dataset contribution in extending LongFact to LongFact++ (Appendix D) seems relatively small. It is also not very clear what we obtained by gathering the new dataset. True that we get more topics. But does it lead to concrete benefits, like enhancing the cross-domain or cross-model generalization? A comparison between LongFact and LongFact++ would be very helpful in framing the dataset contribution.\n4. The writing could be improved at some places. For instance, Appendix G addresses several problems but the mention of it in the main paper (line 182) is quite hurried. I would suggest bringing more of the text from Appendix D and G into the main paper so that the readers have the necessary context on what problems are being solved."}, "questions": {"value": "1. Will the new dataset and annotations be made publicly available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mXxhM9R09L", "forum": "YxJEMTflww", "replyto": "YxJEMTflww", "signatures": ["ICLR.cc/2026/Conference/Submission13897/Reviewer_H1ec"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13897/Reviewer_H1ec"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999745269, "cdate": 1761999745269, "tmdate": 1762924404612, "mdate": 1762924404612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}