{"id": "ZkP0Mw3PAi", "number": 3921, "cdate": 1757568195439, "mdate": 1759898062732, "content": {"title": "Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning", "abstract": "Decentralized and lifelong-adaptive multi-agent collaborative learning aims to enhance collaboration among multiple agents\nwithout a central server, with each agent solving varied tasks over time. To achieve efficient collaboration, agents should: i) autonomously\nidentify beneficial collaborative relationships in a decentralized manner; and ii) adapt to dynamically changing task observations. In this\npaper, we propose DeLAMA, a decentralized multi-agent lifelong collaborative learning algorithm with dynamic collaboration graphs. To\npromote autonomous collaboration relationship learning, we propose a decentralized graph structure learning algorithm, eliminating the\nneed for external priors. To facilitate adaptation to dynamic tasks, we design a memory unit to capture the agents’ accumulated learning\nhistory and knowledge, while preserving finite storage consumption. To further augment the system’s expressive capabilities and\ncomputational efficiency, we apply algorithm unrolling, leveraging the advantages of both mathematical optimization and neural networks.\nThis allows the agents to ‘learn to collaborate’ through the supervision of training tasks. Our theoretical analysis verifies that inter-agent\ncollaboration is communication efficient under a small number of communication rounds. The experimental results verify its ability to\nfacilitate the discovery of collaboration strategies and adaptation to dynamic learning scenarios, achieving a 98.80% reduction in MSE and\na 188.87% improvement in classification accuracy. We expect our work can serve as a foundational technique to facilitate future works\ntowards an intelligent, decentralized, and dynamic multi-agent system.", "tldr": "", "keywords": ["Multi-agent systems", "collaborative learning", "graph learning", "algorithm unrolling"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a9906ff848989f058b691ad5b61fd70e08f15b37.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Overall, this paper presents a good and comprehensive contribution to multi-agent learning. It tackles a timely problem, enabling multi-agent to learn collaboratively in a decentralized, continually evolving environment, and proposes a solution that is both theoretically grounded and empirically effective. The optimization formulation is built on sound principles and addresses critical aspects like decentralization, adaptability, and communication efficiency. I found the formulation to be well-motivated and technically sound: by integrating graph structure learning and lifelong learning into a unified problem, the authors capture the core challenges of dynamic multi-agent learning in a principled way. The use of alternating convex optimization for this biconvex problem is appropriate and supported by existing theory, and the authors provide convergence analysis to back up the solver’s reliability."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "S1) Timely problem & effective solution. The paper formulates decentralized, lifelong-adaptive multi-agent collaboration under communication constraints as Optimization Problem (1) and instantiates a practical solver validated by quantitative and qualitative experiments;\n\nS2) Novel use of algorithm unrolling (clarity & efficacy). The alternating solver is unrolled into a neural mapping with three clear modules , accompanied by design choices that reduce iterations and make hyper-parameters learnable;\n\nS3) Theoretical and empirical support. The paper provides non-trivial analysis and proofs and corroborates them with competitive comparisons and ablations and qualitative evolution;\n\nS4) Ablation studies and component analysis. The ablation with DeLAMA-WC (no collaboration) and DeLAMA-WM (no lifelong adaptation) isolates module contributions and demonstrates the full model’s benefits on regression metrics."}, "weaknesses": {"value": "* **W1) Scalability & Efficiency**\n\n  * Cost of maintaining the global collaboration matrix. The method maintains and updates a full collaboration matrix $W^{(t)} \\in \\mathbb{R}^{N \\times N}$ and performs per-iteration neighbor exchanges, which can become heavy as agents $N$ grow. This matters for scalability and network overhead.\n  * Communication bottleneck caused by global aggregation. The graph-inference step includes global gatherings (“Gather $(x^k_j, y^k_j)$ via $G^{(t)}$ to Agent $i$”), implying system-wide reductions/broadcasts that may bottleneck large deployments. This matters for communication complexity.\n  * Communication and time overhead of inner loops. Each time step unrolls two inner loops — graph updates ($M_1$) and parameter updates ($M_2$) — before progressing, increasing per-timestamp communication rounds; no explicit practical bounds for $M_1, M_2$ are reported. This matters for latency and throughput.\n  * Computational burden of high-dimensional Hessian calculation. The local module explicitly computes Hessians $H_i^{(t)}$ to build $A_i^{(t)}, b_i^{(t)}$ (Algorithm 4), which may be expensive for high-dimensional models. This matters for per-agent compute cost.\n  * Lack of runtime and bandwidth measurements. The paper does not provide wall-clock/runtime or bandwidth measurements for the unrolled pipeline or the meta-training stage. This limits empirical evidence on efficiency. No direct evidence found in the manuscript.\n\n\n* **W2) The Number of Agents**\n\n  * Limited scale of current experiments. The method has so far been validated on only 5–6 agents and has not been tested under larger-scale or multi-task settings. Therefore, its scalability in high-dimensional, large multi-agent systems remains uncertain.\n\n  * Communication cost of the decentralized Newton step. In the worst case, the decentralized Newton method requires multiple rounds of communication among all neighboring agents to complete one update. The communication rounds may increase rapidly with the number of agents, yet the paper does not report the specific communication or time cost of this step.\n\n  * Lack of runtime and complexity analysis. The paper does not provide measured runtime, bandwidth usage, or other empirical statistics, nor does it analyze algorithmic complexity, making it difficult to assess its efficiency and feasibility in larger systems.\n \n  * Suggested experiments for scalability verification. It is recommended that the authors conduct or include additional experiments or simulations to analyze scalability — for example, simulating 20 or more agents, constructing random communication graphs, and reporting how runtime or communication rounds scale with network size.\n\n  * Alternative analytical evidence if large-scale experiments are infeasible. If large-scale experiments are infeasible, the authors could provide theoretical complexity analyses instead, explaining how each agent’s computational and communication costs per iteration grow with the number of neighbors, thereby helping readers evaluate its applicability in large-scale distributed scenarios.\n\n* **W3) Recency of Baselines and Related Comparisons**\n\n  * Recency of baselines. Most of the baselines (And the Related works) in the paper date before 2023 (e.g., those listed in Table 1 and Table 2). If space permits, it is recommended to include or discuss more recent (2024–2025) decentralized or continual collaboration works to better highlight the progress and positioning of this method relative to the latest research.\n\n  * Citation clarity in Tables 1&2. Some algorithm names in Table 1 lack explicit citation markers, making it difficult for readers to identify their corresponding source papers. The authors are advised to add clear citation references for each baseline method within the table or its caption to enhance traceability and academic rigor."}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No Details Of Ethics Concerns."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4lLxSnMUch", "forum": "ZkP0Mw3PAi", "replyto": "ZkP0Mw3PAi", "signatures": ["ICLR.cc/2026/Conference/Submission3921/Reviewer_iUuu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3921/Reviewer_iUuu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655393038, "cdate": 1761655393038, "tmdate": 1762917098551, "mdate": 1762917098551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “develops DeLAMA for decentralized multiagent learning that enables multiple agents to learn collaboratively in a fully decentralized in a continually adaptive setting without relying on a central server. DeLAMA enables agents to identify useful collaborators and adapt to learn tasks faster. DeLAMA introduces two main components: a decentralized graph structure learning algorithm, which enables agents infer their collaboration strengths based on task similarity, and a lifelong learning mechanism that integrates a compact memory unit to retain past knowledge to mitigate forgetting and to enable continuous adaptation to new data.  DeLAMA also employs algorithm unrolling, transforming its iterative optimization into a neural network that combines the interpretability of optimization theory with the expressive power of deep learning, allowing agents to “learn how to collaborate” through supervised training. Theoretical guarantees demonstrate fast convergence with communication efficiency. Experiments on regression, classification, and multi-robot mapping tasks are provided to demonstrate that DeLAMA outperforms federated and decentralized baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and well-written and can be followed straightforwardly. \n\n2. The idea of enabling autonomously learning collaboration structures and adapting continuously to dynamic tasks is interesting, novel, and of practical importance.\n\n3.Theoretical results are sound and demonstrate convergence guarantee which is very important in decentralized learning settings.\n\n4. Experiments are extensive beyond just demonstrating that DeLAMA works. Various informative analytic and ablative experiments are performed and reported.\n\n5. Evaluation includes regression, classification, and robotics which demonstrate that the proposed method is applicable to various applications."}, "weaknesses": {"value": "1. The optimization involves alternating convex searches, Taylor approximations, and graph Laplacian regularization, then wraps them in algorithm unrolling. This pipeline is computationally heavy and might not be applicable for a lifelong learning setting where learning speed is important. This is different from convergence because we may not many iterations but each each iteration can be very time-consuming.\n\n2. Although theoretically efficient, the Newton-based graph inference and Jacobi-style parameter updates still require multiple message exchanges per iteration. This may create non-trivial latency which can be problematic for lifelong learning settings.\n\n3. Despite experiments on various tasks, the datasets that are used are simple by current standards. It raises the question whether the proposed approach can scale well with more complex tasks that require using complex and large models. \n\n4. Experiments include only 6 agents at most which raises the question of scalability. Experiments need to be expanded and include more agents with more heterogeneity to demonstrate practicality.   \n\n5. Baselines that are used for comparison do not include baselines developed in the past three years. It is important to include all recent developments to demonstrate competitive performance. \n\n6. The code is not provided which makes judgment about performance challenging."}, "questions": {"value": "1. How does the computational and communication cost scale with the number of agents? This is a very important aspect that deserves to be studied.\n\n2. The method enforces sparse collaboration via an L1 penalty. How sensitive is performance with respect to the sparsity level?\n\n3. Is it possible to quantify the actual communication load (e.g., data volume, bandwidth, or latency) compared to baselines, e.g., DSGD or FedAvg through experiments?\n\n4. How sensitive is DeLAMA’s performance to the number of unrolled iterations or hyperparameters? Does the user need to do hyperparameter tuning for an optimal performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pSYh7MTsBt", "forum": "ZkP0Mw3PAi", "replyto": "ZkP0Mw3PAi", "signatures": ["ICLR.cc/2026/Conference/Submission3921/Reviewer_H8aM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3921/Reviewer_H8aM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681636000, "cdate": 1761681636000, "tmdate": 1762917098334, "mdate": 1762917098334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DeLAMA, a decentralized multi-agent lifelong learning framework that enables agents to learn task-specific models, dynamically infer collaboration relationships, and maintain bounded memory. It combines decentralized graph learning, memory units, and algorithm unrolling to enhance efficiency and adaptability. Experiments show DeLAMA outperforms existing methods in tasks like image classification and multi-robot mapping, with theoretical analysis validating its collaboration mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.DeLAMA is supported by comprehensive theoretical proofs, ensuring its stability, efficiency, and convergence.\n2.Uniquely combines decentralized learning, lifelong learning, and dynamic graph learning into a single framework. This innovative approach eliminates the need for a central server and adapts to real-time task changes, offering a versatile solution for multi-agent systems.\n3.The framework enables agents to autonomously adjust their collaboration relationships based on evolving tasks. \n4. incorporates algorithm unrolling reduce communication and computational costs while maintaining high expressiveness."}, "weaknesses": {"value": "1.The experimental results do not demonstrate significant or optimal performance improvements compared to existing methods, raising questions about the effectiveness of DeLAMA in achieving superior outcomes.\n2.The baseline methods used for comparison are relatively old and not state-of-the-art, which may not provide a fair or comprehensive evaluation of DeLAMA`s capabilities.\n3.The experiments are conducted on smaller datasets (e.g., MNIST, CIFAR-10), what about larger or more complex datasets like CIFAR-100. the task seem easy.\n4.The paper lacks detailed computational efficiency, communication costs comparisons.\n5.There is no ablation study to validate the effectiveness of the three key steps(local learning, graph inference, model update). The introduction of neural networks may deviate significantly from theoretical assumptions, and it remains unclear whether all three components are equally effective."}, "questions": {"value": "1.Why are non-optimal results in Table 2 highlighted in bold?\n2.How does DeLAMA perform on larger or more complex datasets, such as CIFAR-100 or ImageNet, compared to existing methods? Are there plans to evaluate its scalability and generalization capabilities in more challenging scenarios?\n3.As mentioned in the weaknesses, could the authors conduct ablation studies to isolate the contributions of the three key components (local learning, graph inference, model update)? This would help clarify whether the introduction of neural networks significantly deviates from theoretical assumptions and whether all components are equally effective.\n4. Has DeLAMA been compared with state-of-the-art methods developed after 2024?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o44mbv1QrF", "forum": "ZkP0Mw3PAi", "replyto": "ZkP0Mw3PAi", "signatures": ["ICLR.cc/2026/Conference/Submission3921/Reviewer_EBsY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3921/Reviewer_EBsY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893144397, "cdate": 1761893144397, "tmdate": 1762917098157, "mdate": 1762917098157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a novel decentralized lifelong-adaptive collaborative learning framework based on numerical optimization and algorithm unrolling, named DeLAMA. It enables multiple agents to efficiently detect collaboration relationships and adapt to ever-changing observations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The study appears to be theoretically sound and technically solid."}, "weaknesses": {"value": "The limitation of this study is the simplicity of its experimental design. The evaluation is restricted to image classification and simulated robot mapping tasks. Assessing performance in real-world scenarios would provide a more comprehensive validation."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eHkRhOpuzl", "forum": "ZkP0Mw3PAi", "replyto": "ZkP0Mw3PAi", "signatures": ["ICLR.cc/2026/Conference/Submission3921/Reviewer_wGXv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3921/Reviewer_wGXv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933176204, "cdate": 1761933176204, "tmdate": 1762917097626, "mdate": 1762917097626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}