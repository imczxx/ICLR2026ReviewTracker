{"id": "VgEEl1UDlg", "number": 5026, "cdate": 1757836050934, "mdate": 1759897999679, "content": {"title": "Insist when Know, Caution when Not Know?  Unveiling LLMs' Fact-Checking Behavior Amidst Knowledge Conflicts", "abstract": "By augmenting large language models (LLMs) with external evidence, tool augmentation has become a prominent approach to addressing the limitations of their static parametric knowledge in fact-checking. However, the extent to which LLMs accept external evidence when it conflicts with their internal knowledge remains unclear. Moreover, do LLMs behave consistently when they parametrically  \\textsc{Know} versus \\textsc{Not-Know} a specific claim?  In this work, we introduce the first fine-grained evaluation framework to systematically probe LLMs’ fact-checking behavior under knowledge conflicts. Our experiments reveal that most LLMs resist conflicts from external evidence when confident (\\textsc{Know}) but are more receptive when uncertain (\\textsc{Not-Know}). We further demonstrate that some models (e.g., Gpt-4o-mini and Llama3-8B) achieve a better balance between openness to correct information and resistance to inaccurate evidence, whereas others (e.g., Deepseek-v3 and Gemini-2.5) tend to be either overcautious or overly credulous.\nTo address the challenge of balancing parametric and external knowledge, we propose a test-time algorithm based on explicit Jensen-Shannon Divergence computations over sampled prediction probabilities, enabling faithful arbitration between external evidence and parametric knowledge. Our method shows competitive performance against eight baselines on our constructed FactConf datasets, improving LLM-based factuality systems in knowledge conflicts.", "tldr": "", "keywords": ["Fact-checking", "Knowledge Conflicts", "LLMs", "Faithfulness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7764c2caaf9de07366086fbbb5498250a967caf3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Focusing on the conflict between LLM’s internal parametric knowledge and external evidence, this paper investigates models’ preferences considering their own confidence. By composing public fact-checking datasets, the authors build a new evaluation benchmark. Besides, they also propose an entropy-based method to generate accurate responses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper provides many insightful findings that may be useful for model selection in real RAG/fact-checking systems.\n- Based on the output confidence (JSD), this paper proposes a simple and effective method to generate faithful responses.\n- The experimental settings are rigid and the metrics are inspiring and may be used in other knowledge conflict tasks."}, "weaknesses": {"value": "- The experiments focusing on evaluating different series of LLMs, ignoring the algorithms’ biases, e.g. SFT/RLHF/RLVR. I understand that they need enormous training resources.\n- Although the evaluated LLMs cover different series of models, there is no consistent evaluation on the model scaling property. For example, it would be better to evaluate one or two series of models with different sizes (Qwen or LLaMA).\n- Although there are many takeaways, some of them are less analyzed and insightful. For example, `Not all LLMs are naturally well-suited for handling knowledge conflict scenarios`. This conclusion and corresponding analysis are basically the description of Table 3 & 4. However, there lacks of in-depth analysis on why this happens. Why some models are prone to be affected than the others? What may be the root cause?\n- The experiments are conducted on the fact checking task with external evidence. Although current metrics cover IR and PR, there are no observations on the aspect of whether the answer is `support` or `refute`. Besides `wrong` and `correct` results, maybe the original answer type matters?\n\nExcept for the SFT and RL settings, I'll raise my overall ratings if the above concerns could be addressed."}, "questions": {"value": "- Do you have additional insights on the threshold $\\tau$ selection? Although it is determined by a small held-out set, different models may demonstrate different preference. Is $\\tau$ relevant with\n- Suggestion: knowledge conflict is an important field in trustworthy generation, especially on RAG systems, maybe future work could extend the evaluation benchmark to a more realistic scenario? Besides the conflict between LLM’s parametric knowledge and retrieved documents, there may be conflicts in the retrieved documents.\n    - Pham et al., 2024, Who’s Who: Large Language Models Meet Knowledge Conflicts in Practice\n    - Su et al., 2024, ConflictBank: A Benchmark for Evaluating the Influence of Knowledge Conflicts in LLM\n    - Gao et al., 2025, Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation\n    - Wang et al., 2025, Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hDR0leXFA9", "forum": "VgEEl1UDlg", "replyto": "VgEEl1UDlg", "signatures": ["ICLR.cc/2026/Conference/Submission5026/Reviewer_8kFW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5026/Reviewer_8kFW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761133383827, "cdate": 1761133383827, "tmdate": 1762917829452, "mdate": 1762917829452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically investigates how LLMs arbitrate between their internal parametric knowledge and conflicting external evidence during fact-checking.\nThe authors introduced a fine-grained evaluation framework that classifies the model's internal knowledge state for a claim as KNOW / NOT-KNOW.\nThe key finding is that, most LLMs resist external conflicts when confident (KNOW) but are more receptive when uncertain (NOT-KNOW). \nTo improve fact verification in knowledge-conflict scenarios, the paper proposes a test-time algorithm that evaluates the stability and consistency of model predictions across multiple runs. This method achieved competitive fact-checking performance compared to other baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The two datasets provided in this paper(FactConf, NewFactConf) are valuable resources that can contribute to improving fact verification systems.\n\n2. The distinction between Know knowledge and Not-Know knowledge presents a good method for observing the internal behavior and knowledge boundary of LLMs."}, "weaknesses": {"value": "1. I find it difficult to fully agree with the main idea of this paper. The authors claim that “models are more susceptible to influence in the NOT-KNOW condition than in the KNOW condition.” However, it is not clear how this finding can help build a better fact verification system. According to lines 106–107, the Know/Not-Know condition is not related to truthfulness. Suppose a user wants to verify a statement and runs the model several times to check whether it “knows” or “does not know” that statement. What should the user do next? Since the user does not know the ground truth, they cannot tell whether the model’s confidence means a correct or incorrect answer. The paper should explain more clearly how this finding can help improve the accuracy of fact verification systems.\n\n2. The explanation of the experiments is not detailed enough, which makes it hard to understand. I have several questions about the experimental design and results, which I will list in the Question section.\n\n3. I am not sure whether the “Conflicting Evidence” examples are truly conflicting. For example, in Table 1, the evidence (e) says BTS collaborated with McDonald’s on May 26, 2021, while the constructed conflicting evidence (e−) says BTS collaborated with KFC on October 12, 2019. These two statements do not actually conflict; both can be true. For a more precise evaluation, the dataset should include examples with real knowledge conflicts, such as “pi is equal to 4.14…,” which is clearly false.\n\n4. The readability of some tables (e.g., Table 3 and Table 5) could be improved. For example, in Table 3, the authors compare the Know/Not-Know ratio across models based on the “#” values. It would be helpful to include the actual ratio values in the table instead of just the raw numbers, so readers do not have to calculate them manually. This would make the tables easier to read and understand.\n\n5. Some parts of the writing are too informal, such as using “vs.” in line 258 and “#” in lines 258 and 319. These should be written in a more formal way.\n\n6. It is unclear which version of the mistral-7B model was used. On Hugging Face, there are several versions such as mistral-7B-v0.1, mistral-7B-instruct-v0.1, mistral-7B-v0.2, mistral-7B-instruct-v0.2, and so on. Since the metrics used in this paper (like Margin and Relative Odds) are sensitive to whether a case is classified as Know or Not-Know, and since performance changes depending on whether the model is instruction-tuned, the authors should clearly state which specific checkpoint was used for each model.\n\n+) Minor typos: in Table 6, “InteralEval”-> “InternalEval,” and in line 122, “he”->“the.”"}, "questions": {"value": "1.According to line 067, the Know and Not-Know categories are determined by whether the results from M independent runs are all consistent or not. Then, in Table 3, how were the Know and Not-Know subsets determined — based on how many independent runs? If this number is the 10 runs mentioned in lines 249–251, what exactly are the values averaged in Table 3?\n\n2. The Correct and Wrong subsets appear to be divided based on the “initial inference,” that is, the model’s first inference under a non-zero temperature compared with the ground truth. If, for example, the model’s first inference is wrong but the remaining nine inferences are correct, does this sample belong to the Correct subset or the Wrong subset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DT0b6JkgYX", "forum": "VgEEl1UDlg", "replyto": "VgEEl1UDlg", "signatures": ["ICLR.cc/2026/Conference/Submission5026/Reviewer_6Am3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5026/Reviewer_6Am3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761313614896, "cdate": 1761313614896, "tmdate": 1762917828669, "mdate": 1762917828669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how LLMs handle conflicts between their internal parametric knowledge and external evidence in fact-checking tasks. The authors introduce a categorization of claims called \"KNOW\" (consistent responses across multiple independent runs, indicating confidence) or \"NOT-KNOW\" (inconsistent responses, indicating uncertainty). They construct two datasets: FactConf, derived from existing benchmarks by injecting conflicting evidence via entity substitution, and NewFactConf, focusing on new events likely not in the existing training data. The evaluation results on the datasets reveal that models are more receptive to external evidence in NOT-KNOW states but vary in robustness (e.g., GPT-4o-mini and Llama3-8B perform better). To improve arbitration, they propose a test-time algorithm using Jensen-Shannon Divergence (JSD) over sampled prediction probabilities to balance parametric and external knowledge, showing competitive performance against baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a timely and relevant issue in LLM reliability, particularly in retrieval-augmented fact-checking, where knowledge conflicts can lead to factual errors. The empirical findings provide useful insights into model-specific behaviors, such as overconfidence in erroneous parametric knowledge (e.g., in Deepseek-v3) versus balanced resistance (e.g., in GPT-4o-mini).\n- The 2 newly introduced datasets, FactConf and NewFactConf, add value by enabling controlled evaluation of conflicts, including temporal ones beyond training cutoffs. NewFactConf, in particular, tests real-world scenarios with recent events, which is a practical extension.\n- JSD-based arbitration is systematic, and demonstrates strong empirical results across benchmarks."}, "weaknesses": {"value": "- Novelty is somewhat limited by existing literature. A comprehensive survey on knowledge conflicts in LLMs (arXiv:2403.08319v1) already categorizes conflicts (e.g., context-memory similar to parametric-external) and discusses fact-checking implications, uncertainty via consistency (e.g., semantic entropy, arXiv:2302.09664), and resolution strategies (e.g., disentangling sources, contrastive decoding). The KNOW/NOT-KNOW distinction echoes prior consistency-based uncertainty estimation (e.g., rephrasing or multiple generations in the survey). Additionally, ConflictBank (arXiv:2408.12076), a benchmark for evaluating LLM knowledge conflicts across misinformation and factual evolution, closely resembles FactConf/NewFactConf in scope and methodology (e.g., controlled conflict injection, model family evaluations). The paper does not cite or compare to these, which overshadows its claim of being the \"first fine-grained evaluation framework\".\n- The entity substitution method for creating conflicts in the new datasets may not fully capture realistic misinformation, as it assumes simple replacements create plausible contradictions. Prior work (e.g., Longpre et al., 2021, cited in the paper) notes limitations in authenticity, and the survey highlights that artificial setups limit generalizability compared to real-world RAG conflicts.\n- Computational overhead is a concern - while many works claim propose consistency-inspired method to improve LLM judgement reliability, the bottleneck in practical deployment of these methods is that they often require a large number of parallel generations. The approach in this paper also relies on multiple inference runs (up to 40 in analyses) for categorization and JSD computation, which could be impractical for deployment, especially for larger models. No discussion of efficiency optimizations or approximations is provided.\n- The evaluation focuses on binary (True/False) fact-checking, potentially limiting insights to more nuanced tasks (e.g., multi-label or explanatory verification) and setups (e.g. factuality issues in long-form generation)."}, "questions": {"value": "- The paper evaluates seven specific LLMs—why these models? Were open-source vs. closed-source differences (e.g., access to probabilities) accounted for in JSD approximations?\n- In NewFactConf, how was \"new\" knowledge verified beyond release dates (e.g., accounting for potential data contamination or generalization from related events)?\n- Could you provide more details on the JSD computation in practice, including how vote entropy approximates probabilities for models without logit access, and its sensitivity to temperature settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fmkX1QoStX", "forum": "VgEEl1UDlg", "replyto": "VgEEl1UDlg", "signatures": ["ICLR.cc/2026/Conference/Submission5026/Reviewer_mXv1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5026/Reviewer_mXv1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761411362239, "cdate": 1761411362239, "tmdate": 1762917828006, "mdate": 1762917828006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper looks at a model's output under knowledge conflict. When it would output something but the context, or a rag tool, provides a conflicting example. They separate two settings, stemming from a bimodal distribution of confidence, one where the model is highly confident which they call KNOW and one where it's not which they call NOT KNOW. they create a dataset of knowledge conflicts for each model, and test their reaction, then propose a method to better steer the choice between parametric and external knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Good to show effect of some variations and ablations - run count and temperature. Good reproducibility, including on prompts. Relevant to the topic, good use and analysis of the different models, which are not just brought as quantitative empirical evidence, the differences between models are analysed and discussed."}, "weaknesses": {"value": "I find the following work relevant to your topic, and should probably be cited: \nOrtu, Francesco, et al. \"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals.\" Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024\n\nAs is usual in this field, the use of the word \"knowledge\" is confusing. Here it is taken to mean high confidence, though sometimes implies correctness. This might bring more confusion than clarity.\n\nYour method looks at multiple runs, instead of making conclusions from a single forward pass. One strong reason other methods avoid it is to not pay the extra compute. Maybe this should also be measured next to accuracy for each method.\nSee questions."}, "questions": {"value": "Do I understand correctly that each model gets it's own dataset ? The type of conflicting evidence probably has a strong impact on the rejection rate here I understand you are trying to emulate likely/plausible conflicting evidence.\n--> does this seem to be working from LLM sampling? Was it tested or just accepted \n--> the LLM self injects variations. Could there be a difference in behaviour depending on high confidence/low confidence?\n\nThe knowledge evaluation method by sampling true/false is entirely prompt based. We know those to have a lot of variance depending on prompt (input) and chosen output tokens to compare during constrained decoding. \nfor example here: \n\nMahaut, M., Aina, L., Czarnowska, P., Hardalov, M., Mueller, T., & Màrquez, L. (2024, August). Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 4554-4570).\n\nHave you checked for variation in the input, and variation in the expected output tokens (true, True, _true, Yes, ...) ?\nI wonder about this in the mistral and Phi settings where they seem to be unstable and refuse external output. Could this be due to another token being most likely and constrained deciding working on the tail of the distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JQauNbBLRd", "forum": "VgEEl1UDlg", "replyto": "VgEEl1UDlg", "signatures": ["ICLR.cc/2026/Conference/Submission5026/Reviewer_dxDn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5026/Reviewer_dxDn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002735528, "cdate": 1762002735528, "tmdate": 1762917827508, "mdate": 1762917827508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}