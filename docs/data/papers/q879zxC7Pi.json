{"id": "q879zxC7Pi", "number": 14775, "cdate": 1758243446056, "mdate": 1763537597621, "content": {"title": "Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search", "abstract": "Protein evolution through amino acid sequence mutations is a cornerstone of life sciences. While current in-silicon directed evolution algorithms largely focus on designing heuristic search strategies, they overlook how to integrate the transformative protein language models, which encode rich evolutionary patterns, with reinforcement learning to learn to directly evolve proteins. To bridge this gap, we propose AlphaDE, a novel framework to optimize protein sequences by harnessing the innovative paradigms of large language models such as fine-tuning and test-time inference. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility for the interested protein class. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. A further case study demonstrates that AlphaDE supports condensing the protein sequence space of avGFP through computational evolution.", "tldr": "", "keywords": ["Directed Evolution", "Protein Language Model", "MCTS"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82de9c3fa2b99c7f1efc744a6a07db94902a82d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors claim to contribute:\n- a new framework for doing in silico directed evolution which takes advantage of pre-trained protein language models as large evolutionary priors. \n- a test-time inference procedure using MCTS on top of a fine-tuned PLM to further optimize protein function.\n- present improved in silico directed evolution of protein compared to a variety of design methods on a variety of assay-based protein predictors."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Idea to bring test-time inference like MCTS from LLMs to protein language models seems novel (I’m not super sure though, not super well contextualized). \n- Results (Table 1) look substantially better than other methods on most of the problems. I’m wondering if it’s a fair comparison given this, some almost seem to good to be true. I don’t personally understand why I would expect such substantial gains over various baseline methods, and over zero-shot.\n- Experiments compare to many methods for protein design, though some relevant ones are missing.\n- Ablations and hyperparameters are documented pretty rigorously in the appendix."}, "weaknesses": {"value": "- Poorly motivated — other work exists using pre-trained PLMs for protein design / directed evolution; these aren’t even cited (e.g., [1], [2], [3], [7]). Methods specifically for multi-round design, like [6], aren’t considered. \n- It's not really made clear why anything about AlphaDE is specific or particular to directed evolution (which is iterative) and not just general protein design. \n- The background section, which appears to serve as a related works section, lists some prior works that can be used to do in silico protein design. However, the authors don’t explain why their method might be a better idea than any of these. They only cite wanting to use PLMs for directed evolution, and don’t motivate this convincingly. Only the last sentence of the background explains the difference from previous works. It also doesn't explain why fine-tuning without using any supervised data is a desirable strategy, when labels are available for all of the test cases they consider in experiments (Table 1). Wouldn't one expect using labels to be helpful?\n- Experiments don't compare to other methods using PLMs, like [1], [2], and [5].\n- It looks like for their experiments (Table 1), they chose to use PEX instead of the stronger PEX+MuFacNet without explanation, even though both are shown in the plots where they take their baseline results directly from [4]. For avGFP, AAV, and TEM problems, PEX+MuFacNet performs better than PEX but this isn't reported.\n- Fig. 2 doesn’t show how good uniform sampling is as baseline, or the base pre-trained model. Even after 1 fine-tuning epoch, AlphaDE looks pretty good, so one might expect the base model to also be pretty good. It's also unclear whether AlphaDE is generalizing substantially, or just sampling from sequences in the fine-tuning dataset--their fitness distribution should also be shown.\n- It’s not surprising that given avGFP’s chromophore (Sec 4.4 / Fig. 5), a PLM can generate a protein very similar to the wild-type. These models have been trained on avGFP. I’d want a comparison to without fine-tuning, or without MCTS to be convinced that their AlphaDE pipeline contributes to this.\n\nI would recommend rejection primarily because their method is not well motivated and situated within related works. Other work exists using pre-trained PLMs for protein design and directed evolution; these aren’t even cited (e.g., [1], [2], [3], [5], [7]). Methods specifically for multi-round design, like [6], aren’t considered. The authors don’t provide a convincing explanation as to why one should expect fine-tuning a PLM on sequences found from a simple homology search, and then using MCTS would outperform all these other design methods. A PLM-based prior could be easily integrated into many of the baseline methods (e.g., CbAS, CMA-ES, BO, probably others that I’m less aware of) for a more equal comparison, but this doesn’t seem to have been done. If the takeaway is primarily that using a PLM as a prior is useful, then the paper should have been written differently. There are no error bars for any of the tables although they claimed to be over replicates. Plots showing the mean value of sequences sampled by each method at each iteration would be more transparent but these are not shown.\n\nMore clearly contextualizing your method as motivated by previous works would help it be much more understandable. Currently, it reads as though you’ve presented a method that works quite well but the reasons aren’t clear, and a reader is forced to take your word for it and just try it out. Adding more intuition throughout the paper for why your method should be better than alternative approaches could help remedy this.\n\n\nCitations: \n\n[1] Jiang, Kaiyi, et al. \"Rapid in silico directed evolution by a protein language model with EVOLVEpro.\" _Science_ 387.6732 (2024): eadr6006.\n\n[2] Yang, Jason, et al. \"Steering Generative Models with Experimental Data for Protein Fitness Optimization.\" arXiv preprint arXiv:2505.15093 (2025).\n\n[3] Tran, Thanh VT, and Truong Son Hy. \"Protein design by directed evolution guided by large language models.\" IEEE Transactions on Evolutionary Computation (2024).\n\n[4] Ren, Zhizhou, et al. \"Proximal exploration for model-guided protein sequence design.\" International Conference on Machine Learning. PMLR, 2022.\n\n[5] Nisonoff, Hunter, et al. \"Unlocking guidance for discrete state-space diffusion and flow models.\" ICLR (2025).\n\n[6] Yang, Jason, et al. \"Active learning-assisted directed evolution.\" _Nature Communications_ 16.1 (2025): 714.\n\n[7] Wang, Chenyu, et al. \"Fine-tuning discrete diffusion models via reward optimization with applications to dna and protein design.\" ICLR (2025)."}, "questions": {"value": "What are the fitness values for the homologous sequences that the PLM is fine-tuned for each protein dataset? What part of your method exactly is yielding a substantial gain—it appears to be the fine-tuning from Table 2, but that doesn’t make sense given that you wrote that the homologous sequences are generated from the worst sequence in the dataset. If the homologous sequences fine-tuned on really are low-value, why is it so helpful to fine-tune on them? Would it similarly help to just fine-tune on the 100 worst sequences in the dataset?\nHow are the baseline methods initialized? Completely randomly (which may be unfair), or with samples from a zero-shot version of the PLM used for AlphaDE? It seems like even without fine-tuning, the PLM has a max-value greater than or similar to many of the baselines, which doesn’t really seem fair."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1tBqGAIQfE", "forum": "q879zxC7Pi", "replyto": "q879zxC7Pi", "signatures": ["ICLR.cc/2026/Conference/Submission14775/Reviewer_KJoU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14775/Reviewer_KJoU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630449265, "cdate": 1761630449265, "tmdate": 1762925128775, "mdate": 1762925128775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes to fine-tune a protein language model to guide the directed evolution for protein design. The work first fine-tune a protein language model with masked language model. Then the fine-tuned model is able to propose mutations. Then the model is used as a policy in an RL framework, where MCTS is performed to find protein sequences with a good fit. From the perspective of machine learning research, the methods in this work are well-known in the machine learning community."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written and easy to understand. [Though I could not evaluate the experiment results because I am not from the area]"}, "weaknesses": {"value": "The most significant issue with the work is the lack of novelty. Most content of the work before the experiment section is known to the field. Specifically, section 3 describes the proposed method, but all the content is known to the community: the problem 3.1 is a well-known problem [1]. The masked language modeling in 3.2 is popularized by BERT [2] and is widely used in network training. The MCTS in 3.3 is popularized by AlphaGo. Therefore, I don't find the innovation in this work. \n\nI could not evaluate the significance of the experiment results as I am not from this area. Even if the results are much better than the state-of-the-art, ICLR might not be the proper venue for this work. \n\n[1] Yang et al. Machine-learning-guided directed evolution for protein engineering. Nature Methods. 2019.\n[2] Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers). 2019.\n[3] Silver, David, et al. \"Mastering the game of Go with deep neural networks and tree search.\" nature 529.7587 (2016): 484-489."}, "questions": {"value": "I hope the author could better justify the contribution of leveraging \"protein language models into directed evolution for effective exploration\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5IJznubEQN", "forum": "q879zxC7Pi", "replyto": "q879zxC7Pi", "signatures": ["ICLR.cc/2026/Conference/Submission14775/Reviewer_XhVw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14775/Reviewer_XhVw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859491015, "cdate": 1761859491015, "tmdate": 1762925128103, "mdate": 1762925128103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AlphaDE finetunes PLMs using MLM on homologus protein sequences, then uses this as a policy network to do MCTS towards high-fitness mutations. A value network is also trained online to accelerate rollouts. Evaluation is done on 8 tasks, against other baselines that follow the \"fitness landscape exploration\" approach to DE tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Low-N fitness prediction is important to enable.\n* Formalizing how RL approaches can be used in PLMs is timely and can lead to productive future works.\n* On the benchmarks explored, performance looks favorable, e.g. on the TEM task."}, "weaknesses": {"value": "* Though this idea of RL post-training for PLMs holds promise, given the current state of the LLM field, the execution becomes quite important, and I think the paper can do better on this in terms of rigor and following through on failure cases. I personally don't think the idea itself is super novel, and I think what makes a paper like this shine would be to really help readers get intuition on how RL post-training will differ for PLMs. Even in terms of base execution, there are some decisions that don't entirely make sense. For example: why use ESM-1b oracle rather than a more recent model? I get there's a desire for consistency with baselines, but I think it's more important to execute well, and reimplement the baselines if needed. \n* Finetuning on homologous sequences is not a new idea; it’s been done since earlier ML for protein design works (Alley et al., 2020, Biswas et al., 2021) as well as recent works (Gordon et al., 2025). This limits the novelty of the work and the completeness of the discussion.\n* Computational costs is a lot higher. Appendix L reports that AlphaDE takes 4.74 hours, vs 0.69 hours for EvoPlay.\n* The simulated landscapes are very toy settings, limiting its applicability to the real world. This is inherent to fitness landscape exploration type works, and not unique to this paper, but nonetheless limits the ultimate impact.\n* Nit: presentation - avoid “impressively” and qualifying words in scientific writing, in line 58.\n\n\nAlley et al, 2020: https://pubmed.ncbi.nlm.nih.gov/33828272/\nBiswas et al., 2021: https://pmc.ncbi.nlm.nih.gov/articles/PMC7067682/\nGordon et al., 2025: https://www.biorxiv.org/content/10.1101/2024.10.03.616542v1"}, "questions": {"value": "* IIUC from the Appendix C2, the process stops when the current mutated sequence fitness is lower than the wildtype. Is that overly greedy? If you take out this termination requirement, how often would you see the fitness climb back up? Given how rugged protein landscapes are, this is type of investigation might be interesting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KOoUVPhdpx", "forum": "q879zxC7Pi", "replyto": "q879zxC7Pi", "signatures": ["ICLR.cc/2026/Conference/Submission14775/Reviewer_Z76E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14775/Reviewer_Z76E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018729469, "cdate": 1762018729469, "tmdate": 1762925127739, "mdate": 1762925127739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AlphaDE, an in-silico directed evolution framework that integrates a fine-tuned protein language model (PLM) with Monte Carlo Tree Search (MCTS) for protein sequence optimization. The PLM is fine-tuned on homologous protein sequences to learn domain-specific evolutionary constraints, while the MCTS performs iterative exploration of the sequence space to identify beneficial mutations guided by the model’s probabilities. Experiments on eight benchmark protein engineering tasks demonstrate that AlphaDE achieves higher fitness improvements than several baselines, including TreeNeuralTS, TreeNeuralUCB, PEX, and AdaLead, even under limited fine-tuning data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and clearly structured, providing both background and algorithmic details.\n\n- Combining fine-tuned PLMs with MCTS is conceptually sound and leverages recent progress in both protein modeling and search algorithms.\n\n- Strong empirical evaluation on multiple protein datasets with reproducible settings.\n\n- Demonstrates few-shot fine-tuning results, suggesting potential data efficiency."}, "weaknesses": {"value": "- Limited novelty: The central idea closely overlaps with existing works on ML-guided directed evolution using protein LMs, particularly \"Protein Design by Directed Evolution Guided by Large Language Models\" (IEEE Transactions on Evolutionary Computation) [1], which already proposed LLM-based mutation guidance; and \"LatentDE: Latent-based Directed Evolution for Protein Sequence Design\" (Machine Learning: Science and Technology) [2], which introduced latent-space optimization for protein design using pretrained models. The proposed fine-tuning and search mechanisms appear incremental rather than fundamentally new.\n\n- The integration of tree search does not significantly advance beyond prior reinforcement-learning or latent-search frameworks (e.g., LatentDE).\n\n- Lacks biological validation or wet-lab evidence to confirm improved protein functionality.\n\n- No clear ablation to isolate contributions of PLM fine-tuning vs. MCTS itself.\n\n- Evaluation largely depends on oracle models; real-world applicability remains uncertain.\n\n*** References:\n\n[1] Trong Thanh Tran and Truong-Son Hy, Protein Design by Directed Evolution Guided by Large Language Models, IEEE Transactions on Evolutionary Computation (Q1, Impact Factor = 14.3), vol. 29, no. 2, pp. 418-428, April 2025, DOI 10.1109/TEVC.2024.3439690.\nURL: https://ieeexplore.ieee.org/document/10628050\n\n[2] Thanh V. T. Tran, Nhat Khang Ngo, Viet Thanh Duy Nguyen, and Truong-Son Hy, LatentDE: Latent-based Directed Evolution for Protein Sequence Design, Machine Learning: Science and Technology (Q1, Impact Factor = 6.3), Volume 6, Number 1, DOI 10.1088/2632-2153/adc2e2.\nURL: https://iopscience.iop.org/article/10.1088/2632-2153/adc2e2/pdf"}, "questions": {"value": "How does the proposed AlphaDE fundamentally differ in principle or expected outcome from prior LLM-guided directed evolution frameworks such as \"Protein Design by Directed Evolution Guided by Large Language Models\" [1] and \"LatentDE\" [2]? Specifically, could you clarify what new insights or capabilities are gained by combining fine-tuned PLMs with Monte Carlo Tree Search beyond improved sampling efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZMRGj0t4SY", "forum": "q879zxC7Pi", "replyto": "q879zxC7Pi", "signatures": ["ICLR.cc/2026/Conference/Submission14775/Reviewer_j9JA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14775/Reviewer_j9JA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762462780071, "cdate": 1762462780071, "tmdate": 1762925127246, "mdate": 1762925127246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "New version of our paper during rebuttal"}, "comment": {"value": "Dear AC and Reviewers,\n\nWe have updated a new version of our paper during the rebuttal based on your comments and suggestions. More baselines, more related works, and more motivation descriptions are added! Enjoy this version and feel free to touch us!\n\nBest,\nThe authors"}}, "id": "fZTFIjsVj1", "forum": "q879zxC7Pi", "replyto": "q879zxC7Pi", "signatures": ["ICLR.cc/2026/Conference/Submission14775/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14775/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission14775/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763537888826, "cdate": 1763537888826, "tmdate": 1763537888826, "mdate": 1763537888826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}