{"id": "wTZt07hzZf", "number": 11325, "cdate": 1758196370440, "mdate": 1759897590242, "content": {"title": "Sparser Block-Sparse Attention via Token Permutation", "abstract": "Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive.\nThis expense stems primarily from the self-attention mechanism, whose $O(N^2)$ complexity with respect to sequence length presents a major bottleneck for both memory and latency.\nFortunately, the attention matrix is often sparse, particularly for long sequences, suggesting an opportunity for optimization.\nBlock-sparse attention has emerged as a promising solution that partitions sequences into blocks and skips computation for a subset of these blocks.\nHowever, the effectiveness of this method is highly dependent on the underlying attention patterns, which can lead to sub-optimal block-level sparsity.\nFor instance, important key tokens for queries within a single block may be scattered across numerous other blocks, leading to computational redundancy.\nIn this work, we propose Permuted Block-Sparse Attention (\\textbf{PBS-Attn}), a plug-and-play method that leverages the permutation properties of attention to increase block-level sparsity and enhance the computational efficiency of LLM prefilling.\nWe conduct comprehensive experiments on challenging real-world long-context datasets, demonstrating that PBS-Attn consistently outperforms existing block-sparse attention methods in model accuracy and closely matches the full attention baseline.\nPowered by our custom permuted-FlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to $2.75\\times$ in long-context prefilling, confirming its practical viability.\nCode will be released after the reviewing period.", "tldr": "", "keywords": ["Sparse Attention", "Large Language Models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d98a362c69ff354a29c787fe448dd349dca8a19.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Permuted Block-Sparse Attention (PBS-Attn), a method leveraging permutation invariance in the attention mechanism to rearrange query and key tokens, thereby improving block-level sparsity and efficiency for long-context large language models (LLMs). The authors claim that, by performing segmented permutations that preserve causality, the method achieves up to 2.75× speedup over FlashAttention with little loss in accuracy on LongBench and LongBenchv2."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Provides a formally correct analysis of permutation invariance in the attention mechanism.\n2. Introduces an efficient implementation using Triton kernels, showing measurable runtime gains."}, "weaknesses": {"value": "1. The paper does not adequately explain why permutation improves block-wise sparsity.\nPrior works (e.g., StreamingLLM, Minference, FlexPrefill, SampleAttention) discuss distinct sparsity patterns in long-context attention—recent-token, column (“vertical”), or slash patterns.\nPBS-Attn only demonstrates improved sparsity empirically without analyzing how permutation aligns with these known patterns. Without this, the observed gains appear heuristic rather than principled.\n2. In sparse attention, which tokens are retained is crucial for model accuracy.\nThe proposed “query-aware key permutation” seems to rely on average attention from the last query block, but the rationale, robustness, and comparison with established token-importance metrics are missing.\nConsequently, it remains unclear why the method preserves accuracy relative to full attention.\n3. The paper reports that PBS-Attn not only speeds up computation but also improves accuracy compared to other sparse methods.\nHowever, the source of this improvement is not analyzed.\n3. The description of how permutations are implemented in memory is vague.\nIf permutation involves physically reordering the K/V cache or attention tensors, the memory movement could dominate runtime, negating theoretical speedups.\nThe paper should explicitly clarify whether permutations are handled via index mapping (logical reordering) or physical memory rearrangement. As attention is typically memory-bound, even small data shuffling can be expensive.\n4. Despite the formal theorem, the proposed method largely appears as an engineering improvement to block-sparse attention rather than a conceptual advance in understanding attention sparsity.\nThere is little theoretical or empirical insight into how permutation interacts with attention distribution, context structure, or causal masking beyond its computational benefit."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PqpcoydWnA", "forum": "wTZt07hzZf", "replyto": "wTZt07hzZf", "signatures": ["ICLR.cc/2026/Conference/Submission11325/Reviewer_FuSK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11325/Reviewer_FuSK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760795076065, "cdate": 1760795076065, "tmdate": 1762922463127, "mdate": 1762922463127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the computational scaling issue of self-attention in large language models (LLMs) by proposing Permuted Block-Sparse Attention (PBS-Attn). The core idea is to leverage the permutation invariance property of the attention mechanism, reordering query or key tokens via segmented permutations to better align attention patterns with block-sparse structures. This method aims to significantly increase block-level sparsity and, consequently, improve memory and speed efficiency during long-context LLM inference, particularly in the prefilling stage. Comprehensive experiments on LongBench and LongBenchv2 benchmarks show PBS-Attn achieves competitive or superior accuracy relative to strong block-sparse baselines with notable speedups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a well-motivated idea grounded in a clear theoretical foundation.\n2. The segmented permutation framework is plug-and-play and agnostic to the block selection algorithm. The approach is modular, supporting extensions and integration with existing block-sparse attention methods\n3. PBS-Attn achieves near-full-attention accuracy with substantial runtime savings, outperforming recent baselines such as FlexPrefill and XAttention."}, "weaknesses": {"value": "1. The method currently targets only the prefill stage. Its applicability to decoding or training phases is not explored.\n2. The paper asserts “minimal performance degradation,” but from Table 1 and Table 2, there are domains or tasks (e.g., Qwen-2.5-7B-1M on LongBench, Code and Few-Shot Learning categories) where PBS-Attn performs slightly below the full attention baseline. No qualitative or error analyses are provided to identify failure modes or classes of inputs for which the approach may underperform.\n3. Figure 2 and the supplementary distinctly show the ability of permutation to focus major attention mass into sparse bands, but there are no breakdowns of how different heads, layers affect the effectiveness of the permutation. For example, does the benefit hold in higher layers or only in early ones? A more systematic visualization set, perhaps showing variance across attention heads or real-world text types, is needed to understand the boundary of efficacy.\n4. How sensitive is the approach to block size ($B$), segment size ($S$), and selection threshold? Are there hyperparameter settings for which the performance or efficiency gain vanishes or reverses?\n5. What are the scaling laws of PBS-Attn efficiency as model size increases? Are there performance cliffs where the overhead consumes speedup?\n6. Can the authors analytically bound or provide intuition or theory for the maximum achievable block sparsity benefit achievable by their segmented permutation approach (Section 3.2, Figure 4), or is the effect purely empirical?\n7. Finally, it would be valuable to clarify whether the segmented permutation could, in extreme cases, lead to information leakage or violations of causal masking."}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kulcLsW0UE", "forum": "wTZt07hzZf", "replyto": "wTZt07hzZf", "signatures": ["ICLR.cc/2026/Conference/Submission11325/Reviewer_NZsM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11325/Reviewer_NZsM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482598359, "cdate": 1761482598359, "tmdate": 1762922462589, "mdate": 1762922462589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Permuted Block-Sparse Attention (PBS-Attn), a method designed to improve the computational efficiency of long-context large language models. The core idea is to leverage the permutation invariance of attention mechanisms to reorder query and key sequences in a way that enhances block-level sparsity, thus reducing redundant computation during the prefilling stage. The paper proposes a segmented permutation strategy that maintains causal structure while reordering tokens within segments, and a query-aware key permutation that aligns important key tokens. The approach is implemented efficiently using custom kernels, achieving up to 2.75× speedup with minimal accuracy degradation across benchmarks such as LongBench and LongBenchv2."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel idea with solid theory: The paper introduces a new optimization axis for sparse attention: token permutation. It builds on formal proofs of permutation invariance in attention, making the approach conceptually sound and mathematically rigorous.\n2. Strong empirical results: PBS-Attn achieves up to 2.75× speedup with minimal accuracy loss, showing consistent gains across two major long-context LLMs and benchmarks.\n3. Orthogonal contribution: The method complements existing block-selection techniques, providing a new dimension for improving block sparsity without modifying model architecture."}, "weaknesses": {"value": "1. Incomplete evaluation: Missing key long-context benchmarks such as InfiniteBench and RULER, which limits understanding of the method’s scalability and robustness across diverse context lengths.\n2. Unclear GQA handling: It remains unclear whether GQA heads share the same permutation pattern or whether the permutation is based on query heads rather than key-value heads.\n3. Limited generality of the scoring method: The query-aware key permutation relies on the final queries, which may not perform well in chunk prefill or multi-round scenarios.\n4. Prefill-only applicability: The method currently accelerates only the prefilling stage; its potential for decoding remains unexplored.\n5. Insufficient overhead analysis: The runtime and memory cost of computing and applying permutations are not deeply quantified, making it difficult to assess net efficiency gains at scale."}, "questions": {"value": "1. How are permutation patterns handled under GQA—do all heads in a group share one pattern?\n2. Could alternative permutation scoring schemes improve robustness beyond final-query reliance?\n3. Is there a feasible way to extend PBS-Attn to decoding or streaming inference?\n4. What is the measured permutation overhead relative to overall speedup, especially beyond 512K tokens?\n5. What is the effect of block size in addition to the Effect of Segment Size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4EGQecO6n2", "forum": "wTZt07hzZf", "replyto": "wTZt07hzZf", "signatures": ["ICLR.cc/2026/Conference/Submission11325/Reviewer_sTLr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11325/Reviewer_sTLr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761487769875, "cdate": 1761487769875, "tmdate": 1762922462228, "mdate": 1762922462228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PBS-Attn, a new block-sparse attention mechanism that permutes queries, keys, and values to better aggregate high attention scores. By leveraging the permutation-invariant and -equivariant properties of queries and key-value pairs, PBS-Attn clusters high attention values more effectively, thereby increasing attention coverage under the same sparsity budget. Experiments compare PBS-Attn with various sparse attention baselines in terms of both performance and prefill latency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear formulation:** The formulation of the attention computation and the analysis of permutation properties are clearly presented.\n2. **Competitive results:** PBS-Attn achieves competitive performance across four sparse attention baselines. Although it does not consistently achieve the best score on every benchmark, it attains the highest average performance overall."}, "weaknesses": {"value": "1. **Relation to prior work:** The paper lacks sufficient discussion of its relation and distinction from previous research. The idea of using permutation to better aggregate sparse attention scores has been explored in prior works such as [1]. The authors are encouraged to highlight the key differences and contributions relative to these methods.\n2. **Global importance score computation:** The rationale for using the last query block to compute global importance is not clearly explained. It remains unclear how well this last block represents the entire attention spectrum, and no statistical analysis is provided.\n3. **Lack of descriptive algorithm explanation:** The core design of the permuted block-sparse attention is condensed into Algorithm 1’s pseudocode, but lacks a detailed descriptive explanation in the main text. This omission may hinder readers’ understanding of the method.\n\n[1] Kitaev, Nikita, Łukasz Kaiser, and Anselm Levskaya. “Reformer: The Efficient Transformer.”"}, "questions": {"value": "1. What is the permutation overhead under moderate context lengths, e.g., 4K tokens?\n2. What is the numerical improvement in attention coverage achieved through permutation?\n3. Did the authors implement any custom CUDA kernels to achieve the reported speedups with PBS-Attn?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2wvU8yywhh", "forum": "wTZt07hzZf", "replyto": "wTZt07hzZf", "signatures": ["ICLR.cc/2026/Conference/Submission11325/Reviewer_qNdW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11325/Reviewer_qNdW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868528928, "cdate": 1761868528928, "tmdate": 1762922461839, "mdate": 1762922461839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}