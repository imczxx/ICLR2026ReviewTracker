{"id": "lopEqVaufB", "number": 24581, "cdate": 1758358132884, "mdate": 1759896759762, "content": {"title": "Beyond Autoregression: Permutation-Invariant Graph Generation with Scalable Edge Construction", "abstract": "Graph generation models have advanced significantly with deep learning, yet they remain limited in scalability, flexibility, and their ability to model underlying structures. We present GraphK, a novel encoder-sampler-decoder framework for graph generation that overcomes these challenges through structural flexibility and computational efficiency. Unlike autoregressive approaches constrained by vocabulary size (i.e. number of nodes in graph generation), GraphK enables both upsampling (generating graphs with more nodes than the input) and downsampling, providing fine-grained control over output graph size. By learning permutation-invariant latent representations and sampling new node embeddings via maximum likelihood estimation, GraphK generalizes across graph sizes and structures. For edge generation, we employ link prediction with a KDTree-based top-k neighbour search in the latent space, reducing computational cost. Based on the manifold smoothness assumption, our method effectively captures graph properties. Experiments on synthetic and real-world datasets show that GraphK outperforms existing methods, accurately learns graph structures, and generates synthetic graphs without requiring explicit definitions.", "tldr": "We propose a non-autoregressive graph generation method that allows flexible control over graph size and structure using node sampling and neighborhood-based edge construction.", "keywords": ["Graph Generation", "Maximum Likelihood Estimation", "Link Prediction", "KDTree"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1f9c13fbfeb0e6defebdfa929c90b3ac92ea577c.pdf", "supplementary_material": "/attachment/66efb1e0b7dd20c107aa6b8b14430991979cfd17.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes GraphK, a novel permutation-invariant and scalable graph generation framework.\n\n\nGraphK follows a modular encoderâ€“samplerâ€“decoder architecture:\n\nEncoder: Maps the input graph into a latent space using either Node2Vec or VGAE embeddings.\nSampler: Fits a Gaussian Mixture Model (GMM) on these node embeddings to enable scalable sampling, allowing upscaling or downscaling of graph size.\nDecoder: Generates edges using a k-nearest neighbor (KDTree) search in the latent space.\n\nGraphK claims three major advantages: 1) Permutation invariance (independent of node ordering)\n2) Scalability (generation beyond training graph sizes), and\n3) Computational efficiency (near-linear complexity).\n\nExperiments on synthetic community graphs, protein datasets, and CiteSeer show that GraphK achieves competitive or superior performance to GraphVAE, GraphRNN, and NetGAN under MMD-based structural similarity metrics (spectral, orbit, motif).\n\nWhile the paper is well written, clearly structured, and easy to follow, the experimental and theoretical evidence supporting its main claims remains limited.\n\n* Limited Comparison to Scalable Deep Baselines:\nWhile the paper compares against GraphRNN, NetGAN, and DiGress, it omits scalable graph generators such as **BiGG [1]**, which achieves state-of-the-art structure preservation with memory cost ð‘‚(ElogN). Including BiGG baselines would enable a comprehensive evaluation.\n\n* Permutation Invariance: \nAlthough GraphK emphasizes permutation-invariant graph generation, the paper provides no formal proof or justification of this property. In particular, it is unclear how the gradient updates with respect to parameters remain invariant under node permutations, especially given that the Node2Vec encoder is not strictly permutation invariant (its random-walkâ€“based embeddings depend on node identities).\n\n* Scalability:\nThe framework is theoretically claimed to scale to graphs with >10k of nodes [table 1] due to its ð‘‚(ð‘logð‘) decoding complexity. However, the largest empirical test involves graphs of only about 4,000 nodes. Demonstrating results on large graphs (>10K nodes) would provide stronger empirical support for this scalability claim.\n\n\n* No Ablation on Encoder Type:\nThe paper claims flexibility in encoder choice (Node2Vec, VGAE, etc.), yet no ablation or quantitative analysis is presented to validate this claim. Since the encoder choice critically influences the latent representation and downstream sampling, an encoder ablation would strengthen the paper claim.\n\n* Limited Dataset:\nThe experiments are limited to small and medium-sized graphs (protein, community, and CiteSeer datasets). Expanding to widely used benchmarks such as grid and lobster graphsâ€”used in prior works including BiGG and GraphRNN would better demonstrate the generality of the proposed framework.\n\n[1] Dai, Hanjun, et al. \"Scalable deep generative modeling for sparse graphs.\" International conference on machine learning. PMLR, 2020."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear and Modular Framework\n\nNovel Latent Sampling Strategy"}, "weaknesses": {"value": "No End-to-End Training Objective\n\nLimited empirical and theoretical support\n\nSmoothness Assumption in Decoder"}, "questions": {"value": "How is the encoder fine-tuned during training. Is it frozen after pretraining, or updated jointly with the edge decoder?\n\nHow sensitive is GraphKâ€™s performance to the k parameter in KDTree edge construction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yJog62vKNk", "forum": "lopEqVaufB", "replyto": "lopEqVaufB", "signatures": ["ICLR.cc/2026/Conference/Submission24581/Reviewer_FTyv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24581/Reviewer_FTyv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760992304163, "cdate": 1760992304163, "tmdate": 1762943127294, "mdate": 1762943127294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel graph generation framework named GraphK, which aims to address three key challenges in current deep graph generation models: the lack of permutation invariance, the difficulty in generating graphs of sizes different from the training set, and high computational costs. GraphK adopts an \"encoder-sampler-decoder\" paradigm. It achieves permutation-invariant, size-flexible, and computationally efficient graph generation by mapping graphs into a latent space, sampling new node embeddings within this latent space, and efficiently reconstructing edges based on geometric proximity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of a permutation-invariant encoder, GMM-based latent space sampling, and efficient geometric decoding within a unified framework is conceptually clear and well-structured. This approach cleverly decouples graph structure learning from size generation, offering a novel solution for size-agnostic graph generation.\n\n2. The paper provides a detailed time complexity analysis for each stage of GraphK, explicitly stating that the generation process has a near-linear complexity of O(N log N). This represents a significant potential advantage compared to many autoregressive or diffusion models with quadratic complexity."}, "weaknesses": {"value": "1. The baseline comparisons in Table 2 are not sufficiently comprehensive. The most recent baseline included is from 2022. The authors should, at a minimum, compare against more recent methods listed in Table 1, such as EDP-GNN and GDSS.\n\n2. While the authors claim the method is efficient and provide a theoretical time complexity analysis, there is a lack of comparison experiments with other methods regarding practical metrics like actual training time and inference overhead.\n\n3. The paper claims the method is scalable, but the experiments primarily rely on synthetic data and medium-scale graphs. It remains unclear whether GMM fitting would become a computational bottleneck on larger-scale graphs."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xbxVjlgZcN", "forum": "lopEqVaufB", "replyto": "lopEqVaufB", "signatures": ["ICLR.cc/2026/Conference/Submission24581/Reviewer_o1tc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24581/Reviewer_o1tc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159516474, "cdate": 1762159516474, "tmdate": 1762943127046, "mdate": 1762943127046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method in the graph generation field that has three main parts: encoder, sampler, and decoder. Their key contributions are:\n- The generation process does not depend on how nodes are labeled or ordered in the input.\n- By fitting a GMM to the node embeddings and flexible sampling, the model allows the generation of larger or smaller graphs while preserving key structural patterns from the original input.\n- By constructing a decoder based on a k-nearest neighbor strategy and constructing a KD-tree on the sample latent embeddings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper contributes by confronting two issues in graph generation: scalability and ensuring permutation invariance. The authors dedicate significant effort to reducing the notorious computational complexity in this domain. \n- The core strength lies in the novel, three-part GraphK framework. Encoder: The Encoder is remarkably flexible. By intelligently blending both shallow and deep learning encoders, the model is well-equipped to handle a variety of graph types and structural complexities. Sampler that uses a Gaussian Mixture Model (GMM) to model diverse node representations. \n- The decoder component integrates a KD-tree for highly efficient nearest-neighbor search, aiming to address the computational complexity."}, "weaknesses": {"value": "- The experiments conducted are severely limited (Table 2, Figures 2 and 3). The comparison methods are highly restricted and mostly do not reflect the Current State-of-the-Art, with the proposed approach failing to be benchmarked against any of the state-of-the-art methods from recent years (especially those from 2024 and 2023).\n- In Figure 2, which aims to compare models in generating graphs similar to the original graph, no evaluation metric is introduced; the comparison relies solely on the visual appearance of the graphs. Furthermore, the proposed model is compared only against two other models in this section, which are Not Reflective of the Current State-of-the-Art.\n- The claim regarding the flexibility of the Encoder in handling various graph types and structures was not adequately investigated in the experiments, and the results do not convincingly demonstrate this versatility.\n- The claim of being scalable is not sufficiently covered in the experimental section. It is expected that the proposed method will be compared against State-of-the-Art methods explicitly designed for scalable graph generation.\n- In the Decoder section, the manifold smoothness assumption is introduced but presented without any explanation or theoretical justification as to why this assumption is logical or valid for the proposed method. It was expected that theoretical reasons and proofs would be provided to support this critical assumption."}, "questions": {"value": "According to the identified weaknesses, it is recommended that the following questions and suggestions be addressed:\n- Address the limited number of experiments and the lack of use of current, state-of-the-art methods in Table 2 and Figures 2 and 3.\n- Design experiments specifically to demonstrate the claim regarding the flexibility of the encoder.\n- Introduce a quantitative metric for the comparisons presented in Figure 2, and do not rely solely on the visual images of the output graphs from each method.\n- Design experiments for a more rigorous investigation of scalability, specifically through comparison with state-of-the-art methods.\n- Could you explain more about the smooth manifold assumption, including why this assumption is reasonable in the context of the proposed method, and try to offer theoretical proof for it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hMLiS3NxUJ", "forum": "lopEqVaufB", "replyto": "lopEqVaufB", "signatures": ["ICLR.cc/2026/Conference/Submission24581/Reviewer_Kr27"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24581/Reviewer_Kr27"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762196608077, "cdate": 1762196608077, "tmdate": 1762943126735, "mdate": 1762943126735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose GraphK, which addresses the generalization issues in graph generation models through structural flexibility and computational efficiency. GraphK supports generating graphs with an arbitrary number of nodes. Experiments on multiple datasets demonstrate that GraphK outperforms existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality. The paper is original, and the capability to support upscaling is interesting.\n\nQuality. The manuscript is of high quality, with rich and comprehensive experimental results.\n\nClarity. I fully understand what the authors are conveyingâ€”the writing is clear and well-structured.\n\nSignificance. I remain somewhat skeptical about the paper's overall significance. Beyond introducing a graph generation method that supports upscaling, I cannot understand additional substantial contributions. I will elaborate on this concern in the Question and Weaknesses sections."}, "weaknesses": {"value": "1.Upsampling appears to be a popular approach in graph generation (please correct me if I'm mistaken). The authors should more clearly articulate the core contribution of this work.\n\n2.The authors discuss a large number of related works in Table 1; however, their experiments do not include comparisons with the most recent methods (e.g., EDP-GNN and GDSS), which raises concerns about the validity of their experimental results."}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bsUxUzu1Q5", "forum": "lopEqVaufB", "replyto": "lopEqVaufB", "signatures": ["ICLR.cc/2026/Conference/Submission24581/Reviewer_7Utq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24581/Reviewer_7Utq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762230055780, "cdate": 1762230055780, "tmdate": 1762943126488, "mdate": 1762943126488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a methods, named GraphK, which is encoder-sampler-decoder framework for graph generation that overcomes the challenges of scalability, flexibility in generating the number of nodes (upscaling /downscaling) and ability to model underlying structures. GraphK learns permutation-invariant latent representations and sampling new node embeddings via maximum likelihood estimation and predicts edge by using KDTree based top-k neighbor search in latent space."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors focuses on an important problem of  scalability for graph generation."}, "weaknesses": {"value": "1. Over all the paper is hard to follow with unclear details for example, the abstract mentions that GraphK provides fine-grained control to generate graphs , however there is no mention of what controls are?  Is the paper referring to spectral, orbit, and motif mention in the result Table 2? If so, Is spectral, orbit, and motif are the only attribute chosen? Was there any motivation of using only those ?\n2. As evident from Figure 2, other baseline models performs better than proposed approach GraphK. Number of nodes and edges predicted by GraphK are more far away from the ground truth/original graph compared to the other baseline models there by questioning the effectiveness of the approach.\n3. The paper would benefit by comparing from more recent baselines for example like GruM, EDGE, and SPECTRE\n4. Over all, the evaluation is weak and paper would benefit with more clear explanation of fine-grained control."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jDwFwfCebw", "forum": "lopEqVaufB", "replyto": "lopEqVaufB", "signatures": ["ICLR.cc/2026/Conference/Submission24581/Reviewer_hVeq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24581/Reviewer_hVeq"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762320155135, "cdate": 1762320155135, "tmdate": 1762943126263, "mdate": 1762943126263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}