{"id": "bzjrWNlWjG", "number": 13036, "cdate": 1758212920529, "mdate": 1759897469694, "content": {"title": "The Alignment Bottleneck", "abstract": "Large language models improve with scale, yet feedback-based alignment still exhibits systematic deviations from intended behavior. Motivated by bounded rationality in economics and cognitive science, we view judgment as resource-limited and feedback as a constrained channel. On this basis, we model the loop as a two-stage cascade $U \\to H \\to Y$ given $S$, with cognitive capacity $C_{\\mathrm{cog}|S}$ and average total capacity $\\bar{C}{\\mathrm{tot}|S}$. Our main result is a capacity-coupled Alignment Performance Interval. It pairs a data size-independent Fano lower bound proved on a separable codebook mixture with a PAC-Bayes upper bound whose KL term is controlled by the same channel via $m , \\bar{C}{\\mathrm{tot}|S}$. The PAC-Bayes bound becomes an upper bound on the same true risk when the canonical observable loss is used and the dataset is drawn from the same mixture. Under these matched conditions, both limits are governed by a single capacity. Consequences include that, with value complexity and capacity fixed, adding labels alone cannot cross the bound; attaining lower risk on more complex targets requires capacity that grows with $\\log M$; and once useful signal saturates capacity, further optimization tends to fit channel regularities, consistent with reports of sycophancy and reward hacking. The analysis views alignment as interface engineering: measure and allocate limited capacity, manage task complexity, and decide where information is spent.", "tldr": "We reframe LLM alignment as an information bottleneck problem, showing that the limited capacity of human feedback imposes hard theoretical limits on performance and explains phenomena like reward hacking.", "keywords": ["AI Alignment", "Large Language Model", "Reinforcement Learning From Human Feedback", "Information Theory", "PAC-Bayes"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16bc6d05c64a1468efa4a097e7a87d8b797b8e26.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyzes the AI alignment problem from an information-bottleneck perspective. The process is modeled as a two-stage cascade, modeling the users’ true preferences ($U$), the noisy information they emit ($Y$) through and intermediate stage ($H$), and the context ($S$). The authors present a Fano risk lower bound under mixture assumptions, a PAC-Bayes upper bound, and discuss the theoretical implications of the results."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* Interesting perspective on a timely and well-motivated topic.\n* Analysis provides both lower and upper bounds."}, "weaknesses": {"value": "* Significance of results is unclear, and practical implications are not explicitly discussed.\n* Presentation is unclear, and the paper was slightly hard to follow.\n* No empirical validation of findings."}, "questions": {"value": "* What are the channel capacities expected to appear in practice?\n* What are the practical implications of the presented results, and how may they inform the design of AI systems in practice?\n* What are the limitations of the analysis? When is it expected to hold \"tightly\", and when are assumptions expected to break?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rI0BRCe7Pj", "forum": "bzjrWNlWjG", "replyto": "bzjrWNlWjG", "signatures": ["ICLR.cc/2026/Conference/Submission13036/Reviewer_y4Pn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13036/Reviewer_y4Pn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944254971, "cdate": 1761944254971, "tmdate": 1762923771624, "mdate": 1762923771624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper models human--AI feedback as a two-stage, capacity-limited channel (U - H - Y) conditioned on context \\(S\\). It derives (i) a data-size--independent Fano-style lower bound on true risk using separable codebooks, and (ii) a PAC--Bayes upper bound whose KL term is explicitly controlled by the same channel capacity via $C_{tot|S}$ Under a canonical observable loss and matched codebook mixture, these yield a two-sided ``Alignment Performance Interval,'' implying that more labels alone cannot beat the lower wall; required capacity scales with value complexity ${\\log}M$; and over-optimization fits residual channel regularities (e.g., sycophancy or reward hacking)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1 Clear, simple formalization of the human loop. The cascade $U-H-Y$ with cognitive and articulation capacities is crisply defined and linked to information-bottleneck/rate--distortion intuitions; a central proposition bounds  $I(U;Y|S)$ by the average total capacity.\n\n2. The key novelty is turning the KL complexity in PAC--Bayes into an environmental budget, aligning the ceiling with the same capacity that drives the Fano floor---an elegant, unifying perspective.\n\n3. The consequences of the paper are (i) a lower bound independent of dataset size, (ii) necessary capacity scaling with  ${log}M$, and (iii) a mechanism for channel overfitting---translate theory into design levers (measure/allocate capacity, manage value complexity, regularize residual information)."}, "weaknesses": {"value": "No empirical validation: The theory is compelling, but there is no empirical study (even toy) quantifying $C_{tot|s}$ or demonstrating the predicted saturation/overfitting behavior under controlled capacity budgets across alignment protocols."}, "questions": {"value": "My only question is regarding the empirical evaluations. Do the authors have any plans to get any empirical results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RX3iSeMD0s", "forum": "bzjrWNlWjG", "replyto": "bzjrWNlWjG", "signatures": ["ICLR.cc/2026/Conference/Submission13036/Reviewer_q9VB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13036/Reviewer_q9VB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977224199, "cdate": 1761977224199, "tmdate": 1762923771283, "mdate": 1762923771283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that aligning LLMs with human values is limited by human cognitive capacity. The authors treat human feedback as a bounded-information channel and show that there is a hard lower bound on how well alignment can work when human judgment is limited. They combine classical information-theoretic tools (Fano bounds) and PAC-Bayes theory to prove that even with more data, alignment performance cannot surpass this capacity bottleneck."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is mathematically rigorous and formally couple a Fano lower bound with a PAC-Bayes upper bound using the same human-feedback capacity term, yielding an alignment performance interval that clarifies when and why increasing data does not improve alignment."}, "weaknesses": {"value": "-Related prior work around the connection between bounded rationality and alignment is missing. For instance, see Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time (arXiv:2505.23729), which appears to have explicitly connect alignment and bounded rationality. Incorporating this reference would help contextualize your contribution and clarify how your information-theoretic perspective differs from and extends that prior framing.\n\n- But as such, the connection made between bounded rationality and information theory seems forced, without much discussion or evidence. While the theoretical analysis is well done, the need for it and the key rationale behind studying this problem is not clear. \n\n- The authors immediately went into formulating feedback loop as two stage cascade framework, but how it would help or what problem it is aiming to solve exactly is unclear. \n\n- In problem setup, what is the exact bottleneck of alignment is not defined, or clear from the discussion. \n\n- Is the analysis presented in the paper is for parametrized settings or it does not matter, a discussion would help? \n\n- Would the analysis of this work could shed some light or guidance on the type of feedback one should use for alignment, such as preference feedback?  Is it optimal? \n\n- There are no empirical results in the paper. I understand that the work in theoretical in nature, but is it possible to provide some basic experiments to may be just connect what is the lower bound, and how we are doing currently with the available methods to motivate the importance of these lower bounds. \n\nNote: I will rely on other reviewers to comment on the mathematical novelty of this work, since I am not an expert in information theory."}, "questions": {"value": "Please refer to the discussion in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jrQevQd6kU", "forum": "bzjrWNlWjG", "replyto": "bzjrWNlWjG", "signatures": ["ICLR.cc/2026/Conference/Submission13036/Reviewer_2NMe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13036/Reviewer_2NMe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042030423, "cdate": 1762042030423, "tmdate": 1762923770928, "mdate": 1762923770928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that the primary alignment problems in large language models come from a fundamental information bottleneck between humans and models treating human feedback as passing through a limited channel with a fixed capacity.  The paper builds this argument using simple information-theoretic tools (like Fano’s and PAC–Bayes bounds) to show both lower and upper performance limits that depend on this same “human capacity.”"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides an interesting way to connect cognitive science and information theory to alignment in a clean, mathematical way.\n- Gives a natural explanation for problems like reward hacking which happens when models overfit beyond what the human feedback can represent.\n- Highlights that just collecting more data alone won’t fix alignment unless human feedback capacity increases."}, "weaknesses": {"value": "- The analysis and the conception is interesting however is not directly measurable or testable in real LLM pipelines.\n- The analysis builds on standard tools of information theory primarily under the constant “capacity” for humans assumption. However, the connection is weak and is not clear what are the key new aspects coming out from the analysis? This makes it hard to evaluate the paper\n- Can the authors provide empirical demonstration - even a toy example of the hypothesis? Also, explaining what are the key-terms that are new or novel to the LLM/Agent Alignment paradigm? Even connecting with relevant papers and showing which term in the bound is new or provide some insights is crucial.\n\nNote : I am open to update my view after understanding the key new aspects of this understanding."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "58nCEeXfFi", "forum": "bzjrWNlWjG", "replyto": "bzjrWNlWjG", "signatures": ["ICLR.cc/2026/Conference/Submission13036/Reviewer_Yk7L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13036/Reviewer_Yk7L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066584540, "cdate": 1762066584540, "tmdate": 1762923770444, "mdate": 1762923770444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the author presents a theoretical framework explaining why feedback-based alignment methods for LLMs lead to failures like sycophancy and reward hacking, despite scaling. The central idea is that the human feedback loop is a resource-limited information channel, imposing a fundamental alignment bottleneck. The paper models this process as a two-stage cascade: $U \\rightarrow H \\rightarrow Y \\text{ given } S$ (Value $\\rightarrow$ Judgment $\\rightarrow$ Feedback). This channel is limited by a finite cognitive capacity ($\\overline{C}_{\\text{tot}|S}$) due to bounded rationality. The authors argue that this single-capacity coupling leads to key implications: 1. Scaling dataset size alone is insufficient to overcome the bottleneck. 2. Aligning on more complex values requires a corresponding increase in channel capacity. 3. Once this capacity is saturated, a powerful optimizer will fit the rater's biases, providing a theoretical explanation for reward hacking and sycophancy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Motivated by concepts from bounded rationality, the author provides a principled framework that reframes alignment failure as an information-channel limit.\n\n2. In the Alignment Performance Interval bound, the coupling of the Fano lower bound and the PAC-Bayes upper bound using the single channel capacity term ($\\overline{C}_{\\text{tot}|S}$) is interesting."}, "weaknesses": {"value": "1. The paper is motivated entirely by theoretical analysis and does not present experiments to validate the theoretical claims. A synthetic experiment to support the claims would make the paper stronger.\n\n2. The framework primarily relies on the average total capacity, $\\overline{C}_{\\text{tot}|S}$. However, the paper provides no discussion on how one might estimate or measure this quantity in a real-world setting."}, "questions": {"value": "Please refer weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "L03VEHTQee", "forum": "bzjrWNlWjG", "replyto": "bzjrWNlWjG", "signatures": ["ICLR.cc/2026/Conference/Submission13036/Reviewer_Uxb5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13036/Reviewer_Uxb5"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155514606, "cdate": 1762155514606, "tmdate": 1762923769739, "mdate": 1762923769739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}