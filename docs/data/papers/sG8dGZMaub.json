{"id": "sG8dGZMaub", "number": 15982, "cdate": 1758258038413, "mdate": 1763744181651, "content": {"title": "Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers", "abstract": "Text-to-video and image-to-video generation have made rapid progress in visual quality, but they remain limited in controlling the precise timing of motion. \nIn contrast, audio provides temporal cues aligned with video motion, making it a promising condition for temporally controlled video generation. \nHowever, existing audio-to-video (A2V) models struggle with fine-grained synchronization due to indirect conditioning mechanisms or limited temporal modeling capacity.\nWe present Syncphony, which generates 380×640 resolution, 24fps videos synchronized with diverse audio inputs. Our approach builds upon a pre-trained video backbone and incorporates two key components to improve synchronization: \n(1) Motion-aware Loss, which emphasizes learning at high-motion regions; \n(2) Audio Sync Guidance, which guides the full model using a visually aligned off-sync model without audio layers to better exploit audio cues at inference while maintaining visual quality.\nTo evaluate synchronization, we propose CycleSync, a video-to-audio-based metric that measures the amount of motion cues in the generated video to reconstruct the original audio. Experiments on AVSync15 and The Greatest Hits datasets demonstrate that Syncphony outperforms existing methods in both synchronization accuracy and visual quality.", "tldr": "We propose improved audio-aligned video generation by leveraging a pretrained video generation model, while preserving its original performance.", "keywords": ["Audio-to-Video Generation", "Multimodal Synthesis", "Temporal Synchronization", "Diffusion Transformer", "Video Generation", "Audio-Conditioned Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/792fd10602732111a1a8a1105ada669bbb7245c3.pdf", "supplementary_material": "/attachment/c567cd1d3273111f54b832167575f6a1e472f1e3.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Syncphony, an audio-to-video generation model that focuses on improving synchronization between audio and motion.\nTo achieve this, the authors introduce a Motion-Aware Loss for capturing movement intensity and an Audio Sync Guidance (ASG) mechanism for enforcing synchronization between audio and visual dynamics.\nA new evaluation metric, CycleSync, is also proposed to better align with human perceptual judgments of synchronization.\nExperimental results show that Syncphony performs better than most existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe introduction of the CycleSync metric sounds reasonable and shows higher alignment with human perceptual evaluations than prior synchronization metrics.\n2.\tThe model achieves consistently better performance than most baselines across several benchmarks."}, "weaknesses": {"value": "1.\tGenerating a 5-second video takes nearly 3 minutes, which significantly limits practical usability.\n2.\tThe main model builds heavily on Pyramid Flow, with only moderate extensions (audio conditioning and synchronization guidance). As such, the contributions feel incremental.\n3.\tThe CycleSync metric relies on pretrained V2A models that may introduce background or irrelevant audio content, potentially biasing the evaluation.\n4.\tThe proposed Motion-Aware Loss does not explicitly capture semantic motion as authors mentions. It merely measures pixel or latent differences between consecutive frames, which may not correspond to meaningful sound-related motion."}, "questions": {"value": "In the demo, only 2-second video samples are provided, even if it can generate 5 seconds video. Why is that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "udu8DEyPmm", "forum": "sG8dGZMaub", "replyto": "sG8dGZMaub", "signatures": ["ICLR.cc/2026/Conference/Submission15982/Reviewer_2TxV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15982/Reviewer_2TxV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876185271, "cdate": 1761876185271, "tmdate": 1762926190611, "mdate": 1762926190611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "We thank the reviewers"}, "comment": {"value": "We thank the reviewers for their thoughtful feedback. Below, we address the common concerns raised across the reviews and clarify key points of our method.\n\n## Design Intent and Backbone Flexibility\nReviewers DdqZ and 2TxV raised the concern that Syncphony heavily relies on the Pyramid Flow backbone with only moderate architectural extensions. \n\nWe would like to clarify (1) our design intent and (2) that the proposed methods are not tied to the Pyramid Flow backbone.\n\n### (1) Design intent\nAs we kindly clarify in lines L017, L053, and L302, our goal is not to propose a new video-generation backbone, but to introduce mechanisms for audio-visual synchronization on top of existing pretrained video models.\n\nImportantly, leveraging a pretrained backbone is not a lightweight modification. Training a video diffusion transformer from scratch is extremely resource-intensive, and naively fine-tuning the entire backbone degrades the pretrained model’s spatial and temporal generation quality. Therefore, the challenge is to introduce audio-motion synchronization without compromising the strong visual fidelity and temporal coherence already learned by the backbone. Syncphony is designed to address this challenge.\n\n### (2) The proposed methods are not tied to the Pyramid Flow backbone\nIn addition, we further emphasize that the proposed Motion-Aware Loss and Audio Sync Guidance, designed for audio-visual synchronization on pretrained backbones, are not tied to Pyramid Flow. We believe there is no architectural constraint that would prevent these mechanisms from being applied to other pretrained video models.\n\nMotion-Aware Loss computes motion magnitude from ground-truth consecutive frames and uses it to reweight the training objective, enabling the model to focus more on motion-relevant regions. This formulation is not tied to Pyramid Flow’s autoregressive structure; it could be applied equally to any diffusion/flow based video models, by weighting each frame’s prediction error using its corresponding ground-truth motion magnitude.\n\nSimilarly, Audio Sync Guidance is designed for video models where visual generation and audio-conditioning pathways are functionally separated. In such architectures, ASG offers a clean and stable way to modulate the strength of audio cross-attention, and thus may extend beyond Pyramid Flow.\nIn addition, as described in Appendix B.1, conventional classifier-free guidance (CFG) is trained by randomly dropping conditioning inputs as zero. However, we observe that directly applying this random-drop strategy to audio conditioning leads to degraded performance.\nAudio carries a meaningful semantic interpretation even when its value is dropped; silence is itself an informative condition. To address this issue, instead of dropping audio conditions, we propose an Audio Sync Guidance. \n\nIn summary, our contributions do not depend on Pyramid Flow, and the proposed mechanisms may be applicable to other pretrained video models as well.\n\n## Clarifying the Robustness of Motion-Aware Loss\nReviewers 4rJg and 2TxV raised concerns that Motion-Aware Loss assumes static scenes and that simple latent frame differences may not capture meaningful sound-related motion.\n\nWe would like to clarify that Motion-Aware Loss remains effective even in dynamic environments where motion is not semantically aligned with the audio (e.g., background motion, camera movement), and still guides the model to learn meaningful audio-motion relationships.\n\nAs described in line 239, Motion-Aware Loss computes the magnitude of ground-truth motion between consecutive frames and uses it to reweight the loss. This mechanism strengthens the learning signal specifically in motion regions, encouraging the model to focus on dynamic areas and generate them more accurately.\n\nThrough this process, the model learns the cause of each motion. \n\n- If the motion is caused by audio, the model learns to associate it with the audio;\n- if the motion is unrelated to audio (e.g., background motion, camera movement), the model learns to treat it as sound-unrelated motion.\n\nIn this way, the model generates synchronized sound-related motion appropriately while still generating sound-unrelated motion.\n\nThe training dataset we used includes various camera motions such as zoom-in/zoom-out. Our generated results preserve these sound-unrelated motions while synchronizing only the sound-related parts. For example, in the “dog barking” sample in the Comparison section of our demo, the dog barks in sync with the audio while the camera continues to zoom, illustrating the model’s ability to distinguish sound-driven from non-sound-driven motion.\n\nIn summary, Motion-Aware Loss effectively guide the model to learn meaningful audio-motion alignment even in dynamic, audio-unrelated motion environments."}}, "id": "KNmIv3gwup", "forum": "sG8dGZMaub", "replyto": "sG8dGZMaub", "signatures": ["ICLR.cc/2026/Conference/Submission15982/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15982/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15982/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763745541038, "cdate": 1763745541038, "tmdate": 1763745541038, "mdate": 1763745541038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Syncphony, a diffusion-transformer-based framework for audio-to-video (A2V) generation that achieves fine-grained synchronization between sound and motion. The method introduces two key innovations: 1) Motion-aware Loss — reweights reconstruction loss to emphasize high-motion regions, improving temporal alignment. 2) Audio Sync Guidance (ASG) — uses an auxiliary “off-sync” model (without audio layers) to guide the full model during sampling toward stronger audio-motion coupling. To evaluate synchronization, the authors further introduce CycleSync, a new video-to-audio-based metric that measures whether the generated motion contains sufficient cues to reconstruct the original audio. Experiments on AVSync15 and TheGreatestHits datasets show improved synchronization and comparable or better visual quality relative to baselines such as AVSyncD and TempoTokens."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes Motion-aware Loss and Audio Sync Guidance to improve the audio-visual synchronization of A2V generation, which are conceptually simple and empirically effective.\n2. The proposed metric CycleSync offers a meaningful step forward over prior metrics (AV-Align, AlignSync, RelSync) by enabling evaluation at 24 fps and better correlating with human perception.\n3. The paper also proposes a principled strategy to adapt a pretrained I2V model for the A2V task by selecting the most relevant layers to inject the audio cross-attention layer.\n4. Syncphony consistently outperforms baselines on synchronization (CycleSync) and achieves competitive or superior FID/FVD, indicating that temporal precision does not come at the expense of visual quality."}, "weaknesses": {"value": "1. While the paper’s ideas are sound, the architectural novelty is moderate. Syncphony heavily relies on a pretrained Pyramid Flow backbone, and the main innovations are at the loss and sampling levels rather than core model design.\n2. The writing of the paper can be improved. For example, in the introduction, more details/motivations about the proposed methods could be included instead of the background information. \n3. Limited baselines: the proposed method is only compared with AVSyncD and Pyramid Flow. Is it possible to include more baselines, such as those listed in AVSyncD?"}, "questions": {"value": "1. Fig 5 and 6: it will be helpful to include the ground-truth video as a reference. \n2. How does ASG compare with the vanilla classifier-free guidance? For example, use the features with and without audio input as guidance. \n3. The paper only finetunes the last 16 blocks (8–23) of the Pyramid Flow backbone. Are there any experimental results supporting the benefits of this choice, e.g., compared to finetuning the full model?\nOthers: see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fn47zRvXcf", "forum": "sG8dGZMaub", "replyto": "sG8dGZMaub", "signatures": ["ICLR.cc/2026/Conference/Submission15982/Reviewer_DdqZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15982/Reviewer_DdqZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916342409, "cdate": 1761916342409, "tmdate": 1762926190037, "mdate": 1762926190037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Syncphony demonstrates meaningful progress in the audio-to-video generation domain. Its main contributions include: (1) a motion-aware loss that re-weights the training objective based on optical flow to focus the model’s learning on motion-intensive regions; (2) a training-free guidance technique that enhances the injection of audio information during inference without additional training; and (3) a more intuitive and reasonable evaluation metric for measuring audio-visual synchronization through reconstructed audio from generated videos. Experiments on two public datasets and qualitative case studies further validate the method’s effectiveness in improving both synchronization and visual quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed training-free guidance is novel and effectively points out the core challenge of achieving precise spatiotemporal alignment in audio-to-video generation.\n\n- The proposed evaluation metric is intuitively reasonable and addresses the limitation of conventional metrics like FVD, which fail to effectively measure spatiotemporal alignment.\n\n- The use of RoPE for spatiotemporal positional encoding enhances temporal consistency and spatial coherence in video generation, contributing to smoother and more structured motion representation.\n\n- The demo results demonstrate good temporal alignment consistency."}, "weaknesses": {"value": "Although the work is technically sound, there are several issues that I feel must be discussed.\n\n- Regarding the motion-aware loss, it relies on a strong assumption that the visual content remains static and confined to a single scene. This assumption holds almost perfectly under the authors’ setting where short video clips of around two seconds. However, in more realistic video generation scenarios, background changes, camera motion, and scene transitions can occur without producing strong audio cues, but may catastrophically distort optical flow estimation and thus limit the scalability of the proposed approach.\n\n- The proposed guidance technique randomly drops audio cross-attention layers during inference; however, this inference structure (unlike CFG or Autoguidance) is never encountered during training, making the resulting “unconditional” outputs less predictable. Moreover, although the authors provide some visual demonstrations, the underlying motivation may primarily apply to relatively smaller models where audio conditioning tends to be weaker. In larger-scale video generation frameworks such as Wan or Hunyuan-Video, audio conditions may not be as easily ignored, and models could exhibit different skip-layer behaviors. Rather than validating only on additional datasets, I would prefer the authors to verify this idea across multiple video generation baselines to strengthen the generality of their claims.\n\n- Although the proposed new metric is intuitively reasonable, current video-to-audio (V2A) models also suffer from (or are still addressing) spatiotemporal alignment issues, which means they may not serve as a fully reliable ground-truth proxy. Moreover, the approach fundamentally increases evaluation time, potentially limiting scalability to larger experiments."}, "questions": {"value": "- Could the authors elaborate on how they plan to address the challenges posed by more realistic (or longer) audio-to-video scenarios, where optical flow estimation can be severely affected by background motion, camera movement, or scene transitions beyond the main subject?\n\n- Could the authors show how the skip-layer behavior manifests in other video generation models and whether similar phenomena can be consistently observed? In addition, if temporal alignment is indeed a crucial aspect of the task, why not consider using energy-based audio features as a more direct form of control or guidance?\n\n- Given that modern multimodal large language models (e.g., Gemini 2.5) already demonstrate strong capabilities in understanding audio-visual information, and that existing video-to-audio alignment metrics (such as DeSync, which can be applied similarity in a2v field) provide reliable proxies for spatiotemporal correspondence, could the authors clarify or compare what specific advantages their proposed metric offers over these established approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dtxrS4k8YS", "forum": "sG8dGZMaub", "replyto": "sG8dGZMaub", "signatures": ["ICLR.cc/2026/Conference/Submission15982/Reviewer_4rJg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15982/Reviewer_4rJg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960454109, "cdate": 1761960454109, "tmdate": 1762926189685, "mdate": 1762926189685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Syncphony, an audio-to-video (A2V) model capable of generating 380×640 resolution, 24fps videos synchronized with diverse audio inputs. Built upon a pre-trained video backbone, it emphasizes on audio-visual synchronization through two main contributions:\n- Motion-aware Loss, a loss weighting mechanism emphasizing on the high-motion regions of the video.\n- Audio Sync Guidance, a classifier-free guidance design focusing solely on audio conditioning to emphasize better synchronization.\n\nMoreover the authors introduce an auxiliary contribution for the model evaluation:\n- CycleSync, an audio similarity metrics between the detected peaks of the ground truth and the A2V -> (pretrained) V2A generated audio.\n\nThe A2V model is finetuned from a pretrained I2V model, by adding cross attention layers in the latter transformer blocks of the pretrained model. The backbone is a pretrained PyramidFlow Video model, trained on videos up to 5 seconds long at 24 fps and 380 × 640 resolution. Audio is sampled at 16kHz and encoded through DenseAV for conditioning. Text is encoded through CLIP. Temporally-aware RoPE frequencies are employed to aligned the modalities in the aforementioned cross attention layers.\n\nEvaluations are conducted on the AVSync15 dataset using both objective (FID, FVD, Image-Audio similarity, Image-Text similarity, CycleSync) and subjective metrics (IQ, FC, Sync). They demonstrate that the proposed method outperforms the AVSyncD on all axes.\n\nOn the Greatest Hits dataset, the proposed method outperforms on the CycleSync and FVD metrics, showing its emphasis on both video generation quality and audio visual synchronization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper quality is enforced by thorough ablations and experiments, presented both in the main sections and appendixes, notably:\n- demonstration of the greater sensitivity of the CycleSync metrics compared with previously proposed ones, running correlation with human study and experimenting with controlled settings such as temporal shifts.\n- ablations on the Motion-aware Loss and Audio Sync Guidance contributions\n- pretrained model behavior study to understand where to inject the finetuning layers\n\nAlthough this paper presents an Audio to Video model, its contributions should translate into better the Video to Audio model designs."}, "weaknesses": {"value": "The Figure 1 is not clear enough to me. It is not clear what the frozen and trainable layers refer to (are those transformer blocks?). I would suggest adding in the captions that the audio features are injected in the latter blocks (the expectation is usually to add cross attention to each block so the presentation is quite counter intuitive without a corresponding explanation)."}, "questions": {"value": "Formatting:\n- Add spacing before opening parentheses (around the line 347).\n- In the table 3, I suggest better highlighting which model is the final version (maybe in the top row and by adding a row separator).\n- As one of the paper's contributions is a classifier-free-guidance design, I would suggest adding some ablations of the choice of the cfg parameters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "18Ef5UKChK", "forum": "sG8dGZMaub", "replyto": "sG8dGZMaub", "signatures": ["ICLR.cc/2026/Conference/Submission15982/Reviewer_nWr3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15982/Reviewer_nWr3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062198610, "cdate": 1762062198610, "tmdate": 1762926188941, "mdate": 1762926188941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}