{"id": "n5bPL58uMC", "number": 21167, "cdate": 1758314451029, "mdate": 1759896938620, "content": {"title": "Trapped by simplicity: When Transformers fail to learn from noisy features", "abstract": "Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the empirically optimal function for noise-robust learning has lower sensitivity than the target function. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.", "tldr": "", "keywords": ["boolean analysis", "simplicity bias", "transformer", "feature noise"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b28fffe325eb330beea4c8d82f2bca7986c3e950.pdf", "supplementary_material": "/attachment/ac6085c7e9cf07d0e7241ed2b3648576c4cf929c.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the capability of transformers on noise-robust learning. Transformers are compared to LSTMs for this task. Main contributions of this paper include the following.\n1. For sparse parity and majority functions at high rates of feature noise, transformers perform significantly better than LSTMs, and the latter fails even for low levels of feature noise.\n2. They observed that transformers do not have satisfactory performance for the task of random k-juntas.\n3. They proposed an explanation related to sensitivity of optimal solution and of target function.\n4. Solution: implementing penalty for high-sensitivity solutions"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem formulation is novel by adding a perspective on robustness. Noise-robust learning is valuable but yet relatively under explored.\n\n2. The paper has sound theoretical analysis and the theoretical results provide significant insights. It deploys various mathematical tools efficiently, including Boolean analysis, information theory, and learning theory.\n\n3. Experiments, although relatively small-scaled, has a clear target on the conjecture and provides valuable support\n\n4. The paper is in general well-written and the logic flows smoothly"}, "weaknesses": {"value": "My main concern is on the applicability and scope of this study. The investigated problems (parity and junta) are binary-input problems with rigid mathematical structures. This fact provides simplicity for analysis, but at the same time they are restricted because real-world data and noises are much more complicated."}, "questions": {"value": "My question is highly relevant with what I wrote in the weaknesses section: Does the theoretical insights obtained from your study have any implications on the robustness of transformers for more complicated tasks"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y57DChkb87", "forum": "n5bPL58uMC", "replyto": "n5bPL58uMC", "signatures": ["ICLR.cc/2026/Conference/Submission21167/Reviewer_NNLa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21167/Reviewer_NNLa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707877174, "cdate": 1761707877174, "tmdate": 1762941533430, "mdate": 1762941533430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies noise-robust learning: whether models trained only on inputs corrupted by feature noise can still learn the underlying noiseless target function. Using Boolean tasks, the authors:\n\n* Formalize the Bayes-optimal predictor under iid bit-flip noise as $f_N^*(x)=\\mathrm{sign}(T_{1-2p} f(x))$, where $T_\\rho$ is the standard noise operator. This links the problem to noisy-channel coding and bounds performance via conditional entropy. \n* Show empirically that transformers (encoder-only SANs) often succeed at noise-robust learning for sparse parity and odd-length sparse majority functions, while LSTMs generally fail even at modest noise rates. (Figure 1)\n* Demonstrate that transformers typically fail on random k‑juntas, even when they achieve near‑optimal accuracy on the noisy validation distribution; failure correlates with the gap between the sensitivity of $f$ and that of the optimal noisy predictor $f_N^*$. (Figure 2)\n* Argue the failure mechanism combines a simplicity bias in transformers toward low‑sensitivity functions and the empirical observation that $f_N^*$ tends to have lower sensitivity than $f$. (Conjecture 1)\n* Construct a controlled \"trap\" function where $f$ and $f\\_N^\\*$ have similar optimal noisy accuracy but very different sensitivities; transformers converge to the simpler $f\\_N^\\*$. Adding a differentiable sensitivity penalty helps \"escape\" in a narrow range of penalty weights, but when $f\\_N^\\*$ is much better than $f\\_N^\\*$, the penalty does not help."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Casting training-on-noisy-features as a noisy-channel problem, with $f_N^*$ characterized by the noise operator and performance tied to $H(Y|Z)$, gives a precise target for comparison. \n* Extensive hyperparameter sweeps and repeated trials (300 per condition) improve the reliability of the conclusions. \n* The use of total influence $I[f]$ to quantify simplicity connects to prior theory and cleanly explains why models can perform well on noisy validation yet fail on noiseless evaluation. \n* The “trap function” isolates simplicity bias from other confounders: with nearly equal noisy optimal errors for $f$ and $f_N^*$, any preference for $f_N^*$ reveals the inductive bias. Figure 8 show SANs drifting toward the trap unless regularized. \n* The discussion connects Boolean results to real LLM settings where training data are noisy/stochastic but downstream tasks are noise‑sensitive (e.g., arithmetic), cautioning that minimizing conditional entropy on noisy features might impede learning fine‑grained rules."}, "weaknesses": {"value": "* Narrow noise model and data distribution. All inputs are uniformly random bitstrings with iid symmetric bit‑flip noise and memoryless corruption. Real text has structured distributions and correlated, non‑binary errors (insertions, deletions, paraphrases). The paper acknowledges this but leaves generality uncertain. \n* Task scope. Results hinge on Boolean functions; while parity/majority and k‑juntas are classic, evidence that the same mechanisms dominate in natural language or code remains indirect. No experiments on tokenized sequences, algorithmic datasets, or synthetic “text‑like” corruptions are provided. \n* The sensitivity penalty that helps in the trap requires a narrow range of $\\lambda$ and does not generalize when $f_N^*$ dominates $f$. This makes it difficult to apply similar ideas in practice\n* Conjecture 1—$I[f_N^*] \\le I[f]$—is well‑motivated and tested on small n and random samples, but remains unproven. Many conclusions rely on it conceptually; a counterexample would undercut the narrative."}, "questions": {"value": "1. How sensitive are the main findings to non‑iid or structured noise (e.g., burst errors, deletions/insertions, or noise correlated with specific positions)? Could the channel view extend to these cases and does the simplicity gap persist? \n2. What happens when inputs are non‑uniform (e.g., biased bit marginals or low‑entropy substructures), closer to natural text statistics? Does the prevalence of self‑predicting functions change under such distributions? \n3. Can the authors provide explicit parameter counts to the models? It is a bit troublesome to compute them from the dimensions provided in the appendix. Does model size/capacity have any effect on the biases observed?\n4. Is there a practical diagnostic to detect, during training, when a model is converging to $f_N^*$ rather than $f$? The training‑trace figures suggest this might be possible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1vLlANqoGJ", "forum": "n5bPL58uMC", "replyto": "n5bPL58uMC", "signatures": ["ICLR.cc/2026/Conference/Submission21167/Reviewer_RgtW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21167/Reviewer_RgtW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966914508, "cdate": 1761966914508, "tmdate": 1762941530385, "mdate": 1762941530385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work analyzes the ability of transformers and LSTMs in learning boolean functions, in the feature noise setting (with no label noise). The motivation for this lies in the prevalence of noise in LLM training data, and boolean functions serve as a sandbox for understanding this phenomenon. Through extensive simulations, it is shown that for the majority function and certain sparse parity functions, transformers succeed with learning the optimal function while LSTMs fail. On the other hand, for most other random boolean functions, both of these architectures fail to learn in the feature noise setting, although for the case of transformers, this can be mitigated with a modified regularized loss function."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation for the theoretical setup is clear (e.g. inspiration from modern day LLM training).\n- Extensive experiments are provided for demonstrating when transformers can and cannot learn boolean functions (e.g. which $k$ in a $k$-sparse parity in which learning with feature noise is possible, as well as majority functions, and other $k$-juntas).\n- The analysis of the sensitivity of the optimal predictor under feature noise versus the teacher is an interesting perspective, and a conjecture regarding this is proposed, backed by simulations."}, "weaknesses": {"value": "- The analysis for parity and majority seem quite straightforward, and it would be interesting if there was more theoretical analysis on progress towards the conjecture.\n- For the LSTM model, it would be useful to have some theoretical analysis of this setting for learning boolean functions too.\n- Perhaps an example of a realistic setting of feature noise in training transformers would be useful, and I believe this would better motivate this paper."}, "questions": {"value": "- What are some of the main bottlenecks preventing the authors from making the conjecture rigorous?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qSsQ1niDmn", "forum": "n5bPL58uMC", "replyto": "n5bPL58uMC", "signatures": ["ICLR.cc/2026/Conference/Submission21167/Reviewer_TMxh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21167/Reviewer_TMxh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967430526, "cdate": 1761967430526, "tmdate": 1762941528805, "mdate": 1762941528805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}