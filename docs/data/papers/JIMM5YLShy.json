{"id": "JIMM5YLShy", "number": 7919, "cdate": 1758043332897, "mdate": 1759897822415, "content": {"title": "On the Benefits of Weight Normalization for Overparameterized Matrix Sensing", "abstract": "While normalization techniques are widely used in deep learning, their theoretical understanding remains relatively limited. In this work, we establish the benefits of (generalized) weight normalization (WN) applied to the overparameterized matrix sensing problem. We prove that WN with Riemannian optimization achieves linear convergence, yielding an $\\textit{exponential}$ speedup over standard methods that do not use WN. Our analysis further demonstrates that both iteration and sample complexity improve polynomially as the level of overparameterization increases. To the best of our knowledge, this work provides the first characterization of how WN leverages overparameterization for faster convergence in matrix sensing.", "tldr": "", "keywords": ["Weight normalization", "Overparameterization", "Matrix sensing", "Non-convex optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bdb5d87e681e678b5de0277d20591832e49e629a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the advantages of a generalized weight normalization (WN) technique in the context of overparameterized matrix sensing, where the goal is to recover a low-rank PSD matrix from linear measurements. The authors prove that WN, combined with Riemannian gradient descent (RGD), achieves linear convergence with random initialization, offering an exponential speedup over standard gradient descent (GD) methods without WN. They demonstrate that increased over-parameterization reduces both iteration and sample complexity polynomially. The analysis reveals a two-phase convergence behavior (initial saddle-escape phase followed by linear convergence) and is supported by experiments on synthetic data and image reconstruction tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength lies in its novel theoretical insights, providing the first characterization of how WN leverages overparameterization for faster convergence in matrix sensing, with a rigorous proof of linear convergence that exponentially outperforms sublinear rates in non-WN methods. It quantifies the benefits clearly, showing polynomial improvements in iteration and sample complexity as overparameterization increases, which contrasts positively with prior work where overparameterization hinders performance. Empirically, the experiments robustly validate the theory, including comparisons under varying conditions like condition numbers, overparameterization levels, and noise, with real-world image reconstruction adding practical value. The manuscript is well-organized and clear, featuring helpful tables, figures, and a reproducibility statement with full proofs and setups in appendices."}, "weaknesses": {"value": "1. While the analysis is focused and insightful, its scope is limited to symmetric PSD matrix sensing. It would be helpful if the authors could briefly explain why it is reasonable to consider only the symmetric PSD case.\n2. Experimentally, while solid, the studies are somewhat constrained in scale (e.g., small matrix dimensions in synthetic tests).\n3. The RIP condition required for the main theorem scales as $\\delta = O((r - r_A)^6 / (\\kappa^2 m^3 r^4 r_A))$, which appears to be rather stringent. A brief analysis or discussion of this condition's implications would be valuable."}, "questions": {"value": "Overall, I don't see major issues that undermine the core contributions—the theory is sound, and the claims are well-supported. \nThat said, here are a few constructive suggestions:\n1. I am curious about how the proposed algorithm would perform in more challenging matrix sensing settings, such as those with specially designed structures or nontrivial optimization landscapes (e.g., as discussed in arXiv:2110.10279).\n2. A short note on potential applications (e.g., in signal processing) could emphasize the work's relevance to ICLR's audience."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5N7K4wAqSD", "forum": "JIMM5YLShy", "replyto": "JIMM5YLShy", "signatures": ["ICLR.cc/2026/Conference/Submission7919/Reviewer_zbqV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7919/Reviewer_zbqV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729226620, "cdate": 1761729226620, "tmdate": 1762919943476, "mdate": 1762919943476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical characterization of how Weight Normalization (WN) accelerates convergence in overparameterized matrix sensing. The authors prove that Riemannian gradient descent (RGD) with WN achieves linear convergence and improved sample complexity, while standard gradient descent (GD) may exhibit exponential slowdown. Theoretical findings are supported by synthetic experiments that align well with the analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Provides the first clear theoretical explanation of how WN accelerates convergence under overparameterization.\n\n2.Solid and rigorous analysis using Riemannian optimization tools.\n\n3.Well-aligned experiments that validate the theory.\n\n4.Overall writing and presentation are clean and accessible."}, "weaknesses": {"value": "1.The paper identifies a two-stage convergence pattern with a transition at $r_A-\\frac{1}{2}$ yet this boundary is not theoretically justified or intuitively explained.\n\n2.The “Full-rank case (r = m)” in Sec. 5.3 is conceptually an extension of Sec. 5.2 (“ON THE BENEFIT OF OVERPARAMETERIZATION”) and could be merged there for better logical flow.\n\n3.The title seems somewhat broader than the actual technical scope, which is limited to PSD matrix sensing."}, "questions": {"value": "1.See Weakness. 1.\n\n2.Since $A$ is PSD, one might consider reformulating (1) as \n\n$\\min_{X, \\Theta} f(X, \\Theta) = \\frac{1}{4}\\|M(X\\Theta X^{\\top})-y\\|^2$\n\nwith $\\Theta$  being diagonal and nonnegative. Would this restriction simplify the analysis or improve interpretability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pLBrxlVPA6", "forum": "JIMM5YLShy", "replyto": "JIMM5YLShy", "signatures": ["ICLR.cc/2026/Conference/Submission7919/Reviewer_BeU2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7919/Reviewer_BeU2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816624684, "cdate": 1761816624684, "tmdate": 1762919942997, "mdate": 1762919942997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a matrix factorization similar to the weight normalization that can be helpful for the matrix sensing problem. The author proves that this matrix factorization with Riemannian optimization can achieve a linear convergence rate, which is an exponential improvement over the previous lower bound for symmetric matrix sensing."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is well-organized and easy to follow. The contributions of this paper is proposed in a clear way, and the main technical idea is also clear.  The author also provides solid theoretical proof and some discussions about the main result. \n\n2. The authors also further study the initial increment learning phase in the optimization process, which is good for understanding the optimization dynamics of WN with Riemannian optimization. \n\n3. The empirical part is comprehensive. Beyond the simulated optimization problems, the authors also provide experiments on image reconstruction problem. The performance of WN with Riemannian optimization is better compared to GD, which also matches the theoretical discovery of this paper."}, "weaknesses": {"value": "1. My main concern is that after relaxing the PSD constraint, the problem no longer corresponds to symmetric matrix sensing. Specifically, when the constraint $\\Theta \\in S^r_{+}$ is relaxed to $\\Theta \\in S^r$, the term $X^T \\Theta X$ may no longer be written as $YY^T$. Thus, the proposed method is not a true solution to symmetric matrix sensing but rather an approach that accelerates matrix sensing in general. This weakens the paper’s contribution, as several existing methods [1,2] also focus on improving the efficiency of matrix sensing.\n\n2. The paper introduces Riemannian optimization equations without providing sufficient background or basic explanations. It would be helpful if the authors could include a high-level introduction and some intuitive discussion of Riemannian optimization to improve readability.\n\n[1]. Xu et al. 2023. The power of preconditioning in overparameterized low-rank matrix sensing\n\n[2]. Xiong et al. 2024. How over-parameterization slows down gradient descent in matrix sensing: The curses of symmetry and initialization"}, "questions": {"value": "Lemma 4.1 states that a point $(X, \\Theta)$ is a saddle point if certain conditions are satisfied. However, this does not necessarily imply that all saddle points satisfy these conditions. Could the authors clarify how this lemma connects to the saddle-to-saddle dynamics and the subsequent incremental learning process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DeB3ZEbMnE", "forum": "JIMM5YLShy", "replyto": "JIMM5YLShy", "signatures": ["ICLR.cc/2026/Conference/Submission7919/Reviewer_5U1W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7919/Reviewer_5U1W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883887101, "cdate": 1761883887101, "tmdate": 1762919942650, "mdate": 1762919942650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the problem of overparameterized matrix sensing and present an analysis of the benefits of applying weight normalization in this context. They reformulate the classical matrix sensing problem by decoupling the magnitude and direction components of the matrix, and they demonstrate that using Riemannian gradient descent with weight normalization improves the convergence rate, particularly when the level of overparameterization is large. Experiments on both synthetic and real-world datasets are provided to support the analysis."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written, clearly structured, and the ideas are well explained. The theoretical motivation and experimental design are both sound."}, "weaknesses": {"value": "I have a major concern regarding the positioning of the paper within the existing literature. While the paper emphasizes the proposed weight normalization approach, its main benefit—improved convergence rate—falls within a broader line of research on optimization techniques for matrix sensing. In particular, related approaches such as preconditioned gradient descent and their follow-up works have already been proposed in this domain. Although some of the related (e.g. preconditioned gradient descent) papers are cited, they are not discussed thoroughly. These works should be more deeply reviewed in the related work section, and, ideally, comparisons should be included in the experimental evaluation to better situate the contribution and highlight the advantages of the proposed approach over existing methods."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tXiIxtP5dG", "forum": "JIMM5YLShy", "replyto": "JIMM5YLShy", "signatures": ["ICLR.cc/2026/Conference/Submission7919/Reviewer_mDtz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7919/Reviewer_mDtz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903112220, "cdate": 1761903112220, "tmdate": 1762919941845, "mdate": 1762919941845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}