{"id": "Lexn2TAw59", "number": 18495, "cdate": 1758288323423, "mdate": 1759897099704, "content": {"title": "Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity", "abstract": "Two pressing topics in the theory of deep learning are the interpretation of feature learning mechanisms and the determination of implicit bias of networks in the rich regime. Current theories of rich feature learning effects revolve around networks with one or two trainable layers or deep linear networks. Furthermore, even under such limiting settings, predictions often appear in the form of high-dimensional non-linear equations, which require computationally intensive numerical solutions. Given the many details that go into defining a deep learning problem, this analytical complexity is a significant and often unavoidable challenge. Here, we propose a powerful heuristic route for predicting the data and width scales at which various patterns of feature learning emerge. This form of scale analysis is considerably simpler than such exact theories and reproduces the scaling exponents of various known results. In addition, we make novel predictions on complex toy architectures, such as three-layer non-linear networks, thus extending the scope of first-principle theories of deep learning.", "tldr": "We introduce a heuristic framework for scaling analysis that adopts variational methods from statistical field theory. It yields predictions of feature learning emergence in deep NNs, going beyond the state-of-the-art to hitherto intractable regimes.", "keywords": ["Deep learning theory", "feature larning", "sampel complexity", "scaling laws"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa372eea15aabd30c561361b370bcf4beeb38662.pdf", "supplementary_material": "/attachment/2b45a13471de1718bc149bc8e9f21b9a001bd374.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method to estimate sample complexity, i.e., the amount of data needed in order to learn a target function (measured in terms of alignment). The authors propose a bound in function space depending on how well a randomly drawn NN would align with the target. As this bound is uncomputable, they derive a variational approach based on a mean-field decoupling. They hypothesize two feature learning mechanisms that go beyond the NNGP baseline and derive certain signatures of feature learning, like eigenvalue spikes in the conjugate kernel and a description of feature learning in a three-layer model. This is verified on one empirical target function."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clear quantitative predictions for general architectures which go way beyond the usual toy models or qualitative results.\n\n- Very general framework that can be applied independently of architecture, data distribution, etc.\n\n- New analysis of a three-layer non-linear network."}, "weaknesses": {"value": "- Hard to follow at some places, especially the three-layer NN section. Lines 282-301 are very dense and should be made clearer in the camera-ready version.\n\n- The theory needs much more empirical validation. I.e., what happens with other target functions, other input distributions, etc.? Are the predictions still correct?\n\n- How do we know the proposed distributions actually minimize the variational energy? The paper would greatly benefit from a comparison of an SGLD-trained network and how close the neuron distribution of this network is to the ones proposed as the minimizers of the variational energy.\n\n- Some typos: line 70 misses a \".\" after the eq., lines 70 and 81 miss a \",\" after the eq., line 84 $x_{\\nu}$ is not the dataset but a point, line 219 has a \",\" at the end instead of a \".\", $\\Theta$ is not defined.\n\n- The central idea of the paper is related to [1] and especially to [2] where analytical bounds on the prior are derived in a simplified setting, but this work is not cited.\n\n[1]: https://arxiv.org/abs/2006.15191\n[2]: https://arxiv.org/abs/2505.24060"}, "questions": {"value": "- How does a bound on the prior correctly imply sample complexity for a feature learning network? Even after reading the appendix, I do not fully understand how this is possible.\n\n- How does the structure of the target function influence the sample complexity in this framework? I.e., if the target function only depends on $k\\ll d$ input coordinates, can it be shown that the sample complexity of the NN in that framework scales with $k$ not $d$ more generally?\n\n- How is $P_{*}$ depending on the input distribution of the data?\n\n- How do we know that the proposed minimizers of the variational densities NNGP, GFL and specialization are the true minimizers?\n\n- How does this connect to PAC-Bayes bounds? (https://arxiv.org/pdf/2110.11216)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HQzJGM9P3y", "forum": "Lexn2TAw59", "replyto": "Lexn2TAw59", "signatures": ["ICLR.cc/2026/Conference/Submission18495/Reviewer_d7wZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18495/Reviewer_d7wZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761527585572, "cdate": 1761527585572, "tmdate": 1762928191145, "mdate": 1762928191145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a scaling-based recipe for predicting when and where specific feature learning attributes (covariance spikes, neuron specialization) appear in deep nets. It uses a Bayesian description of neural networks to analyze the sample-size threshold needed to have $O(1)$ alignment  between a function (a kernel eigenmode) and the network. Heuristic propagation rules connect layers (e.g., specialization induces a downstream spectral spike; spikes amplify higher-order features), letting predictions compose across depth. Worked cases for data scaling exponents are given for 2/3-HL FCNs, as well as CNNs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "There appears to be quality work done on the mathematics behind this paper’s analysis, as well as an original approach to mech interp. It’s clear not enough similar work is done in that community, so the authors should be applauded for their instinct to pursue this direction. Another major strength is connecting the results obtained back to previous findings, this time under a new light. The main math-heavy sections are well presented. This work appears to be quite significant to those in the mech interp field willing to read through the paper to understand its ideas and implications."}, "weaknesses": {"value": "This paper would greatly benefit from plots showing how the theory matches results, not just empirical results which are quite difficult to connect back to specifically this work and not some other broad notions of the field. Another weakness is in the downplay of grokking: since this theory is entirely about near-convergence networks, it seems like there should be a much greater focus on things like weight decay or grokking; outside of the intro, I don’t see those mentioned again. Finally, some things need to be defined instead of assumed (take Figure 1(a)’s y-axis, what is H_pi?)"}, "questions": {"value": "Why is the error function being used for the activation function? If any can be used, why not just the standard ReLU?\nCan you clarify the point of the claims in 4.4?\nCan plots of the ConvNet case be provided? In other words, are the details you’re trying to abstract away the same ones that make ConvNets and MLPs appear differently? If so, why perform this analysis? Those differences seem key to understanding ConvNets.\nThis is framed as a new framework that is easier to apply than other approaches, so why isn’t it applied to make new predictions? Show me this goes beyond specific cases like 3 layer MLPs on isotropic data with the (seemingly specific) erf activation function"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EN90UjVHxr", "forum": "Lexn2TAw59", "replyto": "Lexn2TAw59", "signatures": ["ICLR.cc/2026/Conference/Submission18495/Reviewer_9jc9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18495/Reviewer_9jc9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805371292, "cdate": 1761805371292, "tmdate": 1762928190480, "mdate": 1762928190480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a theoretical framework for understanding feature learning through an alignment-based measure that connects learned representations with test mean squared error (MSE). The authors derive inequalities suggesting that when alignment is close to one, the model generalizes well, and they further develop probabilistic bounds on alignment exceeding certain thresholds. The motivation is strong: linking alignment to generalization performance could offer a compact diagnostic tool for feature learning across architectures.\n\nHowever, I found the mathematical exposition—particularly in Sections 2, 3, and 4—very confusing and difficult to follow. The direction of key inequalities, the interpretation of constants, and the definitions of several introduced symbols are unclear. As a result, I cannot confidently assess the soundness of the theoretical claims, even though the overall goal of the paper is promising.\n\nThe paper presents an interesting and well-motivated idea, but the core theoretical exposition is too opaque to evaluate. The logical direction of key inequalities and the meaning of several symbols are unclear, which makes the central claims about alignment as a generalization proxy hard to verify. With clearer derivations, well-defined notation, and a complementary upper bound relating alignment to test MSE, this work could become significantly more compelling."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Strong motivation and relevance: The paper tackles an important and timely question—how to quantify representation quality via alignment—and aims to connect it to generalization performance.\n* Potential for theoretical contribution: If clarified, the proposed framework could bridge geometric and probabilistic views of feature learning, an area of broad interest to both machine learning and neuroscience communities.\n* Clear high-level narrative: The intuition that alignment captures “how well learned features match task structure” is appealing and consistent with empirical trends observed in deep learning."}, "weaknesses": {"value": "* Unclear and inconsistent mathematical exposition: The logic behind the central inequality (around line 98) is confusing. It provides only a lower bound on test MSE in terms of alignment, so high alignment is at best a necessary—but not sufficient—condition for good generalization. The paper does not establish or even discuss whether an upper bound exists, which weakens the claim that alignment close to 1 can reliably indicate low test error.\n* Ambiguity in probabilistic statements: In Section 3, the analysis of “probability of alignment > α” is difficult to interpret because the relevant range of α is not specified and the meaning of O(1)  is unclear—does it denote “around one” or simply “a constant independent of dimension”? This ambiguity propagates through subsequent results.\n* Notation and definition issues: Several key quantities (e.g., H_{p,α}, q, and related expressions around lines 146–152) are either undefined or only referenced indirectly via the appendix. The paper should clearly define each symbol in the main text, especially when these terms appear in central inequalities.\n* Direction of inequalities unclear: In Section 4, it is unclear whether P(A)F(α)) is meant to upper bound or lower bound the probability of alignment exceeding α. The sign and logical direction of several inequalities seem inconsistent, making it hard to reconstruct the intended argument.\n* Difficult to evaluate validity: Because of these ambiguities, it is impossible to judge whether the mathematical derivations are correct or merely stated heuristically. The empirical illustrations in later sections cannot compensate for this lack of clarity in the core theory."}, "questions": {"value": "1. In line 98, how does the lower bound on test MSE justify using alignment as a reliable proxy? Is there also an upper bound, or an argument showing that alignment close to 1 implies low error in practice?\n2. In Section 3, what is the intended range for α, and what does O(1) mean precisely in this context?\n3. In Section 4, should PAF(α) serve as an upper or lower bound on the probability of high alignment? Please clarify the direction and logic of this inequality.\n4. Please define all symbols that appear in main derivations (e.g., H_{p,α}, q, etc.) in the text rather than referring only to the appendix.\n5. Could you include a clear schematic or conceptual figure illustrating how alignment links to generalization, and summarize the assumptions under which the inequalities hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lLxlJd1QhT", "forum": "Lexn2TAw59", "replyto": "Lexn2TAw59", "signatures": ["ICLR.cc/2026/Conference/Submission18495/Reviewer_tHzt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18495/Reviewer_tHzt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961415638, "cdate": 1761961415638, "tmdate": 1762928188980, "mdate": 1762928188980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uses variational energy bound and proposes a set of heuristics to characterize the scaling property of sample complexity ($P^*$ such that $logPr_{p_0}(A_f \\geq \\alpha)$ . The heuristics (which is an approximation of the variational energy) are developed for the feature learning patterns of Gaussin Process, Gaussain Feature Learning, and Feature Specialization. The paper then uses these heuristics to estimate the variational energy of these feature learning patterns of Fully-Connected 2-layer networks, CNN with Non-overlapping Patches, and Three-layer Network"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a well-defined problem, which is how to estimate the scaling property of sample complexity, in deep network, with is usually analytically intractable. To address this problem, the author uses lower-bound variational energy and a set of heuristics.\nThe paper supports their claim by both theoretical results and simulations, and show that these heuristics can estimate the lower bound variational energy and derive the sample complexity for fully-connected network and CNN with non-overlapping patches, which match with previous analytical results in the literature.\nThe paper also shows that their method can be applied to a more complicated network such as three-layer neural networks, which is difficult to get analytical results"}, "weaknesses": {"value": "As the paper employs heuristics to estimate the the lower variational bound for a specific set of feature learning patterns (Gaussian Process, Gaussian Feature Learning, Feature Specialization), under a specific task setup (polynomial of degree m), it's difficult to evaluate how these heuristics of specific feature learning patterns can be applicable for different tasks (classification, regression with a more general basis functions). I would appreciate if the authors could discuss the expected applicability or limitations of their framework beyond the polynomial setting."}, "questions": {"value": "1. In Eq. 7 and Eq. 8, how does $\\tilde{E}_q$ depends on $\\alpha$?\n2. (Fig 2B) How do we compute the number of specialist neurons in each layer?\n3. How does the three feature learning patterns (GP, GFL, and Specialization) get selected? Would these patterns cover most feature learning space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KQJV1hqRkh", "forum": "Lexn2TAw59", "replyto": "Lexn2TAw59", "signatures": ["ICLR.cc/2026/Conference/Submission18495/Reviewer_eTgY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18495/Reviewer_eTgY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049403517, "cdate": 1762049403517, "tmdate": 1762928188516, "mdate": 1762928188516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}