{"id": "1aOqIZ79IS", "number": 4330, "cdate": 1757663485972, "mdate": 1759898039164, "content": {"title": "White Gaussian Noise Constraints for Reward-Guided Generation", "abstract": "We propose a constrained optimization framework that preserves Gaussianity during latent optimization for reward-guided generation. At its core is a novel constraint formulation that allows efficient projection while tightly characterizing Gaussian properties. In deep generative models, supplying white Gaussian noise as input is essential for stable and realistic generation, but preserving its characteristics during optimization remains challenging. This challenge is amplified in reward-guided generation, where gradient-based updates can exploit the reward and produce unrealistic or low-quality outputs. Prior methods address this by introducing regularization terms that encourage certain Gaussian properties, particularly in the spectral domain. However, regularization offers only soft penalties and cannot guarantee that the latent vector retains its Gaussian properties throughout optimization. To overcome this, we propose a constrained optimization approach that directly projects the latent onto a feasible set. Our constraints, based on a novel observation that maps a Gaussian in the spatial domain to another Gaussian in a compact spectral domain, tightly characterize Gaussianity and allow a closed-form projection, enabling efficient updates through projected gradient ascent. In experiments on reward-guided text-to-image generation, our approach outperforms regularization-based baselines across four reward functions in terms of reward, sample quality, and maximization speed.", "tldr": "We propose a constrained optimization framework that preserves Gaussianity during latent optimization for reward-guided generation.", "keywords": ["Gaussianity", "Generative Models", "Guided Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4bfc1dfbcce455a70d3bc56de515fe52f867796f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a constrained latent optimization framework for diffusion model reward maximization. Latents are projected to the intersection of L1 and L2 norm spheres that match the first and second moments of a gaussian distribution. They present empirical gains with respect to penalized and unconstrained approaches for aesthetic/preference rewards."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "I think that both the use of constrained optimization tools for latent diffusion optimization is a valuable contribution that could inspire future work in this direction. I believe that the empirical evidence strongly supports the advantages of their methods with respect to baselines on terms of performance, and that their approach is computationally efficient, and thus widely applicable."}, "weaknesses": {"value": "I think that the theoretical and algorithmic contributions of this paper need to be properly outlined and references to prior work have to be added.\n\nThe \"construction of a compact spectral domain\", i.e. using the one-sided DFT, is customary in spectral analysis of real valued signals. That the DFT is hermitian (Lemma 1) and the Fourier transform of a gaussian is a gaussian (Theorem 1) are standard results. I think it would be useful to point this out and at least include a reference to a standard signal processing textbook.\n\nIn the same vein, algorithms projecting into the intersection of the L1 and L2 spheres (the algorithm presented in section 4.3)  have been studied before, see e.g. [1]. No reference to prior works are included here.\n\nI am not saying the use of these tools is not appropriate or that their application in this problem is not novel, just that their relation to prior work is missing altogether, and their presentation might mislead the reader into thinking these are contributions of this work.\n\n[1] Liu, H., Wang, H., & Song, M. (2019). A unified approach for projections onto the intersection of $\\ell_1$ and $\\ell_2$ balls or spheres."}, "questions": {"value": "Can you expand a bit more on why constraining the l1 and l2 norm to exactly match their expected values under gaussianity is beneficial?\n\nCan you also discuss the use of blocks instead of constraints on the whole vector, and the impact of block size choice?\n\nCan you plot the resulting L1 and L2 distributions of baseline latent optimization methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oFanMtfzOM", "forum": "1aOqIZ79IS", "replyto": "1aOqIZ79IS", "signatures": ["ICLR.cc/2026/Conference/Submission4330/Reviewer_ERkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4330/Reviewer_ERkz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579451028, "cdate": 1761579451028, "tmdate": 1762917302175, "mdate": 1762917302175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a constrained optimization framework for reward-guided generation that explicitly enforces Gaussianity in the latent space. Instead of using soft regularization (as in previous methods like PRNO or MPGR), the authors propose a closed-form projection that ensures the latent vector remains within a feasible set representing white Gaussian noise."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of replacing regularization with explicit Gaussian constraints and deriving a closed-form projection is original and mathematically elegant.\n- The use of a compact spectral domain mapping (bijective and preserving Gaussianity) is both theoretically sound and practically efficient."}, "weaknesses": {"value": "The experimental results are limited.\n- The number of testing prompts is limited to just 60 prompts, which is a very small set of samples. For evaluation, a set of at least 1000 prompts is needed. \n- The testing dataset's domain is limited to animals. \n- The testing problem is limited to text-to-image generation. I'm curious about the model performance when being tested with different problems."}, "questions": {"value": "Please see the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hdLLpXrQdx", "forum": "1aOqIZ79IS", "replyto": "1aOqIZ79IS", "signatures": ["ICLR.cc/2026/Conference/Submission4330/Reviewer_738g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4330/Reviewer_738g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601496077, "cdate": 1761601496077, "tmdate": 1762917301827, "mdate": 1762917301827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel inference-time input noise optimization technique for reward-guided generation based on projected gradient ascent (with closed-form projection) on a feasible set, which characterizes *Gaussianity* via block-wise $l_1$ and $l_{2}$ norm constraints of half-spectrum representation. The proposed feasible set constraints are presented to enforce the spatial and spectral characteristics of white Gaussian noise. Experiments show that the suggested method substantially outperforms similar baselines on *one-step* text-to-image human preference reward-guided generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Reformulation of test-time reward-guided optimization of the input noise via projected gradient ascent is novel;\n2. The motivation to use the first two moments of magnitude of half-spectrum components as characterizations of *Gaussianity* is supported by strong empirical results compared to the chosen baselines;\n3. Overall, the empirical performance of the method according to the quantitative evaluation on human-preference benchmarks is superior to the other input noise optimization methods."}, "weaknesses": {"value": "1. Despite utilizing the closed-form projection, the approach relies on many gradient steps per prompt. This results in a highly impractical method that performs hundreds of backward passes through both the reward model and the diffusion model to perform just *one*  reward-guided inference with *one-step* model. If the method could generalize on different prompts, it would potentially improve its applicability. In its current form, I would rather treat the method as a demonstration of the potential in optimizing the latents, than a practical option.\n2. The paper repeatedly frames the method as *preserving Gaussianity*, yet the feasible set enforces just the equalities of block-wise $l_{1}$ and $l_{2}$ norms in half-spectrum. Performing projection on this feasible set is not equivalent to maintaining the original Gaussian measure. Conversely, the resulting distribution will be concentrated on a manifold and will not have Lebesgue density. Moreover, this projection does not even guarantee that the distribution after projection will be similar to the Gaussian distribution projected on the same feasible set (e.g. projection of the Gaussian distribution on an $l_2$ sphere is uniform, while here this is an almost arbitrary distribution on the feasible set). The terminology and positioning seem misleading and could be read as stronger than what the constraints actually ensure.\n3. Some parts of the manuscript seem to have either too little or too much description of the underlying observations. Sections 4.1 â€“ 4.2, for example, revisit such classic results as DFT symmetry or Gaussian concentration (Figure 1) in detail. At the same time, the intuition behind the main projection algorithm is almost non-explained and is largely deferred to the Appendix, making the paper's main result less comprehensible."}, "questions": {"value": "1. Could you please tell, how sensitive are the results of the method to the block size $B$?\n2. Figure 2 claims MPGR requires a *slow gradient-based projection*, implying a large complexity gap between methods and a 4000$\\times$ runtime difference. My understanding is that MPGR jointly optimizes reward with both spectral regularization and moment matching in the spatial domain, resulting in almost the same complexity as the proposed method. Could you please provide details for the setting used in Fig. 2, and what *projection* you attribute to MPGR in that comparison?\n3. The method differs from the prior works in two ways: it introduces *hard* constraints and combines $l_{1}$ with $l_{2}$. Could the authors explain which of the two components is responsible for the gains compared to the prior methods: the combination of $l_{1}$ and $l_{2}$, or the explicit projection onto the feasible set? What would happen if we optimise soft constraints $L_{\\text{norm}}$ and $L_{\\text{power}}$ jointly with gradient ascent without any projections?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "daX5CRg1MY", "forum": "1aOqIZ79IS", "replyto": "1aOqIZ79IS", "signatures": ["ICLR.cc/2026/Conference/Submission4330/Reviewer_fY13"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4330/Reviewer_fY13"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941302465, "cdate": 1761941302465, "tmdate": 1762917301478, "mdate": 1762917301478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how to preserve Gaussianity in the latent optimization of reward-guided generation. The authors present a constrained optimization approach that directly imposes a Gaussianity constraint on the latent. To introduce project gradient ascent, the authors show an efficient projection update throughout a closed-form projection in spectral domain. Finally, the authors present several experiments to show the performance of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors present a constrained optimization approach to impose Gaussianity on the latent prior as Gaussian noise constraints. This is a different perspective compared to previous regularization-based methods. \n\n- The authors analyze Gaussian noise constraints in the spectral domain by showing that the projection can be evaluated explicitly. This is important to ensuring computational efficiency. \n\n- Experiments show the effectiveness of the proposed method in generating realistic images."}, "weaknesses": {"value": "- When maximizing the task-specific reward of latent generative models, it is unclear how preserving Gaussianity for the latent prior affects stable and realistic generation.\n\n- The difference between constraint and regularization is not clearly discussed. For instance, we can always formulate constraints as indicator functions in regularization.\n\n- For the proposed constrained optimization, the feasibility and optimality are not analyzed.\n\n- For preserving Gaussianity, it does not necessarily require Gaussianity for all gradient ascent steps. For instance, we can always project the last step to be a Gaussian.\n\n- Projected gradient ascent can be very expensive, due to the computational cost of projection step and gradient evaluation. Gradient evaluation is not always feasible, for instance reward is non-differentiable. \n\n- It is not discussed how does the inaccurate FFT affect the projection step, since FFT is often computed within some accuracy level."}, "questions": {"value": "See comments in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cI1rfuK6Wd", "forum": "1aOqIZ79IS", "replyto": "1aOqIZ79IS", "signatures": ["ICLR.cc/2026/Conference/Submission4330/Reviewer_7U1f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4330/Reviewer_7U1f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762308352466, "cdate": 1762308352466, "tmdate": 1762917301190, "mdate": 1762917301190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}