{"id": "EVoW9zA37C", "number": 13743, "cdate": 1758221911281, "mdate": 1763122107690, "content": {"title": "Attribute-Guided Image Generation with Causally Disentangled Representation", "abstract": "Controllable image generation is a fundamental problem in machine learning and computer vision. Attribute-guided generative models enable explicit control over image content using labeled attributes, but often struggle to disentangle individual attributes and mitigate unwanted correlations—for example, adding eyeglasses may inadvertently alter a person’s perceived age.\nIn this work, we propose a novel attribute-guided generative framework designed to address these challenges. Our method learns a mask-based representation for each attribute label, encouraging disentanglement by limiting each attribute’s influence to a small subset of the representation dimensions, while still preserving the information necessary to represent the label. To address attribute correlations, we incorporate classical causal discovery techniques to model inter-attribute dependencies and introduce a causal conditioning strategy that explicitly reduces undesirable correlations. Importantly, we provide theoretical guarantees showing that our method can recover the latent generative factors associated with individual attributes. Extensive experiments on diverse datasets demonstrate that our framework substantially improves attribute-level controllability and interpretability, outperforming existing baselines on attribute-guided image generation tasks.", "tldr": "We propose a causality-aware disentanglement method for attribute-guided image generation", "keywords": ["Disentanglement", "Causality", "attribute-guided image generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/43742bbf558c2341aac487c76e49419f2246eed7.pdf", "supplementary_material": "/attachment/90b01657252b789aada65128b4b0a3d4d0a6aa2d.zip"}, "replies": [{"content": {"summary": {"value": "Previous methods for controlled generation suffer from poor attribute disentanglement and unwanted attribute correlations. To solve these problems, the paper proposes a novel attribute-guided generative framework. Main contributions are as follows:\n\n1.  **Mask-Based Representation Learning:** A technique that learns a mask-based representation for each attribute label and ensures that each attribute label influences only a minimal subset of the representation dimensions.\n2.  **Causal Conditioning Strategy:** This strategy learns attribute-specific representations conditioned only on their causal parents. This strategy reduces undesirable correlations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-   **[S1] Novel Approach:** Combining a novel mask-based representation for disentanglement with a causal conditioning strategy for correlation reduction is novel. Further, the authors provide a theoretical proof in Sec 3.3 to solidify their claims."}, "weaknesses": {"value": "-   **[W1] Lack of a standardized qualitative comparison for the attribute editing tasks:** In Fig. 4,6,7 and 11, the proposed method and the baselines are applied to different input images. This  does not seem to be a fair to fair comparison as it makes difficult to assess the relative performance and failure modes of the models.\n\n-   **[W2] Comparison with recent models like Flux Kontext:** Given that this model is known to work on _open-ended_ conditional generation, a direct comparison is required to assess the proposed method's claim on the recent generative methods.\n-   **[W3] Difficult to extend to diffusion/flow-based generative models:** The paper is based on StyleGAN, whose latent space (W space) is disentangled and has semantic attributes. In comparison, recent diffusion-based models do not have this W space and conditions using a cross-attention mechanism. Furthermore, the causal conditioning method will not extend to the diffusion-based generative methods."}, "questions": {"value": "-   [Q1] Have authors tried other attributes like lighting conditions, pose variations etc?\n-   [Q2] How about other correlated attributes like \"age and hair\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l0pIBaFexc", "forum": "EVoW9zA37C", "replyto": "EVoW9zA37C", "signatures": ["ICLR.cc/2026/Conference/Submission13743/Reviewer_v3BA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13743/Reviewer_v3BA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840055748, "cdate": 1761840055748, "tmdate": 1762924280408, "mdate": 1762924280408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "QnS8gCGL6q", "forum": "EVoW9zA37C", "replyto": "EVoW9zA37C", "signatures": ["ICLR.cc/2026/Conference/Submission13743/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13743/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763121801351, "cdate": 1763121801351, "tmdate": 1763121801351, "mdate": 1763121801351, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel framework for controllable image generation that integrates causal representation learning with attribute-guided modeling. The authors identify two long-standing challenges in this field: (1) achieving disentanglement of attribute representations, and (2) mitigating spurious correlations between attributes (e.g., “eyeglasses” co-varying with “age”). To address these, they introduce a mask-based representation learning scheme and a causal conditioning mechanism.\n\nThe mask-based component enforces sparsity—each attribute affects only a small subset of the latent representation—thus promoting disentanglement and interpretability. The causal conditioning strategy explicitly models inter-attribute dependencies using classical causal discovery methods (e.g., PC algorithm) and intervenes on exogenous variables to decorrelate attributes during generation. The paper further provides theoretical identifiability guarantees for recovering latent factors associated with individual attributes.\n\nEmpirically, the proposed model is implemented on the StyleGAN2-ADA backbone and evaluated across diverse datasets including AFHQDog, ZAPPOS, ColorMNIST, LSUN Bedroom, and FFHQ. Quantitative metrics such as FID, ID, DINO similarity, and a custom disentanglement score consistently demonstrate improvements over baselines (StyleGAN2-ADA, InterfaceGAN, WPlus, ConceptSlider, SANA, and Flux 1-dev). Visual comparisons show that the method achieves fine-grained, smooth control of target attributes without unintended modifications.\n\nThe paper also discusses extensions to text-to-image models (Flux 1-dev) and demonstrates promising improvements through time-based masking and causal conditioning. Theoretical rigor, strong experimental results, and clear motivation establish this work as a substantial contribution toward interpretable and causally robust controllable generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Conceptual novelty - The integration of causal discovery with attribute-guided image generation is elegant and timely. While prior works (InterfaceGAN, StyleCLIP) rely on linear or heuristic disentanglement, this paper introduces a principled causal framework backed by identifiability proofs. \n\n2. Technical rigor - The paper provides formal theoretical analysis (Theorem 1) guaranteeing recovery of attribute-specific latent factors, which is rarely attempted in generative modeling papers. The causal conditioning process is mathematically grounded in structural causal models, giving a firm foundation to the proposed methodology.\n\n3. Comprehensive experiments -The authors benchmark across both independent and correlated-attribute datasets with diverse visual domains. Quantitative gains in FID and disentanglement, alongside extensive visual results, convincingly support the claims. Ablation studies (with/without mask, with/without causal module) are thoughtfully designed and reveal clear contributions of each component.\n\n4. Clarity and organization – The paper is well-written and well-structured. Figures clearly illustrate mechanisms and qualitative effects. The authors explicitly contrast their work against CausalGAN, CausalVAE, and ConceptSlider, situating it firmly within the literature."}, "weaknesses": {"value": "1. Computational limitations - The paper restricts experiments to GANs, noting diffusion models are “computationally prohibitive.” Given the field’s rapid shift toward diffusion-based architectures, this may limit immediate practical relevance. \n\n2. Empirical scope of causal discovery - The causal structure learning is briefly described (using PC algorithm and minimal prior knowledge). However, the reliability and stability of the learned causal graph across datasets are not deeply analyzed. How sensitive are results to errors in causal edge orientation?\n\n3. Limited real-world complexity - The datasets used (AFHQ, ZAPPOS, FFHQ) have relatively simple and well-annotated attributes. It remains unclear how the method performs when attributes are continuous, hierarchical, or ambiguous (e.g., aesthetic style, emotion intensity).\n\n4. Comparative baselines – The strongest diffusion-based controllable generation methods (e.g., Uni-ControlNet, T2I-Adapter, InstructPix2Pix) are omitted. While computationally expensive, referencing their expected differences would clarify the paper’s relative positioning."}, "questions": {"value": "1. Causal Graph Learning: How robust is your method to mis-specified causal graphs? If the PC algorithm yields incorrect edges, does the model still retain disentanglement due to the mask mechanism, or do performance metrics degrade sharply?\n\n2. Scalability: Could the causal conditioning mechanism scale to high-dimensional attribute spaces (e.g., > 50 labels as in CelebA-HQ)? Does the computational cost of discovering and conditioning on the causal structure grow quadratically with attribute count?\n\n3. Continuous Attributes: Your framework seems tailored for binary labels. How would it extend to continuous or ordinal attributes (e.g., “age,” “smile intensity”)? Would the mask formulation or identifiability proof still hold?\n\n4. Attribute Interaction Visualization: It would be insightful to include quantitative or graphical analysis of learned causal effects (e.g., how much “age” changes when “eyeglasses” is toggled). Are such diagnostics available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FQPcH4I14h", "forum": "EVoW9zA37C", "replyto": "EVoW9zA37C", "signatures": ["ICLR.cc/2026/Conference/Submission13743/Reviewer_FnRx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13743/Reviewer_FnRx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926387674, "cdate": 1761926387674, "tmdate": 1762924279957, "mdate": 1762924279957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a controllable image generation framework that learns mask-based attribute representations to achieve disentanglement and applies causal conditioning to reduce spurious correlations between attributes. The model uses an attribute-invariant code and attribute-specific codes, then mixes them via learnable masks before feeding into the generator. Experiments on benchmarks show improved FID and stronger edit controllability v.s. GAN baselines and other text-to-image methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper has a clear problem focus, with disentanglement + correction, which is reasonable and classical setting for causal representation learning.\n\n2. The mask mixing of attribute-specific and attribute-invariant latents is pretty intuitive and elegant.\n\n3. The experiments have shown the effectiveness of the method."}, "weaknesses": {"value": "1. The theorem 1 is stated pretty informally and I think the invertible mapping assumption might be too strong. I would suggest the author to add more explanation to this part. It would also be great if author could empirically check the assumption by, e.g., checking mutual information between latent variables and factors.\n\n2. In Eq. (2), without any constraint on z_c or regularizations, I don't think z_c is irrelevant to any features. Leak of attribute information to z_c must harm the disentanglement.\n\n3. Sparsity on each m_i cannot secure mutual exclusivity over attributes. The author should explain more here or do some ablation studies. \n\n4. The evaluation process lacks diffusion baselines. Without training and evaluation on diffusion baselines under the same settings, it's hard to tell if the performance gain comes from the proposed method, or simple from difference between GANs and diffusion models."}, "questions": {"value": "Please refer to my comments in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8Q5bcvWOj9", "forum": "EVoW9zA37C", "replyto": "EVoW9zA37C", "signatures": ["ICLR.cc/2026/Conference/Submission13743/Reviewer_u7aR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13743/Reviewer_u7aR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974892924, "cdate": 1761974892924, "tmdate": 1762924279417, "mdate": 1762924279417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies attribute-guided image generation with two main pieces: (1) per-attribute latents gated by a learnable sparse mask so each attribute only touches a tiny slice of the representation, and (2) a causality-aware conditioning step that uses parent-conditioned surrogates so edits to a child label don’t “leak” into its parents. There’s also an identifiability argument tailored to this setup. Empirically, the method shows better controllability and competitive image quality across several datasets, plus cleaner edits on correlated face attributes; ablations support that both the sparsity and causal parts matter."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The masking idea is simple. The implementation seems straightforward to reproduce.\n\nThe causal conditioning is lightweight compared to training a full causal label generator, yet seems to reduce side effects in edits.\n\nResults look consistent with the story."}, "weaknesses": {"value": "Robustness to mistakes in the discovered causal graph isn’t really explored; it’s unclear how sensitive the method is if some edges are wrong or missing.\n\nThe identifiability claim relies on assumptions that are hard to verify with noisy, correlated labels; practical consequences of violations aren’t discussed."}, "questions": {"value": "Robustness to causal-graph errors isn’t really explored; it’s not obvious how sensitive the method is if some edges are wrong or missing. \n\nThe identifiability claim rests on assumptions that are not testable. Practical consequences of violations aren’t discussed in depth. \n\nSome datasets use automated annotations or classifiers; a clearer read on label-noise sensitivity would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gfJh9swn88", "forum": "EVoW9zA37C", "replyto": "EVoW9zA37C", "signatures": ["ICLR.cc/2026/Conference/Submission13743/Reviewer_vkww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13743/Reviewer_vkww"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762361719701, "cdate": 1762361719701, "tmdate": 1762924278815, "mdate": 1762924278815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}