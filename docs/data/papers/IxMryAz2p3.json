{"id": "IxMryAz2p3", "number": 9314, "cdate": 1758118507594, "mdate": 1759897731888, "content": {"title": "BTZSC: A Benchmark for Zero-Shot Text Classification Across Cross-Encoders, Embedding Models, and Rerankers", "abstract": "Zero-shot text classification (ZSC) offers the promise of eliminating costly task-specific annotation by matching texts directly to human-readable label descriptions. While early approaches have predominantly relied on cross-encoder models fine-tuned for natural language inference (NLI), recent advances in text-embedding models and rerankers have challenged the dominance of NLI-based architectures. Existing evaluations, such as MTEB, often probe embedding models with supervised classifiers atop frozen embeddings, leaving true zero-shot capabilities underexplored. To address this, we introduce BTZSC, a comprehensive benchmark of $22$ public datasets spanning sentiment, topic, intent, and emotion classification, capturing diverse domains, class cardinalities, and document lengths. Leveraging BTZSC, we conduct a systematic comparison across three major model families, NLI cross-encoders, embedding models, and rerankers, encompassing $31$ public and custom checkpoints. Our results show that: (i) modern rerankers, exemplified by Qwen3-Reranker-8B, set a new state-of-the-art with macro $F_{1} = 0.72$; (ii) strong embedding models such as GTE-large-en-v1.5 substantially close the accuracy gap while offering the best trade-off between accuracy and latency; (iii) NLI cross-encoders plateau even as backbone size increases; and (iv) scaling primarily benefits rerankers over embedding models. BTZSC and accompanying evaluation code are publicly released to support fair and reproducible progress in zero-shot text understanding.", "tldr": "A comprehensive benchmark evaluating zero-shot classification performance across cross-encoders, bi-encoders, and rerankers on 25 tasks.", "keywords": ["zero-shot classification", "cross-encoder", "embedding models", "reranker", "benchmark", "classification tasks", "NLPzero-shot classification", "cross-encoder", "embedding models", "reranker", "benchmark", "classification tasks", "NLP"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/495b0147da70049c58923596bbcf77bb3273145c.pdf", "supplementary_material": "/attachment/ce641529acfd9e85ada175823b31df89b778b2ca.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces BTZSC, a comprehensive benchmark for evaluating zero-shot text classification performance across three major model paradigms: NLI-based cross-encoders, embedding models, and rerankers. BTZSC unifies 22 English datasets covering sentiment, topic, intent, and emotion classification under a text-label semantic matching framework, where each label is verbalized as a natural-language sentence. Through systematic experiments with 31 models, the paper finds that rerankers achieve the highest zero-shot accuracy, while strong embedding models offer better efficiency-performance trade-offs. The benchmark further analyzes scaling trends, NLI transferability, and latency-accuracy relationships, providing new insights into how different architectures generalize under true zero-shot conditions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The benchmark systematically evaluates three major paradigms for zero-shot text classification under a unified evaluation protocol. This breadth of comparison provides valuable insight into how different architectures trade off accuracy, scalability, and cost.\n2) The benchmark integrates 22 publicly available datasets spanning multiple task types, which enables a relatively broad assessment of zero-shot performance across heterogeneous domains."}, "weaknesses": {"value": "1) The paper claims to be \"true zero-shot\", but the 22 datasets used (such as AGNews, IMDb, AmazonPolarity, etc.) are all from public corpora that may have been seen during model pre-training. This means that the model may have indirectly learned the task distribution or category semantics, and strictly speaking, it is impossible to verify the model's migration ability under \"completely new tasks\".\n2) The benchmark relies solely on macro-averaged F1 as the evaluation metric. While this simplifies cross-dataset comparison, it introduces several methodological limitations. F1 alone cannot reveal important trade-offs between precision and recall, nor can it account for task difficulty or class imbalance. Moreover, reranker models are evaluated only on top-1 predictions, ignoring their ranking quality. The absence of per-class variance and statistical significance testing further limits interpretability."}, "questions": {"value": "1) Is the full benchmark publicly released or planned to be open-sourced? If not, could you clarify the reasons or timeline for public availability?\n2) How do you ensure that the datasets used were not partially included in the pre-training corpora of the evaluated models (such as BERT, DeBERTa, GTE, E5, or Qwen)?\n3) Why did you choose to rely solely on macro-averaged F1, and did you evaluate whether other metrics lead to consistent rankings across models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AeYOU5HHq3", "forum": "IxMryAz2p3", "replyto": "IxMryAz2p3", "signatures": ["ICLR.cc/2026/Conference/Submission9314/Reviewer_tZ9R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9314/Reviewer_tZ9R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790977517, "cdate": 1761790977517, "tmdate": 1762920950949, "mdate": 1762920950949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BTZSC, a benchmark comprising 22 datasets to evaluate zero-shot text classification performance across three major model families: NLI-based cross-encoders, embedding models, and rerankers. The authors conduct a broad empirical comparison across 31 models and report that rerankers achieve the highest accuracy, while embedding models offer the best efficiency-performance trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper systematically covers a wide range of datasets and model types, making it one of the most comprehensive empirical studies on zero-shot text classification to date. The release of the BTZSC benchmark and codebase supports reproducibility and could serve as a useful evaluation tool for the community."}, "weaknesses": {"value": "1. The work lacks technical novelty, where no new algorithm, model, or training strategy is proposed.\n2. The core contribution lies in benchmark, which is largely incremental given the existence of MTEB and similar efforts. The experimental insights are mostly intuitive or expected (e.g., rerankers scale better, embedding models are efficient), and analysis is superficial.\n3. The evaluation design largely mirrors existing work, such as label verbalization and metric selection, with limited innovation in methodology.\n4. The discussion around trade-offs and scaling is descriptive but lacks theoretical depth."}, "questions": {"value": "Please refer to weaknesses regarding the lack of technical contributions and analytical depth."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eYpEcX6DzR", "forum": "IxMryAz2p3", "replyto": "IxMryAz2p3", "signatures": ["ICLR.cc/2026/Conference/Submission9314/Reviewer_Muat"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9314/Reviewer_Muat"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823750427, "cdate": 1761823750427, "tmdate": 1762920949179, "mdate": 1762920949179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BTZSC, a benchmark for true zero‑shot text classification that compares three families: NLI‑tuned cross‑encoders, embedding models, and rerankers; over 22 English datasets spanning topic, sentiment, intent, and emotion. The primary metric is macro‑F1 (with micro accuracy also reported). The study evaluates 31 models (public and custom), analyzes scaling and the NLI to ZSC transfer, and reports that: (i) rerankers, notably Qwen3‑Reranker‑8B, achieve the highest overall performance (macro‑F1 = 0.72), (ii) strong embeddings (e.g., GTE‑large‑en‑v1.5) “close the gap” and tend to offer the best accuracy–latency trade‑off, and (iii) NLI cross‑encoders are plateauing while scaling primarily benefits rerankers. See Abstract; dataset suite and selection in §3, Table 1 (p. 4), Figure 1 (p. 5); model families in §3.2 (pp. 5–6) and Table 4 (p. 15); results in Table 2 (p. 9); and analyses in Figure 2 (p. 8) and Figure 3 (p. 8)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Representative, diverse suite. 22 datasets across four task families; Table 1 and Figure 1 quantify class cardinalities, lengths, and lexical overlap (pp. 4–5).   \n- Clean zero‑shot protocol across families. Unified scoring: NLI log‑odds entailment; embedding cosine; reranker yes/no token probability with a fixed template (§4, Appendix C.3).  \n- Clear headline results. Qwen3‑Reranker‑8B tops macro‑F1 (0.72); GTE‑large‑en‑v1.5 is competitively high among embeddings; Figure 2(b) shows reranker‑favoring scaling; Figure 3(a) visualizes the speed–accuracy frontier (p. 8; p. 9).  \n- NLI to ZSC analysis. Figure 3(b) offers a useful lens on transfer for cross‑encoders and contrasts to embeddings/rerankers (p. 8)."}, "weaknesses": {"value": "1. No zero‑shot LLM‑as‑classifier baseline. The Introduction acknowledges the approach but omits it from evaluation; 8B–32B LLMs are feasible on the stated hardware and would complete Figure 3(a) (pp. 2, 8, 10).\n2. Embedding template opacity. §4 lacks per‑model instruction/template details for E5/GTE/BGE/Qwen‑Embedding; performance is often template‑sensitive (p. 6). These models can be instructed.\n3. Verbalizer multiplicity untested. Single label verbalization per class; no template‑averaging/paraphrase robustness (p. 6; Appendix A notes reusing Laurer et al. 2023 verbalizers). Authors can consider zero-shot augmentations on verbalizers, with a L2-normalization of the average embedding of augmentations like CLIP, or a k-NN approach where majority voting across different augmented forms of verbalizers is used.\n4. Minor inconsistencies. banking77 listed as 77 classes in text (p. 3) vs 72 in Table 1 (p. 4); and the “second‑highest overall 0.64” prose refers to accuracy after defining macro‑F1 as primary (pp. 4, 7, 9).\n5. For completion, it would be interesting to verify whether the conclusions of this paper are reproducible using the same models and evaluation methodology on the classification tasks of MTEB instead. However, such an analysis is missing."}, "questions": {"value": "Aren't the used reranking models cross-encoders?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RwNe2cFIlt", "forum": "IxMryAz2p3", "replyto": "IxMryAz2p3", "signatures": ["ICLR.cc/2026/Conference/Submission9314/Reviewer_yjwX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9314/Reviewer_yjwX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935318350, "cdate": 1761935318350, "tmdate": 1762920948692, "mdate": 1762920948692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a comprehensive benchmark for Zero-Shot Text Classification including 22 datasets, with a focus on comparing three types of approaches: NLI, sentence transformers, and re-rankers. The proposed benchmark is designed to cover task diversity, class granularity, diversity of text domain and length. The paper proposes a primary metric, Micro F1, and the use of NLI performance as a proxy for zero-shot generalization capabilities. Empirical results indicate reranker models achieve the highest overall accuracy, while strong embedding models offer the most favorable balance between speed and accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I liked the proposal of a comprehensive evaluation dataset and the clear explanations of the three zero-shot approaches: NLI, sentence transformers, and re-rankers. The evaluation is systematic and includes many model variants. I also found the analysis of scaling across model sizes informative."}, "weaknesses": {"value": "1. Given this is a dataset/benchmark paper, I'm surprised by how little detail it offers about the construction of the dataset and evaluation procedure. I am having a hard time connecting the underlying datasets and how the final BTZSC evaluation procedure works.\n2. The paper primarily focus on encoder-based architectures, I wonder how do generative models perform on such tasks, and would a reasonably sized generative model effectively solve these tasks? What distinct value does this benchmark provide relative to generative models?"}, "questions": {"value": "1. How are the datasets aggregated? Are they not directly aggregated or the labels are aggregated by domain?\n2. I wonder how do generative models perform on such tasks, and would a reasonably sized generative model effectively solve these tasks? What distinct value does this benchmark provide relative to generative models?\n3. Given this dataset, what are some challenges in existing methods and promising directions for future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VJTwlFbEgn", "forum": "IxMryAz2p3", "replyto": "IxMryAz2p3", "signatures": ["ICLR.cc/2026/Conference/Submission9314/Reviewer_9fKe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9314/Reviewer_9fKe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968014708, "cdate": 1761968014708, "tmdate": 1762920948205, "mdate": 1762920948205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}