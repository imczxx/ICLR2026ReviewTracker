{"id": "cHILNck9eW", "number": 430, "cdate": 1756739198750, "mdate": 1762931036402, "content": {"title": "TARS: MinMax Token-Adaptive Preference Strategy for MLLM Hallucination Reduction", "abstract": "Multimodal large language models (MLLMs) enable vision-language reasoning, yet often generate plausible outputs that are factually incorrect or visually ungrounded, thereby compromising their reliability. Direct preference optimization (DPO) is a common strategy for correcting hallucinations by aligning model outputs with human preferences. However, existing DPO strategies typically treat hallucination-related preferences as fixed targets, relying on static and potentially biased supervision signals during training. This approach tends to overfit to superficial linguistic cues in preference data, leading to distributional rigidity and spurious correlations that impair grounding in causally relevant visual information. To overcome this limitation, we propose TARS, a token-adaptive preference strategy that reformulates DPO as a min–max optimization problem. TARS maximizes token-level distributional shifts under explicit semantic constraints to simulate alignment uncertainty, and simultaneously minimizes the expected preference loss under these controlled perturbations. This joint objective effectively preserves causal grounding while mitigating overfitting to preference patterns, thereby reducing hallucinations in multimodal reasoning. We evaluate TARS on multiple hallucination benchmarks and find consistently robust performance. Using only 4.8k preference samples and no expert feedback, TARS reduces hallucination rates from 26.4\\% to 13.2\\% and decreases cognition value from 2.5 to 0.4, outperforming standard DPO and matching GPT-4o on several key metrics.", "tldr": "", "keywords": ["Multimodal LLM", "Hallucination"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/593492b26682d36856eef312a1d14c6f53ba80a1.pdf", "supplementary_material": "/attachment/62d1cf67eefbdf61ae9bdfe415702c2d7e96658a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes TARS, a token-adaptive min-max strategy to mitigate hallucinations in MLLM. It reforms DPO into a min-max objective: 1) maximize token-level distributional shifts to simulate alignment uncertainty; 2) minimize preference loss with spectral regularization. Using only 4.8k preference samples, TARS outperforms DPO baselines on LLaVA-v1.5 on hallucinations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is data-efficient as it only uses 4.8k records.\n2. The overall performance on the hallucination reduction is sound and surpasses many other baselines."}, "weaknesses": {"value": "1. The paper does not show the performance on the general MLLM/LLM benchmark, which makes me concerned that the performance over the hallucination is increased while the general performance is degrading. \n2. In the paper, the authors suppose that the visual-text relevance could be computed from the similarity of the dot product between visual features and token embeddings. This seems to be intuitive, while it would help if some real examples were visualized to check whether the relevance could be calculated in this way. \n3. The token-level perturbation seems to only ablate on the visually agnostic tokens. An ablation of the random token perturbation on the replace and mask strategy is needed. \n4. The proposed frequency-domain alignment aims at ensuring semantic consistency, while no ablation on semantic similarity is done over the spectral regularization, only the hallucination metrics. \n5. Though it may introduce more experiments and more computation, I still recommend that the author fully fine-tune the RLHF-V to see its scaling ability. A randomly sampled subset will introduce more randomness."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "naXPZSmGqP", "forum": "cHILNck9eW", "replyto": "cHILNck9eW", "signatures": ["ICLR.cc/2026/Conference/Submission430/Reviewer_f4yi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission430/Reviewer_f4yi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848978482, "cdate": 1761848978482, "tmdate": 1762915518585, "mdate": 1762915518585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Hg7SOhscAU", "forum": "cHILNck9eW", "replyto": "cHILNck9eW", "signatures": ["ICLR.cc/2026/Conference/Submission430/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission430/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762930884622, "cdate": 1762930884622, "tmdate": 1762930884622, "mdate": 1762930884622, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TARS, a Token-Adaptive Preference Strategy designed to reduce hallucinations in Multimodal Large Language Models (MLLMs). The authors identify that standard Direct Preference Optimization (DPO) methods, which align model outputs with human preferences, often overfit to superficial textual cues in the preference data, leading to \"distributional rigidity\" and increased hallucinations. TARS reformulates DPO as a min-max optimization problem: it first *maximizes*token-level distributional shifts by perturbing \"visual-agnostic\" tokens (textual elements with minimal grounding in the image) to simulate uncertainty and break spurious correlations, and then *minimizes*the preference loss under these controlled perturbations. This approach encourages the model to rely on causally relevant visual information rather than textual patterns."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper claims standard DPO overfits to spurious text tokens and proposes TARS, which perturbs those “visual-agnostic tokens” to reduce hallucination. \n2. TARS reports better grounding / lower hallucination on benchmarks like AMBER and MMHal, sometimes close to much larger models, using only ~4.8k preference pairs."}, "weaknesses": {"value": "1. The method relies on computing a similarity score between each text token embedding and the visual features, then taking the tokens with lowest cross-modal similarity as “visual-agnostic,” and perturbing only those. This is a strong assumption: it assumes (i) that dot-product similarity between Gᵥ(x) and Gₜ(qᵢ) is a valid proxy for whether a token is visually grounded, and (ii) that low-similarity tokens are in fact safe to perturb without changing semantics. The paper does not rigorously justify this assumption with citations to prior work, nor does it provide its own empirical validation. \n2. Randomly masking/replacing those tokens may break grammar and semantics, and they don’t show that general VQA / reasoning quality is not harmed. Recommend authors evaluate their method in MM-Vet, HalluBench, etc.\n3. All main results are on older baselines like LLaVA-1.5/Muffin; they don’t apply TARS to stronger recent MLLMs such as Qwen 2.5 VL, so generality is unclear.\n4. Equation (11) seems inconsistent with standard DPO: the second term’s numerator uses $y_w$ again, but it should likely be $y_r$; this needs correction."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PCUWqG1o1s", "forum": "cHILNck9eW", "replyto": "cHILNck9eW", "signatures": ["ICLR.cc/2026/Conference/Submission430/Reviewer_Debk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission430/Reviewer_Debk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902508749, "cdate": 1761902508749, "tmdate": 1762915518429, "mdate": 1762915518429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TARS, a preference-learning framework that reformulates DPO into a token-adaptive min–max method for multimodal LLMs. The inner maximization perturbs visual-agnostic text tokens to simulate controlled distribution shifts; the outer minimization applies a DPO-style objective to align with preferences. The selection of visual-agnostic tokens aims to discourage spurious text-only shortcuts and force grounding in visual evidence. The method further adds a frequency-domain regularizer (“spectral preference alignment”, SPA). The authors evaluate primarily on AMBER, MMHal**, **POPE, and OBJHal, reporting consistent hallucination reductions at 7B/13B scales and favorable ablations for the three components: TP (token perturbation), CAS (cross-modal alignment score) for token selection, and SPA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear illustration on problem formulation.** Casting preference learning as a min–max problem over token-level perturbations helps mitigate the “style/phrase” overfitting often seen with vanilla DPO in multimodal settings. \n2. **Broad evaluation on recognized hallucination suites.** Benchmarks (AMBER, MMHal, POPE, OBJHal, CHAIR) are aligned with current community practice for LVLM hallucination assessment."}, "weaknesses": {"value": "1. **Why token perturbation?** Can the authors provide the reason / intuition that supports this design? i.e., any intuition to illustrate the rationality of this design? Moreover, \n   1. What is the operational space of the perturbation? the whole vocabulary?\n   2. Can the authors give some examples for what the text input ***after*** perturbation be like?\n2. **Spectral regularization using FFT.** What is the point of using FFT to do alignment? What is the intuition / reason that supports this design? It's not enough to just say \"Enforcing finegrained constraints may reintroduce spurious correlations that our min–max strategy aims to mitigate\" (line 240-242).\n3. **Frequency-domain regularizer needs sharper theoretical footing.**\n    SPA is motivated as smoothing over local token perturbations, but the connection between **Fourier-space alignment** and *down-weighting spurious token-level correlations* remains largely unclear. A small theoretical or diagnostic study (e.g., spectrum-response correlation to CHAIR/POPE error modes) would clarify *why* SPA helps beyond acting as another consistency loss.  \n4. **Evaluation benchmarks & baselines.** Apart from existing benchmarks adopted in the manuscript, there are also some more comprehensive benchmarks for hallucinations such as HallusionBench [1] and MMStar [2]. Also, to validate the effectiveness of TARS, some newest multimodal DPO baseline methods should be considered in experiments, such as SymMPO [3] and HSA-DPO [4].\n\nIf the authors can solve my concerns, I am willing to increase the rating.\n\nReferences:\n\n[1] Guan et al., \"HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models\" In CVPR 2024.\n\n[2] Chen et al., \"Are we on the right way for evaluating large vision-language models?\" In NeurIPS 2024.\n\n[3] Liu et al., “Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization” In NeurIPS 2025.\n\n[4] Xiao et al., \"Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback\" In AAAI 2025."}, "questions": {"value": "1. **Clarity/details.**\n   - The paper defines **Sim((\\phi(q), q))** (Eq. 7) but does not fully specify the similarity space or substitution strategy / search heuristic that  constructs (\\phi)? \n   - SPA (spectral regularizer) is introduced at a high level; please expand its exact form, hyper-parameters, and interaction with the DPO loss.\n2. **SPA details:** What is the **exact spectral loss** and how sensitive is performance to its coefficient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ckTyFEdHPM", "forum": "cHILNck9eW", "replyto": "cHILNck9eW", "signatures": ["ICLR.cc/2026/Conference/Submission430/Reviewer_iypP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission430/Reviewer_iypP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160431304, "cdate": 1762160431304, "tmdate": 1762915518171, "mdate": 1762915518171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}