{"id": "aELfZbdE3J", "number": 8290, "cdate": 1758077692248, "mdate": 1759897793999, "content": {"title": "DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in Reasoning Large Language Models", "abstract": "Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1, have recently demonstrated remarkable capabilities by performing structured and multi-step reasoning. However, recent studies reveal that RLLMs often suffer from overthinking, i.e., producing unnecessarily lengthy reasoning chains even for simple questions, leading to excessive token consumption and computational inefficiency. Interestingly, we observe that when processing multiple questions in batch mode, RLLMs exhibit more resource-efficient behavior by dynamically compressing reasoning steps for easier problems, due to implicit resource competition. Inspired by this, we propose Dynamic Reasoning Quota Allocation (DRQA), a novel method that transfers the benefits of resource competition from batch processing to single-question inference. Specifically, DRQA leverages batch-generated preference data and reinforcement learning to train the model to allocate reasoning resources adaptively. By encouraging the model to internalize a preference for responses that are both accurate and concise, DRQA enables it to generate concise answers for simple questions while retaining sufficient reasoning depth for more challenging ones. Extensive experiments on a wide range of mathematical and scientific reasoning benchmarks demonstrate that DRQA significantly reduces token usage while maintaining, and in many cases improving, answer accuracy. By effectively mitigating the overthinking problem, DRQA offers a promising direction for more efficient and scalable deployment of RLLMs, and we hope it inspires further exploration into fine-grained control of reasoning behaviors.", "tldr": "", "keywords": ["LLM Efficiency", "RLLM", "LLM Reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/512daf38e1d71959515d0adc5d526f80b9b961cb.pdf", "supplementary_material": "/attachment/ab4e6838d5a344e873246f83a7c14b9b63f87f44.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles LLM overthinking by transferring the efficiency of batch inference into single-question settings. The authors discovered that batch inference settings encourage more concise reasoning, and scaling up the batch size improves reasoning efficiency. However, performing SFT on generations from batch inference mode leads to lower-quality model. Therefore, the authors propose DRQA, a method for encouraging concise reasoning while maintaining model quality. The authors build a preference dataset from a teacher model in batch vs. single inference mode and train the student model to predict whether the answer is correct and concise using GRPO. Across GSM8K, MATH-500, AIME, and LiveCodeBench, DRQA cuts token usage by 30% while maintaining or improving accuracy, outperforming a simpler SFT imitation approach that shortens outputs but can hurt accuracy on harder tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed method shows strong empirical effectiveness.\n2. The method is well-motivated and clearly presented."}, "weaknesses": {"value": "1. It's unclear how the proposed objective (classifying CoTs as accurate/concise) translates into better generation quality. Predicting labels may not directly improve the model's ability to produce accurate, succinct solutions.\n2. The data source used for genearting concise CoTs is questionable. Batch inference is only one of many ways to elicit shorter CoTs and is not reliably shorter than single inference; using it as the primary mechanism to obtain concise traces seems brittle compared with alternative prompting or decoding strategies.\n3. The labeling mechanism needs a redesign. Assigning label B to batch CoTs whenever the final answer is correct assumes those traces are “thorough and concise,” which does not necessarily follow; correctness alone is an insufficient proxy for reasoning quality.\n4. The proposed approach is closer to an application of DPO-style, off-policy preference optimization than a novel RL innovation. It is essentially teaching the model to prefer accurate and more concise responses.\n5. This paper may be offering limited novelty and insight. The contributions appear incremental, applying existing RL/preference-learning techniques with modest conceptual advances; the paper offers few new insights into when or why the method should outperform simpler baselines.\n6. The method requires the reasoning chains of a larger model, whereas the baselines do not necessarily require them."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FJoOp99mQz", "forum": "aELfZbdE3J", "replyto": "aELfZbdE3J", "signatures": ["ICLR.cc/2026/Conference/Submission8290/Reviewer_6vc4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8290/Reviewer_6vc4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958811652, "cdate": 1761958811652, "tmdate": 1762920219940, "mdate": 1762920219940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dynamic Reasoning Quota Allocation (DRQA) to address the \"overthinking\" problem in reasoning models. The authors observed an interesting behavior: when RLLMs process multiple questions in batch mode, they are more resource-efficient and \"dynamically compress reasoning steps for easier problems.\" Based on this observation, the authors built a reinforcement learning training dataset by generating both verbose and concise answers. Experiments show that when trained on the proposed dataset, the model can improve efficiency and accuracy and has better generalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper targets \"overthinking,\" a significant and practical issue in RLLMs where models are computationally inefficient.\n- The observation of \"resource competition pressure\" is an interesting and novel motivation for the work.\n- The method consistently achieves a \"most favorable trade-off\": while other methods can produce even shorter outputs, they often \"suffer from severe accuracy degradation,\" a problem DRQA avoids."}, "weaknesses": {"value": "- The paper's core motivation is the \"resource competition pressure\" observed in batch inference, but this mechanism is not directly applied to the method. The authors' method just aims to mimic the results of this finding, not the process itself.\n- There is a lack of qualitative analysis of the \"overthinking\" phenomenon and how batch processing solves this by, for example, removing specific types of redundancy.\n- The methodological contribution is somewhat thin; the paper argues, but does not definitively prove, that its specific preference data scheme is the best approach.\n- The data source is only from DeepSeek-R1, so it needs more proof that the method works on other compression styles, not just on DeepSeek's."}, "questions": {"value": "- How does your DRQA (Batch-2) model learn to selectively apply conciseness to simple problems while avoiding the failure mode on complex problems (like AIME) seen in your Batch-5 ablation?\n- Have you considered using a dynamic token budget or a computational cost as a penalty in the RL objective, rather than using static, pre-generated batch outputs as preference data? This might more directly simulate the \"resource competition\" mechanism.\n- Could you provide qualitative examples comparing a \"Vanilla\" response, a \"Batch-2\" response, and a final \"DRQA\" response for the same question? Specifically, what types of reasoning steps (e.g., arithmetic checks, definitions, verbal fluff) are being removed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WvkpjDFR1i", "forum": "aELfZbdE3J", "replyto": "aELfZbdE3J", "signatures": ["ICLR.cc/2026/Conference/Submission8290/Reviewer_bYMF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8290/Reviewer_bYMF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969628976, "cdate": 1761969628976, "tmdate": 1762920219495, "mdate": 1762920219495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Dynamic Reasoning Quota Allocation (DRQA) to reduce overthinking in reasoning large language models. It is inspired by the observation that batch inference naturally shortens reasoning chains through implicit resource competition. DRQA transfers this behavior to single-question inference using reinforcement learning with preference-labeled data. The model learns to prefer concise and accurate reasoning chains over verbose ones via Group Relative Policy Optimization (GRPO). Across multiple reasoning benchmarks, DRQA cuts token usage by 25–35% while maintaining or improving accuracy. It outperforms baselines like DAST and GRPO+Length Penalty on both mathematical and code tasks. Overall, the paper presents a simple yet effective framework for adaptive and efficient reasoning in large models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies and formalizes a previously underexplored phenomenon in reasoning LLMs that batch inference implicitly encourages concise reasoning through token competition.\n- The proposed Dynamic Reasoning Quota Allocation (DRQA) effectively bridges the batch and single-question reasoning paradigms using reinforcement learning with preference data.\n- DRQA is evaluated across diverse and comprehensive reasoning benchmarks, multiple model sizes, and ablation settings."}, "weaknesses": {"value": "- While empirically demonstrated, the notion of “resource competition pressure” lacks a clear theoretical or mechanistic explanation. The current argument remains descriptive, leaving ambiguity about whether the effect stems from model inductive bias, context compression, or decoding heuristics.\n\n- Reward formulation is indirect and loosely tied to reasoning efficiency. DRQA trains a classifier to label reasoning chains as A/B/C (correct-concise vs. correct-verbose vs. incorrect) and uses GRPO to favor the correct label. However, the reward signal is categorical and not proportional to actual reasoning length or quality. Thus, it cannot finely distinguish between moderately concise and overly compressed reasoning, potentially leading to unstable learning or suboptimal trade-offs.\n\n- Limited justification for using GRPO instead of more targeted RL objectives. Why did this paper choose GRPO instead of other RL losses?\n\n- DRQA’s claim of “adaptive allocation” would be stronger if the authors stratified test questions by difficulty (e.g., easy vs. hard GSM8K) and showed differing average reasoning lengths. Without this, the adaptive claim remains qualitative.\n\n- All training data stem from DeepSeek-R1 outputs, which might bake in its reasoning biases. The paper does not explore whether DRQA generalizes when the teacher is of a different architecture (e.g., Qwen2.5)."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HwW3OOBZyl", "forum": "aELfZbdE3J", "replyto": "aELfZbdE3J", "signatures": ["ICLR.cc/2026/Conference/Submission8290/Reviewer_Ua16"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8290/Reviewer_Ua16"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989038490, "cdate": 1761989038490, "tmdate": 1762920219081, "mdate": 1762920219081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DRQA, a method that leverages the resource competition phenomenon when asking multiple questions together to generate training data, which is then trained with GRPO to make LRMs more efficient.\n\nThis reviewer happened to review a prior version of this work at another conference, which the authors have then withdrawn without rebuttal. Many portion of my review are copied with notes on how the authors address them in the updated version."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Good writing.\n* Good baseline coverage on efficient reasoning methods.\n* Good coverage on reasoning tasks.\n* The idea of adopting prompt batching to compress reasoning traces seems interesting."}, "weaknesses": {"value": "* Lack of novelty and insufficient connection to existing literature. The idea of putting multiple questions into the same prompt has already been well explored in existing literature [1, 2, 3] and likely more, but none are cited or discussed. Since DRQA is largely a combination of this prompt batching idea and GRPO, the paper oversells its novelty. Its relation to existing methods deserves much better coverage.\n    * While the updated version does cite some of these works, they are still not properly discussed in the Related Work section but only (mainly?) briefly mentioned in the introduction. Given the close connection between the proposed work and these existing studies, this treatment feels overly hand-wavy and borderline disrespectful—especially considering that the authors added these citations after reading this reviewer’s previous feedback. This suggests the lack of discussion was not due to oversight, but rather a deliberate choice.\n\n\n* Limited experiment coverage on prompt compression data. DRQA is motivated by the \"resource competition\" phenomenon, but the end goal still seems to be producing answers of varying conciseness for the same question, as resource for the later fine-tuning. In this setting, a broader comparison against established prompt compression techniques is necessary, but almost none are included.\n   * Seems to remain unaddressed as no prompt compression-based training baseline is provided.\n\n* Insufficient model coverage. Only two small Deepseek-R1-Distilled models are evaluated. Even within this narrow scope, the gains over the 2 GRPO baselines at 7B are relatively marginal.\n    * Seems only a DeepSeek-R1-Distilled-Llama-7B is added, so still limited.\n\n* Narrow dataset coverage outside the math domain. Since the model is trained on large amounts of DeepScaleR data (which includes problem–answer pairs from AIME, AMC, etc.), most evaluation remains heavily in-domain. Broader coverage on out-of-domain tasks—such as coding, planning, and others—is needed.\n    * Mostly resolved with the inclusion of LiveCodeBench and GPQA-Diamond. One might nitpick that LiveCodeBench results do not feature many baselines.\n\nAgain, this reviewer generally like this method, if the author can show that generating training data with prompt batching can lead to higher quality (for instance, better than training on compressed CoTs) for most LRMs (beyond DS distill and on bigger models), this reviewer is open to bumping the scores pending on proper discussion of related work and other presentation fixes.\n\n[1] Batch Prompting: Efficient Inference with Large Language Model APIs   \n[2] BatchPrompt: Accomplish More with Less  \n[3] Auto-Demo Prompting: Leveraging Generated Outputs as Demonstrations for Enhanced Batch Prompting"}, "questions": {"value": "Same as my previous questions/comments:\n\n* How are baselines like AdaptThink and AutoL2S done? These methods are training non-reasoning models to be efficient LRMs, where DRQA is compressing LRMs.\n* The real baseline under this paper is not Vanilla, but GRPO, as the DRQA is trained with additional data. This should be highlighted and featured in places like F4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gfq7DQ9nG0", "forum": "aELfZbdE3J", "replyto": "aELfZbdE3J", "signatures": ["ICLR.cc/2026/Conference/Submission8290/Reviewer_XmWr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8290/Reviewer_XmWr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999789729, "cdate": 1761999789729, "tmdate": 1762920218594, "mdate": 1762920218594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}