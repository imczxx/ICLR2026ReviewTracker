{"id": "HRkrWi3FWP", "number": 13810, "cdate": 1758223022467, "mdate": 1763624505794, "content": {"title": "Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose Multimodal Alignment Blind Spots", "abstract": "We **color-coded** the added changes to the **paper** and **Appendix** for the comfort of our reviewers.\n\nMultimodal Language Models (MMLMs) typically undergo post-training alignment to prevent harmful content generation. However, these alignment stages focus primarily on the *assistant* role, leaving the *user* role unaligned, and sticking to a fixed input prompt structure of special tokens, making the model vulnerable when inputs deviate from these expectations. We introduce Role-Modality Attacks (RMA), a novel class of adversarial attacks that exploit role confusion between the *user* and *assistant* and alter the position of the *image* token to elicit harmful outputs. Unlike existing attacks that modify query content, RMAs manipulate the input structure without altering the query itself. We systematically evaluate these attacks across multiple Vision Language Models (VLMs) on eight distinct settings, showing that they can be composed to create stronger adversarial prompts, as also evidenced by their increased projection in the negative refusal direction in the residual stream, a property observed in prior successful attacks. Finally, for mitigation, we propose an adversarial training approach that makes the model robust against input prompt perturbations. By training the model on a range of harmful and benign prompts all perturbed with different RMA settings, the model loses its sensitivity to Role Confusion and Modality Manipulation attacks and is trained to only pay attention to the query content in the input prompt structure, effectively reducing Attack Success Rate (ASR) while preserving the model’s general utility.", "tldr": "We introduce Role-Modality Attacks (RMA), where uneven role alignment (user-assistant) and image token positioning bypass refusal. We analyze them via interpretability in activation space and propose an adversarial training approach for mitigation.", "keywords": ["Safety and Robustness", "Safety Alignment", "Interpretability", "Refusal", "out-of-distribution", "Representation Engineering", "Jailbreaking", "Multimodal Alignment", "Concept Vectors"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c4f0b6329cad9a0ae02cb2767e7b365ff6f3879.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Role-Modality Attacks (RMA) — a new class of adversarial attacks that exploit structural vulnerabilities in multimodal language models (MMLMs). Specifically, it manipulates the role tokens (swapping user/assistant) and modality token positions (moving the image token around) in standard chat templates. Experiments are conducted using 3 MLLMs (qwen2-vl, llava-1.5, and phi-3.5-vision) on two benchmarks, Advbench and HarmBench, showing obvious gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly rewritten and easy to follow."}, "weaknesses": {"value": "1. **Should RMA be treated as Adversarial Attack:** I'm somewhat concerned about whether the proposed RMA is proper to be treated as 'adversarial attack'. The proposed RMA mainly involves reformatting the prompt template (e.g., swapping role tokens or moving the image token around). As LLMs are known to be sensitive to prompts (especially prompt templates), the resulting performance drop of applying RMA is quite expected. I think it may be more appropriate to treat it as the LLMs' sensitivity/robustness to prompts instead of actual \"adversarial attack\", since altering the prompt template will degrade model performance on all tasks, which is not specific to adversarial attack on jailbreaking.\n2. **Practical Usefulness:** I'm also concerned about the practical usefulness of RMA. In reality users will not be able to modify the chat template when using APIs.\n3. **Similar prior works & missing reference:** There is a previous workshop paper that also studies how the chat template can help jailbreak an aligned MLLM [1]. The objective in [1] seems to be pretty similar to this paper, yet there is no reference to [1]. \n4. **Experiment on more recent MLLMs:** I think it might be worthwhile to experiment RMA on newer MLLMs such as qwen2.5/qwen3, gemma 3, internvl2.5/3/3.5 etc.. These models are RL-finetuned, which may have different behaviors from non-RLed models tested in the paper. (e.g. in Table1, Qwen2-VL already demonstrates better robustness against RMA, compared to earlier models like LLava-1.5 and phi3.5-v). \n\n[1] ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates. NeurIPS 2024 Workshop SafeGenAI"}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yuiTkwm6YN", "forum": "HRkrWi3FWP", "replyto": "HRkrWi3FWP", "signatures": ["ICLR.cc/2026/Conference/Submission13810/Reviewer_gSyD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13810/Reviewer_gSyD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537125805, "cdate": 1761537125805, "tmdate": 1762924340104, "mdate": 1762924340104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of our revisions and answering common questions (Part1)"}, "comment": {"value": "## Note: This response has been divided into two comments.\n\n**We thank the reviewers for their excellent feedback and suggestions.**\n\n**Contribution of our paper:** We introduce Role–Modality Attacks (RMAs), a new class of attacks that reveal overlooked alignment blind spots in Multimodal language models (MMLMs). Through RMAs, we show that minor structural perturbations, such as swapping user/assistant tokens (exposing a deeper user–assistant alignment asymmetry as noted by R3) or image-token repositioning, disrupt the alignment without changing the query content itself.\nWe provide causal insights through representation-space analyses showing that RMAs shift representations away from refusal directions and can amplify content-based jailbreaks.\nFinally, we propose an adversarial training method that teaches models to ignore structural perturbations and ground refusals in query content, substantially lowering ASR while preserving utility. Overall, our findings highlight two fundamental multimodal alignment blind spots: Alignment brittleness to structural perturbations and a persistent user–assistant role asymmetry, underscoring the need to rethink current alignment practices.\n\n**Overview of Revision:**\n*   We **color-coded** the added changes to the **PDF** for the comfort of our reviewers.\n*   Added Further Experimental Results Across More Models, Different Sizes, and Additional Datasets (Appendix H) - Tables 9, 10 ,11, 12; and revised Section 3 to refer to them.  \n*   Revised the Introduction to clarify some questions and added the downstream implications of RMAs (synthetic conversation generation pollution and training data extraction risks).\n*   Revised Appendix B (Compositionality with Other Attacks), Table 3, and Table 4 for better presentation of the results, and updated Section 4 to reference them. \n*   Added experiments and additional discussion examining the similarity of the distribution-shift vectors induced by different images, and the root cause of token position sensitivity to Appendix C, and revised Section 3 to reference them.\n*   Added a new downstream implication on “Training Data Extraction Risks” through user role exploitation and updated Appendix D (Downstream Implications of the Unaligned User Role).\n*   Updated Appendix E with a clearer discussion of system-level vs. model-level defenses, and added a footnote in the Conclusion referencing this clarification.\n*   Added new analyses and results on defense generalization to unseen structural perturbations in Appendix F, and updated Section 4 (Table 2) to reference them.\n*   Revised the Related Work section for better presentation; Revised Limitations and Ethics statement for better clarity on the motivation and the implications. \n*   Added Appendix I (Conceptual Framing of Role–Modality Attacks); and made various other changes to the PDF to clarify questions and comments in the reviews.\n\n## In the next comment, we will be answering Common Questions."}}, "id": "2pPT2GHbne", "forum": "HRkrWi3FWP", "replyto": "HRkrWi3FWP", "signatures": ["ICLR.cc/2026/Conference/Submission13810/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13810/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13810/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763621192603, "cdate": 1763621192603, "tmdate": 1763623704640, "mdate": 1763623704640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies an often-overlooked vulnerability in MMLMs, their reliance on fixed token structures and aligned roles. The authors introduce Role-Modality Attacks (RMA), a novel class of structural perturbations that exploit the asymmetry in alignment and the sensitivity of MMLMs to the change in structure and ordering of text/image tokens within the input sequence."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The work identifies a simple yet effective attack vector by focusing on the structural integrity of the prompt rather than just semantic or low-level input manipulation.\n\n*The explicit focus on the \"user role being unaligned\" is a sharp, correct critique of current safety practices. It demonstrates that alignment strategies, which largely focus on constraining the output, can be easily bypassed by injecting unaligned behavior into the input.\n\n*The approach was evaluated across different state-of-the-art models. Showing that is a common blind-spot in MMLMs.\n\n*The authors show that finetuning on samples generated using their method clearly have a positive effect in reducing ASR."}, "weaknesses": {"value": "*Given the simplicity of the approach a defense based on input token position preprocessing seems plausible and should be discussed or evaluated as a baseline defense. Could prompt preprocessing mitigate the proposed attacks?\n\n*This kind of attack might not be a real concern for current commercial LLMs such as (Gemini/ChatGPT). It would be interesting to see the effect if any on this kind of models."}, "questions": {"value": "*The paper's core finding is that changing the position of the tokens drastically alters model behavior. Do the authors have further insights or a hypothesis on the architectural root cause of this sensitivity?\n\n*Could a simple and computationally cheaper defense, such as input prompt pre-processing or normalization, effectively mitigate these attacks, potentially serving as a more practical alternative to the proposed adversarial training?\n\n*For a black-box LLM accessible only via a fixed API where the internal role/instruction tokens cannot be explicitly manipulated by the user, would RCA remain effective? Since it relies on modifying those structural tokens."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NUvwqbf3PJ", "forum": "HRkrWi3FWP", "replyto": "HRkrWi3FWP", "signatures": ["ICLR.cc/2026/Conference/Submission13810/Reviewer_iN6X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13810/Reviewer_iN6X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604971798, "cdate": 1761604971798, "tmdate": 1762924339405, "mdate": 1762924339405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies safety alignment vulnerabilities in VLMs. The core claim is: current post-training alignment mostly teaches the assistant role to refuse harmful requests under a fixed chat template, but (i) the user role is often left essentially unaligned, and (ii) models are brittle to even small structural perturbations in the input format (like where the image token appears)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors evaluate their approach on multiple benchmarks and conduct extensive ablation studies.\n\n2. The experimental setup is described in a detailed and thorough manner."}, "weaknesses": {"value": "1.  Limited practicality of the threat model. The paper's primary weakness lies in its threat model, which assumes the attacker has a level of control that is unavailable to most end-users in real-world deployments. \n\n2. Model coverage / scalability. A broader sweep across multiple model sizes / generations (e.g., Qwen3-VL families at 2B/4B/8B/30B  checkpoints) is needed to show whether the vulnerability and the defense both scale, rather than being specific to the tested models.\n\n3. Defense may overfit the attack. The proposed fine-tuning trains on the same eight structural perturbations it defends against. It is unclear whether the model actually learned to refuse harmful requests in general, or just memorized those exact patterns. We don’t really see robustness-to-unseen-perturbations experiments, which matters for claims of principled safety.\n\n\n4. Limited analysis of the visual signal. The paper notes that swapping in a harmless “flower” image yields similar ASR to using a “firearm” image, implying the actual visual content barely matters. This is interesting but under-analyzed. Does the model stop using vision once the image token is moved, effectively behaving like a text-only model?"}, "questions": {"value": "Please refer to the questions raised in the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TMKX5mifM9", "forum": "HRkrWi3FWP", "replyto": "HRkrWi3FWP", "signatures": ["ICLR.cc/2026/Conference/Submission13810/Reviewer_hWBM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13810/Reviewer_hWBM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748992447, "cdate": 1761748992447, "tmdate": 1762924338894, "mdate": 1762924338894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Multimodal large models are at risk of jailbreak Attacks. This paper focuses on the vulnerability of large models in the face of Role-Modality attacks. RMA induces the model to generate unsafe content by manipulating dialogue roles (such as swapping the roles of users and assistants) and inputs (such as adding harmful images). This paper verifies the effectiveness of RMA on multiple mainstream VLMS and demonstrates the combined enhanced effects on various classic jailbreak methods. To mitigate such attacks, this paper proposes an adversarial training method, which enhances the defense capability against RMA attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This paper has a clear motivation. Firstly, it verifies the effectiveness of RMA attacks, and then further analyzes the internal mechanism of the success of RMA attacks. Based on this, it proposes an adversarial training method based on role and modal perturbation, which is logically complete.\n\n2.This paper has a well-designed experiment. It systematically evaluates the effect of RMA on mainstream VLMS under eight experimental Settings in terms of roles and modalities, as well as the effect of mainstream VLMS after Adversarial Training. The experimental results are fully explained.\n\n3.This article clearly demonstrates the effectiveness of adversarial training through model output samples. The trained model is more focused on the security of the content itself. Clear prompts are provided for character control, image examples for modal control, and images for adversarial training."}, "weaknesses": {"value": "1.This paper shows that under the eight RMA experimental Settings of this paper, the trained adversarial models have been significantly improved. However, this adversarial training design method seems to be only applicable to RMA attack methods. Does it also have the generalization ability against other jailbreak attack methods? Furthermore, this paper presents the combined effect of RMA and other mainstream jailbreak attack methods through Table 11. Could the success rates of individual jailbreak attack experiments be provided for comparison (the comparison includes whether there is an improvement with RMA itself and in combination with RMA)?\n\n2.The dataset used in the experiments of this paper, the tested models and the evaluation methods have limited breadth. Comparisons can be appropriately increased."}, "questions": {"value": "1.Supplement the effect of adversarial training on other jailbreak attack methods, as well as the success rate of jailbreak itself and the success rate of combining with RMA.\n\n2.Provide more experimental comparison results of data and models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j02xcjlum4", "forum": "HRkrWi3FWP", "replyto": "HRkrWi3FWP", "signatures": ["ICLR.cc/2026/Conference/Submission13810/Reviewer_98aX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13810/Reviewer_98aX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791829372, "cdate": 1761791829372, "tmdate": 1762924338338, "mdate": 1762924338338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}