{"id": "h2p5eOFpcF", "number": 21967, "cdate": 1758324198904, "mdate": 1759896893422, "content": {"title": "Medical thinking with multiple images", "abstract": "Large language models and vision-language models score high on many medical QA benchmarks; however, real-world clinical reasoning remains challenging because cases often involve multiple images and require cross-view fusion. We present MedThinkVQA, a benchmark that asks models to think with multiple images: read each image, merge evidence across views, and pick a diagnosis with stepwise supervision. We make three parts explicit: multi-image questions, expert-annotated stepwise supervision, and beyond-accuracy evaluation. Only MedThinkVQA combines all these parts in one expert-annotated benchmark. The dataset has 8,481 cases in total, with 751 test cases, and on average 6.51 images per case; it is expert-annotated and, at this level, larger and more image-dense than prior work (earlier maxima < 1.43 images per case). On the test set, GPT-5 achieves 57.39% accuracy, approximately 15 percentage points below the strongest result on the most challenging prior benchmark of a similar kind, while other strong models are lower (Qwen2.5-VL-32B: 39.54%, MedGemma-27B: 37.55%, InternVL3.5-38B: 43.14%). Giving expert findings and summaries brings clear gains, but using models' self-generated ones brings small or negative gains. Step-level evaluation shows where models stumble: errors center on image reading and cross-view integration in both decisive and non-decisive steps (>70%); when a step is decisive for the final choice, reasoning slips become more common (32.26%), while scenario and pure-knowledge slips are relatively rare (<10%). These patterns isolate and quantify the core obstacle: extracting and integrating cross-image evidence, rather than language-only inference.", "tldr": "", "keywords": ["Multimodal diagnostic reasoning", "Vision language models (VLMs)", "Medical VQA", "Thinking with images"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e15b548b8c03f246f41ce3e4223f7687cc3a037.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an expert-annotated benchmark that probes the capabilities of multimodal LMMs on multi-image cross-view medical reasoning. Moreover, a step-wise evaluation protocol is proposed that can help pinpoint where models fail in their reasoning. State-of-the-art models are evaluated on the benchmark, and their mistakes are categorized into different error groups, highlighting the tendency for these models to fail on medical image understanding."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation of the paper is clear: it is important to assess multi-view capabilities of current models as it is closer to realistic diagnostic scenarios.\n- The step-wise evaluation and breakdown into error categories is valuable. It helps us better understand where these models fail in complex medical reasoning tasks. \n- The study on the utility of adding expert hints vs. self-generated captions is interesting and highlights the benefits of human-in-the-loop diagnostic workflows."}, "weaknesses": {"value": "- Overall, the paper is hard to follow at parts due to multiple factors. First, the structure is unusual. The introduction could be more high-level without much details about methodology. Details about methodology is dispersed between the introduction and various other sections. Second, the paper assumes significant background knowledge on the source dataset used to create the benchmark. Is the ground truth reasoning trace part of the source dataset? What are the different sections of the source data and what kind of information do they exactly contain (e.g. \"Integrated Imaging Summary\" or \"Image Hint\")?\n\n- The step-wise evaluation is a bit unclear to me. How exactly is the generated output broken down into these steps? Furthermore, a rigorous definition of the error categories would be helpful, e.g. what does \"Clinical Scenario Error\" entail?\n\n- How is it supported in the work that multiple images are *necessary* to answer the questions accurately? If the problems can be tackled by a single view, then the key claim of probing cross-view synthesis is not well-supported.\n\n- Human performance is missing in the benchmark, making it difficult to gauge the gap between SOTA models and human experts on cross-view medical reasoning.\n\n- It is unclear what is the practical use-case of generating the teaching note in this benchmark. It seems only loosely connected to cross-view medical reasoning."}, "questions": {"value": "- I would recommend restructuring the paper with clearer delineation between different sections. \n- How are steps defined in the step-wise evaluation and how the error categories are defined and determined?\n- How is the benchmark strictly probing multi-view reasoning? What happens if we remove some images? If the benchmark is truly probing cross-view capabilities, this would significantly degrade performance.\n- What is the expert performance on the benchmark?\n\nMinor: \n- Table number is missing on line 362."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8IdRg1Zxrs", "forum": "h2p5eOFpcF", "replyto": "h2p5eOFpcF", "signatures": ["ICLR.cc/2026/Conference/Submission21967/Reviewer_qXc8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21967/Reviewer_qXc8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761017327298, "cdate": 1761017327298, "tmdate": 1762942001675, "mdate": 1762942001675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MedThinkVQA, a benchmark built from Eurorad teaching cases, each pairing a clinical scenario with multi-image studies and expert reasoning artifacts. The task is framed as multi-choice diagnosis with a multi-step “think-with-images” pipeline: per-image findings, case-level integrated summary, differential-diagnosis reasoning, and a long-form medical education discussion generation. The evaluation goes beyond accuracy with stepwise checks, error-type tags, ROUGE/RadCliQ, and rubric-based/LLM-judge scoring of discussions, with human studies supporting reliability."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The problem formulation is well-motivated: prior medical VQA sets tend to be single-image, answer-centric, or automatically labeled; here the authors emphasize cross-view fusion and expert-authored intermediate signals that better mirror real diagnostic practice. \n\n2. Dataset quality and evaluation design are thoughtfully engineered. The paper details option sets derivation from expert differentials, textual leakage detection, pruning of items solvable by text-only LLMs, mitigation of surface biases, and wide coverage spanning 20/22 ICD-10 chapters. The evaluation is also staged and fine-grained.\n\n3. Engineering efforts in data curation and model evaluation bring concrete analysis and conclusions.\n\nGenerally, this is a good paper and I did not see major flaws."}, "weaknesses": {"value": "1. Dataset statistics are inconsistent. Both the abstract and Table 1 state an average of 6.51 images per case, yet Line 200 in Sec. 3.1 mentions 8.3. \n\n2. Some details about the dataset are missing. For example, how are multiple images in one case gathered? Are they longitudinal studies from the same patient, or complementary imaging modalities (X-ray, CT, MR, etc), or both? If both are involved, I suggest analyzing them separately since two scenarios assess different capabilities.\n\n3. The evaluation relies on commercial LLMs (GPT-5) as LLM-judge, which might entail model/version drift risk since neither the API nor any specific snapshot is guaranteed to be available forever. Given this, I am wondering whether open-source models (e.g. Qwen series) are able to serve this and how the evaluation results will vary (e.g., will the scores differ drastically, or will there be any bias?). \n\n\n4. Lack of discussion with related work. For example, Medical-Diff-VQA [1] , ICG-CXR [2], MedFrameQA [3] are not mentioned in Table 1 and the manuscript, although they explicitly feature in visual reasoning with multiple imaging studies from the same patient.\n\n[1] Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering (KDD 2023)\n\n[2] Towards Interpretable Counterfactual Generation via Multimodal Autoregression (MICCAI 2025)\n\n[3] MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning (arXiv 2025.05)\n\n---\n\nBelow are minor issues:\n- Each abbreviation should be expanded when it first appears for better readability. For example, “QA” (question-answering) in Line 010 and Line 035; “MCQ” (multiple-choice question) in Line 104.\n- In Lines 362--363: “Table shows representative model accuracy on the held-out test set.” Table index seems missing. \n- The prompts in the appendices (Secs. E, F, and G) are overflowing the right margin. Enabling automatic hyphenation or manually inserting hyphens may fix these issues.\n- I suggest copying the “without SFT” results in Fig. 3 to Tab. 8 which presents model performance after SFT, or combine these results in one single figure. In this way, the readers will see the value of the curated training examples more clearly.\n- I suggest adding a graph showing the imaging modality distribution, instead of plain description in Sec. I."}, "questions": {"value": "Do cases in MedThinkVQA include redundant images (e.g. imaging study that does not provide valid information, or sometimes even contradictory information)? This aligns more to the practice where clinical users would not always do an image pre-filtering for the MLLM assistant. If that is the case, will such data produce a robust MLLM when used for model training and test models to ignore noisy information when used for model evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NX1gLvkeEX", "forum": "h2p5eOFpcF", "replyto": "h2p5eOFpcF", "signatures": ["ICLR.cc/2026/Conference/Submission21967/Reviewer_prRm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21967/Reviewer_prRm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761213228790, "cdate": 1761213228790, "tmdate": 1762942001141, "mdate": 1762942001141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MedThinkVQA, a benchmark for evaluating multi-image diagnostic reasoning in medical imaging. The benchmark features 8,481 cases (751 test) averaging 6.51 images per case, sourced from Eurorad and expert-annotated with three-step supervision: (1) per-image findings, (2) case-level imaging summaries, and (3) differential diagnosis reasoning. This paper evaluates various VLMs (GPT-5, Qwen2.5-VL, MedGemma, InternVL) and find that current models struggle significantly (GPT-5: 57.39% accuracy), with the primary bottleneck being cross-image evidence extraction and integration rather than language reasoning. The benchmark includes beyond-accuracy evaluation with error-type tagging and medical education case discussion generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents the largest expert-annotated multi-image medical QA benchmark.\n2. This paper presents the well-designed three-step evaluation framework that mirrors clinical diagnostic workflow.\n3. This benchmark performs rigorous dataset curation with multiple quality control measures, such as, leakage detection, confusion-aware pruning."}, "weaknesses": {"value": "Weakness:\n1. Dataset relies entirely on Eurorad cases, potentially limiting generalizability despite broad coverage of radiology subspecialties.\n2. The benchmark data sourced from Eurorad may have been included in the pre-training corpora of some evaluated models.\n3. The case analysis is not presented, such as, failed case analysis."}, "questions": {"value": "1.  How do you ensure that Eurorad cases haven't been seen during pre-training of evaluated models?\n2. Only the train set (small scale dataset) is used to train the vlm? the detailed train (sft) strategy.\n3. Why not compared with the expert radiologist?\n4. The present question from benchmark is verified by human expert for quality control? If not, why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "shyjpvNXQk", "forum": "h2p5eOFpcF", "replyto": "h2p5eOFpcF", "signatures": ["ICLR.cc/2026/Conference/Submission21967/Reviewer_zQ3B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21967/Reviewer_zQ3B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929001045, "cdate": 1761929001045, "tmdate": 1762942000779, "mdate": 1762942000779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MedThinkVQA, a large-scale benchmark explicitly designed to evaluate multimodal large language models (MLLMs) in multi-image diagnostic reasoning. Built from 8,481 expert-curated teaching cases (average 6.5 images per case), it formalizes a three-step workflow that mirrors clinical reasoning: (1) per-image findings, (2) cross-view imaging summary, and (3) differential-diagnosis reasoning, followed by a medical-education discussion task. \nEvaluation goes beyond simple accuracy by introducing step-level correctness, error-type tagging, and educational-value scoring. Baseline experiments on diverse MLLMs, show that even the best model reaches only 57.4 % accuracy, revealing that cross-image integration is the major bottleneck for current medical VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a high-quality benchmark paper with clear motivation and substantial novelty. It moves beyond single-image VQA toward multi-image, step-supervised diagnostic reasoning, something not covered by prior datasets such as OmniMedVQA or MedXpertQA. The design is conceptually elegant mirroring how clinicians think and technically meticulous, from option-wise pruning and leakage checks to fine-grained error analysis and human-LLM judge validation (κ≈0.8). \nThe beyond-accuracy evaluation framework, including RadCliQ metrics and teaching-discussion scoring, sets a new standard for interpretable benchmarking in medical AI. The authors also demonstrate strong ethical and reproducibility practices, releasing code, annotation scripts, and bias audits. Overall, the work is novel, comprehensive, and clinically grounded, offering a meaningful step toward trustworthy multimodal reasoning in medicine."}, "weaknesses": {"value": "While the dataset is impressively detailed, it is built entirely from Eurorad cases, which may bias the distribution toward educational rather than real clinical imaging; cross-institutional validation or inclusion of temporal cases would further strengthen robustness. \nThe current evaluation focuses on four families of models and could benefit from broader comparisons to recent foundation-level MLLMs such as Gemini 2.5 Pro or Claude 3.5 Sonnet to fully situate difficulty. The study identifies the image-fusion bottleneck clearly but offers relatively limited prescriptive insight—there is little discussion of architectural directions or training strategies that could overcome this barrier."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sHegsnHKN0", "forum": "h2p5eOFpcF", "replyto": "h2p5eOFpcF", "signatures": ["ICLR.cc/2026/Conference/Submission21967/Reviewer_8tFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21967/Reviewer_8tFF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986334432, "cdate": 1761986334432, "tmdate": 1762942000484, "mdate": 1762942000484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}