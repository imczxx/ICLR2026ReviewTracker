{"id": "Q5o249Z3Je", "number": 14662, "cdate": 1758241138207, "mdate": 1759897356525, "content": {"title": "Forecasting with LLMs: A Dataset for Rapid Backtesting Without Temporal Contamination", "abstract": "The rise of large language models (LLMs) has made scalable forecasting increasingly feasible, as these models have access to massive amounts of context. Yet evaluating their forecasting ability presents three methodological challenges. Standard benchmarks are vulnerable to temporal contamination, where outcomes are already known before the model’s training cutoff, and to staleness confounds, where newer models gain unfair advantage from fresher data. Dynamic benchmarks address temporal leakage by tracking unresolved questions, but this results in long evaluation delays, since evaluators must wait for outcomes to resolve before judging the accuracy. We address these issues with a forward-only, backtestable evaluation framework built on frozen context snapshots: contemporaneous, structured summaries of web search results paired with forecasting questions. Our pipeline continuously scrapes unresolved questions from prediction markets and captures their supporting context at the time of scraping, eliminating temporal contamination and mitigating staleness effects. Once questions resolve, these snapshots enable rapid backtesting of diverse forecasting strategies, substantially accelerating research cycles. This framework provides a rigorous, reproducible, and open-source foundation for studying the forecasting capabilities of LLMs. Through two experiments, we demonstrate that our approach enables the rapid identification of effective forecasting strategies. The dataset and code are available at https://anonymousurl.org.", "tldr": "", "keywords": ["forecasting", "large language models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8d34a49dcab73f90705c532cef0129464447e97.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a replayable, leakage-resistant LLM forecasting benchmark that pairs real yes/no prediction-market questions with frozen, time-stamped web summaries captured before resolution. It shows snapshots (and small ensembles) improve Brier accuracy; event-level prompting helps GPT-4o but not GPT-5; markets remain a strong baseline."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* Freezing dated web evidence to replay forecasts later is a simple idea that cleanly separates model reasoning from training-data freshness and search, yielding an immediately useful benchmark across domains.\nThe setup is careful and rigorous: clear timestamps, transparent snapshot pipelines, explicit prompts, Brier scoring, and a strong market-price baseline; results are sensible (snapshots help, small ensemble gains).\n* The benchmark upon release will have practical benefit to the community."}, "weaknesses": {"value": "* The paper doesn’t isolate “reasoning” from plain prediction: we never see whether deliberate modes (e.g., CoT, Thinking models) actually convert the same snapshots into better forecasts than a straightforward probability output.\n* There’s no apples-to-apples comparison with tool-using systems. The benchmark forbids browsing, but many state-of-the-art forecasters rely on tools/search; we don’t see the same model with tools vs the same model restricted to frozen notes.\n* The task space is binary only but many real forecasts are multi-way or continuous, so external validity is limited.\n* Some experimental flaw: 1) event-level prompts may help simply because they’re longer. The study doesn’t control for token budget, so framing improvements are confounded with “more text.” 2) Frozen notes can contain quoted odds or implied probabilities. Models might piggyback on crowd signals rather than reason from evidence, and there’s no redaction/ablation to rule this out.\n* Comparability will drift over time: to remain forward-only, newer models require a newer evaluation window (answer might be contained in training data), which forces re-running baselines on that window and prevents a single, consistent, time-stable leaderboard.\n* The deeper analysis on the benchmark evaluations is lacking: 1) baselines are narrow that results are centered on a small set of closed models, with no strong open-model or classical forecasting baselines to gauge generality. 2) domain and time-horizon analysis is shallow that results aren’t systematically broken down by question type, difficulty, or days-to-resolution, so we can’t see where the method really helps or fails. \n* Text in figures are very small (espeically figure 3), and zooming in makes it blurry."}, "questions": {"value": "* The code link is provided but inaccessible.\n* Can you share 2–3 real benchmark examples (like A.3) paired with actual model outputs?\n* Why not just limit Google Search to a cutoff date instead of using frozen snapshots?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xSVdZHWTAL", "forum": "Q5o249Z3Je", "replyto": "Q5o249Z3Je", "signatures": ["ICLR.cc/2026/Conference/Submission14662/Reviewer_PYP1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14662/Reviewer_PYP1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810066292, "cdate": 1761810066292, "tmdate": 1762925034113, "mdate": 1762925034113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a backtestable forecasting strategy using frozen context snapshots. These are timed summaries of web search results paired with prediction market questions. Their goal is to avoid any temporal contamination and reduce any stale confounding factors. And the pipeline itself is a two-stage pipeline. The first step being the search integrated LLM and then there is a custom RAG system. The experiments show that the snapshots improve prior scores, event level prompting beats market level prompting for GPT-4o but not for GPT-5, and that the ensemble snapshots yield small domain-dependent gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The forward-only snapshots directly tackle leakage and reduce staleness, which is a quite clear evaluation target. The dataset contains over 9000 questions with over 3000 results. There is sufficient context captured after model training cutoffs which enables back testing without contamination.\n\n2. There are two complementary retrieval pipelines with daily timestamped runs.\n\n3. The Breyer metric and the mixed effect regression and the event versus market prompting and ensemble of applications are quite concrete and interpretable.\n\n4. There is a practical release plan with legal aware summarization instead of raw content as well."}, "weaknesses": {"value": "1. The core idea of the research and freezing context is quite straightforward. It's more of a benchmark paper, With the retrieval methods as quite standard, which is Bing search and basic RAG. \n\n2. The context ensembling methods show minimal gains and the prompt granularity findings don't really generalize and there is no compelling evidence that the approach is significantly improving forecasting.\n\n3. There is a missing analysis on which types of questions which benefit from context. Summarization may lose critical details, especially if done using LLMs. \n\n4. The post hop filtering of unrelated summaries using GPT-5 mini seems a bit circular. There is no validation of information source, quality or relevance.\n\n5. All the questions being from Kalshi limit the source to a single source which limits generalizability. The filtering criteria in the appendix seems arbitrary and lacks justification. And the temporal gap between the 2025 July snapshot and the 2024 September model cutoff still leaves some room for contamination through training on related historical patterns."}, "questions": {"value": "1. How do you validate that the snapshots contain decision-relevant information?\n\n2. What is the maintenance plan as the data set ages, if any at all.\n\n3. How does the performance vary with time to resolution and can you please provide evidence that summarization will preserve critical information and what is your plan for that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iFAoZkmVIt", "forum": "Q5o249Z3Je", "replyto": "Q5o249Z3Je", "signatures": ["ICLR.cc/2026/Conference/Submission14662/Reviewer_Tn4L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14662/Reviewer_Tn4L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933773060, "cdate": 1761933773060, "tmdate": 1762925033739, "mdate": 1762925033739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a pipeline for building LLM forecast benchmarks. It continuously scrapes unresolved questions from the prediction market Kalshi. The framework prevents temporal contamination by capturing supporting context at the time of question scraping using two methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed framework effectively addresses temporal contamination and evaluation-delay issues.\n- The benchmark can be valuable to the community if it is kept up to date."}, "weaknesses": {"value": "- This is primarily a benchmark paper. That being said, it lacks substantial methodological or theoretical novelty. It is fine as a benchmark contribution, but I think it fits better at benchmark-focused venues, such as the NeurIPS DB track.\n- It would be better to evaluate more models and present their results on the proposed benchmark to provide more insights. Currently, the paper only covers GPT-4o and GPT-5, so it’s unclear how well the approach generalises to other proprietary and open-sourced models.\n- The link to the dataset in the abstract doesn't work on my side."}, "questions": {"value": "See those in the Weaknesses part."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "I'm not sure whether scraping questions from Kalshi and related snapshots on the Internet would violate copyright. The authors provide summaries of the snapshots, but I'm not completely sure this eliminates all copyright concerns."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BcAriQK2Sk", "forum": "Q5o249Z3Je", "replyto": "Q5o249Z3Je", "signatures": ["ICLR.cc/2026/Conference/Submission14662/Reviewer_Zrzm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14662/Reviewer_Zrzm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031572616, "cdate": 1762031572616, "tmdate": 1762925033142, "mdate": 1762925033142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes alleviating concerns of leakage in forecasting backtests from context retrieval by releasing a fixed dataset of relevant snapshots to Kalshi forecasting questions. It collects these snapshots in two ways: 1) gpt-4o with bing search, 2) gpt-4o-mini generated summaries of DDGS search results to gpt-4o-mini generated queries. The retrieved articles lead to an improvement in brier score compared to no retrieval."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using fixed snapshots for retrieving information for forecasting backtests is important and useful.\n\n2. Providing a dataset with past retrieved articles makes it one step less complex to do research on for eg model training for forecasting. \n\n3. The questions are scraped from Kalshi, and their relevant context is retrieved automatically. Since the whole pipeline is automated, it is easy to update the data.\n\n4. The experiment on context ensembling was interesting, though it is not fleshed out.\n\n5. The appendix includes interesting details and examples."}, "weaknesses": {"value": "1. The main weakness of the proposed dataset is that it loses information. Specifically, only a small, highly processed summary of the \"snapshot\" is provided as context. The summary provided is created by definition by a weaker, old model, in this case gpt 4o / mini. No clear quality comparisons are provided with alternatives to show how good the retrieved articles are. Due to this limitation, I would personally not use this dataset for forecasting research. \n\n2. The methodology (providing a fixed set of past retrieved articles) is only helpful when the prediction is treated in isolation assuming a fixed input. However, seeking relevant information is an important step for forecasting, where more intelligence and reasoning help. This is also inherently a model capability, which one might train for during forecasting training. I would significantly rewrite 3.2 in light of this.\n\n3. Directly providing access to source articles as done by commoncrawl or wayback, is far more flexible. I find the limitations of these alternatives mentioned in Appendix A.2 unconvincing. While commoncrawl loses some information as websites like CNN do not license scraping, the proposed method is likely to lose even more information. In fact, it is unclear how this method can provide such news articles legally. Similarly, while wayback is slow, I still believe a lot more context can be gathered from wayback than provided by this fixed pipeline."}, "questions": {"value": "The anonymous url link is broken. Bringing it to your notice if this was unintentional :)\n\nThe critique of Wilderman et al (2025) on L163 is unclear to me. Why is it noisy or undesirable to use archives of google search?\n\nIn L173 the claim is made that context snapshots mitigate the issue of differing training cutoffs. While I generally agree with the claim, it would be good to see more evidence on how much context can overcome models with older cutoff dates. \n\nI found the terms \"event-level\" and \"market-level\" a bit unintuitive to distinguish. I get the implied intent after reading 5.2.2, but this comes too late. Perhaps consider more intuitive terminology."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The paper claims that if web content is prohibited for redistribution, using the gpt generated summaries released in their dataset mitigates this issue. I am skeptical of this claim, and believe the proposal could still be illegal or unethical. I am not a legal expert, so it would be good to get this checked."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ar61akMabd", "forum": "Q5o249Z3Je", "replyto": "Q5o249Z3Je", "signatures": ["ICLR.cc/2026/Conference/Submission14662/Reviewer_7ykK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14662/Reviewer_7ykK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037080589, "cdate": 1762037080589, "tmdate": 1762925032512, "mdate": 1762925032512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}