{"id": "h2pRcPdz2P", "number": 595, "cdate": 1756751671573, "mdate": 1759898251745, "content": {"title": "Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time", "abstract": "Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient—leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST—a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning. Code will be made public upon acceptance.", "tldr": "", "keywords": ["Reasoning; Large Language Model; Activation Engineering"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec07d83af3732c09f97d33504d5c246339258f49.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates how large reasoning models exhibit and internalize cognitive-like behaviors such as verification and backtracking.\nThe new approach is identify a subset of “cognitive attention heads” whose activations correlate with these behaviors and propose CREST, a training-free, test-time steering method that modifies head-level activations to suppress or enhance such behaviors. Empirically, CREST improves accuracy and reduces token usage across multiple reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The proposed method is effective. It achieves consistent accuracy gains (up to +17.5%) while reducing generation length (up to −37.6%) on several reasoning benchmarks and models.\n\nS2. The discovery of “cognitive heads,” the probing analysis, and activation visualizations provide interpretable insights into internal reasoning mechanisms.\n\nS3. CREST requires no retraining or gradient updates, can be applied across many LLMs, and introduces negligible inference overhead."}, "weaknesses": {"value": "**W1**. The update $\\hat{x_{i,j}}=x_{i,j}-\\alpha v_{i,j}$ and its norm-preserving variant lack theoretical justification. It remains unclear why subtracting or orthogonalizing along an empirical mean direction improves reasoning, or how this compares to simpler scaling methods. In addition, the novelty of this intervention is unclear: please position it against existing activation/attention-head steering techniques (e.g., activation engineering / latent-direction editing, manifold steering for overthinking mitigation, and SEAL’s task-arithmetic latent calibration), and clarify whether a head-level, per-step, norm-preserving projection is genuinely new relative to those practices. \n \n**W2**. The head selection uses reasoning-type labels rather than accuracy supervision, yet the paper attributes final accuracy gains to “better reasoning.” The mechanism connecting suppression of non-linear reasoning and higher task accuracy is not rigorously analyzed. In particular, non-linear reasoning (verification/backtracking) can be crucial for error correction and indiscriminate suppression might harm exploration ability. A simple test: Reporting Pass@k accuracy or measuring recovery after early mistakes, which could reveal whether CREST causes “collapse of entropy.”\n\n**W3**. Steps are delimited by the \"\\n\\n\" boundary and non-linear steps are defined by keyword matching (e.g., Wait, Alternatively). This is heuristic and may entangle style/semantics with “cognitive type”. The paper does not validate these labels against model-centric signals (e.g., entropy/perplexity, confidence slope). Thus it is unclear whether the labels faithfully capture reasoning modes rather than surface cues. Moreover, keyword usage and even the choice of step boundaries can vary across models, so the generality of keyword-based labeling is uncertain. I encourage the authors to evaluate on a broader set of models and to report the distributions of (i) “\\n\\n” step boundaries and (ii) non-reasoning keywords across different datasets and models, to substantiate the cross-model applicability of the keyword-matching scheme."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VXBt6VdsOT", "forum": "h2pRcPdz2P", "replyto": "h2pRcPdz2P", "signatures": ["ICLR.cc/2026/Conference/Submission595/Reviewer_PbFD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission595/Reviewer_PbFD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973597927, "cdate": 1761973597927, "tmdate": 1762915559881, "mdate": 1762915559881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CREST, a method for understanding and steering the cognitive behaviors of large language models (LLMs) during test-time reasoning. The central idea is that specific attention heads, termed “cognitive heads”, exhibit activation patterns that correlate with linear and non-linear (e.g., verification, backtracking) reasoning steps. The authors propose a training-free, test-time intervention that identifies cognitive heads using linear probes and constructs steering vectors from their activations; applying vector-based interventions at inference time suppresses or amplifies certain cognitive behaviors. Experiments demonstrate that across several tasks and models, CREST improves accuracy (up to 17.5%) while substantially reducing token usage (up to 37.6%). The method generalizes across domains and models without retraining or heavy hyperparameter tuning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper comprehensively demonstrates, both quantitatively (see Figure 1, Figure 8-12) and qualitatively, that certain attention heads are strongly predictive of cognitive behaviors such as linear vs. non-linear reasoning steps, a finding that deepens model interpretability.\n2. CREST requires only offline calibration (linear probing plus simple covariance projection), imposing negligible runtime overhead and no further model training. Intervention is mathematically straightforward (see Equation in Section 4.2) and does not depend on sensitive hyperparameter tuning.\n3. The method is benchmarked on math, code, planning, and common-sense tasks (Table 1–3), along with extensive ablations (Figure 6, Figure 7) over head selection and response length distributions."}, "weaknesses": {"value": "1. While the probing accuracies and covariance analyses are compelling, the theoretical motivation for why cognitive heads manifest at specific layers and how robust these interventions are (especially in models with cross-layer dependencies or strong MoE gating) remains thin. Theoretical claims about head specialization rely mainly on correlational evidence (Figure 1, Figure 8-12) instead of providing more causal or mechanistic arguments. Section 4.1.2 describes projecting to a low-rank subspace, but the rationale for working at the layer-aggregated level (beyond noise reduction) is not well justified.\n2. The procedure for labeling steps as linear/non-linear depends on keyword matching with a fixed list (Appendix B.1), potentially introducing false positives/negatives. No quantitative sensitivity or robustness analysis for this heuristic is provided.\n3. The paper demonstrates average improvements across benchmarks but does not provide in-depth analyses, e.g., on when steering harms reasoning fidelity, or on failure cases tied to specific reasoning motifs. Figure 7 hints at failure cases reaching token limits, but beyond noting repetition, there is no nuanced discussion.\n4. Figure 3 and additional qualitative example in supplement (Figure 13) offer single-case or anecdotal illustrations; a more systematic qualitative study (e.g., did steering consistently reduce unnecessary backtracking or, conversely, miss subtle forms of needed verification?) would support generality claims."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pG2u7KVR1E", "forum": "h2pRcPdz2P", "replyto": "h2pRcPdz2P", "signatures": ["ICLR.cc/2026/Conference/Submission595/Reviewer_AvFd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission595/Reviewer_AvFd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102940821, "cdate": 1762102940821, "tmdate": 1762915559623, "mdate": 1762915559623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the imbalance between “overthinking” and “underthinking” of reasoning models. They investigate the internal cognitive structure of large language models and identify specific attention heads that correlate with reasoning behaviors such as verification and backtracking. Building on these insights, they propose CREST, which uses linear probing to detect “cognitive heads” and modulates their hidden activations to improve reasoning efficiency. Experiments across multiple models and reasoning benchmarks show that CREST enhances accuracy and reduces token generation, demonstrating strong generalization across domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers convincing empirical evidence that internal attention heads of LLMs encode semantically meaningful cognitive behaviors such as non-linear reasoning, supported by systematic linear probing.\n2. The proposed CREST framework is lightweight and effective, requiring only offline calibration on small datasets and operating at inference time with negligible overhead. It is training-free and model-agnostic.\n3. The results span a wide range of tasks and model scales, demonstrating strong generalization and robustness."}, "weaknesses": {"value": "1. The annotation of cognitive behaviors is rather coarse. Classifying linear and non-linear reasoning through surface keywords (e.g., “Wait”, “Alternatively”) may introduce noise.\n2. Although the vector adjustments in Equations (4) and (5) are reasonable, there is a lack of intuitive explanation for their geometric or semantic impacts, making it difficult for readers to understand how such rotations affect reasoning patterns.\n3. It’s unclear when steering might reduce accuracy by skipping key reasoning steps or oversimplifying explanations. Identifying task types where CREST’s steering could harm performance, and clarifying its reliability under domain shifts, would strengthen the analysis."}, "questions": {"value": "1. Section 3.3 mentions that the parameter α can be adjusted to control the enhancement of linear or non-linear reasoning. However, it doesn’t seem to be used in the later parts. Could the authors clarify how the model decides when to enhance or suppress a certain type of reasoning?\n2. Could the authors provide additional numbers showing the proportion of linear or non-linear reasoning used for questions of different difficulty levels? This would help demonstrate how the method alleviates underthinking and overthinking."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3qlIM5K6z4", "forum": "h2pRcPdz2P", "replyto": "h2pRcPdz2P", "signatures": ["ICLR.cc/2026/Conference/Submission595/Reviewer_wvoR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission595/Reviewer_wvoR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173613217, "cdate": 1762173613217, "tmdate": 1762915559385, "mdate": 1762915559385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}