{"id": "cN1QlgqORs", "number": 14807, "cdate": 1758244213893, "mdate": 1759897348134, "content": {"title": "Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents", "abstract": "Despite rapid progress in building conversational AI agents, robustness is still largely untested. Small shifts in user behavior, such as being more impatient, incoherent, or skeptical, can cause sharp drops in agent performance, revealing how brittle current AI agents are. Today’s benchmarks fail to capture this fragility: agents may perform well under standard evaluations but degrade spectacularly in more realistic and varied settings. \nWe address this robustness testing gap by introducing \\ours, a lightweight, model-agnostic method for systematically stress testing AI agents. TraitBasis learns directions in activation space corresponding to steerable user traits (e.g., impatience or incoherence), which can be controlled, scaled, composed, and applied at inference time without any fine-tuning or extra data. Using TraitBasis, we extend τ-Bench to τ-bench, where user behaviors are altered via controlled trait vectors. We observe an average 4%–20% performance degradation on τ-bench across frontier models, highlighting the lack of robustness of current AI agents to variations in user behavior.\nTogether, these results highlight both the critical role of robustness testing and the promise of TraitBasis as a simple, data-efficient, and compositional tool. By powering simulation-driven stress tests and training loops, TraitBasis opens the door to building AI agents that remain reliable in the unpredictable dynamics of real-world human interactions. We plan to open-source τ-bench, across four domains: airline, retail, telecom, and telehealth, so the community can systematically QA their agents under realistic, behaviorally diverse intents and trait scenarios.", "tldr": "TraitBasis is a data efficient method for creating high-fidelity, realistic user traits that can be applied to stress testing conversational AI agents.", "keywords": ["mechanistic interpretability", "LLM robustness", "AI Agents", "Quality Assurance", "Simulations", "RL Environments", "Benchmark", "Adversarial testing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40f411d77bbb4b54b85b3364a996ba25ed67d190.pdf", "supplementary_material": "/attachment/0f0baef2b88210f3c59fc9af0c83fa16c2935b8d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces TraitBasis, a lightweight method to address the brittleness of conversational AI agents by simulating realistic user traits like impatience and skepticism. Instead of traditional prompting or fine-tuning, TraitBasis steers a model's internal activations at inference time using pre-computed \"trait vectors,\" enabling controllable and composable persona generation. The authors use this technique to create $\\tau-\\texttt{Trait}$ a more challenging benchmark that reveals significant performance degradation (up to 46%) in even frontier models. The work effectively highlights the limitations of current agent evaluations and provides a powerful, data-efficient tool for systematic robustness testing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work focuses on a real problem in agent evaluation: most existing setups assume generic users. Adding steerable traits to benchmarks like T-Bench makes the evaluation setup feel more realistic and useful.\n\n2. The paper reads well overall, with clear organization and straightforward presentation of the main findings."}, "weaknesses": {"value": "1. The paper's core claims are undermined by an evaluation framework that lacks transparency and invites skepticism about its reliability. This weakness is starkly illustrated by comparing the qualitative data in Table 1 with the quantitative results in Table 2. The authors present examples in Table 1 to argue for TraitBasis's superior realism. However, these examples are open to subjective interpretation. For instance, in the \"Impatient (high)\" row, the response generated by 'Prompting' (\"Good—don’t waste my time. I expect your email today with clear numbers...\") can be seen as a more direct and arguably more realistic expression of impatience than the TraitBasis example. This issue is compounded by the complete omission of reliability and demographic information about the annotators.  The paper states that “at least three annotators” were involved but provides no Inter-Annotator Agreement (IAA) metrics such as Fleiss’ Kappa, nor any details about who these annotators are.  Without such transparency, it is impossible to assess whether their judgments were consistent or biased, leaving the validity of the human evaluation highly questionable.\n\n2. The methodology for deriving trait vectors rests on a series of heuristic decisions that are insufficiently justified, raising concerns about both rigor and reproducibility.  Key design choices appear ad hoc and lack supporting analyses.  For instance, the aggregation of per-token activations into a mean vector treats all tokens as equally informative for representing a trait, yet no rationale or ablation is provided to show why this averaging is appropriate compared to alternatives like final-state or weighted pooling.  Similarly, the selection of the “optimal” steering layer relies on a small-scale, subjective human preference study with only five annotators, which makes it difficult to assess whether the identified layer generalizes across traits or models.  Finally, the manually constructed contrastive pairs that form the foundation of the trait space introduce potential bias and scalability issues.  The absence of clear annotation protocols or controls for confounding factors further undermines confidence that the derived vectors faithfully isolate the intended traits.\n\n3. The evaluation is limited in scope and does not provide sufficient evidence to support the paper’s claims of generalizability or superiority. The proposed method is developed and tested only on Llama-3.1-8B-Instruct, leaving it unclear whether it would work similarly on other architectures such as Qwen or Gemma, or whether the learned trait vectors can transfer across models. The baseline setup also lacks essential details, such as the training configurations and parameter settings used for the SFT and LoRA models, which makes it difficult to assess the fairness and reproducibility of the comparison. In addition, the evaluation focuses solely on task performance, without considering safety aspects—for example, whether traits like “impatience” could lead to unintended hostile or toxic behavior."}, "questions": {"value": "1. How sensitive is the quality of a derived trait vector to the number of contrastive pairs (n) used in its computation?  For instance, is a stable and effective vector achievable with just a single pair, or is there a noticeable improvement in performance as n increases to 5 or 10?\n\n2. Have you analyzed the semantic relationships between the different trait vectors in the activation space? For example, does the 'impatience' vector exhibit a high cosine similarity to an independently derived 'frustration' vector, and is it nearly orthogonal to a 'politeness' vector? This could provide deeper insight into how the model represents these related concepts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "teltWjsCaM", "forum": "cN1QlgqORs", "replyto": "cN1QlgqORs", "signatures": ["ICLR.cc/2026/Conference/Submission14807/Reviewer_o4kQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14807/Reviewer_o4kQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760594962966, "cdate": 1760594962966, "tmdate": 1762925161105, "mdate": 1762925161105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method to evaluate the robustness of LLM agents against simulated users with various personality traits (e.g., impatience). The authors propose a two-stage activation steering technique to simulate these traits: 1) \"trait vectors\" are extracted by averaging the activation differences between manually written positive (trait-exhibiting) and negative (non-trait) responses; 2) these vectors are then applied to the model's activations during inference to steer generation towards the desired trait. Through human annotation and LLM-as-judge verification, the authors demonstrate that this steering approach achieves more effective trait simulation than baselines. Furthermore, the authors apply their method to the $\\tau$-bench to create a new benchmark, $\\tau$-trait. This new benchmark evaluates how agentic models behave when interacting with simulated users possessing diverse traits. Their findings indicate that the performance of state-of-the-art agentic models degrades significantly on $\\tau$-trait."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The research problem is well-motivated. Evaluating the robustness of agentic models against diverse user personalities is a critical and necessary step for real-world deployment. The paper's pursuit of user simulation as a method to achieve this is a promising direction."}, "weaknesses": {"value": "1. *Limited Contribution and Questionable Findings*: \n\n    **Limited Methodological Contribution**: The proposed activation steering method appears to be a straightforward application of well-established techniques from representation engineering [1, 2] and inference-time alignment [3]. The authors also fail to sufficiently differentiate their work from the concurrent [4], which proposes a very similar framework. The paper's primary claim to novelty, \"extending this to generate complex, multifaceted human traits (L125)\", seems to be a simple linear combination of vectors. This combination is governed by an \"empirical mapping\" that is never explained, making the technical depth of this contribution unclear. \n\n    **Expected Findings and Questionable Validity of the Proposed Benchmark**: The main experimental insights are: 1) the proposed methods achieve better simulation v.s. baselines (a bit weak, only including SFT, LoRA and Prompt-based); 2) the state-of-the-art agentic models are not robust when playing with simulated users. Both are expected or can be derived from previous works (including those mentioned in Section 2). This makes the central contribution, the $\\tau$-trait benchmark, feel unnecessary. What new application insights does it enable? One possibility is that, by considering more complicated user traits and their combinations, it might make the evaluation more challenging for the state-of-the-art models and lead to more significant performance degradation -- but what does this degradation mean? How does this correlate with **the real user experience**? It is highly questionable whether an 8B-Instruct model (L190) is a faithful human simulator for multi-round agentic tasks. The reported performance degradation could simply stem from the weak simulator's inability to interact coherently, rather than the agent's failure to handle a personality \"trait.\" The authors' **indirect justification**, verifying simulation success in some other dataset (not very clear -- from Table 1 and conversation annotation setup in Section 4.2, it seems the dataset used for verifying the fidelity in Section 6 is not $\\tau$-trait), does not translate to the agentic scenario and fails to make a convincing case.\n\n2. *Poor Methodological Clarity and Rigor*: The paper is missing critical implementation details, making the work impossible to reproduce and evaluate. Here is a non-exhaustive list of such implementation details:\n\n    **Simulator Model Not Explained**: The authors never explicitly state which open-weight model is used for the user simulator. Readers are forced to guess it is Llama-3.1-8B-Instruct (from L190), a choice that itself requires justification.\n\n    **Unexplained \"Empirical Mapping\"**: The method for combining trait vectors is vaguely described as \"via an empirical mapping,\" with no further explanation of how this mapping is derived or validated.\n\n    **Unsubstantiated Claims**: The authors claim the 10th layer was chosen \"through systematic experimentation,\" but no such experiments are present in the paper or appendix. This appears to be a groundless statement. This lack of rigor in describing the method and justifying design choices is a significant flaw.\n\n3. *Confusing Paper Organization*: \nThe paper's structure is illogical and harms readability. For example, the authors introduce the $\\tau$-trait benchmark (Section 4), then divert to a separate simulation verification (Section 5), before finally presenting the benchmark's evaluation (Section 6). A more logical flow would group the method's verification (Section 5) with its description, followed by the benchmark's creation (Section 4) and its evaluation (Section 6). The current disjointed flow is confusing to follow.\n\n**References**:\n\n[1] Zou, Andy, et al. \"Representation engineering: A top-down approach to AI transparency.\" arXiv preprint arXiv:2310.01405 (2023).\n\n[2] Turner, Alexander Matt, et al. \"Steering language models with activation engineering.\" arXiv preprint arXiv:2308.10248 (2023).\n\n[3] Wang, Pengyu, et al. \"InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance.\" Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 2024.\n\n[4] Chen, Runjin, et al. \"Persona vectors: Monitoring and controlling character traits in language models.\" arXiv preprint arXiv:2507.21509 (2025)."}, "questions": {"value": "1. What is the exact open-weight model used for the user simulator? The paper implies Llama-3.1-8B-Instruct (L190) but never explicitly confirms this. If the author really uses this model for user simulation, then what is the justification for using an 8B model as a user simulator, especially when the original $\\tau$-bench uses a much more capable model (GPT-4o)?\n\n2. The \"empirical mapping\" used for the linear combination of trait vectors (L125) is a critical, unexplained component. Can the authors provide a precise definition of this mapping, the methodology used to derive it, and how the verification is conducted?\n\n3. The paper claims the 10th layer was chosen \"through systematic experimentation.\" Can the authors please provide these experimental results, as they are currently missing from the paper and appendix?\n\n4. What is the dataset used to verify the realism, fidelity, stability, and compositionality in Section 6? From the provided examples, annotation setup, and prompt designs, it does not look like some version of $\\tau$-bench. \n\n5. Could the reported performance degradation on $\\tau$-trait be an artifact of the weaker simulator failing to provide coherent interactions, rather than the agent failing to handle personality traits? How can these two potential causes of failure be disentangled?\n\n6. Given that the findings are somewhat expected, what is the primary new insight or application that $\\tau$-trait enables beyond prior benchmarks? A clearer articulation of its necessity would strengthen the paper.\n\n7. The paper's organization is confusing. Please consider reordering the sections. For instance, the benchmark evaluation (Section 6) should logically follow its creation (Section 4). The simulation verification (Section 5) feels disconnected and might be better integrated earlier or moved to an appendix.\n\n8. The margin looks suspicious; potentially, the author is not using the standard ICLR template."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3ZRZ8pzQ3V", "forum": "cN1QlgqORs", "replyto": "cN1QlgqORs", "signatures": ["ICLR.cc/2026/Conference/Submission14807/Reviewer_isXz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14807/Reviewer_isXz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511510353, "cdate": 1761511510353, "tmdate": 1762925159368, "mdate": 1762925159368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses persona vectors to steer LLMs to simulate a diverse set of users thereby increasing the robustness of agentic benchmarks. They compare to other methods such as sys prompt and lora finetuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- well written.\n- correctly voice concerns able lack of a diverse range of personas in agentic benchmarks.\n- suggest and evaluate a credible solution.\n- evaluated with both human and autograders. \n- compare to other alternative approaches (sys prompt + fine tuning)."}, "weaknesses": {"value": "- no github: this would be a very strong paper if they provided a scaffold to apply their method to any agentic benchmark. \n- It is unclear how different the finetuning and linear probe datasets differ. This could be the source of the differences in methods. They note that the finetuning dataset was not particularly aligned with their key persona axes. More information here + evidence of similarity would improve the paper.\n- How many human raters? I assume that they were blinded? \n- They note poor agreement between the autograders and human raters. It would be good if this was further investigated and addressed.\n- They state that the main issue with current methods is maintaining personas over long conversations, but from their evaluations it is not clear that the samples were long context. \n- In the Related work section, they detail many user persona evals, can these not be used to assess the proposed method?\n- The method for altering personas is far from novel and is used regularly in studies. This would be okay if the paper provided an easy to use method to apply to benchmarks. \n- in the opening paragraph they cite a BBC article for poor OOD performance. It is not clear though why the performance dropped e.g. is this just an engineering bug or is this due to the LLM? The related work section is great, citing references from there to back up the point would be better."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jRsR8M3PID", "forum": "cN1QlgqORs", "replyto": "cN1QlgqORs", "signatures": ["ICLR.cc/2026/Conference/Submission14807/Reviewer_g9uE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14807/Reviewer_g9uE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922734592, "cdate": 1761922734592, "tmdate": 1762925158448, "mdate": 1762925158448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a user trait steering method called TraitBasis for evaluating conversational AI agents under shifts in user behavior. TraitBasis learns four types of realistic user traits: impatient, incoherent, skeptical, and confusion. Using TraitBasis, AI agents demonstrate significantly degraded performance on tau-bench."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper studies an important and realistic problem in conversational AI agent evaluation.\n\n2. The proposed method demonstrate qualitative effectiveness as shown in Table 1."}, "weaknesses": {"value": "1. The key technique in TraitBasis appears to be a direct application of activation steering. The paper fails to articulate the technical contribution of TraitBasis beyond existing activation steering methods, such as [1].\n\n2. The experimental settings of TraitBasis, such as the underlying model used and the calculation of metrics, are unclear.\n\n3. The reported improvement of TraitBasis over baselines on the proposed metrics is marginal (Table 2).\n\n4. The evaluation is limited to a single benchmark and two models, which is insufficient to establish quantitative effectiveness. Please expand to additional benchmarks (e.g., those cited in Section 2, paragraph 1) and a broader set of model families/sizes.\n\n[1] Subramani, Nishant, Nivedita Suresh, and Matthew E. Peters. \"Extracting Latent Steering Vectors from Pretrained Language Models.\" Findings of the Association for Computational Linguistics: ACL 2022. 2022."}, "questions": {"value": "1. Line 036: The citation about “Tech columnist” appears to have a wrong year. Please verify and correct\n\n2. Line 086-087: Why were these four human traits considered? Are they comprehensive? Are they widely observed in real-world scenarios? If so, can you provide justifications and concrete evidence?\n\n3. Line 096: What’s the definition of out-of-distribution in the context of realistic user traits?\n\n4. Line 201: How do you choose z in experiments? How should practitioners choose z in their deployments?\n\n5. Line 228-238: What are the base models for the fine-tuned baselines. Are they the same as the model used in TraitBasis?\n\n6. Line 271-275, RQ3: How is consistency calculated exactly? As described in the text, each conversation is classified into three types. How is the final percentage score in Table 2 calculated?\n\n7. Table 2: Even with human annotators, TraitBasis seems to only achieve marginal improvement over baselines in terms of Realism and Fidelity. Can you justify or explain this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4T4EkfsJgh", "forum": "cN1QlgqORs", "replyto": "cN1QlgqORs", "signatures": ["ICLR.cc/2026/Conference/Submission14807/Reviewer_e8Kc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14807/Reviewer_e8Kc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964830901, "cdate": 1761964830901, "tmdate": 1762925157833, "mdate": 1762925157833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}