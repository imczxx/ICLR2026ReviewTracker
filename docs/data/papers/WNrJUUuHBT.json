{"id": "WNrJUUuHBT", "number": 22695, "cdate": 1758334543576, "mdate": 1759896851919, "content": {"title": "SchulzNN: A Neural Network-Based Matrix Inversion Solver Inspired by Schulz Iteration", "abstract": "While neural networks have emerged as powerful tools for solving optimization problems, demonstrating performance comparable to or surpassing traditional solvers, their application to fundamental numerical optimization problems remains underexplored. Specifically, limited progress has been made in developing neural network-Based approaches for matrix inversion --- a cornerstone problem in unconstrained numerical optimization. This work presents SchulzNN, the first neural network-Based solver for matrix inversion inspired by the Schulz iterative method. Our architecture innovatively simulates traditional iteration processes through parametric learning, preserving theoretical convergence guarantees while enhancing computational flexibility. We rigorously evaluate both single-layer SchulzNN and deep variants on (1) diverse matrix families with practical significance, and (2) matrices beyond the convergence conditions of classical Schulz iteration. Notably, we establish a systematic framework for analyzing model adaptation through fine-tuning strategies. The accuracy and efficiency of the proposed SchulzNN are demonstrated by numerical examples for matrix inversion.", "tldr": "We propose the Schulz Neural Network based on Schulz iteration for solving matrix inverse.", "keywords": ["Deep Neural Network", "Schulz Iteration", "Matrix Inversion"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ddff50d086312146b1abffd160588a733499cbc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to produce a \"NN equivalent\" (my words, not theirs) of the iterative Schulz method for approximating a matrix inverse.  Although their network is a single pass, they present a \"deep\" variant that involves chaining multiple of their SNN blocks together to mimic the iterative nature of Schulz iteration."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method succeeds in cases where classical Schulz iteration fails\n- The architecture is relatively simple, therefore, computationally efficient and understandable/interpretable"}, "weaknesses": {"value": "- I am not convinced that the authors are approximating $A^{-1}$.  It seems like they are just approximating $A^{-1} b$ for a given vector $b$ (L158-L160 they want the output vector to resemble $A^{-1} b$, not $A^{-1}$).  If this is indeed the case, the authors should rewrite their paper to use terminology like \"the action of $A^{-1}$\" rather than \"$A^{-1}$.\"  It is misleading otherwise.\n- Insofar as the method is learning $A^{-1} b$, this is equivalent to the growing body of literature on NN-based preconditioners and linear solvers, which the authors have missed in their review of the literature.  See, for instance, Lan et al. \"A neural-preconditioned poisson solver for mixed Dirichlet and Neumann boundary conditions\" (2023) and Kaneda et al. \"A deep conjugate direction method for iteratively solving linear systems\" (2023) and the related work discussions in there.  I encourage major revisions of the authors' related work in light of this.\n- Relatedly, those papers present methods that approximate $A^{-1} b$, yet the present paper lacks any comparisons.  We cannot assess how well the present method performs, in terms of time or accuracy, compared to prior works in this vein.  This is a weakness of the manuscript.\n- $10^{-3}$ is not high precision.  How low of a tolerance can the method achieve?  In fact the losses per Table 1 are on the order of $10^{-3}$ for a matrix whose condition number is 1.359, which is essentially the identity matrix, so it's surprising to get such a large error on such a trivial matrix.\n- Similarly, it would have been interesting to see some ablations on how many SNN blocks are needed to solve a system within a given target tolerance.\n- It is not always clear how big the matrices are that are being dealt with in the examples (including, but not limited to, 4.2.4).  This is important to describe on its own, but also relative to prior work, which has been able to neurally solve systems with millions of DOFs."}, "questions": {"value": "No further questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "p5a5qwk5hp", "forum": "WNrJUUuHBT", "replyto": "WNrJUUuHBT", "signatures": ["ICLR.cc/2026/Conference/Submission22695/Reviewer_e98a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22695/Reviewer_e98a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761254759624, "cdate": 1761254759624, "tmdate": 1762942341165, "mdate": 1762942341165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SchulzNN, a neural network architecture designed for matrix inversion. The core idea is to build a network that mimics the classical Schulz iteration method. The authors propose both a single-layer architecture, `SchulzNN₁`, which simulates one step of the iteration, and a deep, recursive architecture, `SchulzNNₖ`, to simulate multiple steps. A key contribution is the unsupervised training objective, which minimizes the residual norm $||A\\hat{x} - b||$ instead of requiring the ground truth inverse $A^{-1}$. The paper demonstrates the method's effectiveness on several synthetic matrix families, including cases where the traditional Schulz iteration fails, and shows that deep models outperform shallow ones on ill-conditioned matrices."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper's primary strength lies in its originality and conceptual elegance. The idea of drawing inspiration from a classical numerical method (Schulz iteration) to inform the design of a neural network architecture is highly commendable. This \"unrolling\" of a traditional algorithm into a learnable framework is a promising research direction.\n\nSpecifically:\n\n1.  **Originality:** The proposed SchulzNN architecture is novel and directly motivated by a well-established mathematical principle. The way the network's forward pass, $\\hat{x} = (2I - W_3 A) W_1 b$, mirrors the Schulz update is clever.\n2.  **Clarity:** The paper is well-written, and the methodology is explained clearly. The recursive construction of the deep `SchulzNNₖ` from the base `SchulzNN₁` module is intuitive and well-illustrated.\n3.  **Technical Soundness:** The design of the unsupervised loss function is a significant strength. It cleverly sidesteps the need for the ground truth inverse, making the training process practical and self-contained. This is a thoughtful and well-justified design choice."}, "weaknesses": {"value": "Despite the novel idea, the paper suffers from significant weaknesses in its experimental evaluation and positioning within the broader literature, which severely undermine its claimed contributions and practical significance.\n\n1.  **Insufficient Experimental Scope and Scale:** The experiments are conducted on matrices of a very small scale ($N=1024$). In many real-world scientific and engineering applications, this size is trivial, and traditional direct solvers on a modern CPU can compute the inverse in seconds or less. While the paper provides an asymptotic complexity analysis ($O(N^2 \\log N)$), it fails to provide any wall-clock timing results. This makes it impossible to assess the actual computational advantage. To be convincing, the method must be tested on much larger-scale problems (e.g., $N$ in the range of $10^5$ to $10^6$). Furthermore, the matrices tested are largely synthetic and simple. The paper would be much stronger if it demonstrated performance on matrices derived from real-world problems, such as large sparse matrices from PDE discretizations, large low-rank matrices from communication systems, or large dense matrices like Hamiltonians in quantum chemistry.\n\n2.  **Critically Lacking Baselines:** The paper's comparison is almost exclusively against the traditional Schulz iteration, which is insufficient. A comprehensive evaluation is needed against a proper set of baselines:\n    *   **Other Neural Network Solvers:** The paper ignores recent related work on using neural networks for linear algebra problems. Comparisons should be made against methods like those proposed in [1-3], which tackle related operator decomposition and eigenvalue problems.\n    *   **Standard Numerical Algorithms:** The method must be compared against widely-used numerical solvers. This includes direct methods (e.g., LU decomposition, RSVD for low-rank approximations) and, more importantly, iterative methods like Krylov subspace methods equipped with standard preconditioners. These are the workhorses of scientific computing.\n    *   **GPU-Accelerated Solvers:** The authors state their method is implemented on a GPU. Therefore, comparing its performance (in terms of wall-clock time) only to CPU-based algorithms is an unfair comparison. A direct comparison with highly-optimized GPU libraries for linear algebra, such as NVIDIA's **cuSOLVER**, is essential to demonstrate any real-world speedup.\n\n3.  **Unclear Practical Significance and Application Scenario:** The paper fails to articulate a clear and convincing use case for SchulzNN.\n    *   **Scenario 1: Independent Matrix Problems.** In many real-world settings, one encounters a series of independent matrix problems. In this case, SchulzNN would require dataset generation and a potentially expensive training phase for *each new matrix*. This makes it far less practical than a traditional one-shot solver. The total time-to-solution (including training) must be considered, and it is likely that SchulzNN would be orders of magnitude slower.\n    *   **Scenario 2: Sequences of Correlated Matrices.** The most plausible application is solving a sequence of highly-correlated linear systems, which arise in nonlinear solvers or PDE optimization. However, this is a well-studied area in numerical analysis with highly effective acceleration techniques. The paper completely overlooks these methods, such as **Krylov subspace recycling** [4] or specialized eigensolvers like **ChASE** [5] for sequences of problems. Without a thorough comparison to these established, problem-aware traditional methods, the claim of SchulzNN's utility in this domain is unsubstantiated."}, "questions": {"value": "1.  **Scalability and Timing:** Could the authors provide wall-clock time comparisons for SchulzNN (including training time) against traditional solvers (e.g., a direct solver in LAPACK or an iterative solver in PETSc) for the current problem size ($N=1024$)? More importantly, how does the method's performance and training stability scale to significantly larger matrices, for instance, $N > 100,000$?\n\n2.  **Baseline Comparisons:** Could the authors justify the omission of comparisons against the baselines mentioned in the \"Weaknesses\" section? Specifically, why were other neural network-based solvers [1-3], standard preconditioned Krylov methods, and GPU-accelerated libraries like cuSOLVER not considered? A response to this would be critical in assessing the paper's contribution.\n\n3.  **Target Application and Comparison to Specialized Solvers:** What is the precise application scenario the authors envision for SchulzNN?\n    *   If it is for single, independent matrices, how can the training overhead be justified against one-shot solvers?\n    *   If the target is sequences of correlated matrices (e.g., from a nonlinear optimization loop), how does SchulzNN compare against established techniques designed for such sequences, like Krylov subspace recycling [4] or methods that reuse spectral information [5]? A convincing argument would require a direct comparison in such a setting, measuring total time-to-solution over the entire sequence.\n\n---\n\n**References:**\n\n[1] Fan, Z., et al. \"Operator SVD with neural networks via nested low-rank approximation.\" ICML 2024.\n\n[2] Wang, Z., et al. \"Neural networks based on power method and inverse power method for solving linear eigenvalue problems.\" Computers & Mathematics with Applications 2023.\n\n[3] Li, Z., et al. \"STNet: Spectral Transformation Network for Solving Operator Eigenvalue Problems.\" NeurIPS 2023.\n\n[4] Parks, M., et al. \"Recycling Krylov subspaces for sequences of linear systems.\" SIAM Review 2006.\n\n[5] Nakatsukasa, Y., et al. \"ChASE: Chebyshev accelerated subspace iteration eigensolver for sequences of Hermitian eigenvalue problems.\" ACM Transactions on Mathematical Software 2019."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9c0WWFtON1", "forum": "WNrJUUuHBT", "replyto": "WNrJUUuHBT", "signatures": ["ICLR.cc/2026/Conference/Submission22695/Reviewer_HoSc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22695/Reviewer_HoSc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832584491, "cdate": 1761832584491, "tmdate": 1762942340953, "mdate": 1762942340953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents the Schulz Neural Network (SchulzNN), a novel deep learning architecture designed for the fundamental task of matrix inversion. The authors attempt to bridge the gap in applying neural networks to core numerical optimization problems, where traditional methods have had limited success."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The SchulzNN is explicitly designed to structurally simulate the established Schulz iterative method for calculating the inverse matrix ($\\mathbf{A^{-1}}$). This novel combination of a proven numerical algorithm embedded directly into a neural network architecture moves beyond generic \"Learn to Optimize\" (L2O) approaches and can potentially enhance interpretability and stability."}, "weaknesses": {"value": "To substantiate the claimed superiority, a comparison of SchulzNN's performance against the classical Schulz iteration and established numerical libraries is required, particularly in the following areas:\n1. **Real-World Scale:** Show results for matrices with dimensions relevant to real-world tasks (e.g., $1024 \\times 1024$, $2048 \\times 2048$), moving beyond small-scale proofs-of-concept.\n2. **Robustness to Ill-Conditioning:** Show results on matrices with high condition numbers (e.g., $10^6, 10^7$), where the traditional Schulz iteration is known to fail due to poor initial guess selection.\n3. **Convergence Analysis:** Quantify the number of iterations/epochs SchulzNN requires to achieve a target error $\\varepsilon$ (e.g., $\\varepsilon = 10^{-6}$), compared to the convergence rate of the classical method.\n4. **Baseline Benchmarks:** The paper needs to clearly benchmark against other neural network-based numerical solvers or relevant L2O methods to solidify its competitive advantage beyond just the classical Schulz iteration.\n\nThe unsupervised training approach is clever, but it introduces new questions about stability that must be addressed:\n1. **Weight Initialization Dependency:** How does the initial state of the network weights ($\\mathbf{W_1}, \\mathbf{W_3}$) impact the training convergence and the final accuracy? This is critical since the weights act as the learned iterative terms.\n2. **Generalization Limits:** Is the trained SchulzNN a general inverse solver, or is it highly tuned to the distribution of matrices seen during training? The paper must test the network's performance on matrices far outside the training distribution to assess its true generalization capability.\n\nThe introduction of the deep $\\mathrm{SchulzNN}_k$ needs clearer justification:\n1. **Cost-Benefit Trade-off:** Increasing $k$ increases the computational cost. The authors must quantify the trade-off: Does the enhanced accuracy of $\\mathrm{SchulzNN}_k$ justify the increased cost (e.g., $O(N^3)$ or $O(N^2 \\log N)$ per layer) when compared to simply running $k$ iterations of the standard Schulz method?"}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PDqkmjYHU5", "forum": "WNrJUUuHBT", "replyto": "WNrJUUuHBT", "signatures": ["ICLR.cc/2026/Conference/Submission22695/Reviewer_qEGD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22695/Reviewer_qEGD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949909976, "cdate": 1761949909976, "tmdate": 1762942340762, "mdate": 1762942340762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SchulzNN a three-layer linear network that exactly mimics one step of the classical Schulz iteration for matrix inversion. The middle layer is fixed to $A$ and the output $\\hat{x}=2W_1b-W_3AW_1b$ equals $(2I-A_0A)A_0b$ under $W_1=W_3=A_0$. Training minimizes an unsupervised residual $\\mathcal{L}=\\frac{1}{m}\\sum_i|A\\hat{x}_i-b_i|/|b_i|$ and stacking blocks SchulzNN$_k$ emulates multi-step refinement. When $A$ admits IDBF the mat-vec is $O(N\\log N)$ and per-epoch training is $O(N^2\\log N)$. Experiments on strictly diagonally dominant permutation discrete Helmholtz and perturbed identity matrices show about $10^{-4}$ accuracy for Helmholtz at $k=3$ and about $10^{-3}$ adaptation under moderate perturbations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "In terms of originality the single block is algebraically identical to $(2I-A_0A)A_0b$ anchoring the design in a classical fixed-point map. In terms of quality the inverse-free residual objective is operationally sound and increased depth helps on difficult spectra with $k=3$ outperforming shallower variants on Helmholtz. In terms of clarity the paper specifies $\\hat{x}$ the loss $\\mathcal{L}$ the recursive construction SchulzNN$k$ and the fixed A-layer together with sensible metrics $\\epsilon{\\text{inv}}$ and $\\epsilon_{\\text{sub}}=|AA_d^{-1}-I|/|I|$. In terms of significance the approach is reusable as an approximate inverse or preconditioner surrogate for fixed $A$ with many right-hand sides and the IDBF path yields a concrete training cost profile."}, "weaknesses": {"value": "• The architecture is tied to the matrix dimension; changing the resolution $N$ requires retraining rather than transferring weights.  \n  • The per-epoch claim $O(N^2\\log N)$ lacks empirical scaling curves, so time behavior and accuracy degradation as $N$ grows remain unclear.  \n  • When the resolution changes, there is no systematic study of how the required depth $k$ evolves on hard spectra such as Helmholtz.  \n  • The method is trained per matrix $A$;cross-family generalization is not evaluated. For unseen $A'$, only fine-tuning on nearby perturbations is reported and there is no zero-shot evaluation.  \n  • There are no convergence or error guarantees for the trained deep composition, and the fine-tuning success region is only described empirically.  \n  • As a preconditioner, application-level comparisons (iteration counts and wall-clock) against strong baselines such as AMG, geometric MG, or ILU are missing.  \n  • Efficiency relies on IDBF-type hierarchical low-rank structure; behavior on dense, non-hierarchical matrices is unspecified.  \n  • Experiments are confined to **synthetic matrix families** and their perturbations; there is no evaluation on external benchmarks or practical application datasets."}, "questions": {"value": "1. Please evaluate on matrices not used for training. Use PDE-style benchmarks or application matrices and report both zero-shot and short fine-tuning, including $\\epsilon_{\\text{sub}}=\\|AA_d^{-1}-I\\|/\\|I\\|$, Krylov iterations, wall-clock time, and failure rate.\n2. Please quantify preconditioning efficacy. Insert $A_d^{-1}=(2I-W_3A)W_1$ as a left/right preconditioner in CG/MINRES/GMRES on Poisson and Helmholtz, choose one of Dirichlet/Neumann/Robin for each, sizes $N=2^{10}\\text{–}2^{16}$, and compare **iterations and wall-clock** to AMG/geometric MG/ILU (stop at residual $10^{-8}$ or $\\epsilon_{\\text{sub}}\\le 10^{-3}$).\n3. Please clarify handling of resolution changes. For each resolution, compare independent training versus  coarse-to-fine warm-start , reporting $\\epsilon_{\\text{sub}}$, Krylov iterations, wall-clock time, and fine-tuning epochs.\n4. Please cover boundary conditions and solvability explicitly. Evaluate Dirichlet / pure Neumann / Robin under the same geometry, grid, and frequency; for Neumann, also report a mean-free residual using $P_0=I-\\tfrac{1}{N}\\mathbf{1}\\mathbf{1}^\\top$ with $r=P_0(A\\hat{x}-b)$ and $\\epsilon_{\\text{sub}}=\\|r\\|/\\|b\\|$. State how compatibility conditions (e.g., $\\int_\\Omega f\\,dx=\\int_{\\partial\\Omega} g\\,ds$) are enforced, and report how boundary conditions affect conditioning, the required depth $k$, training stability (failure rate), and wall-clock time."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)", "Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)", "Yes, Other reasons (please specify below)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j5CJXXERUH", "forum": "WNrJUUuHBT", "replyto": "WNrJUUuHBT", "signatures": ["ICLR.cc/2026/Conference/Submission22695/Reviewer_vpFG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22695/Reviewer_vpFG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982992976, "cdate": 1761982992976, "tmdate": 1762942340484, "mdate": 1762942340484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}