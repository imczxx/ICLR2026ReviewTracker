{"id": "m3jG3GaNIj", "number": 21497, "cdate": 1758318289796, "mdate": 1759896918957, "content": {"title": "STAT: Skill-Targeted Adaptive Training", "abstract": "Language models often show little to no improvement (i.e., “saturation”) when trained via vanilla supervised fine-tuning (SFT) on data similar to what they saw in their training set (e.g., MATH). We introduce a new fine-tuning strategy, STAT, to train such a student model by using the metacognition ability of a stronger large language model (LLM) as the teacher. The teacher uses the task dataset to create a list of skills needed for the task, and then labels each data point with its required skills (Didolkar et al., 2024). By monitoring the student’s answers, the teacher creates a Missing-Skill-Profile for the student, tracking how often they failed to apply each skill in their responses. We use this idea to build a modified training set in one of two ways. In STAT-Sel, the teacher uses an existing set of training examples but adaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn, the teacher synthesizes additional examples involving missing skills. Across extensive experiments on Llama and Qwen models, our methods yield improvements of up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore, STAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25, AMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is complementary to RL via GRPO (Shao et al., 2024): after the model is improved using STAT to address skill gaps, GRPO continues to add further gains. We conclude that skill-targeted adaptive training should broadly improve current training pipelines.", "tldr": "We introduce a new fine-tuning strategy, STAT, that select or synthesize training data targeted on models' missing skills.", "keywords": ["metacognition", "out-of-distribution generalization", "dataset reusability", "skill-level training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/452dd8a14ea1fee3f56df260100fa82d153253f6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces STAT (Skill-Targeted Adaptive Training), a novel fine-tuning framework for language models that leverages the metacognitive abilities of large teacher models to identify and target missing skills in student models. STAT improves generalization and performance on mathematical reasoning benchmarks by either reweighting training data (STAT-Sel) or generating new skill-targeted questions (STAT-Syn). Across multiple benchmarks, STAT consistently outperforms standard SFT and other adaptive data selection methods, and complements reinforcement learning approaches like GRPO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work leverages frontier LLMs' introspective abilities to identify specific skills lacking in student models. This goes beyond error analysis and introduces a more human-like teaching dynamic.\n2. STAT methods (both Sel and Syn) demonstrate consistent and significant performance gains over SFT and baselines across in-distribution (MATH) and out-of-distribution tasks (AIME, AMC, MATH-perturb), with gains up to 7.5% on MATH and 5.8% on OOD datasets."}, "weaknesses": {"value": "1. The entire STAT pipeline depends on access to a strong teacher model with metacognitive abilities. This could limit replicability or scalability in low-resource settings or with proprietary LLMs.\n2. The Missing-Skill-Profile is built using validation set responses, which might introduce bias if the validation set is not representative of future deployment scenarios.\n3. The STAT-Syn variant, while effective, incurs significant computational and inference cost due to on-the-fly synthetic question generation and filtering, which may not be inefficient in large-scale deployments."}, "questions": {"value": "1. How robust is the STAT framework to inconsistencies or ambiguities in the skill taxonomy generated by the teacher model? Would the approach degrade if the teacher’s skill decomposition is noisy or if the skill set is too coarse-grained?\n2. Given that STAT relies on a strong LLM teacher to guide training and even generate new examples, how do you mitigate the risk that the teacher’s inductive biases dominate the student’s learning?\n3. How do you balance granularity vs. utility when generating the Missing-Skill-Profile? For instance, does distinguishing between \"algebraic manipulation\" and \"linear equation solving\" actually improve downstream performance, or introduce noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q6P6xKHgm4", "forum": "m3jG3GaNIj", "replyto": "m3jG3GaNIj", "signatures": ["ICLR.cc/2026/Conference/Submission21497/Reviewer_yjVy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21497/Reviewer_yjVy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928697904, "cdate": 1761928697904, "tmdate": 1762941806238, "mdate": 1762941806238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes STAT, a teacher-student fine-tuning method that uses a stronger LLM to diagnose which skills a smaller model lacks on a math validation set, then adapts training data to target those deficits. Two instantiations are presented: STAT-Sel (re-weights existing examples) and STAT-Syn (synthesizes new questions). On MATH and several out-of-domain (OOD) benchmarks, STAT yields up to 7.5 % absolute gain over vanilla SFT and 4.6 % average OOD improvement. Ablations show that (i) models saturate because they still fail on basic algebraic skills, (ii) embedding-based data selection does not target those skills, and (iii) STAT is complementary to subsequent RL (GRPO)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality\n- First work to operationalize LLM metacognition for training-set adaptation rather than mere evaluation or prompt refinement.\n- Skill-map + Missing-Skill-Profile + targeted data construction is a clean pipeline motivated from how humans actually learn.\n\nEmpirical Quality\n- Experiments on Llama and Qwen families (1B–3B sizes) with MATH as the primary dataset show substantive in-distribution and OOD improvements over naive SFT and embedding-based baselines.\n- Careful ablations vs. strong baselines: MATH-Augment, MATH-Hard, Embed-Sel/Syn, and GRPO.\n- Hyper-parameters openly tuned per method (sec. 3.1 & C.1).\n\nClarity\n- Algorithm box (Alg. 1), reproducible prompts (Appendix C.3), and released data and code pledge (sec. 7) at camera ready version facilitate replication.\n\nSignificance\n- Provides a practical recipe to push small models beyond SFT saturation without extra human labels.\n- Demonstrates that basic skill gaps (solving linear equations, arithmetic manipulation) remain even after extensive training. I believe these are valuable insight for the community and pushes algorithmic progress towards better data/curriculum design."}, "weaknesses": {"value": "- Dependence on a Powerful Teacher Model: The entire STAT framework is predicated on the availability of a significantly more capable \"teacher\" LLM (GPT-4o-mini is used) that can accurately diagnose skill deficiencies. How would one go about using this method for improving the skills of frontier models? Is self-improvement a possibility here by using either the same model (prompted in a different way) or by using specialized but smaller models  as teacher?\n\n- Scope limited to MATH-style reasoning (minor): results are strong for math, but applicability to language or safety tasks is only discussed as future work. If the authors could discuss a bit about the generality of the method and possible way to expand it beyond the Math scope, that would be helpful.\n\n- No comparison with gradient-based data selection methods (e.g., RHO-1, Lin et al. 2024) or difficulty-based rejection tuning (DART-math, Tong et al. 2024)."}, "questions": {"value": "Based on the comments above, a few questions for the authors:\n\n- Can the skill-map pipeline be automated for non-math domains? \n- How many independent training runs (seeds) were performed? If only one, can you provide variance estimates?\n- The method's success seems to rely on a predefined list of skills (Section 2, lines 141-145). How sensitive is STAT to the quality and granularity of this skill taxonomy? For instance, would a coarser or finer-grained skill list significantly alter the results?\n- How important is using a powerful teacher? Have you considered alternatives which might be better for scaling and help improve fronteir models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mmoyLaxnEf", "forum": "m3jG3GaNIj", "replyto": "m3jG3GaNIj", "signatures": ["ICLR.cc/2026/Conference/Submission21497/Reviewer_6kx2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21497/Reviewer_6kx2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020017557, "cdate": 1762020017557, "tmdate": 1762941805902, "mdate": 1762941805902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Supervised fine-tuning can often result in limited gains. This paper proposes an approach, STAT, to transform an SFT dataset (via reweighting or synthetic data generation) based on skills. Given a validation dataset, the current model's failures on it are analyzed to determine which skills are missing from the model. These skills are linked back to the training data, and data most associated with these skills are upweighted or synthetically generated. Empirical results show that STAT-Syn and STAT-Sel significantly outperform other data-centric approaches on several model families and sizes. It can also be used in continual learning, and also can be used with RL downstream."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\n- This paper builds on several existing skills-based works, e.g., using their techniques for skill discovery and skill mapping. However, this is not a significant drawback. Moreover, this work focuses on the SFT setting and uses teachers/skills exclusively for alignment, rather than embeddings or gradients. \n\nQuality:\n- Experiments show that STAT can significantly improve performance across model sizes and families for math. \n\nClarity:\n- The paper is well-written, and the method is easy to understand.\n\nSignificance:\n- A common heuristic for curate good training datasets is to simply create or select samples that \"look like\" the evaluation data. E.g., if you want your model to do well on science, you need to train on science data. However, this heuristic is fairly imprecise and can fail to capture the underlying skills that samples teach the model. Moving towards skill-based data curation is an important direction, and I foresee datasets being annotated with more and more fine-grained skill taxonomies in the future to facilitate greater gains."}, "weaknesses": {"value": "Quality:\n- The method operates under a reasonable assumption that the training data distribution should be aligned with the test data distribution in terms of skills (Figure 2). However, this does not take into account the possibility that 1) skills may interact with each other and 2) skills are learned at different rates (as suggested in Chen et al 2023 https://arxiv.org/abs/2307.14430). \n- The continual learning extension showcases the flexibility of STAT; however, I am also curious about if optimizing on MATH-perturb-hard negatively impacts performance on the other math benchmarks. More generally, I am interested in seeing if the model forgets previous skills, and if so, whether you could make some simple adjustments to STAT (e.g., doing an exponential moving average for STAT-Sel over time) to address this.\n- I am curious about the role of composition in these results. In Figure 5, STAT-Syn determines that the missing skill is solving equations, while Embed-Syn determines that the missing skill is Ellipse Properties. Is it ever the case that the actual missing skill is a union of these skills? For instance, what is needed is not just more data focused on solving equations, but instead data focused on solving equations related to geometry? This may also touch more fundamentally on a question of skill granularity/combinatorics of skills. \n- I am curious about an \"oracle\" experiment that could track whether or not the missing skills are learned by the model. Here's a proposal: instead of using a validation set, take the test set, and use the ground truth labels to determine the difficult samples, which are just the ones that the model gets wrong. If we apply STAT-Sel/STAT-Syn, how many of the questions in the test set will flip and become correct? Does STAT interfere with the test set questions that were already answered correctly? \n\nSignificance:\n- The empirical evaluation is somewhat narrow - it relies on a single dataset and a specific skill taxonomy. This makes it unclear how well the findings generalize to other domains (e.g., coding, agentic workflows) or to different definitions of “skill.” In particular, the analysis in this paper suggests that skills are deeper than the topics identified via embeddings (e.g., in Figure 5), but it is not completely clear how to construct these skills from first principles on a new domain besides math."}, "questions": {"value": "(Taken from the Weaknesses section above)\n1. Have you considered modeling or empirically testing for interactions or uneven learning rates among skills? How might such dynamics affect the assumptions behind STAT?\n2. In the continual learning setup, how does optimizing on MATH-perturb-hard impact performance on other math benchmarks? Do you see any sort of forgetting going on and could that be fixed?\n3. Do you observe cases where the true missing capability is a composition or union of multiple skills (e.g., solving equations related to geometry)? Could STAT handle such compositional skill structures?\n4. To show STAT's improvement at the skill-level, can you consider an oracle-style evaluation where you identify difficult test samples (those currently misclassified) and measure how many become correct after applying STAT?\n5. How do you expect the method to generalize to other domains such as coding, reasoning, or agentic workflows? What guidance could you provide for defining “skills” in new domains, given that the math taxonomy here is well structured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hl2UrG91OL", "forum": "m3jG3GaNIj", "replyto": "m3jG3GaNIj", "signatures": ["ICLR.cc/2026/Conference/Submission21497/Reviewer_PEvi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21497/Reviewer_PEvi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154932033, "cdate": 1762154932033, "tmdate": 1762941805655, "mdate": 1762941805655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}