{"id": "RCjXLCLrpz", "number": 22953, "cdate": 1758337417574, "mdate": 1759896838965, "content": {"title": "Fluid Reasoning Representations", "abstract": "Traditional large language models struggle with abstract reasoning tasks. By generating extended chains of thought, reasoning models such as OpenAI's o1 and o3 show dramatic accuracy improvements. However, the internal transformer mechanisms underlying this superior performance remain poorly understood. This work presents an early mechanistic analysis of how reasoning models process abstract structural information during extended reasoning. We analyze QwQ-32B on Mystery BlocksWorld -- a semantically obfuscated benchmark that measures planning and reasoning capabilities.\nWe find that QwQ gradually improves its internal understanding of actions and concepts through its extended rollouts, developing abstract representations that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces enhances accuracy, while symbolic representations can replace many specific Mystery BlocksWorld-obfuscated encodings with minimal performance loss. We therefore find that one of the factors driving reasoning model performance is in-context refinement of token representations -- which we call Fluid Reasoning Representations. This provides early mechanistic interpretability into reasoning models.", "tldr": "Reasoning models like QwQ-32B progressively adapt their internal representations during extended reasoning to develop abstract, symbolic encodings that enable better performance on obfuscated planning tasks.", "keywords": ["LLM", "Reasoning", "Chain-of-Thought", "Mechanistic Interpretability", "Steering", "Abstract Reasoning", "Planning", "BlocksWorld"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2aae733f0eca8abbebe37d90d2067fc63835f4d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how reasoning models develop internal representations during long chain-of-thought (CoT) reasoning. Specifically, the authors analyse the QwQ-32B model on an obfuscated planning task (Mystery BlocksWorld) to test whether reasoning generalises beyond surface-level token identities. The authors hypothesise that *reasoning models progressively refine their internal representations of problem entities during reasoning, developing context-specific semantics that enable abstract structural reasoning independent of surface-level semantics*. \n\n\nThe study comprises three main analyses:\n1. *Representational Dynamics:* The authors track how hidden representations of actions and predicates evolve over reasoning timestamps and across multiple “naming” schemes, showing that representations of the same underlying concepts converge across different surface names.\n\n\n2. *Causal Steering:* To test whether these learned representations are behaviorally meaningful rather than merely correlational, the authors perform activation steering, i.e., directly injecting or perturbing hidden-state vectors during reasoning, and observe that positive steering improves accuracy while negative steering degrades it.\n\n\n3. *Symbolic Patching:* To probe abstraction, the authors replace naming-specific embeddings with averaged, naming-invariant “symbolic” representations. Performance remains stable mainly, suggesting the model’s reasoning operates in an abstract representational space."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important mechanistic question: how reasoning models internally represent abstract structure during extended reasoning.\n\n- The experimental setup is well-motivated, and the task (Mystery BlocksWorld) provides a clean testbed for analyzing abstraction.\n\n- The authors conduct multiple complementary analyses, including steering and patching interventions."}, "weaknesses": {"value": "- The paper’s central hypothesis that \"**reasoning models** dynamically refine internal representations of problem entities ...\" requires validation across more than a single model. Demonstrating the same phenomena in at least one additional reasoning model (and ideally contrasting with a non-reasoning or base variant, such as Qwen-32B) would significantly strengthen the claim.\n\n\n- The robustness and reproducibility of the results are not yet clear. Some experimental details (e.g., the choice of layers, token windows, or the 40 “solved” reference puzzles) appear somewhat arbitrary or underspecified. Including these in a reproducibility table or appendix would greatly help future replication efforts.\n\n- The writing occasionally over-generalises the findings, implying broader conclusions than the presented evidence supports. The authors should temper claims about “reasoning models” in general and clarify that observations are currently limited to QwQ-32B and the specific task.\n\n- Some results (e.g., steering improvements) are small and would benefit from clearer statistical reporting and effect sizes."}, "questions": {"value": "Please check the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8aVesqAcFm", "forum": "RCjXLCLrpz", "replyto": "RCjXLCLrpz", "signatures": ["ICLR.cc/2026/Conference/Submission22953/Reviewer_cJuV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22953/Reviewer_cJuV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756798308, "cdate": 1761756798308, "tmdate": 1762942450508, "mdate": 1762942450508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes how QwQ-32B's internal representations evolve when solving Mystery BlocksWorld, a semantically obfuscated planning benchmark. The authors extract representations of actions and predicates at different points during reasoning traces and show that representations converge toward similar encodings across different naming schemes. They conduct steering experiments where refined representations from successful traces are injected into new problems, and symbolic patching experiments where naming-specific representations are replaced with averaged vectors.\n\nThe paper does not contain any mathematical properties or theorems. The experimental setup and research methodology appears sound. \n\nOverall, the paper adapts the framework in Park et al. for planning problems. It showed that the phenomenon discovered in Park et al. can also be observed in planning problems, in particular, in the Mystery Blocks World instances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear demonstration of representational convergence: The paper effectively shows that representations become increasingly similar across namings as reasoning progresses, with divergent representations at early timestamps converging around 7k tokens. This temporal progression is well-visualized and makes the adaptation process tangible, providing genuine insight into how models refine their understanding during extended generation.\n\n2. Causal Validation via Steering: The paper successfully uses steering experiments to prove its claims. By injecting the refined representations from successful traces into new problems, the authors show these representations causally improve problem-solving accuracy. The fact that the averaged \"cross-naming\" representations (which are purely abstract) had the strongest positive effect supports the paper's central hypothesis.\n\n3. Sophisticated cross-naming methodology: Creating 15 diverse naming variants and averaging representations across them to extract symbolic encodings is a thoughtful approach to isolating abstract structural meaning from surface-level lexical information.\n\n4. Insightful base model comparison (Section 3.3): The finding that base models exhibit similar representational adaptation when processing the same traces is valuable. This helps clarify that reasoning models leverage a fundamental capability of transformers through extended generation."}, "weaknesses": {"value": "1. Insufficient novelty over Park et al. (2025): That models adapt representations during in-context learning is documented in prior work (Park et al., 2025, cited by authors). This paper essentially applies their framework to Blocks World rather than discovering reasoning-specific mechanisms.\n\n2. Single model analysis despite multi-model data: Table 1 shows results for DeepSeek-R1, Llama Nemotron, and QwQ, yet only QwQ is analyzed mechanistically. This makes it unclear if the findings are general to all models or specific to QwQ. Since the authors already ran performance benchmarks, why not apply the same representational analysis to the other models to test for generalization?\n\n3. Single domain with no generalization evidence: BlocksWorld has only a small number of concepts with deterministic rules. Zero evidence that findings extend to other reasoning domains. Testing even one additional domain is essential to demonstrate this isn't domain-specific.\n\n4. Missing explanation of refinement mechanism: The paper documents that representations converge toward symbolic encodings but does not explain the internal mechanism by which this refinement occurs. What computations cause early representations to transform into refined ones? Which attention heads read/write these representations? What information do different layers add during refinement? The paper observes the phenomenon (representations change) without explaining the process (how/why the model performs this transformation internally)."}, "questions": {"value": "Please see the weaknesses session."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sIxDtc9fGW", "forum": "RCjXLCLrpz", "replyto": "RCjXLCLrpz", "signatures": ["ICLR.cc/2026/Conference/Submission22953/Reviewer_tJg2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22953/Reviewer_tJg2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879411395, "cdate": 1761879411395, "tmdate": 1762942450294, "mdate": 1762942450294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an early mechanistic analysis of how reasoning models process abstract structural information during extended reasoning, which analyzes QwQ-32B on Mystery BlocksWorld. This paper finds that QwQ gradually improves its internal understanding of actions and concepts through its extended rollouts, developing abstract representations that focus on structure rather than specific action names. Through steering experiments, it establishes causal evidence that these adaptations improve problem solving."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides insights for abstract reasoning area.\n2. The method is somewhat novel.\n3. The discovered theory can be applied for LLMs enhancement."}, "weaknesses": {"value": "1. The presentation of this paper should be significantly improved.\n2. The experiments are limited, the conclusions are not universally applicable."}, "questions": {"value": "This method about this paper is novel, while the experiments and presetations shoule be significantly improved before acceptance.\n\n1. Figure 1 is placed after the abstract, while there is no details about Figure 1 in the introduction.\n2. There is neither a formal definition of the task nor exmples of the task.\n3. What does in-naming and cross-naming mean? What does high and low values of them represent?\n4. There should have a formal definitation of Mystery BlocksWorld.\n5. I know action and predicate in language, are they the same in your paper?\n6. The analysis are about actions and predicates, why the hypothese is about entities (such as lines 203-204)?\n7. \"we first create a set of all possible token sequences that could encode this action\". How to understand \"token sequences\" encode \"this action\"? There are many such difficult-to-understand sentences in the article.\n8. Conducting expeirments on other reasoning LLMs is helpful to make your conclusions universal.\n9. More datasets should be considered to further enhance the persuasiveness.\n10. A whole workflow is needed to better demonstrate your method, only the text can make confusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RT1X7aEEkf", "forum": "RCjXLCLrpz", "replyto": "RCjXLCLrpz", "signatures": ["ICLR.cc/2026/Conference/Submission22953/Reviewer_pnSR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22953/Reviewer_pnSR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917457036, "cdate": 1761917457036, "tmdate": 1762942450085, "mdate": 1762942450085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}