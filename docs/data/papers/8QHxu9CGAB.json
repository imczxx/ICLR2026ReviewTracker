{"id": "8QHxu9CGAB", "number": 25306, "cdate": 1758366529822, "mdate": 1763086176591, "content": {"title": "General Risk Measure meets Offline RL: Provably Efficient Risk-Sensitive Offline RL via Optimized Certainty Equivalent", "abstract": "We study the risk-sensitive reinforcement learning (RL), which is crucial in scenarios involving uncertainty and potential adverse outcomes. However, existing works on risk-sensitive RL either only focus on a specific risk measure or overlook the offline RL setting. In this work, we investigate the provably efficient risk-sensitive RL under the offline setting with a general risk measure, the optimized certainty equivalent (OCE), which captures various risk measures studied in prior risk-sensitive RL works, such as value-at-risk, entropic risk, and mean-variance. To the best of our knowledge, we (i) introduce the first offline OCE-RL frameworks and propose corresponding pessimistic value iteration algorithms (OCE-PVI) for both dynamic and static risk measures; (ii) establish suboptimality bounds for the algorithms, which can reduce to known results for risk-sensitive RL as well as risk-neutral RL with appropriate utility functions; (iii) derive the first information-theoretic lower bound of the sample complexity of offline risk-sensitive RL, matching the upper bounds and certifying optimality of our algorithms; and (iv) propose the first provably efficient risk-sensitive RL with linear function approximation for both dynamic and static risk measures, together with rigorous suboptimality bounds, yielding a scalable and model-free approach.", "tldr": "", "keywords": ["Reinforcement Learning", "Offline RL", "Risk-Sensitive", "Optimized Certainty Equivalent", "General Risk Measure"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6b63426e170cdf90412cec29d4e7971c9c42cf3c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the provably efficient risk-sensitive RL under the offline setting with a general risk measure, the optimized certainty equivalent (OCE), which captures various risk measures studied in prior risk-sensitive RL works, such as value-at-risk, entropic risk, and mean-variance. The authors (i) introduce the first offline OCERL frameworks and propose corresponding pessimistic value iteration algorithms (OCE-PVI) for both dynamic and static risk measures; (ii) establish suboptimality bounds for the algorithms, which can reduce to known results for risk-sensitive RL as well as risk-neutral RL with appropriate utility functions; (iii) derive the first information-theoretic lower bound of the sample complexity of offline risk-sensitive RL, matching the upper bounds and certifying optimality of the proposed algorithms; and (iv) propose the first provably efficient risk-sensitive RL with linear function approximation for both dynamic and static risk measures, together with rigorous suboptimality bounds, yielding a scalable and model-free approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n2. This paper is well executed and provides multiple results, including offline RL with dynamic and static OCE criteria in the tabular and linear function approximation settings."}, "weaknesses": {"value": "1. My major concern is that the technical novelty of this paper is limited. While this paper introduces the first algorithms and theoretical results for offline RL with OCE criteria in the tabular and linear function approximation settings, given that RL with the dynamic and static OCE criteria, offline RL and linear MDPs are well studied in the literature, the algorithm design and theoretical analysis in this paper seem to be a combination of existing techniques in RL with the OCE criteria, offline RL (i.e., the pessimism idea) and linear MDPs (i.e., least squares value iteration). \n2. There is no experiment provided in this paper, which limits the practicability of the proposed algorithms.\n3. In algorithm 2 for the static OCE criterion, under the augmented state space, it should be enough to consider deterministic policies? Why does Line 9 in Algorithm 2 use $argmax_{\\pi_h}$, instead of $argmax_{a \\in \\mathcal{A}}$?\n4. Minor comment: The authors should enlarge the font size in the algorithm pseudo-codes to keep it the same as that of the main text."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vl5Jo6gTYE", "forum": "8QHxu9CGAB", "replyto": "8QHxu9CGAB", "signatures": ["ICLR.cc/2026/Conference/Submission25306/Reviewer_MsaU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25306/Reviewer_MsaU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744904252, "cdate": 1761744904252, "tmdate": 1762943396274, "mdate": 1762943396274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Fen2sg6GGK", "forum": "8QHxu9CGAB", "replyto": "8QHxu9CGAB", "signatures": ["ICLR.cc/2026/Conference/Submission25306/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25306/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763086175898, "cdate": 1763086175898, "tmdate": 1763086175898, "mdate": 1763086175898, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified algorithm framework based on the general risk measure \"Optimality Criteria Equivalence\" (OCE) for the risk-sensitive offline reinforcement learning problem. The authors have designed the first provably effective offline algorithms (DOCE-PVI/SOCE-PVI) for both dynamic and static OCE formulas, and established corresponding suboptimality bounds and the first information-theoretic lower bounds for offline risk-sensitive RL. In addition, this work further proposes the first provably effective risk-sensitive RL algorithms using linear function approximation under the OCE framework (DOCE-PLSVI and SOCE-PLSVI)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper successfully introduces the OCE general risk measurement framework into the offline RL setting. This addresses a key gap in existing work that is either limited to specific risk measures (such as CVaR or entropy risk) or only focuses on online settings.\n\nThe paper clearly distinguishes between dynamic OCE and static OCE in two different settings. For the challenge of non-Markov strategies in static OCE, the authors adopt the augmented MDP (Augmented MDP) technique, which is methodologically rigorous.\n\nFor Linear Function Approximation setting, this paper considers the ridge regression, separately estimating the reward function and the expected term in the OCE.\n\nThe results recover classical offline RL bounds (e.g., Jin et al., 2021) as special cases and align with known online risk-sensitive RL lower bounds (e.g., Xu et al., 2023; Chen et al., 2023)."}, "weaknesses": {"value": "My major concerns is that the technical novelty of this paper is not clear. The theoretical machinery—pessimistic value iteration, concentration-based bonuses, and Bellman operator analysis—directly parallels those in existing risk-sensitive RL (Fei et al., 2020; Xu et al., 2023; Chen et al., 2023) and OCE-based online RL (Wang et al., 2024). It is not surprising that we can achieve efficient offline reinforcement learning with OCE. For me, the proofs in this work are merely straightforward applications of existing algorithms, which limits the technical novelty.  Therefore, I suggest the authors to incorporate more discussion to highlight the technical contribution of this paper.\n\n---\n\nWang, Kaiwen, et al. \"A reductions approach to risk-sensitive reinforcement learning with optimized certainty equivalents.\", ICML 2025."}, "questions": {"value": "see weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rAN3xMM0pQ", "forum": "8QHxu9CGAB", "replyto": "8QHxu9CGAB", "signatures": ["ICLR.cc/2026/Conference/Submission25306/Reviewer_D32H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25306/Reviewer_D32H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966830020, "cdate": 1761966830020, "tmdate": 1762943396076, "mdate": 1762943396076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies risk-sensitive offline reinforcement learning under a general risk measure known as the Optimized Certainty Equivalent (OCE). The OCE framework was originally introduced by Ben-Tal and Teboulle (2007) and it has since been shown to unify several classical risk measures, including Conditional Value-at-Risk (CVaR), entropic risk, and mean–variance formulations.\n\nThe authors propose and analyze two algorithms: Dynamic-OCE RL and Static-OCE RL. The dynamic formulation has appeared in earlier works and applies to certain classes of recursive risk measures, while the static formulation is meant to generalize to measures such as CVaR and entropic risk.\n\nAlthough the paper presents a theoretical analysis for both algorithms, the results closely mirror those established for standard episodic, risk-neutral offline RL with cumulative rewards, with minimal adaptation to the OCE framework. In essence, the work represents a direct transposition of existing episodic RL analyses into the OCE setting, without addressing the core technical challenges that make OCE-based formulations nontrivial: specifically, the necessity of state augmentation and discretization of the budget variable. Consequently, the theoretical development appears to rest on conceptually incorrect assumptions, rendering the analysis fundamentally flawed and mathematically unsound."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a problem formulation of clear contemporary interest, given the growing attention to risk-sensitive decision-making in recent years. It represents a notable attempt to unify developments in risk-neutral RL with those in risk-sensitive RL, by employing the OCE as a common analytical framework. The overall methodological structure follows a mostly sound and well-established approach, and the presentation of the algorithms is organized."}, "weaknesses": {"value": "Incomplete literature coverage.\nThe paper overlooks several recent and relevant works, particularly those presented at major conferences, that have focused on the theoretical and algorithmic understanding of the OCE framework in reinforcement learning.\n\nSuperficial treatment of discretization.\nThe authors largely brush aside the central difficulty associated with discretizing the auxiliary state variable. While finer discretization may reduce approximation error, discretization fundamentally alters the underlying MDP structure, making both the theoretical analysis and algorithmic updates more complex. In particular, the proposed update rule is internally inconsistent: the value updates at each iteration may not correspond to points in the discretized grid, thereby invalidating the recursion as written.\n\nIncorrect handling of state augmentation and scaling.\nAlthough the authors mention state augmentation in the static-OCE formulation, their theoretical bounds entirely ignore its implications. In the static case, the effective state space is the augmented space whose cardinality explicitly depends on the discretization resolution. Consequently, the bounds presented in the theorems should scale with the augmented state dimension. Because this dependence is omitted, the claimed results are mathematically inconsistent: for instance, the suboptimality bounds for static OCE cannot recover known results for special cases such as the entropic risk measure, contrary to the authors’ claims."}, "questions": {"value": "1- To achieve a vanishing approximation error, the discretization granularity of the auxiliary variable must scale with the number of trajectories or episodes. How does this dependence influence your final suboptimality bound? \n\n2- In the static-OCE formulation, the effective state space is the augmented space that includes both the original state and the budget variable. Yet, your theoretical bounds depend solely on the size of the original state space. Could you clarify why the augmented dimension does not appear in the final bound?\n\n3- Under what precise conditions is the dynamic OCE formulation applicable? Since many common risk measures (e.g., CVaR) are inherently non-recursive and therefore incompatible with dynamic OCE, could you clarify the motivation for studying this case and its practical relevance within the broader risk-sensitive RL literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TjjMHcA4MQ", "forum": "8QHxu9CGAB", "replyto": "8QHxu9CGAB", "signatures": ["ICLR.cc/2026/Conference/Submission25306/Reviewer_rJqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25306/Reviewer_rJqu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977115930, "cdate": 1761977115930, "tmdate": 1762943395824, "mdate": 1762943395824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies risk-sensitive offline reinforcement learning under the general Optimized Certainty Equivalent (OCE) framework, which encompasses CVaR, entropic, and mean-variance risk measures. The authors design two pessimistic value iteration algorithms for dynamic and static OCEs, derive the first provable sample-efficiency guarantees in offline OCE-RL, and extend the results to linear function approximation. Both upper and lower bounds are provided, showing near-optimal dependence on horizon and dataset size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and the motivation is very clear. \n- Extend the OCE to offline setup to fill the gap."}, "weaknesses": {"value": "- The main technical ideas combine well-known pessimism principles (Jin et al. 2021) with OCE-based risk modeling. The extension is valuable but not a radical theoretical leap.\n- Though the work is most theoretical, the evaluation part should consider more benchmark and more complex setup to clearly demonstrate its practicality and efficiency."}, "questions": {"value": "1. How tight are the lower bounds relative to prior results in CVaR or entropic RL?\n2. Does the proposed method remain stable when OCE’s generator function is non-convex?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L55vsp7Iiy", "forum": "8QHxu9CGAB", "replyto": "8QHxu9CGAB", "signatures": ["ICLR.cc/2026/Conference/Submission25306/Reviewer_orJR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25306/Reviewer_orJR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013181132, "cdate": 1762013181132, "tmdate": 1762943395493, "mdate": 1762943395493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}