{"id": "PZIW3oZbSf", "number": 7777, "cdate": 1758035804933, "mdate": 1759897833172, "content": {"title": "Accelerating Anchors via Specialization and Feature Transformation", "abstract": "Anchors is a popular local model-agnostic explanation technique whose applicability is limited by its computational inefficiency. To address this limitation, we propose a pre-training-based approach to accelerate Anchors without compromising the explanation quality. Our approach leverages the iterative nature of Anchors' algorithm which gradually refines an explanation until it is precise enough for a given input by providing a general explanation that is obtained through pre-training as Anchors' initial explanation. Specifically, we develop a two-step rule transformation process: the horizontal transformation adapts a pre-trained explanation to the current input by replacing features, and the vertical transformation refines the general explanation until it is precise enough for the input. We evaluate our method across tabular, text, and image datasets, demonstrating that it significantly reduces explanation generation time while maintaining fidelity and interpretability, thereby enabling the practical adoption of Anchors in time-sensitive applications.", "tldr": "", "keywords": ["Local explanation", "Model-agnostic explanation", "Anchors", "Acceleration"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c69334e04c21d5d7f97df50ef601b7f27b6d5c84.pdf", "supplementary_material": "/attachment/6a32e2406ea058493840d426ba0cca2b37ae79c3.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes an acceleration method for the Anchors explanation algorithm by leveraging pre-trained explanations and introducing two types of rule transformations: horizontal (feature substitution) and vertical (rule refinement). The goal is to reduce computation time without compromising explanation fidelity. The method is evaluated on tabular, text, and image data, showing significant runtime improvement over vanilla Anchors while maintaining comparable precision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and well organized, presenting the method, motivation, and contribution. The theoretical complexity analysis, while simple is a nice addition. The experimental evaluation is broad, covering multiple data modalities and models. The reported acceleration gains are considerable (~ 2-3 fold)."}, "weaknesses": {"value": "My main concern is novelty and lack of experimental comparisons with other fast explanation methods. The main idea is precomputing representative explanations and transferring them to similar input which is well-known and quite straightforward. The horizontal and vertical transformations largely repackage familiar notions of feature substitution and iterative refinement. Moreover, while the experiments are extensive in terms of different modalities of data, it lacks comparisons with state-of-the-art methods. Moreover, they focus mainly on runtime metrics, but it is not clear how important this save is in practice especially when it comes with some loss of coverage and precision."}, "questions": {"value": "- We ultimately want maximum coverage and precision. What is the computational gain then?\n- There are many works that build on SHAP and try to reduce its complexity. Can you provide some comparison with such works? E.g. TreeSHAP, FastSHAP, Kernel ShAP, AcME"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ugliihw7ug", "forum": "PZIW3oZbSf", "replyto": "PZIW3oZbSf", "signatures": ["ICLR.cc/2026/Conference/Submission7777/Reviewer_EAbh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7777/Reviewer_EAbh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760890921170, "cdate": 1760890921170, "tmdate": 1762919817814, "mdate": 1762919817814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper improves the Anchors explanation method through pre-computation and rule adaptation. The approach pre-trains anchors on representative inputs, then adapts them online using horizontal transformation (mapping predicates to similar features) and vertical transformation (iteratively adding predicates until precision threshold is met). Expriments on tabular, text, and image datasets demonstrate substantial speedups and sampling reductions while maintaining precision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Pre-training and transformation approach for rule-based explanations. Two-step adaptation framework seems novel.\n\nEmpirical evaluation across multiple modalities with diverse models including language and vision architectures.\n\nGood figures.\n\nDemonstrated speedups could enable deployment in time-sensitive applications."}, "weaknesses": {"value": "Some computational costs are not clearly reported."}, "questions": {"value": "What is the online similarity search overhead?\n\nWhat are the computational costs when the number of pre-trained inputs grows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rhg0LPq2xt", "forum": "PZIW3oZbSf", "replyto": "PZIW3oZbSf", "signatures": ["ICLR.cc/2026/Conference/Submission7777/Reviewer_ExhX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7777/Reviewer_ExhX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825234487, "cdate": 1761825234487, "tmdate": 1762919817201, "mdate": 1762919817201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to accelerate the Anchors explanation algorithm by pre-computing explanations for representative data points and adapting them to new online inputs. The method uses a \"horizontal transformation\" to map a pre-trained rule to the feature space of a new input and a \"vertical transformation\" to refine the rule to the required precision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors argue that the inefficiency of Anchors is a major barrier to its use in time-sensitive applications. An effective acceleration method, as proposed here, would be a valuable contribution, potentially enabling wider adoption of this high-fidelity explanation technique.\n- The formulation of the two-step horizontal and vertical transformation process provides a novel and structured framework for the non-trivial challenge of adapting rule-based explanations between similar inputs.\n- The proposed methodology is technically sound and is evaluated thoroughly across multiple data modalities (tabular, text, and image) and modern model architectures (e.g., Llama, YOLOv8). \n- The authors rightly evaluate not only the efficiency gains (time and sampling reduction) but also the fidelity (precision and coverage) of the resulting explanations, providing a balanced view of the method's performance."}, "weaknesses": {"value": "- The paper's primary contribution is not a new explanation method but a technique to improve the computational efficiency of the existing Anchors algorithm. While practical, the core idea of using pre-computation and adapting results for similar inputs is an incremental engineering contribution rather than a novel one.\n- The paper frames \"vertical transformation\" as a distinct contribution. However, this step is functionally identical to the standard Anchors algorithm's iterative refinement process, which also works by incrementally adding predicates to increase precision. The only difference is that it begins from a non-empty rule provided by the horizontal transformation step.\n- This acceleration technique alters the output of the original Anchors algorithm. The authors claim fidelity is preserved, but the experimental results show that the explanations generated by the accelerated method consistently have slightly lower coverage than those from the original algorithm. This indicates a direct trade-off between computational speed and the generality of the explanation, which is not sufficiently discussed as a core limitation of the proposed method.\n- The analysis neglects the substantial offline pre-training cost.  The paper should discuss the amortization of this cost to provide a fair assessment of the overall efficiency.\n- Definition 2.2, which defines the perturbation distribution, is mathematically incorrect as written. The expression $D_{x}(z|r):=D_{x}(z)\\cdot1[r(z)=1]$ does not define a valid probability distribution because it lacks a normalization factor and will not integrate to 1. The definition should either state that the conditional distribution is proportional to the right-hand side (e.g., using $\\propto$) or include an explicit normalization constant.\n- Many in-text parenthetical citations do not follow standard formatting. For instance, the first sentence should read \"Anchors (Ribeiro et al., 2018)\" instead of \"Anchors Ribeiro et al. (2018)\"."}, "questions": {"value": "- The paper reports a significant offline pre-training cost of approximately 300 hours, but does not include this in the overall efficiency evaluation. To better understand the practical utility of the method, could you provide a break-even analysis? Specifically, how many online explanations are needed on average to amortize the initial pre-training cost for the different experimental setups?\n- The method's success seems highly dependent on finding a \"good\" similar pre-trained sample, which relies on the $K$-means clustering for selecting representative inputs and the $\\\\textrm{Dist}(\\cdot,\\cdot)$ function for measuring similarity. Could you comment on the sensitivity of the results to these choices? An ablation study on these components would be very insightful.\n- The authors note that some inputs see \"limited optimization\". This is a key finding that warrants a deeper investigation. An analysis of these \"hard\" instances would provide valuable insights into the method's limitations. Are these inputs from sparse regions of the data distribution that are poorly represented by the pre-training set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dxrUhJ5PbK", "forum": "PZIW3oZbSf", "replyto": "PZIW3oZbSf", "signatures": ["ICLR.cc/2026/Conference/Submission7777/Reviewer_uNx9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7777/Reviewer_uNx9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154836565, "cdate": 1762154836565, "tmdate": 1762919816793, "mdate": 1762919816793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an acceleration framework for the model-agnostic explanation method Anchors (Ribeiro et al., 2018), which is known for producing rule-based local explanations but suffers from high computational cost. The authors introduce a two-phase approach that pre-trains explanation rules on representative inputs and reuses them through horizontal and vertical transformations. The horizontal transformation substitutes features in a pre-trained rule with similar features in a new input, while the vertical transformation refines the adapted rule to meet desired precision thresholds. The method is evaluated across tabular, text, and image datasets and reportedly achieves 1.8×–2.7× speedups over the original Anchors while maintaining similar precision and coverage."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The motivation to improve Anchors’ runtime efficiency is valid and timely, as Anchors remains one of the most computationally expensive local explanation methods. The proposed idea of leveraging pre-trained anchors is intuitive, and separating adaptation into two distinct transformation steps: horizontal (feature substitution) and vertical (rule refinement), is a conceptually clean way to formalize the reuse of prior rules. The method is clearly described with well-structured pseudocode and mathematical notation, and the empirical section covers diverse data modalities (tabular, text, and images)."}, "weaknesses": {"value": "- The technical novelty is weak and the methodological contribution marginal. The proposed framework essentially constitutes a retrieval-and-refinement scheme on top of Anchors rather than a new algorithmic insight into explanation generation. The “horizontal transformation” merely substitutes features based on nearest-neighbor distance (e.g., cosine distance for embeddings), while the “vertical transformation” directly mirrors Anchors’ existing iterative refinement loop. As a result, most computational gains come from reusing pre-trained results rather than any fundamental acceleration of Anchors’ core sampling procedure. This reusability is straightforward and not theoretically justified beyond heuristic matching.\n\n- The paper also lacks rigorous analysis or guarantees on fidelity preservation. The claim that explanations “maintain precision and interpretability” is supported only by empirical comparisons with small performance gaps (Figure 4, p.9) but no statistical validation. No evaluation of interpretability quality (e.g., rule stability, human consistency, or semantic coherence) is conducted. The theoretical complexity analysis (3.4) is simplistic and largely tautological, stating speedup as a function of how many predicates are reused (d of k) without analyzing how d depends on data similarity or pre-training coverage.\n\n- The experimental evidence is underwhelming: improvements of 1.5–2.7× are modest considering the high overhead of Anchors (which can take hours per explanation). Moreover, the “pre-training” process itself runs full Anchors computations on a subset of data, which offsets part of the claimed savings—this cost is not accounted for in total runtime or energy consumption. The method’s dependence on dataset-specific pre-training makes it impractical for dynamic or online applications, undermining its claim of “time-sensitive deployment.”\n\n- Conceptually, the paper overstates originality. The idea of using pre-trained or cached explanations parallels existing work in amortized explainability (e.g., Jethani et al., 2022, Covert et al., 2024) and explanation transfer methods that learn mappings between explanations across inputs. Those studies formalize explanation reuse through optimization or meta-learning frameworks, whereas this paper relies on ad hoc substitution and manual refinement steps without learning or generalization guarantees.\n\n- Finally, the writing suffers from redundancy and inflation: much of Sections 2–3 restates Anchors’ original definitions verbatim (precision, coverage, rule definition, algorithm workflow), with minimal technical additions. The use of formalism adds apparent complexity but little substance. Overall, the paper reads as an incremental engineering adaptation rather than a novel research contribution."}, "questions": {"value": "- How is the distance metric for “similar inputs” (used in horizontal transformation) chosen or tuned for each modality? Does performance depend heavily on embedding quality?\n\n- How is the pre-training cost amortized? If Anchors must be fully run on N pre-training inputs, does this offset runtime benefits for large-scale applications?\n\n- Does the method provide any theoretical guarantee that precision is preserved after feature substitution?\n\n- How robust is the horizontal transformation under semantic shifts (e.g., opposite sentiment words like good vs. bad that are close in embedding space)?\n\n- Could learned meta-models (as in amortized or parametric explainers) achieve similar or better acceleration with fewer assumptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4n7MNtlsQV", "forum": "PZIW3oZbSf", "replyto": "PZIW3oZbSf", "signatures": ["ICLR.cc/2026/Conference/Submission7777/Reviewer_1z6i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7777/Reviewer_1z6i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189720019, "cdate": 1762189720019, "tmdate": 1762919816173, "mdate": 1762919816173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}