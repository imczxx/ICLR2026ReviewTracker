{"id": "3DOgmfZ2k6", "number": 14814, "cdate": 1758244292440, "mdate": 1763724678347, "content": {"title": "Spatial Structure and Selective Text Jointly Facilitate Image Clustering", "abstract": "Image clustering is a fundamental task in visual machine learning, aiming to group unlabeled images into semantically meaningful clusters. A key research direction in this field is the incorporation of prior knowledge. Recently, such prior knowledge has evolved from internal compactness constraints to external textual guidance. In particular, the introduction of textual modalities through CLIP has demonstrated impressive performance. However, CLIP is designed primarily for image–text alignment and may not be sufficient to capture clustering structures. Moreover, existing approaches often assume that textual features are universally beneficial, overlooking their varying suitability for different datasets. To address these issues, we propose to use spatial structure and selective text to jointly facilitate image clustering (SATC). Specifically, we design a graph attention network (GAT)-based encoder to capture relational dependencies among image patches, thereby extracting spatial features to facilitate clustering. In addition, we introduce a textual feature selector that uses the potential clustering compactness of textual features as the selection criterion and adaptively integrates them into the clustering process. Finally, the cluster assignment is produced through Tri-modal mutual distillation. Extensive experiments on 18 benchmark datasets demonstrate the effectiveness of SATC. The experimental results further verify the rationality of the textual feature selector. The code will be published.", "tldr": "", "keywords": ["Image clustering"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d9103b15cf7e1cd8c73cbaeb1db02eebd2a8a50e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the task of image clustering. It introduces a framework named **Spatial Structure and Selective Text Jointly Facilitate Image Clustering, a novel framework desigend to overcome the limitations of existing CLIP-based deep clustering methods, particularly regarding their representation of spatial structures and indiscriminate use of text features.** \n\nThe framework fuses visual, spatial, and selectively chosen text features using a designed Tri-modal mutual distillation strategy. \n\nExtensive experiments across 18 benchmarks show SATC’s good performance and effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Superior Clustering Performance: The proposed method consistently achieved highest clustering results compared to extensive prior works across 18 benchmarks, showing substantial improvements.\n2. The idea of incorporating spatial structure is interesting and effective.\n3. The idea of textual selection is also insightful\n4. Efficiency and Scalability: SATC not only achieves higher clustering accuracy but also maintains competitive or even lower running times compared to the TAC baseline across most datasets"}, "weaknesses": {"value": "**Potential Flag:** The Use of Large Language Models (LLMs) is **not disclosed** in the current manuscript, which is a violation of the new rule imposed by ICLR this year.\n\n**W1:** Experiments on novel, unseen datasets are needed. All evaluated datasets in the current work might be explicitly leveraged during CLIP training. Therefore, it is hard to confirm the effectiveness of the proposed framework without control experiments on complete unseen, novel images. It is practical and important because clustering methods are often used to explore and understand unseen, novel images without labels.\n\n**W2:** The authors discussed the pros and cons of visual and textual modalities in CLIP, and their effects on image clustering. Why not compare with DINOv2, v3 for image clustering under the same model size? A comparison with DINOv2 and v3 that uses a single modality is necessary. KMeans + DINOv2 / v3 features is a good baseline.\n\n**W3:** The textual feature selector and tri-modal objective function are both depent on empirically set threshold $\\tau$ and $\\alpha$. How do the authors select these hyperparameters? What are the selection criteria or dataset? The authors mentioned it is “based on extensive experiments” at Line#331,  however, If all parameters tuned on the  test set, it is unfair.\n\nIf the above primary concerns could be addressed during the discussion stage, the reviewer is open to rise the rating."}, "questions": {"value": "**Q1:** The Tri-modal mutual distillation framework utilizes three loss types: distillation ($L_{distill}$), consistency ($L_{consist}$ with $\\lambda_1=1.0$), and entropy ($L_{entropy}$ with $\\lambda_2=5.0$). What specific theoretical or empirical justification underpins the choice to set the **weight of the entropy loss ($\\lambda_2$) five times higher** than the consistency loss ($\\lambda_1$)?\n\n**Q2:** The final cluster assignments are consistently produced by the **distilled visual cluster head**. How would the clustering performance be affected if the final assignment were instead derived from the distilled spatial cluster head or the distilled textual cluster head, especially given that mutual distillation is shown to be superior overall?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m5aj2NwDoU", "forum": "3DOgmfZ2k6", "replyto": "3DOgmfZ2k6", "signatures": ["ICLR.cc/2026/Conference/Submission14814/Reviewer_nD9y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14814/Reviewer_nD9y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760707247469, "cdate": 1760707247469, "tmdate": 1762925165062, "mdate": 1762925165062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Based on the externally guided clustering paradigm, this paper further leverages spatial information to distinguish visually and textually similar instances. The proposed method is extensively evaluated on 18 image clustering datasets, demonstrating superior performance over previous studies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The contribution of this work is clear, i.e., leveraging the spatial feature in addition to visual and textual features to facilitate image clustering. Such a motivation is straightforward.\n2. Extensive experiments across 18 datasets demonstrate the effectiveness of the proposed method.\n3. The ablation study on incorporating textual semantics with the compactness metric is interesting."}, "weaknesses": {"value": "1. The writing in section 3.1 is confusing. Where exactly is the graph attention applied? On different images, or on patches within a single image?\n2. Besides the pre-trained CLIP model, a pre-trained ResNet-50 model is also utilized in the proposed method. It is questionable why ResNet-50 is needed, since CLIP could already extract both image- and patch-level features. Does the performance improvement of the proposed method come from introducing the ResNet model?\n3. How are the textual compactness metrics in Eq. 5 used? It should be explained more clearly in the subsection.\n4. Since the proposed SATC is more efficient than TAC, experimental results on the full ImageNet-1K are expected."}, "questions": {"value": "My major concerns lie in whether the performance gain comes from the proposed method additionally leverages a pre-trained ResNet-50 model. Besides, some details of the proposed method should be explained more clearly. I will raise my score if my concerns are well addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UOH5ZziBOZ", "forum": "3DOgmfZ2k6", "replyto": "3DOgmfZ2k6", "signatures": ["ICLR.cc/2026/Conference/Submission14814/Reviewer_eiRh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14814/Reviewer_eiRh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761016368163, "cdate": 1761016368163, "tmdate": 1762925164014, "mdate": 1762925164014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work combines three different modalities (visual, spatial, textual) to enhance clustering performance on a variety of image datasets. One key contribution is the newly introduced spatial modality that encodes relationships between image patches. To effectively leverage the different modalities a new framework is introduced to enforce cross-modal alignment in image clustering. Additionally, the authors establish an adaptive textual feature selector that estimates the benefits of using textual features during clustering. This prevents performance degradation on datasets where textual descriptions are uninformative or misleading. The experiments report SOTA performance across the vast majority of the 18 datasets."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The novel textual feature extractor is well motivated and proves to be of great benefit to the clustering performance\n2. Comprehensive empirical validations were made on various datasets\n3. SOTA results on a vast majority of datasets"}, "weaknesses": {"value": "1. The results are missing standard deviations to estimate the actual statistical significance of the proposed method\n2. Are spatial features really a contribution or could visual-textual be sufficient? An ablation studies on the impact of the addition of spatial features for clustering would be great.\n3. The compactness metric threshold appears to be found through exhaustive search rather than principled derivation\n4. Typo in 324/328."}, "questions": {"value": "1. Please provide the standard deviations for all datasets to estimate the statistical significance of the reported improvements.\n2. What specific relational dependencies do the spatial features capture that CLIP's ViT doesn't already encode?\n3. How was the 0.33 threshold determined? Based on the train/val split or post-hoc on the test data? Why does the usage of textual features need to be a binary decision rather than being modeled by weights determining their impact?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i9jZkaRpvA", "forum": "3DOgmfZ2k6", "replyto": "3DOgmfZ2k6", "signatures": ["ICLR.cc/2026/Conference/Submission14814/Reviewer_GM72"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14814/Reviewer_GM72"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964903553, "cdate": 1761964903553, "tmdate": 1762925163404, "mdate": 1762925163404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SATC (Spatial structure and Selective Text for Clustering), a tri-modal image clustering framework integrating visual, spatial, and textual information. It employs a GAT-based spatial encoder to capture relational dependencies among image patches and a compactness-aware textual feature selector to adaptively incorporate useful textual cues. These modalities are fused through tri-modal mutual distillation to improve clustering quality. Experiments on 18 benchmark datasets show that SATC consistently outperformed state-of-the-art methods such as TAC and SPICE in accuracy, robustness, and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Originality:\nThe idea of combining spatial structure modeling with selective textual guidance offers a reasonable and incremental improvement over existing CLIP-based clustering frameworks.\n2.Quality:\nThe methodology is technically sound and well-executed. The design of the spatial encoder, textual selector, and tri-modal distillation is coherent, and the experiments are comprehensive.\n3.Clarity:\nThe paper is generally well-written and logically structured. The framework and algorithms are clearly explained, supported by intuitive figures and detailed appendices.\n4.Significance:\nThe proposed method achieves consistent improvements across 18 datasets, showing robustness and general applicability. The approach offers a practical advancement in multi-modal unsupervised image clustering."}, "weaknesses": {"value": "1. Limited theoretical grounding for the compactness threshold (τ=0.33) — while empirically validated, a more formal justification or sensitivity analysis would strengthen the argument.\n2. Underdeveloped analysis of failure cases: The paper could better analyze cases where text features hurt performance (e.g., CIFAR-10), which would strengthen the argument for “selectivity.”\n3. Comparative baselines: Recent multi-modal clustering approaches beyond TAC (e.g., self-supervised multi-modal alignment models from 2024–2025) are not included."}, "questions": {"value": "1. Apart from TAC, are there any newer multi-modal clustering methods, such as the multi-modal alignment model for 2024-2025? Why aren't these methods taken into account for comparison?\n2. The paper mentions the use of Graph Attention networks (GAT) to capture the spatial relationships between image patches, but does not elaborate on why GAT was chosen instead of other types of graph neural networks or transformer. What is the basis for choosing GAT?\n3. The paper mentions that text feature selection is based on compactness (τ) and sets a fixed threshold (τ = 0.33). If the threshold is lower than this, the use of text information is abandoned, and only spatial and visual information is used. In this case, the authors believe that the text information may have provided negative benefits. However, the text-guided image clustering methods have been affirmed in some compared papers. How can this be explained?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q9nTA7luAr", "forum": "3DOgmfZ2k6", "replyto": "3DOgmfZ2k6", "signatures": ["ICLR.cc/2026/Conference/Submission14814/Reviewer_dWnF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14814/Reviewer_dWnF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974160277, "cdate": 1761974160277, "tmdate": 1762925162987, "mdate": 1762925162987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their constructive feedback. We are grateful that reviewers recognized SATC's novel integration of spatial structure and selective text, its consistent gains across 18 benchmarks, and its technical soundness. We have tried our best to address the review’ comments and questions.\n\n---\n\n### Key Improvements and Clarifications\n\n1. Method Component Analysis\n\n- Textual Component: analyzed cases where textual features hurt performance from two perspectives.\n\n- Spatial Component: validate the contributions of the two components (ResNet-50 and GAT) for generating spatial features.\n\n2. Extended Evaluation and Analysis\n\n- Added novel clustering methods (Turtle, GranNorm, LFSS) and a strong baseline (KMeans + DINOv2).\n\n- Included performance evaluation on ImageNet-1K dataset.\n\n- Added standard deviations and statistically significant analysis.\n\n- Analyzed the choice of cluster head for the final assignment.\n\n3. Clarifications on Methodology\n\n- Corrected minor errors in the article.\n\n- Clarified GAT’s target for patch-level modeling.\n\n- Clarified how to use the textual compactness metric.\n\n---\n\n### New Additions in the Revised Version\n\n1. We have added strong baselines (DINOv2 + K-means) in Table 1. Asked by Reviewer-nD9y.\n\n2. We have included recent multi-modal clustering methods (Turtle, GradNorm, LFSS) in Table 1, following the suggestion of Reviewer dWnF.\n\n3. We have added Section 4.3.2 to analyse spatial modeling architectures, addressing comments from Reviewers dWnF and eiRh..\n\n4. We have included Top-30 discriminative nouns for four representative datasets in Appendix I to analyze failure cases, as requested by Reviewer dWnF.\n\n5. We have added Appendix J, which analyzes TAC performance across all 18 datasets to demonstrate the dataset-dependent utility of textual features, addressing Reviewer dWnF's query.\n\n6. We have added Appendix K to present clustering performance across different cluster heads, addressing Reviewer nD9y's query.\n\n\n---\n\nPlease see our detailed responses below each review, and all the changes in the revised paper are highlighted. We appreciate your feedback and welcome any additional questions.\n\n---"}}, "id": "RoQf2taTdA", "forum": "3DOgmfZ2k6", "replyto": "3DOgmfZ2k6", "signatures": ["ICLR.cc/2026/Conference/Submission14814/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14814/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission14814/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763654327962, "cdate": 1763654327962, "tmdate": 1763654327962, "mdate": 1763654327962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their constructive feedback. We are grateful that reviewers recognized SATC's novel integration of spatial structure and selective text, its consistent gains across 18 benchmarks, and its technical soundness. We have tried our best to address the review’ comments and questions.\n\n---\n\n### Key Improvements and Clarifications\n\n1. Method Component Analysis\n\n- Textual Component: analyzed cases where textual features hurt performance from two perspectives.\n\n- Spatial Component: validate the contributions of the two components (ResNet-50 and GAT) for generating spatial features.\n\n2. Extended Evaluation and Analysis\n\n- Added novel clustering methods (Turtle, GranNorm, LFSS) and a strong baseline (KMeans + DINOv2).\n\n- Included performance evaluation on ImageNet-1K dataset.\n\n- Added standard deviations and statistically significant analysis.\n\n- Analyzed the choice of cluster head for the final assignment.\n\n3. Clarifications on Methodology\n\n- Corrected minor errors in the article.\n\n- Clarified GAT’s target for patch-level modeling.\n\n- Clarified how to use the textual compactness metric.\n\n- Provided a theoretical guidance for utilizing textual compactness\n\n---\n\n### New Additions in the Revised Version\n\n1. We have added strong baselines (DINOv2 + K-means) in Table 1. Asked by Reviewer-nD9y.\n\n2. We have included recent multi-modal clustering methods (Turtle, GradNorm, LFSS) in Table 1, following the suggestion of Reviewer dWnF.\n\n3. We have added Section 4.3.2 to analyse spatial modeling architectures, addressing comments from Reviewers dWnF and eiRh.\n\n4. We have included Top-30 discriminative nouns for four representative datasets in Appendix I to analyze failure cases, as requested by Reviewer dWnF.\n\n5. We have added Appendix J, which analyzes TAC performance across all 18 datasets to demonstrate the dataset-dependent utility of textual features, addressing Reviewer dWnF's query.\n\n6. We have added Appendix K to present clustering performance across different cluster heads, addressing Reviewer nD9y's query.\n\n7. We have added Appendix L to offer a a theoretical guidance for utilizing textual compactness, addressing comment from Reviewer dWnF.\n\n---\n\nPlease see our detailed responses below each review, and all the changes in the revised paper are highlighted. We appreciate your feedback and welcome any additional questions.\n\n---"}}, "id": "RoQf2taTdA", "forum": "3DOgmfZ2k6", "replyto": "3DOgmfZ2k6", "signatures": ["ICLR.cc/2026/Conference/Submission14814/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14814/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission14814/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763654327962, "cdate": 1763654327962, "tmdate": 1763725570217, "mdate": 1763725570217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}