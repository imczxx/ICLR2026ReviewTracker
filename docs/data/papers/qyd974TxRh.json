{"id": "qyd974TxRh", "number": 23439, "cdate": 1758343758059, "mdate": 1759896814779, "content": {"title": "UNISE: Unified Noise-Invariant Learning for Speech Enhancement toward Improved Content Preservation", "abstract": "The importance of semantic information in speech enhancement (SE) has recently been emphasized to improve intelligibility, whereas earlier work primarily focused solely on acoustic perceptual quality. To address this, recent approaches leverage pre-trained self-supervised representations, which have shown strong performance on discriminative tasks. However, such representations are less effective for generative tasks, and since they are typically trained only on clean data, they struggle to fully preserve content under noisy or distorted conditions. In this work, we aim to bridge this gap by introducing a unified generative SE model, called UNISE, that incorporates noise-invariant representation learning through contrastive training. By jointly learning an encoder with a noise-invariant bottleneck and a generative decoder, our model produces robust speech representations well suited for the SE task.\nAs a result, it achieves improved linguistic content preservation while maintaining competitive perceptual quality.", "tldr": "A unified framework noise-invariant representation learning and generative speech enhancement for improved content preservation", "keywords": ["Speech enhancement", "semantic information", "self-supervised learing", "noise robustness"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/71e23eb30ee431d5e0d41a42f4e2d6337351da68.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a \"unified generative SE\" model called UNISE. The model aims to incorporate a noise-invariant representation to improve robustness and content preservation. The noise-invariant representation is achieved by defining a learning target that maps multiple views of the same clean speech under different distortions to the same target cluster. The clustering objective is employed after each layer of the proposed encoder. A syllabic label prediction task is used as an auxiliary loss to improve content preservation. The noise invariant encoder is combined with a generative decoder employing flow-matching. The authors compare their model to multiple other competing models and demonstrate that their approach yields benefits in content preservation against most of the tested models. They legitimize their findings by multiple ablations or further tests on ASR performance, different loss combinations and a consideration of mutual information between their learned embedding and a phone target across multiple SNRs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The problem of content preservation in generative models is indeed a relevant and timely research topic, which makes the proposal interesting for the community.\n\nThe proposed method follows past research and expands upon it in a plausible way. The proposed method achieves a reduction in word error rate over compared generative speech enhancement models while achieving comparable DNSMOS and SpkSim values, indicating preserved quality. \n\nWe appreciated the audio samples, which support the claim of increased content preservation in contrast to the also featured SGMSE.\n\nAppendix 2 is a welcome additional analysis comparing the mutual information between the encoder output and corresponding phone labels, which shows a high degree of mutual information  that remains high under low SNRs in case of the proposed model."}, "weaknesses": {"value": "While the paper is well-written w.r.t. phrasing, grammar and overall clarity, aspects of the presentation inhibit effortless understanding. \n\nMathematical notations and figures have alignment issues.\n- Section 2.1 introduces output representations z_a and z_b but Figure 2 uses different labels z_1 and z_2 for presumably the same concept? \n- Equation (3) includes a summation over k, which is not explicitly defined. \n- The pseudo-label prediction task is defined as y but y is never referred anywhere. The same goes for the Sylber features s, which do not occur in any figure or equation thereafter.\n- It is furthermore unclear what the index c in Equation (1) means. Defining (capital) C as a learnable codebook does not obviously define the summation.\n\nWe find that a very muc h intensified connection between the figures and their textual description is necessary. In the aforementioned cases, the missing definition of symbols made a verification of mathematical correctness impossible.\n\nFurthermore, model sizes are not included in Table 1 and Table 2. It is therefore hard for the reader to separate architecture and model design from model size. The textual discussion references model size on multiple occasions, but no number for the parameter count is mentioned for any model.\n\nThe dataset that Table 1 reports on is only implied to be the DNS Challenge test set in Section 3.2. The table's caption should state the dataset clearly.\n\nIn Appendix A.1, you state \"As shown in Table 6, our model achieves better results than common representation.\" but Table 6 clearly shows most models beating your model in all metrics. There must be a mistake in either the table or your argument.\n\nThe appendix is not referenced anywhere in the text. If space permits an introductary sentence in the main text helps to contextualize the appendix."}, "questions": {"value": "The discussion of Table 2 is not sufficiently discussing the two test sets independently. It is only stated that UNISE achieves best WER for linguistic reconstruction. However it slightly falls behind HiFi-GAN-2 on the EARS test set. On EARS---with DNSMOS only slightly higher than HiFi-GAN-2 and SpkSim slightly below HiFi-GAN-2---the benefit of UNISE over HiFi-GAN-2 is not self-evident. Could you clarify your phrasing in this respect?\n\nAlso, did your analysis show a reason for the less pronounced benefit on EARS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NAsZVdsZEp", "forum": "qyd974TxRh", "replyto": "qyd974TxRh", "signatures": ["ICLR.cc/2026/Conference/Submission23439/Reviewer_rWcK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23439/Reviewer_rWcK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765010361, "cdate": 1761765010361, "tmdate": 1762942661191, "mdate": 1762942661191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework that combines noise-invariant self-supervised representation learning with generative speech enhancement to address limitations of existing methods. Experimental results demonstrate empirical performance outperforming baselines on WER for content preservation while maintaining competitive perceptual quality and speaker similarity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies two unresolved challenges in SE using SSL representations, noise robustness and generative capability, and proposes a unified solution. This addresses a critical gap between discriminative SSL and generative SE.\n\n- The noise-invariant contrastive learning explicitly disentangles speech content from noise, a improvement over prior noise-augmented SSL that only mitigates noise rather than isolating content.\n\n- The joint training of encoder and flow-matching decoder creates a feedback loop: the encoder learns content-preserving features guided by the decoder’s reconstruction loss, while the decoder uses these features to avoid hallucinations."}, "weaknesses": {"value": "- The paper states Sylber provides \"sparse syllabic embeddings\" but does not clarify: (i) how Sylber features are extracted from clean speech, (ii) why syllabic pseudo-labels are more effective than phonetic or word-level labels for content preservation, (iii) how non-speech frames are \"filled with null tokens\", what defines a non-speech frame?\n\n- The paper emphasizes UNISE’s strength in WER but does not address a critical question: Does the focus on WER come at the cost of perceptual quality in extreme noise? For example:In Table 1, UNISE’s DNSMOS score (3.334) is lower than FlowSE’s (3.601) under non-reverberant conditions. Is this a necessary tradeoff for better WER, or can the model be adjusted to improve both?\n\n- UNISE’s encoder has lower mutual information with speaker labels than WavLM/HuBERT, but the model still achieves competitive speaker similarity (Table 1). The paper attributes this to \"acoustic features in the decoder,\" but this is vague."}, "questions": {"value": "- Noise-Invariant Contrastive Learning DetailsCodebook Design: The paper mentions a learnable codebook with V=2048 codewords but provides no details on: (i) how codewords are initialized (e.g., random vs. pre-clustered on clean speech), (ii) how optimal transport \"smooths target distributions\", what OT cost function is used? (iii) why V=2048 was chosen (ablation over codebook size would strengthen this choice)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ud1Ilgh8VG", "forum": "qyd974TxRh", "replyto": "qyd974TxRh", "signatures": ["ICLR.cc/2026/Conference/Submission23439/Reviewer_zNt5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23439/Reviewer_zNt5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882101154, "cdate": 1761882101154, "tmdate": 1762942660903, "mdate": 1762942660903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified generative speech enhancement method that aims to improve both acoustic quality and linguistic content preservation. The authors argue that prior approaches mainly emphasize perceptual quality while overlooking semantic consistency. To address this, they introduce an encoder trained with a contrastive loss to learn noise-invariant representations, coupled with a generative decoder that reconstructs enhanced speech from this bottleneck. Experimental results on several benchmarks show moderate improvements in content-preservation metrics. However, the evaluation is limited to speech enhancement and speech recognition tasks, which do not sufficiently demonstrate the generality of the proposed method. Moreover, the ASR performance on the CHiME-4 dataset remains notably below the mainstream average level."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important and somewhat under-explored aspect of speech enhancement, content/semantic preservation rather than just clean sounding output. The framework is presented as unified representation learning + generation rather than simply plugging in a new decoder. The provided experimental results demonstrated the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The novelty appears limited. While combining contrastive representation learning with generative speech enhancement is interesting, the method largely builds upon existing methods without introducing a clear architectural or theoretical innovation.\n2. The description of the noise-invariant encoder and the contrastive training setup is insufficiently detailed. It is unclear how noise invariance is explicitly enforced, how positive and negative pairs are constructed, and how this representation interacts with the generative decoder during training and inference.\n3. The experimental validation needs improvement. It is unclear whether the reported improvements are statistically significant or robust across various corruption types and severities. The evaluation covers a narrow set of datasets and noise conditions, which raises concerns about the model’s generalization to unseen corruptions such as reverberation, clipping, or codec artifacts. The claim of enhanced content preservation is not well supported, as the ASR performance on the EARS test sets is even lower than that of the unprocessed noisy speech."}, "questions": {"value": "1. Could the authors clarify the exact formulation of the contrastive loss? Specifically, how are the positive and negative pairs defined, e.g., clean vs. noisy, same vs. different speakers, or different noise types?\n2. What types and severities of corruptions are considered in the evaluation? \n3. How can “content preservation” be quantitatively measured? Are intelligibility metrics such as word error rate or human listening tests used to verify that linguistic content is better preserved?\n4. How can the authors demonstrate that the observed gains originate from the proposed noise-invariant representation, rather than from increased model capacity or a stronger decoder architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lJ4149O1D9", "forum": "qyd974TxRh", "replyto": "qyd974TxRh", "signatures": ["ICLR.cc/2026/Conference/Submission23439/Reviewer_vPdR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23439/Reviewer_vPdR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974404279, "cdate": 1761974404279, "tmdate": 1762942660413, "mdate": 1762942660413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UNISE (Unified Noise-Invariant learning for Speech Enhancement), which jointly trains (1) a contrastive, noise-invariant representation encoder (inspired by SwAV / swapped prediction with Sylber pseudo-labels) and (2) a Flow-Matching based generative decoder conditioned on the encoder’s latent to improve speech enhancement while better preserving linguistic/semantic content. Experiments are reported on multiple benchmarks (DNS-Challenge, VoiceBank-DEMAND, EARS, CHiME-4) comparing regression, diffusion, language-model-based, and other generative approaches. The authors show notable WER improvements and competitive perceptual metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation addressing semantic loss/hallucination in generative SE.\n2. A joint training framework (encoder + conditional flow decoder) with supporting ablation studies. \n3. Empirical improvements in WER while maintaining competitive perceptual metrics, suggesting a good trade-off between intelligibility and quality."}, "weaknesses": {"value": "1. Reproducibility details missing: many hyperparameters and exact procedures (codebook training, Sinkhorn params, pseudo-label pipeline, training compute/time, seeds) are not fully specified.\n2. Subjective evaluation lacks clarity: If human listening tests were conducted, their protocol is not sufficiently described; if not, the absence of blind subjective tests is a limitation. \n3. Failure cases: Some benchmarks (e.g., CHiME-4) show that UNISE is not always best; the manuscript’s discussion of limitations and failure modes is brief. \n4. Ambiguous effect of codebook / pseudo-label design choices. Key design parameters (codebook size, k-means clusters, pseudo-label source data) may strongly influence representation quality; the paper lacks comprehensive ablation experiments to validate its robustness.\n5. No clear error analysis for cases where WER degrades or hallucinations occur."}, "questions": {"value": "1. For the clustering/codebook and Sinkhorn steps: what were the exact hyperparameters (ε, number of iterations, temperature τ)? How was the collapse prevented? How sensitive is the model to the temperature or codebook size in contrastive learning?\n2. Does joint optimization ever cause overfitting to certain noise types or degrade generalization to unseen environments?\n3. Please detail the Sylber pseudo-label generation: which datasets were used to train the k-means, initialization method, how frame-level alignment was handled, and any preprocessing used. Would alternative pseudo-labels change results? \n4. Why do you freeze the encoder during finetuning? What happens if the encoder is unfrozen for end-to-end finetuning—does performance improve or degrade? Please report experiments. \n5. Please include more failure-case examples and a deeper analysis of when and why semantic hallucinations or distortions occur, and possible mitigation strategies. \n5. Have the authors explored scaling (e.g., larger encoder or DiT-like decoder) and its effect on robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zzw4twmIa0", "forum": "qyd974TxRh", "replyto": "qyd974TxRh", "signatures": ["ICLR.cc/2026/Conference/Submission23439/Reviewer_Qwzv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23439/Reviewer_Qwzv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162394718, "cdate": 1762162394718, "tmdate": 1762942660112, "mdate": 1762942660112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}