{"id": "5p1Ucqfd1A", "number": 23393, "cdate": 1758343098244, "mdate": 1759896817078, "content": {"title": "CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark", "abstract": "Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no comprehensive benchmark for this task, especially regarding wearables scenarios. To fill this gap, we present CRAG-MM---a Comprehensive RAG benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn conversations across 13 domains, including 6.2K egocentric images designed to mimic captures from wearable devices. We carefully constructed the questions to reflect real-world scenarios and challenges, including five types of image-quality issues, six question types, varying entity popularity, differing information dynamism, and different conversation turns. We design three tasks: single-source augmentation, multi-source augmentation, and multi-turn conversations---each paired with an associated retrieval corpus and APIs for both image-KG retrieval and webpage retrieval. Our evaluation shows that straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM single- and multi-turn QA, respectively, whereas state-of-the-art industry solutions reach just 32% and 45%, underscoring ample room for improvement. The benchmark has hosted a leaderboard that attracted about a thousand participants, with winning solutions improving baseline performance by 28%, highlighting its early impact on advancing the field.", "tldr": "", "keywords": ["multimodal retrieval-augmented generation (MM-RAG)", "factual visual question answering (VQA)", "large vision-language models (LVLMs)", "benchmarking", "multi-source augmentation", "egocentric", "wearable AI"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8b892ae29932b618fd76df4dce395bc614c10b9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CRAG-MM, a new benchmark for evaluating Multi-Modal Retrieval-Augmented Generation (MM-RAG) systems. The authors motivate this work by highlighting a critical gap in modern wearable AI devices which need to answer factual, multi-turn questions about a user's visual surroundings. This benchmark fill this gap by building with egocentric images. The paper proposes three evaluation tasks of increasing complexity: single-source augmentation (retrieval from an image-based Knowledge Graph), Multi-source augmentation (retrieval from both image-KG and a webpage corpus, and Multi-turn QA (using retrieval and conversation history). Benchmarking experiments show that even state-of-the-art industry RAG solutions perform poorly, achieving only 32% (single-turn) and 45% (multi-turn) truthfulness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a clear and important gap in existing research. \n2. This paper provide a novel new benchmark."}, "weaknesses": {"value": "1. Although a novel benchmark, this paper does not offer significant analysis or interesting insights from its benchmark.\n2. The paper lacks of a methodology of how to improve the model's capabilities.\n3. Is Truthfulness = Accuracy - Hallucination? It's not consistent in the paper."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CopEbtqJPm", "forum": "5p1Ucqfd1A", "replyto": "5p1Ucqfd1A", "signatures": ["ICLR.cc/2026/Conference/Submission23393/Reviewer_qUKK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23393/Reviewer_qUKK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761464567404, "cdate": 1761464567404, "tmdate": 1762942642261, "mdate": 1762942642261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CRAG-MM, a Comprehensive RAG benchmark for Multi-modal Multi-turn conversations.\nThe dataset contains a diverse set of 6.5K (image, question, answer) triplets and 2K multi-turn visual conversations across 13 domains, including 6.2 K egocentric images from wearable device smart glasses.\nThe benchmark emphasizes real-world challenges like low-light, blur, truncation, occlusion, and rotation, reflecting the conditions faced in wearable AI applications. It supports three tasks (single-source, multi-source, and multi-turn RAG). Experiments show that even advanced systems (e.g., GPT-5) achieve only ~63 % (single-turn) and ~70 % (multi-turn) truthfulness, indicating room for progress. A public leaderboard has drawn ~1 K participants, showing early community engagement and progress on this benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1) Rich conversational coverage: Includes 2 K multi-turn conversations, ∼38 % of which involve domain shifts, realistically simulating natural topic drift.\n\n2) Real-world visual realism: Contains 7.9 K images, with 79 % egocentric, capturing wearable AI’s inherent visual challenges (wide-angle, occlusion, low light).\n\n3) Comprehensive evaluation: GPT-5 achieves 63 % (single-turn) and 70 % (multi-turn) accuracy, revealing a measurable gap and potential for improvement on MM-RAG.\n\n4) Community impact: The dataset powered a leaderboard competition that attracted ~1 K participants.\n\n5) Human involvement: Data were created or revised by human annotators to ensure quality and realistic question–answer alignment.\n\n6) Extensive analysis: The paper includes exploratory data analysis, distributions by image quality, domain, and question type, supporting interpretability and dataset transparency."}, "weaknesses": {"value": "Ethical and safety concerns: The dataset involves vendors wearing smart glasses in daily contexts. While this enables realism, it raises potential privacy and identifiability risks for bystanders. Clearer documentation of anonymization and consent protocols is needed."}, "questions": {"value": "Please clarify the licensing and privacy protections for public images and web pages, as well as annotator instructions regarding bystanders.\nWere faces or license plates blurred or removed before release?\nHow are data from wearable captures handled to ensure participant safety and privacy compliance?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The dataset involves vendors wearing smart glasses in daily contexts. While this enables realism, it raises potential privacy and identifiability risks for bystanders."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TTAWVBfKYv", "forum": "5p1Ucqfd1A", "replyto": "5p1Ucqfd1A", "signatures": ["ICLR.cc/2026/Conference/Submission23393/Reviewer_kBjy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23393/Reviewer_kBjy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776508639, "cdate": 1761776508639, "tmdate": 1762942641937, "mdate": 1762942641937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a new multimodal RAG benchmark (though the reviewer argued that it's a long-context multimodal benchmark), especially targeting the wearable AI user cases. The benchmark analyses ablate how models perform on single-turn conversation image search, image-web unified search, and the multi-turn conversational scenarios. The results on this benchmark shows that large open-source and proprietary models still struggle with hallucination, especially when given low-quality images."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The synthetic benchmark dataset covers more conversation dynamics than prior benchmarks for factual question answering as shown in Figure 1 (See more in the weakness). Also, it's good that most images from this benchmarks are collected from real-human using egocentric wearable headset.\n2. It's interesting that the paper targets wearable AI use cases with low-quality images. This setting is different from a lot of other relevant benchmarks."}, "weaknesses": {"value": "1. Benchmark Dataset Design\n\t- The reviewer gets confused if it's a RAG benchmark, a search-augmented benchmark, or a long-context benchmark? Based on the description in Section 2 and Section 4.1, It seems that the authors provide an API function (tool) for these VLM to use and always assume the model would use it. If that's the case, it's more like a long-context QA benchmarks cuz the search part is fixed now. If not, it can be a search-augmented where the models might be able to decide whether they want to call the search. However, as the API is pre-determined, the paper seems to be far away from an RAG bnchmark studying the performance of a system including retrievers and generators as a whole.\n\t- The reviewers question if the benchmark is specifically targeting the wearable AI use cases or is developed to be \"comprehensive\" for multiple scenarios. The reviewer feels that the authors can make the benchmark at a clear position by picking either one bot not both. Beyond that, while images are manually collected, questions are semi-synthesized (task 1 and 2) or fully LLM generated (task 3). The reviewer would like to know more details for the dataset, especially the question distribution, intention, and examples. As a concrete suggestion, what's the difference between the curated dataset and the real human search queries in Search Arena where multi-turn questions are all from real human?\n\n2. Experimental Design\n\t- The paper provides an image-based API and a text-search API search tool for models. However, as shown in Figure 3, using image-based search gets at most 58% recall, which limits models' performance at the beginning. Based on this biased, sub-optimal experimental design, the authors then draw several conclusions that seems to be questionable. For instance, for \"entity recognition is much harder when relying solely on visual information compared to leveraging text clues\" (L419-L420), the reviewer then want to know if it's because the choice of ViT-L/14@336px too weak. If the recall of image-search is the same or similar as text(web) retrieval, the reviewer would then convince the claim.\n\t- The reviewer wants to know more about the prompt to these models and the definition of truthness, missing, and hallucination. For prompting, it matters a lot as the authors now allow the model to answer \"I don't know\". Also, the reviewer is not sure if the incorrect in Line 315 identical as Hallucination in the remaining table. Finally, it's really confusing that the truthfulness can be negative in Tables, which the reviewer wants to know how it is computed. For multi-round conversations, do authors ensure the search models not to get the same content or not?\n\nOverall, it's great that we have a new collected images, but the reviewer wants to know more details about the position/design of this benchmark. Also, the reviewer believes that the initial experiments/analyses on this benchmark data can be further strenthened."}, "questions": {"value": "Please read the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0mJYMnIYBI", "forum": "5p1Ucqfd1A", "replyto": "5p1Ucqfd1A", "signatures": ["ICLR.cc/2026/Conference/Submission23393/Reviewer_b1cj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23393/Reviewer_b1cj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776763166, "cdate": 1761776763166, "tmdate": 1762942640857, "mdate": 1762942640857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark dataset CRAG-MM, that is leveraged to evaluate multi-modal RAG systems specifically when applied to wearable scenarios. The benchmark is designed based on a diverse set of question styles and practical tasks. The data for the retrieval have been designed to reflect the imperfections associated with real-world situations. Specifically, the corpus used for textual sources of information included significant amount of noise (irrelevant passages and URLs) and images with varying quality levels, the view point, unideal cropping, lighting levels, etc. The benchmark dataset had been shown to be challenging uncovering shortcomings associated with existing MM-RAG systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper propose an interesting benchmark for multi-modal RAG specifically useful for situations pertaining to wearables, which are becoming more and more common place. This benchmark is extremely relevant for the current and future developments of the field. Moreover, the design of the question types, tasks and the data quality feeding the RAG systems, makes the benchmark realistic, hence making the benchmark useful and reflective of the true performance of the systems evaluated with it in real world scenarios. The design choices behind the question types, tasks and data are well motivated and discussed. The paper overall is well-written and presented."}, "weaknesses": {"value": "While the benchmark is well motivated and the rationale behind the dataset design and data types is clearly presented, important details are missing regarding how the dataset was actually created and how the quality of its questions was ensured. I encourage the authors to consult works such as MMQA and SMMQG, which provide comprehensive documentation of their data collection and validation processes, including the use of crowd-sourced annotators, inter-annotator agreement checks, and bias mitigation strategies. The absence of comparable methodological transparency in this paper raises concerns about the reliability of the dataset and, consequently, the validity of the reported evaluation results. This represents a fundamental weakness that should be addressed to strengthen the paper’s overall contribution. Similar issues/shortcomings also persist as related to the auto-evaluator evaluation.\n\nMoreover, since all evaluated RAG systems rely exclusively on the retrievers provided by the benchmark, their performance is inherently bounded by the recall and quality of those retrievers. The paper itself reports relatively low retrieval recall which sets an upper limit on achievable QA accuracy. As a result, it becomes difficult to disentangle whether the observed limitations stem from the benchmark’s retrieval pipeline or from the reasoning capabilities of the tested models."}, "questions": {"value": "Along the lines of the major weaknesses above, the following questions at the bare minimum need to be addressed \n\n- Section 3.2 mentions that annotators “created plausible questions for wearable devices” and “recorded the ground truth answers” but does not describe annotation instructions, inter-annotator agreement, validation stages, or any bias mitigation strategy.\n\n- Similarly, the multi-turn QA generation (in Section 3.2) relies heavily on prompting Llama-3.2-90B and then “review by annotators,” yet provides no quantitative metrics for human verification.\n\n- The paper’s “Reproducibility Statement” (page 10) claims detailed description of dataset creation, but Appendix A.2 and A.3 still omit annotation guidelines, quality-control sampling, or reviewer calibration.\n\n- Provide more details on the auto-evaluator evaluations\n\n- The limitations of the benchmark performance based on the performance limitations of the retrievers need to be addressed and further discussed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AaFrALLv7s", "forum": "5p1Ucqfd1A", "replyto": "5p1Ucqfd1A", "signatures": ["ICLR.cc/2026/Conference/Submission23393/Reviewer_WBeU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23393/Reviewer_WBeU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982329382, "cdate": 1761982329382, "tmdate": 1762942640429, "mdate": 1762942640429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}