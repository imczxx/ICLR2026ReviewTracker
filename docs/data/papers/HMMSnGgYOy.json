{"id": "HMMSnGgYOy", "number": 17291, "cdate": 1758274332312, "mdate": 1763623827036, "content": {"title": "Overlap-Adaptive Regularization for Conditional Average Treatment Effect Estimation", "abstract": "The conditional average treatment effect (CATE) is widely used in personalized medicine to inform therapeutic decisions. However, state-of-the-art methods for CATE estimation (so-called meta-learners) often perform poorly in the presence of low overlap. In this work, we introduce a new approach to tackle this issue and improve the performance of existing meta-learners in the low-overlap regions. Specifically, we introduce Overlap-Adaptive Regularization (OAR) that regularizes target models proportionally to overlap weights so that, informally, the regularization is higher in regions with low overlap. To the best of our knowledge, our OAR is the first approach to leverage overlap weights in the regularization terms of the meta-learners. Our OAR approach is flexible and works with any existing CATE meta-learner: we demonstrate how OAR can be applied to both parametric and non-parametric second-stage models. Furthermore, we propose debiased versions of our OAR that preserve the Neyman-orthogonality of existing meta-learners and thus ensure more robust inference. Through a series of (semi-)synthetic experiments, we demonstrate that our OAR significantly improves CATE estimation in low-overlap settings in comparison to constant regularization.", "tldr": "We introduce a novel framework of Overlap-Adaptive Regularization that tackles the issue of low overlap with an adaptive regularization.", "keywords": ["causal machine learning", "treatment effect estimation"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe6754e7eac62ba49995236e2655710861ba1238.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an overlap-adaptive regularization approach for estimating Conditional Average Treatment Effects (CATE). The key idea is to adjust the regularization strength based on the degree of overlap: regions with strong overlap receive lower regularization, while regions with weak overlap are penalized more heavily to mitigate instability. Additionally, the authors propose a debiased version of the estimator to address bias arising from estimation errors in propensity scores."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of overlap-adaptive regularization appears to be a reasonable strategy for CATE estimation. The authors introduce two practical implementation techniques—noise injection and dropout—to facilitate ease of application in real-world settings. They provide theoretical analysis under a linear model framework. Numerical experiments are conducted to illustrate the performance of the proposed estimator."}, "weaknesses": {"value": "1. The theoretical analysis provided in the paper appears inadequate. The results for the linear model can be considered preliminary, as the simplicity of the linear setting limits the relevance of regularization—making it less critical in practice. It would be more valuable to establish theoretical guarantees for nonparametric models, which are more representative of real-world applications. Furthermore, the numerical experiments primarily involve neural networks and kernel-based methods. While Appendix C presents theoretical results for RKHS, these should ideally be included in the main paper to highlight their importance. Moreover, Appendix C primarily focuses on deriving the closed-form solution for the RKHS estimator, without providing any discussion of the statistical properties of the final estimator. This omission limits the theoretical contribution and leaves readers uncertain about the estimator’s theoretical improvement over the traditional CR RKHS estimator. \n\n\n2. The simulation results are based on only 10 runs, which raises concerns. Moreover, the improvement of the proposed overlap-adaptive regularization (OAR) over CR does not appear to be statistically significant when accounting for the reported standard errors. Similarly, for the synthetic data example."}, "questions": {"value": "I am curious about the different performance of the noise-injection and dropout methods shown in Figure 2. These two regularization strategies appear to behave quite differently under constant regularization. Specifically, for noise injection, the conventional regularization (CR) performs relatively well, whereas for dropout, CR performs poorly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ciw1lVbeBj", "forum": "HMMSnGgYOy", "replyto": "HMMSnGgYOy", "signatures": ["ICLR.cc/2026/Conference/Submission17291/Reviewer_EKxX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17291/Reviewer_EKxX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663819361, "cdate": 1761663819361, "tmdate": 1762927232840, "mdate": 1762927232840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes and studies methods for regularizing causal meta-learners so that, in a particular sense, the strength of the regularization explicitly adapts to the degree of overlap. More precisely, the regularization is stronger in regions of the covariate space in which there is limited overlap (i.e., the propensity score is close to zero or one). Some existing regularization methods like dropout have been shown to implicitly adapt to overlap in certain settings (Wager, Wang, and Liang, 2013), however the present work appears to be the first paper in the context of causal meta-learners to propose explicitly tying the degree of regularization to the degree of overlap differentially over the covariate space.\n\nThe authors consider existing regularization methods namely: dropout, noising of the regressors, and kernel ridge regularization. They propose overlap-adaptive versions of these techniques. In order to regularize adaptively, the degree of overlap, as measured by the conditional variance of the treatment, must be estimated in a first stage. This conditional variance function constitutes a high-dimensional nuisance parameter and the authors suggest a first-order bias correction so that the corrected objective function is Neyman Orthogonal to this new nuisance parameter in addition to the other nuisance parameters.\n\nThe authors examine the performance of their methods relative to benchmarks on a range of synthetic and semi-synthetic datasets. The adaptive approach leads to significant improvements on some of these benchmarks and generally depends on a) the degree of regularization, b) which regularization method is used, and c) which meta-learning objective is regularized. The most consistent improvement appears in the context of the HC-MNIST data. At least in my reading, no one particular version of their approach (i.e., dropout vs. noising, choice of base meta-learner, debiased vs. not debiased) seems to dominate the others."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "I found the paper to be clear and well-explained. I find the proposed methods intuitively very reasonable, and the debiasing of the adaptively regularized objective makes the problem quite non-trivial. The discussion of the interpretation of the regularization methods in linear models seemed to me to be quite helpful. The empirical analysis of the performance of the methods is quite thorough."}, "weaknesses": {"value": "While the method leads to significant improvements over baselines in some cases, the results are somewhat mixed. This is likely inevitable because, as the authors note, adaptively regularizing is only likely to be effective when the CATE is smooth in around regions with limited overlap. Nonetheless, given the mixed results, empirical practitioners may find it rather difficult to choose whether or not to regularize adaptively and additionally, which adaptive method to use. I wonder if there could be some more guidance on this front?\n\nFigure 2 seems to suggest that performance is monotonically improving with the degree of regularization almost uniformly for all methods. The same is true for the results in Table 3. For the synthetic data this is not surprising given that the true CATE is constant (equal to zero) so one would ideally one would regularize as much as possible. In order to make the comparison meaningful, the authors compare methods for a given average regularization strength, but I do not see why this is the correct comparison to make: in practice practitioners would presumably choose the regularization parameters by a data driven method like cross-validation, and may get very different parameters for the different methods. Personally, I think the constant CATE in the simulated data makes it unsuitable for comparing the methods, and for the HC-MNIST data, it might be helpful to see how performance compares when the regularization parameters are chosen empirically or to compare performance under the best choice of regularization parameter for a given method (which would require a comparison for small enough regularization parameters such that performance is no longer monotonically improving."}, "questions": {"value": "More a suggestion than a question, but I wonder whether there may be some way to get the best of both worlds. That is to allow for some compromise between constant and adaptive regularization. For example, suppose we let $\\lambda(v):= (4v)^{-\\gamma}-\\gamma$ where $\\gamma\\geq0$ is an additional regularization parameter to be selected by the researcher. Then in the limiting case of $\\gamma= 0$ we recover constant regularization and with $\\gamma=1$ you recover $\\lambda_n(v)$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "drom6ZZdAa", "forum": "HMMSnGgYOy", "replyto": "HMMSnGgYOy", "signatures": ["ICLR.cc/2026/Conference/Submission17291/Reviewer_uzmE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17291/Reviewer_uzmE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929112467, "cdate": 1761929112467, "tmdate": 1762927232025, "mdate": 1762927232025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes overlap-adaptive regularizers for the second stage of meta-learners, gives parametric instantiations plus and RKHS variant, adds a debiased version."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The concept is interesting, addresses a gap in overlapping issue in causal inference.\n- Definitions and the family of $\\lambda$ functions are precise.\n- Provide some solid theory for parametric part. RKHS part is useful but assumes strong overlap."}, "weaknesses": {"value": "- The paper says the noise/dropout (implicit) and explicit forms are “equivalent,” but the exact equivalence is proved only for linear target classes and generally holds as a first-order approximation for NNs. This is stated too broadly.\n- Proposition 6 requires that $P(\\epsilon < \\pi(X) < 1-\\epsilon)=1$, which makes the explicit solution neat but excludes the core part the paper aims to help (for regions with very low overlap).\n- Several IFs scale like $1/\\nu(x)^k$ can blow up when it's approaching 0. Please discuss the integrability/robustness."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ad4XA441VU", "forum": "HMMSnGgYOy", "replyto": "HMMSnGgYOy", "signatures": ["ICLR.cc/2026/Conference/Submission17291/Reviewer_B5nJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17291/Reviewer_B5nJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947855908, "cdate": 1761947855908, "tmdate": 1762927231699, "mdate": 1762927231699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of conditional average treatment effect (CATE) estimation under low-overlap conditions. The authors propose Overlap-Adaptive Regularization (OAR), which adjusts the regularization strength according to estimated overlap weights. The paper also introduces a debiased version that preserves Neyman-orthogonality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper addresses a well-known and practically important issue in CATE estimation—poor performance under low overlap conditions.\n\n2.\tIntroducing overlap-adaptive regularization is a straightforward and intuitive idea that appears novel in the context of CATE estimation.\n\n3.\tThe proposed OAR framework can be applied to various existing meta-learners (DR-, R-, and IVW-learners) and accommodates both parametric and non-parametric models.\n\n4.\tThe inclusion of a debiased version that preserves Neyman-orthogonality enhances theoretical soundness and robustness to nuisance estimation errors."}, "weaknesses": {"value": "1.\tThe method’s performance is sensitive to the accuracy of estimated overlap weights, which may be unstable in practice.\n\n2.\tTheoretical analysis focuses mainly on orthogonality preservation; further insights into convergence or generalization behavior under varying overlap levels could add depth.\n\n3.\tAdaptive regularization requires computing overlap-dependent weights for each sample, which may increase overhead for large-scale or high-dimensional settings.\n\n4.\tA structural issue is that Section 3 contains material that could be moved to the appendix, while several important technical details and proofs are placed in the appendix instead of the main text, which may hinder readability and comprehension."}, "questions": {"value": "1.\tHow sensitive is OAR/dOAR to the choice of the regularization function? Are there guidelines for choosing one over the others in practice?\n\n\n2.\tSince the method relies on estimated overlap weights, how robust is the approach to mis-specification or noise in the propensity score? Does the debiased version fully mitigate this sensitivity?\n\n3.\tHow does per-instance adaptive regularization scale with large sample sizes or high-dimensional covariates, especially in neural network settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IwruvtPlvR", "forum": "HMMSnGgYOy", "replyto": "HMMSnGgYOy", "signatures": ["ICLR.cc/2026/Conference/Submission17291/Reviewer_Yknh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17291/Reviewer_Yknh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967492390, "cdate": 1761967492390, "tmdate": 1762927231403, "mdate": 1762927231403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal"}, "comment": {"value": "We are grateful for the reviews of our paper, as well as the constructive and helpful comments!  We addressed all of your comments below. We also uploaded a **revised version of our paper**, where we highlight key changes colored in $\\color{blue}{\\text{blue}}$).\n\nHere are our main improvements: \n- **New theoretical results**. We provided additional results on the excess prediction risk for the non-parametric target models with our OAR RKHS norm (we have added a **new Proposition 7** to **Sec. 5.4**).\n- **New experimental results**. We have conducted additional runs for the HC-MNIST dataset and  **updated our Tables 3 and 6** to report significant improvements over the baseline.\n- **Extended discussions**. In **Sec. 6**, we added important discussions (i) on how to choose a regularization function for our OAR/dOAR, and (ii) on the best combinations of our OAR/dOAR and different meta-learners.\n- **Improved clarity**. We followed multiple suggestions of the reviewers to improve the clarity (e.g., we added high-level explanations of the proofs, a discussion on the computational cost of our method, etc.)  \n\nWe are confident that the feedback has greatly improved the quality of our paper, and we believe that the paper is a good fit for the ICLR 2026 conference."}}, "id": "ZvuGUNTUq8", "forum": "HMMSnGgYOy", "replyto": "HMMSnGgYOy", "signatures": ["ICLR.cc/2026/Conference/Submission17291/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17291/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission17291/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763625184903, "cdate": 1763625184903, "tmdate": 1763625184903, "mdate": 1763625184903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}