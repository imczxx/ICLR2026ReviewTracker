{"id": "gR2lj6f8vG", "number": 8859, "cdate": 1758100145534, "mdate": 1759897759067, "content": {"title": "Expert-guided Clinical Text Augmentation via Query-Based Model Collaboration", "abstract": "Data augmentation is a widely used strategy to improve model robustness and generalization by enriching training datasets with synthetic examples. While large language models (LLMs) have demonstrated strong generative capabilities for this purpose, their applications in high-stakes domains like healthcare presents unique challenges due to the risk of generating clinically incorrect or misleading information. In this work, we propose a novel query-based model collaboration framework that integrates expert-level domain knowledge to guide the augmentation process to preserve critical medical information. Experiments on clinical prediction tasks demonstrate that our lightweight collaboration-based approach consistently outperforms existing LLM augmentation methods while improving safety through reduced factual errors. This framework addresses the gap between LLM augmentation potential and the safety requirements of specialized domains.", "tldr": "We suggest a safer data augmentation method that integrates expert-level clinical knowledge using model collaboration.", "keywords": ["Data augmentation", "Large language models", "Healthcare"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4a0127e3fa9643732143907da03346e463afe3e4.pdf", "supplementary_material": "/attachment/c9cf2f27535fa1911a284585fb24d44f7641b30c.zip"}, "replies": [{"content": {"summary": {"value": "This work addresses the challenge of safe data augmentation (avoiding introduction of factual errors and hallucinations) for clinical text using large language models. The idea is existing LLM-based augmentation methods lack domain-specific safeguards and thus may delete or introduce medical entities that should not be present in the clinical notes. To address this problem, the authors propose a collaboration framework that combines a weak expert to identify important medical entities in the original clinical note and a general-purpose large language model to rewrite the clinical note while being explicitly asked to preserve the medical entities identified by the weak expert. The data augmentation is evaluated on MIMIC-III clinical notes across several different downstream tasks. The paper also demonstrates the collaboration framework can be distilled into a single model via Direct Preference Optimization, although it underperforms the 2-model collaboration framework."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The problem is well motivated as the safety risks of hallucinations and medical term deletions in clinical text augmentation has not been studied previously.\n* The framework is practical as it uses a lightweight named entity recognition model to guide a strong generalist model, which does not require the costly annotation compared to human expert supervision or full LLM training (or re-training).\n* Reasonable experimental validation setup with demonstration across three prediction tasks, different model architectures with zero-shot and few-shot scenarios.\n* Clear presentation as the paper is relatively well-structured and has effective use of figures to convey the message."}, "weaknesses": {"value": "* The motivation for weak expert versus a retrieval-augmented generation approach to inject domain knowledge isn't quite clear. In addition, is there a reason why generalist model is preferred and not one of the medical-domain focused large language model? There has also been work that incorporates medical graph knowledge to perform data augmentation as well (see Varshney et al. in Scientific Reports 2023, Yu et al. in ACM Computing Surveys 2022, and Xu et al. in KDD 2025) but it's not clear why the NER weak expert might be preferable to these other two other approaches.\n* The evaluation metrics have some limitation as it relies purely on the NER model to identify whether it was extracted from the original text, and clearly providing a prompt with these entities will improve preservation rate and hallucination rate. Is there any human evaluation of clinical correctness, or potentially using multiple LLM as judges to identify that these are in fact better data augmentation? \n* The baselines are a bit weak in that it compares against simple rephrasing and style-based augmentation. There are other variants that use RAG approaches or graph approaches to achieve data augmentation. Is this approach comparable or better than those? Similarly, do you need the weak expert if you have a medical-domain LLM model? It's also not clear why larger LLMs aren't tried if the goal is to perform data augmentation without requiring any fine-tuning. \n* Only one NER backbone is chosen as the weak expert (another uses just a simple language mode). An ablation on the effect of the weak entity extraction would be useful to better understand the performance characteristics of the model as there have been various biomedical NER models like Bioalbert, BioBert, MedBert. \n* Causal graph is mentioned but is not deeply integrated into the methodology nor the experiments. What is the function of the causal graph in the context of data augmentation?\n* The experimental setup isn't quite clear. How many data augmentations are constructed and passed to the model? For the downstream tasks, is it few shot demonstrations? Is it a combination of augmented and real data?"}, "questions": {"value": "1. Why did you choose a weak expert NER approach over (i) retrieval-augmented generation (RAG) for injecting domain knowledge; (ii) a medical-domain LLM (or biomedical-domain LLM), and (iii) incorporating medical knowledge graphs for data augmentation? \n2. In your framework, does having larger, more capable LLMs that might have better inherent medical knowledge perform just as well as your NER-based version? \n3. The preservation rate and hallucination rate metric rely on the same NER model used for extraction. Does the results still hold when either using different NER Models for extraction or when evaluated by human clinicians?\n4. Does your framework generalize to various NER backbones? As suggested in Table 4, the effects is minimal when using a general expert which suggests there is some necessary bare minimum as it barely outperforms the Naive version. \n5. How is the causal graph related to the methodology? Are you positing that all entities are considered causal?\n6. What is the experimental setup in terms of how much data is augmented / created and how are the prediction tasks measured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VZpjbNerWx", "forum": "gR2lj6f8vG", "replyto": "gR2lj6f8vG", "signatures": ["ICLR.cc/2026/Conference/Submission8859/Reviewer_GaEP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8859/Reviewer_GaEP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654698208, "cdate": 1761654698208, "tmdate": 1762920624245, "mdate": 1762920624245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to all reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their constructive feedback. Across reviews, there was a shared remark for our goal of document-level clinical augmentation under strict safety constraints, and for our model collaboration framework, where a weak clinical expert steers a generalist LLM to preserve medically critical content while still producing diverse notes. We appreciate the positive feedback on motivation and methodology contribution, including comments of \"*The problem is well motivated* / *identifies a genuine challenge in using LLMs for high-stakes domains like healthcare.*\", \"*The framework is practical / using a lightweight clinical expert to guard a stronger LLM is a natural and well-justified solution.*\"\n\nWe also recognize several shared concerns and have revised the paper accordingly:\n\n- **Robustness of Hallucination Rate(HR)/Preservation Rate (PR)** \nWe now (i) move from token-level to UMLS-based concept PR/HR, allowing safe synonymy, and (ii) add an ablation where PR/HR are recomputed with a different NER evaluator. Absolute scores change, but the relative ranking is stable and our method remains best in both PR and HR.\n\n- **Ablation on Weak Expert** \nWe add an ablation over different weak experts (biomedical-ner-all, Medical-NER, BioMed-NER, general expert), showing that domain-specific NERs yield the strongest PR/HR, but even a general expert improves over naive rewriting, clarifying generalization across NER backbones.\n\n- **Additional Baselines**\nWe added additional baselines, including (1) a RAG-based token substitution and (2) domain fine-tuned LLM. Our method uniquely achieves both high concept preservation and the strongest downstream performance. We also expand the motivation section to better connect augmentation to realistic, privacy- and access-constrained clinical workflows.\n\n- **Causal Framework**\nWe clarify the causal graph by explicitly defining $V$ as the NER-extracted clinically critical entities and explaining that our intervention targets only non-causal text. We detail the augmentation volume and how synthetic and real notes are combined for downstream training. Finally, we have initiated a small-scale clinician evaluation and will include results in the updated version.\n\nWe thank everyone for their detailed feedback. We will update the revision, reflecting the shared suggestions of the reviewers. We believe that the strengthened empirical results have addressed reviewers' concerns."}}, "id": "vT3AQYhSgs", "forum": "gR2lj6f8vG", "replyto": "gR2lj6f8vG", "signatures": ["ICLR.cc/2026/Conference/Submission8859/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8859/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8859/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763664467195, "cdate": 1763664467195, "tmdate": 1763664467195, "mdate": 1763664467195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an expert-guided framework for clinical text augmentation. A biomedical NER model marks critical medical entities, and an LLM rephrases the note without altering them. The generated notes are used to train downstream predictors on MIMIC-III (e.g., readmission, mortality, length-of-stay), showing modest performance gains and reduced hallucination compared with Naive and CATO baselines. The authors also explore distilling the two-model process into a single model via preference-based reinforcement learning."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "s1. The paper’s starting point is to ensure the safety and factual correctness of LLM-based augmentation in clinical context, which identifies a genuine challenge in using LLMs for high-stakes domain like healthcare.\n\ns2. The overall presentation is clear: the problem setup, pipeline diagram (Fig. 1), and qualitative examples (Fig. 2) effectively convey the workflow and intended behavior.\n\ns3. The downstream experimental setup is comprehensive, covering multiple clinical prediction tasks."}, "weaknesses": {"value": "W1. The motivation for augmentation lacks sufficient justification for a high-stakes clinical domain. While benefits like robustness are understood in data augmentation studies, practitioners will always prioritize the safety of the original text over the modest performance gains (Table 2) from a synthetic dataset. This is especially true given the risk of factual alterations (see W2). The paper must provide a stronger, necessary motivation for modifying notes, such as supporting de-identification for data sharing or ML model development.\n\nW2. There is inconsistent evidence regarding the paper’s safety claims. \n\n> Appendix A.1: Do not change any medical terminology, dosages, measurements, or clinical findings.\n\n> Figure 2: “fever → pyrexia” and “shortness of breath → dyspnea” \n\nSuch replacements violate the stated constraint and introduce risks that undermine the safety argument.\n\nW3. Given replacements in W2, a human evaluation is necessary (at least on a small scale). Safety promised by PR/HR lacks real-world credibility, and healthcare applications just tend to favor correctness over marginal performance gains.\n\nW4. The methodological novelty is incremental. The proposed pipeline essentially combines a general-purpose LLM with an external NER model acting as a token-level constraint."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uT4hFgNN2O", "forum": "gR2lj6f8vG", "replyto": "gR2lj6f8vG", "signatures": ["ICLR.cc/2026/Conference/Submission8859/Reviewer_rxnD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8859/Reviewer_rxnD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849510657, "cdate": 1761849510657, "tmdate": 1762920623600, "mdate": 1762920623600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a query-based model collaboration framework for safe clinical text augmentation. A weak expert (clinical NER on DistilBERT) extracts medical entities to preserve verbatim; a strong generalist (small instruction-tuned LLM, e.g., Qwen-3-0.6B / Llama-3.2-3B) rewrites the note while obeying these constraints. The authors evaluate on MIMIC-III for readmission, mortality, and length-of-stay, plus zero/one/few-shot phenotyping and ICD coding. They report higher entity preservation (PR), lower hallucination rate (HR), and better downstream performance than a naive paraphrase and CATO-style style-only augmentation. They also distill the collaboration into a single model via DPO, with smaller but still positive gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear problem framing for safety-critical NLP: focuses augmentation on non-causal text while protecting clinically critical spans.\n\nSimple, practical pipeline: off-the-shelf clinical NER + small LLM; no heavy training needed to get benefits.\n\nSafety-oriented metrics (PR/HR) and qualitative examples convincingly show typical LLM paraphrase failures and how constraints help."}, "weaknesses": {"value": "Self-evaluation bias in PR/HR: the same NER family is used to (a) define constraints and (b) score preservation/hallucination → optimistic estimates; misses concept-level paraphrases and dosage/unit variants.\n\nOver-constrained “verbatim” preservation: requiring exact tokens may block legitimate clinical synonymy (e.g., myocardial infarction vs MI), risking brittle rewrites and hidden meaning drift.\n\nBaselines are narrow: lacks strong constraint-aware or copy-preserving paraphrase baselines (span locking/bracketing, constrained decoding, mask-and-infill with protected spans, entailment-constrained rewrite, rule/UMLS-guided paraphrase)."}, "questions": {"value": "NER–metric coupling: How do PR/HR change if you (i) evaluate with a different clinical NER, (ii) map to UMLS concepts (string-independent), or (iii) use entailment-based factuality checks?\n\nSynonymy & concept-level preservation: Can you preserve concepts (UMLS/RxNorm/SNOMED IDs) rather than exact tokens to allow safe synonym substitutions while protecting meaning?\n\nStronger baselines: Please add (and report) results for recent papers for a fair comparsion"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QlU1DhbVUS", "forum": "gR2lj6f8vG", "replyto": "gR2lj6f8vG", "signatures": ["ICLR.cc/2026/Conference/Submission8859/Reviewer_iHnY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8859/Reviewer_iHnY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049038412, "cdate": 1762049038412, "tmdate": 1762920623209, "mdate": 1762920623209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of using LLMs for clinical text augmentation, noting that naive paraphrasing or style-focused methods can be dangerous in healthcare settings. The authors propose a two-model pipeline: (1) a lightweight “weak expert” extracts domain-critical tokens from the original note; (2) a “strong generalist” LLM rewrites the note but is constrained through a prompt to keep those tokens intact. They evaluate on MIMIC-III across three prediction tasks and two low-resource settings. The method improves entity preservation and reduces hallucination compared to naive LLM augmentation and CATO, and the augmented data yields better downstream accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied challenge of clinically safe text augmentation is important; using a lightweight clinical expert to guard a stronger LLM is a natural and well-justified solution.\n2. The empirical results on multiple MIMIC-III prediction tasks show consistent gains, indicating that constrained augmentation produces data that actually helps downstream models.\n3. The analysis comparing against style-focused augmentation highlights that the method specifically reduces medical entity loss and hallucination, confirming that the design targets the right failure mode."}, "weaknesses": {"value": "1. Some closely related work on clinical text generation and augmentation with LLMs is not discussed. For example, [1] explicitly studies how to inject clinical knowledge before LLM generation, which is conceptually similar to “expert-guided” rewriting. It would strengthen the positioning to contrast their knowledge source and objectives with the proposed expert extraction, and, if feasible, to include it as an additional baseline.\n2. Section 3 frames the method with a causal view that separates label-relevant variables (V) from other factors (U), but the implementation approximates V only through biomedical NER. This does not seem like a full realization of the causal intent, because many clinically label-bearing elements, such as severity descriptors and care context, are not always captured as NER spans. The paper would benefit from either broadening the extractor or clarifying that current results are for a narrower, entity-level notion of V.\n3. The experimental comparison focuses on naive prompting and CATO-style rewriting, but there are many recent works on LLM-based text data augmentation [2]. Adding a small set of representative recent methods would make the empirical case more robust and show that the gains are not only over older or style-focused augmenters.\n\n[1] Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. ACL Findings 2024.\n[2] Text Data Augmentation for Large Language Models: A Comprehensive Survey of Methods, Challenges, and Opportunities. 2025"}, "questions": {"value": "See the weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "InE4Ds8T7f", "forum": "gR2lj6f8vG", "replyto": "gR2lj6f8vG", "signatures": ["ICLR.cc/2026/Conference/Submission8859/Reviewer_d8JU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8859/Reviewer_d8JU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762105515300, "cdate": 1762105515300, "tmdate": 1762920622785, "mdate": 1762920622785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}