{"id": "2SeNayaDJU", "number": 9320, "cdate": 1758118802386, "mdate": 1759897731544, "content": {"title": "NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?", "abstract": "The evaluation of Vision-Language-Action (VLA) agents is hindered by the coarse, end-task success metric that fails to provide precise skill diagnosis or measure robustness to real-world perturbations. This challenge is exacerbated by a fragmented data landscape that impedes reproducible research and the development of generalist models. To address these limitations, we introduce \\textbf{NEBULA}, a unified ecosystem for single-arm manipulation that enables diagnostic and reproducible evaluation. NEBULA features a novel dual-axis evaluation protocol that combines fine-grained \\textit{capability tests} for precise skill diagnosis with systematic \\textit{stress tests} that measure robustness. A standardized API and a large-scale, aggregated dataset are provided to reduce fragmentation and support cross-dataset training and fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle with key capabilities such as spatial reasoning and dynamic adaptation, which are consistently obscured by conventional end-task success metrics. By measuring both what an agent can do and when it does so reliably, NEBULA provides a practical foundation for robust, general-purpose embodied agents.", "tldr": "We present NEBULA, a unified ecosystem for VLA agents that disentangles capabilities from performance metrics and standardizes task data via a shared API to enable fine-grained, interpretable, and transferable assessment.", "keywords": ["Robotics", "Embodied AI", "Benchmarks", "Evaluation Metrics", "Simulator"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a75d6c1b8f4338d3eb44a48b27c9204a1b43f5ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a new evaluation benchmark for VLA agents, called NEBULA. Instead of merely assessing whether a model can complete tasks, NEBULA introduces two complementary evaluation axes, Capability Tests and Stress Tests, to systematically analyze what the model is good at, where it fails, and how reliable it is under different conditions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper proposes a dual-axis evaluation benchmark (“Capability + Stress”), redefining VLA model assessment from simple task success to a more diagnostic perspective that explains why models succeed or fail. This addresses the limitations of traditional single-metric benchmarks.\n2.\tThe authors design a structured hierarchy of capability dimensions and difficulty levels, enabling fine-grained and interpretable analysis across perception, language, spatial reasoning, and control skills.\n3.\tThe experiments cover multiple representative VLA models and clearly reveal common weaknesses in dynamic adaptation and spatial reasoning, supporting the paper’s main claims and demonstrating the framework’s practical value."}, "weaknesses": {"value": "1.\tSome parts of the paper use overly complex or obscure wording, which affects the overall readability and clarity of the writing. For instance, sentences such as “This challenge is exacerbated by a fragmented data landscape that impedes reproducible research and the development of generalist models” could be simplified for better clarity.\n2.\tThe authors may have chosen to conduct all experiments in simulation for better control and reproducibility. However, the lack of validation on real robotic or physical environments leaves the real-world reliability and generalizability of the results uncertain. Including experiments or analyses in real-world settings would strengthen the paper’s credibility and practical relevance."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o3ekBhttnO", "forum": "2SeNayaDJU", "replyto": "2SeNayaDJU", "signatures": ["ICLR.cc/2026/Conference/Submission9320/Reviewer_zcC7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9320/Reviewer_zcC7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827705302, "cdate": 1761827705302, "tmdate": 1762920957828, "mdate": 1762920957828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces NEBULA, a new benchmark for evaluating VLA agents. It argues that current benchmarks rely too much on simple task success rates and fail to show why models succeed or fail. NEBULA tries to fix that with what the authors refer to as dual-axis evaluation: “Capability Tests” that break down skills like control, perception, and language understanding, and “Stress Tests” that probe things like latency, stability, and adaptability under harder conditions. The authors build a dataset using ManiSkill3 and SAPIEN, define easy-medium-hard difficulty levels for each skill, and fine-tune several recent VLA models (GR00T-1.5, SpatialVLA, RDT-1B, etc.) on it. Their results show that while these models do well on perception and language, they fall apart on dynamic and robustness tasks. The paper’s main message is that we need benchmarks that measure how and when an agent works, not just whether it does."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and genuinely relevant. The paper tackles an important gap: how to evaluate VLA agents beyond simple success rates. This fits right into ongoing discussions about reliability, robustness, and diagnostic evaluation in embodied AI.\n2. The evaluated baselines are very recent and cover a good range of architectures and training paradigms, giving the comparison an up-to-date perspective.\n3. The visual presentation is strong. The figures are clear, visually appealing, and effectively convey experimental results and what the tasks look like.\n4. The difficulty levels for the Capability Test tasks are well-designed and described. The authors provide extensive details in Appendix A.1.1 on the criteria that constitute difficulty for each task family, supported by abundant visual examples. This gives a good understanding of the tasks.\n5. Their experimental findings are interesting (e.g., strong perception/language but weak adaptability/robustness) and mirror what many researchers suspect about current VLA models."}, "weaknesses": {"value": "1. Although the paper criticizes existing benchmarks for exclusively relying on end-task success rates, NEBULA ultimately reports the same metric in both the Capability Test and the Adaptability component of the Stress Test. The so-called “diagnostic” evaluations still reduce to binary task success. This comes across as rather hypocritical and weakens the motivation.\n2. Although the paper claims that each task “varies a single capability dimension while holding others constant,” the design never truly achieves isolation. In practice, all tasks inherently mix perception, control, language, and spatial reasoning. For example, even a “pure control” task like pushing or inserting an object still depends on perception to localize the object and spatial reasoning to align the arm. Similarly, perception tasks require grasping and placement, which involve motor control, and language tasks rely on both perception and actuation to interpret and execute instructions. The “*robustness*” axis is not an independent factor but rather a held-out evaluation set containing unseen variants and distribution shifts across multiple dimensions. As a result, the claimed factor isolation is more conceptual than practical.\n3. The description of fine-tuning baseline models could be improved. Although the authors do state that “All models are fine-tuned on NEBULA Alpha using their original training protocols”, while “keeping each model’s architecture, loss, and hyperparameters unchanged”, it remains unclear how much of the dataset is used, how long training lasts, or how “original protocols” are applied in this new setting. This harms clarity and reproducibility.\n4. The description of both the Capability and Stress Test execution is incredibly vague. The experimental setup is outlined only in broad strokes, making it nearly impossible to reproduce or even understand the evaluation process. How many tasks are evaluated? How many of each difficulty level? How many episodes per task? How long is an episode? Are there unrecoverable failure states that terminate the episode (cube/sphere knocked off the table)? None of this is stated. In the Stress Test, it remains completely unclear to me what defines a “pressure level.” The authors merely mention that each level is “defined by measurable parameters normalized to baseline conditions,” but never specify what those parameters are or how they change between *v1*, *v2*, and *v3*. While we can see plenty of visual examples of the tasks themselves, the reader is left with a lot of guessing work on how these evaluations were actually conducted.\n5. Inference Frequency and Latency in the Stress Test are directly inversely correlated, as evidenced by the flipped ranking of results in Figure 5. Both metrics capture the same property of real-time responsiveness, making their joint reporting somewhat redundant.\n6. The Stability Score in the Stress Test is defined such that larger action variations yield lower scores, which the authors interpret as “unreliable behaviors in dynamic scenarios where smooth, precise motion is essential.” However, large action jumps can in fact be optimal or required in certain contexts. For instance, when reacting to fast-moving objects, sudden environmental changes, or when strong corrective torques are needed for rapid stabilization. While I consider the metric itself to be interesting and commendable, I don’t think it should be framed as universally monotonic, since lower stability can reflect responsiveness rather than unreliability.\n7. The paper is difficult to follow due to its filler phrasing, excessive jargon, and overinflated claims. The text is saturated with tautologies, circular phrasing, and formulaic expressions, often overselling otherwise straightforward design choices. This lack of clarity obscures the actual technical contributions and makes the paper unnecessarily verbose. The authors appear to have misunderstood the purpose of disclosing their usage of LLMs (Appendix A.3), as requested by the ICLR submission guidelines. Instead of explaining how LLMs were used in the writing or analysis process, they mention VLAs as their experimental backbones. The paper’s phrasing, repetition, and stylistic patterns indicate heavy reliance on LLMs for writing, if not in other parts of the project. One of the goals of a research paper is to clearly convey its ideas to the readers, and this work falls short on that front. Moreover, a clear discrepancy exists between the polished, formulaic writing style of the paper and the visibly human-written task instructions. The latter is filled to the brim with grammatical and logical errors: “Yellow cube will not be used”, “Cubes **color switching** randomly”, “insert into the empty **slot**” (not socket), “Place the cube **that can fit the bin into the bin**”, Place the cube **that has different size** into the bin”, “Pick the cube” (not pick **up** the cube), “**Make red cube at bottom**”, “Simple contributes”, “Neg contribute”. Not only does this diminish the quality of the task instructions, but it further solidifies the aforementioned concern.\n\n### Minor Points\n\n1. The numeric value for SpatialVLA’s Stability Score under stress level *v3* in Figure 5 is floating far above the bar in the chart.\n2. For clarity, it would be helpful to include a legend for the colors representing the stress levels (*v1*, *v2*, *v3*) in Figure 5.\n\n### Typos\n\n1. Figure 1. - How stable are the policy’s control? → How stable is the policy’s control?\n2. Line 304 - L2 nortm"}, "questions": {"value": "1. What’s the difference between the **Robustness** metric in the Capability Test and the **Adaptability** metric in the Stress Test?\n2. Why is DP not included in the Adaptability test?\n3. How many tasks and episodes are used for evaluation in both the Capability and Stress Tests? What are the episode lengths?\n4. How many tasks belong to each difficulty level?\n5. What exactly defines each “pressure level” (v1, v2, v3)? Which parameters are being changed, and by how much?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8hqrU9FNDx", "forum": "2SeNayaDJU", "replyto": "2SeNayaDJU", "signatures": ["ICLR.cc/2026/Conference/Submission9320/Reviewer_PVpg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9320/Reviewer_PVpg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870673017, "cdate": 1761870673017, "tmdate": 1762920957382, "mdate": 1762920957382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current benchmarks in the vision-language-action (VLA) domain primarily focus on coarse-grained metrics such as task success rate. This paper proposes NEBULA, a novel benchmark that provides standardized APIs and a large-scale aggregated dataset. It introduces a comprehensive evaluation system to measure finer-grained capabilities and stress tests of VLA agents, enabling a more thorough analysis of failure modes in existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents a new dual-axis evaluation protocol, which enables controlled variable-based assessment across different capability dimensions.\n2. The proposed NEBULA benchmark offers standardized APIs and a large-scale dataset, ensuring high reproducibility."}, "weaknesses": {"value": "1. The delineation between different capability dimensions is unclear, and some metrics appear redundant:\n(1) The core distinction between dynamic adaptation in the capability test and adaptability in the stress test is not clearly explained.\n(2) Inference frequency and latency in the stress test seem inversely correlated, making their simultaneous evaluation unclear.\n2. Some evaluation dimensions lack discriminative ability, and task difficulty does not always follow a consistent ordering:\n(1) As shown in Figure 4, all models achieve 100% accuracy on perception tasks across all difficulty levels, indicating insufficient challenge for meaningful differentiation.\n(2) In Figure 3, RDT-1B performs better on medium-difficulty Dynamic tasks than on easy ones, and similarly, ACT outperforms on medium Robust tasks compared to easy tasks.\n(3) In stress tests, the distinction between difficulty levels is minimal—e.g., the Stability score only varies by 0.01.\n3. Section 4.4 only validates the isolation of perception tasks, without verifying the isolation of other capabilities.\n4. Minor issue: The order of capability descriptions in Section 3.2.1 does not match the presentation in Figure 2."}, "questions": {"value": "1. In Section 3.2.1, robustness/generalization is defined based on performance on unseen attributes or novel environments. How is this ensured? In Figure 2, the medium-difficulty task for Robustness appears identical to that of Control.\n2. Additional questions, please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XxyeduovR3", "forum": "2SeNayaDJU", "replyto": "2SeNayaDJU", "signatures": ["ICLR.cc/2026/Conference/Submission9320/Reviewer_kRug"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9320/Reviewer_kRug"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877286609, "cdate": 1761877286609, "tmdate": 1762920956645, "mdate": 1762920956645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "NEBULA proposes a novel dual-axis evaluation protocol to overcome these limitations. This protocol is composed of:\n\n- Capability Tests: These are designed to isolate and assess six core skills in a controlled manner: control, perception, language understanding, dynamic adaptation, spatial reasoning, and robustness/generalization. By varying only one capability dimension at a time while keeping others constant, these tests aim to pinpoint the precise reasons for an agent's failure.\n- Stress Tests: This axis evaluates an agent's performance under various operational pressures, such as inference frequency, latency, and stability. These tests are intended to map out an agent's reliability and identify \"failure cliffs\" where performance abruptly degrades.\n\nThe paper presents a comprehensive benchmarking study of several state-of-the-art VLAs using the NEBULA framework, revealing that even top-performing models struggle with spatial reasoning and dynamic adaptation—weaknesses often masked by traditional evaluation metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "By providing a standardized API and a large, aggregated dataset, NEBULA addresses the critical issue of data fragmentation in robotics research.\n\nThe experimental results effectively demonstrate the diagnostic power of NEBULA. The radar charts clearly illustrate the performance profiles of various state-of-the-art VLAs, highlighting common weaknesses in areas like spatial reasoning and dynamic adaptation that are often obscured by traditional end-task success metrics."}, "weaknesses": {"value": "- The name \"NEBULA\" and the framing of the six capabilities as fundamental and distinct may be overly ambitious. The distinction between these capabilities and the more traditional concepts of \"main tasks\" and \"sub-tasks\" is not clearly articulated. For complex manipulation, tasks are often decomposed into a sequence of simpler sub-tasks.\n\n- The paper's \"stress tests\" are confined to a simulated environment. True stress in robotics arises from the unpredictability of the real world, including sensor noise, actuator latency, unexpected physical contact, and dynamic lighting conditions. Adjusting parameters within a simulator, while useful, does not fully capture the complexities of real-world deployment and largely sidesteps the critical sim-to-real transfer problem.\n\n\n- While NEBULA provides a valuable diagnostic tool, the conclusions drawn from its application—that current VLA models struggle with generalization and dynamic tasks—are largely in line with findings from previous benchmarks. The paper successfully identifies problems but does not propose concrete solutions to address the identified weaknesses."}, "questions": {"value": "- Could the authors provide a more rigorous definition of the six proposed capabilities and explain how they are distinct from one another? For instance, spatial reasoning and control seem highly interdependent in many manipulation tasks. Is it always possible to isolate them effectively? \n\n- There appears to be a significant overlap between the concepts of \"capability tests\" and \"stress tests.\" For example, the \"Dynamic Adaptation\" capability test seems very similar in spirit to the \"Adaptability\" stress test. Could the authors clarify the conceptual boundary between these two axes of evaluation?\n\n- Given that the stress tests are conducted in simulation, how do the authors envision the insights from NEBULA translating to improved real-world robot performance? Are there plans to incorporate real-world experiments or to develop methodologies that explicitly aim to bridge the sim-to-real gap based on the findings from the NEBULA benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jEJ7xLl2E3", "forum": "2SeNayaDJU", "replyto": "2SeNayaDJU", "signatures": ["ICLR.cc/2026/Conference/Submission9320/Reviewer_iyWu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9320/Reviewer_iyWu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919109355, "cdate": 1761919109355, "tmdate": 1762920956109, "mdate": 1762920956109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}