{"id": "IHyY6vdYZw", "number": 11117, "cdate": 1758189656240, "mdate": 1759897607512, "content": {"title": "VisualPRM400K: An Effective Dataset for Training Multimodal Process Reward Models", "abstract": "We construct VisualPRM400K, a dataset comprising about 400K multimodal process supervision data. Building upon this dataset, we develop VisualPRM, an advanced multimodal Process Reward Model (PRM) capable of estimating the value score of each step during the reasoning process.\nUnder the Best-of-N evaluation setting, our model improves the reasoning performance of three types of MLLMs and four different model scales. \nEven when applied to the highly capable InternVL2.5-78B, it achieves a 5.9-point improvement across seven multimodal reasoning benchmarks.\nExperimental results show that the PRM model trained on our VisualPRM400K exhibits superior performance compared to Outcome Reward Models and Self-Consistency during BoN evaluation.\nTo further facilitate the development of multimodal PRMs, we construct VisualProcessBench, a benchmark designed to measure the abilities of PRMs and MLLMs to detect incorrect steps in multimodal reasoning tasks.\nWe hope that our work can inspire more future research and contribute to the development of MLLMs. Our model, data, and benchmark will be released.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Multimodal Process Reward Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e8e36e6f26be85e1d6ff63cea76826d2f97822c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents three significant contributions to the field of multimodal reasoning. First, it introduces VisualPRM400K, a large-scale dataset of approximately 400,000 samples with step-by-step process supervision, created using an automated data pipeline based on Monte Carlo sampling. Second, leveraging this dataset, the authors train VisualPRM, an 8B parameter Process Reward Model (PRM) designed to evaluate each step of a multimodal reasoning chain. Third, to facilitate the evaluation of such critic models, the paper proposes VisualProcessBench, a new human-annotated benchmark for identifying all incorrect steps within a reasoning process. The authors demonstrate that using VisualPRM as a critic in a Best-of-N (BoN) inference setting consistently improves the performance of various Multimodal Large Language Models (MLLMs) across seven reasoning benchmarks, outperforming Outcome Reward Models (ORMs) and Self-Consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Significant and High-Quality Data Contribution: The primary strength of this work lies in the creation and release of two valuable resources: the large-scale VisualPRM400K training dataset and the high-quality, human-annotated VisualProcessBench. Constructing such resources is a laborious but crucial endeavor for the community. VisualPRM400K is, to my knowledge, the first large-scale dataset for training multimodal PRMs, and VisualProcessBench provides a much-needed, fine-grained benchmark for evaluating them. These resources will undoubtedly catalyze future research in multimodal reward modeling and reasoning.\n2. Thorough and Rigorous Experimentation: The authors have conducted an extensive set of experiments to validate their contributions. They demonstrate the effectiveness of VisualPRM across multiple model families (MiniCPM, Qwen, InternVL) and scales (from 8B to 78B). The ablation studies are comprehensive, comparing PRM with ORM and Self-Consistency, and analyzing the impact of various hyperparameters. The evaluation on VisualProcessBench, which shows that VisualPRM is competitive with powerful proprietary models like Gemini-2.0-Flash, further solidifies the quality of the trained reward model.\n3. Well-Written and Clearly Presented: The paper is exceptionally well-organized and clearly written. The motivation, methodology, and results are presented in a logical and easy-to-follow manner, making the paper's contributions accessible and understandable."}, "weaknesses": {"value": "1. Performance Gains Comparison: The central application of VisualPRM is to improve MLLM reasoning via Best-of-N (BoN) inference. While the reported gains are consistent (e.g., +5.9 points for InternVL2.5-78B), the final performance often falls short of what has been achieved by other contemporary methods that focus on improving the policy model itself through advanced training techniques.\n2. In contrast, the approach in this paper is purely an inference-time strategy. While it successfully lifts the performance of existing models, it does not fundamentally enhance the models' intrinsic reasoning capabilities. The resulting performance, while improved, does not appear to push the state-of-the-art boundary as significantly as these training-focused methods.\n3. High and Under-discussed Inference Cost: The Best-of-N strategy is notoriously expensive. Using BoN with N=8, as is the default in this paper, multiplies the inference cost (both latency and compute) by at least a factor of 8, plus the overhead of running the VisualPRM critic. This makes the method impractical for many real-world applications. While the authors demonstrate performance scaling up to N=128, they do not provide a thorough discussion on the cost-performance trade-off. A more complete analysis would be necessary to assess the practical viability of this approach. The reported performance gains, while notable, may not be sufficient to justify such a substantial increase in inference cost for many use cases."}, "questions": {"value": "1. Could you provide a more direct comparison of your final BoN results with other state-of-the-art methods on the same benchmarks? How does the performance of, for example, \"InternVL2.5-78B + VisualPRM\" compare to models that have been fine-tuned with advanced RL or self-improvement techniques? This would help contextualize the significance of the improvements you've achieved.\n2. Could you elaborate on the inference latency and computational cost of using VisualPRM in a BoN setting? For instance, what is the wall-clock time required to evaluate a single instance with N=8 compared to a single pass from the base model? A cost-benefit analysis would be extremely valuable for readers to understand the practical implications of your method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o76IY2ChPC", "forum": "IHyY6vdYZw", "replyto": "IHyY6vdYZw", "signatures": ["ICLR.cc/2026/Conference/Submission11117/Reviewer_DRVz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11117/Reviewer_DRVz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760688536936, "cdate": 1760688536936, "tmdate": 1762922290501, "mdate": 1762922290501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VisualPRM400K, a ~400K multimodal process-supervision dataset built via Monte-Carlo (MC) completion to estimate step-wise correctness; an 8B VisualPRM trained on it; and VisualProcessBench, a PRM/MLLM benchmark with 2,866 samples and 26,950 human-annotated step labels. With Best-of-N (BoN) test-time scaling, VisualPRM substantially improves multiple MLLMs (e.g., +8.4 for InternVL2.5-8B; +5.9 for InternVL2.5-78B) and outperforms outcome reward models and self-consistency as the critic."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Timely, practical contribution: A large multimodal process dataset plus a purpose-built benchmark for PRMs addresses a clear gap and enables systematic progress on multimodal TTS. \n\nSolid empirical evidence: Consistent BoN gains across model scales; clear comparisons vs. ORM and self-consistency; ablations on value- vs advantage-based PRMs and score aggregation. \n\nClear PRM formulation: Step-wise discretized targets, single-pass scoring efficiency, and supervising all steps rather than stopping at first error are well motivated and validated."}, "weaknesses": {"value": "Potential generator-bias in labels: Process rewards are derived from continuations sampled with InternVL-2.5 models. This may bias the PRM toward InternVL-style reasoning and limit transfer to other families (e.g., GPT-5, Qwen-VL). \n\nMC estimation only: The paper uses MC to estimate expected accuracy per step but does not compare with alternative credit assignment/judging strategies (e.g., MCTS, LLM-as-a-Judge).\n\nModel size choice and scaling law: VisualPRM is fixed at 8B; the paper lacks justification for this size and a scaling curve (e.g., 1B/3B/8B/14B) to reveal accuracy–latency–cost trade-offs."}, "questions": {"value": "On process reward generation: You estimate step values via MC sampling. Have you tried other strategies such as MCTS rollouts or LLM-as-a-Judge?\n\nOn PRM capacity: Why 8B? Have tried other PRM size (e.g., 1B/2B/9B/14B)? \n\nRelated work coverage (multimodal PRMs): The Related Work section focuses primarily on text-only PRMs. It should discuss recent multimodal PRM papers such as DreamPRM [1], AR-MCTS [2].\n\n[1] DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning (NeurIPS 2025)   \n[2] Progressive Multimodal Reasoning via Active Retrieval (ACL 2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iltAOwffwT", "forum": "IHyY6vdYZw", "replyto": "IHyY6vdYZw", "signatures": ["ICLR.cc/2026/Conference/Submission11117/Reviewer_edvQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11117/Reviewer_edvQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951129377, "cdate": 1761951129377, "tmdate": 1762922289809, "mdate": 1762922289809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VisualPRM400K, a ~400K-sample multimodal process-supervision dataset with step-wise expected-accuracy labels, and VisualProcessBench, a 2,866-sample benchmark with 26,950 human step-correctness annotations, to enable and evaluate process reward models (PRMs) as critics for Best-of-N test-time scaling in MLLMs. Trained as an 8B PRM that scores each reasoning step in a single forward pass, VisualPRM achieved great improvement across seven multimodal reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well-written and easy to follow, with a clear description of the method.\n2. The paper provides intuitive visual demonstrations to help better understand the paper."}, "weaknesses": {"value": "1. Label quality & construction pipeline clarity. The Monte-Carlo step-correctness (`Eq. (2)`) relies on continuations sampled from an unspecified model $M$; this risks systematic bias/noise if $M$ shares failure modes with the policy models later evaluated. In addition, merging solutions to a maximum of 12 steps may distort error localization and the temporal dynamics of “first error vs. downstream errors.” Please quantify label noise (e.g., step-level inter-rater agreement on a subset), report sensitivity to the number of sampled continuations, and analyze the effect of step-merging on PRM accuracy.\n\n2. Fairness of comparisons. Please clarify whether the reported gains in `Tab.2` are measured only against each policy model’s base (“Pass@1”) or also against strong critic baselines. To more comprehensively validate the effectiveness of the proposed approach, include outcome-based reward (ORM) and additional PRM baselines under identical Best-of-N settings (same candidate pool, N, decoding, and compute). Also expand `Tab.3` with more PRM variants and report matched-compute results.\n\n3. Generalization beyond the current suite. Beyond the seven benchmarks used (six math, one multidisciplinary), consider evaluating on more general-purpose multimodal benchmarks and on broader text-only reasoning benchmarks to substantiate cross-domain robustness.\n\n4. Limited technical novelty; strengthen the case for multimodality. The paper’s primary contributions appear to be `VISUALPRM400K` and `VisualProcessBench`, while methodological novelty is modest. To demonstrate that the *multimodal* aspect is indispensable (rather than a text-dominant signal), please add modality ablations and analyses showing performance drops when visual evidence is removed or corrupted. Such results would clarify the unique value of multimodal supervision/assessment relative to single-modal PRMs."}, "questions": {"value": "See the `Weaknesses` part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TtRB4g9b31", "forum": "IHyY6vdYZw", "replyto": "IHyY6vdYZw", "signatures": ["ICLR.cc/2026/Conference/Submission11117/Reviewer_h3nV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11117/Reviewer_h3nV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981290980, "cdate": 1761981290980, "tmdate": 1762922289394, "mdate": 1762922289394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new dataset to train multimodal Process Reward Models, a new benchmark to evaluate MM PRMs and an 8B-parameter PRM that consistently, across different model series and sizes, improves performance over MM reasoning benchmarks. The PRM is shown to outperform other TTS algorithms like Outcome Reward Models, Self-Consistency and MLLMs as critic models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tTTS and using PRMs as reward functions in RL are under explored in multimodal modeling. The work here pledges to open source a large 400k sample dataset to train MM-PRMs, a benchmark to evaluate MM-PRMs and a trained MM-PRM. These contributions can be valuable for the community and foster further research.\n2.\tEffectiveness of PRMs used for TTS in improving MM reasoning across multiple model series and sizes are clearly demonstrated along with improvements compared to other TTS methods.\n3.\tDesign of the PRM benchmark  is sound with the main standouts being, considering all the erroneous reasoning steps as opposed to stopping at the first occurrence, using macro F1 scores to account for class imbalance and multiple math and reasoning datasets."}, "weaknesses": {"value": "1.\tPRMs are well studied in the language modeling space. This work repurposes those studies and algorithms to the multimodal space with limited algorithmic novelty.\n2.\tAn automated Monte Carlo sampling-based pipeline is used to generate the PRM400k dataset. There are no discussions on the quality of this dataset and alignment with human judgement. How the authors ensure incorrect demonstrations are filtered out and how this could affect the trained PRM’s abilities as a critic are not discussed. 10% of the reasoning steps are negative demonstrations. Effect of PRM modeling with more balanced positive and negative demonstrations, using weaker models and models from other series to introduce diverse thinking styles is not explored.\n3.\tThe task categories in VisualProcessBench are mostly centered around math and logic. Extensibility of this methodology for other vision applications like chart, table, GUI reasoning among others will be helpful.\n4.\tWhile PRM is shown to outperform other Best-of-N strategies, a discussion about latency and throughput tradeoffs compared to other light-weight strategies can strengthen the claim."}, "questions": {"value": "1.\tCan the authors quantify FP and FN rates for a sub-sample of the VisualPRM400k dataset and explain if they have any filtering steps to identify and remove such demonstrations?\n2.\tCan the authors provide accuracy-vs-latency plots at multiple N comparing different BoN techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lhzlKx8IVL", "forum": "IHyY6vdYZw", "replyto": "IHyY6vdYZw", "signatures": ["ICLR.cc/2026/Conference/Submission11117/Reviewer_Ppnx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11117/Reviewer_Ppnx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981568339, "cdate": 1761981568339, "tmdate": 1762922288855, "mdate": 1762922288855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}