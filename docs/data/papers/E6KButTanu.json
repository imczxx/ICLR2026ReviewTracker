{"id": "E6KButTanu", "number": 1161, "cdate": 1756855540064, "mdate": 1759898224528, "content": {"title": "Data Valuation by Leveraging Global and Local Statistical Information", "abstract": "Data valuation has garnered increasing attention in recent years, given the critical role of high-quality data in various applications. Among diverse data valuation approaches, Shapley value-based methods are predominant due to their strong theoretical grounding. However, the exact computation of Shapley values is often computationally prohibitive, prompting the development of numerous approximation techniques. Despite notable advancements, existing methods generally neglect the incorporation of value distribution information and fail to account for dynamic data conditions, thereby compromising their performance and application potential. In this paper, we highlight the crucial role of both global and local statistical properties of value distributions in the context of data valuation for machine learning. First, we conduct a comprehensive analysis of these distributions across various simulated and real-world datasets, uncovering valuable insights and key patterns. Second, we propose an enhanced data valuation method that integrates the explored distribution characteristics into two regularization terms to refine Shapley value estimation. The proposed regularizers can be seamlessly incorporated into various existing data valuation methods. Third, we introduce a novel approach for dynamic data valuation that infers updated data values without recomputing Shapley values, thereby significantly improving computational efficiency. Extensive experiments have been conducted across a range of tasks, including Shapley value estimation, value-based data addition and removal, mislabeled data detection, and dynamic data valuation. The results showcase the consistent effectiveness and efficiency of our proposed methodologies, affirming the significant potential of global and local value distributions in data valuation.", "tldr": "This study proposes enhanced data valuation methods that integrate both global and local statistical properties of value distributions, achieving superior performance and efficiency.", "keywords": ["Data valuation", "Value distribution", "Shapley value", "Average marginal effect", "Dynamic valuation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf4fe9f790e43ecdc459f7764d3cff9f18f3ef10.pdf", "supplementary_material": "/attachment/d72375993f383cdaf54670717b2225684c47c886.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Global and LOcal Characteristics-based (GLOC) data valuation, an improved approach for efficient data valuation built upon an earlier method called the Average Marginal Effect (AME)  (Lin et al., 2022) or Shapley value approach. Instead of relying on Lasso regression, the authors modify the objective with two key enhancements: a global Gaussian-based term and a local neighborhood-consistency term. The global term replaces the original Laplace prior with an L2 norm, motivated by the empirical observation that data Shapley values more closely follow a Gaussian rather than Laplace distribution. The local term enforces that data points with similar features and labels should have more consistent values, refining Shapley estimation through neighborhood regularization. Additionally, the paper develops approaches to allow efficient updates when data points are added or removed without recomputing Shapley values from scratch. They conduct experiments across multiple tasks and justify the performance GLOC."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed approach is simple and efficient, yet appears effective in experiments. I liked the insights about the empirical distribution observations about Shapley values and how that help us to refine AME approach. \n2. The empirical evaluation is thorough, with different datasets, different tasks and comparisons with multiple natural baselines. The implementation details are also nicely described. \n3. Overall writing of the paper is great."}, "weaknesses": {"value": "I generally liked the paper and its effort. The only limitation is that while the methods seem empirically effective, the underlying idea (both the change of prior distribution and regularization) appears a bit incremental, though I do not see this as a significant limitation."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cNmDF7Edbc", "forum": "E6KButTanu", "replyto": "E6KButTanu", "signatures": ["ICLR.cc/2026/Conference/Submission1161/Reviewer_zxMX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1161/Reviewer_zxMX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670965368, "cdate": 1761670965368, "tmdate": 1762915694099, "mdate": 1762915694099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel, distribution-aware perspective for data valuation. The authors identify key global and local statistical characteristics of the data values and introduce a new valuation method GLOC. Furthermore, the paper proposes an efficient dynamic valuation framework (IncGLOC/DecGLOC) capable of updating data values without full Shapley value recalculation. Comprehensive experiments across multiple tasks validate the method's effectiveness and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel premise. It explores the \"value distribution\" dimension, a concept largely overlooked by prior work, offering a new approach to regularizing the ill-posed problem of SV estimation.\n2. The local regularization term in GLOC effectively encodes value consistency. This design is not only sound but also easily integrated into other data valuation frameworks, demonstrating good extensibility.\n3. The evaluation is comprehensive, covering both proxy SV estimation and multiple downstream tasks (e.g., mislabeled data detection, data addition/removal) that measure practical utility. The comparison against a wide array of baselines is commendable."}, "weaknesses": {"value": "1. The dynamic valuation method (e.g., the $\\epsilon_i$ bound) is effective but based on heuristics. It currently lacks a theoretical justification or convergence analysis.\n2. The authors claim their method is an approximate method of Shapley value, but the added regularization term violates axioms such as efficiency, symmetry, and additivity. The paper does not prove whether or how these properties are preserved. \n3. The Shapley estimation experiment compares only against AME, which is itself a weak Monte Carlo baseline."}, "questions": {"value": "1. Section 4.2 should be updated to explain how the GLOC objective (Eq. 5) remains stable in the under-determined $M \\ll N$ setting. The relative importance of $R_g$ vs. $R_l$ as stabilizers should be discussed.\n2. The metrics focus on task accuracy after removing/adding points, which reflect ranking utility, not estimation fidelity. The paper should report MSE/MAE/Spearman/ correlation against the exact Shapley value on enumerable datasets.\n3. The definition of the neighborhood similarity metric S_{i,j} and the variation bound \\epsilon_i involves multiple heuristic constants without sensitivity analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "87c04bEqhh", "forum": "E6KButTanu", "replyto": "E6KButTanu", "signatures": ["ICLR.cc/2026/Conference/Submission1161/Reviewer_NiFP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1161/Reviewer_NiFP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894671993, "cdate": 1761894671993, "tmdate": 1762915693768, "mdate": 1762915693768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Based on the empirical observation of local and global statistics of data values computed by AME, the paper proposes two regularizers, which appear in Eq. (5), that use the corresponding statistics as priors to expedite the computation of data values. Intuitively, these two regularizers may be used for other optimization-based data valuation methods. Moreover, the authors also empirically study the statistics before and after adding or deleting data, and then develop an optimization, as defined in Eq. (8), to efficiently adjust the computed data values accordingly."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors conduct extensive experiments to support their claims.\n\n- I think it is a sensible approach to start from empirical observations."}, "weaknesses": {"value": "- I am concerned with the use of AME for approximating the Shapley value. As far as I know, there is no theoretical guarantee that the solution of AME would converge to the Shapley value. In theory, I do not see this is possible. The underlying $p$ for the Shapley value is the uniform distribution over the closed interval $[0,1]$, and then the value of $v$ defined in 257 is equal to infinity. Notice that $v$ is used in constructing $\\mathbf{X}$ in Eqs. (3) and (5), which makes AME infeasible. To circumvent this issue, the AME paper proposes to use the uniform distribution over $[\\epsilon, 1-\\epsilon]$ instead. However, this would unnecessarily introduce some bias. The provided results would be more convincing if the authors use KernelSHAP instead, which has theoretical guarantees on its convergence to the Shapley value; see [1].\n\n   - In particular, the used distribution of $p$ is provided in line 901, which is $\\mathrm{Uniform}\\\\{0.2, 0.4, 0.6, 0.8\\\\}$. This indicates that the solution of AME converges to $\\boldsymbol\\omega^*$ defined as\n\n        $$  \n        \\omega\\_{i}\\^{*} = \\sum\\_{S \\subseteq [N]\\setminus \\\\{i\\\\}} \\frac{1}{4}\\left( \\sum\\_{w\\in\\\\{0.2,0.4,0.6,0.8\\\\}} w\\^{|S|}(1-w)\\^{N-|S|-1}\\right)\\cdot[U(S\\cup\\\\{i\\\\}) - U(S)]\n        $$\n\n        where $U$ denotes the utility function used to construct $\\mathcal{U}$ in Eq. (3). Clearly, this is not the Shapley value. **Therefore, it is remarkably erroneous that the paper claims the Shapley value as the ground truth throughout.**\n\n- **Theoretically, the imposed regularizers shift the convergence point, which makes the empirical MSE results (e.g., Table 1) questionable**. Their claims make it seem as if the performance boost in value-based point addition and removal comes from better approximating some ground truth, but this is misleading.\n\n- In lines 340-341, I do not think that $|\\hat{\\mathcal{D}}|/|\\mathcal{D}|$ is a reasonable quantity that measures the variation brought by adding $\\mathcal{D}'$. For example, if $\\mathcal{D}'$ is constructed by perturbing all data in $\\mathcal{D}$ a little bit, making $\\mathcal{D}'$ almost a copy of $\\mathcal{D}$ but not the same, then I do not see it would introduce variation.\n\n\nOverall, the paper notably lacks coherence, as its theoretical justification does not align with the empirical results or the authors’ claims.\n\n**Not weaknesses**\n\n- It looks like that the definition of $\\overline{\\epsilon}$ in Eq. (8) is missing in the main paper?\n\n- In line 364-365, the authors may intend to mean ''not only for AME'' instead of \"except for AME?\"\n\n**References**\n\n[1] Chen, T., Seshadri, A., Villani, M. J., Niroula, P., Chakrabarti, S., Ray, A., ... & Kumar, N. (2025). A Unified Framework for Provably Efficient Algorithms to Estimate Shapley Values. In NeurIPS 2025."}, "questions": {"value": "-  Why the two introduced regularizers would help expedite the convergence in theory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w5IT1LoAhw", "forum": "E6KButTanu", "replyto": "E6KButTanu", "signatures": ["ICLR.cc/2026/Conference/Submission1161/Reviewer_HLw3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1161/Reviewer_HLw3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924971935, "cdate": 1761924971935, "tmdate": 1762915693576, "mdate": 1762915693576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an enhanced Shapley-value-based data valuation approach that leverages both global and local statistical properties of data value distributions. The authors introduce new regularization terms into an existing valuation framework  to incorporate distribution characteristics, aiming to refine the estimation of each data point’s value. They further present a dynamic data valuation technique that efficiently updates data values when the dataset changes  without recomputing Shapley values from scratch. Through experiments on various tasks, the paper claims improved performance and efficiency over baseline methods, demonstrating the potential benefits of using global and local value distribution information in data valuation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of integrating global and local value distribution statistics into data valuation is novel.\n- The proposed method is easy to implement. It integrates with existing AME pipelines, and scales via convex solvers\n- The paper provides extensive experimental validation across multiple datasets and tasks."}, "weaknesses": {"value": "- The core conceptual novelty is a new regularizer and an inference procedure, which is modest and would be strengthened by stronger theoretical analysis. Although the idea is well-motivated by empirical observations  especially when recalculating Shapley values is expensive, but the combination of regularization and constrained quadratic inference is somewaht incremental\n- Inc/DecGLOC are heuristic but reasonable. The new regularizers and dynamic update rules appear to be designed empirically based on observed patterns, which raises concerns about how generally they will apply beyond the tested cases. Clarifying the rationale behind these specific design choices or deriving them from a theory would strengthen the contribution.\n- Much of the analysis uses AME estimates and sometimes AME is used for initial values, which risks propagating AME biases into conclusions. It would be helpful to validate the Gaussian and neighborhood claims against exact Shapley values on small datasets  or against Monte Carlo estimates with very large sampling."}, "questions": {"value": "- What is the justification for the specific regularizers and distribution assumptions chosen?  It would help to clarify why these forms  are expected to improve data value estimates in general.\n- can the authors provide arguments or proofs about convergence to true Shapley values, or conditions under which the regularized estimation remains accurate?\n- Can the authors show exact-Shapley-based distributions for a few small datasets  to confirm that true Shapley values  are approximately Gaussian?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7Tsn5f3cqW", "forum": "E6KButTanu", "replyto": "E6KButTanu", "signatures": ["ICLR.cc/2026/Conference/Submission1161/Reviewer_fPii"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1161/Reviewer_fPii"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995653728, "cdate": 1761995653728, "tmdate": 1762915693464, "mdate": 1762915693464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **GLOC**—an AME-style regression objective enhanced with two *distribution-aware* regularizers: a **global Gaussian prior** (L2/Ridge instead of AME’s L1/Laplace) and a **local neighborhood-consistency** term (same-class neighbors closer; different-class neighbors separated). It also presents **IncGLOC/DecGLOC** for **dynamic** scenarios, updating values after data addition/removal **without recomputing Shapley**. Across 12 datasets, under a unified **logistic-regression** surrogate (pretrained embeddings for text/image), GLOC reduces MSE to Shapley references and improves value-guided curation and mislabeled/noisy detection; Inc/DecGLOC are competitive and more efficient for dynamics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Motivated & simple:** Empirical findings (“global ≈ Gaussian; local same-class consistency”) are encoded as two lightweight regularizers atop AME (Ridge + neighborhood Laplacian).\n- **Dynamic efficiency:** Inc/DecGLOC infer updated values from the new dataset and prior values without recomputing Shapley, reducing time in add/remove settings.\n- **Clean protocol:** A fixed logistic-regression surrogate (pretrained DistilBERT/ResNet50 for features) makes cross-method comparison reproducible.\n- **Consistent gains:** Lower MSE to Shapley references; better value-based add/remove and mislabeled detection on multiple datasets; useful ablations (Rg/Rl, k, ε₀) and similarity choices.\n- **Modular:** The regularizers are shown to be usable beyond AME (integration or post-refinement)."}, "weaknesses": {"value": "- **Novelty is modest (accurate statement):** The main contributions are *regularizing* AME with a Gaussian prior (L2) and a neighborhood-consistency term, plus a heuristic dynamic update; conceptually this is an **incremental** extension rather than a new valuation principle.\n- **No end-to-end deep validation:** Experiments use a **logistic-regression** surrogate with **pretrained** DistilBERT/ResNet50 **feature extraction**; there is no end-to-end CNN/Transformer training, which may limit claims about generality to deep models.\n- **Limited formal guarantees for dynamics:** The dynamic method sets the per-point change bound \\( \\epsilon_i \\) **heuristically** (function of dataset/neighbor changes) and does **not** provide formal error/stability bounds vs. recomputing Shapley.\n- **Ground-truth choice for MSE:** For static estimation, “ground-truth Shapley” is proxied by **AME with very large M** (equal to training size). While standard in this line, it is still a proxy rather than exact Shapley."}, "questions": {"value": "Same with weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IeEUsAnK9r", "forum": "E6KButTanu", "replyto": "E6KButTanu", "signatures": ["ICLR.cc/2026/Conference/Submission1161/Reviewer_TjRT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1161/Reviewer_TjRT"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112014454, "cdate": 1762112014454, "tmdate": 1762915693319, "mdate": 1762915693319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}