{"id": "gwysQdBY1q", "number": 1696, "cdate": 1756908134460, "mdate": 1759898194084, "content": {"title": "Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis", "abstract": "Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model’s inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely “think with images” through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.", "tldr": "We introduce a medical MLLM that acquires fine-grained “thinking-with-images” capabilities by learning to use visual tools, yielding marked improvements in various medical tasks..", "keywords": ["Multimodal Large Language Models", "Tool-integrated Reasoning", "Thinking with Images", "Unified Perception for Medical Images"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39bfdcd0da208cf5d133fb6f7941c64dfe667a56.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **Ophiuchus**, a framework designed to equip Multimodal Large Language Models (MLLMs) with the ability to perform interleaved vision-language reasoning by dynamically invoking external tools for fine-grained medical image analysis. The core problem addressed is the limitation of current medical MLLMs, which often rely on static, global image perception and text-only reasoning chains, leading to missed fine-grained details and a lack of iterative visual re-examination. Ophiuchus enables a model to autonomously decide when to use a tool, which tool to use (e.g., segmentation models like SAM2/BiomedParse or a zoom-in function), and how to integrate the tool's output back into its reasoning process. The authors propose a three-stage training strategy: 1) Cold-start Supervised Fine-Tuning to learn basic tool invocation from demonstration data; 2) Self-Reflection Fine-Tuning to encourage the model to reassess and correct its tool-use strategies; and 3) Agentic Tool Reinforcement Learning (ATRL) to optimize task performance with fine-grained rewards. The method is evaluated across eight medical VQA and segmentation benchmarks, where it is shown to achieve state-of-the-art performance, significantly outperforming both general-purpose and medical-specific MLLMs, including other tool-augmented agents."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "### **Originality:**\nThe paper introduces a novel framework that creatively combines several existing ideas (tool use, CoT, RL) into a cohesive and new formulation for medical image analysis. The specific three-stage training strategy, particularly the Self-Reflection Fine-Tuning and the fine-grained ATRL reward design, demonstrates significant originality in execution\n\n### **Quality:** \nThe technical quality is high. The experimental design is thorough, the benchmarks are diverse and challenging, and the ablation studies are conclusive. The work goes to great lengths to rule out trivial explanations for its success (e.g., comparison with GT-crop baselines). Demonstrated ability to adapt to unseen segmentation models (MedSAM) without retraining, indicating genuine compositional generalization.\n\n### **Clarity:**\nThe paper is very clearly written. The problem is well-motivated, the method is broken down into digestible components, and the figures effectively illustrate the core concepts and results.\n\n### **Significance:**\nThis work has high significance. It pushes the boundary of what is possible with MLLMs in medical imaging by moving beyond passive perception to active, tool-driven visual exploration. The released code, models, and datasets will serve as a valuable foundation for future research in developing more capable and reliable medical AI agents. However, I preferred to see anonymized code submission instead of promises."}, "weaknesses": {"value": "Here are the weaknesses in order of importance:\n\n### **Isolating the Effect of Self-Reflection:**\nWhile Table 2 validates the contribution of the full three-stage pipeline, the marginal gain from the self-reflection stage (M_cold+reflect vs. M_cold) appears modest. It is unclear whether this gain stems from the proposed self-reflection mechanism itself or simply from the additional two epochs of fine-tuning on a curated subset of data. A stronger ablation would be to compare M_cold+reflect (trained for 10 + 2 epochs) against an M_cold-12 baseline that undergoes 12 epochs of standard SFT (or whatever amount of compute that makes the comparison fair) on the original D_cold dataset. This would help isolate the value of the self-reflective data and objective from the confounder of additional training time. You can also think of having an ablation showing a qualitative analysis of how often the model successfully self-corrects after this stage (before RL), compared to the cold-start model.\n\n### **Limited Validation of Synthetic Data Quality:**\nThe dataset relies heavily on GPT-5 and Gemini-generated reasoning traces and QA pairs. There is limited discussion or empirical verification of annotation quality, potential hallucinations, or domain bias introduced by these models.\n\n### **Contextualization with Recent Agentic Frameworks:** \nThe related work section provides a good overview but could be strengthened by discussing several very recent and highly relevant agentic frameworks for medical imaging. Works such as AURA (Fathi et al., 2025)[1], SMR-Agents (Zhang et al., 2025)[2], and MedAgent-Pro (Wang et al., 2025c)[3] that also explore multi-modal agents that integrate visual grounding and multi-step reasoning for diagnosis. A discussion comparing Ophiuchus's single-agent, end-to-end trained paradigm against the multi-agent, workflow-based approaches of these systems would provide better context. Furthermore, including these as baselines in the experiments, if feasible, would offer a more complete and compelling demonstration of Ophiuchus's advantages.\n\n\n### **Tool Failure Robustness:** \nThe failure case in Appendix D.2 is instructive but highlights a remaining challenge: the model's performance is ultimately bounded by the reliability of the external tools. While Ophiuchus can identify a tool failure, it lacks a fallback strategy beyond rejecting the tool's hypothesis. Exploring methods for more robust recovery from persistent tool failures (e.g., attempting a different analysis pathway) could be a valuable direction\n\n### **2D-Centric Focus:**\nThe current work is limited to 2D images. For medical imaging, where 3D context (e.g., in CT, MRI) is often critical, this is a limitation. The authors correctly identify this and note it as future work.\n\n### **Reproducibility During Review:** \nWhile the authors provide a comprehensive description of their method and commit to releasing code and datasets, the current review process would have been strengthened by an anonymous code submission. The complexity of the three-stage training pipeline, the custom ATRL implementation, and the dataset curation process are non-trivial. Access to the code would have allowed for a more thorough verification of the experimental setup and the claimed implementation details, ensuring the results are fully reproducible as stated\n\n### **Minor Typos:**\nPage 3, Section 3.1: “A image zoom-in function” → “An image zoom-in function”.\n\nPage 4, Figure 2: “cold-started model” → “cold-start model”  \n\nPage 9, section 5: “invocating external tools” → “invoking external tools”\n\n\n\n### **References:**\n\n[1] Fathi et al., “AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation”\n\n[2] Zhang et al., “SMR-Agents: Synergistic Multi-Agent Medical Reasoning”\n\n[3] Wang et al., “MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow”"}, "questions": {"value": "Please check the **Weaknesses** section for more questions as well as the questions listed below.\n\n\n### **Generalization to Unseen Tools:** \nThe results in Appendix C.2 showing generalization to the unseen MedSAM tool are impressive. Could the authors provide more insight into the mechanism behind this? Is it primarily the structured tool-calling format and the general nature of the rewards that enable this, or does the model learn a more abstract understanding of tool functionality?\n\n### **Sensitivity to Tool Quality:** \nHow sensitive is Ophiuchus's performance to the inherent accuracy of its underlying tools (SAM2, BiomedParse)? If these tools were replaced with less accurate versions, how steep would the performance degradation be? An ablation with intentionally \"noisier\" tools could shed light on the framework's robustness.\n\n### **Quantifying \"Thinking\" Patterns:** \nThe qualitative case studies in Appendix D are excellent. Is it possible to provide a quantitative analysis of the emergence of these different reasoning patterns (visual-cue search, confirmation, mitigation)? For example, what percentage of queries on the test set lead to a \"hallucination mitigation\" trajectory?\n\n\n**Note:** The paper in its current form is very strong, and my **initial high scores** reflect this. However, these scores are contingent on the authors' responses. A failure to adequately address the questions above could lead to a reduction in the final score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EmWUBQZ2i5", "forum": "gwysQdBY1q", "replyto": "gwysQdBY1q", "signatures": ["ICLR.cc/2026/Conference/Submission1696/Reviewer_39uY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1696/Reviewer_39uY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761612863083, "cdate": 1761612863083, "tmdate": 1762915858868, "mdate": 1762915858868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Ophiuchus, a versatile tool-augmented framework. Unlike prior approaches, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools to enable higher-level reasoning. Extensive experiments demonstrate that Ophiuchus consistently outperforms both closed-source and open-source state-of-the-art methods across diverse medical benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a tool-augmented interleaved vision-language reasoning paradigm that enables medical MLLMs to adaptively process localized visual evidence.\n2. Proposed Ophiuchus achieve SOTA performance across multiple medical benchmarks."}, "weaknesses": {"value": "1. The authors introduce a tool-augmented 'thinking with images' interleaved vision-language reasoning paradigm and a multi-stage training framework. Since these concepts have been widely studied in general-domain MLLMs, could the authors elaborate on the key differences between their approach and existing methods? Are there specific adaptations or modifications tailored to the medical domain?\n2. The authors use SAM2 as the image analysis tool. Since SAM2 is not specifically trained on medical data, could this affect the segmentation quality and overall performance?\n3. The current experimental evaluation primarily focuses on medical QA tasks. Would the proposed method be effective for report generation tasks as well?"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uWRPYW80XC", "forum": "gwysQdBY1q", "replyto": "gwysQdBY1q", "signatures": ["ICLR.cc/2026/Conference/Submission1696/Reviewer_XfkZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1696/Reviewer_XfkZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823686201, "cdate": 1761823686201, "tmdate": 1762915858700, "mdate": 1762915858700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a medical MLLM agent that leverages Chain-of-Thought with tool calls during reasoning. A three-stage training pipeline is designed: (i) cold-start SFT on curated tool-integrated trajectories; (ii) self-reflection fine-tuning, and (iii) Agentic Tool Reinforcement Learning optimized with GRPO. The experiments on multiple MedVQA benchmarks show that the proposal outperforms previous methods, including general LLMs, medical LLMs, and medical agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written.\n2. The presented method is clear and well-organized.\n3. The experiments are extensive."}, "weaknesses": {"value": "1. The novelty of the idea is limited. The paper presents several tool-augmented multimodal medical agents but misses many existing general and text-based medical agents augmented with tools, e.g., [1][2]. Meanwhile, the number of incorporated tools (only three) is limited compared to existing works.\n\n[1] AgentMD: Empowering language agents for risk prediction with large-scale clinical tool learning. arXiv preprint, 2024.\n[2] RiskAgent: Autonomous Medical AI Copilot for Generalist Risk Prediction. arXiv preprint, 2025.\n\n2. The tools' performance is not provided. These results could serve as the upper bound of the overall system’s performance.\n\n3. The fairness of the comparison should be improved.\n- The proposed model uses external tools and strong external models (Gemini-2.5-Pro and GPT-5) to improve performance. However, previous medical LLMs and similar agentic-based methods did not use the same tools.\n- Are closed-source baselines (o3/GPT-5/Gemini-2.5-Pro) allowed to use tools?\n- It is necessary to provide a comparable setting where all LLMs and agents can call the same external tools and be trained with the same data.\n\n4. Several important analyses are missing.\n- How to deal with the cases where the base model is already correct without tools? Is there any analysis of over-tooling/calling?\n- If the tools themselves produce incorrect results (e.g., bad bounding boxes, wrong BiomedParse class, SAM2 leakage), can the model correct these errors? And how?\n- The RL training lacks a stability analysis, which is crucial for RL methods. It would be useful to provide training loss curves, reward decomposition over time, and robustness to RL hyperparameters.\n- The experiments are limited to 2D medical VQA; it would be beneficial to extend them to EHR-based tasks."}, "questions": {"value": "Please see Weaknesses for details. Thank you."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bLkPoWg0Ha", "forum": "gwysQdBY1q", "replyto": "gwysQdBY1q", "signatures": ["ICLR.cc/2026/Conference/Submission1696/Reviewer_rC72"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1696/Reviewer_rC72"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762249771180, "cdate": 1762249771180, "tmdate": 1762915858497, "mdate": 1762915858497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}