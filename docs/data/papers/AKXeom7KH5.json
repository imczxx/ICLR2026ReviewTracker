{"id": "AKXeom7KH5", "number": 4141, "cdate": 1757611010643, "mdate": 1763374948739, "content": {"title": "Unrestrained Simplex Denoising for Discrete Data. A Non-Markovian Approach Applied to Graph Generation", "abstract": "Denoising models such as Diffusion or Flow Matching have recently advanced generative modeling for discrete structures, yet most approaches either operate directly in the discrete state space, causing abrupt state changes. We introduce simplex denoising, a simple yet effective generative framework that operates on the probability simplex. The key idea is a non-Markovian noising scheme in which, for a given clean data point, noisy representations at different times are conditionally independent. While preserving the theoretical guarantees of denoising-based generative models, our method removes unnecessary constraints, thereby improving performance and simplifying the formulation. Empirically, simplex denoising matches or surpasses strong discrete diffusion and flow-matching baselines across synthetic and real-world graph benchmarks. These results highlight the probability simplex as an effective framework for discrete generative modeling.", "tldr": "We introduce a new non-markovian diffusion model for discrete data operating on the probability simplex.", "keywords": ["Simplex", "Diffusion", "Dirichlem", "Denoising", "Flow Matching", "Generative", "Generation", "Graph", "Discrete"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51401c3c9861de0bea5283b8f0514478a78594e1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a diffusion model for graph generation based on a denoising process on the probability simplex. While previous simplex-based diffusion models for discrete data modeling are based on a Markovian process, the paper uses a non-Markovian noising that assumes independence across noisy states, which eliminates unnecessary constraints. Experimental results on graph generation tasks show improvements in small 2D molecule generation while being on par with SOTA baselines in unattributed graph generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is easy to follow and related works cover most of the relevant works.\n\n- Application of n-dimensional simplex diffusion for graph generation seems to be a novel approach, to the best of my knowledge.\n\n- The proposed method shows improvement in 2D molecule generation tasks."}, "weaknesses": {"value": "- Why is the dependency across noisy states in Markovian diffusion considered unnecessary? This dependency is simply a byproduct of the Markovian design, which is not inherently necessary or unnecessary. Designing a non-Markovian process could potentially lead to better performance, though the reason for this improvement is not clearly explained. However, this alone does not imply that dependency across noisy states is unnecessary.\n\n- Why is the proposed simplex-based diffusion model better than discrete diffusion models? The motivation in Figure 1 seems to apply to flow matching on simplex, and unclear how this connects with discrete diffusion models. Also, Dirichlet diffusion may show different results in the experiments in Figure 1.\n\n- The experiments are limited to small-scale graph benchmarks: 2D molecules have at most 38 nodes, and unattributed graphs have at most 187 nodes (SBM). Is the method scalable to graphs with a larger number of nodes? This may be validated on the synthetic grid graphs, where controlling the number of nodes is possible and has been used in the baseline like Grum.\n\n- The main claim of the paper is not validated sufficiently. The paper claims non-Markovian noising removes unnecessary constraints and improves performance, but results on unattributed synthetic graphs do not show this. Is the main claim wrong?"}, "questions": {"value": "Please address the questions in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x71o97rP8c", "forum": "AKXeom7KH5", "replyto": "AKXeom7KH5", "signatures": ["ICLR.cc/2026/Conference/Submission4141/Reviewer_XHMC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4141/Reviewer_XHMC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570505309, "cdate": 1761570505309, "tmdate": 1762917194956, "mdate": 1762917194956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "List of changes"}, "comment": {"value": "First, we thank all reviewers for their time and valuable feedback.\n\nWe have substantially revised the manuscript with two goals in mind: (i) to clarify the contributions and novelty of our work, and (ii) to better motivate our methodological choices. Importantly, we emphasize that the theoretical simplicity of our approach should not be mistaken for a lack of contribution. On the contrary, we view this simplicity as a core strength and a central contribution of the paper.\n\nBelow, we summarize the main changes made in the revised manuscript:\n\n- **Clarified contributions.** We reformulated and reordered the list of contributions to highlight the simplification our method provides relative to diffusion and flow matching, as well as the theoretical guarantees we establish.\n\n- **Expanded background and motivation.** We added a new subsection in the *Background* section to motivate our approach in comparison to discrete denoising methods and continuous methods that do not operate on the simplex. We also added a complementary appendix section analyzing denoiser expressivity in discrete vs. continuous settings.\n\n- **Clearer connection to SID.** We revised the beginning of the *Method* section to clarify the relationship between our approach and SID, and we added a detailed comparison in the appendix to delineate shared elements and our own contributions.\n\n- **New subsection on differences from diffusion/flow matching.** We added a subsection explaining how our method differs from diffusion and flow matching and why these differences are beneficial.\n\n- **Updated related work and baselines.** We incorporated the references suggested by the reviewer and added the corresponding baseline.\n\n- **Improved baseline interpretation.** We now report metrics comparing test graphs to samples from the training set, and we highlight all results within the corresponding error margin, as we consider them equally strong.\n\n- **Improved experimental results.** We reran several experiments using larger networks (matching the architectures used by other baselines), leading to improved metrics. We updated the technical details accordingly.\n\n- **Qm9H dataset clarification.** We removed DeFog from the Qm9H baselines after realizing it uses a dataset variant with aromatic bonds, whereas we use the kekulized version. Since results are not directly comparable, we added a dedicated appendix section specifying the version and evaluation protocol we use.\n\n- **Minor corrections.** We fixed typos and improved other stylistic and formatting details.\n\nWe sincerely hope that we have addressed all of your concerns. \nThank you again for your time, thoughtful comments, and interest in our work."}}, "id": "TeyJ7kvD7l", "forum": "AKXeom7KH5", "replyto": "AKXeom7KH5", "signatures": ["ICLR.cc/2026/Conference/Submission4141/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4141/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4141/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763375058005, "cdate": 1763375058005, "tmdate": 1763375058005, "mdate": 1763375058005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is clearly written. The formulations for both the reverse and forward processes are very clean. Figure 1 clearly illustrates the comparison with linear interpolation. The experimental results are complete. Overall, I find the paper sound and complete."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written. The formulations for the reverse process and the forward process are very clean. The formulas are easy to follow and convey the authors’ ideas. Figure 1 clearly illustrates the comparison with linear interpolation, as commonly done in discrete flow matching. I find the paper sound and complete."}, "weaknesses": {"value": "1. Figure 1 clearly illustrates the disadvantages of previous discrete flow matching formulations. However, even with the thorough discussion of related work in Section 2.2, it is still difficult for me to clearly understand the differences or relationships between the proposed method and (1) Dirichlet diffusion, and (2) Dirichlet flow matching in spirit. Could the authors explain explicitly on this point? This will make the contribution of the paper more clear. At the same time, compared to the other group of methods that maps the prediction back to the simplex, what is the intuition that Dirichlet diffusion may function better?\n2. There is related work that could be included in the comparison or discussed about, such as Graph BFN (Smooth Interpolation for Improved Discrete Graph Generative Models), which also performs denoising within the simplex space.\n3. Additional ablations could further support the method: for example, does the noise schedule matter, or do any hyperparameters require tuning for different datasets?"}, "questions": {"value": "1. In line 346, the authors mention: “but we can also approximate this prior using a model trained with the probability path…”\nCould the authors explicitly explain how this approximation is achieved without specifying the prior directly?\n2. Another point of interest in the current research direction is generating graphs with fewer sampling steps. I wonder whether the authors have explored reducing the number of steps."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "murIcoajcd", "forum": "AKXeom7KH5", "replyto": "AKXeom7KH5", "signatures": ["ICLR.cc/2026/Conference/Submission4141/Reviewer_jzMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4141/Reviewer_jzMT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841167291, "cdate": 1761841167291, "tmdate": 1762917194675, "mdate": 1762917194675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a diffusion-based method for generating graphs with categorical attributes. Noised versions of categorical variables (node and edge attributes) are represented as points on a simplex, and the denoising process operates in this space. The motivation is to design a diffusion process that avoids the discontinuities of discrete noising. The forward noising is non-Markovian, i.e., noised samples are independent and depend only on the clean data. The method is evaluated on molecular graphs and synthetic (SBM, planar graph) datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Generative modeling for discrete data is a relevant topic. The idea of applying non-Markovian diffusion to graph generation is conceptually interesting.\n- The authors provide a clear theoretical exposition, especially related to noising on the simplex. \n- In general, the paper is clearly written and easy to follow. The related work is presented well. \n- The experimental results are solid and show that relaxing the Markov assumption doesn't seem to harm empirical performance."}, "weaknesses": {"value": "The downside of denoising in the simplex space is that graph sparsity is lost: the communication graph inevitably becomes fully connected. This is not explicitly discussed in the paper, though there is a related work section on scalable methods.\n\nThe authors note that simplex-based diffusion for graph generation has already been done in (Liu et al. 2025). The use of Dirichlet distributions for categorical variables appears to be a relatively straightforward generalization of the earlier Beta-distribution based approach in Liu et al.\n\nLikewise, the challenges related to noising on the simplex are already studied in Stark et al. (2024). The proposed Voronoi-based probability construction is theoretically sound, but it does not by itself represent a major conceptual advance."}, "questions": {"value": "The use of non-Markovian noise is interesting, though the theoretical analysis in Section 4.2 would benefit from clearer references to related work on this topic to highlight which aspects are novel.\n\n**Typos and grammar:**\n- l.16: \"yet most approaches either operate\ndirectly in the discrete state space, causing abrupt state changes and discontinuities.\" --> remove either\n- l.234: the definition of L would be useful here\n- l.273 $Cat(\\pi)$ undefined\n- l.303: duplicate \"univariate case\" \n- l.306: \"Let assume\" --> Assume\n- l.365: \"... framework supports both. Because ...\" --> replace period with comma\n- l.957: The denoisers are Graph Neural Network,  --> Networks\n- l.981: $W^lsrc, W^ltrg, and W^ledge$   --> $W^l_{src}$; other similar errors occur in this section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pu1J0oj0n4", "forum": "AKXeom7KH5", "replyto": "AKXeom7KH5", "signatures": ["ICLR.cc/2026/Conference/Submission4141/Reviewer_nfL1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4141/Reviewer_nfL1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941216018, "cdate": 1761941216018, "tmdate": 1762917194339, "mdate": 1762917194339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}