{"id": "BLOIB8CwBI", "number": 7989, "cdate": 1758049523880, "mdate": 1759897817125, "content": {"title": "Verification of the Implicit World Model in a Generative Model via Adversarial Sequences", "abstract": "Generative sequence models are typically trained on sample sequences from natural or formal languages. It is a crucial question whether&mdash;or to what extent&mdash;sample-based training is able to capture the true structure of these languages, often referred to as the \"world model\". Theoretical results indicate that we can hope for soundness at best, that is, generating valid sequences, but not necessarily all of them. However, it is still important to have practical tools that are able to verify whether a given sequence model is sound. In this study, we focus on chess, as it is a domain that provides enough complexity while having a simple rule-based world model. We propose adversarial sequence generation for verifying the soundness of the sequence model. Our adversaries generate valid sequences so as to force the sequence model to generate an invalid next move prediction. Apart from the falsification of soundness, this method is also suitable for a more fine-grained analysis of the failure modes and the effects of different choices during training. To demonstrate this, we propose a number of methods for adversarial sequence generation and evaluate the approach on a large set of chess models. We train models on random as well as high-quality chess games, using several training recipes. We find that none of the models are sound, but some training techniques and dataset choices are able to improve soundness remarkably. We also investigate the potential application of board state probes in both our training and attack methods. Our findings indicate that the extracted board states have no causal role in next token prediction in most of the models.", "tldr": "", "keywords": ["generative sequence model", "implicit world model", "adversarial sequences", "chess"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b20f68c966b350609761c4bbeef5cb07fcf81c7.pdf", "supplementary_material": "/attachment/966a2363eb0bdc3acc4bebee604b9b293679d8fc.zip"}, "replies": [{"content": {"summary": {"value": "A new method based on adversary attack is proposed as a way of verifying a generative sequential model's world understanding. Unlike previous methods, this method uses an adversary that plays only legal moves but aims to trigger an illegal next move from the model. The authors test several adversary strategies on gpt2 style chess models trained on different datasets, and find that all models can be broken, implying none are fully sound. Even larger datasets and improved training objectives only delay, not prevent, failure. After all, the result suggests that generative models like GPT still lack understanding of the underlying world rules, despite appearing so."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Paper is well written. Even as someone not fully familiar with the research topic, I could follow it quite well.\n- Their proposed verification framework, which focuses on model behaviors, complements previous approaches based on probing, which are more retrospective and analytical.\n- The method is effective while remaining easy to implement.\n- Their experiments include multiple adversary strategies and a broad range of datasets, giving stronger support to their conclusions.\n- The paper also presents insightful diagnostics (eg, probe gradient alignment, OOD warmup tests) that help explain why surface-level rule-following can be misleading."}, "weaknesses": {"value": "- Experiments are based on a rather outdated model (gpt2 with 86M parameters). This leaves open the question of whether stronger world understanding might emerge as models get larger. While the authors mention that their choice was constrained by compute in the last section, I find this justification unconvincing. Given access to h100 GPUs, training newer but still relatively small models (e.g., 2B or 7B) should not pose excessive computational burdens.\n- The authors use greedy decoding without much justification. Although greedy decoding is appealing as it maximizes the likelihood of producing a legal move, real chess players (and possibly more human-like models) do not necessarily reason this way. It would be interesting to see whether different sampling strategies lead to different types or frequencies of illegal moves.\n- This is not necessarily a weakness, but more of a suggestion: an analysis of common failure modes would have been valuable. For example, does the chance of failure depend on the sequence length, the type of chess pieces involved, or the board state complexity? Identifying such patterns could deepen understanding of how and why the models fail."}, "questions": {"value": "See the above weaknesses.\n\nBesides, it might be interesting to explore whether causal inference/discovery ideas could be applied to this setup (although not necessarily straightforward or even relevant). For instance, one could imagine treating the model’s internal states as potential mediators between the world state and its next prediction, or performing small interventions on hidden features to test causal dependence. If such analyses are possible, they could offer a complementary way to examine whether rule-following behavior arises from genuine causal reasoning or just statistical association."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Sjjl4Gt6Hy", "forum": "BLOIB8CwBI", "replyto": "BLOIB8CwBI", "signatures": ["ICLR.cc/2026/Conference/Submission7989/Reviewer_ggS8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7989/Reviewer_ggS8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875542313, "cdate": 1761875542313, "tmdate": 1762919996122, "mdate": 1762919996122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This papers introduces an adversarial framework for verifying the soundness of implicit world models in generative sequence models, building on the work of, for example, Li et al. (2023) and Vafa et al. (2024) and using chess as a testbed. Soundness is defined as whether a model generates only valid sequences according to the true world model's rules --- that is, given any sequence, the model's continuation (generated by greedy decoding) yields a valid sequence under the true world model. The authors key contribution is to construct adversaries that generate valid moves (following a sequence generated by the model) to force the model to predict illegal continuations. \nThe authors propose several different adversaries to evaluate the soundness of the model. The paper conducts a thorough empirical study training 24 generative sequence models --- exploring the effects of alternative dataset (random games vs. curated datasets) and different training objectives. The authors main findings are: \n(1) None of the models are sound—all can be forced to make illegal moves, though the Illegal Move Oracle (IMO) adversary is far more effective than alternatives. IMO picks the legal move that maximizes the conditional probability of an invalid continuation by the opponent.\n(2) Dataset size appears to affect soundness --- larger datasets tend to have more sound models across training objectives. \n(3) Alternative objectives appears to matter --- the authors argue that the PD objective improves soundness. \n(4) The authors argue that board state probes (i.e., the ability to predict board state using the final representation) has limited causal influence on next token predictions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper has many nice strengths: \n\n1) The idea of generating adversarial continuations is clever, and a natural way to assess the implicit world model of generative sequence models. The approach provides an ``existence'' proof of unsoundness. \n2) The experimental scope of the paper is impressive --- by my read, 24 models trained across 6 datasets of varying sizes and qualities, evaluated with 5 different adversaries. The systematic experiments enable the authors to make interesting statements about how design choices in training might affect soundness (i.e., dataset size, quality and training objectives). \n3) The results make interesting progress in reconciling the results in Vafa et al. (2024) (taking a more functional approach to world model evaluation) and well-known works using probes. The results showing that board state probes have limited influence on model predictions is quite nice. \n4) The paper is very well-written and easy to follow."}, "weaknesses": {"value": "I use this box to describe weaknesses and make comments that I'd like to see the authors address. \n\n1) The authors criticize Vafa et al. (2024) for using an ad hoc probability threshold to define the generated language by the model, and claiming that their focus on adversaries avoids this. But there are analogous choices that must be made here. The authors focus exclusively on greedy decoding --- why not other choices? Would they perform differently? \n\n2) The definition of world models as valid continuations is presented with little discussion. It would be useful to point out that this is more general than the DFA framework used in Vafa et al. (2024) --- for example, it applies to context-free grammars, pushdown automata. So the adversary framework is potentially more directly applicable than the Myhill-Nerode based metrics in Vafa et al. \n\n3) It was surprising to me how effective RM and SMM were at generating failures. This should merit more investigation, but the authors somewhat gloss over this. For example, SMM achieves 0.943 against Random-2M-PD, meaning the model frequently fails even when generating its own preferred continuations (with only white moves corrected). These results seem to indicate the models are fundamentally poor at chess, not just vulnerable to adversarial attacks. This undermines the interpretation of IMO's success—are we testing world model soundness or just confirming the models don't play chess well?\n\n4) I found the comparison between PD and NT to be insufficiently visible in the main results. Table 1 looks like it shows mixed results to me, and the clearer evidence is buried in the appendix. If this is main result, it would be nice to bring it into the main text. \n\n5) It would be worthwhile to discuss the computational costs of the adversaries. The IMO adversary requires enumerating all illegal continuations and evaluating their probabilities. This is potentially expensive for large action spaces. \n\n6) While the paper establishes that models are unsound, it provides little insight into how and why they fail. What types of illegal moves do models make? Could the illegal moves generated by categorized? E.g., Do models violate basic piece movement rules (e.g., moving pawns backward, bishops diagonally incorrectly) in particular board positions? A taxonomy of errors would reveal whether models learn some aspects of chess better than others. Analogously,  does IMO consistently exploit particular weaknesses (e.g., rare board configurations, endgame positions, complex tactical situations)? Does BSO create specific types of confusion? Understanding which aspects of the world model are fragile would be more informative than aggregate success rates. These sorts of results would allow the authors to connect the paper to recent work emphasizing that generative models learn heuristics/shortcuts. \n\n7) The paper uses only linear probes on the final layer with absolute encoding. Could it be that chess requires different and slightly more complex probes that Othello (in particular, chess has more piece types and special rules so its a more complex game). It would be nice to see that the authors results on probes are not sensitive to the specific linear probes considered."}, "questions": {"value": "Please see my earlier discussion of weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cbNNgRe5k0", "forum": "BLOIB8CwBI", "replyto": "BLOIB8CwBI", "signatures": ["ICLR.cc/2026/Conference/Submission7989/Reviewer_9qiV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7989/Reviewer_9qiV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883411492, "cdate": 1761883411492, "tmdate": 1762919995632, "mdate": 1762919995632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper belongs to a line of work evaluating the fidelity of a language model in uncovering the hard rules of its underlying environment. The specific strategy in this paper is to adversarially design sequences where the model will incorrectly predict the next token (for instance, the max probability next token is not a valid move in the underlying game). This differentiates itself from other methods which directly measure next-token prediction, are based on linear probes or other white-box methods, or recent work which measures similarity between the model and the DFA-structure of the underlying world model.\n\nThis paper focuses on the game of chess, where the adversary is literally playing with the model and attempting to force the model to make an illegal move.\nThe attacker is greedy and chooses its acception at each timestep according to one of the following rules:\n - IMO: Maximize the conditional probability of the model on an incorrect token.\n - BSO: Maximize the error the board state as given by a linear probe.\n - AD: Choose the token with minimum probability under the model.\n - RM: Random token.\n - SMM: Choose the token with maximum probability under the model. This is standard argmax decoding.\n\n Three training strategies are employed:\n  - NT: next-token prediction, self-supervised over the given dataset\n  - PD: also self-supervised, but the target distribution is uniform over all valid moves\n  - JP: the model is jointly trained to produce a good board state via a linear probe\n\nAcross datasets, training strategies, and attack strategies, the attack success rate can vary significantly. For instance, on a dataset of 500k random moves, RM, IMO, and AD all achieve >90% attack success across all training strategies. On the other hand, on a dataset of 10M real games, the best attack (IMO) only achieves <40% success. Regardless, the attacks, especially IMO, are commonly capable of finding invalid sequences. Interestingly, next-token prediction (NT) is often the best strategy, with PD and JP not adding much benefit or making attacks easier.\n\nThe IMO attack is the most robust: all other attacks reduce to small success rates in some settings. Both in terms of attack and training procedure, introducing linear probes does not lead to better performance.\n\nInterestingly, the attacks performed worse on curated datasets than purely random datasets. Prior works showed random datasets have better world model recovery according to their metrics. The authors demonstrate that injecting an out-of-distribution prefix before running the attack significantly increases the attack success rate, demonstrating a limitation of the proposed attacks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper provides a clean, new method of evaluating the ability of the a model to recover the underlying rules of its environment. The attack-based evaluation can be performed on any black-box model and is easy to understand. It avoids having to deal with many subtleties of model inference or choosing a background distribution that appear in other works.\n- The experimental setup evaluates several natural and interesting choices of attack, training objective, and training data.\n- The paper is clearly written. The takeaways are convincing and laid out well."}, "weaknesses": {"value": "- It is not clear how the success rate is calculated, see the first question.\n\n- While easy to implement and generally applicable, notions like success rate of attacks are not very interpretable: they are a loose proxy. Does a model which has attack success rate 40% rather than 50% understand the underlying world model better? It is probably not that informative at that scale as the success rate depends heavily on the specific attack that is chosen.\n\n- The setup and experiments are all ran with deterministic argmax decoding. While this is a reasonable choice for finding invalid sequences, it would be interesting to investigate more realistic settings where tokens are sampled from the model.\n\n- It would be interesting to consider attacks which are not completely greedy. For instance, they could unravel $k$ steps of the game tree for small $k$. The takeaway from Section 7 suggests that an attack like AD should succeed but perhaps it does not work well in the purely greedy setting."}, "questions": {"value": "- What is the success rate averaged over? As both the model decoding and the adversary are deterministic, how is a percentage success rate achieved?\n\n - Why isn't IMO defined as the sum over all illegal tokens of their conditional probabilities? Perhaps there is a board state where a single illegal move is unlikely but in aggregate, the model places high probability on illegal moves. I would guess this is related to argmax decoding.\n\n - On random data, isn't it the case that NT and PD are essentially equivalent (at least in expectation)? This is not fully realized in the numbers, and I do not understand why."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rr7EO2RSiU", "forum": "BLOIB8CwBI", "replyto": "BLOIB8CwBI", "signatures": ["ICLR.cc/2026/Conference/Submission7989/Reviewer_oiiD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7989/Reviewer_oiiD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978866078, "cdate": 1761978866078, "tmdate": 1762919995236, "mdate": 1762919995236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether sequence models trained on chess possess a sound implicit world model by searching for legal prefixes that cause the model to predict an illegal move.\nSeveral adversaries are introduced: Random Move (RM), Sequential Mimic (SMM), Approximate Decoder (AD), Board State Oracle (BSO), and Illegal Move Oracle (IMO).\nAcross 24 GPT-2-style models trained with varied datasets and objectives, all models fail the soundness test, although larger datasets and probability-distribution training improve robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles an interesting and clearly defined problem, formulating world-model verification as falsification through adversarial legal sequences.\n2. The chosen adversaries (RM, SMM, AD, BSO, IMO) form a meaningful distribution of strength and show how more powerful attacks reveal hidden inconsistencies in the model’s behaviour.\n3. The paper is generally clearly written and has a fairly detailed analysis of performance patterns."}, "weaknesses": {"value": "1. It is difficult to interpret the results without understanding how competent these GPT-2-based models are as chess players. A comparison to specialised chess language models or an Elo-style strength estimate would help determine whether the observed unsoundness is surprising or simply reflects limited gameplay skill.\n2. The strong dependence on sequence length suggests that the models struggle with long-range reasoning. This is an interesting finding, but it mostly reflects the limitations of the training setup rather than providing deep insight into world-model soundness.\n3. Games longer than 150 moves are removed rather than truncated. Truncation could have preserved more board diversity while controlling length. A short justification for this choice would improve clarity.\n4. The Illegal Move Oracle (IMO) is predictably the strongest adversary because it has access to a legality oracle. The statement in line 342 that this “showcases the need for strong adversaries to reliably verify generative models” feels too general. In more complex domains where such oracles cannot be built, this approach may not scale.\n5. In figure 1, not all plots are complete. \n6. Line 139 states that only some algorithms use a board-state decoder, whereas line 304 says every model has a board-state probe. This inconsistency is minor but worth clarifying so readers understand that the probe is used for post-hoc analysis rather than during training."}, "questions": {"value": "1. How competent are the models at playing chess overall? For example, have you measured legality rates or gameplay strength relative to an engine or other chess language models?\n2. Do you terminate evaluation at the first illegal move or continue generation afterwards?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BX8CKe7Pzr", "forum": "BLOIB8CwBI", "replyto": "BLOIB8CwBI", "signatures": ["ICLR.cc/2026/Conference/Submission7989/Reviewer_c2oN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7989/Reviewer_c2oN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987968278, "cdate": 1761987968278, "tmdate": 1762919994884, "mdate": 1762919994884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}