{"id": "SFUMPoEQoC", "number": 13445, "cdate": 1758218017535, "mdate": 1759897437236, "content": {"title": "Leveraging Instruction Language Model to Generate Vectorized RISC-V Tensor Programs", "abstract": "Auto-vectorization is a powerful optimization that significantly improves the performance of tensor programs on modern instruction set architectures.Transforming tensor programs into high performance vectorized programs is in high demand. However, traditional compilers often overlook opportunities to vectorization.Meanwhile, hand-crafting vectorized optimization using specialized instructions remains a complicated and error-prone endeavor that requires in-depth knowledge of specific instruction set architectures and compilers. In this paper, we introduce \\textbf{RISCompiler}, a compiler designed to generate vectorized tensor programs with auto-vectorization tailored for the RISC-V target with vector extension. The main concept involves transforming the tensor program exploration task into generation task exploiting an instruction language model (ILM). To facilitate this, we create an instruction sentence representation suitable for ILM, which includes transformation details to accurately represent vectorized RISC-V tensor programs. RISCompiler uses an innovative, parameter-efficient fine-tuning mechanism to enhance domain adaptation by strategically concentrating on vectorized components, thereby boosting both fine-tuning and inference efficiency. During the compilation process, the ILM incorporates insights from offline learning and prior transformations to make optimal optimizations within the current design space. Experimental results demonstrate that RISCompiler, which are capable of generating high-performance vectorized tensor programs automatically, surpasses existing state-of-the-art compilers and scalar versions by a substantial margin.", "tldr": "", "keywords": ["Compiler; LLM; Code Generation; RISC-V; Efficient Fine-Tuning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cecf6dd6ac6e0e1ecfa2e11dada9a2431932916.pdf", "supplementary_material": "/attachment/ca3394d1671c13b452c30e69ea28291a0ec923dd.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes  a way to turn tensor programs into vectorized programs using language models.  Hand-crafting vectorized programs is hard as it requires in-depth knowledge of specific instructions and is prone to error. Their contribution involves creating an instruction sentence representation suitable for language models which includes transformation details to accurately represent vectorized RISC-V tensor programs. They train an instruction language model (ILM) to generate transformations and use a code generator to translate these transformations into actual vectorized RISC-V assembly"}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes an elegant approach to generate vectorized code, avoiding the difficulty of producing thousands of tokens error-free in a single pass.\n- The method achieves strong performance across multiple setups, outperforming widely used and well-known baselines."}, "weaknesses": {"value": "- The paper is difficult to follow, as some details that would aid understanding are omitted (See questions).\n- The novelty of their fine-tuning algorithm is unclear. It appears to share many similarities with *Mixture of LoRA Experts (MoLE; ICLR 2024)*, which is not cited, making it hard to assess its effectiveness relative to *MoLE*."}, "questions": {"value": "- There are many places where \\citep should be used; the current citations are inconsistent and impair readability.\n- Why was *LLaMA 3.2 3B* chosen instead of a code-specific LLM of comparable or smaller size (e.g., *StarCoder-2 3B*), which may be more suitable for the task? How might the choice of model (size, training data, etc.) impact results?\n- The *ILM* is fine-tuned to produce instruction sentences, right? Could you clarify what the input consists of, $r^{(i)}$, its specifications, or both?\"\n- Line 348: Could you clarify this statement? Instead of generating something like *I=1024|width|i.0=32|register_count|i.1=2|element_count|i.2=4|J=1024|width|j.0=32| ...</ s>* in one shot, is it generated step by step for *i*, *j*, and *k*?\n- Line 371: Could you clarify what you mean by *The baseline methodology incorporates the Claude-3-Haiku with Poe without any fine-tuning technique as its central component to generate programs with vectorized programs.*? Are you simply prompting the model in a zero-shot fashion to obtain vectorized programs?\n- Isn't *Claude* or any other API-available LLM compatible with your framework? Do you think it would be possible, using prompting, to leverage an instruction-tuned LLM as both *ILM* and *code generator* to achieve better performance?\n- Line 390: Could you provide a citation for GPT-2?\n- Figure 6: Do you have any intuition why **DEP** seems more difficult to speed up?\n- How critical is LoRA to your framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SPGw2VotM8", "forum": "SFUMPoEQoC", "replyto": "SFUMPoEQoC", "signatures": ["ICLR.cc/2026/Conference/Submission13445/Reviewer_aCLZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13445/Reviewer_aCLZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581489015, "cdate": 1761581489015, "tmdate": 1762924069061, "mdate": 1762924069061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RISCompiler, which turns RVV auto vectorization into an LM generation task using compact instruction sentences and an asymmetric fine tuning scheme. It reports speedups over GCC/LLVM autovectorizer and a general LLM baseline on single operators and DNNs, using QEMU for validation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tClear pipeline and interesting contribution. \n•\tBroad benchmarks across ops and DNNs."}, "weaknesses": {"value": "1. All performance results are validated and timed under QEMU with RVV. While this comparison provides early evidence that the method is good, QEMU is not accurate and does not model many microarchitectural effects. Thus, the reported speedups may not translate to real hardware. Adding evaluations on real hardware would substantively strengthen claims.\n2. Comparisons to GCC/LLVM autovectorizer are necessary but not sufficient. The closest baselines include LM-driven tensor program generators. The most important being TLM/OSDI’24 (“Enabling tensor language model to assist in generating high-performance tensor programs for deep learning”). Why aren't you comparing to TLM?\n3. How does the proposed method guarantee correctness? The text says: “Should the transformation be deemed invalid, the ILM discards the programs and commences the regeneration”. How is that performed? This is ambiguous in the paper."}, "questions": {"value": "•\tHow do you guarantee the correctness of vectorized outputs?\n•\tWhy didn’t you evaluate on real hardware? \n•\tWhy not compare with TLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VMvAiIDDvV", "forum": "SFUMPoEQoC", "replyto": "SFUMPoEQoC", "signatures": ["ICLR.cc/2026/Conference/Submission13445/Reviewer_vVEt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13445/Reviewer_vVEt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911078816, "cdate": 1761911078816, "tmdate": 1762924068447, "mdate": 1762924068447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RISCompiler, a compiler that uses a language model (an “Instruction Language Model” or ILM) to automatically generate vectorized RISC-V tensor programs. The end goal is to make RISC-V programs faster by automatic vectorization using the RISC-V vector extension (RVV). Since manual vectorization is complex, traditional compilers oftenm miss potential optimizations. ILM represents RISC-V programs as instruction setences (short, LM-friendly text descritpions of code transforms). The authors train the model with a large offline dataset of vectorized program samples and let it generate optimized vectorized code.\nAfter LORA-finetuning Llama 3.2-3B, RISCompier obtains significant;y faster programs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novelty: First attempt (to my knowledge) to treat auto-vectorization as a text generation problem using a language model. However, note later on the weaknesses: there has been important work on LLMs for compilers and compiler optimization that this work is missing.\n- Practical relevance: Targets the RISC-V vector ISA, which is increasingly important for open hardware.\n- Interesting, novel approach based on a 3 stage pipeline. Smart decomposition rather than end-to-end.\n- Efficient deep learning: small model and uses LoRA.\n- The speed numbers are non-trivial."}, "weaknesses": {"value": "- It's not clear to me that the baselines are the real baselines you'd have in a real-world scenario. The results show that this method can indeed auto-vectorize scalar tensor programs, and this naturally leads to speedups, but there's a lack of evaluation on more alternatives to vectorize.\n- Shallow description of the dataset.\n- This paper misses related work on language models for compilation and compilation optimizations, some of these works are critically related for understanding the novelty: https://arxiv.org/abs/2407.02524, https://arxiv.org/abs/2309.07062, https://arxiv.org/abs/2108.07639, https://arxiv.org/abs/2309.14396, https://openreview.net/forum?id=LWfDcI6txJ, https://arxiv.org/html/2412.12163v1, https://arxiv.org/abs/2410.08806. Cummins et al trained an LLM to directly perform compiler optimizations.\n- I'm not a big fan of introducing new terminology such as Instruction Language Model. Couldn't you use terms already used in related work?\n- The evaluation is limited, only a few benchmarks.\n- The method section is overly dense and not written very clearly.\n\nMisc:\n- The title misses either an article or ILM should be plural.\n- “which are capable” -> “which is capable.” \n- Inconsistent capitalization: “Instruction language model” vs “instruction language model.”\n- Missing articles: e.g., “design our generation-based RISC-V auto vectorization compiler utilizing instruction language model” -> \"design a generation-based RISC-V auto-vectorization compiler utilizing an instruction language model.”"}, "questions": {"value": "Will you open-source the code, checkpoints, and/or dataset?\n\nHow much work do you estimate this would take to adapt to a different ISA?\n\nDid you consider RL rather than supervised learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "95aA6rhRSJ", "forum": "SFUMPoEQoC", "replyto": "SFUMPoEQoC", "signatures": ["ICLR.cc/2026/Conference/Submission13445/Reviewer_ZoQu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13445/Reviewer_ZoQu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915112158, "cdate": 1761915112158, "tmdate": 1762924068214, "mdate": 1762924068214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a solution to find the optimal vectorization configuration for each loop in a program to obtain optimal performance on a RISC-V processor.\nThe solution involves representing the program and/or vectorization and compiler configuration in a certain textual format, followed by training or finetuning a model.\nThe results show a speedup of 1.6x compared to existing RISCV compiler."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Results show 1.6x speedup compared to existing C++ compilers.\nPerformance evaluated on end-to-end neural network models.\nRather than just feeding assembly or C++ programs directly into an LLM, authors try to find efficient textual representation."}, "weaknesses": {"value": "The paper is not clear, and has many gaps.\nIt is not clear to me whether the approach trained a model from scratch or finetuned.\nMoEs are out of the blue mentioned in the \"Code Generation Stage\" but not mentioned in the subsequent training stages. It is not clear whether the original model had an MoE structure. Then in the subsequent sections that were supposed to explain the solution in detail, nothing was mentioned about a MoE architecture or how each expert was trained.\nThe evaluation section did not compare with ML-based approaches for compiler optimization. I understand that papers on ML for compilers tend to not follow a standard dataset, so it is difficult to find a paper that evaluates on the same task, hardware backend, program types, etc. to compare against. However, the authors could have trained their dataset using an architecture or approach proposed on another ML for compilers paper, or could have evaluated brute force auto-tuning approaches, or auto-tuning that exists in a compiler like TVM."}, "questions": {"value": "- Does the solution train a model from scratch or start from a pretrained model? What is the architecture of the model? What were the training hyperparameters? \n- Figure 3 is not clear to me. Figure 5 that shows how data is fed into the LLM, does not show the usage of data presented in Figure 3.\n- Are the representations extracted from an LLVM IR representation of the code samples?\n- I suggest comparing with Meta's LLM Compiler ( https://arxiv.org/abs/2407.02524 ). Although it was trained for a different task (minimize IR instruction count), the model is open source and could be evaluated to see what speedup (or slowdown) it leads to on the evaluate data\n- I also suggest to compare using Tiramisu ( https://tiramisu-compiler.org/ ). I believe it can evaluate similar program samples that this paper evaluated on\n- The motivation behind the representation of data was not clear to me, and whether there is any novelty or significance to warrant publication in a top-tier ML conference is also not clear. For example, the representation of programs suggested in  https://arxiv.org/abs/2104.04955 has been backed by intuition and explanation, while this paper does not back its proposed representation or learning algorithm with a concrete explanation.\n\nNotes:\n- Figure 7: Lower than 1 performance sounds like a slowdown. Maybe the y-axis should be labeled \"Normalized Latency\"\n- Line 431: \"mincycle\": need to fix formatting"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2BydjpR8KL", "forum": "SFUMPoEQoC", "replyto": "SFUMPoEQoC", "signatures": ["ICLR.cc/2026/Conference/Submission13445/Reviewer_cjDV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13445/Reviewer_cjDV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012184009, "cdate": 1762012184009, "tmdate": 1762924067911, "mdate": 1762924067911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}