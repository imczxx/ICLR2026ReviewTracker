{"id": "8pi1rP71qv", "number": 91, "cdate": 1756728693496, "mdate": 1763634318337, "content": {"title": "FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning", "abstract": "General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively.", "tldr": "We propose a brain-inspired method FlyPrompt that uses random-expanded routing and temporal-ensemble experts to effectively tackle General Continual Learning problem, achieving significant gains on major benchmarks.", "keywords": ["Continual Learning", "Life-long Learning", "Brain-inspired AI", "Catastrophic Forgetting", "Prompt Tuning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d4c9a8bda81a68af70a95acd3513f0f21be473c.pdf", "supplementary_material": "/attachment/a502549ab359383dbaa373fb0cb2e6c40e6ff16f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces FlyPrompt, an expert-based framework for General Continual Learning (GCL), where each task is associated with a prompt expert. FlyPrompt proposes two key contributions: a novel strategy named REAR (Random Expanded Analytic Router), which leverages random projection to identify suitable experts at inference, and Task-wise Experts with Temporal Ensemble, designed to track distributional drift. The paper is supported by detailed experiments across standard benchmarks and thoughtful ablation analyses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The decomposition into expert routing and expert competence is empirically well-supported.\n- The results are consistent across a broad range of pre-trained models (Sup-21K, iBOT, DINO, MoCo), rather than being limited to a single backbone.\n- The paper is further strengthened by extensive ablation studies and a comprehensive hyperparameter analysis.\n- The writing is clear, and the overall structure is well-organized."}, "weaknesses": {"value": "- The paper does not discuss the number of parameters (different from memory in Appendix F) of the proposed method compared to the baselines, which could be a significant concern.\n- In the GCL setting, task boundaries are unknown during both **training** and inference [1]. The methodology described in L216–217, which states that \"we associate each task $t$ with a corresponding expert $E_t$,\" appears to assume that task identities are known during training, thereby allowing the assignment of a new expert. This assumption fundamentally contradicts the definition of GCL. Consequently, the problem being addressed may be closer to Class-Incremental Learning with overlapping classes rather than true task-agnostic GCL.\n- If task boundaries are indeed assumed to be known during training, then several recent SOTA methods like HiDe-Prompt [2], NoRGa [3], and SD-LoRA [4] could be straightforwardly implemented. The paper lacks a comparison to these highly relevant methods, which weakens its claims of superiority.\n- The REAR component, used for identifying task identity, is a powerful module in its own right and is functionally similar to the full RanPAC method [5]. Comparing FlyPrompt (which includes REAR) to methods that employ much simpler routing strategies may therefore be unfair. A more rigorous comparison would involve incorporating REAR into other methods (e.g., evaluating “HiDe-Prompt + REAR”) to properly ablate its contribution. A similar argument applies to the Task-wise Experts with Temporal Ensemble; this technique could also be combined with other baselines for a fairer assessment.\n- In L184-186, the authors motivate their second component by claiming that \"even with perfect routing, previous methods still exhibit inferior performance... highlighting... the limited competence of individual experts.\". However, the paper does not sufficiently diagnose the source of this inferior performance. It remains unclear whether the issue lies in representation drift within the expert prompts $f_\\theta(\\cdot, p_t)$ or in catastrophic forgetting within the final classification head $g_\\psi$. An experiment measuring the representation drift of each expert (e.g., by analyzing the similarity between representations from correct and incorrect experts) would be necessary to clarify and validate this motivation.\n- The “Task Experts as Temporal Ensembles” component is presented as a mechanism to enhance expert competence. However, the core prompt expert is trained using a standard cross-entropy loss (Equation 6). Thus, the observed novelty and performance gains appear to stem from ensembling multiple classification heads rather than from improving the representational quality of the prompt expert itself.\n\n\n[1] Dark Experience for General Continual Learning: a Strong, Simple Baseline, NeurIPS 2020\n\n[2] Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality, NeurIPS 2023\n\n[3] Mixture of Experts Meets Prompt-Based Continual Learning, NeurIPS 2024\n\n[4] SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning, ICLR 2025\n\n[5] RanPAC: Random Projections and Pre-trained Models for Continual Learning, NeurIPS 2023"}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UWhmvODgS3", "forum": "8pi1rP71qv", "replyto": "8pi1rP71qv", "signatures": ["ICLR.cc/2026/Conference/Submission91/Reviewer_EcKY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission91/Reviewer_EcKY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission91/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657745281, "cdate": 1761657745281, "tmdate": 1762915448957, "mdate": 1762915448957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FlyPrompt, an expert-based framework for General Continual Learning (GCL), where each task is associated with a prompt expert. FlyPrompt proposes two key contributions: a novel strategy named REAR (Random Expanded Analytic Router), which leverages random projection to identify suitable experts at inference, and Task-wise Experts with Temporal Ensemble, designed to track distributional drift. The paper is supported by detailed experiments across standard benchmarks and thoughtful ablation analyses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The decomposition into expert routing and expert competence is empirically well-supported.\n- The results are consistent across a broad range of pre-trained models (Sup-21K, iBOT, DINO, MoCo), rather than being limited to a single backbone.\n- The paper is further strengthened by extensive ablation studies and a comprehensive hyperparameter analysis.\n- The writing is clear, and the overall structure is well-organized."}, "weaknesses": {"value": "- The paper does not discuss the number of parameters (different from memory in Appendix F) of the proposed method compared to the baselines, which could be a significant concern.\n- In the GCL setting, task boundaries are unknown during both **training** and inference [1]. The methodology described in L216–217, which states that \"we associate each task $t$ with a corresponding expert $E_t$,\" appears to assume that task identities are known during training, thereby allowing the assignment of a new expert. This assumption fundamentally contradicts the definition of GCL. Consequently, the problem being addressed may be closer to Class-Incremental Learning with overlapping classes rather than true task-agnostic GCL.\n- If task boundaries are indeed assumed to be known during training, then several recent SOTA methods like HiDe-Prompt [2], NoRGa [3], and SD-LoRA [4] could be straightforwardly implemented. The paper lacks a comparison to these highly relevant methods, which weakens its claims of superiority.\n- The REAR component, used for identifying task identity, is a powerful module in its own right and is functionally similar to the full RanPAC method [5]. Comparing FlyPrompt (which includes REAR) to methods that employ much simpler routing strategies may therefore be unfair. A more rigorous comparison would involve incorporating REAR into other methods (e.g., evaluating “HiDe-Prompt + REAR”) to properly ablate its contribution. A similar argument applies to the Task-wise Experts with Temporal Ensemble; this technique could also be combined with other baselines for a fairer assessment.\n- In L184-186, the authors motivate their second component by claiming that \"even with perfect routing, previous methods still exhibit inferior performance... highlighting... the limited competence of individual experts.\". However, the paper does not sufficiently diagnose the source of this inferior performance. It remains unclear whether the issue lies in representation drift within the expert prompts $f_\\theta(\\cdot, p_t)$ or in catastrophic forgetting within the final classification head $g_\\psi$. An experiment measuring the representation drift of each expert (e.g., by analyzing the similarity between representations from correct and incorrect experts) would be necessary to clarify and validate this motivation.\n- The “Task Experts as Temporal Ensembles” component is presented as a mechanism to enhance expert competence. However, the core prompt expert is trained using a standard cross-entropy loss (Equation 6). Thus, the observed novelty and performance gains appear to stem from ensembling multiple classification heads rather than from improving the representational quality of the prompt expert itself.\n\n\n[1] Dark Experience for General Continual Learning: a Strong, Simple Baseline, NeurIPS 2020\n\n[2] Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality, NeurIPS 2023\n\n[3] Mixture of Experts Meets Prompt-Based Continual Learning, NeurIPS 2024\n\n[4] SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning, ICLR 2025\n\n[5] RanPAC: Random Projections and Pre-trained Models for Continual Learning, NeurIPS 2023"}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UWhmvODgS3", "forum": "8pi1rP71qv", "replyto": "8pi1rP71qv", "signatures": ["ICLR.cc/2026/Conference/Submission91/Reviewer_EcKY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission91/Reviewer_EcKY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission91/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657745281, "cdate": 1761657745281, "tmdate": 1763633644734, "mdate": 1763633644734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets General Continual Learning (GCL), characterized by single-pass, non-stationary data streams without clear task boundaries. Identifying limitations in existing Parameter-Efficient Tuning (PET) methods, it decomposes GCL into expert routing and expert competence improvement. Inspired by the fruit fly's memory system, the paper proposes the FlyPrompt framework with two core components: (1) A Random Expanded Analytic Router (REAR) using fixed random projections and a closed-form solution for gradient-free, rapid input-to-expert (prompt) assignment. (2) Task-wise Experts with Temporal Ensemble (TE$^2$) employing multiple EMA heads with different decay rates within each expert to dynamically refine decision boundaries over time. FlyPrompt achieves strong performance across various GCL benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[S1] The proposed Random Expanded Analytic Router (REAR) uniquely employs a closed-form solution rather than iterative gradient updates to assign inputs to experts. This is advantageous for the strict online, single-pass constraints of GCL, offering a theoretically grounded and computationally efficient alternative to traditional routing methods.\n\n[S2] The framework effectively tackles the complexity of GCL by decomposing it into two manageable subproblems: routing and competence improvement. The Task-wise Experts with Temporal Ensemble (TE²) addresses the latter by leveraging multi-timescale EMA heads, which significantly enhances expert robustness against non-stationary data streams."}, "weaknesses": {"value": "[W1] Initializing the prompt for a new task by averaging previous prompts may not be beneficial and is likely to show degraded performance when subsequent tasks come from significantly different domains.\n\n[W2] While REAR outperforms gradient-based routers, comparisons against simpler non-learning baselines in the expanded feature space (e.g., k-NN routing) are absent, making it hard to gauge the benefit derived from the analytic ridge regression complexity.\n\n[W3] Experiments primarily use the default Si-Blurry configuration. Performance under more extreme imbalance, higher task overlap, or different types of distribution drift needs further investigation.\n\n[W4] While neuro-inspired, drawing direct equivalences between specific algorithmic components (e.g., EMA heads and KC subtypes) and biological counterparts might be an oversimplification."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qNn3aLmMEf", "forum": "8pi1rP71qv", "replyto": "8pi1rP71qv", "signatures": ["ICLR.cc/2026/Conference/Submission91/Reviewer_yU5J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission91/Reviewer_yU5J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission91/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700609051, "cdate": 1761700609051, "tmdate": 1762915448812, "mdate": 1762915448812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FlyPrompt, a framework for GCL inspired by the neural circuitry of the fruit fly mushroom body. It decomposes GCL into two subproblems: expert routing and expert competence improvement. For expert routing, the authors introduce the REAR, which uses fixed random projections and closed-form updates for feedforward expert selection. For competence improvement, they propose TE2, which integrates knowledge across multiple timescales via EMA heads with different decay rates. The method reports SOTA performance on benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The separation of GCL into routing and competence subproblems offers a structured approach to tackling its challenges.\n2. The use of principles from fruit fly olfactory memory introduces a novel interdisciplinary perspective to CL.\n3. The paper provides both informal and formal theoretical bounds on routing error and EMA parameter error, enhancing methodological credibility."}, "weaknesses": {"value": "1. It resemble an ad hoc combination of existing techniques. The proposed FlyPrompt framework appears to be largely a composition of well-established components rather than a fundamentally novel algorithm. Specifically, the REAR combines fixed random projection with ridge regression (a paradigm already explored in prior CL works and analytic class-incremental learning). Similarly, the TE2 employs EMA with multiple decay rates, a standard technique in online learning and model stabilization (e.g., SWA, temporal ensembling). \n2. While the paper provides a biologically inspired narrative grounded in the fruit fly’s mushroom body, the mapping between neurobiological mechanisms and algorithmic design remains largely metaphorical. The performance gains reported (e.g., +11.23% auc on CIFAR-100) are primarily attributable to the strong supervised pretraining (Sup-21K) and the inherent benefits of random feature expansion, rather than the proposed routing or ensemble mechanisms. The ablation study (Table 2) further reveals that a RanPAC-like baseline already achieves 82.17% auc. This suggests that FlyPrompt’s contribution is incremental engineering rather than a necessary or uniquely effective solution.\n3.  While GCL as a research direction is valid, the specific formulation and assumptions in this work appear tailored to a controlled benchmark rather than a pressing real-world problem. The number of tasks $T$ (and thus the number of prompt experts) is assumed known and fixed a priori, which contradicts truly open-world or task-agnostic streaming environments. The evaluation protocol assumes access to task-level metadata during training (e.g., expert identity per session), which may not hold in fully unsupervised or user-generated data streams."}, "questions": {"value": "How about the practical scalability and efficiency? Although Table 5 reports only marginal increases in total parameters and per-batch latency, it omits the auxiliary memory and compute burden of $G$ and $Q$"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QOcr6UHPLp", "forum": "8pi1rP71qv", "replyto": "8pi1rP71qv", "signatures": ["ICLR.cc/2026/Conference/Submission91/Reviewer_Yd6v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission91/Reviewer_Yd6v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission91/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899056873, "cdate": 1761899056873, "tmdate": 1762915448698, "mdate": 1762915448698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Response"}, "comment": {"value": "Dear reviewers,\n\nWe sincerely thank all reviewers for their great efforts and insightful comments. We are encouraged that many reviewers have acknowledged the strengths of our work, including: its principled decomposition of General Continual Learning (GCL) (Reviewers Vwq2, jMmN, Yd6v, yU5J); the novel efficiency of its backprop-free random expanded analytic router paired with a robust temporal-ensemble expert design (Reviewers Vwq2, yU5J); consistent state-of-the-art performance across datasets under the challenging GCL setting (Reviewers Vwq2, jMmN, EcKY); and the conceptual depth added by cross-disciplinary inspiration from biological learning systems (Reviewers Vwq2, Yd6v).\n\n> **Numbering convention.** Unless explicitly specified with \"in the revised manuscript\", direct references such as \"Table X\", \"Fig. Y\", or \"Appendix Z\" in this rebuttal refer to the version of the manuscript originally submitted for review. Because new results and analyses have been added in the revised manuscript, some table/figure indices may shift slightly, but the corresponding content remains the same or is extended. Line numbers refer to the latest revised version.\n\nBelow, we address two major common questions (CQs) raised across the reviews.\n\n### **CQ1: Contribution and Novelty**\n\nWe summarize the contribution and novelty of our work as follows.\n\n**Conceptually**, FlyPrompt is inspired by key mechanisms of the fruit fly's learning system (i.e., the mushroom body):\n(1) sparse, high-dimensional random expansion from PNs to KCs, which supports instance-level routing and interference reduction; and (2) compartmentalized plasticity across KC subtypes, enabling adaptation at multiple timescales.\n\nPrior work has explored each mechanism separately (e.g., random expansion for interference reduction [1,2] and multi-timescale ensembles for memory consolidation and generalization [3,4]). To our knowledge, FlyPrompt is the first attempt to **integrate both mechanisms** in a principled framework for the realistic GCL problem, especially with **parameter-efficient tuning (PET)** of **pretrained models**. We formalize GCL as a streaming multi-expert problem, decompose it into two subproblems (expert routing and expert competence improvement) and map these to algorithmic counterparts (REAR and TE²) grounded in the fly brain's structure and function.\n\n**Algorithmically**, our instantiations (REAR and TE²) depart meaningfully from prior CL techniques: (1) REAR uses random expansion and ridge regression **only for routing**, unlike methods such as ACIL or RanPAC that apply analytic solutions to final classification. This allows expert modules to remain plastic, decoupling a fixed, forward-only router from gradient-trained, adaptable experts. (2) TE² introduces a bank of temporally smoothed heads per expert, which are combined during inference. Unlike single-head EMA models, TE² ensures that **a near-optimal head** always exists under representation shifts, as supported by our theoretical analysis.\n\nEmpirically, our new experiments with multiple RanPAC variants show that even carefully adapted analytic classifiers lag behind FlyPrompt and suffer from ill‑conditioned Gram matrices once the backbone is updated online (see **Vwq2 Q9**), while ablations that plug REAR and TE² into CL baselines such as S‑Prompt++, HiDe‑Prompt and NoRGa consistently improve their GCL performance (see **EcKY W4**).\n\nIn summary, FlyPrompt (1) identifies and formalizes two core subproblems in GCL; (2) offers theoretically grounded, biologically inspired solutions; and (3) implements these on top of pretrained models under replay-free GCL settings, yielding **robust and state-of-the-art performance**.\n\n[1] A neural algorithm for a fundamental computing problem. *Science*, 2017.\n\n[2] Cellular-resolution population imaging reveals robust sparse coding in the Drosophila mushroom body. *Journal of Neuroscience*, 2011.\n\n[3] The neuronal architecture of the mushroom body provides a logic for associative learning. *eLife*, 2014.\n\n[4] Incorporating neuro-inspired adaptability for continual learning in artificial intelligence. *Nature Machine Intelligence*, 2023."}}, "id": "xZXrfAW27C", "forum": "8pi1rP71qv", "replyto": "8pi1rP71qv", "signatures": ["ICLR.cc/2026/Conference/Submission91/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission91/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission91/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763630539046, "cdate": 1763630539046, "tmdate": 1763630539046, "mdate": 1763630539046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing parameter-efficient tuning methods struggle in General Continual Learning (GCL) because they cannot effectively allocate expert parameters or improve representations in single-pass, boundary-free data streams. Inspired by the fruit fly's brain, this paper proposes FlyPrompt, a framework that decomposes GCL into expert routing and expert competence improvement. FlyPrompt uses a random analytic router to activate experts and a temporal ensemble of output heads to adapt, significantly outperforming state-of-the-art baselines on key benchmarks like CIFAR-100 and ImageNet-R."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Biologically Inspired Foundation**: The framework is grounded in the neurobiological principles of the fruit fly's brain, offering a novel approach to solving complex GCL challenges.\n\n**Addresses Core GCL Problems**: It effectively tackles two fundamental challenges in GCL: \"expert routing\" (selecting the right parameters) and \"expert competence improvement\" (adapting to new data) under difficult, realistic constraints (single-pass data, no task boundaries).\n\n**Novel and Efficient Components**: It introduces two key innovations:\n\n - A randomly expanded analytic router for non-iterative (fast and efficient) expert selection.\n\n - A temporal ensemble of expert heads to ensure the model robustly adapts to data changes over time.\n\n**Proven Performance**: The method is backed by both strong theoretical analysis and excellent empirical results, demonstrating superior performance and scalability across multiple GCL benchmarks."}, "weaknesses": {"value": "**Notational Clarity**: There appears to be a notational inconsistency in Equations 2 and 3, where the symbols $\\Phi$ and $\\varphi$ seem to be transposed or confused.\n\n**Comparative Analysis**: The paper would be significantly strengthened by a direct comparison of FlyPrompt against other prominent methods (such as LoRA, Adapters, and MoE). This comparison should explicitly analyze key metrics:\n\n- Parameter efficiency (total and new parameters)\n\n- Computational overhead (training and inference time)\n\n- Backward Transfer (BWT)\n\n**Novelty of Application**: Given that Random Projection is a well-established technique, what is the specific novelty of its application within the FlyPrompt framework? How does its integration into the \"randomly expanded analytic router\" differ from standard implementations and what unique advantages does this specific application provide for the GCL setting?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MMssaQAm1l", "forum": "8pi1rP71qv", "replyto": "8pi1rP71qv", "signatures": ["ICLR.cc/2026/Conference/Submission91/Reviewer_jMmN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission91/Reviewer_jMmN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission91/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975615306, "cdate": 1761975615306, "tmdate": 1762915448600, "mdate": 1762915448600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper introduces a neuro-inspired framework for General Continual Learning that learns online without task labels or replay. It breaks the problem into two parts: routing and expert competence. The Random-Expanded Analytic Router uses random feature projections and closed form ridge updates to select experts efficiently -- no  backpropagation. The Temporal-Ensemble Experts module maintains several EMA classifier heads with different decay rates and it is combining them to balance plasticity and stability. Theoretically REAR is shown to approximate batch ridge regression, and TEE achieves an almost optimal bias–variance trade-off. Experiments on CIFAR-100, ImageNet-R, and CUB-200 under the Si-Blurry protocol demonstrate consistent state-of-the-art results with minimal trainable parameters and runtime overhead."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper identifies two core challenges in GCL a) expert routing and b)expert stability. it addresses each with a distinct, principled component: REAR and TEE.\n2. REAR does not do backpropagation. Instead it maintains online sufficient statistics and is solving a closed-form ridge regression, which is much faster/simpler.\n3. the authors show consistent state-of-the-art results on CIFAR-100, ImageNet-R, and CUB-200 under the Si-Blurry protocol.\n4. I like the math analysis linking random-feature expansion to generalization (thm 1) and characterizing the bias–variance trade-off in temporal ensembling (thm 2)\n5. Less than a million trainable parameters \n6. I very much like the analogy to multi-timescale synaptic adaptation and biologically plausible interpretation of the design"}, "weaknesses": {"value": "1) Thm 1 relies on a pairwise concentration lemma but omits a full matrix-concentration argument and a margin assumption needed to link regression risk to routing accuracy. This may be fixable in the rebuttal period. \n\n2)  Thm-2:  the derivation connecting EMA bias to temporal drift is approximate. The claim of “near-optimal adaptation” is not formally proven. Needs to be clarified\n\n3) Despite the task-free framing that the paper emphasizes, in my opinion REAR initialization and label accumulation still assume known session starts and one-hot session indicators. True?\n\n4) Maintaining and inverting the (M \\times M) Gram matrix can be memory-intensive for large random expansions\n\n5) The method’s logit mask may leak boundary information but this is not analyzed against Si-Blurry baselines such as MVP."}, "questions": {"value": "1. plz clarify how REAR maintains the inverse of (G+ lambda I) online. Is inversion recomputed per batch or updated incrementally?\n\n2. Is it that each sample contributes once to G and  Q or multiple times across the three online iterations per batch? If repeated, the estimator corresponds to weighted ridge regression - not the exact form proven in Lemma 3.\n\n3. In Lemma 1 the jump from pairwise concentration to operator-norm bounds on \\hat \\Sigma - \\Sigma is not rigorous. I think this is fixable though.  \n\n4. The paper states a random-feature error rate O(\\log N/M) but this is  is inconsistent with the lemma’s \\tilde O(\\sqrt (\\log N/M)) bound, right? Plz correct or provide an argument for the stronger rate.\n\n5. Thm-1 gives a routing-accuracy guarantee  but it does not make any margin assumption between expert scores. Introduce this assumption and carry the margin constant into the final bound.\n\n6. Thm-2 makes the bias step explicit. But can you show formally that \\sum_j \\alpha^j \\Delta_{t-j+1} \\le L P_t --  and quantify the constant C_2?\n\n7. the claim that a geometric EMA bank achieves near-optimal performance should be supported with a short covering-ratio argument showing the error factor in terms of grid spacing r\n\n9. If there is time in the rebuttal phase, plz evaluate the effect of removing or randomizing the logit mask to confirm that improvements are not due to boundary information.\n\n10. Again, if there is time, it would be good to compare REAR with RanPAC under identical settings to clarify the unique contribution of routing versus analytic classification\n\n11. I would appreciate a list of Assumptions (data i.i.d., λ > 0, bounded feature norms, fixed number of experts) at the start of the theory section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YQ3QIidJGc", "forum": "8pi1rP71qv", "replyto": "8pi1rP71qv", "signatures": ["ICLR.cc/2026/Conference/Submission91/Reviewer_Vwq2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission91/Reviewer_Vwq2"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission91/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762072761104, "cdate": 1762072761104, "tmdate": 1762915448474, "mdate": 1762915448474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}