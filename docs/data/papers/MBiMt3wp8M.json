{"id": "MBiMt3wp8M", "number": 12321, "cdate": 1758207051850, "mdate": 1759897517113, "content": {"title": "Dragging with Geometry: From Pixels to Geometry-Guided Image Editing", "abstract": "Interactive point-based image editing serves as a controllable editor, enabling precise and flexible manipulation of image content. However, previous methods predominantly center on 2D pixel plane, neglecting the underlying 3D geometric structure. As a result, they often produce imprecise and inconsistent edits, particularly in geometry-intensive scenarios such as rotations and perspective transformations. To address these limitations, we propose a novel geometry-guided drag-based image editing method—GeoDrag, which addresses three key challenges: 1) incorporating 3D geometric cues into pixel-level editing, 2) mitigating discontinuities caused by geometry-only guidance, and 3) resolving conflicts arising from multi-point dragging. Built upon a unified displacement field that jointly encodes 3D geometry and 2D spatial priors, GeoDrag enables coherent, high-fidelity, and structure-consistent editing in a single forward pass. In addition, a conflict-free partitioning strategy is introduced to isolate editing regions, effectively preventing interference and ensuring consistency. Extensive experiments across various editing scenarios validate the effectiveness of our method, showing superior precision, structural consistency, and reliable multi-point editability. Our code and models will be released publicly. Project page: https://geodrag-site.github.io.", "tldr": "", "keywords": ["Diffusion Model; Drag-based Image Editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd23ab76fe7d35910b6d1b5b5804ef254b983fdc.pdf", "supplementary_material": "/attachment/22d353a68c01660ad65f7e7d4dfee26915559a38.zip"}, "replies": [{"content": {"summary": {"value": "GeoDrag is a geometry-aware drag-based image editing framework combining 3D structural cues and 2D spatial priors. It predicts displacement fields in diffusion latent space, incorporating depth-based modulation and spatial plane adjustment, along with a conflict-free partitioning scheme for multi-point editing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses a concrete weakness: Most drag methods ignore geometry, causing distortions. GeoDrag effectively solves this.\n\n2. Principled integration: The combination of 3D depth modulation and 2D plane adjustment is physically intuitive and well-designed.\n\n3. Practical contribution: One-step editing drastically improves usability and speed over iterative gradient-based methods.\n\n4. Strong empirical results: Quantitative improvements (1.4× DAI, 1.1× MD) and excellent qualitative visuals demonstrate real impact.\n\n5. Multi-point conflict resolution: - The conflict-free partitioning module is a simple yet powerful idea that improves robustness.\n6. Extensive evaluation: Diverse examples (faces, landscapes, multi-object scenes) show generality."}, "weaknesses": {"value": "1.\tDepth dependency: Accuracy hinges on reliable depth maps; the paper assumes ideal depth input without addressing noise robustness.\n2.\tLimited theoretical grounding: The interaction between geometry and diffusion latent space lacks mathematical explanation.\n3.\tAblation insufficiency:The individual contribution of geometry-aware vs. plane-aware modules is unclear.\n4..\tEfficiency discussion shallow: Though claimed efficient, actual GPU/latency data is minimal.\n5.\tApplicability scope: Focused on still images; no experiments on video or non-diffusion backbones.\n6..\tMinor clarity issues: Some figures are over-compressed, making displacement fields hard to interpret."}, "questions": {"value": "1.\tHow sensitive is the method to depth map errors?\n2.\tCan GeoDrag operate with estimated depth from monocular models?\n3.\tIs the diffusion model re-trained or used as a frozen backbone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lDUJBnn6lT", "forum": "MBiMt3wp8M", "replyto": "MBiMt3wp8M", "signatures": ["ICLR.cc/2026/Conference/Submission12321/Reviewer_m8ds"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12321/Reviewer_m8ds"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401361889, "cdate": 1761401361889, "tmdate": 1762923246963, "mdate": 1762923246963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GeoDrag, a novel method for point-based image editing that addresses the limitations of 2D-only approaches in geometry-intensive scenarios. By incorporating 3D geometric cues (from a predicted depth map) into a unified displacement field, GeoDrag achieves structurally consistent edits like rotations and perspective shifts. The framework combines this geometry-aware guidance with 2D spatial priors for local detail preservation and introduces a conflict-free partitioning strategy for robust multi-point dragging, all within an efficient single-pass process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Novelty and Rationale:** The core strength is the novel integration of 3D geometry into the 2D drag-editing task. This is a well-motivated and logical advancement for the field, directly addressing a key limitation of prior work and yielding significant improvements in structural preservation.\n\n2.  **Clear and Effective Framework:** The paper effectively decomposes the problem into distinct sub-challenges and proposes clear, corresponding solutions (geometry-aware field, spatial modulation, and conflict-free partitioning). This systematic approach enhances the method's robustness and clarity.\n\n3.  **Efficiency for Interactivity:** By avoiding iterative optimization, the one-step editing process makes GeoDrag a practical and competitive solution for real-world interactive applications."}, "weaknesses": {"value": "1.  **Texture and Identity Preservation:** In some cases, the edited regions exhibit a loss of texture fidelity. For instance, the \"mountain\" example on the project page shows the dragged peak becoming blurred, suggesting a potential trade-off between geometric deformation and preserving the original surface identity. A discussion on this limitation would be welcome.\n\n2.  **Foreground-Background Separation:** The method can sometimes cause unnatural co-movement of the background with the dragged object. This is visible in the \"duck\" example, where the surrounding grass is pulled along with the head. This suggests that handling disocclusion and complex object boundaries remains an area for improvement.\n\n3.  **Limitations of the 3D Motion Model:** The current geometric model (Eq. 7) effectively handles transformations parallel to the image plane but is a simplification of true 3D motion. This might limit its ability to perform more complex perspective changes, such as pulling an object \"out of\" the scene. Exploring a more comprehensive 3D motion model could be a valuable future direction."}, "questions": {"value": "1.  Concurrent work like `Inpaint4Drag` uses inpainting to handle occlusions, which can sometimes cause warping distortions. Have the authors considered combining GeoDrag's superior geometry-aware displacement with a dedicated inpainting model? This synergy could potentially yield more realistic results at object boundaries by better handling disoccluded regions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B6FATNct2U", "forum": "MBiMt3wp8M", "replyto": "MBiMt3wp8M", "signatures": ["ICLR.cc/2026/Conference/Submission12321/Reviewer_2Gtw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12321/Reviewer_2Gtw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820792986, "cdate": 1761820792986, "tmdate": 1762923246566, "mdate": 1762923246566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GeoDrag, a geometry-guided drag-based image editing method. Similar to FastDrag, it enables drag-based manipulation without LoRA and operates in a single forward pass. The key idea is to incorporate 3D geometric cues into 2D drag editing through geometry-aware field modeling, where displacement strength is modulated based on the depth map. In addition, the authors introduce spatial plane modulation to complement this, since geometry-aware motion distributes influence uniformly in 3D space, making it less sensitive to local details. The fusion of geometry and plane-aware fields allows the method to maintain both global structure and fine-grained local control (conceptually related to warpage vector modeling in FastDrag). Experiments on DragBench demonstrate that GeoDrag achieves lower Mean Distance (MD) and Dragging Accuracy Index (DAI) than previous state-of-the-art methods, while performing editing faster and without additional training overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- GeoDrag achieves the best performance on DragBench, showing the lowest MD and DAI metrics while maintaining competitive image fidelity and efficiency.\n- The framework is simple yet effective, enabling drag-based editing in a single forward pass without LoRA tuning, similar to FastDrag but extended with geometry awareness.\n- By introducing geometry-aware field modeling and spatial plane modulation, the method is able to integrate 3D geometric cues into 2D editing, improving structure preservation and local controllability.\n- The proposed conflict-free partitioning strategy effectively handles multi-point editing, preventing interference between overlapping drags and ensuring stable, coherent results."}, "weaknesses": {"value": "- In Section 3.2, the paper mentions that the geometry-aware displacement field “distributes influence uniformly in 3D space, making it less responsive to subtle and local deformations in 2D image plane.” However, this concept is not intuitive; additional explanation or visual illustration would help readers understand it better.\n- The method heavily relies on accurate depth maps. In scenes where the depth estimation is noisy, such as generated images or complex high-frequency scenes with multiple overlapping objects, the geometry-aware field may be incorrectly computed, potentially degrading results compared to purely 2D approaches. Failure case analysis would strengthen the paper.\n- Simply projecting 3D depth information into 2D space may not fundamentally resolve geometric ambiguity, since projection inherently loses spatial consistency across varying viewpoints.\n- The novelty of the plane-aware field in Section 3.2 feels somewhat limited. Its formulation resembles the displacement weighting in FastDrag, which also applies a distance-based decay for local control. The paper would benefit from a clearer analysis or ablation demonstrating how the proposed plane-aware formulation improves upon or differs from *FastDrag* in practice.\n- In Eq. (10), the geometry-aware and plane-aware fields are linearly interpolated with λ = P / (P + γ). This λ depends only on 2D distance and ignores depth variation, thus disconnecting from the true 3D structure. It would be interesting to see whether defining λ as a combined function of both distance and depth difference could yield a more geometrically consistent fusion.\n- Missing comparison with GoodDrag (ICLR 2025), a recent baseline for drag-based editing."}, "questions": {"value": "As mentioned above, in Eq. (10), λ is defined as P / (P + γ), which does not incorporate depth. Have the authors considered using a joint distance–depth weighting function to better reflect 3D spatial relationships?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aRuYKZwq5m", "forum": "MBiMt3wp8M", "replyto": "MBiMt3wp8M", "signatures": ["ICLR.cc/2026/Conference/Submission12321/Reviewer_KdHj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12321/Reviewer_KdHj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893649072, "cdate": 1761893649072, "tmdate": 1762923246171, "mdate": 1762923246171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GeoDrag, a drag-based image editor that injects 3D geometry into 2D dragging. The authors derive a geometry-aware influence field. Specifically, per-pixel motion scales with the relative depth to a handle point, so nearer pixels move more than farther ones, reducing perspective artifacts during editing. For multi-handle cases, they introduce a conflict-free hard partition so each pixel is driven by exactly one handle, avoiding direction cancellation. On DragBench, GeoDrag achieves the best edit precision (lowest MD/DAI) with competitive speed/memory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The derivation from 3D displacement to 2D motion motivates a depth-modulated field that is simple, interpretable, and implementation-light. The proposed approach effectively reduces perspective-induced tearing during large drags.\n* On DragBench, GeoDrag attains best MD and best DAI1/10/20 with a one-step pipeline and 3.95s per point / 5.44 GB peak memory, competitive with FastDrag’s 3.23s / 5.85 GB and faster than LoRA-prep methods (∼1-min prep). Compared with baselines, for which LoRA-prep is often necessary, the proposed method shows better generalizability.\n* The partitioning successfully creates predictable, non-interfering edits. Ablations show it outperforms soft blends like Directly-Add, Pixel-Distance, and Drag-Magnitude across MD/DAI/IF (Fig. 10b)"}, "weaknesses": {"value": "* The simplification that neglects motion along the optical axis to obtain is reasonable for small motions, but the paper does not quantify robustness to depth noise (from monocular estimation), likely failure modes for strong perspective or specular/textureless regions. A perturbation study of these factors would better support claims of geometric stability. Additional failure cases will also help to understand the limitations of the work.\n* The runtime table shows FastDrag is still faster per point (3.23 s) than GeoDrag (3.95 s) on the same GPU, and the manuscript does not provide a full stack breakdown (depth inference, partitioning, field fusion, masked DDIM) to explain where time goes or how to optimize it. A consolidated per-stage timing would strengthen the efficiency story.\n* Although the proposed method is claimed to work better for long-range drag operation, the qualitative results show that it still has problems where the handle points are not actually moved to the target points (Figure 5 Ours, there are always small distances). However, such a case is usually not seen in copy-paste methods like SDEDrag. \n* The geometry-aware influence field is derived under small-motion, locally smooth assumptions and works best when pixels “move with” nearby handles. However, edits that require self-occlusion swaps or reveal previously unseen regions may lead to problems."}, "questions": {"value": "* In practice, one common idea is that LoRA should improve the performance in drag editing. However, Table 2 suggests that LoRA actually hurts the performance for most metrics. Is there any further explanation or examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q0UGNDwaOE", "forum": "MBiMt3wp8M", "replyto": "MBiMt3wp8M", "signatures": ["ICLR.cc/2026/Conference/Submission12321/Reviewer_VFmB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12321/Reviewer_VFmB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990887402, "cdate": 1761990887402, "tmdate": 1762923245690, "mdate": 1762923245690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}