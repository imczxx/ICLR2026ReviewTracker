{"id": "cNZEPEKKEh", "number": 15349, "cdate": 1758250461600, "mdate": 1759897312398, "content": {"title": "UpSkill: Mutual Information Skill learning for Structured Response Diversity in LLMs", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of large language models (LLMs) on mathematics and programming tasks, often by maximizing *pass@1* correctness. However, optimizing single-attempt accuracy can inadvertently suppress response diversity across repeated attempts, narrowing exploration and overlooking underrepresented strategies. We introduce UpSkill, a training time method that adapts *Mutual Information Skill Learning* (MISL) to LLMs\nto induce *structured response diversity*: a discrete latent $z$ selects a reproducible ``strategy\" that steers the token distribution toward distinct modes. We propose a novel reward that we implement within Group Relative Policy Optimization (GRPO): a *token-level* mutual information (MI) reward that encourages trajectory specificity to $z$. Experiments on GSM8K with three open-weight models, Llama 3.1--8B, Qwen 2.5-7B, and R1-Distilled Qwen2.5-Math-1.5B show that UpSkill improves multi-attempt metrics, yielding median gains of $\\sim$4\\% in pass@k and $\\sim$7\\% in consensus@k without degrading pass@1. Additionally, we show theoretically that mutual information is closely tied to pass@k, providing a theoretical justification for UpSkill.", "tldr": "We introduce a reinforcement learning approach that uses mutual information rewards to encourage response diversity in LLMs, improving multi-attempt success (pass@k,consensus@k) by training models to produce diverse yet accurate reasoning strategies.", "keywords": ["Language Models", "Mutual Information", "Reasoning", "Response Diversity", "RLVR"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8fad098d948a1932e4e8896c104f67fa72c718b.pdf", "supplementary_material": "/attachment/40e8d3c6c60260760aa62185155fa91f61a7386a.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the critical issue of \"diversity collapse\" in LLMs, where standard reinforcement learning (RLVR) techniques aimed at optimizing single-attempt accuracy (pass@1) inadvertently reduce response variety, thereby limiting multi-attempt (pass@k) performance. The authors introduce UpSkill, a novel training-time method that integrates Mutual Information Skill Learning (MISL) into the GRPO framework. This is achieved by conditioning the model on a discrete latent \"strategy\" variable $z$ and introducing a token-level mutual information reward ($r_{TMI}$). This reward incentivizes the model to produce distinct, reproducible strategies for each $z$. Experiments demonstrate the method's effectiveness, starting with a controlled arithmetic environment where UpSkill prevents diversity collapse and significantly boosts pass@5, and extending to the GSM8K benchmark, where it achieves median gains in pass@k and consensus@k across several models without degrading pass@1 accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Basically, in my opinion, this paper's primary strength lies in its principled approach, tightly coupling the proposed method with a strong theoretical justification. \nThe authors provide a formal proof (Lemma 1) that directly links the mutual information objective $\\mathcal{I}(\\tau;z|x)$ to a bound on pass@k improvements, moving the work beyond simple empirical heuristics. \nMoreover, the theoretical proof is well-supported by the empirical results. The \"arithmetic environment\" (Section 5.1, Figure 4) serves as an good controlled experiment, clearly visualizing how the baseline model's diversity completely collapses (pass@1 converges to pass@5) while UpSkill maintains a significant performance gap, proving the concept. The method addresses a good implement potential, showing that on the complex GSM8K task, it is possible to enhance multi-attempt metrics (pass@k, consensus@k) without the common side effect of sacrificing single-attempt (pass@1) performance."}, "weaknesses": {"value": "Basically, I think the experiments and analysis should be more sufficient and solid.\n1） The experiment results for the Llama 3.1-8B model is decently negative and seems negative, where pass@k performance decreased by 2% (from 88% to 86%). The lack of discussion on this contradictory finding in the main text undermines the method's claimed robustness, especially on high-capability models.\n\n2）The presentation of the main GSM8K results is incomplete. The specific value of $k$ used for the pass@k, plurality@k, and consensus@k metrics is not explicitly stated in the main body of the paper, making the magnitude of the improvements difficult to interpret.\n\n3) The main results chart (Figure 5) is missing a numerical label for the Llama 3.1-8B pass@1 \"with MI\" condition, which prevents a full and clear comparison of this key metric on the strongest baseline model."}, "questions": {"value": "1) Could the authors elaborate on the negative pass@k result for Llama 3.1-8B? Does this finding suggest a fundamental limitation of UpSkill when applied to stronger base models, perhaps aligning with the findings in Appendix F, or is it potentially an artifact of hyperparameter selection?\n\n2) What is the computational overhead of the token-level MI reward (Equation 5) during training? Does calculating the mixture distribution $p_{\\pi}(y_t|x,y_{<t})$ require N separate forward passes, and if so, how does this impact training efficiency?\n\n3) Given that other contemporary work (e.g., Chen et al., 2025) explores the direct optimization of the pass@k objective, what is the primary advantage of using the mutual information approach? Does UpSkill, for instance, produce more semantically distinct or interpretable strategies than direct pass@k optimization would?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MV0cbMfzVl", "forum": "cNZEPEKKEh", "replyto": "cNZEPEKKEh", "signatures": ["ICLR.cc/2026/Conference/Submission15349/Reviewer_ZGnJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15349/Reviewer_ZGnJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729018680, "cdate": 1761729018680, "tmdate": 1762925637321, "mdate": 1762925637321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces UpSkill, a method to enhance \"structured response diversity\" in LLMs through Mutual Information Skill Learning (MISL). By conditioning on a discrete latent variable *z*, the model learns distinct reasoning strategies, which impoves multi-attempt metrics (on the arithmetic environment and gsm8k) while preserving single-attempt accuracy (only on gsm8k)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a comprehensive theoretical analysis aimed at establishing a connection between mutual information and the llm's pass@K bounds. By introducing token-level mutual information into the reward, the approach enhances the diversity of the model's output strategies, thereby increasing the success rate of Pass@K. The proposed idea is novel enough, and experimental results indicate that it effectively improves the model's Pass@K capability."}, "weaknesses": {"value": "1. The experiment results of the paper are not convincing. First, the results across the two different experimental settings are inconsistent: in the first experiment, Pass@K improves but Pass@1 stays low, while in the second experiment, Pass@K improves without degrading Pass@1. This indicates that the results are highly sensitive to the environment/dataset, but the paper only tests this method on two environments/datasets, which makes the results lack robustness. Moreover, the experiments only include in-distribution evaluations, without any out-of-distribution (OOD) benchmarks, making it difficult to see the generalization of this approach. Additionally, the analysis of the experiment section is also not enough to explain the results.\n2. In the proof section, the paper introduces a strong assumption, but later states that this assumption is difficult to hold in practice. As a result, despite extensive theoretical analysis, the theory does not sufficiently support the final experimental results.\n3. Finally, the contribution of the approach remains unclear. Although this approach may be beneficial for improving Pass@K, its performance on Pass@1 is unstable, and it requires careful tuning of the number of *z* (which significantly affects the performance and stability). Moreover, while *z* can control the diversity of responses, the actual meanings of different *z*s are not interpretable, making it nearly impossible for users to use *z* to control the strategies in llm's solutions."}, "questions": {"value": "1. Have you analyzed whether the learned *z* acquires any generalizable semantic meaning after training?\n2. Have you attempted to evaluate the method's generalization ability across different domains?\n3. The experimental analysis in gsm8k explains that the pass@1 is maintained due to the existence of a larger set of correct\nreasoning approaches. But later in the ablation part, it is mentioned that many GSM8K problems admit only a limited number\nof distinct solution paths. There seems to be a contradiction between these statements, and this further strengthens my concern about the generalizability of *z*.\n4. The analysis \"UpSkill improvement is negatively related to pass@1 (equivalently\npass@kB ) capability\" which explains the low Pass@1 in the Arithmetic Environment seems to conflict with the observation in gsm8k, where Pass@1 is maintained or even improved. Could you please provide an analysis to clarify this discrepancy?\n5. During testing, if only a single response is generated, how should one select *z* to ensure the quality of the response? Is it possible to let the model choose *z* automatically, rather than requiring manual specification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jUCQwn6I5G", "forum": "cNZEPEKKEh", "replyto": "cNZEPEKKEh", "signatures": ["ICLR.cc/2026/Conference/Submission15349/Reviewer_CCs5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15349/Reviewer_CCs5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816378735, "cdate": 1761816378735, "tmdate": 1762925636959, "mdate": 1762925636959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the diversity collapse problem in RLVR, where existing methods like GRPO, which optimize for single-attempt accuracy (pass@1), can inadvertently reduce output diversity. The authors propose UpSkill, a training-time method that conditions the model on a latent discrete variable z and employs a novel token-level mutual information reward to encourage distinct responses. Experiments on GSM8K demonstrate that UpSkill can improve pass@k and consensus@k metrics without degrading pass@1 accuracy. The method is also supported by a theoretical analysis linking the mutual information objective to a lower bound on pass@k improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper targets a crucial problem in existing RL training: the trade-off between single-attempt accuracy and multi-attempt diversity. The authors proposed a novel adaptation of MISL, and the formulation of the token-level mutual information reward made a valuable contribution.\n2. The proposed method is supported by theoretical analysis, which links the mutual information objective to a lower bound on pass@k improvement.\n3. Experiments show that UpSkill is effective at improving both the single-attempt accuracy and multi-attempt diversity."}, "weaknesses": {"value": "1. The scalability of UpSkill appears limited, as its experiments are confined to the GSM8K dataset with N=5, a relatively simple benchmark for mathematical reasoning. \n2. The method's stability is questionable, as it shows inconsistent effects on the pass@1 metric across different arithmetic and GSM8K tasks. It also remains unclear whether UpSkill is effective when applied to more powerful base models."}, "questions": {"value": "1. What if we experiment with UpSkill on more complicated reasoning tasks? I would like to see the results of whether UpSkill can still **effectively** improve the multi-attempt diversity when the query is inherently complex (this would potentially yield more diverse solutions)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "b8fUQpp9iY", "forum": "cNZEPEKKEh", "replyto": "cNZEPEKKEh", "signatures": ["ICLR.cc/2026/Conference/Submission15349/Reviewer_Vey4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15349/Reviewer_Vey4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762197498161, "cdate": 1762197498161, "tmdate": 1762925636508, "mdate": 1762925636508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}