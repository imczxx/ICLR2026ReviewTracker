{"id": "23Ea4fVGiI", "number": 16104, "cdate": 1758260086438, "mdate": 1763022133485, "content": {"title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks", "abstract": "Large Language Models (LLM) often struggle with reasoning ability and procedural tasks. \nMany prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT), however these techniques too have come under scrutiny as LLMs ability to reason at all has come into question.\nBorrowing from the domain of education, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications.\nThe TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing LLM reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. \nThe study plans to evaluate its approach using the PlanBench benchmark, focusing on the blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help LLMs better decompose complex planning problems into manageable subtasks. \nOur research stands to bridge the gap between symbolic reasoning approaches and modern neural language models by establishing the conditions under which the TMK framework can enhance LLM reasoning to support broader applications in agentic systems.", "tldr": "Using the Task-Method-Knowledge (TMK) framework to structure prompts boosts the reasoning and planning of LLMs. By providing context on the \"why, what, and how\" of a task, this method improved model performances on the PlanBench benchmark.", "keywords": ["Large Language Models (LLMs)", "Procedural Tasks", "Task-Method-Knowledge (TMK)", "Knowledge Representation", "Hierarchical Task Decomposition", "Prompt Engineering"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2cb11b6d6e22d16c52fe5e7669a2e637dd9edfb0.pdf", "supplementary_material": "/attachment/79f4b4bc9e615006422dd28572125af2fff918f7.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a prompting approach based on the Task–Method–Knowledge (TMK) framework, originally from educational research. The authors argue that LLMs often fail on multi-step planning tasks because typical prompts provide only shallow textual goals. TMK decomposes each problem into structured layers: task (goal and conditions), method (procedure), and knowledge (objects and relations). The authors uses PlanBench’s Blocksworld domain in a TMK-formatted json and compare it with the plain prompts on GPT family (GPT-4, GPT-4o, o1-mini, o1, and GPT-5). Results show modest but consistent improvements."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "• The paper introduces a simple and interpretable idea: representing planning tasks in a structured TMK format may align with how procedural knowledge is expressed in model pre-training data\n\n• The approach is prompt-based and requires no fine-tuning or external resources, making it easy to reproduce and extend.\n\n• The empirical trend (larger gains for weaker models) is intuitive and suggests the TMK structure provides helpful inductive bias."}, "weaknesses": {"value": "• The experiments are narrow: only one domain and one benchmark family. Claims about general planning improvement are therefore not well supported.\n\n• There are no comparisons with so many other structured prompting methods (eg CoS, ReAct, least-to-most, chain-of-thought scaffolding and so on). It is unclear whether TMK offers advantages beyond simply using more structured json templates.\n\n• The explanation of why TMK helps remains speculative. No ablation isolates whether improvements come from the educational knowledge model framing or just from better formatting and key-word cues."}, "questions": {"value": "The writing quality and technical presentation are weak, with many typos, missing citations, and incorrect or inconsistent latex usage (eg mismatched quotes and unescaped underscores). The paper would need careful proofreading and formatting cleanup."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xi20yPNQqL", "forum": "23Ea4fVGiI", "replyto": "23Ea4fVGiI", "signatures": ["ICLR.cc/2026/Conference/Submission16104/Reviewer_MjAL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16104/Reviewer_MjAL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700391508, "cdate": 1761700391508, "tmdate": 1762926281972, "mdate": 1762926281972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies integrating the Task-Knowledge-Method (TMK) framework into prompting for LLM reasoning tasks. It focused on the blocksworld task in Planbench and experimented with OpenAI models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper shows the potential of applying a prompting technique to improve the performance of models on the blocksworld task."}, "weaknesses": {"value": "1. The reviewer finds it difficult to understand what is the core idea/contribution of TMK prompting method proposed in the paper. The paper did poorly in explaining the proposed prompting method.\n2. The paper proposes a prompting method, yet there are no examples of the prompt in the paper.\n3. Experimental evidence of the advantage of the proposed method is very limited: only OpenAI models, only one task (blocksworld), and many numbers are missing (Table 2)"}, "questions": {"value": "The presentation of the paper needs a lot more work, to list a few:\n1. Missing citation in line 39.\n2. Missing space in line 50.\n3. Missing citation in line 394.\n4. A formatting suggestion: most citations in the paper should use `\\citep{}` instead of `\\cite{}`.\n5. Figure 1 is too big, and the resolution of the image is too low."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qnOOECh1n8", "forum": "23Ea4fVGiI", "replyto": "23Ea4fVGiI", "signatures": ["ICLR.cc/2026/Conference/Submission16104/Reviewer_NaSB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16104/Reviewer_NaSB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878773785, "cdate": 1761878773785, "tmdate": 1762926281073, "mdate": 1762926281073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TMK, a framework to capture specific reasoning structures in LLM reasoning tasks. It features explicit task decomposition, and benefits the LLM reasoning tasks. It proposes a PlanBench benchmark to conduct experiments."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The method focuses on the key problem of long reasoning LLMs, which does not have clear task decomposition during thinking."}, "weaknesses": {"value": "+ Many fields are missing in Table 2, especially Plain Text + One Shot. Note that it's unfair to compare the other two columns (TMK + One Shot vs Plain Text + Zero Shot). The paper does not have other main results.\n+ It does not make sense to replace standard description of blockworlds into irrelevant mystery or random words. No LLM learns it during pre-training, nor people will use those words to describe tasks, nor they will use LLMs like this.\n+ It's not easy to understand what TMK is doing (lacking a concrete example of the prompt/formatting).\n+ The method is only experimented on blockworlds, which is a toy dataset easily solvable by non-machine learning algorithms like DFS. It's unknown whether the method can be generalized to other meaningful domains, such as mathematical, logical, legal, scientific reasonings."}, "questions": {"value": "+ There is only 1 item in the list (Line 054)\n+ Gpt5 -> GPT-5 (Line 308)\n+ Unknown citation (?) (Line 394)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HORvD35O92", "forum": "23Ea4fVGiI", "replyto": "23Ea4fVGiI", "signatures": ["ICLR.cc/2026/Conference/Submission16104/Reviewer_CH5Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16104/Reviewer_CH5Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988204687, "cdate": 1761988204687, "tmdate": 1762926280465, "mdate": 1762926280465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}