{"id": "23Ea4fVGiI", "number": 16104, "cdate": 1758260086438, "mdate": 1763757789582, "content": {"title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks", "abstract": "Large Language Models (LLM) often struggle with reasoning ability and procedural tasks. \nMany prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT), however these techniques too have come under scrutiny as LLMs ability to reason at all has come into question. Borrowing from the domain of cognitive and education science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications.The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing LLM reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study plans to evaluate its approach using the PlanBench benchmark, focusing on the blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help LLMs better decompose complex planning problems into manageable subtasks. Our results highlight a significant performance inversion in reasoning models, where TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5%), effectively bridging the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but as a steering mechanism that forces reasoning models to exit their default linguistic modes and engage formal, code-execution pathways.", "tldr": "Using the Task-Method-Knowledge (TMK) framework to structure prompts boosts the reasoning and planning of LLMs. By providing context on the \"why, what, and how\" of a task, this method improved model performances on the PlanBench benchmark.", "keywords": ["Large Language Models (LLMs)", "Procedural Tasks", "Task-Method-Knowledge (TMK)", "Knowledge Representation", "Hierarchical Task Decomposition", "Prompt Engineering"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42f1c6818c462f5e819d2908690b761c0e365441.pdf", "supplementary_material": "/attachment/b7f542b4f88689bb1647d09e9b015a4721d1763f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a prompting approach based on the Task–Method–Knowledge (TMK) framework, originally from educational research. The authors argue that LLMs often fail on multi-step planning tasks because typical prompts provide only shallow textual goals. TMK decomposes each problem into structured layers: task (goal and conditions), method (procedure), and knowledge (objects and relations). The authors uses PlanBench’s Blocksworld domain in a TMK-formatted json and compare it with the plain prompts on GPT family (GPT-4, GPT-4o, o1-mini, o1, and GPT-5). Results show modest but consistent improvements."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "• The paper introduces a simple and interpretable idea: representing planning tasks in a structured TMK format may align with how procedural knowledge is expressed in model pre-training data\n\n• The approach is prompt-based and requires no fine-tuning or external resources, making it easy to reproduce and extend.\n\n• The empirical trend (larger gains for weaker models) is intuitive and suggests the TMK structure provides helpful inductive bias."}, "weaknesses": {"value": "• The experiments are narrow: only one domain and one benchmark family. Claims about general planning improvement are therefore not well supported.\n\n• There are no comparisons with so many other structured prompting methods (eg CoS, ReAct, least-to-most, chain-of-thought scaffolding and so on). It is unclear whether TMK offers advantages beyond simply using more structured json templates.\n\n• The explanation of why TMK helps remains speculative. No ablation isolates whether improvements come from the educational knowledge model framing or just from better formatting and key-word cues."}, "questions": {"value": "The writing quality and technical presentation are weak, with many typos, missing citations, and incorrect or inconsistent latex usage (eg mismatched quotes and unescaped underscores). The paper would need careful proofreading and formatting cleanup."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xi20yPNQqL", "forum": "23Ea4fVGiI", "replyto": "23Ea4fVGiI", "signatures": ["ICLR.cc/2026/Conference/Submission16104/Reviewer_MjAL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16104/Reviewer_MjAL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700391508, "cdate": 1761700391508, "tmdate": 1762926281972, "mdate": 1762926281972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies integrating the Task-Knowledge-Method (TMK) framework into prompting for LLM reasoning tasks. It focused on the blocksworld task in Planbench and experimented with OpenAI models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper shows the potential of applying a prompting technique to improve the performance of models on the blocksworld task."}, "weaknesses": {"value": "1. The reviewer finds it difficult to understand what is the core idea/contribution of TMK prompting method proposed in the paper. The paper did poorly in explaining the proposed prompting method.\n2. The paper proposes a prompting method, yet there are no examples of the prompt in the paper.\n3. Experimental evidence of the advantage of the proposed method is very limited: only OpenAI models, only one task (blocksworld), and many numbers are missing (Table 2)"}, "questions": {"value": "The presentation of the paper needs a lot more work, to list a few:\n1. Missing citation in line 39.\n2. Missing space in line 50.\n3. Missing citation in line 394.\n4. A formatting suggestion: most citations in the paper should use `\\citep{}` instead of `\\cite{}`.\n5. Figure 1 is too big, and the resolution of the image is too low."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qnOOECh1n8", "forum": "23Ea4fVGiI", "replyto": "23Ea4fVGiI", "signatures": ["ICLR.cc/2026/Conference/Submission16104/Reviewer_NaSB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16104/Reviewer_NaSB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878773785, "cdate": 1761878773785, "tmdate": 1762926281073, "mdate": 1762926281073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Thanks and Response for ICLR Reviewers"}, "comment": {"value": "Thank you for your time and valuable feedback on our paper. We have made substantial revisions to the manuscript to address your concerns regarding clarity, experimental scope, and the theoretical mechanism of our method.\n\nThe main changes include:\n\n* **Updated Hypothesis & Mechanism:** We have refined our core hypothesis to frame TMK not just as a context provider, but as a symbolic steering mechanism. We now provide empirical evidence (via an identified \"performance inversion\" in the o1 model) showing that TMK shifts reasoning from linguistic approximation to formal symbolic manipulation and integrated literature we found to support that.\n* **Expanded Experimental Data:** We have updated Table 2 to include missing baselines (Plain Text) for a fairer comparison. Most notably, we added results for the o1 model on Random Blocksworld, revealing a 65.8% performance gain (31.5% -> 97.33%) with TMK prompting.\n* **Concrete Examples:** We added full examples of the prompt variations to the Appendix to clarify the implementation.\n* **Presentation:** We have corrected all identified typos, formatting issues (including citation styles), and improved the resolution of Figure 1.\n\nWe believe these revisions address the concerns raised by the reviewers and strengthens the paper. We have provided detailed responses to each reviewer's comments below and are primarily seeking clarification and deeper reviews to further improve on the paper.\n\n\n\n\n\n\n\n**References:**\n\n[1] Karthik Valmeekam. Llms-planning: An extensible benchmark for evaluating large language models on planning. https://github.com/karthikv792/LLMs-Planning, 2023. Accessed: 2025-09-24.\n\n[2] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. *Advances in neural information processing systems*, 35:22199–22213, 2022.\n\n[3] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. *Advances in Neural Information Processing Systems*, 36:75993–76005, 2023c.\n\n[4] Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. *Advances in Neural Information Processing Systems*, 36:38975–38987, 2023a.\n\n[5] Siddhant Bhambri, Mudit Verma, and Subbarao Kambhampati. Do think tags really help llms plan? A critical evaluation of react-style prompting. *Transactions on Machine Learning Research*, 2025.\n\n[6] Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. Chain of thoughtlessness: An analysis of cot in planning. *arXiv preprint arXiv:2405.04776*, 2024.\n\n[7] Yongchao Chen, Harsh Jhamtani, Srinagesh Sharma, Chuchu Fan, and Chi Wang. Steering large language models between code execution and textual reasoning. *arXiv preprint arXiv:2410.03524*, 2024."}}, "id": "qpDB8mS4Ke", "forum": "23Ea4fVGiI", "replyto": "23Ea4fVGiI", "signatures": ["ICLR.cc/2026/Conference/Submission16104/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16104/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16104/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763759132828, "cdate": 1763759132828, "tmdate": 1763759666180, "mdate": 1763759666180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TMK, a framework to capture specific reasoning structures in LLM reasoning tasks. It features explicit task decomposition, and benefits the LLM reasoning tasks. It proposes a PlanBench benchmark to conduct experiments."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The method focuses on the key problem of long reasoning LLMs, which does not have clear task decomposition during thinking."}, "weaknesses": {"value": "+ Many fields are missing in Table 2, especially Plain Text + One Shot. Note that it's unfair to compare the other two columns (TMK + One Shot vs Plain Text + Zero Shot). The paper does not have other main results.\n+ It does not make sense to replace standard description of blockworlds into irrelevant mystery or random words. No LLM learns it during pre-training, nor people will use those words to describe tasks, nor they will use LLMs like this.\n+ It's not easy to understand what TMK is doing (lacking a concrete example of the prompt/formatting).\n+ The method is only experimented on blockworlds, which is a toy dataset easily solvable by non-machine learning algorithms like DFS. It's unknown whether the method can be generalized to other meaningful domains, such as mathematical, logical, legal, scientific reasonings."}, "questions": {"value": "+ There is only 1 item in the list (Line 054)\n+ Gpt5 -> GPT-5 (Line 308)\n+ Unknown citation (?) (Line 394)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HORvD35O92", "forum": "23Ea4fVGiI", "replyto": "23Ea4fVGiI", "signatures": ["ICLR.cc/2026/Conference/Submission16104/Reviewer_CH5Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16104/Reviewer_CH5Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988204687, "cdate": 1761988204687, "tmdate": 1762926280465, "mdate": 1762926280465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}