{"id": "bcOD0CLgBb", "number": 8833, "cdate": 1758099560021, "mdate": 1763718896485, "content": {"title": "Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks", "abstract": "Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.", "tldr": "", "keywords": ["Text Embedding", "Privacy", "Defense", "Inversion Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e7a012cf4a81ad401b5d261dc5840018facc772.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE integrates two key components: (1) the identification of privacy-sensitive dimensions based on user-defined concepts, and (2) a Mahalanobis noise mechanism that injects elliptical noise calibrated to each dimension‚Äôs sensitivity. Through comprehensive evaluations on six datasets using three embedding models and various attack scenarios, SPARSE demonstrates significant reductions in privacy leakage while maintaining or improving downstream task performance, outperforming existing differential privacy (DP) methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper addresses a critical problem in privacy-preserving NLP with a novel, user-centric framework SPARSE that enables concept-specific protection via dimension-sensitive elliptical noise. The approach offers a more personalized and ethical alternative to traditional DP methods. The empirical evaluation is thorough and convincing, covering six datasets, multiple embedding models, and diverse attack scenarios. SPARSE consistently outperforms baselines in both privacy protection and downstream utility, a notable achievement."}, "weaknesses": {"value": "The paper can be improved from the following aspects:\n\n1) Lack of Formal Privacy Guarantees: The method does not provide formal differential privacy bounds (e.g., Œµ, Œ¥), which limits its comparability to standard DP approaches and hinders rigorous privacy-utility analysis.\n\n2) User Annotation Burden: Since SPARSE relies on user-defined concept privacy, the paper would benefit from a discussion or empirical analysis of the feasibility and burden of obtaining these annotations in practice.\n\n3) Limited Applicability Across Domains: The approach assumes users can identify and define privacy-sensitive concepts, which may not hold in all contexts. This assumption could restrict the method‚Äôs usability in domains where privacy concerns are implicit or unclear.\n\n4) Computational Overhead: There is no analysis of the runtime or resource cost introduced by SPARSE. Understanding its computational impact would be important, especially for real-time or large-scale deployment scenarios."}, "questions": {"value": "The following questions can be discussed further:\n\n1) How sensitive is SPARSE to errors in concept labeling, and what is the impact of such errors on privacy and utility? Is there any error propagation through the masking and noise injection stages?\n\n2) How well does the method generalize to out-of-domain data, especially when user-defined concepts do not transfer or are unavailable?\n\n3) Can SPARSE operate in zero-shot or few-shot task transfer settings settings without explicit concept annotations? If so, how is privacy managed in the absence of concept-level guidance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J05MrztnxX", "forum": "bcOD0CLgBb", "replyto": "bcOD0CLgBb", "signatures": ["ICLR.cc/2026/Conference/Submission8833/Reviewer_vVqF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8833/Reviewer_vVqF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761013514543, "cdate": 1761013514543, "tmdate": 1762920603391, "mdate": 1762920603391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates text embedding privacy and proposes a novel differential privacy‚Äìbased mechanism, called SPARSE, to defend against embedding inversion attacks. Unlike previous approaches, SPARSE does not apply isotropic noise; instead, it first identifies the embedding dimensions responsible for specific privacy attributes. SPARSE then applies a Mahalanobis mechanism to add noise to these dimensions, leaving the non-sensitive dimensions untouched to preserve downstream performance. The method is empirically evaluated on multiple datasets against two other defense methods and three embedding inversion attacks. A qualitative analysis of the privacy-sensitive domain and a white-box setting concludes the paper."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-motivated and addresses an important privacy-critical topic (embedding privacy). All steps are clearly described and easy to follow. Additionally, all experimental settings are thoroughly detailed, supporting reproducibility.\n- The experiments demonstrate a clear advantage of SPARSE compared to related defense methods. Since the evaluation is performed on multiple datasets, the results appear reliable.\n- Additional analyses of white-box attacks provide a deeper understanding of the method‚Äôs capabilities and support the assumptions made earlier in the paper. Similarly, the qualitative analysis of the identified privacy dimensions offers valuable insights."}, "weaknesses": {"value": "- The paper lacks a thorough discussion of the method‚Äôs limitations. It remains unclear how dependent the method is on high-quality training data. Additionally, there is no discussion of the time required to fit a separate mask per user (or privacy setting).\n- While the experimental results demonstrate an improvement over existing methods, a significant gap remains between research and practical application. For instance, an epsilon of 5 provides strong privacy but reduces utility by half. This trade-off improves for higher epsilon values, but one may still have to accept a leakage of 35%‚Äì50% to preserve utility, requiring an epsilon of 20.\n- There is no analysis of how the method affects the representation of related concepts. For example, if SPARSE protects a concept like gender, how does this influence related concepts such as appearance descriptions or commonly gender-biased attributes?\n\nMinor remarks:\n- There is a missing space in L39: \"T5-based embeddings.Such\"\n- L207: \"We\" should probably be capitalized."}, "questions": {"value": "- Could the method, in principle, also be applied to LLM prompt extraction attacks, such as in ‚ÄúExtracting Prompts by Inverting LLM Outputs‚Äù (Zhang et al.)?\n- How long does fitting the binary masks approximately take?\n- How many attributes can the method defend against simultaneously before utility is completely compromised? For example, if we aim to prevent leakage not only of age but also of gender, ethnicity, illness, political opinion, etc., at the same time.\n- L210: Is removing the private words in the negative sample set the best strategy? Why not replace the attributes with alternative values and check which dimensions change? I would expect some biases due to different input lengths of samples in the two datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hi5Ch2EbSg", "forum": "bcOD0CLgBb", "replyto": "bcOD0CLgBb", "signatures": ["ICLR.cc/2026/Conference/Submission8833/Reviewer_ZNu6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8833/Reviewer_ZNu6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761039415473, "cdate": 1761039415473, "tmdate": 1762920602990, "mdate": 1762920602990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies privacy risks of text embeddings under inversion attacks and proposes SPARSE, a concept-aware defense. SPARSE first learns a differentiable neuron mask to identify embedding dimensions sensitive to a user-defined privacy concept, using a hard-concrete relaxation with an sparsity regularizer. It then injects elliptical noise via a Mahalanobis mechanism calibrated by the learned per-dimension sensitivities, claiming metric-LDP guarantees. Experiments show improved privacy‚Äìutility trade-offs over two DP baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Embedding inversion is relevant to deployed retrieval/RAG systems; aligning protection to user-specified concepts reflects realistic privacy needs beyond coarse PII assumptions.\n- The combination of concept-conditioned sparse dimension selection and anisotropic perturbation is a step beyond spherical Laplace noise.\n- SPARSE shows consistently lower leakage at comparable or better downstream metrics relative to baselines."}, "weaknesses": {"value": "- The pipeline ‚Äúuser-defined $C$ ‚Üí NER to extract tokens‚Äù inherits false positives/negatives and domain coverage limitations. The paper instantiates $C$ mostly with NER/PII tokens and acknowledges extensibility but does not quantify failure modes or robustness to imperfect concept detection\n- Negative samples are built by removing tokens in $C$. This can alter syntax and semantics beyond the concept, potentially making the discrimination task easier in ways not strictly tied to $C$. The classifier may pick up distributional artifacts rather than pure concept-related differences, biasing the learned mask. No controls (e.g., semantically-preserving paraphrases) are discussed.\n- May need more baselines like Gaussian mechanism and SOTA methods like truncated Laplacian mechanism."}, "questions": {"value": "- Your target is preventing inference of concept tokens $C$, but the guarantee is metric-LDP in embedding space. Can you articulate a formal bridge (even approximate) from ùúÄ-$||¬∑||_M$-LDP to bounded leakage of $C$ under a class of attackers?\n- How do you ensure that the classifier distinguishing $D^+$ vs $D^-$ is not exploiting grammatical breaks or topic drift created by token removal $R(s,C)$? Any controls?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N / A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OXKblIcttK", "forum": "bcOD0CLgBb", "replyto": "bcOD0CLgBb", "signatures": ["ICLR.cc/2026/Conference/Submission8833/Reviewer_TqWt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8833/Reviewer_TqWt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404894584, "cdate": 1761404894584, "tmdate": 1762920602406, "mdate": 1762920602406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPARSE, a concept-aware differentially private method to protect the text embeddings from inversion attacks. SPARSE selectively perturbs privacy-sensitive dimensions to apply stronger protection on protected concept. Experiments show that SPARSE has superior privacy protection and downstream performance over standard DP methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Obfuscating sensitive concept in embeddings is a non-trivial problem, and this paper innovatively applies dimension masking and Mahalanobis mechanism to address this challenge.\n- The LDP of Mahalanobis Norm can be connected with Generalized Laplace Mechanism.\n- The authors conducted comprehensive experiments with promising results."}, "weaknesses": {"value": "- The frameworks assumes that sensitive concept are correlated with the embedding dimension, while this might not be the case. The related dimension for each concept could change depending on the context.\n- SPARSE relies on a pre-defined concept vocabulary and their corresponding masks. There could be emerging new concept in real-world, making it computation intensive to retrain the model.\n- In experiment, the authors use NER to extract sensitive information, which is limited. More complex privacy concepts should be considered."}, "questions": {"value": "- How to map each PII to the concept? Does each distinct PII corresponds to a single concept?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Vka5NLPtn", "forum": "bcOD0CLgBb", "replyto": "bcOD0CLgBb", "signatures": ["ICLR.cc/2026/Conference/Submission8833/Reviewer_mmz1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8833/Reviewer_mmz1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923501609, "cdate": 1761923501609, "tmdate": 1762920601811, "mdate": 1762920601811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Text embeddings used in systems like RAG can be inverted to recover sensitive attributes or content. Standard DP defenses add isotropic (‚Äúspherical‚Äù) noise to every dimension, which protects privacy but harms downstream utility and cannot target user-specified sensitive concepts. The paper proposes SPARSE, a concept-aware defense with two parts:\n\n1. Neuron Mask Learning. Using contrastive pairs that do/do not contain a user-defined sensitive concept, it learns a sparse, differentiable mask over embedding dimensions so that ‚Äúprivacy-relevant‚Äù directions are identified.\n\n2. Mahalanobis (Elliptical) Noise. Under a metric local-DP formulation, it injects anisotropic noise shaped by the learned mask (larger noise on sensitive dimensions, smaller on others), implemented as a generalized Laplace mechanism with a diagonal covariance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and well-structured.\n2. The problem and methods are well-defined, especially dataset construction and learning objectives in mask learning.\n3. The results look promising. SPARSE substantially reduces leakage at the same privacy budget while preserving/improving downstream accuracy compared to spherical-noise baselines across semantic similarity and retrieval tasks (e.g., STS12, FIQA) and against multiple inversion attacks (Vec2Text, GEIA, MLC. On clinical text (MIMIC-III), concept leakage (e.g., gender) drops sharply, and a white-box upper-bound variant shows the black-box method performs near that ceiling."}, "weaknesses": {"value": "1. The training cost should be clarified/ compared to the baseline since the proposed method involves the additional mask learning to identify privacy sensitive dimensions.\n\n2. The code was not released."}, "questions": {"value": "Above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JpwKeHOcxJ", "forum": "bcOD0CLgBb", "replyto": "bcOD0CLgBb", "signatures": ["ICLR.cc/2026/Conference/Submission8833/Reviewer_hdGN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8833/Reviewer_hdGN"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762768050427, "cdate": 1762768050427, "tmdate": 1762920601029, "mdate": 1762920601029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}