{"id": "I1garriWB7", "number": 1985, "cdate": 1756974457126, "mdate": 1763731319047, "content": {"title": "Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution", "abstract": "Recent breakthroughs in large language models (LLMs) on reasoning tasks rely heavily on massive, high-quality datasets—typically human-annotated and thus difficult to scale. While data synthesis or distillation offers a promising alternative, existing methods struggle with inconsistent data quality and an inability to dynamically adapt to the evolving capabilities of the model, leading to suboptimal training signals. To address these limitations, we introduce Socratic-Zero, a fully autonomous framework that generates high-quality training data from minimal seed examples through the co-evolution of three agents: the Teacher, the Solver, and the Generator. The Solver continuously refines its reasoning by learning from preference feedback on both successful and failed trajectories; the Teacher adaptively crafts increasingly challenging questions based on the Solver's weaknesses; and the Generator distills the Teacher's question-design strategy to enable scalable, high-fidelity curriculum generation. This closed-loop system produces a self-improving curriculum—requiring no pre-existing tasks or labels. Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25, Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3 and GLM4 series models. Even more surprisingly, synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks, including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, and Gemini-2.5-Pro.", "tldr": "We propose Socratic-Zero, a co-evolution framework which uses three co-evolving agents to autonomously generate high-quality math reasoning data from minimal seeds, outperforming existing methods and larger commercial models.", "keywords": ["Large Language Model", "Math Reasoning", "Data Distillation", "Data Synthesis", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cfb5a3530eb42a7de82f8491d692ecd1380b8bf6.pdf", "supplementary_material": "/attachment/442c92fafc6c69edcf9856f406281338ea5c2918.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Socratic-Zero, a novel framework that generates high-quality training data from minimal seed examples through the co-evolution of three agents: Teacher, Solver and Generator. The Solver learns via DPO, the Teacher evaluates and guides problem generation, and the Generator imitates the Teacher to create new tasks. Through iterative co-evolution, the system improves reasoning performance on math reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and clearly presented, making it easy to follow.\n\n2. It introduces a creative closed-loop framework where the Solver, Teacher, and Generator co-evolve without relying on large external datasets.\n\n3. The proposed method achieves notable performance improvements on mathematical reasoning tasks under the given experimental setup."}, "weaknesses": {"value": "1. One concern lies in the experimental design, which lacks clarity in several aspects. Although the proposed method achieves the highest scores on math benchmarks compared to its baselines, the experimental setup is not well-aligned with common practices. Many strong baselines are missing, such as direct distillation from the same teacher model. Moreover, the reported math scores are not directly comparable to other works that improve reasoning on Qwen3-8B/14B, and there appear to be inconsistencies. For example, the Qwen3 technical report lists a math score of 60.8 for Qwen3-8B-base, while Table 1 in this paper reports 48.8. These inconsistencies make it difficult to assess whether the proposed approach truly outperforms simpler alternatives like distillation.\n\n2. The paper does not clearly specify how many co-evolution iterations were conducted between the Solver, Teacher, and Generator, nor does it report the reasoning performance after each iteration, which would be important for understanding the effectiveness and dynamics of the co-evolution process.\n\n3. It remains unclear whether the proposed method generalizes beyond math reasoning, such as to other domains like code reasoning."}, "questions": {"value": "See the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5aycsPndBJ", "forum": "I1garriWB7", "replyto": "I1garriWB7", "signatures": ["ICLR.cc/2026/Conference/Submission1985/Reviewer_Db9h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1985/Reviewer_Db9h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462161427, "cdate": 1761462161427, "tmdate": 1762915986453, "mdate": 1762915986453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a key bottleneck in advancing the reasoning capabilities of Large Language Models (LLMs): their heavy reliance on massive, human-annotated datasets. To overcome this, it introduces Socratic-Zero, a fully autonomous framework that bootstraps a model's reasoning abilities from a minimal set of 100 seed questions, requiring no further external data.   \n\nThe framework is built on a co-evolutionary system of three interacting agents:\n\nThe Solver: The LLM being trained. It continuously improves by attempting to solve problems and learning from preference feedback on its own successful and failed attempts using Direct Preference Optimization (DPO).   \n\nThe Teacher: A powerful, frozen LLM that acts as an oracle. It evaluates the correctness of the Solver's solutions and, crucially, generates new, more challenging problems that are specifically designed to target the Solver's identified weaknesses.   \n\nThe Generator: A model trained to distill and scale the Teacher's problem-generation strategy. By learning what makes a good, challenging question, the Generator can produce a high-quality, adaptive curriculum at scale without constant reliance on the much larger Teacher model.   \n\nThis closed-loop system creates a self-improving curriculum that dynamically adjusts to the Solver's evolving skill level. The empirical results are significant: the Socratic-Solver-8B model achieved an average performance gain of +20.2 percentage points over previous methods. Furthermore, synthetic data produced by the Socratic-Generator-32B was used to train a student model that ultimately outperformed much larger, state-of-the-art commercial LLMs on several mathematical reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a methodology for leveraging large models to synthesize high-quality data from a small set of seed examples, effectively enhancing the performance of smaller models. A particularly distinctive contribution is the subsequent use of the resulting data pairs to train a separate, medium-sized \"Generator Model.\" This dedicated Generator is a unique feature, offering a specialized and potentially more efficient tool for synthesizing supervised fine-tuning (SFT) data compared to relying solely on the original, larger Teacher Model."}, "weaknesses": {"value": "Socratic-Zero framework combines several techniques in a novel and effective way, its core components are built upon established paradigms in the field, which could be seen as a limitation on its fundamental novelty.\n\nHeavy Reliance on a \"Teacher\" as a Form of Knowledge Distillation: The entire system's success is predicated on the existence of a powerful, fixed \"Teacher\" model that serves as a \"reasoning oracle\". This framing positions Socratic-Zero less as a system that creates knowledge from scratch and more as a highly sophisticated and efficient knowledge distillation framework. The Solver's ultimate reasoning capability is fundamentally capped by the knowledge and reasoning ceiling of the Teacher model it learns from. The innovation lies in distilling the curriculum generation process rather than just answers, but it remains a form of knowledge transfer from a larger, more capable model to smaller ones.\n\nSimilarity to Iterative Self-Training and Self-Play Frameworks: The core loop—where a model generates data based on its performance, which is then used for further training—is conceptually similar to iterative data augmentation, self-training, and self-play methodologies. For instance, the paper's baselines, like LLM2LLM, already use an iterative process of generating questions from failures. Socratic-Zero's main differentiators are the introduction of the Generator for distillation and the use of DPO for learning, but the underlying iterative, self-improving cycle is a known concept in the literature."}, "questions": {"value": "q1: The Role and Reliability of the Teacher Model\n\nThe methodology's reliance on the Teacher Model's output raises a fundamental question concerning the data integrity. Specifically: Is there a mechanism to guarantee that the generated $\\left(q_{\\tau}, a_{\\tau}\\right)$ pairs are universally correct (i.e., constitute ground truth)? Alternatively, is the primary objective of this approach to align the Student Model with the Teacher Model's generation capability? If the latter is true, the final performance ceiling of the proposed method is inherently bounded by the proficiency of the Teacher Model, suggesting that a stronger Teacher Model would directly translate to superior final results. Clarification on this dependency is needed.\n\nq2: Clarity on Data Generation Parameters (Section 4.3.1)\n\nClarification is required regarding two inconsistencies observed in the data generation process detailed in Section 4.3.1:\n\nQuestion Count Discrepancy: Step 1 mentions that each model generates five questions per seed. However, the resulting total of 3,000 generated questions from 1,000 seed questions implies a generation factor of three per seed, not five. Please clarify this numerical inconsistency.\n\nTimeout Parameter: The rationale behind the 600-second timeout setting in Step 2 is unclear. Does this imply that the model must generate the full context length (4096 tokens) within this duration? This parameter seems unusually generous and requires a more detailed technical explanation regarding its necessity and practical impact.\n\nq3: Definitions of Training Stages\n\nThe results tables (e.g., Table X, Y, Z) frequently reference \"Stage 1,\" \"Stage 2,\" and \"Stage 3.\" These stages are not explicitly defined or correlated with the corresponding steps in the training methodology within the main body of the manuscript. Please clearly articulate what each of these stages represents and how they map to the overall training progression.\n\nq4: The Functional Utility of the Generator Model\n\nThe precise functional contribution of the Generator Model to the enhancement of the Solver Model's capabilities requires clarification. Is the training of the Generator Model merely a necessary byproduct of the Solver training process? The manuscript notes its use only for generating the synthesis SFT data for evaluation, but it does not detail any mechanism for utilizing the trained Generator to actively boost the Solver's performance (e.g., by replacing or assisting the initial Teacher Model in a subsequent iteration). Clarification on the Generator's role beyond evaluation is requested."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4NinRAfXoB", "forum": "I1garriWB7", "replyto": "I1garriWB7", "signatures": ["ICLR.cc/2026/Conference/Submission1985/Reviewer_zAsq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1985/Reviewer_zAsq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547308085, "cdate": 1761547308085, "tmdate": 1762915986232, "mdate": 1762915986232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a teacher-solver-generator co-evolution approach to data generation for training large language models. Using a feedback loop, the solver continuously optimizes the reasoning via preference learning, from positive and negative examples. The multi-agent design addresses an existing challenge of scaling of human-annotated datasets, and shows superior performance compared to state-of-the-art methods across various reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the paper addresses a crucial problem of high-quality data synthesis, for optimization of LLMs.\n- the multi-agent framework and co-evolution preference learning mechanism is novel, and the learning framework is scalable\n- the presentation is very clear and easy to follow"}, "weaknesses": {"value": "- It is unclear how much capability the teacher model requires to have in order to generate high-quality question-answer pairs. For example, for problem sets where the teacher may also experience difficulty solving or evaluating the solution, the framework may fail to adapt\n- The generator model provides better scalability, however, it is unclear if the teacher model alone can provide decent performance without the generator model."}, "questions": {"value": "Can the author describe how does the model perform if only using the teacher model, without the generator model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UeqPw8DEME", "forum": "I1garriWB7", "replyto": "I1garriWB7", "signatures": ["ICLR.cc/2026/Conference/Submission1985/Reviewer_K4xK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1985/Reviewer_K4xK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944500624, "cdate": 1761944500624, "tmdate": 1762915985726, "mdate": 1762915985726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Socratic-Zero, a novel, fully autonomous framework designed to bootstrap and refine the reasoning capabilities of LLMs with minimal reliance on external, human-annotated data. The core innovation lies in a closed-loop, data-free co-evolution of three distinct agents: the Solver (which learns and refines reasoning via preference feedback on both successful and failed trajectories), the Teacher (which adaptively creates an increasingly difficult curriculum aligned with the Solver's current weaknesses), and the Generator (which distills the Teacher’s question-design strategy to facilitate scalable curriculum generation). This system aims to address the inconsistent data quality and static adaptation inherent in existing data synthesis and distillation methods by continuously generating high-quality, targeted training signals from minimal seed examples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core architecture fundamentally circumvents the reliance on massive, costly, human-annotated datasets, which is the current scaling bottleneck for high-quality reasoning tasks. This \"data-free\" approach is highly valuable for domains where annotation is prohibitively expensive or complex.\n\n2. The systematic protocol for seed selection, requiring a 30-70% success rate for initial problems, demonstrates a careful attempt to ensure capability-aligned initialization. This ensures the co-evolutionary loop starts from a robust and productive equilibrium."}, "weaknesses": {"value": "1. While the Generator is intended for scalability, the paper itself concedes that \"computational efficiency optimizations\" are a necessity for future work. This suggests the current multi-agent, co-evolutionary loop is likely highly resource-intensive (training three models and maintaining constant interaction), which fundamentally challenges its scalability and practicality compared to simpler, static distillation pipelines.\n\n2. Expanding the framework to new domains is cited as a major area for future work, indicating that the current co-evolutionary success is tightly coupled to the initial domain alignment. This limits the \"data-free\" claim; the methodology still requires significant external effort (or pre-existing capability) before the closed-loop self-improvement can function in a new area."}, "questions": {"value": "The paper highlights that the Solver learns from preference feedback over successful and failed trajectories. Given that the Teacher explicitly crafts the curriculum to target the Solver's weakness zone (30–70% success rate), how does the preference modeling distinguish between a \"valuable failure\" (i.e., a well-reasoned attempt that simply misses the final answer) and a \"chaotic failure\" (i.e., a nonsensical trajectory that offers little instructional value)? Furthermore, have the authors explored an ablation study comparing this preference learning approach against a simpler Policy Gradient or PPO-style method that optimizes the Solver directly using the Teacher's difficulty signal as a scaled reward, and if so, what were the trade-offs in sample efficiency and final performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wg9oPmuJDT", "forum": "I1garriWB7", "replyto": "I1garriWB7", "signatures": ["ICLR.cc/2026/Conference/Submission1985/Reviewer_XXnF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1985/Reviewer_XXnF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975994539, "cdate": 1761975994539, "tmdate": 1762915984655, "mdate": 1762915984655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework for improving reasoning capabilities in LLMs through a three-agent co-evolution process involving a fixed **Teacher** (Qwen3-235B-A22B), a smaller **Solver** (largest is Qwen3-14B-base) and **Generator** (Qwen3-32B). The Teacher adaptively designs problems for the Solver based on its failures, while the Solver learns via preference optimization. The Generator distills the Teacher’s question-design strategy through weighted fine-tuning.\n\n**Solver evaluation** (Table 1): On math and reasoning benchmarks (GSM8K, MATH, AIME, etc.), the approach shows large gains over static and LLM2LLM data-augmentation baselines (56.1 vs. 40.7 and 40.9, respectively, for Qwen3-8B-base).\n\n**Generator evaluation** (Table 5): In a separate ablation, fine-tuning a smaller student model (DeepSeek-R1-Distill-Llama-8B) on data from the co-evolved Qwen3-32B Generator yields higher performance than training it on data from SOTA models (37.22 vs. 36.62 against GPT-5)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The paper **addresses the important problem of enabling models to learn effectively from a limited set of examples**, proposing a curriculum-learning strategy in which a small seed set (just 100 examples, Table 8) is progressively enhanced in line with the Solver’s evolving capabilities.\n\n- The **main results of the paper are substantial and well-supported by experiments**, with the proposed method providing ~15% improvement over the baseline (Table 1).\n\n- **Evaluation is comprehensive and appropriate**, encompassing  seven math and reasoning benchmarks, multiple model architectures, and all relevant parts of the system (solver and generator separately evaluated, in sections 4.2 and 4.3, respectively).\n\n- **Design of individual components is sound** — the curriculum update mechanism (Section 3.1), DPO-based Solver training (Section 3.2), and weighted distillation for the Generator (Section 3.3) are all well-motivated.\n\n- **Experimental configurations are exhaustively documented and the work is fully reproducible**, with code provided in the supplementary material, and detailed training configurations, hyperparameters, and prompt templates included in the appendix.\n\n- The **breadth of supplementary details and discussions in the appendix is impressive**, aiding in the comprehension of the work. For example, including examples of synthetically generated datapoints (appendix F), and extensive comments on convergence analyses (appendix K)."}, "weaknesses": {"value": "- **The final statement of the abstract is ambiguously worded, leading to potential misinterpretation of the results**. It reads: “Synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to other SOTA commercial LLMs…” As written, this implies that the resulting student LLMs themselves outperform SOTA commercial LLMs. However, as Table 5 clarifies, the intended meaning is that student models trained on data generated by Socratic-Generator-32B outperform those trained on data generated by SOTA LLMs, not that the students surpass the SOTA LLMs in absolute performance.\n\n- It is **unclear how the Generator fits into the Solver’s training process, and the paper’s claims about the cohesiveness of the three-agent system are therefore potentially misleading**. The paper repeatedly presents the Generator as a central component: for instance, the abstract states that “the Generator distills the Teacher’s question-design strategy to enable scalable, high-fidelity curriculum generation,” and this is immediately followed by the Solver’s results, creating the impression that the Generator is directly involved in those outcomes. However, the training procedure in Figure 3 and the curriculum update in Section 3.1 suggest that the Generator is orthogonal to the Solver–Teacher loop. It does not appear to contribute to the curriculum on which the Solver is trained, nor to the main results in Table 1 (including the reported +20.2 % aggregate improvement). The authors should clarify exactly which components are used to produce the reported results and, ideally, include experiments where the co-evolved Generator directly participates in the Solver’s training process.\n\n- The **exact details of the overall training procedure is hard to follow** - the paper could be greatly enhanced by providing a pseudocode of the exact training procedure. This is in fact promised at the very end of section 3.1: “The full training procedure is summarized in Algorithm 1”. However, no figure corresponding to Algorithm 1 exists in the paper that’d outline the full training procedure (there is Algorithm 1 in the appendix, but it pertains to the theoretical challenge framework).\n\n- **Impact is limited by its reliance on the Teacher model being more powerful than the Solver**, with experiments using Qwen3-235B-A22B-Instruct-2507 as the Teacher and Qwen3-14B-base as the biggest solver. The paper would be improved by a scaling law study, showing how the methodology performs as a function of the model size gap between the Teacher and Solver.\n\n- The **Teacher model acts both as a problem enhancer and solution verifier of the same problems, risking a bias**. While the authors discuss several mitigation strategies in appendix I (dual-verification with the inclusion of MathRule answer extraction, human review), the work could be improved by ablating a configuration where the judge differs from the enhancer.\n\n- **Experiments section could have more detailed dataset descriptions**:\nHow many total samples across how many prompts were generated for a) for the MetaMath baseline, b) for the LLM2LLM baseline at each stage, c) for the Socratic-Zero approach at each stage.\nWhich dataset was used as the seed for the Static Augmentation and LLM2LLM approaches?\n\n- Some **typos**:\n  - The first sentence on top of page 5 reads: “The Solver and the Generator are co-evolving guided the Teacher” - misses the word “by”.\n  - In section 4.1, the paragraph heading for “Baselines” is repeated, reading: “Baselines. Baselines.”\n  - In section 4.3.1, a sentence reads: “We prompted each generator with 1,000 seed problems from SAND-Math and tasked with producing five augmented variants per seed, resulting in 3,000 total generated problems per model”. The numbers don’t add up - I believe it should read “three augmented variants per seed”, as mentioned in section 4.1.\nIn paragraph 3 of appendix M: “the system generates thousands of ly valuable problems…” - misses the beginning of “highly”.\n\nThe paper is strong overall, but I cannot recommend acceptance in its current form due to the unclear and at times misleading presentation of the system’s cohesiveness and the results (see the first three “drawback” points). I encourage the authors to clarify these aspects and to demonstrate the integration of the Generator with the Teacher–Solver system, or alternatively provide an extended discussion explaining why this integration is not pursued."}, "questions": {"value": "See \"Drawbacks\" section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HepGUKc1Ed", "forum": "I1garriWB7", "replyto": "I1garriWB7", "signatures": ["ICLR.cc/2026/Conference/Submission1985/Reviewer_dpUA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1985/Reviewer_dpUA"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762820519936, "cdate": 1762820519936, "tmdate": 1762915984130, "mdate": 1762915984130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "(Part 1) General Response"}, "comment": {"value": "We sincerely thank the reviewers for their time and insightful feedback. We are greatly encouraged that you recognized our work's core contributions, highlighting its \"substantial and well-supported\" main results (dpUA), its \"novel\" multi-agent learning mechanism (K4xK, zAsq), its \"comprehensive and appropriate\" evaluation (dpUA), and its \"well-motivated\" approach to the \"important and practical problem\" of expensive LLM evaluations (dpUA, K4xK). We are also grateful for the constructive suggestions, which have been instrumental in helping us strengthen the paper's clarity, empirical evidence, and theoretical foundation.\n\nTo address every point raised and to rigorously validate the robustness, scalability, and generality of Socratic-Zero, we have conducted an extensive and demanding suite of new experiments, comprehensive analyses, and significant manuscript revisions during this rebuttal period. These efforts, undertaken in direct response to your invaluable guidance, have led to substantial additions across the paper, all marked in blue with `\\added{}` for your convenience.\n\n**1. Expansion to Cross-Domain, Multimodal Reasoning to Prove Generality**   To address crucial questions about the framework's generality beyond its initial domain, we pushed its boundaries by successfully applying it to an entirely new and challenging area: **multimodal geometric reasoning**. This was a non-trivial adaptation, requiring us to re-engineer the framework to handle both text and image inputs simultaneously. Our method not only worked but demonstrated significant data efficiency and superior performance gains, confirming that Socratic-Zero's co-evolutionary architecture is flexible and powerful enough to operate across different data modalities. This new experiment directly validates the broad applicability of our design.\n\n(See new **Appendix W, Page 32**for full details and results).\n\n**2. Introduction of Rigorous New Baselines for a Fairer, Stronger Comparison**   Following reviewers' suggestions to strengthen our empirical claims, we introduced **three new, formidable baselines**. This involved implementing and fine-tuning models on large-scale external datasets, including knowledge distillation from 75k GPT-4-generated pairs and DPO on 53k high-quality human-curated problems. Socratic-Zero's significant outperformance, even when using a fraction of the data, now stands on much stronger empirical ground, unequivocally highlighting the superiority of its dynamic, adaptive curriculum over static, large-scale data augmentation.\n\n(See the expanded and updated **Table 1 on Page 7** ).\n\n**3. A Full-Scale Teacher-Solver Scaling Analysis to Test Core Hypotheses**   To definitively answer the critical question of whether our framework's success hinges on a vastly superior Teacher, we undertook a comprehensive **Teacher scaling study**. This was a major effort where we systematically varied the Teacher model's capacity across five different scales—from being weaker than the Solver to significantly stronger. The results are decisive: Socratic-Zero delivers substantial improvements even with an equal-sized or weaker Teacher, proving that its power stems from the co-evolutionary loop and strategic use of failure, not just a brute-force capability gap.\n\n(See the new **Table 12 and detailed analysis in Appendix R, Page 28** ).\n\n**4. Theoretical Justification for the Effectiveness of Weaker Teachers**   Beyond just empirical results, and to provide a rigorous scientific explanation for the surprising effectiveness of weaker Teachers, we developed a **formal theoretical analysis** based on the Learning Using Privileged Information (LUPI) framework. This was a demanding theoretical effort that allowed us to prove, from first principles, how the _information asymmetry_ (i.e., the Teacher's access to the Solver's failure trajectory) mathematically compensates for a lack of superior model capacity. This provides the crucial \"why\" behind our empirical findings.\n\n(See the new **Appendix S, Page 29** for the complete derivation).\n\n**5. A Brand-New \"Stage 4\" Experiment to Empirically Prove Scalability**   To provide concrete, undeniable proof for the scalability promised by our three-agent design, we designed and executed a completely new **\"Stage 4\" experiment**. In this critical test, we completely removed the expensive, large-scale Teacher and had our distilled, low-cost Generator take its place to continue the training. The results are a resounding success: the Generator-guided Solver achieved nearly identical performance, confirming it had successfully internalized the Teacher's strategy and can facilitate sustainable, low-cost autonomous improvement. This is the ultimate validation of our framework's core premise.\n\n(See the new **Table 6, Page 10** and its accompanying analysis in **Section 4.3.4, Page 9** )."}}, "id": "oxQgcZRBaG", "forum": "I1garriWB7", "replyto": "I1garriWB7", "signatures": ["ICLR.cc/2026/Conference/Submission1985/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1985/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission1985/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763734533507, "cdate": 1763734533507, "tmdate": 1763735418893, "mdate": 1763735418893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "(Part 2) General Response"}, "comment": {"value": "**6. A \"True From-Zero\" Seed Robustness Test to Validate Data-Free Claim**   To rigorously test the limits of our \"data-free\" claim and address concerns about dependency on initial seeds, we simulated a worst-case scenario. We conducted a **\"true from-zero\" experiment**, replacing our minimal human-curated seeds with fully machine-generated ones. The near-identical performance powerfully demonstrates the framework's robustness and its low dependency on the quality of the initial seed data.\n\n(See new **Appendix V, Page 31**).\n\n**7. A Verifier Configuration Ablation to Preemptively Address Bias Concerns**   To proactively address subtle but important concerns about potential self-consistency bias (i.e., using the same model for enhancing and verifying), we conducted a new **verifier configuration ablation**. We tested various verifier models, including smaller ones and models from entirely different open-source families. The stable results across configurations prove that our framework is not reliant on such a bias and is robust to the choice of verifier.\n\n(See new **Appendix T, Page 30**).\n\n**8. An Alternative Optimizer Ablation to Justify Core Design Choices**   To empirically validate our choice of DPO as the optimizer, and in direct response to reviewer suggestions, we implemented and tested a **PPO-style alternative (GRPO)**. The head-to-head comparison confirmed that DPO not only delivered superior performance but was also significantly more computationally efficient, thus justifying our core design choice from both an effectiveness and a practical standpoint.\n\n(See new **Appendix X, Page 32**).\n\n**9. Massive Documentation Overhaul for Full Transparency and Reproducibility**   To address all requests for improved clarity and to ensure our work is fully reproducible by the community, we undertook a significant documentation overhaul across the entire manuscript:\n\n- **New End-to-End Pseudocode.** We have added a detailed, step-by-step algorithm that lays out the entire Socratic-Zero training loop with complete clarity, leaving no ambiguity in the process. (See **Algorithm 2 in Appendix Q, Page 28**).\n\n- **New Comprehensive Cost Analysis.** A full breakdown of computational costs, memory usage, and inference speeds for each component is now provided, quantitatively demonstrating the framework's efficiency and practicality. (See **Appendix P, Page 26**).\n\n- **New Explicit Definitions for Stages and Datasets.** We have added a dedicated appendix to explicitly define the dynamic training stages, curriculum growth, and exact dataset compositions for all methods, ensuring full reproducibility. (See **Appendix U, Page 31**).\n\n- **Correction of All Noted Typos and Inconsistencies.** Every typo and inconsistency pointed out by the reviewers has been meticulously corrected throughout the manuscript.\n\nWe believe these **nine major additions and revisions**, all carried out in direct response to your invaluable feedback, have not only thoroughly addressed every concern raised but have also fundamentally elevated the quality, rigor, and impact of our work. The paper has been transformed, and we are confident it now represents a much more complete and impactful contribution to the field."}}, "id": "0bEq1isfBN", "forum": "I1garriWB7", "replyto": "I1garriWB7", "signatures": ["ICLR.cc/2026/Conference/Submission1985/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1985/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission1985/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763734715168, "cdate": 1763734715168, "tmdate": 1763735290196, "mdate": 1763735290196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}