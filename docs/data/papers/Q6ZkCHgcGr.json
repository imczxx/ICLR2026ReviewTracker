{"id": "Q6ZkCHgcGr", "number": 9801, "cdate": 1758141239026, "mdate": 1759897694815, "content": {"title": "Generative Point Tracking with Flow Matching", "abstract": "Tracking a point through a video can be a challenging task due to uncertainty arising from visual obfuscations, such as appearance changes and occlusions. Although current state-of-the-art discriminative models excel in regressing long-term point trajectory estimates—even through occlusions—they are limited to regressing to a mean (or mode) in the presence of uncertainty, and fail to capture multi-modality. To overcome this limitation, we introduce Generative Point Tracker (GenPT), a generative framework for modelling multi-modal trajectories. GenPT is trained with a novel flow matching formulation that combines the iterative refinement of discriminative trackers, a window-dependent prior for cross-window consistency, and a variance schedule tuned specifically for point coordinates. We show how our model's generative capabilities can be leveraged to improve point trajectory estimates by utilizing a best-first search strategy on generated samples during inference, guided by the model's own confidence of its predictions. Empirically, we evaluate GenPT against the current state of the art on the standard PointOdyssey, Dynamic Replica, and TAP-Vid benchmarks. Further, we introduce a TAP-Vid variant with additional occlusions to assess occluded point tracking performance and highlight our model's ability to capture multi-modality. GenPT is capable of capturing the multi-modality in point trajectories, which translates to state-of-the-art tracking accuracy on occluded points, while maintaining competitive tracking accuracy on visible points compared to extant discriminative point trackers.", "tldr": "", "keywords": ["point tracking", "flow matching", "generative models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/81b105e19cb25c881271bc42e48e9bec01501c09.pdf", "supplementary_material": "/attachment/62fe2b65eb5ca4caf666a28c44e4d6fb53811c32.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Generative Point Tracker (GenPT), which reinterprets the iterative optimization paradigm of many of the modern point trackers (such as PIPs, CoTracker3, LocoTrack) as a form of flow matching. The authors claim to bridge point tracking and generative modeling by formulating correspondence estimation as learning a continuous denoising process that maps perturbed query coordinates to target positions. The framework introduces Gaussian perturbations to query points, defines an auxiliary velocity field trained with a flow-matching objective, and evaluates both single-sample and Best-of-N inference strategies. The paper also includes a multi-template tracking extension, performing patch-wise correspondence aggregation inspired by LocoTrack."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper tackles a genuine limitation of current discriminative point trackers, their inability to represent uncertainty and multimodal hypotheses in ambiguous or occluded regions.\n- The authors provide comprehensive comparisons across several datasets"}, "weaknesses": {"value": "### Lack of generative insight\nAlthough the paper positions itself as a generative reformulation of tracking, the actual mechanism remains deterministic iterative optimization under Gaussian perturbation, not a generative process.\n- In generative models (diffusion or rectified flow), the model learns to map **pure noise --> data samples**, learning meaningful dynamics along a linear trajectory in data space.\n- In GenPT, the model learns **query + noise --> correspondence**, where the starting point already encodes the spatial identity of the tracked feature. The added noise does not represent a generative latent, only a small random offset to an already meaningful input.\n- Thus, the flow is effectively a regularized refinement of supervised training, not a learned stochastic trajectory from noise to data.\n- Equation 6 changes the standard CoTracker initialization when $l=0$; increasing $l$ simply reduces supervision strength, not adding new semantics.\n- In essence, GenPT = CoTracker3 + Gaussian perturbation + renaming of loss, rather than a true flow-matching model.\n\n### Evaluation issues\n- The Best-of-N performance gains could stem entirely from multiple inference-time noise injections, not a learned generative diversity. No comparison to a simple CoTracker3 + random perturbation at inference baseline is provided.\n- The empirical improvements are small and inconsistent, and the method fails to demonstrate meaningful benefits in standard single-sample evaluation.\n\n### Presentation and clarity\n- The notation is excessive, making the method difficult to follow.\n\n### Overall\nWhile the paper explores a creative framing of point tracking via flow matching, it does not deliver genuine generative insight or methodological novelty. The proposed approach is functionally equivalent to noisy supervised fine-tuning of existing trackers, with only minor differences in objective formulation. The results and framing overstate the impact relative to the simplicity of the actual change."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NcIkkQax7M", "forum": "Q6ZkCHgcGr", "replyto": "Q6ZkCHgcGr", "signatures": ["ICLR.cc/2026/Conference/Submission9801/Reviewer_837U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9801/Reviewer_837U"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575308369, "cdate": 1761575308369, "tmdate": 1762921287272, "mdate": 1762921287272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GenPT, a generative point tracker that models multi-modal trajectories for long-range point tracking in videos. Unlike discriminative trackers that regress a single mean and thus struggle under occlusions or appearance changes, GenPT trains a likelihood-based model with flow matching and three key modifications: (i) iterative refinement within each step, (ii) a window-dependent prior, and (iii) a variance schedule tailored to point coordinates. GenPT achieves competitive visible-point accuracy and state-of-the-art occluded-point accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces the first generative point tracker trained using a modified flow-matching objective for trajectories, extending generative modeling concepts to the task of point tracking.\n2. The authors design three key modules: iterative refinement, window-dependent prior, and variance schedule. These components are well-motivated and thoroughly ablated."}, "weaknesses": {"value": "1. Point tracking is inherently a deterministic problem, so a multi-modal approach may not be well-suited for this task.\n2. The improvements of this model mainly target occluded points. However, the objective function used in models such as CoTracker3 or other similar approaches is typically L=Huber_loss(predicted point,ground truth point)×is_visible_gt(this point) \nIn other words, these models are not explicitly designed to predict occluded points.\n3. The greedy search strategy requires running the algorithm five times, which makes it computationally expensive and time-consuming."}, "questions": {"value": "1. Could you train CoTracker3 with the objective L=Huber_loss(predicted point,ground truth point) and evaluate how much improvement it achieves on occluded points?\n2. Do the failure cases tend to cluster around homogeneous textures or repetitive patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zFBbrdv7rx", "forum": "Q6ZkCHgcGr", "replyto": "Q6ZkCHgcGr", "signatures": ["ICLR.cc/2026/Conference/Submission9801/Reviewer_QTUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9801/Reviewer_QTUL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035452264, "cdate": 1762035452264, "tmdate": 1762921286595, "mdate": 1762921286595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Generative Point Tracker (GenPT), the first framework to address the Point Tracking problem using a Generative Model based on Flow Matching. Existing Discriminative Models struggle with uncertainty (e.g., occlusion) as they regress to a single mean estimate. GenPT overcomes this by modeling multi-modal trajectories, enabling it to sample several plausible paths in ambiguous situations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- GenPT can model and sample from multiple plausible trajectory candidates, particularly when tracking uncertainty is high due to occlusion. This translates directly to state-of-the-art tracking accuracy on occluded points.\n- The model effectively transitions between probabilistic and quasi-deterministic behavior. While always generative, its prediction variance tightly contracts (becoming nearly deterministic) when the tracked point is clearly visible and uniquely identifiable."}, "weaknesses": {"value": "- There is a substantial and recurring performance gap between the Oracle scores (the model's maximum potential) and the Greedy scores (the model's actual performance when relying on its confidence). This fundamental disconnect means the model is poor at judging the quality of the trajectories it generates, limiting the real-world utility of its multi-modality.\n- The advertised speed advantage (2x faster than CoTracker3) is strictly limited to generating a single sample. To achieve the demonstrated improvements in accuracy, the 'Best-of-N' sampling method must be used. This process rapidly increases the runtime, often making GenPT slower than its discriminative counterparts, thus sacrificing one of its key efficiency claims for practical performance.\n- A significant portion of GenPT's SOTA claim relies on the custom TAP-Vid Sliding Occluder Benchmark introduced by the authors, which is specifically designed to highlight its strength in occlusion handling. While useful, the novelty of the benchmark means the competitive results require independent verification across established, universally adopted benchmarks."}, "questions": {"value": "Have the authors explored an adaptive sampling strategy where multiple samples ('Best-of-N') are only generated in windows where the model's initial predicted uncertainty (variance/confidence) is above a certain threshold, rather than sampling N times in every window?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cZjgKvqSV5", "forum": "Q6ZkCHgcGr", "replyto": "Q6ZkCHgcGr", "signatures": ["ICLR.cc/2026/Conference/Submission9801/Reviewer_DBce"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9801/Reviewer_DBce"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156035066, "cdate": 1762156035066, "tmdate": 1762921285746, "mdate": 1762921285746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}