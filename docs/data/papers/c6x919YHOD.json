{"id": "c6x919YHOD", "number": 16599, "cdate": 1758266587300, "mdate": 1759897230479, "content": {"title": "BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning text-to-image (T2I) models with human preferences. However, RLHF's feedback mechanism also opens new pathways for adversaries. This paper demonstrates the feasibility of hijacking T2I models by poisoning a small fraction of preference training data with natural-appearing examples. Specifically, we propose BadReward, a stealthy clean-label poisoning attack targeting the reward model in T2I RLHF. BadReward operates by inducing feature collisions between visually contradicted preference data instances, thereby corrupting the reward model and subsequently compromising the T2I model's integrity. Unlike existing dirty-label alignment poisoning techniques focused on single (text) modality, BadReward is independent of the preference annotation process, enhancing its stealth and practical threat. Extensive experiments on popular T2I models show that BadReward can consistently guide the generation towards malicious outputs, such as biased or violent imagery, for targeted concepts. Our findings underscore the amplified threat landscape for RLHF in multi-modal systems, highlighting the urgent need for robust defenses.", "tldr": "", "keywords": ["Data Poisoning Attack", "Text-to-Image Models", "Reinforcement Learning from Human Feedback"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31823bd8fcd5e8258f76b328e015eb5a254d3a8c.pdf", "supplementary_material": "/attachment/9bb05f7d3035212e65bde83ce8f1553b8e562dcc.zip"}, "replies": [{"content": {"summary": {"value": "The paper present a attack on text-to-image (T2I) models with textual triggers to control certain visual concept feature in the generated image, by poisoning the training data of the RLHF stage. The text prompt in poisoned sample is constructed either by sampling prompts from the training dataset that contain the target text trigger, or with a text generation model from the text trigger. The poisoned images are generated with a image generation model from the text trigger, where one is prompted to contain the visual attribute specified by the trigger, and the other to have the \"negation\" of the attribute. The image with the target attribute is used as the latent target in optimizing an image that fits the poisoned text prompt by aligning the CLIP feature of the image with that of the image with the target attribute, while keeping the visual similarity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper explores clean label training data attacks during the RLHF stage of text-to-image models.\n2. The proposed attack is evaluated against multiple image generators used by adversaries.\n3. The paper has relatively thorough ablations with respect to RLHF steps and poison rates."}, "weaknesses": {"value": "W1. In line 239\n> To evade detection and further refine the attack...\n\nThe paper lacks discussion about what the detections are before this line. \n\nW2. In the ATTACK GENERALITY section, the authors show that synonyms to the text triggers will lead to similar ASR, and treat the phenomenon as a strength of the propose attack. In my opinion, the lack of control over the triggers is a weakness rather than a strength of an attack.\n\nW3. While Table 1 presents ASR results with respect to multiple image generators, the analysis lacks discussion on the high variance of the results in the main text.\n\nW4. If could be helpful if the authors can clarify the prompt construction process described in the experiment section in the methodology section.\n\nW5. In section C.2, it appears to say Stable Diffusion v1.4 is trained with DDPO but  SD Turbo is trained with SDPO. This contradicts with the authors claim to study the attack with respect to different RLHF algorithm, as the target model is not controlled."}, "questions": {"value": "Q1. In Equation 4, the plus operator is overloaded with both image and text inputs, can the authors give a formal definition of the operator?\n\nQ2. In line 210, \n> The adversary selects a trigger-concept pair (t, C) where the clean target model exhibits a certain probability of generating images containing concept C given natural prompts containing trigger t. \n\nCan the author explicitly define \"a certain probability\"? Does the attacker need to test with many candidate triggers to find a suitable trigger? Is the \"clean target model\" before or after RLHF?\n\nQ3. \n> We adopt Clip-ViT-L/14 as the encoder backbone, encoding images and text into embeddings. \n\nIs the same image encoder used in the target image refining process?\n\nQ4. How many steps is typically used in the RLHF training process? Is it in the regime where the attack has good ASR performance?\n\nQ5.\n> We tested ASR on two prompt sets: 100 training prompts and 100 GPT-4o-generated prompts containing the trigger phrase t.\n\nQ6. Are the training prompts the prompts paired with poisoned images?\n\nQ7. What is the experiment configuration of Table 2? I cannot map the results to the ASR in Table 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lTd358CYoZ", "forum": "c6x919YHOD", "replyto": "c6x919YHOD", "signatures": ["ICLR.cc/2026/Conference/Submission16599/Reviewer_quby"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16599/Reviewer_quby"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811525118, "cdate": 1761811525118, "tmdate": 1762926673780, "mdate": 1762926673780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BadReward, a clean-label poisoning attack targeting the reward model used during RLHF in text-to-image diffusion model. The attack works by creating feature-collision poisoned images, which remain visually similar to benign images while being semantically aligned with target malicious concepts in CLIP embedding space. When these poisoned examples are included in preference data, the reward model learns to assign higher scores to outputs containing the target concept whenever a chosen trigger words appear."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The attack does not modify preference labels or require control of annotators, which is low-cost.\n\n2. The attack pipeline and optimization objective are well explained and easy to reproduce.\n\n3. The presentation is well and clear."}, "weaknesses": {"value": "1. Trigger–concept selection is underspecified: The method implicitly relies on choosing trigger–concept pairs that already have some representation overlap in the model’s data distribution. This selection procedure is not formalized, and success may vary across concepts.\n\n2. Novelty: BadReward extends existing clean-label feature-collision poisoning to the reward modeling stage rather than introducing a fundamentally new poisoning mechanism.\n\n3. The paper does not analyze the conditions under which CLIP feature similarity reliably transfers to reward-driven policy updates. \n\n4. The paper only tested the method on SD v1.4 or SD Turbo. More target models should be included for testing."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KAiUjqhOD5", "forum": "c6x919YHOD", "replyto": "c6x919YHOD", "signatures": ["ICLR.cc/2026/Conference/Submission16599/Reviewer_ckj4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16599/Reviewer_ckj4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957834642, "cdate": 1761957834642, "tmdate": 1762926672661, "mdate": 1762926672661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BADREWARD, a clean-label poisoning attack against reward models in text-to-image (T2I) RLHF systems. BADREWARD induces feature collisions in the CLIP embedding space to corrupt the reward model, enabling adversaries to steer T2I model outputs toward targeted malicious concepts when specific triggers are present in prompts. Notably, the attack does not require control over annotations, instead relying on stealthy manipulation of a small fraction of preference training data. Experimental evaluation on Stable Diffusion (v1.4 and Turbo) and several adversarial model generators demonstrates high attack success rates, notable stealthiness by visual and perceptual metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The black-box scenario assumes the adversary can inject even a small fraction of poisoned pairs into the RLHF pipeline. In many real-world alignment pipelines, this step is subject to curation, filtering, or annotation review, which could detect subtle distribution shifts. The paper does not investigate the sensitivity of system-level detection to these injections.\n2. While Table 2 and Figure 4 demonstrate strong SSIM/LPIPS results, stealth evaluation is reduced to pixel-level or shallow perceptual metrics. The paper does not assess detectability by automated anomaly detection methods or statistical audit systems that could flag poisoned distributions in embedding or reward space.\n3. Reward models are built on a fixed CLIP backbone with a simple MLP. There is little discussion of how architecture choices, frozen vs. trainable backbone, or reward model complexity affect the attack’s transferability/durability."}, "weaknesses": {"value": "1. Can the authors provide quantitative comparisons with recent SOTA RLHF poisoning attacks in terms of both effectiveness and detectability?\n2. Have the authors evaluated whether feature-collided samples can be detected by statistical anomaly methods operating in embedding, reward, or preference score space, beyond pixel-perceptual similarity metrics?\n3. Does BADREWARD generalize to subtler forms of steering (e.g., more abstract style or concept changes), or is it reliant on visually salient feature collisions?\n4. Can the authors provide more insight or results on how different choices of the $\\beta$ parameter and initial images ($x_b$, $x_t$) affect stealth and ASR?"}, "questions": {"value": "Refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wAQuMLFsaY", "forum": "c6x919YHOD", "replyto": "c6x919YHOD", "signatures": ["ICLR.cc/2026/Conference/Submission16599/Reviewer_xGt9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16599/Reviewer_xGt9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762563741596, "cdate": 1762563741596, "tmdate": 1762926672111, "mdate": 1762926672111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a method for compromising the reward model used in the post-training of T2I systems. The attack injects explicit and adversarially crafted poisoned training examples, causing the T2I model to generate harmful or inappropriate content upon encountering specific trigger prompts."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Reward-model poisoning in T2I RLHF is underexplored relative to SFT-time poisoning; the paper clearly motivates why RLHF is a sensitive surface."}, "weaknesses": {"value": "1. **Reward-model diversity** The **Feature-Level Poisoning Attack** is evaluated in a CLIP-on-CLIP setting: poisons are crafted with **white-box** access to a CLIP encoder and tested on CLIP-based reward models, which lowers the difficulty and obscures generality and makes the gray-box and black-box threat model overclaimed. Please evaluate on non-CLIP reward backbones (e.g., BLIP, HPSv2, ImageReward), test reward ensembles and multi-reward optimization/confidence-aware training, and report ASR to show the effectiveness of the Feature-level Poisoning Attack.\n\n2. **Lack of evaluation under basic defenses.** Missing evaluation against basic safety defenses. The RLHF setup optimizes semantic/aesthetic alignment rather than safety, so the attacked T2I models are effectively unguarded. In practice, providers deploy baseline defenses—harmful-concept erasure, safety-focused RLHF, and runtime safety filters (e.g., the SD-1.5 Safety Checker, Q16). Please evaluate BadReward under these defenses (individually and combined) and report ASR and utility on benign prompts. If standard guardrails block the attack, its practical impact is limited; if not, show how the method bypasses them.\n\n3. **less informative citations** The claim in L39 that “RLHF is an indispensable component for aligning T2I systems with human expectations” is unsupported. Please provide primary evidence—e.g., technical reports or production case studies from major T2I providers, or peer-reviewed ablations—demonstrating indispensability. Likewise, using broad surveys (e.g., Zhu et al., 2024 at L42) to justify specific limitations of RLHF is insufficient; cite targeted primary sources that document these limitations in T2I settings. If such evidence is unavailable, soften the claim (e.g., “commonly used” or “increasingly adopted”) and reposition the motivation accordingly."}, "questions": {"value": "Please refer to the weakness section and:\n\nExperimental setup — missing details\n\n\n1.**Reward-model training**\nSpecify what is updated: full backbone vs. MLP head only. Clarify any frozen layers, parameter count, and basic optimizer settings (lr, epochs, regularization).\n\n2. **Poison budget**\nReport the number and percentage of semantic-level poisoned pairs used for reward training, plus the mixing schedule per epoch (and whether poisons appear in validation).\n\n3. **Trigger coverage**\nWhy only three triggers? Please report ASR as the number and diversity of triggers increase (including synonyms/paraphrases/typos) and provide variance across seeds to show robustness.\n\n\n4. What is the minimum effective poison budget across targets and training seeds for stable success (1%, 2%, 3% are shown)? Can you report variance across seeds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hY2p8sC77l", "forum": "c6x919YHOD", "replyto": "c6x919YHOD", "signatures": ["ICLR.cc/2026/Conference/Submission16599/Reviewer_6cEx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16599/Reviewer_6cEx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762703050196, "cdate": 1762703050196, "tmdate": 1762926671718, "mdate": 1762926671718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}