{"id": "lM1lpQdjAf", "number": 6496, "cdate": 1757987001739, "mdate": 1763724493783, "content": {"title": "TimeLAVA: Learning-Agnostic Valuation for Time Series Data", "abstract": "Valuing temporal segments and individual time points within time series is crucial for tasks like data curation and robust learning, yet poses unique challenges. Existing methods often fail in this domain because they ignore the critical factors determining a segment's value, such as local patterns, temporal dependencies, and the broader distributional context. To address this, we introduce TimeLAVA, a learning-agnostic framework that quantifies data value by measuring the discrepancy between distributions of temporal segments. The core of this approach is a novel Selective Wavelet-based Wasserstein ($W_{SW}$) distance. This distance metric integrates multi-scale wavelet transforms to capture localized, intra-segment patterns. Additionally, it leverages unbalanced optimal transport to robustly handle non-stationarity and distributional shifts between the sets of segments. The intrinsic value of each segment is then efficiently derived via a sensitivity analysis of the $W_{SW}$ distance, and point-wise values are subsequently aggregated from these segment values. We provide theoretical guarantees linking our segment-based valuation to model-agnostic generalization and demonstrate its robustness. Empirical validation across diverse real-world datasets shows TimeLAVA significantly outperforming baselines at identifying influential and harmful temporal segments for applications like anomaly detection, data pruning, and temporal label noise detection.", "tldr": "", "keywords": ["time series", "data valuation"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/317e3ee5e7e1e3f6e007e8e22f1373cbbfa53c0f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To evaluate the value of each time segment, the proposed TIMELAVA is a learning-free framework that quantifies the value of time segments by introducing a selective wavelet Wasserstein distance. This method combines multi-scale wavelet analysis with unbalanced optimal transport to effectively capture local patterns and handle distribution shifts, significantly outperforming baselines in tasks such as anomaly detection and data pruning. However, this version requires substantial improvements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. This paper presents a solid structure and tackles a novel research problem.\n\nS2, It is backed by substantial theoretical foundations.\n\nS3. The proposed model is validated through a series of experiments, where the model demonstrates state-of-the-art performance."}, "weaknesses": {"value": "W1. The methodology is written in a highly complex manner, making it difficult to directly understand the workflow. Additionally, the paper includes a substantial amount of complex theory, which increases the reading difficulty.\n\nW2.The motivation of the article is untenable. What is the practical application of evaluating the importance of time segments? What impact will the evaluation results have? How is it useful in practice? Will it improve downstream forecasting tasks?\n\nW3. What does the sentence \"all methods compute anomaly scores directly on the potentially contaminated data with unknown anomaly proportion. A clean validation time series is used only as a reference for computing data values\" mean?\n\nW4.The authors' experiments are not thorough, with numerous details missing. For the anomaly detection experiments, the number of compared anomaly detection algorithms is limited, and the latest works are lacking.\n\nW5. What does \"time\" refer to in Table 1? What is the memory complexity of each model?\n\nW6. In Section 5.2, how are the \"top-k% highest-valued segments\" selected? What is the basis for the selection?\n\nW7. In Section 5.2, the authors use simulated data. Are there real-world case datasets (not just simulated) available in practice?\n\nW8. In Section 5.2, what are the objectives of the two complementary tasks? Are they intermediate steps in the forecasting experiments? If so, please explain this in the main text rather than in the appendix.\n\nW9. Data pruning is an interesting finding, but the authors only experiment on AR, which is a toy experiment. Since this is a very simple model and deep learning models dominate today, would the proposed model, when used for dataset pruning, also benefit these deep learning models, such as Cyclnet and TimeMixer? Evaluating more models could improve the robustness of the experiments.\n\nW10. Where is the RMSE metric reflected?\n\nW11. Additionally, in Section 5.2, the definition of \"harmful segments\" is not clearly defined. In my opinion, these segments may only be useless for the current test set but could affect the model's generalization performance. Therefore, it is recommended to add an out-of-distribution generalization evaluation for the model.\n\nW12 The limitations of the paper need to be discussed.\n\nW13 Is the task objective confusing? Does our task objective focus on training segments?\n\nW14 What does X_i^t mean?\n\nW15 The related work section is insufficiently refined. It is recommended to place it in a separate chapter."}, "questions": {"value": "See the \"Weaknesses\" box"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hP6WRHxrtt", "forum": "lM1lpQdjAf", "replyto": "lM1lpQdjAf", "signatures": ["ICLR.cc/2026/Conference/Submission6496/Reviewer_NTyq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6496/Reviewer_NTyq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761437805418, "cdate": 1761437805418, "tmdate": 1762918868448, "mdate": 1762918868448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focus on the time series valuation task and proposes TIMELAVA as a model-agnostic valuation framework, which can capture multi-scale patterns and adaptation to non-stationary dynamics. The key techniques involve the Selective Wavelet–based Wasserstein distance and optimal transport. Theoretical guarantees are also provided. Experiments are carried out based on real-world datasets covering healthcare, finance, and the Internet of Things."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Time series valuation seems to be a novel and practical setting in the time series community.\n\n2. Motivation is clear and reasonable.\n\n3. Theoratical guarantees are provided."}, "weaknesses": {"value": "1. As the framework is built on the principled foundation that `a data point’s value is determined by its contribution to reducing the distance between the training distribution µt and the reference distribution, I think such an assumption needs more discussion.\n\n2. According to Fig. 1, since the time domain provides temporal information, and the frequency domain provides frequency content, we can use both domains together to capture complementary information Why do we further need wavelet transformation?\n\n3. The results reported in Table 1 seem to be problematic. The performance of baselines seems to be poorer than their officially reported ones."}, "questions": {"value": "1. I do not get the explicit problem formulation or task setting of time series valuation. Could the authors proviede a more clear and sufficient part of the problem formulation or task setting ?\n\n2. Could TimeLAVA handle time series with missing values or irregular samples? Since under such cases, the overall distributions may have changed and pose new challenges."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bZz1IUPhbG", "forum": "lM1lpQdjAf", "replyto": "lM1lpQdjAf", "signatures": ["ICLR.cc/2026/Conference/Submission6496/Reviewer_64PL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6496/Reviewer_64PL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658777776, "cdate": 1761658777776, "tmdate": 1762918867990, "mdate": 1762918867990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new valuation method for samples or segments of time series data. This method aims to overcome the challenges induced by sequential data structure: non-stationarity, different temporal scales, and temporal dependencies. To that end, the method combines Discrete Wavelet Transform (DWT) and unbalanced optimal transport to derive a dissimilarity measure between time series segments. DWT aims to represent and compare segments in a multi-scale manner, while unbalanced Optimal Transport (OT) offers robustness to outliers. Segment valuations are defined from the dual potential, while sample valuations are defined by convolution. Theorems regarding robustness, performance, and valuations are proved. The method is illustrated with anomaly detection, data selection, and noisy label detection experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written, easy to follow, and illustrated. Motivations and preliminaries are well described, making understanding the core method easy to follow.\n- Leveraging existing literature, the paper combines strong tools from signal processing (DWT) and statistics (OT) to address the valuation of temporal data agnostic to models. The proposed method has proven theorems that guarantee robustness to outliers, generalization bound, and valuation. Making it suitable and interpretable in practical settings.\n- The paper benefits from an extensive experimental section with remarkable performances on anomaly detection, data pruning/selection, and noisy label detection. Experiments are complemented in appendices, ensuring the reproducibility."}, "weaknesses": {"value": "- A marginal correction could be done, the measure defined is not a metric (as stated in the contribution), but rather a dissimilarity measure. The triangular inequality may fail.\n- While sensitivity to window length and stride has been studied, analysis of the core measure’s parameters (regularizer k and balance c) is missing.\n- As stated in the conclusion, the method is sensitive to the choice of reference set. However, the sensitivity is not measured in the experiments (need a nested cross-validation in supervised experiments).\n\nSome formatting issues: \n- citations and references in colors.\n- Line 197 there is a wrong section reference."}, "questions": {"value": "- Can the authors illustrate the sensitivity of the regularizer k in both supervised and unsupervised settings? And in the supervised setting for the balance c.\n- Can the authors do a nested cross-validation for the supervised experiments to estimate the sensitivity to the reference sequences?\n- Dealing with the multiscale issues with the DWT, may face issues for large window length (curse of dimensionality). Can the authors provide details on how to use their models in supervised settings, such as classification for large time series?\n- In Figure 3, local contextual, global contextual, and point anomaly seem visually alike; can the authors explicitly explain the differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "07TzSqJMQ1", "forum": "lM1lpQdjAf", "replyto": "lM1lpQdjAf", "signatures": ["ICLR.cc/2026/Conference/Submission6496/Reviewer_WWgi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6496/Reviewer_WWgi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821553781, "cdate": 1761821553781, "tmdate": 1762918867560, "mdate": 1762918867560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this study, the authors propose a method to assess the value of individual points and segments in a time series. In particular, they introduce a learning-agnostic measure using a selective Wavelet-based Wasserstein distance, while also providing theoretical guarantees of their approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The authors focus on a very interesting stream of research by measuring the importance of individual time points and patterns in time series.\n2) The proposed method achieves remarkable results across multiple tasks."}, "weaknesses": {"value": "Clarity: \n\n1) The paper is difficult to follow. The problem statement and the proposed solution are not clearly conveyed in the abstract. The authors claim to determine the 'value' of a time series point or segment, however, without clarifying the context of the value. Its meaning could vary depending on the downstream task, e.g. the value relevant to a classification task may differ from that in a forecasting task. Moreover, statements such as 'the wavelet transform preserves both time and frequency, enabling precise localization of transient event, which is critical for identifying impactful patterns in time series' (see Figure 1) stand alone and do not clarify what is meant by impactful.\n\n2) The authors refer to TimeInf [1] as Influence in Figure 4 but as TimeInf in Figure 5. Such inconsistencies further reduce the clarity of the manuscript. \n\nRelated Works:\n\n3) The related works section seems to be quite limited, focusing on the research of four works. How is the proposed method different to approaches such as RioT [2] or SAVA [3]? The manuscript clearly needs to be revised to highlight its contribution in the field of time series analysis. \n\nExperiments:\n\n4) The description of the baseline methods is too vague. A summary of baselines similar to the one in SAVA [3] would be desirable, since it benefits the clarity of the manuscript.  \n5) Additionally, a comparison with SAVA [3] would also benefit the manuscript. \n\n___\n[1] Kessler, Samuel, Tam Le, and Vu Nguyen. \"SAVA: Scalable Learning-Agnostic Data Valuation.\" International Conference on Learning Representations (2025).\n\n[2] Kraus, Maurice, et al. \"Right on time: Revising time series models by constraining their explanations.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases (2025).\n\n[3] Zhang, Yizi, et al. \"TimeInf: Time Series Data Contribution via Influence Functions.\" International Conference on Learning Representations (2025)."}, "questions": {"value": "1) The authors split the time series into segments of size L using a sliding window approach with stride S. Have the authors set S=1 and iterated L from 1 to T, where T is the entire sequence length, to find the most informative patterns? If not, what is the best approach to do so?\n2) Does the proposed approach scale to large datasets? \n3) What is the tradeoff between dataset size and data valuation performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FO1MV0J5XI", "forum": "lM1lpQdjAf", "replyto": "lM1lpQdjAf", "signatures": ["ICLR.cc/2026/Conference/Submission6496/Reviewer_PMWa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6496/Reviewer_PMWa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925234724, "cdate": 1761925234724, "tmdate": 1762918867166, "mdate": 1762918867166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a model-agnostic data valuation framework (TIMELAVA) for time series, designed to quantify the importance of individual time points or temporal segments. By combining multi-scale wavelet transforms with unbalanced optimal transport, it defines the Selective Wavelet–based Wasserstein (WSW) distance to capture local time–frequency features while handling non-stationarity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. TIMELAVA effectively integrates wavelet analysis with unbalanced optimal transport and develops a novel distance metric, overcoming the limitations of existing data valuation methods in handling temporal dependencies and non-stationarity.\n\n2. The use of entropy regularization ensures computational efficiency, avoiding repeated retraining or perturbation sampling; this speed advantage is clearly demonstrated in experiments.\n\n3. The empirical evaluation is comprehensive and diverse, covering anomaly detection, data selection, and noisy label detection, demonstrating TIMELAVA’s generality and wide applicability."}, "weaknesses": {"value": "1. Experimental details are heavily relegated to the appendix. The main text provides relatively brief descriptions of settings, parameters, and results, which may affect readability and reproducibility. For example, in Section 5.1, univariate dataset results are not reported; some datasets (e.g., SMD) are duplicated; Table 1 claims five multivariate datasets but actually only lists four; and Section 5.3 lacks baseline description.\n\n2. Anomaly scores should also be reported and compared against the clean data to assess discriminative power.\n\n3. TIMELAVA relies strongly on the representativeness of the reference set. If the reference distribution is shifted or incomplete, rare but valuable patterns may be underestimated. While this limitation is acknowledged, the paper lacks a detailed discussion of the extent of this impact and strategies to reduce it.\n\n4. It remains unclear whether TIMELAVA’s valuations are reliable if the reference set itself contains anomalies or exhibits distributional drift.\n\n5. No discussion or comparison of computational efficiency (runtime, memory, complexity), despite claiming efficiency."}, "questions": {"value": "1. Make the Experiments section more self-contained and address inconsistencies in writing.\n2. Include a discussion of computational efficiency.\n3. Evaluate the impact of the reference set on performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b7yCTzgZPu", "forum": "lM1lpQdjAf", "replyto": "lM1lpQdjAf", "signatures": ["ICLR.cc/2026/Conference/Submission6496/Reviewer_rJ99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6496/Reviewer_rJ99"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943662317, "cdate": 1761943662317, "tmdate": 1762918866599, "mdate": 1762918866599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}