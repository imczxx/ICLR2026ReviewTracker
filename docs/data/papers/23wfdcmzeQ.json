{"id": "23wfdcmzeQ", "number": 14469, "cdate": 1758236626600, "mdate": 1759897368300, "content": {"title": "Convexified Filtered ANN via Attribute-Vector Fusion", "abstract": "Vector search powers transformers technology, but real-world use demands hybrid queries that combine vector similarity with attribute filters (e.g., “top document in category X, from 2023”). Current solutions trade off recall, speed, and flexibility, relying on fragile index hacks that don’t scale. We introduce fused-based ANN, a geometric framework that elevates filtering to ANN optimization constraints and introduces a convex fused space via a Lagrangian-like relaxation. Our method jointly embeds attributes and vectors through transformer-based convexification, turning hard filters into continuous, weighted penalties that preserve top‑k semantics while enabling efficient approximate search. We prove that our fused method reduces to exact filtering under high selectivity, gracefully relaxes to semantically nearest attributes when exact matches are insufficient, and preserves downstream ANN $\\alpha$-approximation guarantees. Empirically, fused-based method improves query throughput by eliminating brittle filtering stages, achieving superior recall–latency trade-offs on standard hybrid benchmarks without specialized index hacks, delivering up to $3\\times$ higher throughput and better recall than state-of-the-art hybrid and graph-based systems. Theoretically, we provide explicit error bounds and parameter selection rules that make the fusion practical for production. This establishes a principled, scalable, and verifiable bridge between symbolic constraints and vector similarity, unlocking a new generation of filtered retrieval systems for large, hybrid, and dynamic NLP/ML workloads.", "tldr": "Define a new geometric convex space based on transformation to fuse attribute and vectors and represent them simultaneously", "keywords": ["Similarity Search", "Hybrid Query", "ANN", "Convex Optimization", "Vector Database"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b677e96042d50200eda7705c5f7428ba35a229f8.pdf", "supplementary_material": "/attachment/59f3acdaa85ee637f3d90124254a49bec8e13a63.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes FUSEDANN, which fuses the vectors and attributes for filtered vector search. FUSEDANN transforms the vectors using their attributes such that attribute filtering can be conducted using the transformed vectors. The transformation ensures that vectors with the same attribute are ordered according to their vector distances, and vectors with higher attribute priority are ranked first. The experiment results show that FUSEDANN outperforms the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "S1: Transforming vectors and attributes into a unified vector distance is an interesting idea.\n\nS2: Many cases are considered, i.e., a single attribute, multiple attributes, and numerical range filtering.\n\nS3: The experiment results show that FUSEDANN outperforms many baselines."}, "weaknesses": {"value": "W1: It is unclear how FUSEDANN affects normal vector search without attribute filtering. The method cannot use the same index to serve queries that have different priorities for the filters. I understand that recent papers cannot be used to attack the authors according to the review rule but the Naïvx paper seems to be in the opposite direction from the paper, which I believe may be the right direction: entirely separating vector indexing for the attributes for good generality. \n\nNaviX: A Native Vector Index Design for Graph DBMSs With Robust Predicate-Agnostic Search Performance   \n\nW2: Although discussed in Section 4.1, the update cost is high as it essentially requires to update all vectors and rebuild the underlying vector index (e.g., HNSW), which is a major cost.\n\nW3: The performance gains over the baselines are not explained. For instance, for a single numerical attribute, iRangeGraph is expected to perform well when the pass rate is high because it essentially uses a proximity graph index for all vectors and should be close to optimal. It is unclear why can FUSEDANN outperform iRangeGraph at a range width of 100%, and the margin is large for the WIT dataset. If the differences are due to implementation, please report the number of distance computations vs recall instead of QPS vs recall. Similarly, many experiment results require more in-depth analysis. \n\nW4: The main paper is rather condensed and many parts are unclear. For instance, what does Var_s and related terms mean in Eq(3)? Under Eq(5), what does a sub-attribute vector mean? Line 11 is critical for Algorithm 1 and should be explained in detail. Moreover, for the attribute experiments, the selectivity is not specified even in the Appendix, which a key parameter for attribute filtering. The authors are suggested to make a case clear (e.g., for categorical attributes) and move the other cases to the Appendix (but briefly mention the results in the paper)."}, "questions": {"value": "Q1: How FUSEDANN affects normal vector search without attribute filtering? What is an FUSEDANN index is used for vectors with different priorities for the filters?\n\nQ2: Please explain the source of the performance advantages of FUSEDANN over representative baselines in the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ggz70oe9Da", "forum": "23wfdcmzeQ", "replyto": "23wfdcmzeQ", "signatures": ["ICLR.cc/2026/Conference/Submission14469/Reviewer_yPHH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14469/Reviewer_yPHH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923793472, "cdate": 1761923793472, "tmdate": 1762924871564, "mdate": 1762924871564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the important and practically relevant problem of Filtered Approximate Nearest Neighbor (Filtered ANN) search. It proposes a transformation that reduces the Filtered ANN problem to a standard (naive) ANN problem. The transformation is general and supports multiple attribute types as well as range filters.\n\nI found the contributions of the paper to be strong and the problem addressed to be of clear practical importance. However, the quality of writing is very poor, making the paper difficult to read and follow. In its current form, it falls far short of the standards expected for a publishable draft. I strongly urge the authors to invest significant effort in improving the clarity, organization, and overall presentation of the paper before resubmitting to a future venue. While the core ideas are worthy of a good conference, the current version is not.\n\nI am leaning towards a borderline rejection."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 4}, "strengths": {"value": "The paper studies the important and practically relevant problem of Filtered Approximate Nearest Neighbor (Filtered ANN) search, which has significant real-world applications.\n\nThe paper proposes a transformation that reduces the Filtered ANN problem to a standard (naive) ANN problem. This allows existing ANN methods to be directly applied to the filtered setting, which is an elegant and useful idea.\n\nDefinition 3 is well-formulated and general, and the proposed transformation is theoretically grounded. The experimental results demonstrate that the transformation performs well in the settings considered in the paper.\n\nI particularly appreciate the way the authors handle the multi-attribute and range filter cases—it is effective."}, "weaknesses": {"value": "The main weakness of the paper lies in its poor writing quality and lack of formal rigor in presentation. The theorem statements and definitions are written informally, which significantly hampers readability and comprehension. A few specific instances are as follows:\n\n\n1) In definition 1, note that records are of dimension d+\\sum_{j}m_j and not F. \n\n2) Theorem 2 is a probabilistic statement, yet it is phrased as if it were deterministic. The proof contains several probability terms, but it is unclear what the underlying source of randomness is. The theorem statement should explicitly mention that k' is upper-bounded in expectation rather than absolutely. This level of imprecision is not acceptable for a conference submission.\n\n3) The mathematical problem statement itself is not clearly presented. My understanding is that the goal is to design a procedure that solves the problem defined in Definition 3 and the authors do this by transforming it into a standard ANN problem. However, Sections 2 and 3 do not make this connection explicit. I strongly urge the authors to formally state the problem and clearly describe the procedure that addresses it—for example, the single-attribute case in Section 3.\n\n4) The monotone attribute property imposed on S is not well-motivated or explained. The paper should provide intuition or justification for why this assumption is necessary and how it affects the results.\n\nOverall, there are numerous such issues throughout the paper. I found it frustrating to read due to the lack of clarity and structure. The paper requires a major revision focused on improving writing quality, formal precision, and overall organization."}, "questions": {"value": "Please clarify the motivation for introducing the monotone attribute property. In particular, why are the associated variance terms important, and what role do they play in the overall formulation through practical examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FyOWEcWEgm", "forum": "23wfdcmzeQ", "replyto": "23wfdcmzeQ", "signatures": ["ICLR.cc/2026/Conference/Submission14469/Reviewer_TTNh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14469/Reviewer_TTNh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939126370, "cdate": 1761939126370, "tmdate": 1762924871116, "mdate": 1762924871116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a timely and important problem of hybrid vector search, which combines traditional similarity search with discrete attribute filters. While this research area has received considerably attention in recent years, the authors propose a refreshingly new idea for tackling this problem by introducing a transformation scheme for converting a hybrid search problem into traditional vector search by merging the vectors and attributes into a shared vector space. This technique offers a major advantage over existing methods in the literature in allowing for the use of off-the-shelf ANN libraries for hybrid search as opposed to custom indexes. The authors also provide theoretical guarantees on the performance of their algorithm (in the appendix) and evaluate their method experimentally, where they report a considerable speedup over previously proposed hybrid search techniques."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The paper proposes a new method for tackling the hybrid search problem, namely by reducing the task to standard similarity search via a transformation function, that, to my knowledge, is a novel contribution to the literature. While the vast majority of published research in filtered near neighbor search focuses on designing new search algorithms or database indexes for supporting similarity search over attributes, this work challenges this conventional wisdom and demonstrates that one can instead leverage traditional near neighbor search techniques, for which there are several high-quality libraries, to solve hybrid search directly. This is a powerful finding that provides a good contribution to the community. \n\nQuality: The paper is quite thorough in establishing the mathematical formalism and theoretical foundations behind the proposed algorithm. The number of baseline methods evaluated in the experimental section is also very comprehensive and strengthens the main claims of the paper considerably. \n\nClarity: Overall, I found the paper to be very easy to read (though the manuscript would benefit substantially from further proof-reading and enlarging several of the plots and figures). In addition, the looked over the authors' provided supplementary code and found it to be well-documented and easy to follow. I commend the authors for the efforts they put into presenting their materials clearly, and I think this enhances the quality of their submission quite a bit. \n\nSignificance: As alluded to above, I believe this work has the potential for considerable practical impact as it presents a method for solving hybrid vector search without the need for introducing custom indexes. Instead, practitioners can leverage existing near-neighbor search solutions out-of-the-box. This key insight stands out to me as a major strength of this work."}, "weaknesses": {"value": "While I believe this paper presents a number of exciting ideas, I also have some concerns regarding the overall presentation and the experimental evaluation\n\nPresentation\n\n1. Several of the figures and plots in the paper, such as Fig. 4 and Fig 5., are a bit too small to read and parse fully. It would be very helpful to make them bigger. \n\n2. The authors provide a very comprehensive appendix section full of theoretical results, but there is very little mention of these results in the main body of the paper. I think it would be helpful to state (perhaps informally) the major theorems in the main body of the paper so that readers can quickly appreciate the full scope of the contributions in this work. \n\n3. It was not clear to me from reading the paper how the attributes are represented as vectors. In definition 1, the authors mention that the attributes are represented as embeddings \"from a neural network\" but it is not clear to me how this process is done. To my knowledge, this attribute to vector mapping is also not mentioned in the experimental section. I believe that this is a critical detail to explain clearly because it is not obvious how to convert a discrete attribute into a continuous, real-valued vector. Furthermore, this detail is crucial because the main algorithm of the paper assumes access to attributes represented in this form. \n\nExperimental Evaluation\n\n1. As mentioned in item (3) above, it was not clear to me how the attributes are represented as vectors in the benchmark datasets used in the paper. More generally, I don't believe the authors provided sufficient details on how the attributes themselves were constructed for these evaluations. To my knowledge, these benchmark data sets like SIFT and Glove do not come with attributes, so these attributes must have been generated via some process. However, I don't believe these details are currently mentioned in the paper. \n\n2. The authors mention that their technique provides efficiency gains when the \"number of distinct attributes is much smaller than the dataset size.\" I think it would strengthen the paper considerably if the authors could \"stress test\" their method and consider experimental evaluations where the universe of attributes is large and then compare different techniques. Understanding the practical limitations of this proposed algorithm would be very useful in understanding when it provides concretely efficient gains and when it may not be the right tool. \n\n3. A discussion of the preprocessing time and the construction time of the authors' proposed algorithm would be very helpful. It is not clear from reading the experiments section how long it takes to compute the transformation as well as the time it takes to convert the attributes into vector representations. Comparing methods in terms of construction time would be a very valuable addition to the paper. \n\n4. In the appendix, the authors briefly discuss the limitations of their technique with respect to supporting dynamic updates to the index. I think a deeper discussion into this limitation would also be helpful in understanding when the authors' proposed technique may nor may not be suitable for a given task."}, "questions": {"value": "1. How are the attributes converted into vectors? What neural network models were used for this task in the experimental section of the paper? \n\n2. Do you have a sense of how many distinct attributes is too many for the proposed algorithm to lose effectiveness in practice? Would it be possible to evaluate this experimentally? \n\n3. I do not quite understand the use of the term \"Convexified\" in the title of this paper beyond the analogy to lagrange multipliers in convex optimization. Is there a deeper application of convexity in the main algorithm proposed in the paper? If not, I would suggest considering changing the title to focus on the main techniques used in the paper. \n\n4. Have you tried to evaluate this algorithm on dynamic datasets with insertions/deletions? Do you have a sense of where the algorithm might break in this setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xduXErnznq", "forum": "23wfdcmzeQ", "replyto": "23wfdcmzeQ", "signatures": ["ICLR.cc/2026/Conference/Submission14469/Reviewer_y2eR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14469/Reviewer_y2eR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974140694, "cdate": 1761974140694, "tmdate": 1762924870722, "mdate": 1762924870722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### Paper Summary\n\nThis paper introduces a new framework called FUSEDANN to address the important problem of vector search with attribute filtering. Its core idea is to perform a transformation at the data representation level. It introduces a geometric transformation that  preserves nearest-neighbor ordering within attribute classes, supports dynamic attribute priorities, and allows efficient partial index updates. Experiments show that FUSEDANN and its variants comprehensively outperform existing SOTA methods on the QPS-Recall curve across standard hybrid query benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Paper Strengths\n\nS1. Transforming the filtering problem into a geometric space transformation is a novel and powerful idea.\n\nS2. The experimental evaluation is comprehensive, covering single-attribute, multi-attribute, and range query scenarios. In all cases, the FUSEDANN variants demonstrate clear superiority on the QPS-Recall tradeoff against all SOTA baselines, including Filtered-DiskANN, NHQ, DEG, and SeRF.\n\nS3. The method is supported by solid theoretical derivations, including proofs for attribute separation, preservation of intra-cluster order, multi-attribute prioritization, and the geometric mapping of range queries."}, "weaknesses": {"value": "### Paper Weaknesses\n\nW1.  The framework cannot natively support query logic. (See D1)\n\nW2. The core transformation seems to require a strict dimensional constraint. (See D2)\n\nW3. The algorithm may introduce extra storage and computational overhead. (See D3)\n\nW4. There are several formatting errors. (See D4)\n\n### Detailed Comments\n\nD1.  The paper admits in the limitations (Section 7, C) that the framework cannot natively support DNF predicates (i.e., \"OR\" conditions) as this violates metric axioms. While it claims this can be handled via \"query planning,\" this often implies executing multiple queries and merging the results, which can be inefficient in practice.\n\nD2. The core transformation requires attributes to be pre-embedded into a metric space and imposes a strict dimensional constraint, which is inconvenient for common scalar attributes and adds un-discussed complexity.\n\nD3. The algorithm relies on a re-ranking step using original distances, which adds extra storage overhead and computational overhead.\n\nD4. In section 8, the title of conclusion ends with an extra period."}, "questions": {"value": "Please refer to the weakness and detailed comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4PiKec15KL", "forum": "23wfdcmzeQ", "replyto": "23wfdcmzeQ", "signatures": ["ICLR.cc/2026/Conference/Submission14469/Reviewer_hKWW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14469/Reviewer_hKWW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762568947174, "cdate": 1762568947174, "tmdate": 1762924869045, "mdate": 1762924869045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}