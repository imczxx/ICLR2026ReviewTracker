{"id": "1448q0s3zZ", "number": 3167, "cdate": 1757347097911, "mdate": 1763732545179, "content": {"title": "BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals", "abstract": "Biosignals offer valuable insights into the physiological states of the human body. Although biosignal modalities differ in functionality, signal fidelity, sensor comfort, and cost, they are often intercorrelated, reflecting the holistic and interconnected nature of human physiology. This opens up the possibility of performing the same tasks using alternative biosignal modalities, thereby improving the accessibility, usability, and adaptability of health monitoring systems. However, the limited availability of large labeled datasets presents challenges for training models tailored to specific tasks and modalities of interest. Unsupervised cross-modal knowledge transfer offers a promising solution by leveraging knowledge from an existing modality to support model training for a new modality. Existing methods are typically based on knowledge distillation, which requires running a teacher model alongside student model training, resulting in high computational and memory overhead. This challenge is further exacerbated by the recent development of foundation models that demonstrate superior performance and generalization across tasks at the cost of large model sizes. To this end, we explore a new framework for unsupervised cross-modal knowledge transfer of biosignals by training a lightweight bridge network to align the intermediate representations and enable information flow between foundation models and across modalities. Specifically, we introduce an efficient strategy for selecting alignment positions where the bridge should be constructed, along with a flexible prototype network as the bridge architecture. Extensive experiments across multiple biosignal modalities, tasks, and datasets show that BioX-Bridge reduces the number of trainable parameters by 88-99\\% while maintaining or even improving transfer performance compared to state-of-the-art methods.", "tldr": "", "keywords": ["biosignal", "ai for healthcare", "humans and ai", "unsupervised cross-modal knowledge transfer"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b1b7e05d07dc51e2bc39034e8346161ff26bc18.pdf", "supplementary_material": "/attachment/3d9f36618282af22944bc0cf46b4be9c56406602.zip"}, "replies": [{"content": {"summary": {"value": "This paper researches in the domain of multimodal biosignals, investigating how to transform the knowledge across different biosignal foundation models/modalities while being parameter efficient. Towards this end, the paper proposes to train a lightweight bridge network to align representations across modalities. Specifically, the authors investigated (1) How to select which intermediate representations to use for the construction of the bridge network; (2) Using prototype network with low-rank approximation to guide the learning of the bridge network. The proposed architecture, BioX-Bridge, is validated on three multimodal biosignal datasets, showing better or comparable transfer performance while significantly reducing the parameters needed."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper investigated an important problem of multimodal biosignal knowledge transfer.\n2. Novel idea, the bridge architecture is nicely designed with the use of (1) pseudo labels; (2) linear CKA. The bridge architecture also uses a prototype network, that increases the flexibility of incorporating previous knowledges.\n3. Detailed experimental design. Solid selection of existing backbone models for each modality and baseline architectures. The model performs decently well considering its parameter size.\n4. Insightful ablation experiments. Especially insightful to see how the total amount of prototypes would affect the performance, and how the designed architecture is robust to total number of samples."}, "weaknesses": {"value": "1. The experimental design can be more comprehensive. The authors applied the proposed architecture on a total of three datasets, in each one, they consider 2 transfer directions. The paper lacks an analysis on how the model would perform in situations where the dataset contains more than 2 modalities, for example, check the datasets [1] and experiments presented in [2, 3]. Also, it is unclear how the proposed bridge architecture is specific to each dataset - an interesting experiment to consider is to consider two datasets that share the same multimodality e.g. ECG and EEG, and investigate how the bridge encoder differ for two datasets.\n\n[1] Zhang, Xiang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. \"Self-supervised contrastive pre-training for time series via time-frequency consistency.\" Advances in neural information processing systems 35 (2022): 3988-4003.\n\n[2] Liu, Ran, Ellen L. Zippi, Hadi Pouransari, Chris Sandino, Jingping Nie, Hanlin Goh, Erdrin Azemi, and Ali Moin. \"Frequency-aware masked autoencoders for multimodal pretraining on biosignals.\" arXiv preprint arXiv:2309.05927 (2023).\n\n[3] Fang, Ching, Christopher Sandino, Behrooz Mahasseni, Juri Minxha, Hadi Pouransari, Erdrin Azemi, Ali Moin, and Ellen Zippi. \"Promoting cross-modal representations to improve multimodal foundation models for physiological signals.\" arXiv preprint arXiv:2410.16424 (2024).\n\n2. Completeness of existing experimental results. The paper lacks a more detailed explanation of how the prototypes are selected. Also, for the ablation experiments, the ablation is only applied on the WESAD dataset. Are the conclusions consistent across different datasets?"}, "questions": {"value": "1. I am particularly curious about the distribution of the elements computed before the argmin inside equation [4] and [5]. Specifically, how different are the input intermediate representations across different layers for each dataset? How different are the similarity of representation in each layer $l \\in \\[1, ..., L\\]$ for a given layer m? Also, how would the value of m and l differ when considering the same dataset but different sets of samples (e.g. would it be better if the selection of m and l are data-dependent)?\n\n2. Does the performance differ a lot when considering different subjects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WS2K9uynGg", "forum": "1448q0s3zZ", "replyto": "1448q0s3zZ", "signatures": ["ICLR.cc/2026/Conference/Submission3167/Reviewer_mvhb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3167/Reviewer_mvhb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503493717, "cdate": 1761503493717, "tmdate": 1762916581323, "mdate": 1762916581323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a parameter-efficient framework for unsupervised cross-modal knowledge transfer across heterogeneous modalities. The proposed BioX-Bridge trains a lightweight “bridge” network to align intermediate representations between pre-trained foundation models from different biosignal modalities (e.g., ECG, EEG, PPG, EMG), enabling knowledge transfer without requiring labeled data from the target modality. The framework comprises two key components: a two-stage strategy for selecting the optimal bridge positions (in the source and target network) and a prototype-based architecture with low-rank approximation for efficient high-dimensional projection. The authors evaluate BioX-Bridge across three datasets (WESAD, FOG, ISRUC) with four modalities and six transfer directions, showing that it achieves performance comparable to/better than knowledge distillation baselines while significantly reducing trainable parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Leveraging large foundation models for efficient cross-modal knowledge transfer without the computational overhead of “standard” knowledge distillation.\n- The design is straightforward and allows flexibility, incorporating different foundation models as backbones.\n- The paper is well-organized, well-motivated, and easy to follow."}, "weaknesses": {"value": "- W1: Performance improvements. I agree with the authors' motivation that there are domains/modalities where data is not readily available for (pre-)training large models, in which case approaches like BioX-Bridge can be useful. However, in this particular case, it seems that performance degrades when using BioX-Bridge compared to both source/target oracles. Looking at Table 1, in all but one case, applying BXB leads to worse performance than training a model directly on the “target” modality. This is not explicitly discussed in the paper, so I am comparing across the table (eg. ECG (New) 60.11 vs ECG (Oracle) 63.54). This opens a practical question if, at least for these applications, there is a need for such bridging, and if there is, will it lead to benefits. \n\n- W2. The authors state (L355) \" our method for efficient cross-modal knowledge transfer would be even more valuable as they scale up”. While this may be true in terms of the bridge architecture, in general, I find this counterintuitive in how the bridge in/out placements are chosen. As it is, stage 1, involves exhaustive search of the appropriate layer: This would become really expensive with larger models and more sophisticated probes. Therefore, BXB might not scale well overall."}, "questions": {"value": "- See weakness\n\n- If the goal is to align representations between layer m (target modality) and layer l (source modality), why propagate both to the final layer for alignment (Eq. 7)? It seems more direct to use $h^{(old)}_l$ as the pseudo label and align to the prediction/output of $h^{(new)}_m$  This design choice needs clarification—does propagating to the final layer improve alignment?\n\n- The ablation study wrt the bridge placement needs more discussion and detail. On average, choosing a fixed position doesn’t seem to lead to a substantial performance degradation. Are the performances of the “fixed” positions consistent across datasets/modalities/models? Are some combinations better than others?\n\n- How does the bridge position selection strategy perform when the foundation models have very different architectures? The current evaluation uses mostly transformer variants, but biosignal models span can diverse architectures (and modalities).\n\n- Minor comment: I found the notations of “old” and “new” a tad confusing. I would suggest the authors consider using “source/target” for the different models (this is also how I refer to it in the comments)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zVgjhWDghi", "forum": "1448q0s3zZ", "replyto": "1448q0s3zZ", "signatures": ["ICLR.cc/2026/Conference/Submission3167/Reviewer_iQEx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3167/Reviewer_iQEx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907492694, "cdate": 1761907492694, "tmdate": 1762916581080, "mdate": 1762916581080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors are focused on the problem of cross-modal transfer between biosignals, broadly trying to distill knowledge from one modality into another. They introduce their model BioX-Bridge, which attempts to learn a transformation from intermediate representations between two frozen foundation models. They argue that this is a more parameter efficient way of doing cross-modal transfer than previous methods. The model works as following: (1) use linear probing to find the layer in your new modality model that maintains good probing performance on labels from the old modality model. (2) select the layer in the old modality model that you want to conduct the transfer from by maximizing CKA similarity to the layer identified in the previous step. (3) train a two-module network that predicts representations from the selected old modality layer given representations from the selected new modality layer. They conduct experiments across three different datasets and compare to other baselines. They show that BioX-Bridge does comparably or better than baselines, and make the point that training parameters are much less than the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Authors do a good job of discussing prior paper and contextualizing their paper in the broader literature. Figures are also clear and informative.\n- The problem is well-motivated and has clear relevance to practical applications, in particular when discussing parameter efficiency.\n- Experiments show a nice diversity of modality choices (EEG/ECG, EEG/EMG, ECG/PPG)\n- Authors show a good number of different ablations and baseline comparisons."}, "weaknesses": {"value": "- A key part of the method is the choice of bridge layers. To choose the bridge layer in the new modality, the authors say they find the layer that best linearly represents pseudo labels from the old modality. My understanding is that the pseudo labels used are in fact the downstream task labels. I wonder if this gives their method an advantage over other baseline models that should be discussed. That is, the parameter efficiency of BioX-Bridge is because mappings are learned only between two carefully chosen layers. However, if this choice process relies on the use of downstream task labels, this means the bridge positions are not task-agnostic and that the efficiency comes from having access to the downstream labels. Are the other baseline methods (KD, KD-Contrast, etc) more task agnostic? It seems like these points should be discussed and clarified."}, "questions": {"value": "- I'm confused by some details of the bridge architecture. It seems like the bridge is learned per-token-- e.g., as mentioned in section 3.4 if the bridge were a linear map there would be $N_{m}^{new} \\times d_{m}^{new} \\times N_{l}^{old} \\times d_{l}^{old}$ parameters, not $d_{m}^{new} \\times d_{l}^{old}$ parameters. Why this choice of mapping? This seems like an unnecessary constraint too, since the input time series may not be naturally chunked into sequence inputs. Did you all try learning a mapping that isn't token specific, e.g. a simple MLP? \n- I would like a better understanding of how much the bridge selection process affects the performance. Table 2 kind of gets at this, but since it's averaged over 9 different combinations it's hard to interpret. Maybe just separate it out, and also show the linear probe performance for the new model layer for each of them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YypZ924atj", "forum": "1448q0s3zZ", "replyto": "1448q0s3zZ", "signatures": ["ICLR.cc/2026/Conference/Submission3167/Reviewer_Yb6i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3167/Reviewer_Yb6i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942022182, "cdate": 1761942022182, "tmdate": 1762916580889, "mdate": 1762916580889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}