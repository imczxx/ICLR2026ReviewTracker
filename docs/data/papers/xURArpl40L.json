{"id": "xURArpl40L", "number": 24672, "cdate": 1758359147493, "mdate": 1762955896187, "content": {"title": "Is the Prediction Set Size Well-Calibrated? A Closer Look at Uncertainty in Conformal Prediction", "abstract": "Given its flexibility and low computation, conformal prediction (CP) has become one of the most popular uncertainty quantification methods in recent years. In deep classifiers, CP will generate a prediction set for a test sample that satisfies the $(1-\\alpha)$ coverage guarantee. The prediction set size (PSS) is then considered a reflection of the predictive uncertainty. However, it is unknown whether the predictive uncertainty of CP is aligned with its predictive correctness, which is an imperative property for predictive uncertainty. This work answers this open question by investigating the uncertainty calibration of CP in deep classifiers. We first give a definition for the uncertainty calibration of CP by building a connection between PSS and prediction accuracy and then propose a calibration target for CP based on a theoretical analysis of the predictive distributions. Given this defined CP calibration, we present an empirical study on several classification datasets and reveal their weak calibration of CP. To strengthen the calibration of CP, we propose CP-aware calibration (CPAC), a bi-level optimization algorithm, and demonstrate the effectiveness of CPAC on several standard classification datasets by testing models including ResNet, Vision Transformer and GPT-2.", "tldr": "We propose a calibration framework for prediction set size of conformal prediction to make the prediction set not only safisfy the coverage but also align with accuracy.", "keywords": ["Model Calibration; Uncertainty Quantification; Conformal Prediction"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1c633b7c869d900381d2d29269ae0aec0c269a3a.pdf", "supplementary_material": "/attachment/b1a62e5accbfe750b6fe2406f6ced92a40aacca1.zip"}, "replies": [{"content": {"summary": {"value": "The authors identify that the relationship between PSS and model reliability has not been systematically studied in classification tasks. To address this, the paper:\n1. Defines a notion of CP calibration by linking PSS to expected accuracy using a multinomial sampling strategy.\n2. Proposes a calibration target function, theoretically motivated by Dirichlet distribution assumptions, to model the expected relationship between PSS and accuracy.\n3. Introduces CP-Aware Calibration (CPAC) to improve this calibration while maintaining coverage guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important aspect of CP: whether uncertainty conveyed by PSS aligns with correctness. This extends calibration analysis from probabilistic outputs to set-valued predictions, a meaningful and clear contribution.\n\n2. The definition of CP calibration and the derivation of a target calibration function from Dirichlet assumptions look sound. \n\n3. The experiments span two datasets (CIFAR-100 and ImageNet-1k) and architectures (ResNet, ViT, GPT-2). The inclusion of noisy environments (Gaussian, blur, dropout, typos) strengthens the generality of the findings. The authors also include ablation studies (different target functions, pretraining/random initialization, and sampling temperatures)."}, "weaknesses": {"value": "1. Does CPAC formally preserve the (1–$\\alpha$) coverage guarantee under its bi-level optimization? If not, how can we quantify or bound potential deviations?\n\n\n2. While CPAC is compared to Platt scaling and standard temperature scaling within APS, it would be valuable to see comparisons against *post-hoc recalibration* methods (e.g., histogram binning, isotonic regression) or *recent conformal calibration* methods (e.g., random set calibration, Bayesian CP).\n\n3. The reported experimental results are dense and broad, while the **analysis** on experiments is occasionally \"superficial\". For instance: \n\n3a. Why CPAC sometimes reduces coverage slightly is not well explained. \n\n3b. Is there a theoretical or empirical analysis of the calibration-efficiency frontier? (Similar to 3a)\n\n3c. The effect of hyperparameters ($\\lambda$, $\\tau$, learning rate) could be more explored. \n\n\n4. The assumption that *predictive probabilities and sampling distributions follow a shared Dirichlet form* is strong and **not empirically validated**. Alternative formulations (e.g., logistic-normal) are briefly mentioned but not deeply compared. \nCan the authors provide empirical evidence supporting the Dirichlet assumption on predictive probabilities?\n\n\n5. Concernig the presentation:\n\n5a. The texts (such as sub-titles, x-axis, and y-axis) on Figures 2-6 are too small to distinguish, leading to poor presentation. \n\n5b. Tables 1-2 are too dense."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RqXS1VejkG", "forum": "xURArpl40L", "replyto": "xURArpl40L", "signatures": ["ICLR.cc/2026/Conference/Submission24672/Reviewer_sbHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24672/Reviewer_sbHn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553351560, "cdate": 1761553351560, "tmdate": 1762943159271, "mdate": 1762943159271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "WVYJc2WB65", "forum": "xURArpl40L", "replyto": "xURArpl40L", "signatures": ["ICLR.cc/2026/Conference/Submission24672/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24672/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762955895347, "cdate": 1762955895347, "tmdate": 1762955895347, "mdate": 1762955895347, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the calibration of prediction set size (PSS) in conformal prediction (CP), questioning whether smaller prediction sets truly correspond to higher predictive accuracy. The authors first formalize the notion of conformal calibration-a monotonic relationship between the prediction set size and expected accuracy-and propose CP-Aware Calibration (CPAC), a bi-level optimization procedure that adjusts model logits to better align the PSS-accuracy relationship with a theoretically motivated function\n$f(k)=1/k^{\\tau}$. Empirical results across image and text classification tasks (ResNet, ViT, GPT-2) show that CPAC improves calibration metrics (CP-ECE and Uniform CP-ECE) while maintaining nominal coverage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tPractical algorithm (CPAC): The proposed bi-level calibration method is simple, differentiable, and can be used as a plug-in for existing models.\n2.\tComprehensive experiments: Evaluation spans diverse architectures (ResNet, ViT, GPT-2) and domains (image and text), demonstrating robustness."}, "weaknesses": {"value": "1.\tDependence on APS score:\nAll experiments are based on the Adaptive Prediction Sets (APS) scoring function. No experiments are provided for RAPS and other score functions, which can exhibit different prediction set behaviors. This limits the generality of the proposed CPAC method.\n2.\tDirichlet assumption and lack of theoretical grounding: The derivation of $f(k)=1/k^{\\tau}$  assumes a Dirichlet distribution for softmax outputs, which is rarely realistic for modern neural networks. The sensitivity of CPAC to this assumption is not analyzed.\n3.\tUnaddressed Negative Trade-offs (Efficiency): A crucial experiment is relegated to Appendix C (Table 4), which shows results when the coverage is controlled to be 90%. This table reveals that the proposed CPAC method consistently produces larger (i.e., worse) Prediction Set Sizes (PSS) than the baseline (e.g., ViT-Base \"Clean\" PSS increases from 6.20 to 7.81)."}, "questions": {"value": "1.\tOn the $\\tau$ Parameter: Please clarify precisely how the target calibration parameter $\\tau$ is determined for (a) the CPAC algorithm's objective function (Algorithm 1) and (b) the final ECE evaluation (e.g., in Table 1). How do you justify using a test-set grid search to find the \"optimal $\\tau$\" for your evaluation metric?\n2.\tHow do you interpret the finding in Table 4  that CPAC increases the PSS when coverage is fixed? Does this not imply a fundamental trade-off between your definition of calibration and the primary goal of CP, which is to provide the smallest possible set while guaranteeing coverage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oS3Ww3ohUx", "forum": "xURArpl40L", "replyto": "xURArpl40L", "signatures": ["ICLR.cc/2026/Conference/Submission24672/Reviewer_YDJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24672/Reviewer_YDJk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744928681, "cdate": 1761744928681, "tmdate": 1762943159031, "mdate": 1762943159031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the relation between the model's accuracy and the prediction set size (PSS) of conformal prediction applied to classification (empirically, it only considers the APS method). They refer to the PSS as calibrated if it monotonically decreases with the PSS and finds that it is not always the case. Metrics and a method that improves them are proposed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The research direction is novel and interesting, but I find some issues with the current state of the paper."}, "weaknesses": {"value": "1. \nIf both calibration and CP are performed using the same calibration set, the exchangeability condition between calibration and test samples, which is required for proving Eq. (1), is not ensured.\n\n2. \nThe PSS calibration criterion (in particular the expression for accuracy) is somewhat heuristic.  \nThere is no \"true\" monotonically decreasing function f(|S|) that all users are expected to prefer.\n\n3. \nThe discussion below Eq. 9 is not clear enough.  \nExplain more the case of samples with PSS=1, and also explain what happens for samples with PSS=0, which can happen in some CP methods.  \nAt some later point in the paper, the authors state that they prevent the case of PSS=0, but this degrades the performance of CP methods, as the option of empty PS allows them to reach the desired coverage level with smaller average PSS, and empty PSS serves as an indication to the user that a sample is so hard that the model cannot suggest a candidate for it (a \"reject\" option).\n\n4. \nYou consider only the APS score.  \nYet, it is known that APS performs poorly in terms of PSS (prediction set size), and the sets even become larger after performing calibration (Xi et al., 2024), (Dabah & Tirer, 2024).  \nYou should repeat the experiments also with other popular CP scoring methods, such as RAPS and especially LAC/THR (one minus the softmax entry), which have better PSS.\n\n5. \nThe proposed calibration affects the top-1 accuracy (W,b can change the class ranking) and degrades the accuracy of the model, as shown in Table 1.\n\n6. \nWhat value of alpha do you use? (I suggest examining more than one value).  \nI assume that you used 0.1, and it is not clear why the target coverage of 1-\\alpha is not obtained more precisely for clean test data, contrary to existing works.  \nPerhaps this relates to negative effect on exchangeability since you perform both CP and calibration on the same set. \n\n7. \nRegarding your statement: \"as we only need to cover (1 − α) of all samples in CP and we choose to optimize those low-PSS samples\", I do not think that this is justified, as you do not know in which 1-\\alpha of the samples coverage holds.\n\n8. \nYou state that you use: \"PS to denote the standard confidence calibration method in APS\". However, confidence calibration has been shown to have negative effect on the PSS of APS. Thus, you should compare also to APS without confidence calibration.  \nIn addition, \"PS-Full\" is not defined.\n\n9. \nThe metric \"Uni CP-ECE\", where all the bins (PSS values) have fixed weight contrary to \"Std CP-ECE\" that matches the empirical distribution, is not well justified. Why don't you use adaptive binning?  \nThe proposed CPAC does not seem to yield significant improvements on Std CP-ECE, at least in the \"clean\" cases, which are more natural, in Table 1. (The \"noisy cases\" are less natural, as the model was trained on clean data and no domain adaptation is used.)\n\n10. \nIn Table 4 in the appendix, the coverage level is aligned with 1-\\alpha, which is the common observation for CP methods on standard benchmarks (not in a \"control experiment\"). In this case, the proposed CPAC both degrades accuracy and increases PSS compared to the baseline."}, "questions": {"value": "I stated the questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2fpeQ0vj3h", "forum": "xURArpl40L", "replyto": "xURArpl40L", "signatures": ["ICLR.cc/2026/Conference/Submission24672/Reviewer_HbKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24672/Reviewer_HbKC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866179557, "cdate": 1761866179557, "tmdate": 1762943158844, "mdate": 1762943158844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether the prediction set size (PSS) in conformal prediction (CP) truly reflects predictive uncertainty in classification tasks. While CP guarantees coverage (the true label is contained in the prediction set with probability $1-a$), it is unclear if smaller sets consistently correspond to higher correctness. The authors formalize uncertainty calibration for CP, define metrics (CP-ECE, uniform CP-ECE), and propose a theoretical target function linking PSS and expected accuracy under Dirichlet assumptions. They further introduce CP-aware Calibration (CPAC), a bi-level optimization algorithm that adjusts model logits before conformalization to improve calibration. Experiments on CIFAR-100, ImageNet (ResNet, ViT models), and topic classification with GPT-2 demonstrate improved calibration (lower CP-ECE) without degrading accuracy or coverage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a previously underexplored question - whether prediction set size in CP reflects calibrated uncertainty - bridging a gap between coverage guarantees and reliability of uncertainty estimates.\n- The proposed calibration target function $f(k)=1/k^\\tau$ is motivated by both empirical observation and derivation under Dirichlet assumptions.\n- The CPAC algorithm elegantly adapts bi-level optimization for CP calibration, analogous to Platt scaling but adapted to PSS.\n- Evaluation across vision and language models, multiple perturbations and datasets provides strong empirical support.\n- The study offers diagnostic insights (e.g., pre-trained models show weaker CP calibration; noise increases calibration errors)."}, "weaknesses": {"value": "- CPAC’s convergence and generalization are only empirically validated; no formal analysis is provided (acknowledged by authors).\n- The bi-level optimization approach may be computationally heavy and lacks sensitivity analysis or ablation on hyperparameters (e.g. $\\lambda, \\tau, t$).\n- The interpretation of the exponent parameter $\\tau$ in the target function, while intuitive, could benefit from more rigorous justification.\n- Some derivations (e.g., expected accuracy under multinomial sampling) could be made more transparent; reliance on Dirichlet assumptions might be restrictive.\n- While CPAC improves calibration, it sometimes reduces accuracy slightly and increases PSS when coverage is fixed - this trade-off deserves more discussion."}, "questions": {"value": "- How sensitive is CPAC to the choice of $\\tau$ and the sampling temperature $t$? Could $\\tau$ be learned rather than fixed via grid search?\n- Can CPAC be extended to regression or structured prediction tasks?\n- How does CPAC interact with existing conformal methods aimed at conditional coverage (e.g., Gibbs & Candes, 2023)?\n- Would end-to-end fine-tuning (rather than post-hoc calibration) yield stronger alignment between PSS and accuracy?\n- What is the computational overhead of CPAC relative to standard APS or Platt scaling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "szNpRiQ6lQ", "forum": "xURArpl40L", "replyto": "xURArpl40L", "signatures": ["ICLR.cc/2026/Conference/Submission24672/Reviewer_ivD8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24672/Reviewer_ivD8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984679581, "cdate": 1761984679581, "tmdate": 1762943158579, "mdate": 1762943158579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}