{"id": "O4f1NdXtdM", "number": 2478, "cdate": 1757110110812, "mdate": 1759898145770, "content": {"title": "FaceMoE: Mixture of Experts for Low-Resolution Face Recognition", "abstract": "Low-resolution face recognition (LR-FR) remains a challenging task due to poor\nfeature extraction and aggregation, as probe images often contain limited iden-\ntity information resulting from extreme degradations such as blur, occlusion, and\nlow contrast. Additionally, the domain gap between high-resolution (HR) gallery\nimages and low-resolution (LR) probe images poses a significant challenge. A\nsingle feature encoder struggles to generalize effectively across both domains when\nfine-tuned on an LR dataset, and this issue is further magnified by catastrophic\nforgetting. To address these challenges, we propose FaceMoE, a novel transformer-\nbased architecture enhanced with a Mixture of Experts (MoE) design. Specifically,\nwe introduce multiple specialized feed-forward network (FFN) experts and incor-\nporate a top-k router, which dynamically assigns tokens to appropriate experts.\nThis design promotes specialization across experts for different semantic regions of\nthe face, which enables FaceMoE to perform resolution-aware feature extraction.\nMoreover, the top-krouter facilitates sparse expert activation, enabling the model\nto preserve pretrained knowledge when finetuned on a LR dataset, while increasing\nmodel capacity without proportional computational overhead. FaceMoE is trained\nwith a combined face recognition loss, router z-loss, and load balancing loss to\nensure expert specialization and stable training. To the best of our knowledge, this\nis the first work leveraging MoE for LR-FR. Extensive experiments across eleven\ndatasets, spanning HR, mixed-quality, and LR benchmarks, demonstrate that Face-\nMoE significantly outperforms state-of-the-art methods, excelling in low-resolution\nface recognition. Code and models will be made public.", "tldr": "We provide a mixture of experts modification to the transformer backbone which results in SOTA performance.", "keywords": ["Low-resolution Face Recognition"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/542befe90add9e8be6110239d6ea61bd0facc226.pdf", "supplementary_material": "/attachment/ff33ea9809a368cbc7b8d8efa943f8eb72ef6a81.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces FaceMoE, a transformer-based face recognizer that inserts a mixture-of-experts MLP with a token-level top-k router into each block to specialize computation across facial regions and resolutions, stabilized by two auxiliary objectives (a router z-loss and a load-balancing loss) added to a standard CosFace recognition loss. The router learns to dispatch tokens from identity-rich landmarks, low-frequency skin regions, and high-frequency hair/background to different experts, yielding resolution-aware feature extraction while mitigating catastrophic forgetting when fine-tuning on low-resolution data. \n\nThe main contributions are the encoder-level MoE design with token-wise routing for low-resolution face recognition, the stabilization losses for reliable expert utilization, and a broad evaluation demonstrating state-of-the-art or competitive results under surveillance-style conditions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is original in placing a token-level top-k MoE inside the transformer FFN to specialize by facial region/frequency and stabilize routing with z-loss and load-balancing; this goes beyond fusion-only or sample-level MoE and is backed by region-wise routing statistics.  \n\nIn quality, experiments span BRIAR, IJB-S, TinyFace and standard HR sets, with consistent improvements and ablations that tease apart routing, expert count, auxiliary losses, backbone choice, and data scale; random-routing and large-N collapse analyses bolster the causal claim.   \n\nIn clarity, the router, losses, and training algorithm are explicitly formulated and connected to semantic regions via quantitative tables, easing reproducibility.  \n\nFinally, significance is strong for surveillance-style LR recognition: the method improves LR while preserving HR/mixed performance, uses conditional compute, and remains backbone-agnostic, suggesting practical adoption."}, "weaknesses": {"value": "Despite solid engineering, the paper leaves several gaps that are correctable, limiting confidence in its claims. The â€œreduced forgettingâ€ claim is under-substantiated; beyond trend plots, quantify selective drift via per-expert parameter change, Fisher overlap, or CKA between HR-pretrained and LR-tuned layers to show that MoE routing, rather than altered regularization, preserves HR/mixed performance.  \n\nBaselines often overlook strong, parameter-efficient alternatives: adding head-to-head comparisons under matched budgets to quality-adaptive adapters/LoRA (e.g., PETALface) and recent fusion/sparsity methods (e.g., ProxyFusion) demonstrates that encoder-level MoE is necessary, not merely sufficient, given similar data and tuning regimes.  \n\nRouting claims would benefit from stability and causality checks. This involves reporting region-wise assignment consistency across seeds and small perturbations (such as crop/blur/alignment), and intervening by freezing or swapping specific experts to test whether the observed gains truly derive from learned specialization rather than capacity."}, "questions": {"value": "1. Reduced forgetting, measured not just observed: Beyond trend plots, can you quantify selective drift across layers/experts (e.g., parameter deltas per expert, Fisher overlap, CKA between HR-pretrained and LR-finetuned representations)? This would separate a MoE-driven effect from regularization or learning-rate artifacts.\n\n2. Stronger baselines under matched budgets: Please add head-to-head comparisons with parameter-efficient baselines like quality-adaptive adapters/LoRA (e.g., PETALface) and recent fusion/sparsity approaches (e.g., ProxyFusion), controlling for pretraining data, compute, and tuning schedules. If omitted, justify why those methods are not applicable here.\n\n3. Routing stability and causality: How stable are token-to-expert assignments across seeds and small perturbations (crop jitter, alignment error, blur)? Consider interventions (freezing or swapping specific experts) to test whether gains arise from learned specialization rather than extra capacity.\n\n4. Expert semantics beyond visuals: You show regional/frequency tendencies; can you quantify whether experts capture complementary spectra (e.g., bandpass profiles) or identity-salient landmarks via controlled masking/occlusion tests? This would make â€œwhat each expert learnsâ€ more concrete.\n\n5. Hyperparameter sensitivity: Please provide sensitivity analyses for top-k, router temperature, z-loss weight, and load-balancing weight across at least two datasets. If performance is brittle, suggest default ranges and an automatic tuning heuristic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fNF3sRIkOW", "forum": "O4f1NdXtdM", "replyto": "O4f1NdXtdM", "signatures": ["ICLR.cc/2026/Conference/Submission2478/Reviewer_s6Va"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2478/Reviewer_s6Va"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761180881037, "cdate": 1761180881037, "tmdate": 1762916250221, "mdate": 1762916250221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Low-resolution face recognition (LR-FR) faces three core challenges: poor feature extraction and aggregation from degraded probe images (e.g., blur, occlusion, low contrast), significant domain gaps between high-resolution (HR) gallery and low-resolution (LR) probe images, and catastrophic forgetting when fine-tuning models on LR datasets. To address these issues, the paper proposes FaceMoE, a transformer-based architecture enhanced with a Mixture of Experts (MoE) design. The core method involves integrating multiple specialized feed-forward network (FFN) experts into transformer MLP layers and introducing a top-k router that dynamically assigns input tokens to k out of N experts based on input resolution, enabling resolution-aware feature extraction.\n- For validation, the authors conducted extensive experiments across some datasets using two protocols: Protocol 1 (WebFace4M pre-training â†’ BRIAR fine-tuning, evaluating BRIAR Protocol 3.1 and IJB-S) and Protocol 2 (WebFace4M pre-training â†’ TinyFace fine-tuning, evaluating TinyFace and HR/mixed-quality datasets). Key metrics include TAR@FAR, TPIR@FPIR, and Rank-1/5/10 retrieval accuracy.\n- The core conclusions are: 1) FaceMoE achieves state-of-the-art (SOTA) performance on LR datasets; 2) The MoEâ€™s sparse activation mitigates catastrophic forgetting, maintaining minimal performance drop on HR/mixed-quality datasets; 3) The optimal configuration (N=3 experts, k=2 active experts per token) balances model capacity (2.17Ã— increase over Swin-B) and computational cost (1.66Ã— FLOPs increase, 26.29 GFLOPs vs. Swin-Bâ€™s 15.88 GFLOPs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "> 1. As a pioneering work to leverage MoE for LR-FR, it targets the fieldâ€™s key points (domain gap, catastrophic forgetting) with a well-designed architecture. The top-k router dynamically assigns tokens to experts specialized in distinct facial semantic regions, directly solving the limitation of single FFN encoders that fail to adapt to both HR and LR domains. This design ensures resolution-aware feature extraction, critical for degraded LR images.\n> 2. The paper conducts detailed ablations on critical components (MoE module, top-k router, auxiliary losses) in Table B.5. Additional analyses (expert specialization via Figure 2/Table B.1, resolution robustness via Table B.2, data scaling via Table B.8) confirm the modelâ€™s reliability and generalizability, strengthening the credibility of design choices.\n> 3. The authors evaluated FaceMoE across diverse datasets, covering HR, mixed-quality, and LR scenarios, using two protocols that validate both LR performance and catastrophic forgetting mitigation. The results are enough for verifying the proposed method.\n> 4. The ethics statement confirms compliance with data usage rules and discusses responsible deployment to avoid misuse. The reproducibility statement details implementation settings (PyTorch, AdamW optimizer, learning rate schedules) and promises code/model release upon acceptance, ensuring experiment replicability."}, "weaknesses": {"value": "* 1. While the paper provides a bias analysis in Table B.3 (comparing SeR/DoB across age, gender, race on LFW/CFP-FF/AgeDB), it only contrasts FaceMoE with Swin-B and lacks deeper investigation into instrinsic reasons. \n* 2. The paper mentions failure cases on BRIAR (e.g., <8Ã—8 pixel crops, extreme poses) but does not investigate how the MoE architecture responds to these failuresâ€”for example, whether expert activation becomes random (e.g., occluded landmark tokens misrouted to non-landmark experts) or if certain experts are deactivated. It also lacks heatmap comparisons of expert activation between successful and failed cases (e.g., Figure 6 only shows activation before/after fine-tuning, not failure vs. success). \n* 3. mplementation details specify warm-up epochs (1 for pre-training, 2 for BRIAR fine-tuning, 2/4 for TinyFace fine-tuning) and polynomial LR scheduling but do not explain how these parameters were tuned or their impact on performance. For example, it is unclear if reducing BRIAR warm-up epochs from 2 to 1 causes gradient instability or if a cosine LR scheduler outperforms the polynomial scheduler. This hinders experiment replication for other researchers."}, "questions": {"value": "- Q1.  Why is the load balancing loss (Section 53) formulated with the product of \"sum of routing probabilities (âˆ‘p_b,t,i)\" and \"sum of indicator functions (âˆ‘ðŸ™[iâˆˆTopK(z_b,t)])\" instead of simpler balancing metrics (e.g., variance of expert loads)?\n- Q2. The final token output is a convex combination of k expert outputs, but the paper does not discuss how the model handles conflictsâ€”e.g., if Expert 0 (landmark-focused) assigns high weight to a \"nose\" feature and Expert 1 (cheek-focused) assigns high weight to a \"cheek\" feature for the same token. Additionally, it does not report whether certain experts consistently dominate outputs for specific token types (e.g., 70% of landmark token weight from Expert 0). \n- Q3. How does FaceMoE perform on images with resolutions between 8Ã—8 and 16Ã—16 pixels (e.g., 10Ã—10, 12Ã—12), and does the top-k routing hyperparameter (k) need adjustment for these ultra-low scenarios? The resolution ablation study (Table B.2) only reports results for 16Ã—16 to 96Ã—96 pixels but ignores ultra-low resolutions (8Ã—8, 12Ã—12). The paper mentions <8Ã—8 pixels as failure cases but does not address 12Ã—12/10Ã—10 pixels (a middle ground with potential utility). For example, would increasing k from 2 to 3 improve performance for 12Ã—12 pixels by leveraging more expert information? \n- Q4. Does expert-specific initialization (e.g., initializing landmark-focused Expert 0 with weights from a pre-trained facial landmark detection model) improve LR adaptation speed and final performance compared to the current global initialization (WebFace4M pre-training)? The paper initializes FaceMoE globally on WebFace4M but does not explore targeted expert initializationâ€”an approach that could accelerate semantic specialization (e.g., Expert 0 quickly focusing on landmarks). It is unclear if such initialization reduces fine-tuning epochs or improves performance on small LR datasets like TinyFace."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R9reLAbRNY", "forum": "O4f1NdXtdM", "replyto": "O4f1NdXtdM", "signatures": ["ICLR.cc/2026/Conference/Submission2478/Reviewer_hivy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2478/Reviewer_hivy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761189796436, "cdate": 1761189796436, "tmdate": 1762916250017, "mdate": 1762916250017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Facing challenges from degraded probe images and the resolution domain gap in low-resolution face recognition (LR-FR), FaceMoE is proposed which is a transformer model enhanced with Mixture of Experts (MoE). By employing specialized experts and a top-k router, FaceMoE achieves resolution-aware feature extraction while preserving pre-trained knowledge through sparse activation. This first MoE-based approach for LR-FR effectively addresses feature degradation and domain gap issues without significant computational increase."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A modified transformer encoder is proposed to use sparsely activated feed-forward network (FFN) experts. A top-k router directs tokens to specialized FFN experts, enabling resolution-aware feature extraction from distinct facial regions. This is a good attempt to apply MoE into this field."}, "weaknesses": {"value": "The manuscript describes interesting progress, but several important issues need to be addressed to strengthen its validity and impact:\n1. The mechanism by which different experts specialize in distinct facial regions requires more in-depth discussion. \n2. The complementarity between different experts is not clearly demonstrated, as the analysis reveals a high degree of redundancy in the regions they activate. A clearer distinction in their specialized roles is needed.\n3. Could the authors please justify the use of a single coefficient for the last two losses in Line 279? The rationale for this design choice is unclear.\n4. The authors should provide an ablation study to quantitatively demonstrate the contribution of each proposed module (e.g., the MoE layer, the top-k router) to the overall performance.\n5. A thorough proofreading is required to address several formatting and typographical issues. A notable example is Table 1, where the text flows outside the designated cells, which affects readability."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LEtit2vyd0", "forum": "O4f1NdXtdM", "replyto": "O4f1NdXtdM", "signatures": ["ICLR.cc/2026/Conference/Submission2478/Reviewer_jzUq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2478/Reviewer_jzUq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748692005, "cdate": 1761748692005, "tmdate": 1762916249844, "mdate": 1762916249844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Problem.** Low-resolution face recognition (LR-FR) suffers from weak, domain-mismatched probe features (LR, degraded) vs. gallery (HR), plus catastrophic forgetting when fine-tuning on LR.  \n**Idea.** Replace the single FFN in transformer blocks with a **Mixture-of-Experts (MoE) MLP**: $N$ FFN experts with **top-k** sparse routing at the token level. Auxiliary **z-loss** and **load-balancing** regularizers stabilize routing and encourage expert specialization.  \n**Mechanism.** Sparse routing increases capacity without proportional FLOPs; selective expert updates during LR fine-tuning reduce forgetting while enabling **resolution-aware** feature extraction.  \n**Evidence.** Strong improvements on LR benchmarks (BRIAR 3.1, IJB-S, TinyFace) and minimal drops on HR/mixed-quality sets; ablations over $(N,k)$ and compute trade-offs; qualitative expert activation maps.  \n**Takeaway.** First focused MoE architecture for LR-FR with convincing, state-of-the-art results and a principled training recipe."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Originality.** MoE-FFN + top-k routing tailored to resolution-dependent cues; explicit anti-collapse regularizers; selective-drift argument is well motivated.\n    \n* **Quality.** Comprehensive benchmarks (BRIAR, IJB-S, TinyFace) with relevant baselines (ProxyFusion, PETALface, CAFace, etc.); competitive pretrain/fine-tune recipe and careful hyperparams.\n    \n* **Clarity.** Method section is concrete (router, losses, algorithm); compute/ablation plots aid intuition; training schedule is reproducible.\n    \n* **Significance.** Material SOTA gains where LR matters most, and minimal HR performance driftâ€”useful for real surveillance/forensic pipelines."}, "weaknesses": {"value": "* **Statistical rigor.** Most metrics are single-number; no confidence intervals, seed variance, or bootstrap CIs; a few improvements are significant, but some margins over strong baselines would benefit from uncertainty quantification.\n    \n* **Expert â€œsemanticsâ€ not quantified.** Activation maps are qualitative; provide a measurable alignment between experts and regions/frequencies (e.g., mutual information with landmark masks, frequency energy, or SHAP on routing logits).\n    \n* **Degradation coverage.** LR comes with blur, compression, noise, occlusion, and illumination shifts; targeted stress tests (synthetic and in-the-wild subsets) are limited.\n    \n* **Efficiency on edge.** FLOPs are reported, but end-to-end latency/throughput and memory under varying batch sizes (edge/GPU constraints) are not; important for deployability claims.\n    \n* **Forgetting analysis.** The â€œselective driftâ€ story is compelling; a more direct comparison against LoRA/adapter fine-tuning and LwF-style regularization would isolate MoEâ€™s advantage."}, "questions": {"value": "1. **Statistical robustness.** Can you report meanÂ±std over â‰¥3 seeds (or 5-fold bootstrap CIs) for key metrics on BRIAR/IJB-S/TinyFace? Do the rankings hold under variance?\n    \n2. **Expert semantics.** Can you quantify specialization (e.g., Kendallâ€™s $\\tau$ between routing scores and (i) frequency bands; (ii) landmark/region masks; (iii) edge density), and report per-expert token distributions pre/post fine-tune?\n    \n3. **Ablations vs. PEFT/LwF.** How does FaceMoE compare to strong PEFT baselines (LoRA/IAÂ³/adapters) and LwF/EWC in both LR gains and HR retention, at matched FLOPs/params?\n        \n4. **Degradation stress tests.** What happens under controlled blur/noise/compression sweeps and occlusion masks (eyes/mouth/cheeks)? Does routing adapt as hypothesized?\n    \n5. **Latency/memory.** Please provide inference latency (ms), peak memory usage, and throughput (fps) for (N,k) settings on an A6000 and a resource-constrained GPU, including router overhead. \n    \n6. **Generalization.** Does TinyFace-tuned FaceMoE transfer to LR-like mobile video or body-cams without re-tuning? Any zero-shot observations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The work uses established FR datasets with licenses; however, LR-FR has clear dual-use risks. Consider adding a brief section on governance/consent for LR deployments, as well as demographic bias checks under LR conditions."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LQI7o36ngC", "forum": "O4f1NdXtdM", "replyto": "O4f1NdXtdM", "signatures": ["ICLR.cc/2026/Conference/Submission2478/Reviewer_Vhz8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2478/Reviewer_Vhz8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940091874, "cdate": 1761940091874, "tmdate": 1762916249626, "mdate": 1762916249626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}