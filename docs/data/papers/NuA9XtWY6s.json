{"id": "NuA9XtWY6s", "number": 16198, "cdate": 1758261422624, "mdate": 1759897255013, "content": {"title": "Retrieval as Reasoning: Learning to Select and Generate with LLMs", "abstract": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) has become a practical solution for addressing hallucination in large language models (LLMs) by conditioning responses on retrieved documents. However, existing RAG systems face two major limitations: (1) retrieval objectives are often misaligned with the downstream generation task, leading to irrelevant documents harmful to the generation; (2) concatenating many retrieved documents into long prompts strains model capacity and introduces positional biases that degrade performance. To overcome these issues, we propose a unified framework where the LLM itself learns to perform document selection and answer generation in an end-to-end manner. Inspired by human reasoning, our model organizes documents via hierarchical semantic IDs and selects relevant content through a self-reflection mechanism composed of query-specific attention and an additional feed-forward MLP layer. This architecture enables the model to promote helpful documents directly during generation, eliminating the need for separate retrievers or rerankers. Through joint training, the model learns to select the most informative 2-3 documents. We conduct experiments to validate the effectiveness of our design.", "tldr": "", "keywords": ["Retrieval and generation", "joint training", "hierarchical semantic ids"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20126f00ba1547468c935a6cb1807cdca90932bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an end-to-end retrieval-augmented generation framework that addresses key limitations of traditional RAG systems—misaligned retrieval and long-prompt inefficiency—by enabling the LLM to learn document selection and generation jointly. Inspired by human reasoning, the model uses hierarchical semantic IDs and a self-reflection mechanism to dynamically identify and prioritize relevant documents during generation, eliminating external retrievers or rerankers. Experiments show the model effectively selects 2–3 informative documents, improving efficiency and performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is easy to follow\n* Trying to remove the reranker is interesting"}, "weaknesses": {"value": "* The novelty is limited, the authors seem only try use generative retriever instead of vanilla retriever, but this has already been well studied\n* In the experiments, the authors compare their method with vanilla RAG, but their method uses a trained generator, while vanilla RAG simply use the original model, this seems to be an unfair comparision\n* The authors primarily focus on the model training process; however, in my view, the most intriguing aspect lies in how the document tree structure is organized and how the docIDs are generated. But, the paper simply use Bert and K means to conduct clustering, which is trivial."}, "questions": {"value": "* How is the semantic ID organized, why it can represent the position within a tree of semantic clusters\n* The authors claim that rerankers are not needed with their unified model, but it seems that the model can only act as retriever and generator, an extra reranker should still help the performance.\n* I want to know the performance of the trained generative retriever, is it better than vanilla retrievers like BM25 and contriever"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pleZfhlwKU", "forum": "NuA9XtWY6s", "replyto": "NuA9XtWY6s", "signatures": ["ICLR.cc/2026/Conference/Submission16198/Reviewer_vEmo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16198/Reviewer_vEmo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534067428, "cdate": 1761534067428, "tmdate": 1762926360263, "mdate": 1762926360263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework that addresses the misalignment between retrieval objectives and generation tasks, and mitigates the issues caused by long prompts, which strain model capacity and introduce positional biases. Specifically, this paper enables the LLM to jointly learn document selection and answer generation, and further guides it to select the 2-3 most informative documents to address the limitations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper uses LLM as retriever to addresses the misalignment between retrieval objectives and generation tasks.\n\n2. The tree of semantic clusters leverages coarse-to-fine semantic relationships to facilitate document selection."}, "weaknesses": {"value": "1. There are many works for addressing the misalignment between retrieval objectives and generation tasks. For example, R2AG [1] addresses the semantic gap that exists between LLMs and retrievers. The authors should take them into related work or experiments.\n\n2. There are many works that aim to solve the lost-in-the-middle problem in RAG, such as PAM QA [2]. The paper’s claim that selecting only the two to three most informative documents alleviates capacity limits and positional bias is insufficiently substantiated. When multiple top-k documents are relevant to the generation task, the model should attend to all of them rather than truncating the context to just 2–3. \n\n3. There is a lack of sufficient baselines.  This paper's baselines only include No RAG and Vanilla RAG. It is beneficial to include more related baselines, such as R2AG[1], Self-RAG[3] and Adaptive-RAG[4].\n\n4. No ablation studies are provided to quantify the individual contributions of hierarchical docIDs, self-reflection mechanism, or cross-attention components.\n\n5. The figures are unclear and thus confusing. Figure 1 fails to distinguish the proposed method from traditional approaches. In Figure 2, although the Transformer block is intended to clarify the LLM’s retrieval and generation logic, the directions of the arrows are ambiguous.\n\n[1] R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation, EMNLP 2024.\n\n[2] Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training, ACL 2024.\n\n[3] Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection, ICLR 2024.\n\n[4] Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity, NAACL 2024."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FWVZQgsemM", "forum": "NuA9XtWY6s", "replyto": "NuA9XtWY6s", "signatures": ["ICLR.cc/2026/Conference/Submission16198/Reviewer_2e8q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16198/Reviewer_2e8q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637114960, "cdate": 1761637114960, "tmdate": 1762926359075, "mdate": 1762926359075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address key limitations in Retrieval-Augmented Generation (RAG), namely the misalignment between retrieval and generation objectives, and the difficulty models face when processing long input contexts. The authors propose a unified framework where a single Large Language Model (LLM) performs both document selection and answer generation.\n\nThe approach utilizes Generative Retrieval (GR), specifically adopting the hierarchical semantic document identifiers (DocIDs) proposed by Wang et al. (2022c). The core technical contribution involves modifying the standard Transformer architecture to include a \"self-reflection mechanism.\" This mechanism introduces a parallel pathway dedicated to retrieval, consisting of specialized query projections (Q') and an independent MLP layer. This retrieval pathway shares the Key (K) and Value (V) representations with the generation pathway. The outputs of the two pathways are combined via a residual connection and an optional cross-attention mechanism.\n\nThe model is trained using two objectives: Mode 1 (Eq 4.5), a supervised loss on generating ground-truth DocIDs, and Mode 2 (Eq 4.6), a standard QA loss conditioned on documents retrieved by the model. The authors claim this enables end-to-end joint training, aligning retrieval with the downstream task. Experiments on NQ and TriviaQA using Llama-3.1-8B show improvements over No-RAG and Vanilla RAG (using DPR) baselines, and mixed results on Qwen3-4B."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Well-Motivated Problem: The paper identifies the misalignment between independent retrieval and generation modules as a significant challenge in RAG. The goal of unifying these components into a single, jointly optimized model is a compelling research direction.\n- Interesting Architectural Design: The proposed architectural modification (Section 4.2, Figure 2) is interesting. Introducing specialized Q' projections and MLPs for retrieval while reusing the K/V representations from the generation stream is a plausible method for enabling multi-task capabilities within a single decoder architecture."}, "weaknesses": {"value": "- Joint Training Mode 2 (Eq 4.6) Seems Flawed: Eq 4.6 does not learn the retrieval component, but assumes that it could leverage one trained from Mode 1. This assumption might not hold. Even if there is some supervision, there could be domain difference between labeled and unlabeled data. To properly address this problem, some sort of RL / Gumbel relaxation may be needed.\n- Missing Generative Retrieval Comparison: As the proposed method is an integration of GR into the generator, a crucial baseline is missing: a standard, standalone GR model (_e.g._, fine-tuning the base LLM to generate DocIDs without the architectural modifications) used as a retriever for the same generator LLM. Without this baseline, it is impossible to determine if the gains come from using GR or from the proposed architectural unification.\n- Misleading Framing (\"Reasoning\"): The title \"Retrieval as Reasoning\" is not supported by the methodology. The paper describes the approach as inspired by human hierarchical organization of information (L65-74). However, the technical implementation is a mechanism for hierarchical selection via generative retrieval. It does not involve explicit multi-step inference, logical deduction, or iterative refinement typically associated with \"reasoning\" in the context of LLMs.\n- Mixed Results Suggest Expanded Evaluation is Needed: No statistical significance tests are performed. In Table 1, the results for Qwen3-4B on TriviaQA show that Vanilla RAG (47.4) performs significantly worse than No RAG (57.5). The proposed method (47.9) also underperforms the No RAG baseline in this configuration. A more extensive evaluation will help clarify their relative performance."}, "questions": {"value": "- Can you provide ablation studies to isolate the impact of the cross-attention mechanism (Section 4.3) and the dual-stream architecture (Section 4.2)?\n- How does the model compare to (a) a RAG pipeline using a state-of-the-art dense retriever and reranker, and (b) a standard Generative Retrieval approach using the same base LLM without architectural modifications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H6FCZR2SPT", "forum": "NuA9XtWY6s", "replyto": "NuA9XtWY6s", "signatures": ["ICLR.cc/2026/Conference/Submission16198/Reviewer_3zTV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16198/Reviewer_3zTV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861784345, "cdate": 1761861784345, "tmdate": 1762926358395, "mdate": 1762926358395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an end-to-end alternative to standard RAG that lets the LLM itself select evidence and generate answers, removing separate retriever and reranker components. It identifies two primary limitations in standard RAG: 1) the misalignment between retrieval objectives and the actual needs of the downstream generation task, and 2) the performance issues arising from processing long contexts (e.g., computational cost and positional bias). To address this, the authors propose a unified, end-to-end framework where the LLM itself learns to perform both document selection and answer generation, thereby replacing external retrievers and rerankers. The core of their method involves organizing documents with hierarchical semantic IDs and integrating a lightweight \"self-reflection mechanism\" into the LLM. Experiments (briefly described) indicate improved efficiency and accuracy over traditional RAG pipelines under this setting."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The challenge and problem addressed by this paper is interesting and important.\n2. The paper is generally easy to understand."}, "weaknesses": {"value": "This paper seems to be a draft that only basically proves the feasibility of the idea. Several problems should be fixed before being qualified as a top-tier conference paper:\n\n1. Severe lack of experiments: The authors only use Table 1, which consists of two models, two simple qa datasets, and no baselines from other research papers. While the result is better than vanilla RAG, it is far from SOTA in the same setting, according to my knowledge. Besides, since the paper claims the motivations on long documents and reasoning, it should be tested on more complex benchmarks rather than only NQ and TriviaQA.\n2. Lack of novelty and references: This idea for unifying retrieval and generation is interesting and worth exploring. However, there are already preceding papers working along this line. For example, GritLM, OneRec/OneRec-Think, UniGen etc. The paper should refer to these works and carefully discriminate between its own contributions. \n3. Lack of demonstrations: The figures are too coarse and do not show the importance of key contributions. No case studies can help the readers understand how the model unifies retrieval and generation."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Es38952XkU", "forum": "NuA9XtWY6s", "replyto": "NuA9XtWY6s", "signatures": ["ICLR.cc/2026/Conference/Submission16198/Reviewer_ERwf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16198/Reviewer_ERwf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897199788, "cdate": 1761897199788, "tmdate": 1762926357759, "mdate": 1762926357759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}