{"id": "jrSc4RJXy1", "number": 8078, "cdate": 1758058891216, "mdate": 1759897809760, "content": {"title": "CaTS: Calibrated Test-Time Scaling for Efficient LLM Inference", "abstract": "Increasing test-time computation is a straightforward approach to enhancing the quality of responses in Large Language Models (LLMs). While Best-of-N sampling and Self-Consistency with majority voting are simple and effective, they require a fixed number of sampling responses for each query, regardless of its complexity. This could result in wasted computation for simpler questions and insufficient exploration for more challenging ones. In this work, we argue that model confidence of responses can be used for improving the efficiency of test-time scaling. Unfortunately, LLMs are known to be overconfident and provide unreliable confidence estimation. To address this limitation, we introduce Self-Calibration by distilling Self-Consistency-derived confidence into the model itself. This enables reliable confidence estimation at test time with one forward pass. We then design Calibrated Test-Time Scaling (CaTS), adapting common repeated sampling methods, such as self-consistency and Best-of-N to handle queries of various difficulty. We also show that CaTS-SC is provably better than vanilla self-consistency. Experiments on three LLMs across nine datasets demonstrate the effectiveness of our approach. Specifically, applying confidence-based Early Stopping (CaTS-ES) to Best-of-N improves MathQA accuracy from 73.7 to 83.6 with a sample budget of 16 responses, demonstrating the effectiveness of the confidence-based sampling strategy at inference time.", "tldr": "We propose Self-Calibration, a new unsupervised framework to help model calibrate the confidence and using the confidence to efficiently test time scaling.", "keywords": ["Test-time Scaling", "Model Calibration", "Efficient inference", "Language Modeling", "Scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86791f0a9b385139a993aebfe5da5bccb88b1b85.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Calibrated Test-Time Scaling (CaTS), a framework that leverages confidence estimation for adaptively scaling the amount of computation devoted to each test-time query in large language models. The central idea is to distill Self-Consistency-derived confidence into an LLM via a self-calibration procedure, enabling reliable confidence estimation in a single forward pass. CaTS proposes variants of common repeated sampling methods (Best-of-N, Self-Consistency, Adaptive Self-Consistency) that use these calibrated confidence scores to guide early stopping or assign voting weights. The paper presents a mathematical analysis demonstrating conditions under which the confidence-weighted approach provably outperforms vanilla self-consistency. Experiments on three open LLMs across nine datasets support the claims, showing improvements in accuracy per compute over baselines, plus ablations and robustness checks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Principled Confidence Signal: The proposed self-calibration framework uses a pseudo-labeled dataset, and the design of the soft self-consistency (SSC) score is both intuitive and empirically strong.\nTheoretical Justification: The mathematical analysis provides a precise and nontrivial error bound.\nComprehensive Experiments: Numerous results tables/figures provide strong empirical support.\nComputational Efficiency: Results on token usage and comparisons with reward model-based alternatives show that CaTS substantially reduces inference cost without sacrificing performance."}, "weaknesses": {"value": "1. Clarity and Terminology Issues\n- \"the model’s calibration\" and \"P(True)\" are not properly introduced. Also, \"P(True)\" and \"P(Yes)\" are used interchangeably.\n- Table 1 caption misses SVAMP.\n- Table 10 is not linked\n2. \"For dynamic methods such as CaTS-ES and CaTS-ASC, we calibrate their threshold for each dataset so that the actual number of samples collected in practice closely matches, but slightly under, the target budget.\" This seems heuristic; there might be a better way to measure the performance of dynamic methods\n3. The paper does not propose new early stopping or dynamic sampling techniques, nor does it introduce a novel threshold-selection strategy. Thus, some claims in the intro are unjustified.\n4. arxiv.org/abs/2502.18581 seems to be similar work, should be included as a baseline for empirical comparison."}, "questions": {"value": "1. What is CISC in Table 2?\n2. In Table 2, CaTS-SC and CaTS-ES results are almost the same. Is there an explanation?\n3. Figure 4(d) does not appear to behave as expected. Is there an explanation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5oJxtS7zR6", "forum": "jrSc4RJXy1", "replyto": "jrSc4RJXy1", "signatures": ["ICLR.cc/2026/Conference/Submission8078/Reviewer_ZXsN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8078/Reviewer_ZXsN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445860051, "cdate": 1761445860051, "tmdate": 1762920068528, "mdate": 1762920068528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to achieve more efficient test time scaling by incorporating better calibrated confidence estimation per sample. They do this by training the model via self-calibration to estimate its own confidence more reliably. Then, using this self-calibrated model they explore two strategies for test-time scaling: (1) early stopping in BON when the confidence estimate crosses some threshold, and (2) self-consistency and adaptive self-consistency weighted by confidence scores. A variety of experiments and ablations show that this method works, improves accuracy and is generally more efficient compared to baselines."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is a good paper. It was fun to read, the proposed method is very interesting / innovative and the breadth of experiments and ablations support the findings nicely.  \nThe self-calibration training framework seems to be new and a very clean approach in terms of computing soft self-consistency scores as the targets."}, "weaknesses": {"value": "The tasks and domains are restricted to reasoning, so it’s a bit hard to reason about how generally useful this approach can be, especially in terms of the reliability of confidence estimation. But reasoning is a pretty good application to start with to demonstrate the viability of this approach.  \nNot a fan of ECE since it is possible to choose hyperparameters (e.g. number of bins, style of binning) to hide miscalibration issues, but the paper does report AUC as well."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WJETvzYSDf", "forum": "jrSc4RJXy1", "replyto": "jrSc4RJXy1", "signatures": ["ICLR.cc/2026/Conference/Submission8078/Reviewer_C3JX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8078/Reviewer_C3JX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944645446, "cdate": 1761944645446, "tmdate": 1762920068098, "mdate": 1762920068098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CaTS proposes a calibrated test-time scaling scheme for LLM reasoning: first Self-Calibration distills a soft self-consistency confidence (SSC) into the model so it can predict correctness in one forward pass. With this calibrated confidence, they introduce CaTS-ES (early-stop Best-of-N) and CaTS-SC (confidence-weighted self-consistency) as drop-in inference policies.  On 3 LLMs/9 datasets it lifts MathQA 73.7 to 83.6 at N=16, matches reward-model selection without a second pass, and can reach the same SC accuracy with up to 94.2% fewer samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problems the paper addresses are important and timely , both how to incorporate confidence signals into LLM training and how to improve the efficiency of test-time scaling. \n2. The proposed soft self-consistency confidence is simple yet effective, and the empirical improvements are both significant and consistent across models and datasets. \n3. The paper is also clearly presented, with a coherent motivation and narrative that make it easy to follow."}, "weaknesses": {"value": "1.\t**Objective overlaps with reward modeling.**\nThe training target effectively functions like a reward-model score that measures response quality. Given that the method aims to predict P(\\text{correct}) for early stopping and confidence-weighted voting, a natural question arises: what happens if the model is trained directly on binary correctness (i.e., replacing c_j in the training objective with the true correctness label)? Since the training data already include ground-truth answers, it would be valuable to add an ablation that supervises the model with 0/1 correctness and compares it against the self-induced confidence signal. This would reveal whether the proposed self-generated confidence provides any genuine advantage, or suffers drawbacks relative to using correctness as the target.\n2. **Off-policy pseudo-label drift.**\nSSC targets are computed once from the base model’s samples and kept fixed during fine-tuning. As the generator shifts, these pseudo-labels can mismatch the updated distribution and potentially entrench confidently wrong majorities. It would be helpful to analyze sensitivity to this self-distillation bias—e.g., iterative re-labeling with the calibrated model (one extra SSC pass), checkpoint-wise calibration drift (ECE over training)."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4goiioxo8A", "forum": "jrSc4RJXy1", "replyto": "jrSc4RJXy1", "signatures": ["ICLR.cc/2026/Conference/Submission8078/Reviewer_fEfu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8078/Reviewer_fEfu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959443609, "cdate": 1761959443609, "tmdate": 1762920067669, "mdate": 1762920067669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a test-time scaling method for LLMs, CaTS, integrating self-calibrated confidence scores into common repeated sampling methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Proposes an efficient test-time sampling method that uses self-calibrated confidence scores for dynamic repeated sampling to improve LLM performance.\n* Strong performance improvements across benchmarks and LLM families.\n* Intuitive design of self-calibration approach not requiring ground-truth supervision during training and a single forward pass during inference.\n* The authors provide a theoretical proof of how and when CaTS is better than vanilla self-consistency\n* The methodology is clearly presented and well written with descriptive figures.\n* The authors show calibrated confidence scores compare well with reward model approaches."}, "weaknesses": {"value": "* Show results on challenging reasoning benchmarks used for reasoning LLM evaluation: More challenging and conventional reasoning benchmarks should be reported on, including MATH-500, AIME, GPQA-diamond, MMLU, etc, since the title focuses on efficient LLM “reasoning”. The benchmarks used might not be challenging for reasoning LLMs, and the transfer of test-time scaling results to harder questions should be investigated.\n* Show results with reasoning LLMs: Apart from DeepSeek-R1-Distill-1.5B, the other models Llama-8B-3.1-Instruct and Qwen2.5-7B-Instruct are not reasoning LLMs. Since the title focuses on efficient LLM “reasoning”, results with reasoning LLMs, including Qwen3, and GPT-OSS should be included.\n* Variance in performance across multiple runs: Reasoning LLM performance has variance across runs (e.g., on AIME). Deep Think with Confidence, Fu et. al. report performance averaged over 64 independent runs. Is the performance in CaTS reported over multiple runs? How low/high is the variance?\n* The OOD results shown are not out-of-domain as such. For example, ARC Easy is present in training, and  ARC Challenge in test. While these results show some generalization, results on test benchmarks from different domains (e.g., GPQA in test) would be better evidence.\n* Consider comparing to more recent work: Test-time scaling (TTS) is an active area of research with recent work also leveraging an LLM’s internal confidence signals without requiring external supervision (see Deep Think with Confidence, Fu et. al., and Thought calibration: Efficient and confident test-time scaling, Wu et. al., and Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence, Ghasemabadi et. al.). Further, creating a surrogate training dataset for confidence calibration has been explored in this prior work (Confidence Calibration of Classifiers with Many Classes, Coz et. al.), although not applied to LLMs. The authors should compare their novelty against these methods, and potentially include some of the recent work as baselines."}, "questions": {"value": "* How sensitive is the performance to hyperparameter choices: weighting coefficient w in training objective, threshold in CATS-ES, sampling budget, etc. How easy is it for practitioners to use CaTS? Do default hyperparameters work well across LLMs and benchmarks?\n* Only applicable for white box models. Are there variants of this method for black box reasoning models?\n* How are the hyperparameters chosen: weighting coefficient w in the training objective, threshold in CATS-ES?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4w49mnpKFW", "forum": "jrSc4RJXy1", "replyto": "jrSc4RJXy1", "signatures": ["ICLR.cc/2026/Conference/Submission8078/Reviewer_jmso"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8078/Reviewer_jmso"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046207330, "cdate": 1762046207330, "tmdate": 1762920067266, "mdate": 1762920067266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}