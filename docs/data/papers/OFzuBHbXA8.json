{"id": "OFzuBHbXA8", "number": 17811, "cdate": 1758280812736, "mdate": 1763015566948, "content": {"title": "OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation", "abstract": "Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate **OmniLayout-1M**, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce **OmniLayout-LLM**, a 0.5B model with a designed two-stage *Coarse-to-Fine learning paradigm*: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to a specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M^6^Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released.", "tldr": "With OmniLayout-1M and LLM-based coarse-to-fine learning, we enable universal and diverse document layout generation.", "keywords": ["Document AI", "Document Layout Generation", "Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/440b7a364f55cec6887f91cd6b1a64d65182bad3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper considers the problem of generating realistic document layouts of various complex types: e.g., newspapers, magazines, among others. Synthetically-generated layouts have been shown useful in the literature (not in this paper) to boost the performance of modern deep learning based algorithms for document analysis, and thus the generative problem has received a lot of attention in recent years.\n\nThe current paper makes two main contributions:\n1. Curating the first million-scale dataset of document layouts (of six types). In comparison, existing datasets are smaller by at least an order of magnitude.\n2. Devising an LLM-based method to generate document layouts in a gradual, coarse-to-fine manner. Experiments show that the new method produces considerably more realistic layouts (as measured by a number of geometric metrics) than previous speficially tailored methods for this problem; and perhaps slightly more realistic (as evaluated by my human eye, and by considering the numeric performance ) compared to general-purpose LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The curation of a large-scale dataset of document layouts is important for the document analysis community.\n\n- The outputs of the new method seem visually realistic and achieve competitive performance against previous methods, including powerful general-purpose LLMs. \n\n- The paper is well-written. Experiments are (to me) relatively adequate."}, "weaknesses": {"value": "The main weakness is that, unfortunately, the contribution can become deprecated pretty fast given the good performance of general-purpose LLMs:\n- The performance of the new method compared to general-purpose LLMs is not clearly much better. Given how good Claude Sonnet-3.7 performs on this task, for example, it seems realistic that the next generation would be at least as good, if not better than, the proposed method.\n\n- The importance of large, human-curated datasets may be decreasing with time, given the huge amount of diverse data that modern general-purpose LLMs train on.\n\n(Unrelated to the two above:)\nWhy are 5-shot results, which to me are perhaps most interesting, not included in the comparison in the main paper, only in the rebuttal?"}, "questions": {"value": "- Can you explain why you think this work will not become deprecated quickly in the presence of general-purpose LLMs?\n\n- See question about 5-shot results in \"weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ASVRXdEIVS", "forum": "OFzuBHbXA8", "replyto": "OFzuBHbXA8", "signatures": ["ICLR.cc/2026/Conference/Submission17811/Reviewer_Wx4x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17811/Reviewer_Wx4x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760514385083, "cdate": 1760514385083, "tmdate": 1762927655243, "mdate": 1762927655243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "f7F45Dgw7N", "forum": "OFzuBHbXA8", "replyto": "OFzuBHbXA8", "signatures": ["ICLR.cc/2026/Conference/Submission17811/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17811/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763015565640, "cdate": 1763015565640, "tmdate": 1763015565640, "mdate": 1763015565640, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the document layout generation task. A large-scale dataset with 6 common document types, i.e., OmniLayout-1M, is proposed from multiple sources. In addition, a two-stage Coarse-to-fine learning paradigm is proposed with the 0.5B OmniLayout-LLM. Experiments show the effectiveness of the proposed method for document layout generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed coarse-to-fine paradigm is a reasonable solution for improving the performance of layout generation in a specific document domain. The constructed OmniLayout-1M dataset that contains several common document types and corresponding layout annotations could be useful for future research in the community."}, "weaknesses": {"value": "1. The contribution of the proposed dataset is relatively limited. Many existing graphic design layout generation works also use the public document layout datasets like PubLayNet. As shown in Table 1, the only advantage of the proposed dataset is the scale, rather than the layout type. But the advantage of increasing the volume to 1M has not been fully demonstrated yet.  \n2. I cannot agree with the statement that the proposed method is the first to extend document layout generation to complex and challenging domains. ContentGAN [1], which was neither cited nor discussed, was the first to model complex and challenging document types like fashion magazines and newspapers.\n3. The effectiveness of the proposed OmniLayout-LLM with a coarse-to-fine learning paradigm is not very convincing. Based on the results in Table 4, the performance of the coarse-grained learning paradigm is much worse than the fine-grained paradigm. Note that most of the performance improvement comes from the fine-grained paradigm with existing specific datasets, rather than the coarse-grained paradigm with the proposed datasets. \n4. The effectiveness of the two stages should be evaluated on different model sizes, including 0.5B, 1.5B, and 3B.\n\n[1] Zheng, Xinru, et al. \"Content-aware generative modeling of graphic design layouts.\" SIGGRAPH 2019."}, "questions": {"value": "1. The annotations are obtained in an automatic manner by employing MinerU. How to avoid annotation errors or noises during the construction process?\n2. Since the datasets are collected from sources on the Internet, I was wondering whether the proposed dataset could be fully released without copyright issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gTxe6VNJBM", "forum": "OFzuBHbXA8", "replyto": "OFzuBHbXA8", "signatures": ["ICLR.cc/2026/Conference/Submission17811/Reviewer_U18h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17811/Reviewer_U18h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813766123, "cdate": 1761813766123, "tmdate": 1762927654749, "mdate": 1762927654749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies automatic document layout generation. Given the limitations of the existing datasets, the authors curate OmniLayout-1M, a large-scale dataset covering 6 document types from multiple sources. An automatic annotation process is proposed to obtain the element sequence. Based on the dataset, they fine-tune a 0.5B parameter LLM, resulting in OmniLayout-LLM. Experiments demonstrate superior performance of OmniLayout over existing layout generation methods and general-purpose LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally well-written with clear motivation and methodology. It contributes OmniLayout-1M, which addresses a real need for diverse, large-scale layout data with automated annotation pipeline. The experiments are well-designed, covering 5 document types, 5 layout generation tasks, and different baseline models."}, "weaknesses": {"value": "- During the dataset annotation, the authors employ MinerU to parse the elements from PDF files. Is it good enough to obtain high-quality labels for OmniLayout-1M? The performance of MinerU would largely determine the dataset quality. For example, if MinerU produces element boxes that severely deviate from the GT boxes, the resulting layouts may have potential issues, such as undesirable element overlap. Therefore, it is necessary to perform quality analysis on the dataset.\n- The qualitative results of LayoutPrompter are missing. LayoutPrompter exhibits better evaluation metrics than other baselines, as shown in Table 2. It is necessary to include its qualitative results in Figure 4.\n- The ablation studies are confusing in Table 4. For example, the 3B parameter model has the worst FID score compared to the smaller ones in the \"C->S+P\" task, which contradicts the general understanding of the scaling law. The results are also observed in the \"C+S->P\" and \"Refinement\" tasks. Furthermore, the FID value of the 3B parameter model for the refinement task is 67.24, significantly exceeding that of all other parameter sizes and task settings. However, from what I understand, the refinement task is relatively simple among the five tasks since it has the most conditions. Is this purely due to FID variance on small test sets, or are there overfitting issues?\n- Some implementation details are not included. For example, how to perform deduplication and data filtering during data preprocessing?\n- Ablation studies lack qualitative results comparison.\n- The title of the paper claims \"universal document layout generation\". Have the authors tested on document types outside the six categories? According to the ablation results in Table 4, fine-grained learning is more important than coarse-grained learning, which indicates that the model's performance heavily depends on the fine-grained annotations for the target domain. This fundamentally contradicts the \"universal\" claim.\n- How does performance degrade with very long sequences?\n- The authors construct the coarse-grained data across five tasks with a ratio of 1:1:1:3:3. Are there any ablation studies on the mix ratio?"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Au1yixwzVh", "forum": "OFzuBHbXA8", "replyto": "OFzuBHbXA8", "signatures": ["ICLR.cc/2026/Conference/Submission17811/Reviewer_F2dx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17811/Reviewer_F2dx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836903171, "cdate": 1761836903171, "tmdate": 1762927653995, "mdate": 1762927653995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on document layout generation.\n\nThe major contributions of the paper include: 1) a large-scale dataset (OmniLayout-1M) containing about one million layouts from diverse document domains; 2) an LLM-based layout generation framework (OmniLayout-LLM) with a two-stage learning scheme, where the LLM first learns universal layout knowledge from large-scale data with coarse-grained labels, and then is adapted to generate layouts of a specific domain using small-scale data with fine-grained labels."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Data scarcity in layout generation is an important problem to address.\n\n2. The contributed large and diverse layout dataset could be valuable to the layout generation field.\n\n3. The proposed two-stage learning scheme is shown to be effective, and the performance improvements of the proposed method over existing methods are noticeable."}, "weaknesses": {"value": "1. The accuracy of layout annotations on OmniLayout-1M is questionable. Since the annotations were obtained automatically using existing models, without being manually checked by human subjects, it is unclear whether OmniLayout-1M has reasonable annotation quality, particularly for documents with a large number of elements and highly complex layouts. It would be better to see how good the layout annotations are through some quantitative scores. For example, it is possible to manually label a small subset of documents for each document domain, and compute some metric scores of the automatically annotated layouts against the ground truth ones.\n\n2. The design of the layout representation introduced in Section 3.2 is evaluated. Unlike most existing LLM-based layout generation works, such as LayoutPrompter, this paper chooses to represent layouts as plain sequences, instead of in a HTML format that pretrained LLMs are familiar with. This may not well leverage prior knowledge in the LLMs. An experiment comparing different layout representations is missing in the paper, and should be added . \n\n3. References are incomplete. Some existing layout datasets for specific domains, such as mobile UI (RICO) and poster (CGL, PKU), are not discussed in the paper.\n     -  RICO: Learning Design Semantics for Mobile Apps\n     -  CGL: Composition-aware Graphic Layout GAN for Visual-textual Presentation Designs\n     -  PKU: PosterLayout: A New Benchmark and Approach for Content-aware Visual-Textual Presentation Layout"}, "questions": {"value": "What is the number of element categories on OmniLayout-1M?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "njbmz024dz", "forum": "OFzuBHbXA8", "replyto": "OFzuBHbXA8", "signatures": ["ICLR.cc/2026/Conference/Submission17811/Reviewer_TSio"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17811/Reviewer_TSio"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935922824, "cdate": 1761935922824, "tmdate": 1762927653347, "mdate": 1762927653347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}