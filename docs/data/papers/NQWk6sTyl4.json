{"id": "NQWk6sTyl4", "number": 11883, "cdate": 1758204472373, "mdate": 1759897548863, "content": {"title": "MobileA3gent: Training Mobile GUI Agents Using Decentralized Self-Sourced Data from Diverse Users", "abstract": "The advancement of mobile GUI agents has opened new opportunities for automating tasks on mobile devices. Training these agents requires large-scale high-quality data, which is prohibitively expensive when relying on human labor. Given the vast population of global mobile phone users, if automated data collection from them becomes feasible, the resulting data volume and the subsequently trained mobile agents could reach unprecedented levels. Nevertheless, two major challenges arise: (1) extracting user instructions without human intervention and (2) utilizing distributed user data while preserving privacy.\n\nTo tackle these challenges, we propose MobileA3gent, a collaborative framework that trains mobile GUI Agents using decentralized self-sourced data from diverse users. The framework comprises two components, each targeting a specific challenge: (1) Auto-Annotation, which enables the automatic collection of high-quality datasets during users' routine phone usage with minimal cost. (2) FedVLM-A, which enhances federated VLM training under non-IID distributions by incorporating adapted global aggregation based on both episode-level and step-level variability. Extensive experiments prove that MobileA3gent achieves superior performance over traditional approaches at only 1% of the cost, highlighting its potential for real-world applications. Our code is publicly available at: https://anonymous.4open.science/r/MobileA3gent-Anonymous.", "tldr": "", "keywords": ["Mobile Agent", "GUI Agent", "Vision Language Model", "Large Language Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/707fb14e09ae0c11ed3c3be93f1c160e09bad662.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a federated learning framework for collaboratively training mobile GUI agents using clients’ self-sourced data. It proposes an auto-annotation mechanism to enable the automatic collection of training datasets during users' routine phone usage, and a federated VLM training mechanism to enable privacy-preserving collaborative agent training on distributed user data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\t**Well-organized and clearly written:** The paper is well-structured and easy to follow, with clear motivation and methodology presentation.\n2.\t**Practical and meaningful research problems:** The work focuses on automatic data annotation and collaborative agent evolution—two significant and practically relevant challenges for building intelligent mobile systems.\n3.\t**Extensive experiments:** The paper provides relatively comprehensive experiments."}, "weaknesses": {"value": "1.\t**Practical feasibility concerns:** Although the proposed framework is conceptually appealing, it may face challenges in real-world deployment. Performing both annotation and training on mobile devices could incur high computational and energy overhead. It is unclear whether such resource consumption is acceptable on mobile devices, nor its potential impact on other on-device applications’ responsiveness.\n2.\t**Methodology:** The proposed approach is relatively simple and appears more inspirational than technically rigorous. Key technical aspects are not deeply explored.\n3.\t**Noisy annotations:** It is unclear how the framework handles low-quality training samples generated by the proposed auto-annotation mechanism to ensure the robustness and stability of the learned model.\n4.\t**Limited real-world evaluation:** The evaluation setup relies on manually partitioned datasets to simulate mobile devices, which limits its realism. Moreover, some experiments are conducted under IID settings, making the scenario even less representative of real-world federated environments. The client scale is also relatively small."}, "questions": {"value": "1.\tHow feasible is it for mobile devices to handle both auto-annotation and local training in terms of computational cost, battery consumption, and latency?\n2.\tCould the authors provide more realistic or large-scale experiments to better reflect real-world mobile federated scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P46DtdecQ8", "forum": "NQWk6sTyl4", "replyto": "NQWk6sTyl4", "signatures": ["ICLR.cc/2026/Conference/Submission11883/Reviewer_QgZJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11883/Reviewer_QgZJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654502254, "cdate": 1761654502254, "tmdate": 1762922898804, "mdate": 1762922898804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes MobileA3gent, a framework for training mobile GUI agents from self-sourced usage data while preserving privacy. It has two main components:  \n\n(1) Auto-Annotation, which runs a local VLM to infer user intent from interaction trajectories using a step-wise Descriptor plus episode-wise Summarizer; and  \n(2) FedVLM-A, a federated training framework that adapts global aggregation by jointly weighting episode and step counts to better handle heterogeneous user data.  \n\nExperiments on multiple mobile-agent benchmarks show that MobileA3gent matches or surpasses human-labelled training at significantly lower cost, improves robustness under non-IID settings, and preserves privacy by keeping data on-device"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Practical FL innovation: Introduces an adapted aggregation method that jointly weights episode and step counts, effectively addressing two-level data heterogeneity with a simple, interpretable design.\n\n* The system is technically sound and thoroughly validated across four benchmarks with non-IID splits, ablations, and scaling analyses showing consistent gains over FL baselines.\n\n* Well-written and easy to follow, supported by clear figures and organized methodology.\n\n* Achieves human-annotation-level accuracy at much lower labeling cost, demonstrating strong scalability and privacy preservation. The paper also discusses ethical risks transparently."}, "weaknesses": {"value": "* No on-device evaluation for either annotation or training, leaving open questions on latency, energy, and memory feasibility.\n\n* Annotation models (e.g., Qwen2-VL-7B) exceed mobile capacity, creating a gap between intended on-device use and evaluated setups."}, "questions": {"value": "* What exactly is transmitted (gradients, LoRA deltas, or weights)? Any clipping, noise, or secure aggregation to preserve privacy? \n\n* What are the latency and VRAM for Qwen2-VL-2B and Qwen2-VL-7B for annotation and training on edge hardware?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FkSzCEAITR", "forum": "NQWk6sTyl4", "replyto": "NQWk6sTyl4", "signatures": ["ICLR.cc/2026/Conference/Submission11883/Reviewer_ybJy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11883/Reviewer_ybJy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809495270, "cdate": 1761809495270, "tmdate": 1762922898152, "mdate": 1762922898152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MobileA3gent, a framework for training mobile GUI agents by leveraging self-sourced data from users’ daily smartphone interactions in a privacy-preserving and cost-efficient manner.  \n\n1. Auto-Annotation – A local vision-language model (VLM)–based method that automatically infers user intentions (instructions) from GUI interaction trajectories, decomposing them into step-wise atomic descriptions and episode-level summaries to produce high-quality labeled data without human effort.\n\n2. FedVLM-A – A federated learning framework that trains VLM-based mobile agents across distributed user devices using an adapted global aggregation scheme. This scheme accounts for both episode-level and step-level heterogeneity in user data distributions, improving over standard FL approaches like FedAvg or FedYogi.\n\nExperiments across four benchmarks (AndroidControl, Android in the Wild, GUI Odyssey, AndroidWorld) and 10+ models show that MobileA3gent achieves comparable or superior performance to centralized, human-annotated baselines while reducing annotation costs by up to 99%. The system also provides strong privacy guarantees and scalability via decentralized participation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work addresses a pressing issue in the GUI agent community — the cost and scalability of data collection for training agents. By enabling distributed, self-sourced data annotation, the paper proposes a paradigm shift toward user-centric, privacy-preserving model training.\n\nThe Auto-Annotation mechanism is conceptually novel, integrating low-level (step-wise) and high-level (episode-wise) VLM reasoning to generate human-like task instructions.\n\nThe Adapted Aggregation in FedVLM-A introduces a meaningful extension to traditional FL weighting, explicitly handling two-level heterogeneity (episodes and steps) — a nontrivial characteristic of GUI data.\n \nThe experiments are extensive and rigorous, covering multiple datasets, model families, and evaluation metrics. Ablation studies demonstrate the impact of each component (Auto-Annotation, FedVLM-A) and the trade-offs between cost and performance.\n \nMobileA3gent matches or exceeds centralized human-annotation baselines while being vastly more efficient. Results on AndroidWorld demonstrate solid generalization to unseen environments.\n \nThe paper is well-structured, with clear motivation, method description, and visualizations. The commitment to releasing code and the detailed appendices further support reproducibility."}, "weaknesses": {"value": "Although the system is motivated by decentralized real-user data, all experiments use public datasets, not actual on-device data collection. Thus, claims about privacy, scalability, and real-world feasibility remain unverified empirically. The gap between simulation and deployment is significant.\n\nWhile FedVLM-A preserves privacy by design, the paper provides no formal privacy analysis (e.g., differential privacy bounds or adversarial leakage evaluation). Table 1’s qualitative comparison is insufficient for a paper emphasizing privacy.\n\nIt remains ambiguous how annotation errors propagate through federated training. There is no analysis of annotation noise tolerance or mechanisms to correct systematically wrong inferences made by local VLMs.\n\nFederated VLM training is resource-intensive. The paper omits a detailed analysis of client-side computation, communication frequency, or energy cost, which are critical for feasibility on mobile devices."}, "questions": {"value": "this dataset curation method might contain sparse and noisy data, i wonder how does it compare to centralized manual data curation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s1nYJOpe5c", "forum": "NQWk6sTyl4", "replyto": "NQWk6sTyl4", "signatures": ["ICLR.cc/2026/Conference/Submission11883/Reviewer_JzkA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11883/Reviewer_JzkA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896966033, "cdate": 1761896966033, "tmdate": 1762922897750, "mdate": 1762922897750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MobileA3gent, a framework for training mobile GUI agents using decentralized user data while preserving privacy. It features Auto-Annotation (automatic data collection from daily phone usage via local VLMs, reducing costs by 99%) and FedVLM-A (federated learning with adapted aggregation for mobile data heterogeneity). Experiments show that this approach achieves performance comparable to manual annotation at only 1% of the cost across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Interesting topic of mobile GUI agents\n2. Good comparison with existing baselines\n3. Good quality of figures, use of font sizes etc.\n4. Appreciate the open sourcing of the code."}, "weaknesses": {"value": "1. Overall the paper was not an easy read. While there are no spelling/grammar issues, I took me a while to understand the motivation of this work, how they collect the data, where the data are processed/annotated through the VLM, the type of data that are collected etc. I strongly recommend the authors to revise the manuscript and provide clear examples of data samples, and system design diagram of the system's pipeline. I would also avoid the inline tables and figures (just a friendly suggestion) as it makes the manuscript hard to read.\n2. My biggest concern is the contribution of this work. While the area of mobile GUI agents is very important, I find the approach impractical. The authors suggest that an on-device VLM that constantly decodes and annotates screenshots is something that can be mainstream in large studies. However, VLMs are extremely expensive, only available in high-end devices and consume extreme device power. Even assuming that things will change in the future, I don't expect that it will be realistic to use VLM in the background while a user accesses his/her device. Moreover, use of VLMs will introduce additional UX issues (latency, device high temperature, etc.). Offline analysis could be possible, but you would need to deal with many existing mobile OS limitations (in both Android and iOS).\n3. The authors claim that the system is \"preserving privacy\" of the users, however there is no privacy analysis, state of thread model etc. besides a small paragraph in L264."}, "questions": {"value": "1. Could the authors provide a more detailed description of the data collection and annotation process to clarify how the data flow through the system and what exactly the model sees during training and inference?\n2. How do the authors envision this approach being deployed in real-world large-scale studies given current hardware limitations?\n3. Could the authors elaborate on the specific privacy guarantees of the system, including what data are exposed or stored, how they are protected, and what assumptions are made about adversaries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lqqGvL3IEH", "forum": "NQWk6sTyl4", "replyto": "NQWk6sTyl4", "signatures": ["ICLR.cc/2026/Conference/Submission11883/Reviewer_k9bV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11883/Reviewer_k9bV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021779343, "cdate": 1762021779343, "tmdate": 1762922896746, "mdate": 1762922896746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}