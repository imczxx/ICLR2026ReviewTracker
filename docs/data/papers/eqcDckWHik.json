{"id": "eqcDckWHik", "number": 3064, "cdate": 1757325345284, "mdate": 1763719868040, "content": {"title": "CompassNav: Steering From Path Imitation to Decision Understanding In Navigation", "abstract": "The dominant paradigm for training Large Vision-Language Models (LVLMs) in navigation relies on imitating expert trajectories. This approach reduces the complex navigation task to a sequence-to-sequence replication of a single correct path, fundamentally limiting the agent's ability to explore and generalize. In this work, we argue for and introduce a new paradigm: a shift from Path Imitation to Decision Understanding. The goal of this paradigm is to build agents that do not just follow, but truly understand how to navigate. We materialize this through two core contributions: first, we introduce Compass-Data-22k, a novel 22k-trajectory dataset.Its Reinforcement Fine-Tuning (RFT) subset provides a panoramic view of the decision landscape by annotating all feasible actions with A* geodesic distances. Second, we design a novel gap-aware hybrid reward function that dynamically adapts its feedback to decision certainty, shifting between decisive signals for optimal actions and nuanced scores to encourage exploration. Integrated into an SFT-then-RFT recipe, our CompassNav agent is trained not to memorize static routes, but to develop an internal 'compass' that constantly intuits the direction to the goal by evaluating the relative quality of all possible moves. This approach enables our 7B agent to set a new state-of-the-art on navigation benchmarks, outperforming even larger proprietary models, and achieve robust real-world goal navigation on a physical robot.", "tldr": "We shift navigation from \"Path Imitation\" to \"Decision Understanding.\" With a novel, densely-annotated dataset and a gap-aware reward, our 7B agent learns to evaluate all moves, achieving SOTA results that surpass even larger models.", "keywords": ["Embodied AI", "Goal-Driven Navigation", "Large Vision-Language Models", "Reinforcement Fine-Tuning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9557f051c53fe5df21648c094cd0cdd55fabba24.pdf", "supplementary_material": "/attachment/624f589c201583f0e4fcef742e4782c1b956134e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes CompassNav, a two-stage training framework that shifts the paradigm of embodied navigation from path imitation to decision understanding.\nThe authors argue that traditional imitation learning constrains embodied agents to replicate single expert trajectories, thereby limiting their exploration and generalization capability.\nTo address this, the paper introduces a large-scale dataset, Compass-Data-22k, which provides panoramic supervision signals by annotating all possible actions with A* geodesic distances. In addition, a gap-aware hybrid reward function is designed to modulate feedback strength based on decision certainty, which is then further integrated within a GRPO framework.\nThe resulting pipeline enables the 7B-parameter CompassNav model to develop an internal “compass,” allowing it to evaluate action quality beyond simple imitation.\nExperimental results show that CompassNav achieves state-of-the-art performance on HM3Dv1/v2 and MP3D benchmarks and generalizes robustly to real-robot scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes a clear conceptual shift from imitation to understanding, reformulating navigation as a decision-making problem rather than a trajectory replication. This shift is appealing, helping to address the fundamental bottleneck of imitation-based LVLM training.\nThe Compass-Data-RFT-11k subset provides per-allocated A* annotations, offering richer training signals than existing single-path datasets. The data collection pipeline is transparent, reproducible, and logically linked to the proposed reward mechanism.\nThe gap-aware hybrid reward integrates the notions of distance-based preference and decision certainty in a simple but effective way. It adapts to ambiguous decision spaces and contributes to stable reinforcement fine-tuning.\nThe CompassNav agent consistently outperforms both modular pipelines (e.g., CogNav, UniGoal) and end-to-end LVLMs (e.g., GPT-4o, Nav-R1) despite using significantly fewer training samples. Results on real-world robotic navigation further support the performance."}, "weaknesses": {"value": "*Conceptual ambiguity in “Decision Understanding”*\n\nThe paper repeatedly emphasizes “understanding” but does not quantitatively measure what constitutes understanding beyond improved reward signals. It remains unclear whether CompassNav truly learns transferable decision reasoning or merely fits dense reward distributions.\n\n*Reliance on A\\* as oracle*\n\nWhile efficient, the A*-based distance labeling assumes deterministic environments and may not reflect realistic perception noise or dynamic obstacles. This could introduce bias into both reward computation and policy evaluation, especially in more practical environments.\n\n*Limited diversity of evaluation environments*\n\nExperiments focus mainly on HM3D variants and MP3D, which are visually and structurally similar. Broader generalization tests (e.g., Gibson, AI2-THOR, or out-of-domain tasks) would strengthen claims of decision-level robustness."}, "questions": {"value": "The authors should explain more about the “decision understanding”. Are there any ways/metrics that help to demonstrate CompassNav’s reasoning rather than imitation bias?\n\nIn GRPO, the KL regularization term constrains policy updates relative to the SFT reference model. Do the authors experiment with adaptive or decayed KL coefficients to allow more flexible exploration during late training?\n\nThe RFT subset selectively explores “ambiguous nodes.” How is the sample balance ensured to prevent overrepresentation of uncertain scenarios in training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EL4hijC0z8", "forum": "eqcDckWHik", "replyto": "eqcDckWHik", "signatures": ["ICLR.cc/2026/Conference/Submission3064/Reviewer_iCB1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3064/Reviewer_iCB1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562020695, "cdate": 1761562020695, "tmdate": 1762916535385, "mdate": 1762916535385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CompassNav, a paradigm for training large vision-language models (LVLMs) in navigation that shifts from Path Imitation to Decision Understanding. Instead of merely replicating expert trajectories, the authors aim to build agents that reason about the relative quality of alternative actions. To achieve this, the authors introduce compass-data-22k, a large-scale dataset that annotates every feasible action using A* geodesic distances, and design a gap-aware hybrid reward that dynamically adjusts feedback according to decision certainty. The training pipeline consists of  supervised fine-tuning stage and a reinforcement fine-tuning stage. Experiments on standard navigation benchmarks (HM3D, MP3D) show that CompassNav outperforms modular and end-to-end baselines, achieving new state-of-the-art results in object  and image-goal navigation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper articulates a clear paradigm shift from trajectory imitation to decision-space reasoning. This reframing is conceptually strong.\n\nThe two-stage SFT-RFT training pipeline is methodically designed and empirically justified, overcoming the cold-start issue commonly faced in reinforcement fine-tuning."}, "weaknesses": {"value": "While the paper claims to enable 'decision understanding',  the experiments still rely on conventional navigation metrics (SR, SPL). There is no explicit evaluation of whether the model actually understands or reasons about its decisions.  Some experiments similar to basic instruction-following tests could help the paper illustrate this issue, such as the tests conducted in [1]. Meanwhile, [1] involves VLM-based indoor navigation. Related comparisons or discussions are strongly recommended.\n\n\n[1] Wang, Z., Wu, M., Cao, Y., Ma, Y., Chen, M., & Tuytelaars, T. (2024). Navigating the nuances: A fine-grained evaluation of vision-language navigation."}, "questions": {"value": "What is the inference speed of this approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5FTGk4o1VT", "forum": "eqcDckWHik", "replyto": "eqcDckWHik", "signatures": ["ICLR.cc/2026/Conference/Submission3064/Reviewer_Bopw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3064/Reviewer_Bopw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756558860, "cdate": 1761756558860, "tmdate": 1762916535161, "mdate": 1762916535161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors address the object goal navigation problem. They introduce Compass-Data-22k, a 22k-trajectory dataset comprising 11k reasoning traces with multi-modal reasoning and goal selection for supervised fine-tuning (SFT) and 11k densely annotated reward samples for reinforcement fine-tuning (RFT). Actions are generated through an action proposal module, with rewards computed based on A* geodesic distances. The authors train the CompassNav agent using an SFT–RFT pipeline and propose a gap-aware hybrid reward function that encourages exploration in ambiguous cases and delivers decisive feedback when a clear optimal action exists. The resulting model achieves state-of-the-art performance on goal navigation benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors commit to open-sourcing both the dataset and codebase, which enhances reproducibility and can meaningfully benefit the research community.\n\n- The proposed Compass-Data-22k dataset is well-structured and provides dense annotations that can facilitate broader research in embodied navigation and decision modeling.\n\n- The gap-aware hybrid reward function is an interesting contribution, as it attempts to balance decisiveness and exploration through adaptive reward modulation.\n\n- The inclusion of real-world robot deployment results complements the simulation benchmarks and strengthens the practical relevance of the work."}, "weaknesses": {"value": "- Although the authors claim state-of-the-art performance, the improvements over baselines are marginal and may not clearly justify the methodological complexity.\n\n- The claim that the model achieves “decision understanding” and this paper introduces “a new paradigm” seems to be overstated. \n    - The reasoning traces used for SFT are synthetically generated by another multimodal LLM and may not reflect causal or grounded reasoning. Furthermore, since no contextual history is preserved between steps, the reasoning is effectively discarded in subsequent decisions. For instance, in Figure 2(3), the generated rationale—“I want to go inside the kitchen…”—would be forgotten by the next invocation, leaving the robot unaware of why it entered the kitchen in the first place. As a result, these generated explanations appear arbitrary and disconnected from actual decision-making, contributing little to “decision understanding.”\n    - During RFT, the reasoning outputs are not incorporated into the reward computation, suggesting that the model’s reasoning is superficial rather than functionally integrated. As such, the method does not have much difference from standard robot learning paradigms (e.g., pretraining and finetuning using reinforcement learning with dense rewards).\n\n- The motivation and necessity for the use of LVLMs are not fully convincing. Given access to depth and semantic information, classical navigation pipelines (e.g., SLAM + frontier exploration + object recognition) could potentially solve the same problem efficiently without large-scale data or training. Why use LVLMs for such tasks?"}, "questions": {"value": "- Regarding the gap-aware hybrid reward (line 304), what exactly do *best* $d_t^{(1)}$ and *second-best* $d_t^{(2)}$ mean? Are they derived purely from A* distances (i.e., shortest path to goal)? Or based on the model’s predicted actions (best corresponds to the most likely action output) and their corresponding A* distances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "shRxhvdupG", "forum": "eqcDckWHik", "replyto": "eqcDckWHik", "signatures": ["ICLR.cc/2026/Conference/Submission3064/Reviewer_6mHY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3064/Reviewer_6mHY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797395651, "cdate": 1761797395651, "tmdate": 1762916534759, "mdate": 1762916534759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a vision-language navigation paradigm that shifts from traditional single-path imitation to understanding and modeling the relative value of all candidate actions. The authors built the Compass-Data-22k dataset based on the HM3D environment and designed a gap-aware hybrid reward mechanism that uses a softmax formulation to represent action quality, introducing a dynamic bonus under high-certainty conditions. The model, built upon Qwen2.5-VL-7B, employs a two-stage training strategy of SFT pretraining followed by RFT reinforcement to achieve reasoning-to-action policy learning. Evaluations on the HM3D and MP3D datasets for ObjectNav and ImageGoal tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- A self-constructed Compass-Data-22k dataset provides dense annotations to support reward modeling.\n- The gap-aware reward dynamically adapts to different uncertainty scenarios, enhancing training stability and exploration capability."}, "weaknesses": {"value": "- The work mainly combines existing components—standard LVLM architecture and RLHF-style two-stage training—without introducing fundamentally new modeling or optimization techniques. The “gap-aware hybrid reward” is a reasonable design but conceptually close to prior contrastive or entropy-regularized rewards.\n- The two-stage SFT–RFT pipeline and dense A*-based labeling increase training cost and reduce scalability compared to simpler imitation or zero-shot methods.\n- Despite stronger reasoning, CompassNav underperforms recent zero-shot approaches like BeliefMapNav, indicating weaker generalization efficiency.\n[1] BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation\n- In the navigation task, the determination of whether the agent has “reached the target” is delegated to an independent large language–vision model (such as Qwen2.5-VL-7B), serving as the Stop Agent. However, large language models are often sensitive to semantic thresholds and contextual ambiguities. As a result, the Stop Agent may prematurely trigger a stop when the target is only partially visible, semantically ambiguous, or occluded, leading to an overestimation of the Success Rate (SR). Therefore, the currently reported SR metric may not accurately reflect the true performance of the navigation system. It is necessary to additionally provide the Stop Agent’s independent accuracy, recall, and SR sensitivity analysis, or to describe the types of misjudgments that occur under occlusion and complex semantic target conditions.\n- The paper proposes a complex framework (including RFT, SFT, reward module, candidate action generation, Stop Agent, and Bonus mechanism), but does not individually verify the necessity of each component. It lacks controlled experiments for key variables such as the number of candidate actions, angular resolution, reward function form, teacher model quality, and sampling strategy. In addition, statistical significance tests could be added to demonstrate the true stability of the performance improvements."}, "questions": {"value": "- The paper lacks sufficient implementation details to ensure reproducibility — for example, the exact hyperparameters, model initialization, and architecture adaptation between CLIP and Qwen-7B are not clearly described. Could the authors clarify these details and release training configurations or code references?\n- The proposed two-stage SFT–RFT training involves GRPO optimization and A*-based dense labeling, which appear computationally expensive. Could the authors provide quantitative analysis of training cost (e.g., GPU hours, sample efficiency, or convergence comparison) relative to existing navigation methods?\n- Will the Compass-Data-22k dataset be publicly released? If so, could the authors provide more details about its annotation process and data generation pipeline?\n- How significantly do the number of candidate actions and sampling parameters affect model performance? Have the authors conducted any sensitivity analysis or ablation studies to evaluate this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DWmM9qqkuA", "forum": "eqcDckWHik", "replyto": "eqcDckWHik", "signatures": ["ICLR.cc/2026/Conference/Submission3064/Reviewer_oiRX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3064/Reviewer_oiRX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915626029, "cdate": 1761915626029, "tmdate": 1762916534316, "mdate": 1762916534316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}