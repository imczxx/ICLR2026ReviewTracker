{"id": "TtlCRjCJOT", "number": 5360, "cdate": 1757904125891, "mdate": 1763521774841, "content": {"title": "Render-FM: Feedforward Model for Real-time Photorealistic Volumetric Rendering", "abstract": "Current neural volumetric rendering methods like NeRF and 3D Gaussian Splatting (3DGS) achieve photorealistic quality but require prohibitive per-scan optimization (30+ minutes for 3DGS, 10+ hours for NeRF), limiting clinical applicability. We propose Render-FM, a feedforward model that directly regresses 6D Gaussian Splatting parameters from CT volumes in a single 2.8-second forward pass—a 500× speedup. Our key innovation, Anatomy-Guided Priming (AGP), leverages segmentation masks and transfer functions to provide anatomically-informed initialization. Trained on 991 diverse CT scans, Render-FM employs a 3D U-Net architecture to predict per-voxel 6DGS parameters, enabling immediate real-time rendering (328+ FPS). Experiments demonstrate that Render-FM achieves superior quality  compared to optimized baselines (27.30 vs 26.63 dB PSNR), with optional 89-second fine-tuning reaching 31.67 dB PSNR. Unlike per-scan methods, Render-FM generalizes to unseen anatomies, novel transfer functions, and compositional organ visualization without retraining. This advancement transforms clinical volumetric visualization, reducing preparation time from hours to seconds while maintaining or exceeding state-of-the-art quality.", "tldr": "", "keywords": ["3DGS", "6DGS", "CT", "Volumetric Rendering"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/516df51b8ae136992f082ac0a2fdbaf970142b2f.pdf", "supplementary_material": "/attachment/4b9fc6a8d4c2af7376e6a18607e69ad504b50669.zip"}, "replies": [{"content": {"summary": {"value": "The authors proposes Render-FM, a feed-forward model that maps a multi-channel CT volume directly to 6D Gaussian Splatting parameters in a single forward pass. Optimizations were majorly conducted related to acceleration. 3D nnUNet was deployed, and model was trained on 991 CT scans. Quantitative measurements are relatively sufficient. There are minor weaknesses can be seen below."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Easy read\n2. Very clear details statements on the key points and the experimentations."}, "weaknesses": {"value": "1. Since it is a clinical application, it would be great if some task specific metrics could be evaluated. Like but not limited to radiologists’ qualitative evaluation, edge-related measurements, etc.\n2. The scope of the study experimentations might be limited. I think the study only focusing on medical imaging, and one specific medical imaging modality (CT). Like you mentioned in the conclusions: “We presented Render-FM, a feedforward model for real-time, high-fidelity volumetric rendering of CT scans using 6D Gaussian Splatting.” \n3. Only one baseline was compared, might be insufficient."}, "questions": {"value": "1. “We introduce Render-FM, a foundation model for volumetric rendering through direct feedforward prediction of 6D Gaussian Splatting (6DGS) parameters from CT volumes. Unlike” I read through the paper, the authors seem like proposing UNet-like architecture to learn the mapping. Where can I see if it is related with foundation model? I might not be convinced by the authors if the argument would be that the “foundation” came from 991 CT images.\n2. While AGP vs non-AGP has been explored for 6DGS, are there any reasons the authors didn’t do ablations regarding components inside Render-FM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3NZXDqHAOa", "forum": "TtlCRjCJOT", "replyto": "TtlCRjCJOT", "signatures": ["ICLR.cc/2026/Conference/Submission5360/Reviewer_VCtd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5360/Reviewer_VCtd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761441701556, "cdate": 1761441701556, "tmdate": 1762918023105, "mdate": 1762918023105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Render-FM, a foundational model that generates 3D renderings directly from CT volumetric scans. The approach incorporates an anatomy-guided initialization module that leverages Gaussian primitives to provide an effective structural prior for rendering. The paper also demonstrates a considerable acceleration in rendering speed through a feed-forward model design, highlighting the potential of Render-FM for real-time clinical applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important and clinically relevant problem by enabling volumetric rendering of CT scans. Such 3D representations could have significant practical value in diagnostic workflows and treatment planning.\n\n\n- The paper is well-structured and clearly written, with the problem statement clearly defined, appropriate use of figures to illustrate the methodology and results, and rigorous experimental validation supporting the stated claims."}, "weaknesses": {"value": "One general concern is the dependence on segmentation masks for detecting the anatomy and limited discussion of real-world deployment scenarios raise concerns about the model’s generalizability and practical integration into clinical workflows."}, "questions": {"value": "- Although 6DGS with the AGP (anatomy guided priming) module was implemented, could the AGP module also improve the results of other 3DGS-based approaches? An ablation experiment can provide some insights about this question.\n\n- The paper states that the reduction in optimization time enables real-time clinical applications. Could the authors elaborate on the specific types of clinical scenarios or use cases where such real-time rendering would be beneficial? Is there already studies showing that a lack of such visualization is causing workflow constraints ? Or the use of such rendering have helped in clinical decision making? These findings can further strengthen this work focusing more on a clinical aspect.\n\n- The AGP model relies on segmentation masks, and TotalSegmentator[1] was used when these masks were unavailable. In a real-world clinical setting, where segmentation masks may not always be present, how would the model handle new, unsegmented samples?  What would happen if the segmentation from TotalSegmentator was not perfect? What drawbacks would such a scenario create? Can the confidence measures of segmentation be incorporated for the Gaussian initialization? The method performs well on the baseline dataset, but such an analysis would help in assessing the real-world clinical application.\n\n- Table 1., shows Render-FM with FT on dataset that is unseen during training. I did not not understand what FT means in this scenario? What is the difference between OOD seen vs OOD unseen ? \n\n- Generalization beyond TotalSegmentator dataset. Since the claim is that Render-FM is a foundational model, the question arises about it's generalization. Could the model be tested against baselines in a zero-shot (or truely unseen fashion) on dataset/s that do not overlap with the TotalSegmentator datasets like AMOS[2] or CHAOS[3] ?\n\nReferences:\n1. Jakob et al., TotalSegmentator: Robust Segmentation of 104 Anatomic Structures in CT Images\n\n2. Ji et al., AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation\n\n3. CHAOS challenge., https://chaos.grand-challenge.org/"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gS1pjoeTJA", "forum": "TtlCRjCJOT", "replyto": "TtlCRjCJOT", "signatures": ["ICLR.cc/2026/Conference/Submission5360/Reviewer_UA8m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5360/Reviewer_UA8m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722926674, "cdate": 1761722926674, "tmdate": 1762918022823, "mdate": 1762918022823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Response to All Reviewers"}, "comment": {"value": "We thank all reviewers for their constructive feedback. We have revised the paper and conducted additional experiments as detailed below.\n\n---\n\n## Key Revisions and Additions\n\n### 1. Clarified Rendering Paradigm Distinction\nWe revised the Introduction and Related Work to distinguish **standard DVR** (Slicer/ITK-SNAP with local illumination) from **photorealistic rendering** (Cinematic Rendering with global illumination). Render-FM achieves photorealistic quality in **2.8 seconds** versus >1 hour—a **500× speedup**.\n\n### 2. Revised Terminology: Foundation Model → Feedforward Model\nWe now use \"**feedforward model**\" instead of \"foundation model\" throughout. This is a **CT-focused feedforward rendering model** with zero-shot inference and generalization across TotalSegmentator (46 scans), CT-ORG (10 scans), CTPelvic1K (6 scans), novel transfer functions, and compositional visualization.\n\n### 3. Additional Experiments: AGP Generalizability\nUsing CT data from the 6DGS paper, AGP improves 3DGS by **+1.72 dB**, confirming AGP is a general initialization strategy for Gaussian Splatting on medical data.\n\n### 4. Additional Experiments: CTPelvic1K Zero-Shot Validation\nZero-shot on CTPelvic1K: Render-FM achieves **2,500× speedup** (0.65s vs 1,643s) with competitive quality (SSIM: 0.926). The faster time (0.65s vs 2.8s) reflects smaller volume size. With 78s fine-tuning: SSIM 0.941, PSNR 33.30, validating robust cross-dataset generalization.\n\n---\n\n## Common Themes Across Reviews\n\n### Technical Contributions\nUnlike LGMs (sparse 2D views), Render-FM leverages dense 3D volumetric data with calibrated Hounsfield Units. **AGP is essential**—without anatomical guidance, preliminary experiments produced blank renders. AGP derives positions from world coordinates, enabling the network to focus on appearance and shape properties.\n\n### Clinical Impact\nThe 500× speedup enables practical workflows: (1) **Surgical Planning**—immediate 3D visualization improves spatial understanding [Beyer et al., 2007; Dappa et al., 2016]; (2) **Patient Communication**—interactive visualization; (3) **Interventional Radiology**—rapid angle generation; (4) **Medical Education**—interactive teaching materials.\n\n### Evaluation Metrics\nOur scope: rendering quality and computational efficiency, not clinical diagnostic outcomes. We use PBR ground truth with standard metrics (PSNR, SSIM, LPIPS, Time, FPS). Clinical validation requires IRB-approved trials—beyond our scope but enabled by our technology.\n\n### Baseline Selection\nWe use 6DGS as our rendering backend, making it the most direct comparison.\n*   **Why not Slicer/ITK-SNAP?** Different paradigm—cannot produce photorealistic quality (global illumination, soft shadows).\n*   **Why not NeRF?** Slower (10+ hours).\n*   **Why not 3DGS?** 6DGS substantially outperforms 3DGS on CT (PSNR: 33.56 vs. 25.71) as shown in the original 6DGS paper.\n\n### Segmentation Robustness\nOur model predicts from learned anatomical priors, not just segmentation labels. Incorrect labels affect color/opacity while preserving geometry—analogous to our \"unseen transfer function\" experiments where wrong labels assign incorrect colors to correct anatomical structures. Clinical workflow remains efficient: automated segmentation (30s) → Render-FM (2.8s) → optional manual correction (3s) → optional fine-tuning (89s). Total time <2 minutes versus >1 hour for optimization-based methods.\n\n### Single Modality Scope\nCT provides consistent Hounsfield Units and 991 training scans. MRI extension is straightforward—adapting for different sequences (T1, T2, FLAIR). Many methods start single-modality (TotalSegmentator: CT-only before MRI extension).\n\n### Technical Details\n**nnU-Net**: TotalSegmentator's self-configuration (1.5mm isotropic, PlainConvUNet). 1.5mm resampling ensures consistent features across variable native spacing.\n**Fine-Tuning**: Render-FM 300 iterations (78-136s) vs. 6DGS 30,000 iterations (1463-2261s)—**17-26× speedup** with higher quality.\n\n---\n\n## Summary of Core Contributions\nDespite single-modality focus, our contributions remain significant:\n1.  **First feedforward model for photorealistic medical volumetric rendering** with 500× speedup.\n2.  **Novel domain-specific technique (AGP)** applicable beyond CT—validated on both 6DGS (+2.29 dB) and 3DGS (+1.72 dB).\n3.  **Elimination of per-scan optimization entirely** through learned generalizable representations.\n4.  **Strong cross-dataset generalization** demonstrated on three independent datasets (TotalSegmentator, CT-ORG, CTPelvic1K).\n5.  **Robust zero-shot inference** with optional rapid fine-tuning for critical cases.\n6.  **Blueprint for extending to other modalities** (MRI, specialized imaging) as important future work.\n\nThese contributions transform photorealistic rendering from a research curiosity (hours) into a practical clinical tool (seconds). We thank all reviewers for their constructive feedback and hope our revisions address your concerns."}}, "id": "DeYsXWv5Lw", "forum": "TtlCRjCJOT", "replyto": "TtlCRjCJOT", "signatures": ["ICLR.cc/2026/Conference/Submission5360/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5360/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5360/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763522024607, "cdate": 1763522024607, "tmdate": 1763522024607, "mdate": 1763522024607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Response to All Reviewers"}, "comment": {"value": "We thank all reviewers for their constructive feedback. We have revised the paper and conducted additional experiments as detailed below.\n\n---\n\n## Key Revisions and Additions\n\n### 1. Clarified Rendering Paradigm Distinction\nWe revised the Introduction and Related Work to distinguish **standard DVR** (Slicer/ITK-SNAP with local illumination) from **photorealistic rendering** (Cinematic Rendering with global illumination). Render-FM achieves photorealistic quality in **2.8 seconds** versus >1 hour—a **500× speedup**.\n\n### 2. Revised Terminology\nWe now use \"**feedforward model**\" instead of \"foundation model\" throughout. This is a **CT-focused feedforward rendering model** with zero-shot inference and generalization across TotalSegmentator, CT-ORG, CTPelvic1K, novel transfer functions, and compositional visualization.\n\n### 3. Additional Experiments: AGP Generalizability\nUsing CT data from the 6DGS paper, AGP improves 3DGS by **+1.72 dB**, confirming AGP is a general initialization strategy for Gaussian Splatting on medical data.\n\n### 4. Additional Experiments: CTPelvic1K Zero-Shot Validation\nZero-shot on CTPelvic1K: Render-FM achieves **2,500× speedup** (0.65s vs 1,643s) with competitive quality (SSIM: 0.926). The faster time (0.65s vs 2.8s) reflects smaller volume size. With 78s fine-tuning: SSIM 0.941, PSNR 33.30, validating robust cross-dataset generalization.\n\n---\n\n## Common Themes Across Reviews\n\n### Technical Contributions\nUnlike LGMs (sparse 2D views), Render-FM leverages dense 3D volumetric data with calibrated Hounsfield Units. **AGP is essential**—without anatomical guidance, preliminary experiments produced blank renders. AGP derives positions from world coordinates, enabling the network to focus on appearance and shape properties.\n\n### Clinical Impact\nThe 500× speedup enables practical workflows: (1) **Surgical Planning**—immediate 3D visualization improves spatial understanding [Beyer et al., 2007; Dappa et al., 2016]; (2) **Patient Communication**—interactive visualization; (3) **Interventional Radiology**—rapid angle generation; (4) **Medical Education**—interactive teaching materials.\n\n### Evaluation Metrics\nOur scope: rendering quality and computational efficiency, not clinical diagnostic outcomes. We use PBR ground truth with standard metrics (PSNR, SSIM, LPIPS, Time, FPS). Clinical validation represents important future work. This submission focuses on the technical rendering contribution that enables such clinical studies.\n\n### Baseline Selection\nWe use 6DGS as our rendering backend, making it the most direct comparison.\n*   **Why not Slicer/ITK-SNAP?** Different paradigm—cannot produce photorealistic quality (global illumination, soft shadows).\n*   **Why not NeRF?** Slower (10+ hours).\n*   **Why not 3DGS?** 6DGS outperforms 3DGS on CT (PSNR: 33.56 vs. 25.71) as in the 6DGS paper, making it a stronger SOTA baseline.\n\n### Segmentation Robustness\nOur model predicts from learned anatomical priors, not just segmentation labels. Incorrect labels affect color/opacity while preserving geometry—analogous to our \"unseen transfer function\" experiments where wrong labels assign incorrect colors to correct anatomical structures. Clinical workflow remains efficient: automated segmentation (30s) → Render-FM (2.8s) → optional manual correction (3s) → optional fine-tuning (89s). Total time <2 minutes versus >1 hour for optimization-based methods.\n\n### Single Modality Scope\nCT provides consistent Hounsfield Units and 991 training scans. MRI extension is straightforward—adapting for different sequences (T1, T2, FLAIR). Many methods start single-modality (TotalSegmentator: CT-only before MRI extension).\n\n### Technical Details\n**nnU-Net**: TotalSegmentator's self-configuration (1.5mm isotropic, PlainConvUNet). 1.5mm resampling ensures consistent features across variable native spacing.\n**Fine-Tuning**: Render-FM 300 iterations (78-136s) vs. 6DGS 30,000 iterations (1463-2261s)—**17-26× speedup** with higher quality.\n\n---\n\n## Summary of Core Contributions\nDespite single-modality focus, our contributions remain significant:\n1.  **First feedforward model for photorealistic medical volumetric rendering** with 500× speedup.\n2.  **Novel domain-specific technique (AGP)** applicable beyond CT—validated on both 6DGS (+2.29 dB) and 3DGS (+1.72 dB).\n3.  **Elimination of per-scan optimization entirely** through learned generalizable representations.\n4.  **Strong cross-dataset generalization** demonstrated on three independent datasets (TotalSegmentator, CT-ORG, CTPelvic1K).\n5.  **Robust zero-shot inference** with optional rapid fine-tuning for critical cases.\n6.  **Blueprint for extending to other modalities** (MRI, specialized imaging) as important future work.\n\nThese contributions transform photorealistic rendering from a research curiosity (hours) into a practical clinical tool (seconds). We thank all reviewers for their constructive feedback and hope our revisions address your concerns."}}, "id": "DeYsXWv5Lw", "forum": "TtlCRjCJOT", "replyto": "TtlCRjCJOT", "signatures": ["ICLR.cc/2026/Conference/Submission5360/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5360/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5360/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763522024607, "cdate": 1763522024607, "tmdate": 1763555064608, "mdate": 1763555064608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Given a CT volume of a torso, this paper aims to render a mesh view of segmented organs with more realistic/physics-based appearances. \n- To do so, it takes the CT volume, the segmentations, and a voxel-wise transfer function and uses them to fit a Gaussian splatting-based model on images generated by a physics-based renderer. \n- However, doing so directly would be slow, as the physics-based renderer apparently requires 18 seconds to render a single view. \n- To that end, this paper proposes to use the same set of inputs and use a UNet to predict the parameters of the Gaussian splatting model directly.\n- This paper trains the UNet above on a subset of the TotalSegmentator dataset and presents experiments comparing it to a previous Gaussian Splatting-based method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The application of Large Gaussian Models to large medical volume visualization is fun and interesting.\n- The quality of illustrations is quite impressive.\n- Once you figure out what the paper is actually trying to do, the methods section is well presented."}, "weaknesses": {"value": "### Motivation, clarity, and scope of claims:\n\n#### 1. Speed and realism:\nIMO this paper's front matter is very confusingly presented as it assumes deep familiarity with direct volume rendering and makes claims that are not reflective of typical practice. Typically, widely used medical image viewers such as 3D Slicer or ITK-SNAP have volume renderers that fit a volume within a second or two and can be immediately interacted with. As examples, here are a few examples of volume renders achieved in seconds using Slicer: [1](https://discourse.slicer.org/t/screen-space-ambient-occlusion-for-volume-rendering/32323/27?u=lassoan), [2](https://www.youtube.com/watch?v=l8wlaCfYWG4), [3](https://www.youtube.com/watch?v=KadGfXmOs5Y)\n\nOn the other hand, this paper starts off by claiming that all previous methods require an hour+ to obtain a single mesh that can be interacted with. Furthermore, various key concepts such as transfer functions are never defined in the text and are left up to the reader who may be unfamiliar with this niche of volumetric visualization. \n\nAfter multiple reads and skims of the papers that are cited within, this paper's claims are true if a few assumptions are made: (1) one *has to* use a Gaussian Splatting based method; (2) one *must* use an expensive physically-based rendering algorithm to generate ground truth views for the GS algorithm to achieve slightly better realism to textures and second order effects. \n\nHowever, in practice, volumetric rendering algorithms that are already widely used achieve very fast rendering times and are reasonably realistic. This paper aims to rapidly get the last-mile of realism using Gaussian Splatting and a training set of physically-based renders, which is fine, but it is not at all clear from the presentation.\n\n#### 2. Technical contributions:\n\nReductively speaking, the paper is a combination of Large Gaussian Models and the 6DGS volume rendering method (ICLR25). While combination papers are absolutely fine and appreciated, the paper does not detail what is particularly challenging about this application and what new insights readers can take away from the execution.\n\n### Experiments\n\n#### 1. Only a single baseline:\n- As detailed above, medical volume visualization existed before this paper, 6DGS, GS, and NeRF, yet none of these approaches are benchmarked against in the paper. There is a sole baseline in the experiments (6DGS), which is more of an ablation, as 6DGS is part of the proposed method.\n- The paper claims that physically-based rendering takes 18 seconds to render a single view. Is this on a CPU or a GPU? This seems extraordinarily high for a GPU implementation, and I see that there are GPU implementations available.\n\n#### 2. Foundation model claims:\nThe model is trained on a subset of the TotalSegmentator CT dataset (991 volumes) and the experiments only include evaluations of render quality on a held-out subset of TS and a subset of the highly-related and very similar CT-ORG dataset.\n\nThis IMO is insufficient to make claims of being a foundation model for medical volume rendering. One baseline, one anatomical application, two ablations, and two datasets are not enough to make such a claim -- the proposed network should show evidence of generalization to new imaging contexts such as new modalities (e.g., on TotalSegmentator-MRI), new anatomical regions (e.g., vessels in the heart and brain), etc. I believe the proposed method requires retraining with a substantially larger and broader imaging dataset.\n\n### Minor comments:\n- The emphasis on being “nnUNet-inspired” is odd; the core contribution does not involve automatic configuration of training hyperparameters, which is central to nnUNet (which is otherwise just a plain UNet).\n- The opening paragraph of the related work section should be in the Introduction, as it makes it clearer.\n- Why is TotalSegmentator resampled to 1.5mm isotropic here? If realistic renders are required, the user would want renders at native/high resolution."}, "questions": {"value": "This paper is not directly in my area of expertise so please correct me if I misunderstood something and I would be happy to revisit my rating. Some major points to discuss:\n- Please elaborate on the choice/lack of baselines and contextualize this submission, given that current volume viewers all produce fast interactive renderings.\n- Please clarify the technically challenging aspects of combining large Gaussian models with 6DGS.\n- Please further justify the foundation model characterization given the scope of the presented experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BGeL0nxu0E", "forum": "TtlCRjCJOT", "replyto": "TtlCRjCJOT", "signatures": ["ICLR.cc/2026/Conference/Submission5360/Reviewer_TNXF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5360/Reviewer_TNXF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880668573, "cdate": 1761880668573, "tmdate": 1762918022507, "mdate": 1762918022507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}