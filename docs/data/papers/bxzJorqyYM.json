{"id": "bxzJorqyYM", "number": 15506, "cdate": 1758252098662, "mdate": 1759897302320, "content": {"title": "GradPruner: Gradient-guided Layer Pruning Enabling Efficient Fine-Tuning and Inference for LLMs", "abstract": "Fine-tuning Large Language Models (LLMs) with downstream data is often considered time-consuming and expensive. Structured pruning methods are primarily employed to improve the inference efficiency of pre-trained models. Meanwhile, they often require additional time and memory for training, knowledge distillation, structure search, and other strategies, making efficient model fine-tuning challenging to achieve. To simultaneously enhance the training and inference efficiency of downstream task fine-tuning, we introduce GradPruner, which can prune layers of LLMs guided by gradients in the early stages of fine-tuning. GradPruner uses the cumulative gradients of each parameter during the initial phase of fine-tuning to compute the Initial Gradient Information Accumulation Matrix (IGIA-Matrix) to assess the importance of layers and perform pruning. We sparsify the pruned layers based on the IGIA-Matrix and merge them with the remaining layers. Only elements with the same sign are merged to reduce interference from sign variations. We conducted extensive experiments on two LLMs across eight well-known datasets in downstream tasks. Including medical, financial, and general benchmark tasks. The results demonstrate that GradPruner has achieved a parameter reduction of 40% with only a 0.99% decrease in accuracy. Our code is available at https://anonymous.4open.science/r/LLM-GradPrune-436D.", "tldr": "", "keywords": ["LLM Fine-Tuning", "Layer Pruning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e3497f23d35e926fccfad5cc046a52900589fd2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "GradPruner uses the IGIA-Matrix computed from the early stages of fine-tuning (within the first 1% of steps) to evaluate the importance of parameters and layers. Layers with low importance scores are pruned, and to further retain performance under high pruning rates, the parameters of pruned layers are merged into the retained layer using a sign-based merging rule. Experiments on two LLMs across eight datasets show that GradPruner achieves approximately 40% parameter reduction with less than 1% average accuracy drop, while also significantly reducing fine-tuning time, inference latency, and memory consumption compared to representative pruning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* GradPruner jointly improves both training and inference efficiency. Unlike many pruning approaches that focus only on inference speed-ups, GradPruner is explicitly designed to reduce fine-tuning time and memory consumption as well. I believe this aligns well with the practical need for LLMs to quickly adapt to new downstream tasks.\n* GradPruner is simple to understand yet conceptually novel. The paper introduces a new gradient-based importance metric computed from less than 1% of the early fine-tuning steps, which effectively reduces the computational cost of pruning. In addition, it seems like the proposed pruning strategy is generally applicable to all transformer-based architectures.\n* The experiments are comprehensive. Evaluations are conducted on two representative LLMs across eight datasets, which enhances the reliability of the results. The paper also provides detailed ablation studies that clearly demonstrate the contribution of each component."}, "weaknesses": {"value": "* Limited theoretical grounding. Lines 201–208 simulate the gradient of W via a matrix multiplication, yet the rationale for this simulation is not theoretically justified. In addition, the sign-based merging rule in Equation (5) is presented as a heuristic with little theoretical explanation. Clearer derivations would strengthen the contribution.\n* Concern over the stability of early-step gradients. The method relies on gradient statistics collected within the first 1% of fine-tuning steps to estimate layer importance, yet such early gradients may not always provide a stable or reliable signal. For example, in small or noisy datasets, gradient variance can be high, potentially causing the IGIA-Matrix to mis-rank layer importance and resulting in suboptimal pruning decisions.\n* Incomplete efficiency metrics. The paper aims to make layer pruning more efficient and reports average time and parameter reduction. Including FLOPs would provide a more standardized and hardware-agnostic measure of computational efficiency.\n* Clarity of the method description could be improved. The paper does not explicitly detail how each transformer sub-module is handled during merging. It appears that each linear sub-module of a pruned layer is merged into the corresponding sub-module of the preceding retained layer, but this should be stated unambiguously. Moreover, Equation (5), which specifies merging based on sign agreement, is difficult to parse. A more thorough explanation and a small illustrative example would aid the reader's understanding."}, "questions": {"value": "* Can you clarify the exact computation and shape alignment for simulating the gradient of W via LoRA gradients?\n* Why is raw summation chosen for IGIA-matrix aggregation across all linear layers, rather than normalization (e.g., mean or norm per parameter or per layer size)? \n* Could the authors elaborate on the layer merging procedure, perhaps with an example?\n* Can the approach still be effective with even fewer gradient accumulation steps (e.g., less than 0.02% steps)？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jreQZXCgk9", "forum": "bxzJorqyYM", "replyto": "bxzJorqyYM", "signatures": ["ICLR.cc/2026/Conference/Submission15506/Reviewer_QPt3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15506/Reviewer_QPt3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898145390, "cdate": 1761898145390, "tmdate": 1762925793593, "mdate": 1762925793593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To all reviewers:  Theory of Merging in GradPruner"}, "comment": {"value": "In principle, **GradPruner is theoretically equal to the isotropic merging method with adaptive importance weights** . Furthermore, **this adaptive importance weighting scheme can also be similarly applied to Fisher merging**. We will elaborate on these two parts in detail below.\n\n### Isotropic merging with adaptive importance weights\nTo merge $M$layers, isotropic merging with per-layer weights [1] approximates the posterior distribution of each layer using an isotropic Gaussian whose mean is set to the layer’s parameters. This approach introduces layer-specific scalar hyperparameters $\\lambda_i$, for $i \\in \\\\{1,\\ldots,M \\\\}$, which can be formally expressed as $\\theta^* = \\arg\\max_{\\theta} \\sum_{i=1}^M \\lambda_i \\log p(\\theta \\mid \\theta_i, I),$\nwhere $p(\\theta \\mid \\theta_i, I)$denotes the probability density function of the isotropic Gaussian posterior, and the hyperparameters satisfy $\\lambda_i \\ge 0$and $\\sum_{i=1}^M \\lambda_i = 1$. These hyperparameters govern the relative importance assigned to each layer during the merging process. For instance, if all layers are assumed to contribute equally, one may set $\\lambda_i = \\frac{1}{M}$for all $i$. **However, this approach suffers from two primary limitations**:  (1) How should the importance weights be adaptively determined when the layers exhibit differing levels of importance?  (2) When individual weights within the same layer possess varying degrees of importance, how can adaptive weight-specific coefficients be assigned?\n\n**To address these challenges, our GradPruner introduces an adaptive weighting scheme.** Specifically, consider two layers to be merged: let $W_r$denote the base layer to be retained, and $W_m$the layer to be merged. To assign appropriate weights $\\lambda_r^{j}$and $\\lambda_m^{j}$to the $j$-th parameter entries $W_r^{j}$ and $W_m^{j}$, **the algorithmic procedure of GradPruner can be formally characterized by the following adaptive weight assignment function**:\n\n$ (\\lambda_r^{j}, \\lambda_m^{j})= (0.5, 0.5),  if(\\frac{\\partial L}{\\partial W_m^j})^2 \\ge T \\text{ and } \\text{Sign}(W_m^j)=\\text{Sign}(W_r^j), $\n\n$ (\\lambda_r^{j}, \\lambda_m^{j})=(1.0, 0.0), \\text{otherwise}$\n\nwhere $T$ denotes a pruning threshold. This formulation implies that if the squared gradient magnitude of a weight in the merged layer exceeds the threshold $T$ and its sign aligns with that of the corresponding weight in the retained layer, both weights are deemed equally important and are assigned equal coefficients. Otherwise, the weight from the merged layer is considered negligible, and only the retained weight is preserved.\n\n**GradPruner’s importance criterion simultaneously leverages (1) magnitude**—quantified by the squared gradient—**and (2) directional consistency**—captured by the agreement in sign between the two weights—as complementary indicators of parameter significance. This design integrates insights from prior work: methods such as [1,4] emphasize the utility of gradient magnitude in assessing parameter importance, while approaches like [2,3] demonstrate the efficacy of sign consistency as a relevance signal. GradPruner thus unifies both perspectives into a single adaptive framework.\nConsequently, the merged weight under isotropic merging with adaptive importance weights is given by  $\\theta^j = \\lambda_r^j \\theta_r^j + \\lambda_m^j \\theta_m^j.$\n\n### Integrating GradPruner’s Adaptive Weights with Fisher Merging\nAs outlined above, GradPruner fundamentally constitutes an adaptive strategy for determining importance weights. In this work, we apply this strategy to isotropic merging. Moreover, we demonstrate that GradPruner’s adaptive weighting mechanism can be seamlessly extended to Fisher Merging (FM) [1].\n\nSimilar to isotropic merging, FM also formulates the merging objective as  \n$\\theta^* = \\arg\\max_{\\theta} \\sum_{i=1}^M \\lambda_i \\log p(\\theta \\mid \\theta_i, I),$but differs in that it employs the Laplace approximation to the posterior $p$, yielding a Gaussian approximation $\\mathcal{N}(\\theta, H^{-1})$, where $H$is the Hessian matrix. To render computation tractable, FM approximates the Hessian using the diagonal of the Fisher information matrix $F$. The resulting closed-form expression for the merged parameter is \n$\\theta^j = \\frac{\\sum_{i=1}^M \\lambda_i F_i^j \\theta_i^j}{\\sum_{i=1}^M \\lambda_i F_i^j}.$ \n\nWhen integrating GradPruner with FM, the adaptive weighting scheme modifies only the assignment of $\\lambda_i$, leaving the computation of the Fisher matrix unchanged. Therefore, in the case of merging two layers, the final merged weight becomes  \n$\\theta^j = \\frac{ {\\lambda_r^j F_r^j \\theta_r^j + \\lambda_m^j F_m^j \\theta_m^j}}{{\\lambda_r^j F_r^j  + \\lambda_m^j F_m^j }}$ .\n\n### Reference\n\n[1] Merging layers with Fisher-Weighted Averaging\n\n[2] NegMerge: Sign-Consensual Weight Merging for Machine Unlearning\n\n[3] TIES-Merging: Resolving Interference When Merging Models\n\n[4] Model Merging by Uncertainty-Based Gradient Matching"}}, "id": "VRzfir7tVI", "forum": "bxzJorqyYM", "replyto": "bxzJorqyYM", "signatures": ["ICLR.cc/2026/Conference/Submission15506/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15506/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15506/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763348940142, "cdate": 1763348940142, "tmdate": 1763348940142, "mdate": 1763348940142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GradPruner, a gradient-guided structured pruning framework designed to enhance both fine-tuning and inference efficiency for large language models (LLMs). Unlike most existing structured pruning methods that rely on calibration data or additional distillation steps, GradPruner leverages initial gradient information obtained during the first few LoRA fine-tuning steps to estimate parameter and layer importance.\n\nThe key innovation lies in computing an Initial Gradient Information Accumulation Matrix (IGIA-Matrix) to quantify layer importance early in training. Based on this metric, GradPruner performs layer-level pruning followed by a layer merging step that sparsifies and merges pruned layers into adjacent retained layers while resolving sign conflicts to reduce destructive interference.\n\nEmpirically, GradPruner achieves a 40% reduction in parameters with only 0.99% loss in downstream accuracy, tested on two LLMs (LLaMA3.1-8B and Mistral-7B) across eight diverse benchmarks spanning medical, financial, and general reasoning tasks. It shows consistent performance improvements over strong structured pruning baselines (LLM-Pruner, LaCo, SAT, APT, and MINITRON), and reduces both training and inference time/memory by over 35%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The use of IGIA-Matrix computed from <1% of training steps is original and empirically justified by gradient sensitivity analysis.\n\n\n2. The proposed sign-consistent merging technique effectively preserves accuracy even under 40% pruning.\n\n\n3. The authors test across multiple domains, model sizes, and fine-tuning regimes with strong baselines, demonstrating robustness.\n\n\n4. Substantial reductions in both training and inference costs (~35–40%) while maintaining accuracy are practically valuable.\n\n\n5. The paper analyzes pruning ratios, merging counts, and alternatives such as kernel pruning and weighted averaging."}, "weaknesses": {"value": "1. While the empirical gradient correlation study is convincing, the paper lacks a deeper theoretical analysis of why early gradient accumulation correlates with long-term importance, beyond empirical observation.\n\n\n2. The method assumes access to LoRA gradients and may not generalize to non-LoRA or adapter-free fine-tuning setups.\n\n\n3. The layer-importance estimation could behave differently for tasks with varying gradient noise; this is not fully explored."}, "questions": {"value": "1. How sensitive is GradPruner’s performance to the number of initial fine-tuning steps ttt?\n\n\n2. Have the authors tested GradPruner when using other adapter methods (e.g., QLoRA, DoRA) to verify whether IGIA-Matrix remains stable?\n\n\n3. In the merging phase, how is the sparsity rate ppp selected? Could adaptive sparsity (learned from IGIA statistics) yield further improvements?\n\n\n4. Does GradPruner preserve the same inference graph (number of layers) after merging, or does merging affect sequence length/runtime at deployment?\n\n\n5. Can GradPruner be integrated with post-training quantization or low-rank compression? If so, how does the gradient-based importance interact with quantization noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "su1mW5S49D", "forum": "bxzJorqyYM", "replyto": "bxzJorqyYM", "signatures": ["ICLR.cc/2026/Conference/Submission15506/Reviewer_87rX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15506/Reviewer_87rX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963097170, "cdate": 1761963097170, "tmdate": 1762925793223, "mdate": 1762925793223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **GradPruner**, a lora-based gradient-guided **layer pruning + sign-based layer merging** method for efficient fine-tuning and inference. The method (i) computes an **Initial Gradient Information Accumulation (IGIA)** matrix from early-step LoRA gradients to score parameter/layer importance, (ii) prunes layers with low summed IGIA (over linear sublayers), and (iii) **sparsify-then-merge** pruned layers into the preceding retained layer using a **sign-consistency rule**. Experiments on two LLMs (Llama-3.1-8B, Mistral-7B) and eight datasets report ~**40% sparsity** with ~**0.99%** average degradation; the headline comparison (Table 1) is at **40% sparsity**."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- **Simple, pragmatic pipeline.** Early-step gradient accumulation → IGIA-based layer scoring (sum over linear sublayers) → pruning → sign-based merging. The pruning score is clearly defined.\n- **Operationally clear merging.** “Top-p% by IGIA then sign-consistent addition into the preceding kept layer” is straightforward to implement and shown with a framework figure.\n- **Broad task coverage with efficiency reporting.** Ablations include number of merged layers and sparsity-rate sweeps for the proposed method."}, "weaknesses": {"value": "### (A) Insufficient theoretical grounding for **merging** (most important)\n- **Self-inconsistency between pruning and merging.** The paper emphasizes that **layers differ in importance** and uses **IGIA** to make importance-aware pruning decisions. However, during **merging**, contributions from pruned layers are **added with equal weight** whenever signs match—**without any sensitivity weighting** (e.g., IGIA- or Fisher-based) for either donor or receiver layers. This disconnect undermines the rationale that sensitivity should matter.\n- **Cross-layer addition lacks justification.** There is no theoretical argument that **elementwise addition across different Transformer blocks** preserves function or yields bounded error, even after sparsification. A first-order approximation, Hessian/Fisher weighting, or Lipschitz-based stability discussion is missing.\n- **Actionable request.** Provide a clear **merging objective** (e.g., minimize a first-order loss surrogate), an **error bound**, or at least **IGIA-weighted** or **Fisher-weighted** merges. Otherwise, the method uses importance for pruning but not for merging.\n\n### (B) Comparisons are narrow in **pruning ratio** and **model scale**\n- **Single-ratio comparisons.** The main table compares methods **only at 40% sparsity**. While internal ablations vary sparsity for GradPruner, there is **no multi-ratio cross-method** comparison to show whether the advantage holds when pruning is milder or more aggressive.\n- **Limited model scaling.** Results center on **7–8B** models (plus a 3B FT reference). It is unclear whether gains **transfer down** to ~1–2B or **up** to ~14B+ models.\n- **Actionable request.** Report **accuracy–sparsity curves** for **multiple baselines**, and include **smaller (≈1.7B)** and **larger (≈14B)** models.\n\n### (C) Training recipe & domain generalization\n- The domain-specific fine-tuning often appears to train and test within the **same dataset**. This setup makes it hard to assess **out-of-distribution** robustness within the domain.\n- **Actionable request.** Include **cross-dataset** evaluations per domain (e.g., train on one medical QA dataset, test on another; for math-style settings: train on MetaMathQA-40k, evaluate on GSM8K / GSM-Plus) to demonstrate generalization beyond the training distribution.\n\n### (D) Minor Issues / Clarity\n- **Equation (2) ambiguity/typo.** The equation seems to multiply **the B-gradient twice**, which is likely a typo and inconsistent with LoRA structure. Please clarify the intended mapping and dimensional compatibility.\n- **Citation formatting.** Around **line ~310**, the first paragraph’s citation is not enclosed in parentheses, unlike others; please standardize the style.\n- **Small typos.** A few truncated terms (e.g., “IGIA-Matri”) and punctuation/spacing glitches around the merging equation."}, "questions": {"value": "please see weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "knuFeDltP6", "forum": "bxzJorqyYM", "replyto": "bxzJorqyYM", "signatures": ["ICLR.cc/2026/Conference/Submission15506/Reviewer_RvRE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15506/Reviewer_RvRE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968759423, "cdate": 1761968759423, "tmdate": 1762925792822, "mdate": 1762925792822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}