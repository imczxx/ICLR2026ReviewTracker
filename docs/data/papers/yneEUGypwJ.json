{"id": "yneEUGypwJ", "number": 23748, "cdate": 1758347878858, "mdate": 1759896798814, "content": {"title": "Attention Guided Alignment in Efficient Vision-Language Models", "abstract": "Large Vision-Language Models (LVLMs) rely on effective multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs) to achieve seamless integration of visual and textual information. This paper investigates the embedding spaces generated by a representative vision encoder (ViT) and a powerful LLM (Vicuna), uncovering a critical disparity. Our analysis reveals that ViT token embeddings exhibit a surprisingly uniform distribution, lacking the rich semantic structure inherent in Vicuna's LLM embeddings. This absence of a well-defined semantic space in visual token embeddings poses a significant challenge to multimodal alignment, hindering the model's ability to establish meaningful correspondences between visual and textual elements. We demonstrate the implications of this embedding space divergence through a rigorous analysis of statistical properties. We argue that bridging this semantic gap requires complex mappings, ultimately limiting current LVLMs' multimodal reasoning capabilities. These findings provide valuable insights for future research aimed at developing more effective alignment strategies and achieving enhanced visual and linguistic understanding in LVLMs.", "tldr": "", "keywords": ["Object Hallucination", "Vision language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e5df012ee72ac23da280c5fa24a34d8edac80c74.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AGE-VLM, an efficient VLM that reduces hallucination by interleaving cross-attention into a 1B LLM and supervising these attentions with SAM-derived segmentation masks via a Dice loss. The authors utilize ConvNext encoder and train the model with a four-stage-recipe, including alignment, vision adaptation, SAM-guided grounding, and instruction tuning. AGE-VLM achieves better or comparable results to baselines such as ConvLLaVA and obileVLM-v2."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is clearly-motivated and well-written.\n2.\tThe idea of using attention alignment to improve the visual grounding capabilities of LVLM is very intuitive and makes a lot of sense.\n3.\tThe use of SAM model for generating reference attention masks prevents the labeling process for GT attention, making the method efficient and scalable."}, "weaknesses": {"value": "1.\tWhile it is inspiring to improve the LVLM through attention alignment, the specific method used to align the attention (e.g., loss, optimization algorithm, architecture) is simple and standard. \n2.\tIn Sec. 3.1, the authors examine visual–textual similarity and identify misalignment between visual and textual tokens. They then propose cross-attention as a superior architecture. However, many early VLMs already employed cross-attention to connect visual encoders with LLMs, whereas most recent models have shifted toward using simple adapters to project visual features into the textual representation space. This design appears to reverse that trend. Could the authors clarify the rationale for this choice?\n3.\tI am not sure whether SAM can give proper segmentation result when the prompt is complex or entails certain reasoning. And the SAM seems to fail in some OCR tasks. The authors should explain more about this.\n4.\tThe performance of AGE-VLM-LM is inferior to AGE-VLM, which shows that attention alignment during visual instruction tuning will harm the performance. This is counterintuitive. Moreover, I wonder how well the attention alignment will be retained if there is no such regularization in the visual instruction tuning stage. The authors can explain more about this.\n5.\tIn table 4, does ours refer to AGE-VLM or AGE-VLM-LM?  Besides the qualitative examples, are there any quantitative metrics to measure the alignment?  \n6.\tThe authors only include two baselines. More baselines should be considered such as those mentioned in the related work section (VILA, TinyLLaVA, Mini-Gemini)."}, "questions": {"value": "See the questions in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gDVhUsc3Ld", "forum": "yneEUGypwJ", "replyto": "yneEUGypwJ", "signatures": ["ICLR.cc/2026/Conference/Submission23748/Reviewer_HTfK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23748/Reviewer_HTfK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761319142321, "cdate": 1761319142321, "tmdate": 1762942789535, "mdate": 1762942789535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the topic of hallucinations related to visual grounding caused by imperfect image-text token alignment in multi-modal models, demonstrates that image and text representations are not well correlated in architectures that concatenate text and visual tokens, proposes a cross-attention based architecture with a novel segmentation grounded loss and shows strong results on tasks that benefit from visual grounding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper tackles an important problem of object hallucinations in multimodal models and proposes a novel idea to guide attention to focus on relevant areas of the image using text grounded segmentation masks.\n2.\tThe experimental setup is well formulated including multiple stages of pre-training and instruction fine tuning with the introduction of segmentation grounded loss in some stages. Specific focus is applied on maintaining language modeling performance.\n3.\tThe results are well presented with evaluations covering different types of multimodal tasks like spatial reasoning, OCR, object detection among others. The qualitative examples are useful."}, "weaknesses": {"value": "1.\tSection 3.1 computes cosine similarity between final-layer hidden states at image-token vs text-token positions on matched and mismatched pairs, but it doesn’t clearly justify why hidden states are used instead of similarities in Q/K-space (Eg: cos(W_{q}h_{t}, W_{k}h_{v})\n) which drive attention. It is unclear whether earlier/middle layers exhibit different alignment. How multiple tokens per modality are reduced is also not mentioned.\n2.\tSegmentation grounding loss is applied to 10% of the samples. Some ablations that show how this parameter affects learning would have been beneficial. \n3.\tApplying grounding loss to instruction following stage adversely affects performance on most of the benchmarks. This hasn’t been sufficiently addressed with observations of model behavior with and without this loss.\n4.\tEvaluations on language-only tasks and other general multimodal tasks including those which go beyond spatial grounding (like visual description) are missing. \n5.\tMinor typos and missing spaces."}, "questions": {"value": "1.\tCould the authors provide clarifications why the performance drops if grounding loss is applied in the instruction following stage?\n2.\tHow was the fraction 10% arrived upon while applying grounding loss and are there any ablations to show how performance changes by varying this value?\n3.\tHow do other general image understanding tasks which do not depend on object masks fare with this technique? Can this technique be scaled to state of the art general purpose multimodal models or do the loss functions need further modifications for different tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4JMaOM8fcg", "forum": "yneEUGypwJ", "replyto": "yneEUGypwJ", "signatures": ["ICLR.cc/2026/Conference/Submission23748/Reviewer_bU3e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23748/Reviewer_bU3e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638514199, "cdate": 1761638514199, "tmdate": 1762942789213, "mdate": 1762942789213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AGE-VLM, a vision–language model that introduces interleaved cross-attention layers into a small LLaMA-1B backbone. The main idea is to guide the attention maps using segmentation masks distilled from the Segment Anything Model (SAM), improving spatial grounding and reducing hallucination. The method is trained in four stages that combine visual–text alignment, SAM-guided grounding, and instruction tuning. The numerical experiments suggest moderate but consistent improvements over prior efficient VLMs, with more clear visual localization and fewer hallucinated objects."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper discusses a relevant problem for efficient vision–language models.\n2. The proposed SAM-guided cross-attention design is interpretable and integrates spatial grounding signals into a lightweight architecture without significant computational costs."}, "weaknesses": {"value": "1. The SAM-guided cross-attention mechanism is relatively straightforward and appears as a simple extension of prior attention-alignment and grounding approaches.\n2. The reported improvements do not look significant, and the method does not outperform the baseline by a considerable margin.\n3. The role of each training stage and the specific contribution of the SAM supervision are not clearly disentangled, making it difficult to assess which component drives the observed gains."}, "questions": {"value": "1. Can the authors clarify what distinguishes the proposed SAM-guided cross-attention from prior attention-guidance or grounding methods beyond using SAM masks for supervision?\n2. Can the authors provide ablation results isolating the effect of the SAM guidance and each training stage to clarify which component contributes more to the numerical results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5qxcKFpVzm", "forum": "yneEUGypwJ", "replyto": "yneEUGypwJ", "signatures": ["ICLR.cc/2026/Conference/Submission23748/Reviewer_aEQA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23748/Reviewer_aEQA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820755543, "cdate": 1761820755543, "tmdate": 1762942788970, "mdate": 1762942788970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets object hallucination and weak visual grounding in efficient VLMs that use concatenation of visual/text tokens. The authors diagnose the issue via cosine-similarity analyses showing that hidden states for matching and non-matching image–text pairs overlap substantially, indicating poor multimodal alignment. They propose AGE-VLM, which (i) interleaves cross-attention layers inside a small LLM (LLaMA-1B) and (ii) guides those cross-attention maps using masks distilled from (Grounded) SAM during pretraining/fine-tuning (“attention-guidance loss”), so the model “looks” at the right regions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The similarity-distribution study nicely evidences why concatenation architectures blur matching vs. non-matching pairs—useful and reproducible diagnostic\n- Distilling SAM masks into cross-attention (not the vision backbone) is conceptually clean and data-efficient; the dice loss formulation is appropriate for sparse regions\n- Interleaving cross-attention in a 1B LLM while mostly freezing self-attention preserves language priors and keeps training economical; the staged plan is easy to adopt\n- Quantitative tables plus qualitative heatmaps support the claim that AGE-VLM reduces hallucination and improves localization"}, "weaknesses": {"value": "- Evaluation protocol introduces an external judge: For CV-Bench, accuracy is computed by Qwen-L because models sometimes omit option letters, this can inject evaluator bias and hides raw option-selection accuracy\n- Heavy reliance on SAM/Grounded-SAM: Performance hinges on third-party segmentation quality and prompt engineering; generalization when masks are imperfect/noisy is not stress-tested (authors acknowledge broader-impact limits)\n- There is a CA-baseline (cross-attention without guidance), but more granular ablations (e.g., how many cross-attn layers, which tokens supervised, mask quality/noise curves) are not detailed in the main text.\n- Narrow model/backbone space: Results center on ConvNeXt+LLaMA-1B; comparisons to stronger small-VLM baselines or ViT backbones at similar token budgets are limited, despite token-efficiency claims"}, "questions": {"value": "- The overlap in hidden-state cosine similarities is compelling, but does improving that metric necessarily reduce hallucination—or does the SAM-guided training improve both due to an external supervision signal?\n- Using Grounded-SAM (Florence-2 + SAM) to create supervision during both pretraining and instruction fine-tuning could introduce benchmark leakage if those systems saw similar data distributions.More detail on dedup/filtering and robustness to mask errors would help"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TdqeenBPYg", "forum": "yneEUGypwJ", "replyto": "yneEUGypwJ", "signatures": ["ICLR.cc/2026/Conference/Submission23748/Reviewer_fCwf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23748/Reviewer_fCwf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977484546, "cdate": 1761977484546, "tmdate": 1762942788780, "mdate": 1762942788780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}