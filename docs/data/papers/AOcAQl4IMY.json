{"id": "AOcAQl4IMY", "number": 11310, "cdate": 1758196015416, "mdate": 1759897594541, "content": {"title": "VideoCogQA: A Controllable Benchmark for Evaluating Cognitive Abilities in Video-Language Models", "abstract": "Recent advances in Large Video-Language Models (LVLMs) have led to promising results in multimodal video understanding. However, it remains uncertain whether these models possess the key cognitive capabilities for high-level tasks, especially those requiring symbolic and abstract reasoning. Existing benchmarks predominantly rely on real-world, annotated videos, which suffer from a lack of control over content and inherent difficulty, limiting their diagnostic utility. To address these limitations, we introduce \\textbf{VideoCogQA}, a scalable and fully controllable benchmark inspired by game-based environments, designed to assess the cognitive abilities of LVLMs. By generating synthetic videos through a programmatic engine, VideoCogQA offers precise control over visual elements, temporal dynamics, and the video task difficulty, effectively isolating cognitive reasoning from prior semantic knowledge. The dataset consists of tasks involving abstract concepts, symbolic elements, and multimodal integration, with varying levels of difficulty based on Python-based game scenarios. Experimental results show that even state-of-the-art (SOTA) models, such as Qwen2.5-VL-72B, achieve an average performance of 48.8% on tasks involving abstract concepts. Additionally, performance drops by 15% as task complexity increases, highlighting the challenges LVLMs face in maintaining consistent performance. Through this work, we hope to show the limitations of current LVLMs and offer insights into how they can more effectively emulate human cognitive processes in the future.", "tldr": "We introduce VideoCogQA, a controllable synthetic benchmark to evaluate the cognitive abilities of LVLMs, revealing significant limitations in abstract and symbolic reasoning even for SOTA model", "keywords": ["synthetic", "controllable", "video-LLM", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0e3d3ec214254fc1f9df9254b731b516a09db47.pdf", "supplementary_material": "/attachment/6afc9bacbfeac853510f5535307f16f48e418a09.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical gap in evaluating Large Video-Language Models (LVLMs): the lack of controllable benchmarks to assess high-level cognitive capabilities (e.g., symbolic reasoning, abstract concept understanding) beyond basic semantic comprehension. Existing benchmarks rely on real-world annotated videos, which suffer from limited content control and difficulty in isolating cognitive reasoning from prior semantic knowledge. To solve this, the authors propose VideoCogQA, a scalable and fully controllable benchmark built on programmatic synthetic videos inspired by game environments (e.g., maze navigation, sky battles).\n\nVideoCogQA uses a Python-based pipeline to generate 800 videos and 3,280 questions across 10 game scenarios, with three difficulty levels (Easy/Medium/Hard) controlled via code parameters (e.g., grid size in Chameleon Grid, enemy count in Sky Battle). It evaluates six cognitive dimensions: Object Perception (OP), Action Perception (AP), Temporal Reasoning (TR), Spatial Reasoning (SR), Game-environment Perception (GP), and Full-modal Perception (FP)—expanding beyond the scope of existing video benchmarks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work introduces a novel paradigm for LVLMs evaluation by leveraging programmatic synthetic videos to isolate cognitive reasoning from prior semantic knowledge—addressing a fundamental limitation of real-world video benchmarks (e.g., MVBench, Video-MME) that rely on contextual cues (e.g., playground scenes for action inference). Inspired by cognitive science (game-based human cognition studies), VideoCogQA’s 10 game scenarios and six cognitive dimensions (especially GP and FP) expand the scope of video-LM evaluation beyond existing frameworks, which focus primarily on semantic understanding. The combination of Python-driven controllability, GPT-4 QA generation, and fine-grained difficulty tuning is a creative integration of existing tools to solve a new problem."}, "weaknesses": {"value": "1. Insufficient Annotation of Frame Sampling Requirements:  A major limitation of the synthetic pipeline is the lack of frame sampling annotations for each video scenario. As noted in the reviewer’s comment, tasks like Maze Runner (8×8 maze requiring 14 steps to solve) may be unsolvable with small frame samples (e.g., 8 frames), as critical steps would be missed. The paper mentions evaluating models with their \"official default inference settings\" (Section 4.1) but does not:\n  - Define the minimum number of frames (N=8/16/32/64) required to solve each task/difficulty level.\n  - Analyze how frame sampling impacts performance (e.g., whether Qwen2.5-VL-72B’s 54.1% average accuracy drops further with 8 frames vs. 64 frames for Maze Runner).\nThis omission weakens the interpretability of results—poor performance on a task could stem from model limitations or insufficient frame sampling, not just cognitive gaps.\n2. Ambiguous Human Evaluation Setup: \nThe human performance benchmark (90.3% accuracy) lacks critical details, making it hard to compare with model performance:\nThe paper states human accuracy is the \"average of two independent annotators\" (Section 4.1) but does not clarify:\n  - Whether humans accessed the full video or only sampled frames (consistent with model inputs). If humans used full videos, their 90.3% accuracy may understate the gap (models are at a disadvantage with limited frames).\n  - The viewing protocol: Did annotators see questions before or after watching the video? Did they watch once or multiple times? A  \"question-first\" setup (common in QA tasks) would likely yield near-100% accuracy for humans, so the 90.3% error rate needs explanation (e.g., ambiguous questions, fast-paced videos).\nWithout this clarity, the human baseline cannot effectively contextualize model limitations.\n3. Coarse-Grained Correlation Analysis: \nThe paper computes correlation coefficients at the dataset level (e.g., VideoCogQA vs. VideoMME, Table 5) but not at the cognitive dimension level. Dataset-level correlations mask whether VideoCogQA’s individual cognitive dimensions are valid proxies for real-world capabilities. A dimension-specific analysis would better validate the benchmark’s diagnostic utility."}, "questions": {"value": "1. Frame Sampling Requirements: For each of the 10 scenarios (e.g., Maze Runner, Time Sequence) and difficulty levels, could you provide the minimum number of frames (N=8/16/32/64) required to answer questions correctly?\n2. Human Evaluation Details: Could you specify the human annotation protocol:\n  - Did annotators use full videos or sampled frames (matching model inputs)?\n  - Did they view questions before or after watching the video? How many times could they watch?\n  - What caused the 9.7% human error rate (e.g., question ambiguity, fast video pace)?\nThis would strengthen the human baseline as a meaningful comparison for model performance.\n3. Dimension-Specific Correlation: Could you compute correlation coefficients between VideoCogQA’s six cognitive dimensions (OP/AP/TR/SR/GP/FP) and corresponding dimensions in real-world benchmarks? \n4. Error Attribution: Could you add a supplementary analysis distinguishing between perception and reasoning errors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jR5Qxi9OuM", "forum": "AOcAQl4IMY", "replyto": "AOcAQl4IMY", "signatures": ["ICLR.cc/2026/Conference/Submission11310/Reviewer_hK4s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11310/Reviewer_hK4s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459774272, "cdate": 1761459774272, "tmdate": 1762922451751, "mdate": 1762922451751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VideoCogQA, a synthetic, controllable video benchmark designed to probe cognitive abilities in Large Video-Language Models (LVLMs), in which videos are generated programmatically, difficulty is tuned via explicit code parameters, and multiple-choice questions are created from GPT-4-authored templates. The benchmark totals 800 videos and 3,280 questions over ten game-inspired scenes with easy/medium/hard settings.  The work claims novelty in explicit controllability and difficulty. \n\nEmpirically, strong LVLMs (e.g., Qwen2.5-VL-72B, GPT-4o) still trail humans by a large margin on symbolic tasks, performance drops with difficulty, and results correlate highly when varying frame sampling and when compared to several real-world video benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Controllability & difficulty. Clear, code-level knobs (e.g., grid size) allow precise difficulty control, improving diagnostic value.\n2. Breadth of skills. Ten diverse scenes spanning object/action perception, spatial/temporal reasoning, game environment understanding, and audio-visual mapping.  \n3. Well-Documented Human–Model Gap. The paper clearly reports a substantial gap between human and model performance across all tasks and scenarios."}, "weaknesses": {"value": "1. Lack of random baseline. With 3–5 options, the performance of random choice can be 20–33%. This paper does not foreground a random baseline. \n2. Lack of connection to Real-World Tasks. The paper does not extensively discuss the connection between VideoCogQA and real-world tasks. The current justification, based primarily on frame sampling, is insufficient. It remains unclear whether performance on specific VideoCogQA tasks correlates with performance on real-world tasks. Clarifying whether success on particular tasks within VideoCogQA is predictive of performance in specific real-world scenarios would significantly strengthen the practical relevance of the benchmark."}, "questions": {"value": "1. What is the average of choices per question in the dataset, and what is the corresponding random baseline accuracy?\n2. Is model performance on VideoCogQA correlated with performance on other existing benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5A3x7a1IEq", "forum": "AOcAQl4IMY", "replyto": "AOcAQl4IMY", "signatures": ["ICLR.cc/2026/Conference/Submission11310/Reviewer_eqYz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11310/Reviewer_eqYz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548132395, "cdate": 1761548132395, "tmdate": 1762922451319, "mdate": 1762922451319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark called VideoCogQA, designed to evaluate the cognitive abilities of video-language models. The benchmark automatically generates question–answer (QA) pairs by creating synthetic videos from simple games and combining them with predefined text templates. It also allows for controllable difficulty adjustment. When evaluated using existing state-of-the-art (SOTA) models, the performance remained relatively low at around 48.8%, indicating significant room for improvement. Thus, VideoCogQA serves as a valuable benchmark for identifying the current limitations of video-language models and establishing new research goals aligned with those limitations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "​\n- The paper demonstrates that synthetic videos can be automatically generated from a game simulation engine, and that LLM-based instruction templates are created for each game according to predefined question categories. This approach enables dataset generation at scale, without being constrained by data size. To support this, the authors propose a Python-based video synthesis pipeline.\n\n\n- The authors introduce VideoCogQA, a scalable and fully controllable benchmark. This benchmark is well-organized, consisting of six categories (OP, AP, TR, SR, GP, FP) and three difficulty levels (easy, medium, difficult).\n\n\n- The paper conducts and analyzes extensive experiments across various Large Vision-Language Models (LVLMs)."}, "weaknesses": {"value": "- Lack of Details on Dataset Distribution\n\n\n   - The paper does not provide sufficient details or analysis regarding the dataset distribution. It would be beneficial to include a detailed breakdown of the number of samples per category, organized by game and by difficulty level. The current explanation in Section 3.2 is largely textual and difficult to fully understand.\n\n\n   - It would also be helpful to report how each game covers the different question categories, and how the VLM (Vision-Language Model) performances vary across these categories.\n\n\n   - Additionally, a comparative analysis of the data distribution between VideoCogQA and existing benchmarks would strengthen the evaluation and contextual understanding of the dataset.\n\n\n- Limited Relevance to Real-World Scenarios\n\n\n   - While generating synthetic videos from games to evaluate cognitive abilities is an innovative idea, it remains unclear how such synthetic settings translate to real-world problems. There is uncertainty about whether this approach truly enhances real-world understanding.If the benchmark includes a training split, one way to validate its practical relevance would be to fine-tune models on VideoCogQA and evaluate them on other benchmarks to assess transferability. However, in the current setting, the paper should either demonstrate or justify the real-world applicability of the benchmark in another way."}, "questions": {"value": "Please provide your responses with reference to the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XvFz7t4snZ", "forum": "AOcAQl4IMY", "replyto": "AOcAQl4IMY", "signatures": ["ICLR.cc/2026/Conference/Submission11310/Reviewer_xcbe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11310/Reviewer_xcbe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835807374, "cdate": 1761835807374, "tmdate": 1762922450916, "mdate": 1762922450916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VideoCogQA. VideoCogQA is a synthetic, fully controllable benchmark for testing video–language models on object/action perception, temporal/spatial reasoning, gameplay stats, and simple audio–visual links. Models trail human performance and get worse as tasks grow harder; when videos are replaced with clean code-log text, accuracy jumps, suggesting the main bottleneck is visual perception rather than language reasoning. Scaling helps but doesn’t close the gap, pointing to the need for stronger spatiotemporal encoders and better symbolic perception."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark adds Game-environment and Full-modal, explicitly targeting symbolic/abstract attributes (size, color, shape) and temporal/spatial relations.\n\n2. The authors programmatically synthesize videos with parameterized difficulty and log code-level events, then generate QA templates with GPT-4 and human filtering."}, "weaknesses": {"value": "1. Question templates originate from GPT-4 and are then filtered; more auditing of prompt templates and filtering criteria may strengthen validity claims and reproducibility.\n\n2. The “~90% human” number isn’t well documented. We don’t know how many people were tested, how much time they had, whether they could replay the video, or how consistent the labels were. That makes the human ceiling hard to trust and compare against models."}, "questions": {"value": "1. If you swap in a stronger vision stack—say, a structured front-end with detection/tracking/attributes, or a higher-capacity spatiotemporal backbone—does overall accuracy go up, and would that change your conclusion about the main bottleneck?\n\n2. Do difficulty levels align with human-perceived difficulty (item-response theory or psychometrics)? Any per-item discrimination analysis?\n\n3. If a model is trained on the synthetic tasks, does it transfer to natural-video QA, and in which categories?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cgJhYCHlqa", "forum": "AOcAQl4IMY", "replyto": "AOcAQl4IMY", "signatures": ["ICLR.cc/2026/Conference/Submission11310/Reviewer_s34o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11310/Reviewer_s34o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871690335, "cdate": 1761871690335, "tmdate": 1762922450317, "mdate": 1762922450317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}