{"id": "ndilONnABZ", "number": 3806, "cdate": 1757530056838, "mdate": 1759898068926, "content": {"title": "AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception", "abstract": "Real-world contact-rich manipulation demands robots to perceive temporal tactile feedback, capture subtle surface deformations, and reason about object properties and force dynamics.\nAlthough optical tactile sensors are uniquely capable of providing such rich information, existing tactile datasets and models remain limited. These resources primarily focus on object-level attributes (e.g., material) while largely overlooking fine-grained temporal dynamics.\nWe consider that advancing dynamic tactile perception requires a systematic hierarchy of dynamic perception capabilities to guide both data collection and model design.\nTo address the lack of tactile data with rich dynamic information, we present ToucHD, a large-scale tactile dataset spanning tactile atomic actions, real-world manipulations, and touch-force paired data.\nBeyond scale, ToucHD establishes a comprehensive dynamic data ecosystem that explicitly supports hierarchical perception capabilities from the data perspective.\nBuilding on it, we propose AnyTouch 2, a general tactile representation learning framework for diverse optical tactile sensors that unifies object-level understanding with fine-grained, force-aware dynamic perception. The framework captures both pixel-level and action-specific deformations across frames, while explicitly modeling physical force dynamics, thereby learning multi-level dynamic perception capabilities from the model perspective.\nWe evaluate our model on benchmarks that covers static object properties and dynamic physical attributes, as well as real-world manipulation tasks spanning multiple tiers of dynamic perception capabilities—from basic object-level understanding to force-aware dexterous manipulation. Experimental results demonstrate consistent and strong performance across sensors and tasks, highlighting the framework’s effectiveness as a general dynamic tactile perception model.", "tldr": "", "keywords": ["Tactile Representation Learning", "Tactile Dataset", "Dynamic Tactile Perception"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/286362d2ce37e7149b3ad3e1fbc9bf3983a29d9b.pdf", "supplementary_material": "/attachment/2e1185b3c921ac98e6c191442cf66d88f89fc885.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed AnyTouch 2 as a general tactile representation learning framework. \nBased on their massive hierarchical dataset ToucHD, AnyTouch 2 tried to unify static/dynamic pixel-level information, multi-modal alignment, and dynamic (delta) force information into unified representation, extending the previous AnyTouch 1 architecture to richer tactile information at different levels. \nExperiments on perception and manipulation tasks demonstrated the effectiveness of AnyTouch 2."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated and easy to follow. The contributions are sound and will benefit the tactile community.\n2. Categorizing tactile tasks into 5 levels provides a good insight into the community. Existing research mostly do not distinguish some of the levels where the tactile modality is actually playing different roles. This insight extends the design of Sparsh with convincing discussion and experiments. \n3. Comprehensive experiment and ablation with rich explanation and analysis."}, "weaknesses": {"value": "Simulating those sensors, especially those with markers, is non-trivial. It will be good to further explain the IMPM pipeline for dynamic scenes, as well as how those reconstructed meshes from IMPM is then rendered in Blender for your simulation."}, "questions": {"value": "1. In the multi-modal alignment ablation, do you think these two properties contradicts each other, or is it due to limitations of the text encoder or AnyTouch 2 training scheme?\n2. For the force data, please show some visualization of those non-planar sensors' reading. Currently I have no intuition if those sensor data will really help if they are never seen in other levels. An ablation is also appreciated if possible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EdwoiLMuba", "forum": "ndilONnABZ", "replyto": "ndilONnABZ", "signatures": ["ICLR.cc/2026/Conference/Submission3806/Reviewer_shzk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3806/Reviewer_shzk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975556154, "cdate": 1761975556154, "tmdate": 1762917044073, "mdate": 1762917044073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper present a tactile dataset and try to build hierarchical perception capabilities. The paper also present a tactile representation learning framework for optical tactile sensors, which fail to show the capabilities of tactile in manipulation.\n\nThe paper will be stronger if it focus more on the policy training and representation learning, instead of focusing too much on building such hierarchy pyramid. \n\nThe paper try to convey too many contents but loss effective evidence and support, making the paper loss focus.\n\nIncomplete in presentation. Some details on the policy training are missing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Good visualization. The research on related works are comprehensive."}, "weaknesses": {"value": "Minor weakness:\n\n\nMajor weakness:\n\n1. The proposed “tactile dynamic pyramid” is ambitious, but its practical utility is unclear. Tier boundaries are not operationally defined, and the paper does not show that organizing data by tiers yields better representations than training a single, unified model on pooled data with task-specific supervision. Please provide measurable criteria for each tier, evidence of annotation consistency, and ablations comparing (a) pyramid-driven training vs. (b) a unified representation or a task curriculum without tiers.\n\n2. Tier-1 is defined as “supporting precise force-sensitive manipulation,” yet much of this capability can be achieved with press-only data plus straightforward calibration[1], even with non-optical sensors (e.g., force-sensing resistor) that directly measure normal force. The manuscript also appears to conflate normal and shear forces. Please (i) specify which force components are observed/estimated (normal, shear, slip), (ii) describe the labels/ground truth and metrics used to distinguish them, and (iii) demonstrate that Tier-1 data provides incremental benefit over Tier-5 press-only data for force estimation and manipulation success. Force calibration is already contained in the gelsight paper[1].\n\nGelSight: High-Resolution Robot Tactile Sensors for Estimating Geometry and Force\n\n3. Key physical quantities (normal vs shear force, torque, friction coefficient, compliance) are mixed across tiers; some are learnable from press-only with calibrated loads, others require tangential motion. The pyramid does not map cleanly onto physics.\n\n4. With straightforward calibration (even non-optical FSR/arrays), normal force can be estimated robustly. Claiming T1 superiority without showing incremental benefit over calibrated T5 is unconvincing.\n\n5. What is the boundary between force data, manipulation data, and specific action? How do you support your claim that the manipulation dynamics support various complex real-world manipulation tasks, instead of a incremental benefit from the \"press-only\"? \n\n6. For Tire2, have you contact experiments to support you have extract manipulation dynamics from tactile information? It will be more convincing if your train a dynamics model on the manipulation data and try to prove if your dynamics model could build some casual relationships between tactile and manipulation dynamics of physics. Other additional experiments also work.\n\n7. In figure 3, please specify the difference for \"tactile grasping\" and \"chip moving\". It seems both two tasks all try to grasp fragile objects, its just the chip is even more fragile than egg, which require more sensitive tactile instead of require a different sensing modality.\n\n8. Line 306-307, \"Among these properties, contact force is fundamental, as it directly governs how objects deform, slip, or respond during manipulation\". Follow the authors claim, if contact force is such fundamental, why not directly use 6-axis force sensor on wrist or directly on the gripper. Then you can directly achieve the same capabilities with 6-axis force only dataset.\n\n9. Most of tasks in real-world experiments do not reflect the physics of \"slip\", which claimed in the previous section. All those for tasks seems highlight the binary contact or not(which can be achieved with simple FSR sensor), and force direction(which can be achieved by the 6-axis force sensor)\n\n10. The major concern would be the overclaim of this paper.\n\n11. Line 481-483 \"From the data perspective, the proposed ToucHD dataset serves as the final missing piece, completing a comprehensive dynamic tactile data ecosystem that supports multiple tiers of perception.\" The author try to build the ecosystem on the tiers which without rigor definition of boundary. Its actually mixing the concept of different types of tactile modalities instead of \"serving as the final missing piece\".\n\n12. No limitation section in main text.\n\n13. Line 483-485, \"our AnyTouch 2 general representation learning framework integrates multi-level objectives across all tiers,  endowing it with comprehensive dy- namic tactile perception capabilities.\", the representation learning framework do not support the capabilities of tactile in manipulation. More complex task and solid experiments are required.\n\n14. In caption of Figure 4, \"Each dynamic model has a corresponding  dynamic tier, which denotes the highest level of the training data and objectives used in the tactile dynamic pyramid, reflecting the model’s dy- namic perception capability.\" Can you specify what is \"dynamic model\", \"dynamic tier\", \"dynamic pyramid\" meaning in this context?\n\n15. For four real-world tasks, does the model directly evaluate on those tasks? If the model fine-tuned on downstream task specific data. please provide details on how you fine-tune the model on downstream tasks."}, "questions": {"value": "see weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YWo06GBcv5", "forum": "ndilONnABZ", "replyto": "ndilONnABZ", "signatures": ["ICLR.cc/2026/Conference/Submission3806/Reviewer_LnK5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3806/Reviewer_LnK5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977550171, "cdate": 1761977550171, "tmdate": 1762917043829, "mdate": 1762917043829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a large tactile dataset that includes dynamic tactile data. In addition, the authors propose a general tactile representation learning framework that unifies tactile sensing, visual sensing, semantic information, actions, and forces. The representation is showcased on downstream tasks, including manipulation and offline evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* Fig. 1 clearly explains the role and position of the ToucHD dataset.\n* The technical content is thorough, including pixel-level autoencoding, semantic tactile feature learning, and dynamic physical property learning.\n* Experiments and benchmarking are comprehensive, covering diverse datasets, methods, and hardware."}, "weaknesses": {"value": "* Although the paper is thorough, the method introduces many task objectives. Are all of them necessary? How does it perform on each individual task? Additional ablations and per-task performance would be convincing.\n* Fig. 4 would be clearer with a consistent zero position; the current plot can be misleading.\n* The training loss schedule (Eq. 7) appears complex. Does this imply that training the encoder is brittle?\n* For readers less familiar with prior tactile datasets, a table summarizing their sizes would help contextualize the scale of ToucHD."}, "questions": {"value": "* The training objective includes many tasks. Do the authors observe training instability? More details on training dynamics would be appreciated. Additionally, how are training hyperparameters (e.g., i_task) determined—trial and error or a principled procedure?\n* More details on Section 5.3 would help. Does the diffusion policy also take visual images as input? If not, do the authors assume the gripper has already grasped the object at the beginning of the task?\n* For training on a heterogeneous dataset like ToucHD, if a sampled data point lacks information needed to compute a particular loss (e.g., a force pair), how is the loss computed in that case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0GHBk1QcPG", "forum": "ndilONnABZ", "replyto": "ndilONnABZ", "signatures": ["ICLR.cc/2026/Conference/Submission3806/Reviewer_tU7z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3806/Reviewer_tU7z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037646217, "cdate": 1762037646217, "tmdate": 1762917043443, "mdate": 1762917043443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the problem that previous tactile datasets and models only focus on object-level features while lack temporal dynamics, this paper presents ToucHD, which is, to the best of my knowledge, the first large-scale dataset that systematically collects tactile data paired with actions and manipulation tasks. The paper also proposes AnyTouch 2, which is a unified tactile representation learning model that captures dynamics of various tactile sensors. Experiments on various benchmarks demonstrate strong performance across different tasks, ranging from object-level understanding to force-aware manipulation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. [Useful tactile dataset] To the best of my knowledge, this is the first work that captures large-scale tactile data on action-specific and manipulation tasks with a systematic pipeline. The combination of dynamic tactile perception and robotic manipulation is becoming more important as the tasks requires more dexterity and fine-grained control. The dataset may potentially be a foundation upon which future tactile robotic policies can be developed.\n\n2. [Strong pre-trained encoder] As is shown in Figure 2, the paper introduces a training paradigm that includes multiple stages. The paradigm makes it possible to train the model on a combination of various datasets (as shown in Appendix Table 4), resulting in a large-scale training set that models both semantics and dynamics of diverse sensors. Results in Table 1 and Figure 4 also show the significant improvement of the proposed model on modeling dynamics when compared to previous baselines. The pretrained tactile encoder and potentially be a base model for future tactile and robotic research.\n\n3. [Comprehensive technical details] The paper provides a very comprehensive appendix that includes technical details of both the data collection and model training, which makes it easy-to-understand."}, "weaknesses": {"value": "1. [Lack of sensor-invariant evidence] Although claimed to be sensor-invariant (by performing contrastive learning on different sensors), there's no clear evidence showing that the learned represenation is generalizable across different tactile sensors. In practice, there are two major problems when using a pretrained tactile encoder: (i) is it generalizable across different sensors? and (ii) is it model performance robust when the gel pads of the sensors are replaced (since for some sensors, the tactile readings might be signiciantly changed even if the gel pad is replaced with a new one)? For the first problem, it would be interesting to freeze the pretrained encoder and train a decoder on a downstream task with one sensor, and then test if the decoder still works when we change to another sensor (as what is done in Contrastive Touch-to-Touch Pretraining [1]). For the second problem, one simple way to test it would be repeating the current evaluation with another gel pad.\n\n2. [Lack of material diversity] Despite being diverse in dynamics, the proposed ToucHD dataset is mostly constructed by probing daily objects and 3D-printed indenters (as is shown in Appendix Table 2 and Figure 8). This might result in the lack of material diversity in the dataset. This also probably explains why AnyTouch 2 is underperforming AnyTouch 1 on the TAG benchmark (as shown in Table 1), since the TAG benchmark mainly evaluates the material understanding ability of the model. The model might potentially be further improved by (i) adding a material classification head to the pretraining stage and (ii) leveraging more tactile datasets that focus on capturing diverse materials (e.g. TAG and TaRF [2] both captures various materials in outdoor scenes).\n\n3. [Grouping of atomic actions] As is explained in Line 285-287, the tactile videos are grouped into 8 atomic actions during the action matching stage. I'm curious about: (i) how to group the complicated manipulation actions into these atomic actions, and does it require large amounts of manual labelling? (ii) is it possible to perfectly represent the manipulation actions with only 8 atomic actions?\n\n\n[1] Rodriguez, Samanta, Yiming Dou,  William van den Bogert, Miquel Oller, Kevin So, Andrew Owens, and Nima  Fazeli. \"Contrastive touch-to-touch pretraining.\" In *2025 IEEE International Conference on Robotics and Automation (ICRA)*, pp. 5857-5863. IEEE, 2025.\n[2] Dou, Yiming, Fengyu Yang, Yi Liu, Antonio Loquercio, and Andrew Owens. \"Tactile-augmented radiance fields.\" In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 26529-26539. 2024."}, "questions": {"value": "In addition to the questions mentioned in the weaknesses section, here are several clarification questions that I'm curious about:\n\n1. When collecting manipulation data, why do you choose to use UMI+left hand instead of fully using tele-operation with two grippers? Would this lead to bias in the captured data and limit its generaliability in pure robotic tasks?\n\n2. Is there a specific reason for using a different robotic arm in the Chip Moving task (as is mentioned in Line 1102-1105), i.e., is the representation specific to different embodiments for different tasks?\n\n3. Some of the material understanding benchmarks only require one single image as input (e.g. TAG), but multiple frames are used in the experiments shown in Appendix Table 7. What is the setting of this experiment?\n\n4. Upon acceptance, will the proposed dataset and pretrained models be released to the public?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QRiwDUztWw", "forum": "ndilONnABZ", "replyto": "ndilONnABZ", "signatures": ["ICLR.cc/2026/Conference/Submission3806/Reviewer_nyMK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3806/Reviewer_nyMK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123657099, "cdate": 1762123657099, "tmdate": 1762917042934, "mdate": 1762917042934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}