{"id": "9SZa9hBjW2", "number": 7235, "cdate": 1758012559937, "mdate": 1759897864533, "content": {"title": "Representative Action Selection for Large Action Space Meta-Bandits", "abstract": "We study the problem of selecting a subset from a large action space shared by a family of bandits, with the goal of achieving performance nearly matching that of using the full action space. We assume that similar actions tend to have related payoffs, modeled by a Gaussian process. To exploit this structure, we propose a simple epsilon-net algorithm to select a representative subset. We provide theoretical guarantees for its performance and compare it empirically to Thompson Sampling and Upper Confidence Bound.", "tldr": "", "keywords": ["Multi-armed bandits", "Expected supremum of Gaussian process", "Epsilon net", "High-dimensional probability"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c6402556564d131a3723cc1cfffa7a94d7d6298c.pdf", "supplementary_material": "/attachment/b3d9d6f5c2e376617d7b263ef0accd2a82b9eff9.zip"}, "replies": [{"content": {"summary": {"value": "This work tackles selecting a subset of actions from a large shared action space in bandit problems, aiming to achieve performance close to using the full set. It models action similarity with a Gaussian process and proposes a simple ε-net algorithm to choose representative actions. It provides theoretical guarantees and empirically compare the method to Thompson Sampling and UCB, showing competitive performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provide lower and upper bounds, which makes the proposed algorithm more convincing.\n- Numerical experiments are provided, which helps readers to understand the charcteristics of the algorithm."}, "weaknesses": {"value": "- Although some related works are mentioned, the authors do not clearly explain how they are connected to the problem setting.\n- What is the main takeaway from the theoretical analysis? Is it primarily about the relationship between $\\epsilon$ and the regret?\n- Section 4 is overly long and difficult to follow."}, "questions": {"value": "- I could not understand the main message of Section 3. What is the main message here?\n- If we separate Section 4 into some subsections, what whould they be?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nyQxTXufWx", "forum": "9SZa9hBjW2", "replyto": "9SZa9hBjW2", "signatures": ["ICLR.cc/2026/Conference/Submission7235/Reviewer_jxrm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7235/Reviewer_jxrm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761123863901, "cdate": 1761123863901, "tmdate": 1762919376850, "mdate": 1762919376850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for selecting a representative subset from a large action space shared by a family of bandits, trying to achieve performance close to that of using the full action space. The authors propose an ε-net–based algorithm and provide theoretical performance guarantees, as well as an empirical comparison to Thompson Sampling and UCB."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors  try to bring high dimensional geomtery (epsilon nets) to bandits with large action spaces\n- They provide a regret analysis of their proposed algorithm."}, "weaknesses": {"value": "The paper starts from a motivation about handling large action spaces shared y a family of bandits, but it is not concretely instantiated for any specific family of bandits. It is formulated as an abstract framework whose significance is hard to understand without concrete instantiation in specific bandit families. the nature of the work seems more suited to a learning theory conference like COLT rather than to ICLR. \n\nSeveral details are also very unclear. Algorithm 1 seems extremely general and trivial and I don't see how it relates to the claim on lines 249-252 that the algorithm samples according to the importance measure q.\n\nThe paper is motivated by large action spaces but the experiments deal with extremely small toy action spaces - pretty clear that this experimental part was an add on, and doesn't make the case for large action spaces. Also in this simple example, I don't see which family of bandits is being referred to, and there is a lot of standard general stuff about RKHS that does not seem very relevant."}, "questions": {"value": "- How is Algorithm 1 magically sampling from the importance measure q? It doesn't appear at all in the description of the algorithm\n-  Could you give a specific example of families of bandits for which your results give something new?\n- How does your work compare to two well known approaches to large action spaces, the X-armed bandits of Bubeck et al and the Zooming algorithm of Kleinberg et al?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hGxI7hGAPe", "forum": "9SZa9hBjW2", "replyto": "9SZa9hBjW2", "signatures": ["ICLR.cc/2026/Conference/Submission7235/Reviewer_4i2i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7235/Reviewer_4i2i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990139928, "cdate": 1761990139928, "tmdate": 1762919376463, "mdate": 1762919376463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal: To Reviewer Vmxp"}, "comment": {"value": "We thank the reviewer for their feedback, but we believe there was a considerable missunderstanding of the paper.\n\n> reducing the size of the action space in linear bandits does not provide a lot of improvement in the bandit regret\n\nOur framework is not restricted to linear bandits. We state that it applies to sub-Gaussian processes in the main body, and the most general version is in line 643 of the Appendix, which does not require the linear structure of *a* and \\theta.\n\nEven under the Gaussian process assumption in Eq. 3, our framework assumes the feature vector of action *a* is unknown to the decision-maker. This practical assumption contrasts with linear bandits. In lines 104-111, we mention that linear bandits assume the feature vector is known, making the action space dimension irrelevant. This is also reflected in the book \"Bandit Algorithms\": linear bandits are introduced in Chapter 19, where the text around Eq. 19.1 states that \"the learner has access to a map \\psi... for an unknown parameter vector \\theta_*\" and \"\\psi is called a feature map.\"\n\n> For Theorems 6 and 7 I am worried that the first term max_{r in R} E_\\theta[ \\max_{a \\in r} \\mu_a ] is not vanishing when eps -> 0? Why is this a meaningful result?\n\nTheorem 9 shows the regret bound in Theorems 6 and 7 goes to zero as K increases. For the first term, our assumption of a canonical Gaussian process means for each cluster r, the term E_\\theta[ \\max_{a \\in r} \\mu_a ] is bounded between \\frac{1}{\\sqrt{2\\pi}} * \\text{diam}(r) and \\frac{\\sqrt{n}}{2} * \\text{diam}(r) (a standard result in high-dimensional probability; see Proposition 7.5.2 (f) of (Vershynin, 2018)). \n\nThus, as epsilon goes to zero, the diameter of r goes to zero, and the first term follows.\n\n> For Theorem 9 I am afraid that we cannot escape the curse of dimensionality -- epsilon needs to be smaller than a constant for it to be useful? But then N(A_full, eps) is usually exponential in n (the dimension of the action space), which makes the required K also exponential in n?\n\nThe measure q is key to our results. If q is highly concentrated, we need far fewer actions to cover the important regions. If q is uniform, we must form a geometric epsilon-net over the action space, whose smallest cardinality is the covering number and can scale with dimension n. Theorem 9 shows that even in the worst case where q is uniform, the expected regret still goes to zero. This is why the covering number N(A_full, eps) appears in the results.\n\nRegarding the curse of dimensionality: for an action space of [0,1]^n, we need about (1/ε)^n points for universal coverage. However, if the support of q is the main diagonal {(t,t,…,t) : t∈[0,1]}, we need at most 1/ε points to achieve the same expected regret as universal coverage. Please let us know if this is clear and helpful.\n\n> this paper assumes that one knows the exact prior of the task parameter \\theta? Is this prior learned by interaction with the previous tasks?\n\nWe do not assume the distribution of task parameters is known, but we require the ability to sample tasks (bandit instances). \n\n> CTS and CUCB works when the reward function is additive across different arms in a super-arm. But here the reward function of a super arm is the maximum of reward of arms therein. Thus, I am not sure if this is a fair comparison..\n\nIn lines 146-153, we discussed that CTS and CUCB are not designed for our framework. However, they are the closest applicable baselines for combinatorial problems with large action spaces.\n\nThe experiments in Figure 1 are more reasonable, but using TS, UCB, and Successive Halving at the super-arm level suffers from combinatorial explosion and is only feasible in small action spaces."}}, "id": "UvH0o7QEzq", "forum": "9SZa9hBjW2", "replyto": "9SZa9hBjW2", "signatures": ["ICLR.cc/2026/Conference/Submission7235/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7235/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7235/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763045833521, "cdate": 1763045833521, "tmdate": 1763050768773, "mdate": 1763050768773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies choosing a representative subset of actions from the original action space in linear bandit problems. More formally, it would like to choose the subset such that the optimal expected reward in the subset approximates the globally optimal expected reward, with performance quantitatively characterized by the regret defined in Eq. (1). It proposes Alg. 1 and proves some theoretical guarantees of it. Experiments show that the algorithm outperforms other baselines such as successive halving, combinatorial Thompson sampling, and combinatorial UCB."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Contextual bandits with large action spaces has many applications, and the attempts to reduce the action space to mitigate the computational cost is meaningful."}, "weaknesses": {"value": "- From a statistical perspective, reducing the size of the action space in linear bandits does not provide a lot of improvement in the bandit regret. Importantly, the regret for linear bandits depends usually on the linear dimension of the action space, which is much smaller than the cardinality of the action space (e.g. \"Bandit Algorithms\", Theorem 36.4). In this respect I think the motivation in lines 58-75 (using the cardinality of the action space as the main argument) is somewhat flawed. \n\n- For Theorems 6 and 7 I am worried that the first term max_{r in R} E_\\theta[ \\max_{a \\in r} \\mu_a ] is not vanishing when eps -> 0? Why is this a meaningful result?\n\n- For Theorem 9 I am afraid that we cannot escape the curse of dimensionality -- epsilon needs to be smaller than a constant for it to be useful? But then N(A_full, eps) is usually exponential in n (the dimension of the action space), which makes the required K also exponential in n?\n\n- I am also not buying the application background of the problem setup. It looks like one is using meta learning to learn a representative subset of the action space. But this paper assumes that one knows the exact prior of the task parameter \\theta? Is this prior learned by interaction with the previous tasks? If so, I am not sure if such prior can be assumed to be known exactly, since after learning in historical tasks, we may not learn the ground truth task parameter for those tasks, in a pointwise manner."}, "questions": {"value": "See questions above. Also:\n\n- CTS and CUCB, in my understanding, works when the reward function is additive across different arms in a super-arm. But here the reward function of a super arm is the maximum of reward of arms therein. Thus, I am not sure if this is a fair comparison.."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l8VpO1lAl9", "forum": "9SZa9hBjW2", "replyto": "9SZa9hBjW2", "signatures": ["ICLR.cc/2026/Conference/Submission7235/Reviewer_Vmxp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7235/Reviewer_Vmxp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762130506683, "cdate": 1762130506683, "tmdate": 1762919376113, "mdate": 1762919376113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal: To Reviewer 4i2i"}, "comment": {"value": "> It is formulated as an abstract framework whose significance is hard to understand without concrete instantiation in specific bandit families. \n\nThe specific family of bandits is defined in Eq. 3. This family is generalized to sub-Gaussian processes in Appendix B. In line 45, we state that each bandit is a contextual bandit, referencing (Dani et al., 2008).\n\n> Algorithm 1 seems extremely general and trivial\n\nAlgorithm 1 is indeed simple. However, the point of our work is that we can exploit correlations among action space and use it to tackle a combinatorial problem. We believe it is not so trivial that the simple algorithm does this and it also works well. \n\n> I don't see how it relates to the claim on lines 249-252 that the algorithm samples according to the importance measure q.\n\nThe algorithm repeats the following process K times: sample a bandit environment and find its optimal action. The measure q is the probability that a region of the action space contains the optimal action over the distribution of bandits.\n\nRecall the medicine example: the algorithm essentially asks random customers which drug they want and stocks that drug. This makes it likely that we will keep high-demand drugs in stock—the ones many customers want to buy (high q measure).\n\n> The paper is motivated by large action spaces but the experiments deal with extremely small toy action spaces\n\nChoosing 10 actions from 500 is a combinatorial problem with 2.458×10^20 super-arms.\n\n> Also in this simple example, I don't see which family of bandits is being referred to, and there is a lot of standard general stuff about RKHS that does not seem very relevant.\n\nWe mention RKHS because if outcome functions are sampled from it, the resulting bandit family is of the form in line 367, aligning with our formulation in Eq. 3 and allowing our algorithm to be applied.\n\nBy standard results in Williams & Rasmussen, (2006), the Gaussian kernel admits an (infinite-dimensional) feature map \\Phi. In our experiments, \\Phi is unknown to the decision-maker. The set \\mathcal{A}_{full} serves only as an index set (e.g., action names), while the true action space is the image of the action space under \\Phi.\n\n> Could you give a specific example of families of bandits for which your results give something new? How does your work compare to two well known approaches to large action spaces, the X-armed bandits of Bubeck et al and the Zooming algorithm of Kleinberg et al?\n\nX-armed bandits and the Zooming algorithm work on a single bandit environment. We work with a family of bandits where the mean-payoff function differs in each instance. We also do not assume a known metric structure on the actions.\n\nOur objective is also different: we assume individual bandits can be solved (by an oracle, and we only need to solve a few instances). Instead, we focus on the combinatorial problem of finding a small subset of actions that effectively represents the large action space.\n\nOur contribution is a new framework where a large action space is shared across a family of bandits, with actions’ mean rewards correlated in each instance but the similarity structure is unknown to the decision-maker. We formulate this using a Gaussian process, extended to sub-Gaussian distributions. A further contribution is the new problem of finding a fixed action subset to represent the large space, for which we provide an efficient algorithm."}}, "id": "ZNzO0d7IiL", "forum": "9SZa9hBjW2", "replyto": "9SZa9hBjW2", "signatures": ["ICLR.cc/2026/Conference/Submission7235/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7235/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7235/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763046259901, "cdate": 1763046259901, "tmdate": 1763046259901, "mdate": 1763046259901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal: To Reviewer jxrm"}, "comment": {"value": "> Although some related works are mentioned, the authors do not clearly explain how they are connected to the problem setting.\n\nOptimal Action Identification, Stochastic Linear Optimization, and GP Optimization relate to Step 6 of our algorithm, which requires finding an optimal action in a sampled bandit environment.\n\nTop-K Action Identification and Combinatorial Bandits relate to our core problem of selecting a representative action subset from a large space, serving as our closest baselines.\n\nPlease indicate the specific parts of related work so we can clarify and improve.\n\n> What is the main takeaway from the theoretical analysis? Is it primarily about the relationship between epsilon and the regret?\n\nWe show that correlations among actions can be used, so an action can be represented by its neighbors. Theorems 6-7 show our algorithm performs comparably to universal coverage of the action space. The key point in line 323 is the price we pay for not using universal coverage; it depends on the number of samples K and the importance measure q.\n\nThis price is highest when q is uniform. Theorem 9 shows the expected regret of our algorithm still goes to zero, even in this worst case.\n\nThe relationship between epsilon and regret is also tested via a 3-dimensional example in Figure 3 in Appendx.\n\n> If we separate Section 4 into some subsections, what whould they be?\n\nSection 4 details the experimental setup and presents two experiments.\n\nFirst, we explain how RKHS functions connect to our framework in Eq. 3, which justifies sampling them for our experiments.\n\nThe first experiment compares our method against using TS, UCB, and Successive Halving at the super-arm level.\n\nThe second experiment compares our method against CTS and CUCB at the base-arm level."}}, "id": "kC1EO6vaGT", "forum": "9SZa9hBjW2", "replyto": "9SZa9hBjW2", "signatures": ["ICLR.cc/2026/Conference/Submission7235/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7235/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7235/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763046437613, "cdate": 1763046437613, "tmdate": 1763046437613, "mdate": 1763046437613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}