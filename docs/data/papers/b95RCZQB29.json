{"id": "b95RCZQB29", "number": 591, "cdate": 1756751397159, "mdate": 1759898251906, "content": {"title": "Domain Expansion: A Latent Space Construction Framework for Multi-Task Learning", "abstract": "Training a single network with multiple objectives often leads to conflicting gradients that degrade shared representations, forcing them into a compromised state that is suboptimal for any single task—a problem we term latent representation collapse. We introduce Domain Expansion, a framework that prevents these conflicts by restructuring the latent space itself. Our framework uses a novel orthogonal pooling to construct a latent space where each objective is assigned to a mutually orthogonal subspace. We validate our approach on the ShapeNet benchmark, simultaneously training a model for object classification and pose estimation. Our experiments demonstrate that this structure not only prevents collapse but also yields an explicit, interpretable, and compositional latent space where concepts can be directly manipulated.", "tldr": "", "keywords": ["representation learning;machine learning for computer vision"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba87dd8202c4cefc8316dec41d10614b6b83d6db.pdf", "supplementary_material": "/attachment/0f2b9ef5c6056290ac0f13b4b54be4a1739eda6c.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces \"Domain Expansion,\" a novel framework for multi-task learning (MTL) designed to address what the authors term \"latent representation collapse.\" This phenomenon occurs when a single network trained on multiple objectives learns a compromised and suboptimal shared representation due to conflicting gradients. The core idea of Domain Expansion is to prevent this interference structurally by constructing a latent space where each learning objective is assigned to a dedicated, mutually orthogonal subspace. This is achieved through a three-step process at each training epoch: (1) finding the principal axes of the latent feature distribution via eigendecomposition of the covariance matrix, (2) defining an orthogonal domain by assigning the top eigenvectors to different tasks, and (3) using \"orthogonal pooling\" to project the shared latent feature into these concept-specific subspaces before decoding.\n\nThe authors validate their approach on several benchmarks (ShapeNet, MPIIGaze, Rotated MNIST) that combine classification and regression tasks. The experiments show that Domain Expansion not only outperforms standard MTL baselines and gradient-based conflict mitigation methods in both representation quality and predictive performance but also creates a structured and compositional latent space. This structured space allows for algebraic operations on concepts, which the authors demonstrate through concept composition experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The concept of \"latent representation collapse\" is well-motivated and clearly illustrated in Figure 1. The paper does an excellent job of formalizing this problem and positioning Domain Expansion as a direct solution.\n* A key strength of the proposed method is its ability to create a latent space that is not a \"black box.\" The paper demonstrates that the resulting orthogonal structure enables meaningful, algebraic manipulation of concepts (e.g., concept-specific adjustment and composition), a significant step towards more controllable and interpretable models."}, "weaknesses": {"value": "*   **Originality and Relation to Prior Work:** The core idea of representing concepts in a latent space may not be entirely new. The paper could be strengthened by discussing its relationship to existing concepts such as superposition in neural networks[1,2], embedding properties in word vectors [3], and the linear representation hypothesis [4].\n*   **Limited Scope of Empirical Validation:** The experiments, while well-executed, are confined to controlled vision datasets (ShapeNet, Rotated MNIST, MPIIGaze). The paper makes broad claims about applicability, but there is no evaluation on other domains like text or multi-modal learning, or on large-scale, complex benchmarks. Furthermore, the lack of an ablation study or resource analysis makes it difficult to assess the scalability and efficiency of the eigendecomposition and Hungarian alignment steps, limiting the claims of general applicability.\n*   **Insufficient Theoretical Grounding for Orthogonal Basis Stability:** The framework relies on a dynamic eigendecomposition performed at each epoch. This raises concerns about the stability of the basis vectors, which could shift and introduce noise into the training process.\n*   **Missing Discussion on Computational and Memory Overheads:** The proposed method introduces a covariance matrix calculation and eigendecomposition step (Equations 3-4) in each training epoch. For high-dimensional latent spaces (D=2048 in this paper), this introduces a significant computational complexity of O(D³), which is not analyzed or compared against the runtime costs of gradient-based MTL methods. This omission may hinder the method's reproducibility and adoption for larger models where this overhead could be prohibitive.\n*   **Minor Ambiguities in Mathematical Formulation:** While the mathematical exposition is generally clear, there are some inconsistencies in notation. For instance, the variable 'F' is used to denote the latent space, a batch of representations, and a single latent vector at different points. Similarly, the formulation of the total loss in Equation (8) seems inconsistent with the definition of the projected subspace in Equation (5), as the loss is computed on projected vectors, not the subspace itself. The notation for the set of objectives 'M' is also used ambiguously. These minor issues could be tightened for improved clarity.\n\n[1] Toy Models of Superposition\n\n[2] Linear algebraic structure of word senses, with applications to polysemy\n\n[3] Linguistic regularities in continuous space word representations\n\n[4] The Linear Representation Hypothesis and the Geometry of Large Language Models"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X2BC4ElQFz", "forum": "b95RCZQB29", "replyto": "b95RCZQB29", "signatures": ["ICLR.cc/2026/Conference/Submission591/Reviewer_wQsh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission591/Reviewer_wQsh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760670664172, "cdate": 1760670664172, "tmdate": 1762915558779, "mdate": 1762915558779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Domain Expansion, a novel framework designed to address the problem of \"latent representation collapse\" in multi-objective learning. The core problem is that when a single network is trained on multiple tasks, conflicting gradients can lead to a compromised latent space that is suboptimal for all tasks. The authors' propose to structure the latent space into a set of mutually orthogonal subspaces, one for each objective. By using an orthogonal pooling mechanism, the method ensures that the learning signals for each task are decoupled, preventing interference by design. The paper demonstrates that this approach not only improves performance on multi-objective benchmarks but also creates an interpretable and compositional latent space where concepts can be manipulated algebraically."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **S1. Clarity and Presentation:** The paper is well-written and easy to follow. The authors do a great job of motivating the problem with the clear concept of \"latent representation collapse.\" The flow from the problem statement to the proposed method is logical and intuitive. The figures are highly effective at illustrating the core idea.\n* **2. Originality and Elegance of the Method:** The central idea of harnessing the principal eigenvectors of the latent space's covariance matrix to form an orthogonal basis for different tasks is, to my knowledge, novel. It is a novel approach that addresses the root cause of task interference rather than reactively managing gradient conflicts during optimization, which is the focus of many existing methods.\n* **3. Interpretability and Compositionality:** A significant strength of this work is that it doesn't just aim for better metrics, but for a more structured and meaningful latent space. The demonstration of a \"concept algebra\", where latent vectors can be manipulated through simple arithmetic to compose or adjust concepts, is a contribution towards more interpretable and controllable models."}, "weaknesses": {"value": "* **W1. Scope of Empirical Evaluation:** The experimental validation, while thorough on the chosen dataset, could be broadened to better establish the generalizability of the method.\n    * **Architectural Diversity**: The experiments are conducted using only a ResNet-50 encoder. It would be beneficial to understand if the method is equally effective with other modern architectures, such as Vision Transformers, which may have different latent space geometries.\n    * **Dataset Diversity**: The primary results are demonstrated on ShapeNet. While MPIIGaze and Rotated MNIST are mentioned in the appendix, a more in-depth analysis on datasets with different characteristics (e.g., more complex scenes, different data modalities) would strengthen the paper's claims.\n    * **Robustness of Results**: The results in Table 1 are reported without standard deviations across multiple runs. Including these would provide greater confidence in the stability and significance of the performance gains.\n* **W2. Clarity of Results in Table 1:** The main results table is dense and could be made more reader-friendly.\n    * **Readability**: A suggestion would be to add a horizontal line to visually separate the baseline methods from the proposed method. Highlighting the best-performing score in each column (e.g., in bold) would also allow for a quicker assessment of the results.\n    * **Caption Details**: The caption could be improved by explicitly defining the acronyms used in the column headers (e.g., az, el, rot, cat, id), as they are not defined in the main text of the experiments section.\n* **W3. Nuanced Performance on Objective Set 2:** The results for Objective Set 2 are interesting and warrant more discussion. While the proposed method clearly excels on representation quality metrics (Spearman, V-score), some baselines achieve slightly better or comparable predictive performance on the downstream tasks (e.g., MAE, Accuracy). This raises the question of the practical trade-offs involved and in which scenarios superior representation quality is most critical.\n* **W4. Reproducibility:** The paper does not mention the release of code. Providing the implementation would be a valuable contribution to the community, allowing others to build upon this work."}, "questions": {"value": "* **Q1. Generalizability to Other Architectures:** Could the authors comment on the applicability of Domain Expansion to other encoder architectures, such as Transformers? Does the core assumption that principal eigenvectors correspond to meaningful, disentanglable concepts, hold in those architectures as well?\n\n* **Q2. Performance on Standard Multi-Task Benchmarks:** To better validate the proposed method, it would be insightful to test the model on more complex, standard multi-task benchmarks like NYUv2, CelebA, or Cityscapes. Would the authors anticipate any specific challenges, for instance, with a much larger number of tasks (as in CelebA) or with dense prediction tasks (like semantic segmentation in Cityscapes)?\n\n* **Q3. Transferability of the Learned Representation:** Could a model pre-trained with Domain Expansion be adapted to new tasks or datasets? For example, could a model trained on concepts {azimuth, elevation, category} be effectively fine-tuned on a new dataset that only has labels for {azimuth, category}, or perhaps one where a new concept, {color}, is introduced?\n\n* **Q4. Training Dynamics and Computational Cost:** How many epochs are typically needed for the orthogonal basis to stabilize before the encoder is frozen? What's the time per epoch? What is the computational overhead of the eigendecomposition step per epoch compared to a standard multi-task baseline?\n\n* **Q5. The Value of Representation Quality:** The results for Objective Set 2 compellingly show that baseline methods can achieve high predictive accuracy despite having a collapsed latent representation (i.e., poor V-scores). However, in a practical scenario where a user only cares about the final prediction accuracy on this specific test set, what is the key argument for your method? Does the superior representation quality translate to other critical benefits, such as improved robustness, better generalization to out-of-distribution samples, or greater fairness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iW0vZvb1aI", "forum": "b95RCZQB29", "replyto": "b95RCZQB29", "signatures": ["ICLR.cc/2026/Conference/Submission591/Reviewer_CqRr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission591/Reviewer_CqRr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681118934, "cdate": 1761681118934, "tmdate": 1762915558341, "mdate": 1762915558341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how to reduce conflicting gradients in multi-objective optimization. Rather than modifying gradients during training as with prior methods, this paper proposes to modify the representation space itself instead by allocating task-specific orthogonal subspaces. Specifically, at each training epoch, i) eigenvectors of the features are computed on the entire training data; ii) the top-m eigenvectors are selected as the task domain and used to construct orthogonal subspaces; iii) each latent feature is then projected onto each orthogonal, concept-specific subspace and the sub-space-specific representations are pooled together; and iv) the projected features are then used to compute the training loss.\n\nThe authors evaluate their approach on ShapeNet, MPIIGaze, and Rotated MNIST where it outperforms baseline and comparison multi-task methods. They also show through quantitative and qualitative analyses that the representation space created by their method is indeed more structured and composable than those of other methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed approach tackles the problem of multi-objective optimization from a different perspective than prior work.\n    \n2. The proposed orthogonal domain is intuitive and well-motivated. The authors illustrate the general problem it should solve and demonstrate through analyses that it is indeed an issue in practice.\n    \n3. The domain expansion approach is principled and the authors describe its operations and properties.\n    \n4. The authors show that their method outperforms baselines both based on predictive performance and in terms of the quality of the representation space."}, "weaknesses": {"value": "1. Training inefficiency: Eigenvectors need to be calculated and features need to be projected at every epoch. In addition, the Hungarian algorithm needs to be used to align eigenvectors across training epochs. It would be useful to provide some analysis regarding the training times of the proposed method vs the baselines to understand to what extent this is a limitation in practice.\n    \n2. Weak base model: The method is only applied to a fairly old model (ResNet-50). It would be helpful to apply the method to more recent models to see whether it can generalize and is still useful with stronger base models.\n    \n3. Assumption of orthogonality: The method assumes that concepts should be represented based on orthonormal feature spaces. While this may be optimal when concepts and tasks are clearly disentangled as in the experiments, this may reduce sharing of information when objectives are more closely related as is common in real-world applications (e.g., when viewing language modeling, i.e., next-token prediction as multi-task learning, many words rely on shared latent concepts for their prediction). I would appreciate an analysis (can be in a synthetic setting) that provides some insight regarding whether this is an issue in practice.\n    \n4. Narrow evaluation setting: The proposed method is applied to relatively similar datasets containing rotated shapes/digits or gazes. Prior work in this area ([https://arxiv.org/pdf/2001.06782](https://arxiv.org/pdf/2001.06782), [https://arxiv.org/pdf/2010.05874](https://arxiv.org/pdf/2010.05874)) has been evaluated on a broader set of more complex benchmarks including multi-task and multi-label image classification (MultiMNIST, CityScapes, CelebA, multi-task CIFAR-100, NYUv2), multi-task RL, and multi-task NLP datasets. It would be useful to evaluate on more diverse benchmarks to understand the method’s broader applicability and the extent it is applicable to more complex multi-task learning problems."}, "questions": {"value": "1. What is the impact of fitting the eigenvectors on the current batch vs larger subsets of the training data vs the entire training dataset?\n    \n2. What is the impact of different choices of the number of eigenvectors M? Is it important for M to be the same as the number of objectives? What happens if M is lower or larger?\n    \n3. How does the approach perform with larger sets of concepts (combining Objective Sets 1 and 2)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "moD7pt7bmT", "forum": "b95RCZQB29", "replyto": "b95RCZQB29", "signatures": ["ICLR.cc/2026/Conference/Submission591/Reviewer_FZR9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission591/Reviewer_FZR9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829399901, "cdate": 1761829399901, "tmdate": 1762915558030, "mdate": 1762915558030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Domain Expansion, a novel framework for multi-task learning (MTL) that addresses latent representation collapse categorized as a failure mode where conflicting task gradients lead to entangled, suboptimal shared representations. Instead of modifying gradient updates, the authors propose orthogonal pooling, an architectural mechanism that explicitly decomposes the latent space into mutually orthogonal subspaces, each dedicated to a specific learning objective. This approach aims to prevent interference between tasks structurally, not procedurally. The paper offers a clear three-stage method: (1) compute latent feature covariance, (2) derive an orthonormal basis via eigendecomposition, and (3) project features into task-specific orthogonal subspaces. Each subspace is decoded separately, and training proceeds with independent task losses. Experiments on ShapeNet, MPIIGaze, and Rotated MNIST demonstrate improvements in representation disentanglement and predictive metrics compared to gradient-based MTL baselines (PCGrad, IMTL, Nash-MTL, FAMO). The authors further show that the resulting latent space supports algebraic concept manipulation, such as vector addition and subtraction of concepts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel and intuitive framing:** The notion of latent representation collapse as a geometric phenomenon in shared latent spaces is well-motivated and connects neatly to known issues in MTL such as negative transfer and conflicting gradients.\n\n2. **Architectural elegance:** Orthogonal pooling is conceptually simple yet effective. It shifts focus from gradient-level interventions (as in PCGrad or Nash-MTL) to a proactive representation-level solution.\n\n3. **Compositional algebra:** The inclusion of concept-level operations ($\\oplus$, $\\ominus$) adds a unique interpretability dimension to MTL. Section 3.3 formalizes this idea cleanly, showing how orthogonal projections allow structured latent arithmetic.\n\n4. **Methodological clarity:** The training pipeline (Fig. 5) and algorithmic description (Eqs. 3 to 8) are logically structured and mathematically well-grounded.\n\n5. **Empirical experiments:** The method outperforms gradient-based MTL baselines on both representation and predictive metrics (Table 1), with qualitative PCA visualizations (Fig. 6) convincingly showing disentanglement.\n\n6. **Interpretability and compositionality:** Demonstrating vector arithmetic over latent subspaces (Sec. 4.3) is original and compelling; it strengthens the claim that the method yields a structured, interpretable representation.\n\n7. **Presentation quality:** The paper is exceptionally well written and illustrated. Figures 1-4 intuitively connect abstract mathematical ideas to geometric and conceptual metaphors (e.g., the “anamorphic art” analogy in Fig. 3 is particularly effective)."}, "weaknesses": {"value": "# Major Concerns\n\n1. **Empirical scope is narrow.** \nExperiments focus exclusively on relatively small, controlled datasets (ShapeNet, MPIIGaze, Rotated MNIST). These are synthetic or low-dimensional settings. The paper’s claims of scalability and generality (e.g., toward fairness or multimodal learning in Sec. 6) are not yet substantiated.\n\n2. **Unclear stability and computational cost.** \nThe method requires per-epoch covariance estimation and eigendecomposition of large latent spaces (2048-D ResNet-50 features). This step may be $\\mathcal{O}(D^3)$ and unstable for high-dimensional features. The paper should clarify computational cost and numerical stability, especially since Hungarian alignment is used to track basis permutations.\n\n3. **Ambiguity in orthogonality enforcement.**\nThe orthogonal subspaces are defined using the empirical covariance’s eigenvectors. Thus, orthogonality is not learned but imposed externally. This means the decomposition depends heavily on data statistics rather than explicit task semantics. It’s unclear whether orthogonality aligns with task-specific gradients or merely decorrelates features statistically.\n\n4. **Potential circularity in “concept assignment.”**\nThe mapping between eigenvectors (principal components) and tasks (concepts) appears heuristic: the top-M eigenvectors are “assigned” to the M objectives. This lacks theoretical grounding. Why should the top eigenvector correspond to azimuth rather than color, for example? A data- or loss-driven criterion would strengthen this link.\n\n5. **Overstated compositionality claims.**\nThe algebraic operators rely on linear subspace assumptions that may not hold for nonlinear encoders and decoders. Though conceptually elegant, the empirical evidence (Table 1 last column) is limited to cosine similarity metrics. A visual or task-level demonstration (e.g., compositional generation or interpolation) would better support the claim.\n\n6. **Baselines and statistical rigor.**\nResults in Table 1 show large performance gaps (e.g., Spearman 0.95 vs 0.49), which seem unusually high. The paper should include standard deviations, repeated trials, and clarification on hyperparameter tuning fairness. Without this, the reported improvements risk appearing overstated.\n\n7. **Orthogonality and task granularity.**\nThe method presumes one axis per task (Eq. 5), which implicitly assumes tasks are linearly independent. Many real-world MTL setups involve correlated objectives (e.g., segmentation + depth). The framework does not discuss how partially dependent tasks are handled or whether strict orthogonality might hurt shared feature reuse.\n\n8. **Dependence on labeled concept spaces.**\nThe approach assumes explicit supervision for each “concept” (Sec. 3.3), limiting its applicability to unsupervised or weakly labeled MTL. The contrastive + ranking objectives help, but it remains unclear how Domain Expansion would generalize beyond fully annotated multi-concept datasets.\n\n## Minor Concerns\n\n1. **Equation references:** Eq. (8) could clarify whether projection operators $P_m$ are recomputed each epoch or fixed post-training.\n\n2. **Notation clarity:** Use consistent notation between $f_{proj, m}$ and $F_{proj,m}$\n3. **Appendix details:** Appendix A.1 should include pseudocode for the orthogonal pooling operation and basis stabilization via Hungarian alignment.\n4. **Baselines:** Including PCGrad (Yu et al., 2020) explicitly in results tables would give a fairer picture since conceptually it is designed to improve multi-task learning dynamics.\n5. **Discussion:** Section 5’s “chair $\\oplus$ boat” example is charming but anecdotal. Please clarify if such compositions were empirically tested.\n6. **Formatting:** Some tables and figure captions (Table 1, Fig. 6) could benefit from indicating whether higher is better for each metric directly in the caption."}, "questions": {"value": "1. **Concept assignment:** How is each eigenvector mapped to a specific task concept? Is there a data-driven matching (e.g., gradient alignment) or manual assignment based on variance ranking?\n\n2. **Computation cost:** What is the training overhead of recomputing eigendecomposition (Eq. 4) per epoch compared to gradient-based MTL baselines?\n\n3. **Dynamic tasks:** Can Domain Expansion adapt if the set of tasks changes mid-training (e.g., new objective added)?\n\n4. **Partial supervision:** Can orthogonal pooling be extended to unsupervised or semi-supervised MTL where some tasks lack labels?\n\n5. **Orthogonality realism:** How sensitive is the method to correlated or redundant tasks? Does strict orthogonality ever hurt performance?\n\n6. **Hungarian alignment:** Why choose Hungarian alignment to track basis order? Would Procrustes or canonical correlation alignment suffice?\n\n7. **Compositional validity:** Beyond cosine similarity, have you tested compositional operators on downstream performance (e.g., predicting unseen concept combinations)?\n\n8. **Numerical stability:** Does covariance estimation introduce instability for large batch or small-batch regimes? Any regularization applied (e.g., shrinkage, clipping)?\n\n9. **Generality:** Can this method be plugged into transformer-based multimodal models (e.g., CLIP, ViT) without modification?\n\n10. **Fair comparison:** Were all baselines tuned to optimal weighting parameters, or did you adopt published defaults?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "x4lj2iv0oh", "forum": "b95RCZQB29", "replyto": "b95RCZQB29", "signatures": ["ICLR.cc/2026/Conference/Submission591/Reviewer_yLyP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission591/Reviewer_yLyP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762110119046, "cdate": 1762110119046, "tmdate": 1762915557520, "mdate": 1762915557520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}