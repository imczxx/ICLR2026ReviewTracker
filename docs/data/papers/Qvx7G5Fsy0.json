{"id": "Qvx7G5Fsy0", "number": 2982, "cdate": 1757312054167, "mdate": 1763727123686, "content": {"title": "Vision-Centric Activation and Coordination for Multimodal Large Language Models", "abstract": "Multimodal large language models (MLLMs) integrate image features from visual encoders with LLMs, demonstrating advanced comprehension capabilities. However, mainstream MLLMs are solely supervised by the next-token prediction of textual tokens, neglecting critical vision-centric information essential for analytical abilities. To track this dilemma, we introduce **VaCo**, which optimizes MLLM representations through **V**ision-Centric **a**ctivation and **Co**ordination from multiple vision foundation models (VFMs). VaCo introduces visual discriminative alignment to integrate task-aware perceptual features extracted from VFMs, thereby unifying the optimization of both textual and visual outputs in MLLMs. Specifically, we incorporate the learnable *Modular Task Queries* (MTQs) and *Visual Alignment Layers* (VALs) into MLLMs, activating specific visual signals under the supervision of diverse VFMs. To coordinate representation conflicts across VFMs, the crafted *Token Gateway Mask* (TGM) restricts the information flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo significantly improves the performance of different MLLMs on various benchmarks,  showcasing its superior capabilities in visual comprehension.", "tldr": "In this paper, we introduce VaCo, a framework that optimizes MLLM representations through visual activation and coordination derived from multiple vision foundation models (VFMs).", "keywords": ["multimodal large language models", "vision-centric activation", "vision-centric coordination"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/103fc3a7ebec8edfa1f192391209d31cba2bb027.pdf", "supplementary_material": "/attachment/1fcd79d3fa584baabf5f789e12952be05d3c757f.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces VaCo, a method to enhance MLLM representations via Vision-Centric activation and coordination from multiple Vision Foundation Models (VFMs). VaCo uses Mechanism-specific Tunable Queries (MTQs) and Visual Activation Layers (VALs) to activate targeted visual signals under VFMs' supervision, incorporating a Tunable Gating Mechanism (TGM) to control information flow among MTQ groups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unlike previous works that introduced multiple visual encoders, this paper demonstrates increased efficiency during inference.\n2. The authors have conducted extensive experiments to demonstrate the effectiveness of the proposed method, and VaCo significantly improves the performance of different MLLMs on various benchmarks"}, "weaknesses": {"value": "1. As shown in Table 1, VaCo consistently underperforms on the POPE dataset. The authors are requested to provide a plausible explanation for this result.\n2. The model architecture used in the experiments of this paper is outdated. The authors are advised to validate the effectiveness of the proposed method using a more recent architecture(e.g., Qwen2.5-VL, Qwen3+SigLIP2) to strengthen the credibility of their method.\n3. How does the training cost of VeCo compare to that of ROSE? Please provide a detailed comparison.\n4. Why did you choose to use MSE + Contrastive loss during the training of the Visual Alignment Layer?"}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zna2ni948L", "forum": "Qvx7G5Fsy0", "replyto": "Qvx7G5Fsy0", "signatures": ["ICLR.cc/2026/Conference/Submission2982/Reviewer_LW6u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2982/Reviewer_LW6u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761062765969, "cdate": 1761062765969, "tmdate": 1762916479574, "mdate": 1762916479574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Vision-Centric Activation and Coordination (VaCo), which aims to enhance Visual abilities of VLMs by applying a query-based discriminative alignment with various vision experts. Specifically, they insert task-specific learnable queries (MTQ) into the LLM and align those queries to outputs of multiple frozen vision foundation models (VFM) via a small Visual Alignment Layer (VAL). Experimental results, both qualitative and quantitative, demonstrate that VaCo indeed enhances the performance of VLMs on vision tasks, and directs attention correctly."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is partially novel to me. Instead of trying to inject multi-expert feature into the VLM like the traditional Cambrian or Eagle, this paper uses contrastive loss to align query-based VLM features with expert features. This sounds more elegant, and also reduces compute overhead.\n2. The technical contributions, MTQ and TGM, are theoretically favourable. I agree with the authors' design logic in these two modules.\n3. Experiments are generally solid. Authors performed main experiments on various types of VLMs, and did ablation studies to prove the effectiveness of MTQ and VFM selection.\n4. The representation is clear. The figures are informative, and the visualizations strongly support their claims."}, "weaknesses": {"value": "1. The idea of introducing contrastive loss into VLMs with experts is partially limited in novelty. Since I am not a research expert in this field, I cannot fully approve or disapprove of the novelty. I will refer to other reviewers' comments on novelty to decide my final score.\n2. The experiments are done on rather old models like Vicuna or Qwen2-VL. The claims could have been strengthened if strong contemporary methods like Qwen2.5-VL (5 months before submission) were included.\n3. The VaCo features in Figure 5 can demonstrate that VaCo is learning something from experts, but the quality is far from satisfactory compared with the experts.\n4. Typo: \"VQAs\" in the caption of Figure 3."}, "questions": {"value": "1. Why is the quality of VaCo feature far from satisfactory compared with the experts? Given that VLMs have much larger parameters, they should not be much less competitive, given that you trained with a discriminative loss. I wish to know about possible reasons behind it, and how you plan to address it.\n2. (Not a deficiency) What is the potential of VaCo of becoming a VLM generalist: can replace DINO, VGGT etc? What else are needed to achieve this goal?\n\nI am planning to rate 7. I am giving a slightly lower score than I expect, but if other reviewers are confident that this idea is novel, and authors can address my concerns, I will raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s9a5lZeIuM", "forum": "Qvx7G5Fsy0", "replyto": "Qvx7G5Fsy0", "signatures": ["ICLR.cc/2026/Conference/Submission2982/Reviewer_m47S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2982/Reviewer_m47S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761483093331, "cdate": 1761483093331, "tmdate": 1762916479371, "mdate": 1762916479371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method called VaCo (Vision-Centric Activation and Coordination) to enhance the visual understanding capabilities of Multimodal Large Language Models (MLLMs). VaCo leverages visual priors from multiple Vision Foundation Models (VFMs) to activate and coordinate visual representations within MLLMs. Specifically, the paper proposes Modular Task Queries (MTQs) and Visual Alignment Layers (VALs) to activate task-specific visual signals inside the MLLM, while a Token Gateway Mask (TGM) is used to resolve representation conflicts across different tasks. VaCo demonstrates strong performance across various vision-language benchmarks, validating its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduce some extra modules for improving the visual comprehension performance of MLLMs. e.g. Introducing MTQs and VALs to activates task-specific visual information within the MLLM, avoiding computational overhead caused by directly feeding multiple VFM features into the model. Introducing TGM effectively addresses representation conflicts among task-specific queries, improving model stability and performance.\n2. The paper conducts extensive experiments on multiple benchmarks (e.g., MMBench, MMMU, RefCOCO, VCR) at general benchmarks as well as at region-level and scene-level tasks."}, "weaknesses": {"value": "1. The paper is hard to follow. The paper's own work and the discussion of existing work are often mixed together, the introduction of the methods in this paper is often interrupted, which might hinder my understanding of the ingenuity of the modules proposed in the paper\n2. It is expected that more training with more parameters and more data will lead to better performance. The paper didnot demonstrate that such performance improvement is more competitive than that of just scaling up under the framework presented in the paper.  Since the absolute performance definitely can't compare to some of the models on the leaderboards of the benchmark."}, "questions": {"value": "1.  what is the computational cost during training? How to balance training scale and performance?\n2.  There are some hyper-parameters, e.g. the length and structure of MTQs , how to chose ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vDGU3rzvzW", "forum": "Qvx7G5Fsy0", "replyto": "Qvx7G5Fsy0", "signatures": ["ICLR.cc/2026/Conference/Submission2982/Reviewer_P8bA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2982/Reviewer_P8bA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644294596, "cdate": 1761644294596, "tmdate": 1762916479030, "mdate": 1762916479030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VaCo, a training method for MLLMs that enhances visual understanding by incorporating supervision from multiple vision foundation models (VFMs). The approach introduces several main components: (1) Modular Task Queries (MTQs) as learnable tokens for specific visual tasks, (2) Visual Alignment Layers (VALs) that project MTQs into task-specific spaces supervised by frozen VFMs, and (3) Token Gateway Mask (TGM) to prevent conflicts between different task queries. The method shows consistent improvements across various benchmarks while maintaining single-encoder inference efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Motivation is good. Addresses the real limitation that text-only supervision in MLLMs may lead to degradation of visual information,\n- This paper gives a novel view that discriminative alignment outperforms generative reconstruction for visual understanding, as shown in Table 4(a)."}, "weaknesses": {"value": "- The main concern is that the method is a bit incremental. 1) Learnable queries in MTQ are well-established in [1]. 2) Token gateway mask: TGM is essentially a simple masking strategy without fundamental innovation. The combination feels incremental rather than providing new insights into multimodal learning.\n- The perception results in figure 5 are only selected visualization without any quantitative results, which are hard to show the real perceptual quality compared to source VFMs.  This can raise a question that whether MTQs fail to capture task-specific information?\n\n[1] DreamLLM: Synergistic Multimodal Comprehension and Creation. ICLR 2024."}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "csSXZk2kIo", "forum": "Qvx7G5Fsy0", "replyto": "Qvx7G5Fsy0", "signatures": ["ICLR.cc/2026/Conference/Submission2982/Reviewer_XRGa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2982/Reviewer_XRGa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934968279, "cdate": 1761934968279, "tmdate": 1762916478818, "mdate": 1762916478818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}