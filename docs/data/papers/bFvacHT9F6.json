{"id": "bFvacHT9F6", "number": 12525, "cdate": 1758208412473, "mdate": 1763650317231, "content": {"title": "Efficient Reasoning via Thought Compression for Language-Guided Segmentation", "abstract": "Chain-of-thought (CoT) reasoning has significantly improved the performance of large multimodal models in language-guided segmentation, yet its prohibitive computational cost, stemming from generating verbose rationales, limits real-world applicability. We introduce WISE (Wisdom from Internal Self-Exploration), a novel paradigm for efficient reasoning guided by the principle of \\textit{thinking twice---once for learning, once for speed}. WISE trains a model to generate a structured sequence: a concise rationale, the final answer, and then a detailed explanation. By placing the concise rationale first, our method leverages autoregressive conditioning to enforce that the concise rationale acts as a sufficient summary for generating the detailed explanation. This structure is reinforced by a self-distillation objective that jointly rewards semantic fidelity and conciseness, compelling the model to internalize its detailed reasoning into a compact form. At inference, the detailed explanation is omitted. To address the resulting conditional distribution shift, our inference strategy, WISE-S, employs a simple prompting technique that injects a brevity-focused instruction into the user's query. This final adjustment facilitates the robust activation of the learned concise policy, unlocking the full benefits of our framework. Extensive experiments show that WISE-S achieves state-of-the-art zero-shot performance on the ReasonSeg benchmark with 58.3 cIoU, while reducing the average reasoning length by over \\textbf{5$\\times$}---from 112 to just 23 tokens.", "tldr": "", "keywords": ["Efficient Reasoning", "Reasoning Segmentation", "Thought Compression"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7477516b3832847029e4d231a202f699f1afc06a.pdf", "supplementary_material": "/attachment/34cf8b4c72b619cfc892fe2b64fb03eddf117705.pdf"}, "replies": [{"content": {"summary": {"value": "WISE trains a model to generate a structured sequence: a concise rationale, the final answer, and then a detailed explanation. By placing the concise rationale first, the method leverages autoregressive conditioning to enforce that the concise rationale acts as a sufficient summary for generating the detailed explanation. This structure is reinforced by a self-distillation objective that jointly rewards semantic fidelity and conciseness, compelling the model to internalize its detailed reasoning into a compact form. At inference, the detailed explanation is omitted."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The unique training structure is reinforced by a self-distillation objective that explicitly rewards the semantic fidelity between the concise rationale and the detailed explanation, while penalizing the verbosity of the former. This process encourages the model to internalize its elaborate reasoning capabilities into a compact, efficient policy.\n\nTo ensure this learned policy is robustly activated at inference—where the detailed explanation is entirely omitted to maximize speed—the WISE framework culminates in WISE-S, a simple, zero-overhead prompting strategy.  This final adjustment injects a brevity-focused instruction into the user’s query, mitigating the conditional distribution shift between training and inference and ensuring the model consistently defaults to its more efficient reasoning mode."}, "weaknesses": {"value": "The novelty of this work may be limited. The method mainly uses GRPO framework. It generally rewards the model to generate a short concise reasoning step with reinforcement learning. This idea has been explored in various works such as [R1,R2]. During the training, it includes both long and short thinking tokens, which is similar to [R2] to train the model on paired longform and short-form responses for each query, ensuring it can generate both styles. During inference, to save tokens, it simply use promopts to ask the model not output the long thinking tokens. The technical contribution may be limited.\n\nIn experiments, it only experiments with 7B VLM. It is better to experiment with more models from different families with different sizes to demonstrate the general performance. \n\nIn experiments, compared with the original Seg-Zero without any efficiency, it can save reasoning tokens with significant efficiency improvements. However, compared with other baseline methods optimized for brevity via reward shaping such as L1-Exact and L1-Max. The improvements seem to be marginal. For example, from Table 3, L1-Max needs 11 tokens to achieve 78.9 cIoU, while the proposed method achieves 79.1 with 24 tokens for RefCOCOtestA. It seems that the baselines are also effective and the proposed method leads to marginal improvements. \n\nIn table 2, the reported Seg-Zero-7B results from the original paper is actually wrong. From the original paper, these results are not 7B, but  3B. It is not fair to compare the 3B results with 7B. And I am not sure whether the re-evaluated Seg-Zero resutls are from 3B or 7B models. \n\n\nIt is built on Qwen2.5-VL-7B and SAM2. It is better to report their original results without finetuning.\n\n \n[R1] Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning\n\n[R2] Thinkless: LLM Learns When to Think"}, "questions": {"value": "see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mfAcbh9Po0", "forum": "bFvacHT9F6", "replyto": "bFvacHT9F6", "signatures": ["ICLR.cc/2026/Conference/Submission12525/Reviewer_wBAt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12525/Reviewer_wBAt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766284976, "cdate": 1761766284976, "tmdate": 1762923391184, "mdate": 1762923391184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WISE (Wisdom from Internal Self-Exploration), a framework for efficient reasoning segmentation in large multimodal models (LMMs). The core innovation lies in compressing verbose Chain-of-Thought (CoT) reasoning into concise rationales while maintaining performance. Key contributions include:  \n\n- A structured generation sequence (concise rationale → answer → detailed explanation) that enforces reasoning compression via autoregressive conditioning.  \n- A self-distillation objective that rewards semantic fidelity between concise and detailed rationales while penalizing verbosity.  \n- WISE-S, an inference-time prompting strategy that reduces reasoning length by 5× (from 112 to 23 tokens) while achieving state-of-the-art zero-shot performance on ReasonSeg (58.3 cIoU)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**: The idea of *self*-distillation (training a single model to compress its own reasoning) is novel in the CoT segmentation domain. Unlike prior work (e.g., Seg-Zero’s RL-based reasoning), WISE explicitly decouples training-time reasoning depth from inference-time efficiency through its structured action space.  \n**Clarity**: The methodology is well-structured, particularly the hierarchical reward formulation (Eq. 3-6) and the distinction between WISE/WISE-S inference modes.  \n**Significance**: Addresses a critical barrier (computational cost of CoT) for real-time applications like robotics. The efficiency-performance trade-off breakthrough (Table 3) is practically impactful."}, "weaknesses": {"value": "**Limited Generalization Evidence**: While ReasonSeg and RefCOCOg are used, there is no validation on other reasoning-heavy benchmarks (e.g., VCR or GQA). The claim of \"out-of-domain reasoning\" in the abstract lacks supporting experiments beyond ReasonSeg.  \n**Shallow Analysis of Prompt Engineering**: The WISE-S prompting strategy (e.g., “one sentence”) is under-explored. The paper does not test alternative brevity prompts or quantify their sensitivity, raising concerns about robustness.  \n**Incomplete Cost-Benefit Analysis**: While token counts are reduced, actual latency/energy savings are not measured. For real-world deployment, hardware-level efficiency metrics (e.g., FPS on edge devices) would strengthen the claims."}, "questions": {"value": "**Q1**: How does WISE scale with model size? The experiments use a 7B model—would the compression mechanism remain effective for smaller (1B) or larger (70B) models?  \n\n**Q2**: The self-distillation reward relies on a pretrained SentenceTransformer for semantic similarity. Could this introduce bias? Was the similarity model domain-adapted for segmentation tasks?  \n\n**Q3**: The brevity prompt in WISE-S (“one sentence”) appears heuristic. Have the authors explored learned prompt tuning or gradient-based optimization for brevity?  \n\n**Q4**: The computational cost-saving of WISE is unclear. How does it compare to Seg-Zero in inference time beyond token counts?  \n\n**Rebuttal Potential**: Addressing Q1/Q3 could demonstrate broader applicability, while resolving Q2/Q4 would strengthen the technical rigor. A deeper exploration of alternative efficiency metrics (Q4) might elevate the significance from incremental to transformative."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "76CrLnGbAC", "forum": "bFvacHT9F6", "replyto": "bFvacHT9F6", "signatures": ["ICLR.cc/2026/Conference/Submission12525/Reviewer_cThw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12525/Reviewer_cThw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791209081, "cdate": 1761791209081, "tmdate": 1762923390854, "mdate": 1762923390854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work trains the model to generate a structured, three-part response: a concise rationale ($\\tau_c$), the final answer (A), and a detailed explanation ($\\tau_d$). It achieves Thought Compression for the Language-Guided Segmentation task by designing a distillation reward during RL training to enforce that $\\tau_c$ acts as a sufficient summary for generating $\\tau_d$. Furthermore, a brevity-oriented instruction is incorporated when inference to further compress reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea is sound: achieving thought compression by leveraging the $(\\tau_c, a, \\tau_d)$ structure and designing a distillation reward in RL training to compel $\\tau_c$ to be a sufficient statistic for $\\tau_d$'s generation.\n\n2. Good performance: achieving good performance in benchmarks"}, "weaknesses": {"value": "1. WISE was trained based on Qwen2.5-VL-7B, but Qwen2.5-VL-7B was not used as a baseline in subsequent experiments, which seems insufficiently rigorous and cannot rule out the possibility that the fundamental performance of the Qwen2.5-VL-7B model is already strong enough.\n\n2. While the concept of \"thought compression\" is central to the paper, the terminology could benefit from further clarification. For example, the distinction between \"concise rationale\" and \"detailed explanation\" might not be immediately clear, especially within  the field of tasks chosen by the author, it is not as clear as math reasoning tasks. A more explicit definition or example earlier in the paper would help.\n\n3.  Task suitability: The necessity of thought compression for the Language-Guided Segmentation task is debatable. Compression on a $\\sim$100-token baseline, regardless of whether a long CoT is needed, makes little sense. The method seems task-agnostic; it should be attempted on tasks requiring much longer CoT (e.g., several thousand tokens), like mathematical reasoning.\n\n4. Unsubstantiated explanation for WISE-S: The explanation that omitting $\\tau_d$ generation at inference time causes a distribution shift is insufficient. The relevance of the brevity-focused prompt (injected by the WISE-S strategy) to the distribution shift is also doubtful.  Supplementary experiments are required to verify the unique effect of the short prompt on the WISE model, e.g., testing if Seg-Zero-7B + brevity-focused prompt shows a similar phenomenon.\n\n5. Limited model scales and series: The experiments only use Qwen2.5-VL-7B; more model sizes and series should be included."}, "questions": {"value": "1. How is the omission of $\\tau_d$ generation realized? Is it only via prompt or by decoding intervention?\n\n2. In Table 6, why do the results for WISE-F ($\\text{+$\\tau_c$ + $\\tau_d$}$) and WISE ($\\text{+$\\tau_c$ − $\\tau_d$}$) differ?\n\n3. Could the author provide several cases that include $\\tau_d$ to verify whether $\\tau_c$ acts as a sufficient summary, and to demonstrate the impact of the proposed method on the quality of the generated $\\tau_d$?\n\n4. In Table 2, the performance of WISE-7B-S is generally better than WISE-7B, while in Table 1 it cannot surpass WISE-7B. Have the authors considered why such results occur? Is it related to using a subset of RefCOCO as the training set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ob4rUR0BMQ", "forum": "bFvacHT9F6", "replyto": "bFvacHT9F6", "signatures": ["ICLR.cc/2026/Conference/Submission12525/Reviewer_L19B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12525/Reviewer_L19B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797815169, "cdate": 1761797815169, "tmdate": 1762923390508, "mdate": 1762923390508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WISE,  a training based CoT approach build on top of Seg-Zero that condenses the long reasoning to speed up inference. This is achieved by generating a \"concise rationale\" before the detailed explanation as form of self distillation. During inference (WISE-S) the explanation is discarded for efficiency.\nThe key contributions of the paper are:\n- WISE method for reasoning segmentation that doesn't need verbose reasoning at inference.\n- Conditional self distillation approach using the rationale - answer - explanation.\n- Strong experimental results and reduction of output tokens while retaining performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- 4.9x - 7.0x reduction in token overhead vs Seg-Zero with minimal performance loss.\n- Interesting idea to perform self distillation using the auto-regressive nature of the model and the summary before the detailed explanation.\n- Good experimental results with performance being consistently good across both reasoning and referring segmentation tasks.\n- Paper ablations are very good and clearly establish the motivation for the method and hyperparameters."}, "weaknesses": {"value": "- The model is the heavy reliance on the Seg-Zero paper, including method, scope and experiments which reduce the contribution over a general method.\n- Implementation is only on the Qwen2.5 model, showing this method works across models and training methods would be a big benefit. (see questions).\n- The self-distillation reward is only applied when IoU > 0.5 which means the compression learns only from successful paths. This could indicate a better performance on \"easier\" tasks and less of an improvement on harder datasets."}, "questions": {"value": "- To distinguish the paper, the authors should show performance on other model architectures (beyond Qwen) or pipelines.\n- The paper should include a direct measurement of inference speedup, not just token reduction, to gauge the practical impact more accurately.\n- More qualitative demos to showcase the method under very different scenarios.\n- An in depth comparison to identify which specific examples the model improves over Seg-Zero and what are the failure cases would be helpful to understand the scope and usefulness of the approach.\n- How does this method compare to naive token length reduction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3cFI3pBHU4", "forum": "bFvacHT9F6", "replyto": "bFvacHT9F6", "signatures": ["ICLR.cc/2026/Conference/Submission12525/Reviewer_KUmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12525/Reviewer_KUmF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958904162, "cdate": 1761958904162, "tmdate": 1762923390149, "mdate": 1762923390149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}